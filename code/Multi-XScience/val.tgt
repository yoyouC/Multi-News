- In his discussion of functional testing, Howden @cite stresses the need to identify input domains, and gives guidelines for systematic selection of test points for several types of input values that occur in scientific programs. Our basic philosophy is similar to Howden's; we develop this point of view further, by automating part of the selection process.
- Gourlay @cite presents a precise framework for the discussion of issues in testing. In his terminology, our test selection criteria are a special form of the test methods for the set-choice construction testing system. Gourlay reinterprets previously published discussions about the suitability of various test selection criteria. In our approach, we do not attempt to decide a priori which criteria are sufficient --- we leave that decision to the test designer. That is why we emphasize the importance of a language in which criteria are specified.
- In the area of spoken dialogue, @cite has proposed a method for adapting initiative in form-filling dialogues. Whenever the system rejects a user's utterance, the system takes more initiative; whenever the user gives an over-informative answer, the system yields some initiative. While this method has the potential of being automated, the method has been neither fully implemented nor empirically evaluated. @cite has evaluated strategies for dynamically deciding whether to confirm each user utterance during a task-oriented dialogue. Simulation results suggest that context-dependent adaptation strategies can improve performance, especially when the system has greater initiative. @cite and @cite have used reinforcement learning to adapt dialogue behavior over time such that system performance improves. We have instead focused on optimizing performance during a single dialogue.
- An early attempt to automatically classify words into semantic classes was carried out in the Linguistic String Project @cite . Semantic classes were derived from similar cooccurrence patterns of words within syntactic relations. Cooccurrence statistics were then considered at the class level and used to alleviate data sparseness in syntactic disambiguation.
- The approaches described in the previous section induce word similarity relationships or word clusters from cooccurrence statistics in a corpus. Other researchers developed methods which quantify similarity relationships based on information in the manually crafted WordNet thesaurus @cite . Resnik:92a,Resnik:95a proposes a node-based approach for measuring the similarity between a pair of words in the thesaurus and applies it to various disambiguation tasks. His similarity function is an information-theoretic measure of the informativeness of the least general common ancestor of the two words in the thesaurus classification. Jiang97 combine the node-based approach with an edge-based approach, where the similarity of nodes in the thesaurus is influenced by the path that connects them. Their similarity method was tested on a data set of word pair similarity ratings derived from human judgments.
- In section , we sketched the simple bracketing technique described by , which provided motivation for our chunking method. As far as other approaches are concerned, our work is most closely related to that of , who use Markov Models in a preprocessing step to reduce the number of tree segments (called supertags ) that can be assigned to a word in a lexicalised Tree Adjoining Grammar. This approach makes parsing more efficient, but it needs a large training corpus, has to fight a large amount of ambiguity and needs a subsequent parsing step (also see @cite for the use of explanation-based learning for this purpose).
- Symbolic NP chunkers usually rely on finite automata and or pattern matching, cf. @cite , @cite . presents a partial parsing technique based on cascaded finite automata. describe a POS tagger and shallow parser combining symbolic and stochastic processing via relaxation labelling .
- DVFS has been used to reduce the power consumption of sever systems by reducing the operating voltage and frequency of the CPU @cite @cite @cite . DVFS has been studied extensively in the literature with some studies suggesting that DVFS can considerably improve power consumption @cite @cite , while others show that for some applications the usage of DVFS can increase the overall energy consumption @cite , or that there is at least a need to do a full system analysis before deciding if DVFS is useful for a workload @cite @cite . The limitations of DVFS, e.g., being coarse grained with only few possible settings, have also been discussed @cite .
- CPU pinning has only recently been proposed as one possible way to save energy in data center servers. @cite analyse how CPU pinning impacts the energy efficiency and the performance interference of two colocated workloads. They conduct experiments on a real testbed -- an Intel Xeon server (Sandy Bridge), using workloads targeting the JVM (from DaCapo and ScalaBench benchmark suites). Throughput is used to quantify the performance of applications. Previous work also looked at the performance of databases when CPU pinning is used @cite . @cite achieve a speed-up of the execution time of multi-threaded applications by dynamically controlling CPU pinning in Xen hypervisor. @cite show that static CPU pinning can improve the performance of an application.
- Studies on using DVFS and CPU pinning do not usually consider the average and tail response times in the evaluation. Studies on using virtual machine elasticity do not consider the case of starting a new virtual machine (horizontal elasticity) instead of adding more cores to the running virtual machine (vertical elasticity). Almost all of the studies use benchmarking applications, with some of them using old benchmarks such as RUBiS @cite , or applications that are not widely used @cite .
- The utility of data from telecommunication system in geography and social science to improve urban planning has been increasingly investigated @cite @cite @cite @cite @cite . In @cite , Willessan presented how evidence obtained from mobile system plays a part in forensic investigation. @cite @cite @cite @cite provided a extensive coverage of the smart city applications adopting data from telecommunication system. However, these application mainly focus on mobile positioning only. @cite presented an interesting application adopting satellite images and linked geographic data to detect wildfire. Ontology and RDF stream processing had also been adopted to develop autonomous vehicles in @cite . A novel approach for spatiotemporal query linked data was reported in @cite . Scholte and Rozenkrane @cite have proposed a system to localize and track each ship, and send personalized alert to those that are expected to be in danger. However, this system might lose function when the communication device fails, and cannot detect an accident when the communication device on the ship is not functioning. @cite designed and developed an ontology for emergency notification, such as a typhoon approaching'', but not for detection tragedies already happened.
- There are several algorithms tackling and its related variants. Some of them only solve the variant of subgraph counting, our main focus is however on algorithms actually solving . Following @cite and @cite , we categorize the algorithms by the approach they use (see also @cite for more detailed description of the algorithms). Many of the approaches can be used both for induced and non-induced variants of the problem, while some algorithms are applicable only for one of them.
- Vast majority of known algorithms for the subgraph enumeration problem is based on the approach of representing the problem as a searching process. Usually, the state space is modelled as a tree and its nodes represent a state of a partial mapping. Finding a solution then typically resorts to the usage of DFS in order to find a path of mappings in the state space tree which is compliant with isomorphism requirements. The efficiency of those algorithms is largely based on early pruning of unprofitable paths in the state space. Indeed, @cite even measure the efficiency in the number of generated search tree nodes. The most prominent algorithms based on this idea are Ullmann's algorithm @cite , VF algorithm and its variants @cite @cite @cite @cite (the latest VF3 @cite only applies to ) and RI algorithm @cite . The differences between these algorithms are based both on employed pruning strategies and on the order in which the vertices of pattern graph are processed (i.e. in the shape of the state space tree).
- Another approach is based on constraint programming, in which the problem is modelled as a set of variables (with respective domains) and constraints restricting simultaneous variable assignments. The solution is an assignment of values to variables in a way such that no constraint remains unsatisfied. In subgraph isomorphism, variables represent pattern graph vertices, their domain consists of target graph vertices to which they may be mapped and constraints ensure that the properties of isomorphism remain satisfied. Also in this approach, a state space of assignments is represented by a search tree, in which non-profitable branches are to be filtered. Typical algorithms in this category are LAD algorithm @cite , Ullmann's bitvector algorithm @cite , and Glasgow algorithm @cite . These algorithms differ in the constraints they use, the way they propagate constraints, and in the way they filter state space tree.
- There are already some implementations based on the color coding paradigm, where the idea is to randomly color the input graph and search only for its subgraphs, isomorphic to the pattern graph, that are colored in distinct colors (see subs:idea for more detailed description). This approach is used in subgraph counting algorithms, e.g., in ParSE @cite , FASCIA @cite , and in @cite , or in algorithms for path enumeration described in @cite or in @cite . Each of these algorithms, after the color coding step, tries to exploit the benefits offered by this technique in its own way; although usually a dynamic programming sees its use. Counting algorithms as ParSE and FASCIA make use of specifically partitioned pattern graphs, which allow to use combinatorial computation. Weighted path enumeration algorithms @cite @cite describe a dynamic programming approach and try to optimize it in various ways. However, to the best of our knowledge there is no color coding algorithm capable of enumerating patterns of treewidth larger than 1.
- Our aim is to make step towards competitive implementation of color coding based algorithm for , in order to see, where this approach can be potentially beneficial against the existing algorithms. To this end, we extend the comparisons of algorithms @cite @cite @cite to color coding based algorithms, including the one proposed in this paper.
- Deep-learning based methods have been introduced to single image deraining by @cite , which boost the performance significantly. The authors decompose the input image into the low and high-frequency layers, and then build a deep residual network which learns a function mapping the high frequency parts to rain streaks. Real rain streaks have irregular distribution, since raindrops in the air have various appearances and occur at different distances from the camera. Therefore, a new rain model has been formulated as where @math is the number of rain streaks layers, @math represents the @math -th rain streaks layer with the same direction. Furthermore, the real rain streaks can be more complicated, especially in the case of heavy rain. The rain appearance is also formed by the accumulation of rain streaks, which is similar to mist or fog @cite . In order to imitate the real rainy environment, they propose an improved rain model as where @math is the atmospheric transmission, is the global atmospheric light @cite .
- Later, @cite presents a conditional generative adversarial network (GAN) and use the perceptual loss to refine the results. @cite develops a deep recurrent dilated joint rain streaks detection and removal network to remove the rain streaks. @cite proposes the multi-stage networks based on the recurrent neural network architecture to remove rain streaks in different directions. @cite presents a density-aware multi-stream connected network for deraining. By maintaining negative residual features, @cite builds a residual-guided network for removing the rain streaks from single images.
- Numerous task-oriented chatbots have been developed for commercial and recreational purposes. Most commercial chatbots today use a frame-based dialogue system, which was first proposed in 1977 for a flight booking task @cite . Such a system uses a finite-state automaton to direct the conversation, which fills a set of slots with user-given values before an action can be taken. Modern frame-based systems often use machine learning for the slot-filling subtask @cite .
- Natural language processing has been applied to other problems in the travel industry, for example, text mining hotel information from user reviews for a recommendation system @cite , or determining the economic importance of various hotel characteristics @cite . Sentiment analysis techniques have been applied to hotel reviews for classifying polarity @cite and identifying common complaints to report to hotel management @cite .
- The earlier work on coin classification approaches targeted modern-day coins, which is comparably straightforward since the use of modern technology for coin manufacturing ensures a uniform visual appearance concerning shape, depictions, and legend on the obverse and reverse sides. As a result, relatively simple image analysis schemes based on traditional approaches @cite @cite @cite achieved notable classification rates on modern coin datasets with as much as 2270 coin classes. Despite their success on modern coins, these approaches were shown to perform poorly on the task of ancient coin classification @cite . As a remedy, attempts on ancient coin classification incorporated additional analysis on visual depictions such as portrait recognition @cite , object recognition @cite @cite , and legend recognition @cite .
- The success of supervised learning mainly comes from the availability of abundant data for the offline training phase. However, in the case of ancient coins, the prevalent problem is the absence of training data due to their rareness and diversity, thereby leading to low recognition rates @cite . In comparison, the feature matching based techniques neither involve an offline training process nor do they require a large number of exemplary images. Even with three or four samples per class, the feature matching substantially outperforms the supervised learning methods @cite . Nevertheless, the online feature matching, as well as the search process it involves, make the feature matching methods computationally intensive. Besides, the complexity increases proportionally with the number of classes in the dataset @cite .
- The first exclusive method for ancient coins @cite uses a combination of local feature descriptors @cite @cite @cite to perform an exemplar-based classification. Zambanini and Kampel @cite apply dense correspondence-based feature matching called SIFT flow @cite . To improve the quality of local features matching, Zambanini al @cite employ the geometric consistency of the matched features. Similarly, a more customized descriptor for ancient coin classification called Local Image Descriptor Robust to Illumination Changes (LIDRIC) @cite is proposed to alleviate the sensitivity to illumination changes. To sum up, in the absence of training data, the feature matching-based methods achieve acceptable classification rates. However, they are not easily scalable to more extensive datasets. They disregard the inherent domain-specific knowledge, which is extremely important from a numismatics perspective to make the classification task complaint with the standard reference books in this subject.
- Unlike legends and portraits, the reverse motif is a discriminative visual cue that is less affected by wear and tear. Besides, a given reverse motif can be shared by coins of multiple classes. Therefore, the search space for the class of a given query coin image is aptly reduced by recognizing its reverse motif. This aspect makes the reverse motif-based coin classification coarse-grained that can further be refined by fine-grained classification methods @cite .
- Extracting road pixels in small image chips from aerial imagery has a rich history (e.g. @cite , @cite @cite , @cite , @cite , @cite ). These algorithms generally use a segmentation + post-processing approach combined with lower resolution imagery (resolution @math meter), and OpenStreetMap labels.
- Extracting road networks directly has also been attempted by a number of studies. @cite attempted road extraction via a Gibbs point process, while @cite showed some success with road network extraction with a conditional random field model. @cite used junction-point processes to recover line networks in both roads and retinal images @cite extracted road networks by representing image data as a graph of potential paths. @cite extracted road centerlines and widths via OSM and a Markov random field process. @cite used a topology-aware loss function to extract road networks from aerial features as well as cell membranes in microscopy.
- We build upon CRESI v1 @cite that scaled up narrow-field road network extraction methods; in this work we focus primarily on developing methodologies to infer road speeds and travel times, but also improve algorithmic performance and inference speed.
- State-of-the-art weakly-supervised action localization methods utilize both appearance and motion features, typically extracted from backbone networks trained for the action recognition task. The work of @cite proposes a framework that consists of a classification and a selection module for classifying the actions and detecting the relevant temporal segments, respectively. The approach uses a two-stream Temporal Segment Network @cite as its backbone and employs a classification loss for training. @cite , a two-stream architecture is used to learn temporal class activation maps and a class-agnostic temporal attention. Their combination is then used to localize the human actions. Classification and sparsity-based losses are used to learn the activation maps and temporal attention, respectively. Recently, @cite proposed a framework to learn temporal localization from video-level labels, where a classification loss and a triplet loss for matching similar segments of an action category in paired videos is employed. In this work, we propose a joint formulation with explicit loss terms to ensure the separability of learned action features, enhance the feature discriminability and delineate adjacent action instances.
- This paper follows an identical twin' experiment framework @cite , where experiment data to be used will be generated from simulation, instead of using real data. The reason is that real data often comes with noise that hides the true state of the bus route (e.g. noise from GPS data). A simulated synthetic data would enable us to control the level of noise in the data, and to evaluate the modelling results against the ground truth rather than noisy data. Figure shows the workflow of this study.
- We review closely-related representation learning techniques developed mainly for human pose estimation @cite , semantic segmentation and object detection, from three aspects: low-resolution representation learning, high-resolution representation recovering, and high-resolution representation maintaining. Besides, we mention about some works related to multi-scale fusion.
- The fully-convolutional network approaches @cite @cite compute low-resolution representations by removing the fully-connected layers in a classification network, and estimate their coarse segmentation maps. The estimated segmentation maps are improved by combining the fine segmentation score maps estimated from intermediate low-level medium-resolution representations @cite , or iterating the processes @cite . Similar techniques have also been applied to edge detection, e.g., holistic edge detection @cite .
- The fully convolutional network is extended, by replacing a few (typically two) strided convolutions and the associated convolutions with dilated convolutions, to the dilation version, leading to medium-resolution representations @cite @cite @cite @cite @cite . The representations are further augmented to multi-scale contextual representations @cite @cite @cite through feature pyramids for segmenting objects at multiple scales.
- An upsample process can be used to gradually recover the high-resolution representations from the low-resolution representations. The upsample subnetwork could be a symmetric version of the downsample process (e.g., VGGNet), with skipping connection over some mirrored layers to transform the pooling indices, e.g., SegNet @cite and DeconvNet @cite , or copying the feature maps, e.g., U-Net @cite and Hourglass @cite @cite @cite @cite @cite @cite @cite @cite @cite , encoder-decoder @cite , and so on. An extension of U-Net, full-resolution residual network @cite , introduces an extra full-resolution stream that carries information at the full image resolution, to replace the skip connections, and each unit in the downsample and upsample subnetworks receives information from and sends information to the full-resolution stream.
- Our work is closely related to several works that can also generate high-resolution representations, e.g., convolutional neural fabrics @cite , interlinked CNNs @cite , GridNet @cite , and multi-scale DenseNet @cite .
- The two early works, convolutional neural fabrics @cite and interlinked CNNs @cite , lack careful design on when to start low-resolution parallel streams, and how and where to exchange information across parallel streams, and do not use batch normalization and residual connections, thus not showing satisfactory performance. GridNet @cite is like a combination of multiple U-Nets and includes two symmetric information exchange stages: the first stage passes information only from high resolution to low resolution, and the second stage passes information only from low resolution to high resolution. This limits its segmentation quality. Multi-scale DenseNet @cite is not able to learn strong high-resolution representations as there is no information received from low-resolution representations.
- Some methods are proposed to deal with LLP problem based on SVM technology. @cite treat the mean of each bag as a super-instance and estimate a classifier based on support vector regression. @cite introduce a large-margin framework called proportion-SVM which jointly optimizes over the unknown instances labels and the known label proportions. For the above framework, the two methods, called alter- @math SVM, conv- @math SVM, have been proposed for LLP problem. Following the idea in @cite , the method in @cite is based on twin SVM and needs to solve two smaller binary classification problems. @cite discuss how to combine the proportion learning framework with Laplacian term and analyze the structured information in proportion learning problem. The method introduces the Laplacian term to exploit the geometric information of data points. @cite try to address the LLP problem via nonparallel support vector machines, where the method can improve the classifiers to be a pair of nonparallel classification hyperplanes. @cite build a LLP-NPSVM method by a generalized classifier that determines instance labels according to two nonparallel hyper-planes under the supervision of label proportion information.
- Some methods are proposed to deal with LLP problem based on probabilistic models. Hern 'a @cite adopt several versions of an Expectation-Maximization algorithm to learn a naive Bayes model which assumes conditional independence between the predictive variables. @cite propose a new learning framework from the Bayesian perspective by estimating the conditional class density to estimate the posterior probability. Meanwhile, with the deep belief networks model for estimating the log-probability, they rebuild the posterior probability for classification problem. @cite build a probabilistic approach applied to the US presidential election, which uses cardinality potentials to perform exact inference over latent variables during learning, and introduces a novel message-passing algorithm to extend cardinality potentials to multivariate probability models. @cite develop a models to estimate the relationship between political sentiment and demographics during the U.S. presidential election.
- Recently, Denoeux and Thierry @cite introduce a method based on the maximization of a generalized likelihood criterion, which can be interpreted as a degree of agreement between the statistical model and the uncertain data. @cite adopt a SVDD-based approach by introducing a confidence score for each input data point to detect outliers on uncertain data. @cite propose a method called uncertain one-class transfer learning, which is capable of constructing an accurate classifier on the target task by transferring knowledge from multiple source tasks whose data may contain uncertain observations. @cite describe how a belief-rule-based association rule is handled with the sensor data uncertainties.
- Regularization methods that are commonly used for fine-tuning ConvNets can be generally grouped into four categories: data perturbation, parameter norm penalty, dropout, and multi-task learning. Image augmentation as a popular form of image perturbation has been proven to be particularly useful to prevent ConvNets from overfitting. In this paper, we also assume to perturb our training instances by random augmentations. The supervision signal can be perturbed for better regularization as well. This can be achieved by learning to predict soft targets rather than hard binary ones as in @cite . In this work, label perturbation is not considered so that we can deliver more ablated studies on the effectiveness of auxiliary training objectives.
- The parameter norm penalty, or weight decay more specifically, has been one of the most common ways of regularization in training deep models. Our PtR leveraged weight decay by default, but we also evaluated PtR without weight decay to study its impact on accuracy. Another method being apparently similar to weight decay is to perform feature norm penalty (FNP) on the representation layer of a network @cite . Superficially, FNP would resemble to PtR if the regression target for PtR was towards a static norm of zero without involving randomness (an additional feature of PtR which should be noted is that it also balances objectives automatically). The technical difference and benefit of PtR over FNP will be elaborated in Appendix . Dropout is also one of the standard techniques to improve model regularization by temporarily shielding a part of the hidden units in the bottleneck layer and fully connected layers during training @cite @cite . For the VGG-16 structures which we employed in our experiments, dropout was also used after flattened hidden layers.
- For fine-grained vision tasks, a very recent approach @cite suggested to confuse'' the network by encouraging different class-conditional probability distributions to come closer together, thus reducing the inter-class distance. In the cases where only style transfer is considered, one can attempt to reduce the domain variance through certain metrics (i.e., perform domain adaptation) as in @cite @cite .
- In what follows, we provide a review of the relevant literature. One should note that by context, in this paper, we do not refer to the or context vector of seq2seq encoders @cite @cite @cite . Rather, we refer to the , , or context. A few studies only have focused on developing models that take that type of context into account. Most of these studies originate from NMT. We briefly describe them next.
- obtain a global context vector by feeding a fixed number of the previous source sentences to HAN. They then compare two ways of injecting it into the encoder-decoder model. First, they propose a approach, in which the encoder and or decoder hidden states are initialized with the context vector. Second, they experiment with an strategy in which the intra-sentence context vector of the encoder is concatenated with the global context vector and passed either (i) directly to the decoder, or (ii) after going through a filtering gate. However, unlike our mechanism and that of @cite @cite @cite , which all feature two gates, the mechanism of has only one gate. All strategies proposed by significantly improve performance, but first place is reached by a combination of the warm-start and gated techniques.
- and both extend the Transformer architecture @cite with a context encoder featuring self-attentional and feed-forward layers. Then, combine the context representation with the source representation produced by the basic Transformer encoder via a gating mechanism. They do not modify the decoder part of the Transformer.
- A number of papers @cite @cite @cite @cite try to achieve tally hiding, either by only calculating the winner(s), or via multi-party computation and other cryptographic means. An idea closer to RLTs is that of Random Sample Voting (RSV) by Chaum @cite . A scheme that seeks to implement RSV in a fully verifiable fashion is Alethea @cite . RSV typically samples a small and predetermined number of voters, regardless of the margins. In contrast, RLTs adjust the sample size to obtain the desired level of confidence in the reported outcome.
- The idea of Risk-Limiting Verification is somewhat analogous to Rivest's ThreeBallot protocol @cite . Recall that, in ThreeBallot, each voter can verify a random 1 3 of her cast ballot. Thus, RLV gives vote handles'' to a fraction of voters, whereas in ThreeBallot each voter gets a handle to a fraction of her vote.
- Primitive fitting is a classic topic in computer vision @cite , with a large number of methods targeting parsimonious shape approximations, such as generalized cylinders @cite and geons @cite . Efficient fitting of these primitives attracted a lot of research efforts @cite @cite @cite @cite . Since these methods analyze shapes independently, they are not expected to use the primitives consistently across different objects, which makes the result unsuitable for discovering a common structure in a collection of shapes, performing consistent segmentation, or correspondence estimation. To address these limitations some methods optimize for consistent primitive fitting over the entire shape collection @cite , or aim to discover a consistent set of parts @cite @cite @cite . The resulting optimization problems are usually non-convex, and thus existing solutions tend to be slow, require heuristics, and are prone to being stuck in local optima.
- Learning-based techniques offer a promising alternative to hand-crafted heuristics. Zhu al @cite use a Recurent Neural Network supervised by a traditional heuristic-based algorithm for cuboid fitting. Tulsiani al @cite use reconstruction loss to predict parameters of the cuboids that approximate an input shape, and thus do not require any direct supervision. Several recent techniques, concurrent to our work, extend this approach by using more complex primitives that can better approximate the surface, such as anisotropic 3D Gaussians @cite , categorie specifique morphable model @cite or superquadrics @cite . All of these techniques use a collection of simple hand-picked parametric primitives. In contrast, we propose to learn a set of deformable primitives that best approximate a collection of shapes.
- One can further improve reconstruction by fitting a diverse set of primitives @cite or constructive solid geometry graphs @cite . These methods, however, usually do not produce consistent fitting across different shapes, and thus cannot be used to discover common shape structures or inter-shape relationships.
- Neural network architectures have been used to facilitate the mesh fitting @cite , learning to predict the deformation of a template to reconstruct unstructured input point cloud. This approach is sensitive to the choice of the template. We demonstrate that our method improves the quality of the fitting by learning the structure of the reference shape. Neural mesh fitting has been also employed for geometrically and topologically diverse datasets that do not have a natural template. In these cases, meshed planes or spheres can be deformed into complex 3D structures @cite @cite . We extend this line of work by proposing a technique for learning the base shapes that are further used to approximate the shapes in the collection. Learning these elementary structures enables us to more accurately and consistently reconstruct the shapes in the collection.
- Entity linking is a well-known database problem that has attracted volumes of research in the literature, especially in the relational data setting. Readers are referred to @cite for details. In general, the existing works focus on two main directions: accuracy and efficiency. The accuracy concern is on finding true matches of different entity profiles when they refer to the same real-world entity without introducing false matches. A more specific term called efficacy is defined to mean accuracy in @cite . The efficiency issue is about alleviating the infeasible pairwise comparison of profiles, and making the linking process scalable in large data.
- For accurate entity linking and deduplication, early works on the subject examined many methods such as cosine similarity match, distance based match, TF IDF, and Soundex. The well known similarity measures for entity linking are summarized and reviewed in @cite ; and the work in @cite presents a comparative evaluation of some existing works.
- The efficiency problem has also drawn significant research attention. The complexity of calculating the exact similarity between profiles is @math . Given a large number of entity profiles, say @math million, the time for computing similarity is too long to be practical. Thus, several ideas have been introduced in the literature to address the problem, like canopy (sorting and moving window), hierarchical, bucketing (clustering), and indexing approaches. In practice, the indexing approach has been found to be more useful, resulting in the proposal of a plethora of indexing methods in the literature (see @cite @cite for surveys of techniques).
- Most data management and software companies claim to support entity linking in structured data, but the systems are often not available for evaluation. In contrast, a number of open source research frameworks are available on entity linking in text data. For example, @cite proposes a method to extend terms in texts using Wikipedia pages. @cite is a framework tagging terms in short texts by Wikipedia pages, which is then followed by the works in @cite @cite for software improvement. @cite and @cite are other tools that contain a three step implementation for linking entity-mentions in text to Wikipedia pages. The work in @cite sets up a framework for entity linking work to be tested and evaluated.
- @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite
- In the task of fake news detection, the main challenge is how to utilize information from different modalities to distinguish fake news posts and real news posts. Most of existing approaches focus on text content @cite @cite @cite @cite and social context @cite @cite which refers to information generated during the news propagation process on social networks. Recently, visual information has been shown to be an important indicator for fake news detection @cite @cite . With the popularity of multimedia content, researchers begin to incorporate visual information to detect fake news.
- Some early works use basic statistical features about attached images to help classify fake news posts, such as the number of attached images @cite @cite , image popularity and image type @cite . However, these statistical features are so basic that they can hardly represent complex distributions of visual contents in fake news.
- Visual forensics features are generally used for image manipulation detection. To evaluate the authority of attached images, some works extract visual forensics features, such as block artifact grids (BAG), to assist fake news detection. For example, the Verifying Multimedia Use task on 2015 @cite and 2016 MediaEval benchmark @cite provide seven types of image forensics features to help detect manipulated and misleading use of web multimedia content. Based on these provided forensics features, @cite extract advanced forensics features and combine them with post-based and user-based features in a semi-supervised learning scheme to tackle the news verification problem. However, most of these forensics features are crafted manually for detecting specific manipulated traces, which are not applicable for the real images attached in fake news. Besides, these hand-crafted features are labor-expensive and limited to learn complicated patterns, causing the poor generalization performance on the task of fake news detection.
- Inspired by the power of CNN, most existing works based on multimedia contents use a pre-trained deep CNN like VGG19 @cite to obtain general visual representations and fuse them with text information. Specifically, @cite first incorporates multimodal contents on social networks via deep neural networks to solve fake news detection problem; @cite proposes an end-to-end event adversarial neural network to detect newly-emerged fake news events based on multi-modal features; @cite presents a novel approach to learn a shared representation of multimodal information for fake news detection. However, these works focus on how to fuse the information of different modalities, ignoring effectively modeling visual contents. These visual features they adopted are too general to reflect the intrinsic characteristics of fake-news images due to the lack of task-relevant information, which degrades the performance of visual contents in fake news detection.
- Embar propose a Bayesian framework for estimating properties of the network, edge strengths as well as cascade properties, propagation trees @cite . Other Bayesian approaches of network inference aim to infer the underlying network when the exact infection time is unknown in both standard and online algorithms @cite @cite @cite . Additionally, some approaches based on MLE use Bayesian techniques at intermediate steps to improve results @cite . Only recently have Bayesian techniques have been introduced to quantify the uncertainty of network structure inference from observation of dynamics @cite @cite @cite @cite . Ghalebi highlight the need for general probabilistic frameworks for inference problems and propose the algorithm DYFERENCE that samples edge and node probabilities in an online algorithm for dynamic network inference @cite . Peixoto @cite recently proposed the most closely related work in an algorithm designed to jointly reconstruct network structure and community labels assuming a stochastic block model structure and highlights the benefits of recovering the full posterior distribution of networks.
- It is worth noting that Bayesian methods are extensively used in sampling exponential random graph models (ERGMs) and fitting coefficients of motifs to observed networks @cite @cite @cite . Fitting ERGMs can be extended to the network inference problem by observing information cascades instead of observing an existing network to infer ERGM coefficients.
- A main challenge in privacy-preserving data mining (PPDM) is countering the capabilities of skilled adversaries @cite @cite @cite @cite . Data modification (data perturbation) @cite @cite and encryption @cite @cite are two main approaches to PPDM. Methods based on encryption provide good security and accuracy. However, cryptographic methods often suffer from high computational complexity which make them unsuitable for large-scale data mining @cite . Compared to encryption, perturbation utilizes lower computational complexity, which makes it effective for big data mining @cite . Examples for perturbation techniques include noise addition @cite , geometric transformation @cite , randomization @cite , condensation @cite , hybrid perturbation (uses several perturbation techniques together) @cite .
- Data perturbation may allow some privacy leak since the data results are released in their original format @cite . Hence, a privacy model should identify the limits of private information protection disclosure mechanism @cite . Earlier privacy models include @math @cite , @math @cite , @math @cite , @math @cite . It has been shown that these models are vulnerable to different attacks such as minimality attack @cite , composition based attacks @cite and foreground knowledge @cite attacks. Differential privacy (DP) is trusted to provide a better level of privacy guarantee compared to previous privacy models @cite @cite @cite .
- Laplace mechanism, Gaussian mechanism @cite , geometric mechanism, randomized response @cite , and staircase mechanisms @cite are a few of the fundamental mechanisms used to achieve differential privacy. There are many practical examples where these fundamental mechanisms have been used to build differentially private algorithm methods. Differential Privacy for SQL Queries @cite , LDPMiner @cite , PINQ @cite , RAPPOR @cite , Succinct histogram @cite and Deep Learning with Differential Privacy @cite are a few examples of such practical applications.
- . Reinforcement learning (RL) has benefited many fields of computer vision, such as image cropping @cite and visual semantic navigation @cite . Regarding the optimization policy, RL can be categorized into the value-based methods, policy-based methods, and their hybrids. The value-based methods ( , deep Q-learning @cite ) are good at solving the problems in low dimensional discrete action space, but they fail in high dimensional continuous space. Although the policy-based methods ( , policy gradient @cite ) are capable to deal with the problems in continuous space, they suffer from high variance of gradient estimation. The hybrid methods, such as Actor-Critic algorithms @cite , combine their advantages and are capable for both of discrete and continuous action spaces. Moreover, by exploiting asynchronous updating, the Asynchronous Advantage Actor-Critic (A3C) algorithm @cite has largely improved the training efficiency. Therefore, we adopt the A3C algorithm to optimize both of our RG agent in continuous action space and our FD agent in discrete action space.
- . Due to the advantages of representing and reasoning over structured data, the graph neural network (GNN) has attracted increasing attention @cite @cite @cite . Graph convolutional network (GCN) generalizes CNN on graph, which therefore can deal with non-Euclidean data @cite . It has been widely applied in computer vision, , point cloud classification @cite , action recognition @cite . Another class of GNN combines graph with RNN, in which each node captures the semantic relation and structured information from its neighbors through multiple iterations of passing and updating, , message-passing neural network @cite , graph network block @cite . Each relation in the former class ( , GCN) is represented by a scalar in its adjacency matrix that is not adequate for modeling the complex context information in group activity. Therefore, our semantic relation graph is built under the umbrella of the latter class that each relation is explicitly represented by a learnable vector.
- Defining @math as a threshold that establishes a minimum allowed similarity or the maximum distance allowed between two occurrences of a Motif, there are two major definitions for the problem of Time Series Motif Discovery (TSMD), according to Mueen (2014) @cite :
- The Variable-Length Motif Discovery (VLMD) @cite is a method that has been proposed to automatically find a suitable set of variable length Motifs. VLMD iteratively separates Motifs of different sizes into groups based on similarity. Within a group, a representative Motif with a normalized minimum distance between pairs of subsequences is selected. Finally, the VLMD returns a set of useful representative Motifs, which is extremely small compared to all possibilities of sliding window lengths. The operation of VLMD consists of two steps. Firstly, it finds a set of groups of Motifs looking for all possible lengths of a sliding window to obtain Motifs of different lengths. If the current Motif and the previous Motif overlap, the current Motif is added to the same set of the previous Motif; Otherwise, a new Motif group is created. Then, for each group of Motifs, a representative Motif is selected with a minimum normalized distance for the others in the group.
- Therefore, the VLMD can return a small set of motifs from a given time series sequence; this method does not present an application in Word Segmentation area, and we not compare with this method because @cite did not provide enough detailed explanation to reproduce the experimental results.
- The operation of the map takes place in three phases: the organization phase, the convergence phase, and the clustering phase @cite . In the organization phase, nodes are dynamically inserted, removed and adjusted in the map in order to cover the regions of the input space in which the input patterns are found as well as possible. When an input pattern is presented for the network, a level of activation is computed for each node in the map, and the node with the highest activation is considered the winner of the competition. This activation level is an inverse function of the distance between the center of the node and the input pattern. An activation threshold, @math , is used for determining when if the winner node is close enough to be adjusted or if it is necessary to insert a new node in the map. The nodes that do not cluster a significant percentage of the input patterns are periodically removed.
- Unlike the spacing provided in the written text, the spoken words are rarely delimited by pauses, so children must learn somehow to identify the boundaries between words as they hear the sentences. Because the structure of words is significantly variable in all languages, it is difficult to know how a child between 9 and 15 months of age achieve this ability. Therefore, segmentation is a key step in language acquisition, specifically in lexical development @cite . It is worth pointing out that finding the exact boundaries of all words in a sentence is not strictly necessary for children to understand what is said. They actually need to recognize the words present in the sentence (simple or compound) in the correct order, what can be done despite the occurrence of certain boundary detection errors, as it is observed in young children @cite .
- First, visualization of DNNs is the most direct way of explaining knowledge hidden inside a DNN, which include gradient-based visualization @cite @cite and inversion-based visualization @cite . Zhou @cite developed a method to compute the actual image-resolution receptive field of neural activations in a feature map of a convolutional neural network (CNN), which is smaller than the theoretical receptive field based on the filter size. Based on the receptive field, six types of semantics were defined to explain intermediate-layer features of CNNs, including objects, parts, scenes, textures, materials, and colors @cite @cite .
- Beyond visualization, some methods diagnose a pre-trained CNN to obtain insight understanding of CNN representations. Fong and Vedaldi @cite analyzed how multiple filters jointly represented a specific semantic concept. Selvaraju @cite , Fong @cite , and Kindermans @cite estimated image regions that directly contribute the network output. The LIME @cite and SHAP @cite assumed a linear relationship between the input and output of a DNN to extract important input units.
- Compared to the post-hoc explanations of DNNs, some studies directly learn more meaningful CNN representations. Previous studies extracted scene semantics @cite and mined objects @cite from intermediate layers. In the capsule net @cite , each output dimension of a capsule may encode a specific meaning. Zhang @cite proposed to learn CNNs with disentangled intermediate-layer representations. The infoGAN @cite and @math -VAE @cite learned interpretable input codes for generative models.
- Formulating and evaluating the representation capacity of DNNs is another emerging direction. Novak @cite proposed generic metrics for the sensitivity of network outputs with respect to parameters of neural networks. Zhang @cite discussed the relationship between the parameter number and the generalization capacity of deep neural networks. Arpit @cite discussed the representation capacity of neural networks, considering real training data and noises. Yosinski @cite evaluated the transferability of filters in intermediate layers. Network-attack methods @cite @cite @cite can also be used to evaluate representation robustness by computing adversarial samples. Lakkaraju @cite discovered knowledge blind spots of the knowledge encoded by a DNN via human-computer interaction. @cite discovered potential, biased representations of a CNN due to the dataset bias. @cite learned the manifold of network parameters to diagnose DNNs. Recently, the stiffness @cite was proposed to evaluate the generalization of DNNs.
- As one of the important applications of distributed optimization, DML has received widespread attentions from researchers. Besides the ADMM scheme, many distributed approaches have been proposed in the existing literature, e.g., subgradient descent methods @cite , local message-passing algorithms @cite , adaptive diffusion mechanisms @cite , and dual averaging approaches @cite . Compared with these approaches, ADMM schemes achieve faster empirical convergence @cite , making it more suitable for large-scale DML tasks.
- For privacy-preserving problems, cryptographic techniques @cite @cite @cite are often used to protect the encrypted information not to be inferred when the key is unknown. In particular, homomorphic encryption methods @cite , @cite allow untrustworthy servers to calculate with encrypted data, and this approach has been applied in an ADMM scheme @cite . Nevertheless, such schemes unavoidably bring extra computation and communication overheads, which may not be suitable for large-scale deployment. Another commonly used approach to preserve privacy is random value perturbation @cite , @cite , @cite . DP has been increasingly acknowledged as the de facto criterion for non-encryption-based data privacy. This approach requires less costs but still provides strong privacy guarantee, though there exists a tradeoff between privacy and performance @cite .
- Privacy-preserving machine learning problems have also attracted the attention of many researchers recently. Under centralized learning scenarios, @cite proposed a DP solution for an empirical risk minimization problem by perturbing the objective function with well-designed noise. For the privacy-aware DML issue, @cite also gave a differentially private mechanism, where the underlying distributed approach is subgradient descent. The works @cite and @cite present two dynamic DP schemes for ADMM-based DML, where the privacy guarantee is provided in each iteration. However, if a privacy violator uses the published information in all iterations to make inference, there will be no privacy guarantee. In addition, an obfuscated stochastic gradient method via correlated perturbations was proposed in @cite , though it cannot provide DP preservation. Different from these works, we remove the trustworthy servers assumption in this paper. Moreover, we take the distinct sensitive levels of data pieces and the diverse trust degrees of servers into consideration, and propose the PDML framework which provides heterogeneous privacy preservation.
- Applying deep-RL to multi-agent decision-making is an active area of research. made early contributions to this field by demonstrating how algorithms such as TRPO, DQN, DDPG, and A3C can be extended to a range of cooperative multi-agent problems @cite . made further contributions to the field of multi-agent deep-RL with the development of multi-agent deep deterministic policy gradients (MADDPG) that was capable of training in cooperative and competitive environments @cite . Multi-agent reinforcement learning is challenging due to the problems of and . The non-stationarity problem arises when a learning agent assumes all other learning agents as part of the environment dynamics. Since, the individual agents are continuously changing their policies, the environment dynamics from the perspective of an agent is continuously changing @cite . While attempt to address the non-stationary problem, MADDPG is still shown to become ineffective at learning for systems with more than three or four agents @cite .
- partially address the non-stationary problem by employing whereby groups of homogeneous agents use identical copies of parameters for their local policies @cite . Parameter sharing techniques can give rise to the second fundamental challenge of multi-agent learning: ---i.e. the challenge of identifying which actions from which agent at which time were most responsible for the overall performance (returns) of the system. avoid explicit treatment of this problem by focusing on environments where the joint rewards can be decomposed into local rewards. However, in general, such local reward structures are not guaranteed to optimize joint returns @cite .
- Finally, we address the recently reported research results that are related to parallel linear solvers that involve sparse triangular solvers. Gonzaga de reported their intensive numerical test results to evaluate various reordering techniques in the ICCG method in @cite . Gupta introduced a blocking framework to generate a fast and robust preconditioner based on ILU factorization in @cite . developed a couple of ILU-based preconditioners on GPUs in @cite . reported the evaluation results of HPCG implementations using nodal and block multi-color orderings on the ARM-based system, which confirmed the superiority of the block coloring method in @cite .
- In this paper, we proposed a parallel ordering that is different from the techniques described above. To the best of our knowledge, there is no parallel ordering method that vectorizes the sparse triangular solver while maintaining the same convergence and number of synchronizations as the block multi-color ordering. Since the vectorization of SpMV has been intensively investigated @cite @cite , one of conventional approaches is the use of multi-color ordering, in which the substitution is represented as an SpMV in each color. However, the multi-color ordering suffers from the problems of convergence and data locality, which are also indicated in the latest report @cite . When we consider the numerical results and mathematical properties of the proposed hierarchical block multi-color ordering, it can be regarded as an effective technique for multithreading and vectorizing the sparse triangular solver.
- There does not exist much literature on adversarial attacks on modulation classification. Recently, @cite used a variant of fast gradient sign method (FGSM) attack @cite on modulation classification on CNN-based modulation classification to highlight the threat of the adversarial examples. FGSM is an adversarial sample crafting algorithm where the adversarial perturbation is calculated by taking a gradient step in the direction of the sign of the gradient of test example. @cite also crafted the adversarial examples for modulation classification by using the FGSM perturbation generation algorithm. Most of the available results on the application of the adversarial attacks are reported by using the FGSM attack.
- A shortcoming with the FGSM attack is its lack of optimality in adversarial perturbation generation as FGSM was designed to quickly craft adversarial examples irrespective of the optimality and the size of the perturbation in the test example. To overcome the lack of optimality and to highlight that optimal adversarial example for modulation classification can be crafted we have used Carlini & Wagner (C-W) attack @cite where the adversarial examples are crafted using the following optimization process provided in equation .
- @PARASPLIT Recently, hybrid approaches to planning have attempted to combine the complimentary benefits of different planning schemes. For example, BIT* @cite combines graph-based and sample-based planning by using a heuristic function to guide a series of random geometric graphs towards the goal state. Its successor, RABIT* @cite performs BIT* while running trajectory optimization when constructing edges between samples, thus reducing sampling complexity by decreasing the number of samples discarded. GPMP-GRAPH @cite , an extension of GPMP2 @cite , constructs a graph of interconnected trajectory initializations, similar to the one shown in Fig. , on a factor graph that when optimized can simultaneously evaluate an exponential number of initializations. After optimization, any path through this graph provides a feasible trajectory. Thus, it has the ability to find solutions in multiple unique homotopy-classes all at once.
- While these planning approaches are typically employed in static environments, a number of algorithms @cite have been proposed to handle navigating dynamic environments. Lifelong Planning A* @cite , D* lite @cite and comparable online planning algorithms outperform repeatedly replanning from scratch by incrementally finding shortest paths on a graph with changing edge costs. RRT-X @cite an extension of RRT is suited to environments with moving obstacles and provides comparable runtime performance. RAMP @cite draws from evolutionary computation by maintaining a population of trajectories and evaluating their respective quality (fitness) with respect to a cost function and has been shown to discover and exploit new homotopies. Since solutions can go in and out of feasibility as the environment changes access to solutions in multiple homotopy classes is beneficial. However, there exists a gap in current research with respect to solving the online planning problem by leveraging trajectories across different homotopy classes.
- In this work, we present a novel trajectory optimization algorithm, Planning Online by Switching Homotopies ( ), to handle such scenarios. We build on GPMP-GRAPH @cite where multiple trajectories are inter-connected and represented as a factor graph upon which probabilistic inference is performed to optimize the entire graph. However, instead of retaining the same optimized trajectory from the initial time step for execution as GPMP-GRAPH does, POSH maintains and updates the entire graph at every time step. Specifically, at any time step the graph is pruned to remove unreachable (in time) states and is then reoptimized considering changes in the environment to find the new optimal trajectory. This grants our algorithm the unique ability to dynamically switch between different homotopy classes as illustrated in Fig. and allows it to better contend with dynamic environments as demonstrated by our experiments.
- Deep neural networks are dominant in the text matching area. Semantic alignment and comparison between two text sequences lie in the core of text matching. Early works explore encoding each sequence individually into a vector and then building a neural network classifier upon the two vectors. In this paradigm, recurrent @cite , recursive @cite and convolutional @cite @cite networks are used as the sequence encoder. The encoding of one sequence is independent of the other in these models, making the final classifier hard to model complex relations.
- More effective models can be built if inter-sequence matching is allowed to be performed more than once. CSRAN @cite performs multi-level attention refinement with dense connections among multiple levels. DRCN @cite stacks encoding and alignment layers. It concatenates all previously aligned results and has to use an autoencoder to deal with exploding feature spaces. SAN @cite utilizes recurrent networks to combine multiple alignment results. This paper also proposes a deep architecture based on a new way to connect consecutive blocks named augmented residual connections, to distill previous aligned information which serves as an important feature for text matching.
- Little work has addressed a problem on how to optimally select defense mechanisms for the IoT. Rullo @cite developed a resource allocation mechanism to ensure the security of the IoT based on an a Stackelberg game. Based on the decisions from the attack-defense game, this work derived the best security resource allocation plan to achieve multiple system goals in terms of minimizing maximal risk, maximal criticality, energy consumption, and allocation cost. Rullo @cite also took another approach to develop an optimal security resource allocation for an IoT network with mobile nodes based on GAs. Both works @cite @cite assume that an attacker can proceed a next attack after it can compromise a security resource. However, it is not realistic as highly sophisticated, stealthy attackers can directly compromise other components of a system while not being detected. Further, these works focus on device-level security but do not concern system-level security.
- La @cite introduced a game theoretic model to analyze the security of honeypot-enabled IoT networks. They assumed that the attacker may deceive the defender with suspicious or seemingly normal traffic and used the honeypot-enabled intrusion detection component which reroutes the suspicious traffic to the honeypots as a defense mechanism. The interaction between the attacker and defender was modeled based on a Bayesian game with incomplete information. Anirudh @cite used honeypots for online servers to mitigate Distributed-Denial-of-Service (DDoS) attacks launched from the IoT devices. Pa @cite developed an IoT honeypot to emulate the IoT devices and capture Telnet-based attacks and designed the IoT sandbox to analyze these attacks against the IoT devices running different CPU architectures. Dowling @cite created a honeypot that simulates a ZigBee gateway and used it to capture attacks for a further analysis.
- There is a large volume of work on image captioning. Here we review methods based on object detection, which are most similar to our work. Currently, end-to-end deep learning approaches @cite @cite are most effective for image captioning. These methods try to learn to generate captions from global image features. However, using a global feature limits their interpretability. Other work @cite @cite @cite @cite @cite @cite employs object-level semantics to generate captions. These methods represent images based on occurring semantic concepts or objects. @cite , explicitly detected objects are employed with their category, size and layout to generate captions. In contrast, based on a bag of word model, impressive captions can also be generated by only using the explicitly detected objects @cite .
- Although deep learning models have led to significant progress in feature learning for 3D shapes @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite , 3D shape captioning has been less explored due to the lack of training dataset. However, the recently proposed 3D-Text dataset @cite has enabled the research in this area. @math Seq2Seq @cite employs multiple views as a 3D shape representation to address the cubic complexity of voxel representations. It learns 3D shape features by aggregating the global feature of each view. Although @math Seq2Seq can generate plausible captions for 3D shapes, it often fails to generate captions with local part details. To resolve this issue, ShapeCaptioner represents a 3D shape as a set of parts detected in multiple views, which not only avoids the cubic complexity of voxels but also enables the ability of capturing part characteristics. This leads to captions that are more similar to the manually annotated ground truth, which usually includes part details, such as color, form, material, and texture of parts shown in Fig. .
- @cite is a benchmark for large-scale graph analysis platforms such as Giraph and GraphX. It comprises several parallel algorithms, standard datasets, synthetic dataset generators, reference output and evaluation of various metrics to quantify multiple kinds of system scalability and performance variability. This benchmark provides comprehensive evaluations of graph analysis platforms on various algorithms and datasets rather than an evaluation of the algorithms themselves (i.e., evaluating the accuracy of the algorithms themselves is outside the scope of this platform).
- Some existing datasets apply a down-facing camera attached to a hovering drone as the recording equipment. For example, in Stanford Drone dataset @cite , the utilization of drone eliminated occlusion so that all participants (pedestrians, cyclists, cars, carts, buses) were individually and clearly tracked. Another dataset HighD @cite , which focuses on vehicle-vehicle interaction on highway driving, also successfully demonstrated the benefit of using the hovering drone to remove occlusion.
- The classical way to perform type inference is the application of logical reasoning, e.g., via RDFS OWL entailment regimes @cite @cite . The resulting accuracy is highly dependent on the cleanliness and correctness of the statements in the knowledge base, though a number of works have attempted to reason on noisy semantic data @cite . Reasoning-based techniques are generally speaking considered as not suitable for cases where the knowledge base contains erroneous or conflicting statements @cite . In addition, logical reasoning only allows to infer information from the facts that are present in the dataset; it is unsuited to infer types when most of the values are missing.
- Many studies have proposed to synthesize realistic ASR errors, and augment them with translation training data, to enhance the robustness of the NMT model towards ASR errors @cite @cite @cite . However, most of these approaches depend on simple heuristic rules and only evaluate on artificially noisy test set, which do not always reflect the real noises distribution on training and inference @cite @cite @cite .
- Beyond the research on translation models, there are many research on the other relevant problems, such as sentence boundary detection for realtime speech translation @cite @cite @cite @cite @cite , low-latency simultaneous interpreting @cite @cite @cite @cite @cite @cite @cite , automatic punctuation annotation for speech transcription @cite @cite , and discussion about human and machine in simultaneous interpreting @cite .
- Focus on the simultaneous translation task, there are some work referring to the construction of the simultaneous interpreting corpus @cite @cite @cite . Particularly, @cite deliver a collection of a simultaneous translation corpus for comparative analysis on Japanese-English and English-Japanese speech translation. This work analyze the difference between the translation and the interpretations, using the interpretations from human simultaneous interpreters.
- Classical image deblurring methods estimate the blur kernel given a blurry image and then apply deconvolution to get the deblurred image. To calculate the blur kernel some techniques assume prior information about the image, and formulate maximum a-posertior (MAP) to obtain the deblurred image. Different priors such sparsity, @math gradient prior, patch prior, manifold Prior, and low-rank have been proposed to obtain regularized reconstructions @cite @cite @cite @cite @cite @cite @cite @cite @cite . @cite estimate the Fourier coefficients of the blur kernel to deblur the image. @cite learn the latent features of the blurry images using the latent features of the clean images to estimate the deblurred image. These CNN-based methods do not perform well compared to the state-of-the-art MAP-based methods for large motion kernels. Recent non-blind image deblurring methods like @cite @cite @cite @cite @cite assume and use some knowledge about the blur kernel. Given the latent blurry input, @cite estimate multiple latent images corresponding to different prior strengths to estimate the final deblurred image. CNN-based techniques like @cite and @cite estimate the blur kernel and address dynamic deblurring.
- The usage of semantic information for image restoration is relatively unexplored. While semantic information has been used for different object classes @cite @cite @cite @cite , a substantial body of literature has focused their attention to human faces @cite @cite @cite @cite . @cite extract the edges of face parts and estimate exemplar face images, which are further used as the global prior to estimate the blur kernel. This approach is complex and computationally expensive in estimating the blur kernel. Recently, @cite proposed to use the semantic maps of a face to deblur the image. Furthermore, they introduced the content loss to improve the quality of eyes, nose and mouth regions of the face. In contrast to these methods, we learn a multi-stream network which reconstructs the deblurred images corresponding to different classes in a facial semantic map. Furthermore, we propose a new loss to train the network.
- Human body part parsing. Human body part parsing is the task of generating body part masks based on human structures. An Auto-Zoom Network @cite is proposed to focus on certain body part regions. Xia @cite propose to jointly conduct body part parsing and pose estimation, and show that the two complementary tasks could help each other. Fang @cite propose a data augmentation method for part parsing based on pose similarities. Part parsing is related to human parsing @cite @cite @cite @cite @cite @cite , as both tasks aim to predict pixel level semantic masks for human parts. The major difference is that body part parsing focuses on predicting part masks that directly reflects body structures, for example regions such as upper lower arms. On the contrary, human parsing contains clothing and object classes such as sunglasses, hats, coats and etc. These classes are not directly related to human structures and are more appearance based.
- Weakly supervised semantic segmentation. Our study is also related to weakly supervised semantic segmentation. Frequently used weak supervisions for semantic segmentation include scribbles @cite @cite , bounding boxes @cite @cite @cite , points @cite and image labels @cite @cite @cite @cite . Despite the promising results achieved on the semantic segmentation task, existing weakly supervised segmentation methods require the saliency of target regions in color space. Without stronger supervisions or revised methods, directly applying these previous studies onto more complicated scenarios, such as scene parsing @cite and part parsing, generates limited performance. To solve the body part parsing task in this study, we utilize the human structure knowledge with the proposed pose based part prior. Our study is also related to previous explorations on transforming body poses @cite @cite .
- We assume that the Data Seller @math and Data Buyer @math are participating in a Data Exchange protocol, such as the Wibson protocol @cite , and have already completed the following steps: The Buyer @math has verified that the Seller @math belongs to the Buyers audience of interest. The Data requested is available. Buyer @math and Seller @math have agreed on a price that is acceptable to both parties.
- Challenges (1) and (2) are known as the problem of , which has been studied for decades. Study @cite showed that fairness is unachievable without the aid of a trusted third party. However, the blockchain can fill the role of the trusted party, and essentially eliminates the trust problem.
- The proposed algorithm, presented in this paper, requires semantic similarity scores between words. For example, it is required to determine if delicious food" and amazing steak" should be clustered into the same topic. There are two types of semantic similarity measures: corpus-based and knowledge-based @cite . In the corpus-based measures, the degree of similarity between words is identified using information derived from the corpus. The algorithms are usually based on the statistical analysis of word usage to capture the occurrences or co-occurrences of words within the text. The algorithms often cannot be expressed with a simpler equation or format. Metrics such as point-wise mutual information @cite , latent semantic analysis @cite , and topic modeling such as LDA @cite are typical corpus-based measures. Compared to the corpus-based approaches, knowledge-based measures utilize additional information obtained from semantic networks to quantify the degree to which two words are semantically associated @cite . These measures mainly use a lexical database, such as WordNet @cite @cite . The present paper proposes a modified knowledge-based similarity measure utilizing website such as thesaurus.com.
- Numerous methods have been proposed for sentiment analysis or opinion mining. As a remarkable reference, Ravi and Ravi @cite explored these numerous sentiment analysis methods at word, clause, sentence, and document levels. For sentiment classification, polarity determination algorithms generate ordered scores of topics. The polarity determination is classified into machine learning-based approaches @cite @cite @cite and lexicon-based approaches @cite @cite @cite @cite . In the machine learning-based approaches, the sentiment level is identified with statistical methods, such as Naive Bayes (NB), Support Vector Machine (SVM), and Neural Network (NN). The lexicon-based approaches employ a dictionary of sentiment words called @cite . For example, each word is assigned a sentiment score based on a predetermined sentiment dictionary or WordNet. In this paper, we use a lexicon-based scoring approach to avoid the sparsity problem and simplify our proposed algorithm. The details are introduced in the next section.
- Many MPI libraries such as MPICH @cite and OpenMPI @cite adopt ROMIO as the implementation for MPI-IO functions. For parallel jobs running multiple MPI processes per compute nodes, ROMIO selects one aggregator per node in its default settings. The selection of I O aggregators is implemented at file open time. If the underneath file system is a Lustre, the default setting changes to select the number of I O aggregators equal to the Lustre file striping count. This strategy produces a ono-to-one mapping between the aggregators and the file servers (OSTs) which avoids any possible file lock conflicts and achieves the best I O performance @cite . When the aggregate file access region is larger than the number of aggregators times the file stripe size, the two-phase I O is carried out in multiple rounds.
- An interesting special case, which is well-understood, is the single-dimensional mechanism design in which the values of the vector are linear expressions of a single parameter. The principal representative is the problem of scheduling related machines, where the cost of each machine can be expressed via a single parameter, its speed . This was first studied by Archer and Tardos @cite who showed that, in contrast to the unrelated machines version, an algorithm that minimizes the makespan can be truthfully implemented --- albeit in exponential time. It was subsequently shown that truthfulness has essentially no impact on the computational complexity of the problem. Specifically, a randomized truthful-in-expectation This is one of the two main definitions of truthfulness for randomized mechanisms, where truth-telling maximizes the expected utility of each player. PTAS was given in @cite and a deterministic PTAS was given in @cite ; a PTAS is the best possible algorithm even for the pure algorithmic problem (unless @math ).
- Scheduling is related to combinatorial auctions, where multiple items need to be assigned to a set of buyers. This is a broad and successful area, and the setting shares both aforementioned features of multi-dimensionality and lack of externalities, therefore insights and techniques can be transferred from the one problem to the other. However the difference is that the objective for combinatorial auctions is social welfare maximization , and this is known to be achieved by the VCG mechanism, albeit in exponential time. Hence the focus on this rich area is on what can be achieved by computationally efficient mechanisms (see for example @cite ). But in the case of the scheduling with the min-max objective, the flavor is more information theoretic, as we know that not even exponential time mechanisms can achieve the optimal makespan.
- Lavi and Swamy @cite proposed an interesting approach to attack the Nisan-Ronen question, by restricting the input domain, but still keep the multi-dimensional flavour of the setting. They assumed that each entry in the input matrix can take only two possible values low'' and high'', that are publicly known to the designer. In this case, they showed an elegant deterministic mechanism with an approximation factor of 2. Surprisingly, even for this special case there is a lower bound of @math . Yu @cite extended the results for a range of values, and @cite studied multi-dimensional domains where the private information of the machines is a single bit.
- The truthful implementation of other objectives have been explored by Mu'alem and Schapi -ra @cite for multi-dimensional problems and by Epstein and van Stee @cite for single-dimensional ones, giving a PTAS for a wide range of objective functions. Leucci, Mamageishvili and Penna @cite showed high lower bounds for other min-max objectives on some combinatorial optimization problems. In the Bayesian setting, Daskalakis and Weinberg @cite showed a mechanism that is at most a factor of 2 from the optimal truthful mechanism , but not with respect to optimal makespan. @cite provided bounds of prior-independent mechanisms (where the input comes from a probability unknown to the mechanism). Giannakopoulos and Kyropoulou @cite showed that the VCG mechanism achieves an approximation ratio of @math under some distributional and symmetry assumptions.
- The problem of control of generation by attributes is somewhat different to the problem of generating specified individuals (or fine-grained categories) tackled by @cite , because individuals are mutually exclusive in the training data and do not need to be produced in combinations. We focus here on control of generation by attributes. The first approaches to this problem (conditional VAEs @cite @cite @cite ) added attribute label information as an extra input to the encoder and the decoder. These approaches generate using a latent @math vector and also a label @math , where the @math vector often conflicts with the label. The @math vector has not been separated into components correlated with @math and components uncorrelated with @math , to remove any information that might conflict with @math . With conflicting inputs the best the VAE can do is to produce a blurry image.
- There is also a conditional Generative Adversarial Network (cGAN) which trains separate encoders for the @math and @math vectors, but does not try to remove potentially conflicting information @cite . The cGAN authors also note that the generator can fail to generate unusual attribute combinations such as a woman with a moustache; in GAN training the discriminator discourages the generator from generating samples outside the training distribution, but a VAE decoder is not so limited.
- More recent work has explicitly tackled this problem of separating the attribute information from the latent vector, using what we call an adversarial' approach @cite @cite @cite : This is inspired by Generative Adversarial Networks (GANs); a new auxiliary network is added (like the discriminator in a GAN) which attempts to guess the label of the latent vector @math . So long as the label can be successfully guessed then there is still label (attribute) information present in @math , and the encoder is penalised via an additional loss term. In this way the encoder and new auxiliary network compete, with the encoder trying to remove label information from @math . This has the effect of minimising the mutual information between @math vector and label.
- While this adversarial approach can successfully remove label information from @math , there is nothing to stop the decoder (generator) from associating other spurious information with the label. For example the decoder might associate the label intended to be for glasses' with an older or more masculine face. This is what we see in their results of two of the adversarial approaches (Figure. ). These examples have been reproduced from the cited papers @cite and @cite , where they are used to showcase how well those systems can independently manipulate attributes. Therefore we assume they are typical examples of the level of disentanglement achieved, and not unrepresentative examples.
- In @cite the label vector is a single binary variable so that the system can only be trained to control (or classify) one attribute. It is not unexpected that a generator will associate spurious information with a label if the association is present in the training data and the system has been trained only on examples labelling a single attribute, e.g., glasses. The system cannot know that it should isolate wearing glasses', and not wearing glasses and older'. @cite trained their system for all 40 attributes and show superior disentanglement when generating from specified attributes.
- @cite , also adversarial, take a slightly different approach in training a multi-dimensional space for an attribute, rather than a single dimension. In contrast to our focus on disentanglement, their focus is on allowing control of a larger variety in generation, for example, with glasses: a variety of rims and different styles of sunglasses', or with facial hair: both moustaches and beards of varying thickness'. They focused on two subsets of the CelebA dataset to train for the attributes glasses and facial hair, and their combination, but not simultaneously for all the 40 labels available.
- In addition to the above works on disentangling labelled attributes there is also work on the more difficult problem of unsupervised learning of disentangled generative factors of data @cite @cite @cite . However the supervised approaches @cite @cite @cite generate much clearer samples of selected attributes, and superior disentanglement. An alternative approach to controlled generation is to simply train a deep convolutional network and do linear interpolation in deep feature space @cite . This shows surprisingly good results, but in changing an attribute that should only affect a local area it can affect more image regions, and can produce unrealistic results for more rare face poses.
- In recent years there has been significant investigation into the use of computer vision methods to identify and classify a number of behavioral atypicalities, including non-intrusive (at a distance) video based monitoring of epileptic seizures in children @cite @cite @cite , human activity recognition @cite @cite and assisting social interactions at various levels @cite @cite @cite @cite @cite @cite . The advantage of computer vision techniques over certain other methods which may involve placement of measuring devices on the subjects body is the non-invasiveness (without interfering the patients) of the method @cite , a carefully placed camera is less likely to interfere with the session and likewise will reduce stress on the child in question. Such non-intrusiveness (measurement at a distance) would not alter the usual natural behavior of the child, thereby, improves the effectiveness of ASD assessment. These attributes are highly desirable by the clinicians and practitioners in the evaluation of ASD among children.
- Recently Dawson in @cite reported differences in motor function are an early feature of ASD. They explored midline head and body posture control by detecting facial landmarks and head pose angles of children. Their finding shows that toddlers with ASD exhibited significant higher rate of head movement as compared to non-ASD toddlers, suggesting difficulties in balancing midline head position while engaging attentional systems.
- In this section, we review the state-of-the-art research works on incentive mechanisms for crowdsourcing. @cite presented two general models of incentive mechanisms to motivate mobile users' participation: platform-centric model and user-centric model. D. @cite proposed a quality-based incentive mechanism for crowdsensing by rewarding participants proportionally to their contribution. Y. @cite presented a quality-driven auction-based incentive mechanism for a Wi-Fi fingerprint-based indoor localization system. In this direction, C. @cite also proposed a Quality of Information (QoI)-aware incentive mechanism to maximize the quality of information.
- In the long-term view, Lee and Hoh @cite proposed a mechanism, called RADP-VPC, that provides long-term incentives to participants to maintain participants and promote dropped ones to participate again. Similarly, L. @cite proposed a mechanism to provide long-term incentives to participants to achieve the maximum total sensing value and the minimum total sensing cost.
- Focusing on dynamic crowdsensing where participants arrive in an online manner, @cite presented two online incentive mechanisms using a multiple-stage sampling-accepting process. Similarly, Y. @cite proposed an online incentive mechanism to maximize the number of matched pairs of participants and crowdsourcing service users when participants and service users are dynamically changing.
- J. @cite proposed a behavior-based incentive mechanism for crowdsensing with budget constraint. This work aims to achieve both the extensive user participation and high quality sensing data submission, based on users behavior abilities. S. @cite presented an incentive mechanism for mobile phones with uncertain sensing time. In order to address the sensing time uncertainty problem, this work modeled the problem as a perturbed Stackelberg game where mobile phone users sensing task times may be different from their original plans. T. @cite proposed an incentive mechanism for heterogeneous crowdsourcing using all-pay contests where workers have not only different type information (abilities and costs), but also the different beliefs (probabilistic knowledge) about their respective type information. In this work, the belief is modeled as a probability distribution.
- X. @cite proposed truthful incentive mechanisms for the case where participants' cooperation may be required to finish a job. Using Tullock contest, T. @cite designed an incentive mechanism to maximize the crowdsourcing service users profit. L. @cite proposed an incentive mechanism for smartphone collaboration in distributed computing using contract theory under two different scenarios having complete and incomplete information of participants.
- However, the heterogeneity of task depreciation over time has not been addressed in the literature. Task depreciation models over time in the existing works are as the value of tasks immediately drop to zero after the given deadline. In other words, there can be only two kinds of task valuation, either full or null. Such dichotomous model overlooks the cases where task valuation remains valid even after the deadline, though depreciating in proportion to the amount of time past the deadline, which can be observed in practice and the literature @cite .
- Starting around 2013, word embeddings like word2vec @cite or GloVe @cite became popular as they were easy to train in an unsupervised fashion on raw text and they improved several downstream tasks when used as features. These word embeddings are invariant to the context the word is in. There has been work to contextualize these embeddings, mainly to account for synonyms (e.g. @cite @cite ) before, but only in 2018 did training of the contextualized embeddings using large deep neural networks and an unsupervised training scheme become popular.
- While ELMo @cite and ULMFiT @cite are based on LSTMs @cite , BERT and GPT are based on the transformer architecture @cite . This architecture outperforms LSTMs on several NLP tasks and we therefore concentrated on these two pre-trained models. The contextualized embedding for each input token is given by the corresponding output of the last encoder layer.
- BERT has been used for various NLP tasks, such as question answering on the SQuAD 2.0 dataset @cite . It also achieved new state-of-the-art results on the General Language Understanding Evaluation (GLUE) benchmark @cite and grounded commonsense inference (SWAG) @cite . All of these tasks are a form of classification or regression. fine-tuned BERT for Extractive Summarization.
- An analysis of different layers of the BERT model was performed by @cite . They found that the classical NLP pipeline appears in the expected sequence. In the context of our experiments in Section , this would mean that the DiscoFuse task profits the most from pre-trained information about POS, constituents, dependencies and semantic roles. A similar study by @cite found that BERT captures phrase-level information in the lower layers and linguistic information in intermediate layers, with surface features at the bottom, syntactic features in the middle and semantic features at the top.
- Addressing is known as a difficult problem. @cite and @cite has shown it is possible to adapt respectively the adversarial framework of @cite and MMD based methods @cite for learning invariant representation by re-weighting at each training step the domain discrepancy measure with an estimated class distribution in the target domain. In the context of Optimal Transport based discrepancy measure @cite @cite , @cite suggests to learn adversarially the class ratio incorporating it into the supremum over the dual critic function of the Wasserstein measure. (where @math may change while keeping @math constant) is also a current assumption for extending invariant representation methods in challenging context of distributional shift. They traditionally use a local-scale transformation for learning such @math @cite . @cite suggests a component-wise version of local-scale transformations for avoiding noisy features which can not be well matched. Those methods are naturally extended in their original work to the context of both target shift and conditional shift using class ratio estimation.
- refer to those triangulation methods that minimize the cost based on reprojection errors. Assuming that the image measurements are independently perturbed by the noise in the same distribution of certain types, the optimal methods find the maximum likelihood (ML) solution. For Gaussian and Laplacian distribution, the ML solution is to minimize the @math norm or @math norm of the reprojection errors, respectively @cite . This can be found in closed form by solving a polynomial of degree six or eight @cite . Alternatively, the @math solution can be obtained using iterative correction methods @cite @cite . While these iterative methods do not guarantee global optimality, they were shown to be faster and more stable. For a uniform distribution, minimizing the @math norm leads to the ML estimate for the lower bound of the noise @cite , and the solution is obtained by solving a quartic polynomial @cite . Unlike the @math and @math cost, the @math cost has a simple shape with a single minimum, but it is relatively more sensitive to noise and outliers @cite .
- Recently, @cite introduced a modification for the simple transfer learning approach, where they learn a cosine classifier @cite @cite instead of a linear classifier on top of feature extraction layers. The authors show that this simple approach is competitive with several proposed few-shot learning approaches if a deep backbone network is used to extract the feature representation of input data.
- @cite took inspiration from spatial context of a image to derive supervisory signal by defining the surrogate task of relative position prediction of image patches. Motivated by the task of context prediction, the pretext task was extended to predict the permutation of the shuffled image patches @cite @cite @cite . @cite leveraged the rotation in-variance of images to create the surrogate task of predicting the rotation angle of the image. Also, the authors of @cite proposed to decouple representation learning of the rotation as pretext task from class discrimination to obtain better results. Along the lines of context-based prediction, @cite uses generation of the contents of image region based on context pixel (i.e. in-painting) and in @cite @cite the authors propose to use gray-scale image colorization as a pretext task.
- Apart from enforcing structural constraints, @cite uses cluster assignments as supervisory signals for unlabeled data and works by alternating between clustering of the image descriptors and updating the network by predicting the cluster assignments. @cite defines pretext task that uses low-level motion-based grouping cues to learn visual representation. Also, @cite proposes to obtain supervision signal by enforcing the additivity of visual primitives in the patches of images and @cite proposed to learn feature representations by predicting the future in latent space by employing auto-regressive models.
- Some of the pretext tasks also work by enforcing constraints on the representation of the feature. A prominent example is the exemplar loss from @cite that promotes representation of image to be invariant to image augmentations. Additionally, some research effort have also been put in to define the pretext task as a combination of multiple pretext task @cite @cite . For instance, in @cite representation learning is augmented with pretext tasks of jigsaw puzzle @cite ,colorization @cite @cite and in-painting @cite .
- : ZSL has been widely studied in many tasks, such as image classification @cite @cite @cite @cite , hashing @cite , video recognition @cite . However, different from these ZSL tasks that are capable of exploiting the extra auxiliary supervision signals of classes (e.g. semantic word embedding of class name and explicit attribute information), . Therefore, how to capture the discriminative and all-sided information only from the input image is the mainstay of this task.
- : With only @math available, many works seek to exploring deep metric learning for ZSIR task. For example, sampling-Matters @cite proposes distance weighted sampling strategy. Proxy-NCA @cite explains why popular classification loss works from a proxy-agent view, and its implementation is very similar with Softmax. ALMN @cite proposes to optimize an adaptive large margin objective via the generated virtual points instead of mining hard-samples. However, all the above methods are to tackle with the unified metric by designing losses and exploring sample-mining strategies, thus suffer from the aforementioned issues easily. Additionally, HDC @cite employs the cascaded models and selects hard-samples from different levels and models. BIER loss @cite @cite adopts the online gradients boosting methods. Although these two methods try to improve the performances by resorting to the ensemble idea, they can only learn from the undiscriminating input and also suffer from the partial learning behavior, as a result, the discrimination and generalization of holistic features are still limited.
- : Attention serves as a tool to bias the allocation of available resources towards the most informative parts of an input. Many are implemented in combination with a gating mechanism (e.g. softmax or sigmoid) and are widely applied in many tasks, e.g. image captioning @cite @cite , lip reading @cite , image classification @cite @cite @cite . Schwartz et at. @cite adopt high order attention modules for VQA. Fu et at. @cite propose to learn an attention network so as to produce the attention proposals, while, it is optimized in a two-stage manner, i.e. iteratively train the embedding model and attention model. We emphasize that our DeML is a hybrid-attention system. First, it adopts a parameter-free , which is performed by random walk graph propagation and can be directly plugged into any bedrock CNNs without training. Second, different from @cite @cite where the channel-attention operations are cascaded, i.e. only the most informative signal will be captured, our DeML is equipped with a series of in parallelism, which are optimized by the adversary module and insist to learn various combinations of attributes simultaneously and to capture knowledge as much as possible.
- Linear combinations linked with convolutional layers have been proposed to improve CNNs in multiple aspects. Separable filters @cite and Sparselet models @cite explore the idea of approximating a set of convolutional filters as a linear combination of a smaller set of basis filters. One other direction suggests linearly combining feature maps, rather than the filters which generate them, to efficiently impose the feature redundancy @cite . In @cite @cite , authors try to generate correlated filters as a matrix multiplication with a set correlation matrices. Here, the authors use a set of static correlation matrices, followed by enabling their parametric learning. Each primary filter is one-to-one mapped in to a dependent filter based on these learnable correlation matrices. We follow a procedure parallel to this, but instead of learning a one-to-one mapping, we linearly combine a group of learnable filters, scaled by learnable linear coefficients to generate a group of correlated filters
- Nearly all traditional BGS algorithms first compute a background model, and then use it to predict the foreground. While a simple model based on the mean or median of a subset of preceding frames offers only a single background value per pixel, a probabilistic Gaussian Mixture Model (GMM) @cite allows a range of background values. This idea was improved by creating an online procedure for the update of GMM parameters in a pixel-wise manner @cite . Kernel Density Estimation (KDE) was introduced into BGS @cite as a non-parametric alternative to GMMs and was subsequently improved @cite . The probabilistic methods achieve better performance compared to single-value models for dynamic scenes and scenes with small background changes.
- In @cite , Barnich and Droogenbroeck introduced a sample-based background model. Instead of implementing a probability model, they modeled the background by a set of sample values per pixel and used a distance-based model to decide whether a pixel should be classified as background or foreground. Since color information alone is not sufficient for complex cases, such as illumination changes, Bilodeau al introduced Local Binary Similarity Patterns (LBSP) to compare the current frame and background using spatio-temporal features instead of color @cite . St-Charles al combined color and texture information, and introduced a word-based approach, PAWCS @cite . They considered pixels as background words and updated each word's reliability by its persistence. Similarly, SuBSENSE @cite combines LBSP and color features, and employs pixel-level feedback to improve the background model. Recently, Isik al introduced SWCD, a pixel-wise, sliding-window approach @cite . They used a dynamic control system to update the background model. Lee al introduced WisenetMD, a multi-step algorithm to eliminate false positives in dynamic backgrounds @cite .
- Over the last few years, many deep-learning-based algorithms were developed for the problem of semantic segmentation and they achieved state-of-the-art performance. In @cite , Braham and Droogenbroeck introduced a post-processing step for BGS algorithms based on semantic segmentation predictions. Given an input frame, they predicted a segmentation map using PSPNet @cite and obtained pixel-wise probability predictions for semantic labels such as person, car, animal, house etc. Then, they manually grouped these labels into two sets -- foreground and background labels, and used this information to improve any BGS algorithm's output in a post-processing step. They obtained very competitive results by using SubSENSE @cite as the BGS algorithm.
- As we have pointed out, many different algorithms have been designed to solve the BGS problem, and they all have some advantages and disadvantages. Bianco al introduced an algorithm called IUTIS which combines the results produced by several BGS algorithms @cite . They used genetic programming to to determine how to combine several BGS algorithms using a sequence of basic binary operations, such as logical and or, majority voting and median filtering. Their best result was achieved by using 5 top-performing BGS algorithms on the CDNet-2014 dataset at the time of publication. Zeng al followed the same idea, but instead of genetic programming used a fully-convolutional neural network to fuse several BGS results into a single output @cite , and outperformed IUTIS on CDNet-2014.
- The experience sampling method (ESM) @cite was born in 1970s with the advent of pagers. It is a validated technique used for capturing frequency, intensity, and overall patterns of one's emotional experience @cite . ESM alleviates people's inability to provide accurate retrospective information on their daily behavior and experience by capturing such information in the moment @cite .
- Despite the strengths of ESM, it also has limitations. This method puts heavy demand on research participants, which makes it a better fit for conscientious individuals rather than the general population @cite . There have been multiple efforts in improving the average user's engagement and compliance. Some researchers have tried to reduce the burden on the user by incorporating ESM more seamlessly into their daily pipelines, for example, by placing it in the phone unlock screen @cite . Other researchers have designed engaging games, making short questionnaires part of the game flow, and have validated ESM responses captured in the game in comparison to the traditional setting @cite .
- While these efforts have tried to address the issue of user engagement with ESM, the tone of delivery of ESM and its implications are not well studied. Specifically, ESM goes beyond being purely a method for capturing data. It provides a means for reflecting on one's past more objectively, which has the potential to improve psychological wellbeing and personal growth @cite . ESM is widely used in applications for improving mental health and wellbeing. Recently, personal assistants, chatbots, and virtual assistants (VAs) are are being used more often for conducting experience sampling. Therefore, it becomes imperative to study the characteristics of such an agent delivering ESM.
- In health and mental health contexts, patients are sometimes reluctant to respond honestly. Extensive research on VAs shows that people are more comfortable disclosing health symptoms to VAs when they know they are automated and there is no human behind the scenes @cite . Interestingly, when VAs exhibit human qualities, they could further improve the relationship with the user. VAs are better at creating rapport when contingent on the human's responses @cite . Also, they are more successful in establishing trust when using relational conversational strategies @cite . It remains an open question how emotional-awareness and empathy in an agent which conducts mood experience sampling is perceived by users, if it is mediated by the user's personality, and if it affects the user's mood.
- Humans are evolved to decode, understand and convey non-verbal information from facial motion, , a subtle unnatural eye blink, symmetry, and reciprocal response can be easily detected. Therefore, the realistic rendering of facial motion is key to enable telepresence technology @cite . This paper lies in the intersection between high fidelity face modeling and 3D face reconstruction from a monocular camera, which will be briefly reviewed here.
- The main benefit of the compact representation of 3D face modeling is that it allows estimating the face shape, appearance, and illumination parameters from a single view image. For instance, the latent representation of the 3DMMs can be recovered by jointly optimizing pixel intensity, edges and illumination (approximated by spherical harmonics) @cite . The recovered 3DMMs can be further refined to fit to a target face using a collection of photos @cite or depth based camera @cite . @cite leveraged expert designed rendering layers which model face shape, expression, and illumination and utilized inverse rendering to estimate a set of compact parameters which renders a face that best fits the input. This is often an simplification and cannot model all situations. In contrast, our method does not make any explicit assumptions on the lighting of the scene, and thus achieves more flexibility to different environments.
- Other methods include @cite @cite , which used cascaded CNNs which densely align the 3DMM with a 2D face in an iterative way based on facial landmarks. The geometry of a 3D face is regressed in a coarse-to-fine manner @cite , and asymmetric loss enforces the network to regress the identity consistent 3D face @cite . @cite utilizes jointly learned geometry and reflectance correctives to fit in-the-wild faces. @cite trained UV regression maps which jointly align with the 3DMM to directly reconstruct a 3D face.
- A key challenge is the oftentimes significant gap between the distribution of training and testing data. To this end, @cite @cite utilized synthetic data to boost 3D face reconstruction performance. A challenge here is to generate synthetic data that is representative of the testing distribution. @cite utilized domain invariant motion cues to perform unsupervised domain adaptation for facial landmark tracking. While their method was tested on landmarks and benefited from a limited source of supervision, our method performs per-pixel matching of textures, providing more supervision for domain adaptation.
- of the pre-trained DAMs ( @math ) @cite and the head pose via fully connected layers. I2ZNet is trained with the losses defined for @math and @math , namely @math and @math , as well as the view consistency loss in Eq. .
- The most successful state-of-the-art deep learning techniques for semantic sementation stem from a common forerunner, the Fully Convolutional Network (FCN) @cite . FCN replaces the later fully connected layers with convolutional ones to output fully resolution maps instead of classification scores, and is the first work to show how CNNs can be trained end-to-end for this problem. Based on FCN, several types of improvements have been made recently.
- The encoder-decoder architectures for semantic segmentation @cite @cite first encode longer range information with the spatial dimension gradually reduced and then decode to recover object details and spatial resolution. For example, SegNet @cite learns extra convolutional layers to densify the feature response. To encode long-range context while preserving object details, four typical methods including image pyramid @cite @cite , CRF @cite , spatial pyramid pooling @cite and atrous convolution @cite @cite are developed. Chen @cite @cite directly resize the input for several scales and fuse the features from all scales. To overcome the poor localization property of CNNs, the DenseCRF @cite is combined with the responses at the final CNN layer @cite . The atrous spatial pyramid pooling is used in DeepLabv2 @cite to capture multi-scale information via equipping parallel atrous convolutional layers. Chen @cite design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates, while Wang @cite adopt the atrous convolution with hybrid atrous rates within the last two blocks of ResNet for capturing long-range information.
- Besides, there are some works to use edge cues for image semantic segmentation @cite @cite @cite . Chen @cite propose to replace the fully-connected CRF with domain transform which is a modern edge-preserving filtering method in which the amount of smoothing is controlled by a reference edge map. Kokkinos @cite designs CRF-based method, which intergrates CNN score-maps with pairwise features derived from advanced boundary detection, to more precisely localize semantic segmentation boundaries. The outputs of edge network are employed to refine semantic segmentation network by adding an edge aware regularization so as to yield spatially consistent and well boundary located results @cite .
- Conditional feature normalization uses a learned function of some conditions to achieve feature-wise affine transformation in batch normalization @cite , and its variants are proven to be effective in several computer vision tasks, such as style transfer @cite @cite , visual question answering @cite , visual reasoning @cite and image super-resolution @cite . Perez @cite propose a feature-wise linear modulation layer (FiLM) to exploit linguistic information for visual reasoning. For the low-level tasks like image super-resolution and semantic segmentation, the spatial information is crucial and thus should be considered in feature modulation. Wang @cite propose spatial feature transform to incorporate semantic prior for image super-resolution. However, noisy information introduced by the priors might affect the performance. Therefore, we design a gate scheme in the spatial feature transform to adaptively incorporate edge priors in our framework.
- In 2D camera perspectives, Hu al @cite extracted scale invariant features and removed the shaky motions by Gaussian kernel filtering. The affine model was also used in @cite to describe interframe transformation, and recursive Kalman filtering was applied to stabilize the trajectory. Particle filtering improved the estimation in @cite and showed increasing robustness. Grundmann al @cite presented the algorithm applying L1-optimization with several constraints, and the synthesized smooth path is composed of either constant, linear or parabolic motions. Subspace constraints were implemented in @cite and applicable to long videos, and generated visually plausible videos. Specifically for the estimation of camera motion, many approaches have been developed. Matsushita al @cite divided the trajectory into global and local motions, and employed motion inpainting to improve video quality. Another method for estimating interframe camera motions is dynamic time warping technique developed by Bosco al @cite intended for moving-object videos. Liu al @cite proposed SteadyFlow model dealing with spatially-variant motions. Camera motions were mitigated under the turbulent conditions by particle advection framework @cite . Instead of focusing on the camera motion path, Lee al @cite optimized the feature trajectories of the unstable videos, and produced less undefined space in the stabilized videos.
- As for 3D perspectives which include the focal length variation, Liu al @cite reconstructed dynamic scenes in 3D camera motion for perceptually plausible results. Goldstein and Fattal @cite adopted epipolar point transfer for moving camera and object videos matching the capabilities of 3D methods without scene reconstruction. Zhang al @cite solved the problem by tackling a sparse linear system of equations. Additionally, Liu al @cite handled camera motions with a bundle of camera paths, and succeeded in dealing with parallax and rolling shutter effects.
- Besides the previous offline approaches, several online methods are also proposed, which are favored by real-time applications. Ratakonda @cite proposed a primitive online stabilization algorithm of integrating cumulative motion curve in two dimensions separately. Optical flows were calculated between successive frames in @cite , and the camera path was optimized with regularization. Liu al @cite presented MeshFlow using only the past motions. Karpenko al @cite demonstrated real-time stabilization with gyroscopes for automatic calibration on a mobile phone. Integrating ConvNet, Wang al @cite recently proposed StabNet to perform online stabilization by loading historical frames into the network and directly producing the homography matrix entries. However, this method may lead to serious error propagation in that the former lapses cause latter abnormality in terms of the visual performance.
- Our proposed system employs ConvNet to extract the interframe transformation, which does not explicitly estimate the camera motion path or feature trajectory but leave them for the network to learn implicitly. In order to achieve multiple levels of refinement on the stabilization process, we referred to the pyramidal implementation in @cite , which improved the performance of optical flow detection, and constructed three levels of different resolutions to do iterative adjustment in stabilization. To our knowledge, our method StableNet is the first which uses deep ConvNet in a multiscale setting for video stablization and produces competitive results compared with state-of-the-art methods.
- In recent years, research and development efforts around autonomous cars have shifted towards producing certifiable systems capable of being deployed into multiple environments @cite . Naturally, sensors play a primordial role within this system by enabling the vehicle to interact with and to account for the ever-changing operational scenarios. Unfortunately, the plethora of available sensor types @cite , in addition to their possible multiplicity and physical location within the vehicle's system, lead to a combinatorial number of possible sensor suite designs from which to choose. This poses both a design and a testing problem, since it becomes monetarily infeasible to iterate through all possible physical prototypes and computationally intractable to account for every combination in driving simulations @cite .
- Amongst the most important metrics to consider when choosing an appropriate set of sensors with which to outfit an autonomous vehicle are performance @cite , reliability @cite , and cost @cite . The quality of the SLAM solution has a direct impact on the behavior of the car, since its output is utilized by the planning and control module for essential guidance tasks @cite . In addition to preserving the warranted performance requirements, the sensor suite of choice must be resilient; it needs to guarantee safe operations in new and unknown environments @cite , as well as robustness against hardware failures and loss of measurements (e.g., unavailable GPS information in certain urban settings) @cite . Moreover, for autonomous cars to become a viable solution, they need to satisfactorily reach the aforementioned system-level performance while maintaining competitive costs in order to successfully rival traditional alternative transportation systems (e.g., private autos, public transportation) @cite @cite .
- Recent advances in smoothing techniques and optimization of submodular functions have encouraged the development of design methods to improve SLAM solutions, resulting in task-specific approaches geared towards resource-constrained systems. It has been shown that for a given sensor budget, optimization over an LQG-control performance metric yields a method to consciously choose which sensors to toggle during real-time operations to reduce uncertainty @cite . On the other hand, alternative routes have opted to work directly with sensor information; by optimizing for accuracy when selecting features for visual odometry @cite or by exploiting notions such as anticipation to prune irrelevant visual cues in visual-inertial navigation @cite , performance can be targeted.
- However, to the knowledge of the authors, there exists a gap within the application space of such methods to help guide and inform the sensor choice and placement on autonomous vehicles, and to aid in the quantification of the tradespace between performance and non-uniform sensor costs. Initial efforts that make use of similar techniques have been presented @cite , but they still lack a systematic strategy to tackle the sensor combination enumeration. Furthermore, none of these advances consider the resiliency of sensor networks.
- Conditional GANs have witnessed outstanding progress in recent years. Significant effort has been devoted to improving training strategies and enhancing architecture designs. Training stability has been improved through the introduction of techniques such as spectral normalization @cite and the two time-scale update rule @cite . Architecturally, conditional generation has been improved through the use of auxiliary classifiers @cite and the introduction of a projection-based conditioning for the discriminator @cite . Image quality has also benefited from the incorporation of self-attention @cite , as well as increases in model capacity and batch size @cite .
- One major limitation of commonly used conditional consistency and diversity metrics is that they require having access to pre-trained models for each given task (e.g. a classification model to test class consistency). As an alternative, Intra-FID @cite has been proposed in the context of class-conditioned image generation. Intra-FID calculates an FID score separately for each conditioning and reports the average score over all conditionings. This method captures intra-class variability without requiring a dedicated pre-trained model for the dataset. However, it scales poorly with the number of unique conditions, as the computationally intensive FID calculation must be repeated for each case, and because FID behaves poorly when the sample size is small @cite . Furthermore, in cases where the conditioning cannot be broken down into a set of discrete classes (e.g. pixel-based conditioning), Intra-FID is intractable. As a result, it has not been applied beyond class-conditioning.
- Beyond IS and FID, a number of GAN evaluation metrics have emerged in the literature. Most of these metrics either focus on the separability between generated images and real images @cite @cite @cite @cite , compute the distance between distributions @cite @cite @cite , assess sample quality and diversity from conditional or marginal distributions @cite @cite @cite , measure the similarity between generated and real images @cite @cite @cite @cite or are log-likelihood based @cite . Despite this progress, there is still no clear consensus on which metrics are most appropriate to use, and the vast majority of available metrics are only concerned with unconditional generation. We refer the reader to @cite for a detailed overview and insightful discussion of existing metrics.
- Most existing visual odometry algorithms @cite @cite @cite @cite @cite assume stationary environments. However, in real applications, there are often a number of non-stationary objects such as vehicles. To deal with such environments, some research has attempted to improve robustness against dynamic situations. Such efforts can be categorized into two types: dichotomous and model-based.
- The first type of research has tried to exclude a region which has a different motion from the major movement in a similar way to RANSAC as mentioned above. A. Dib @cite utilizes RANSAC for direct visual odometry in dynamic environments. They minimize photometric shift over six random patches in an image, unlike the na "ive direct visual odometry whose optimization process covers the whole image. In @cite , they first categorize patches by depth value and subtract a category which has a quite different motion from a majority movement of the camera by comparing standard deviation of motion vectors, including or excluding that category. They still assume that the stationary background occupies a large area on the image so that the camera movement is estimated via the background.
- The second type of research has exploited statistical models to classify motions belonging to independent objects. BaMVO @cite estimates background by choosing pixels whose depth does not change unusually. It is effective in situations where a dynamic object such as a pedestrian moves in parallel with the principal axis of a camera. However, the performance can degenerate when the dynamic object moves perpendicular to the principal axis while reducing the depth transition. Joint-VO-SF @cite formulates a minimization problem with 3D points from RGB-D images. They estimate camera pose and scene flow accurately, but the average runtime is 80 ms on an i7 multi-core CPU at QVGA resolution, which is slightly slow for real-time implementation with current on-board computers. StaticFusion @cite is the enhanced version of Joint-VO-SF. It adds dense 3D modeling of only the static parts of the environment and reduces the overall drift though frame-to-model alignment.
- Among the motion segmentation methods, H. Jung @cite propose randomized voting to extract independent motions using epi-polar constraints with the average computational time of 300ms per frame. However, the number of moving objects should be known. In @cite , MCD5.8ms has an advantage in that it detects a moving object using a dual-mode model with a low computational load while showing the execution time per frame of 5.8 ms. Since they calculate the homography using na "ive visual odometry, however, the performance can deteriorate when a moving object occupies more than half of an image.
- Typical wireless protocol 802.11b g only provides limited channels for users, which is far more than enough for high-quality communication services @cite . To reduce the load in central system, making use of distributed available resources in networks turns out to be an ideal solution. Underlay Device-to-Device (D2D) communication is considered as one of the crucial technologies for cellular spectrum reuse for user devices in communication networks @cite . The advantage of D2D communication that allows end users to operate on licensed channels through power control sheds light on how interference management would work in UAV ad-hoc networks @cite .
- Game theory provides an efficient tool for the cooperation through resource allocation and sharing @cite @cite . A computation offloading game has been designed in order to balance the UAV's tradeoff between execution time and energy consumption @cite . A sub-modular game is adopted in the scheduling of beaconing periods for the purpose of less energy consumption @cite . applied the Bayesian game-theoretic methodology in UAV's intrusion detection and attacker ejection @cite . However, most existing models focus on common scenarios with less number of UAVs, which are not compatible with large-scale scenarios with large numbers of UAVs @cite . Aggregative game is a characteristic game model which treats other agents' strategies as a whole influence, thus avoids overwhelming strategies information from every single agent @cite @cite . Inspired by this, our model is built upon the aggregative game theory which suits for large-scale scenarios.
- Compared with other algorithms, novel algorithm SPBLLA has more advantages in learning rate. Various algorithms have been employed in the UAV networks in search of the optimal channel selection @cite @cite , such as stochastic learning algorithm @cite . The most widely seen algorithm--LLA is an ideal method for NE approaching @cite @cite . The BLLA has been employed by @cite , which is modified from LLA to update strategies in each iteration to converge to the NE. However, only a single agent is allowed to alter strategies in one iteration. In large-scale scenarios, more iterations are required, which makes BLLA inefficient. It is obvious that more UAVs altering strategies in one iteration would be more efficient. To achieve it, the works in @cite and @cite have provided a novel synchronous algorithm. However, there exist superabundant restrictions that make the algorithm impractical in most scenarios. Compared with the formers, SPBLLA has fewer constraints and can achieve synchronous operation, which can significantly improve the computational efficiency.
- Approaches to mitigating catastrophic forgetting in neural networks include ones that selectively regularise the parameters to preserve knowledge from previously trained tasks @cite @cite @cite , ones that allocate more neural resources over time @cite @cite , and ones that explicitly store (or train generative models to mimic) past data which are used in various ways to improve the memory of the network @cite @cite @cite .
- The regularisation methods typically use task boundaries as consolidation posts. While in @cite , for example, a method is employed for these boundaries if they exist @cite , it is still not practical for situations where the data distribution is changing continuously over time. In @cite , task boundaries are not required but, as discussed already, the consolidation does not take into account the output or loss function of the network.
- The episodic memory approaches for continual learning often do not rely on task boundaries. However, for a fixed memory size, they face the challenge of choosing what data to store over time. Recently it has been shown that simply maintaining a uniform distribution of the data over the lifetime of the model yields decent results @cite @cite . Other methods use episodic memory to adapt the model at test time @cite or to only allow updates that do not increase the loss on stored examples @cite . The models that use generative models to mimic older parts of the data distribution suffer from the fact that the generative model can also experience catastrophic forgetting @cite .
- Distillation of knowledge from one network to another @cite , a technique we employed for the PC model, has also featured in other continual learning approaches. In @cite , knowledge is distilled unidirectionally from a flexible network to a more stable one, and vice versa in @cite . The PC model differs from these two in that the distillation is bidirectional and that networks at multiple timescales are used, rather than just two. Bidirectional distillation is also employed in @cite , but for transfer learning in a multitask context.
- SincNet @cite is one of these innovative deep learning architecture for speaker recognition which uses parametrized sinc functions as a foundation to its first convolutional layer. Sinc functions are designed to process digital signals just like audio, and thus the use of them as the first convolutional layer helps to capture more meaningful features to the network. Additionally, the extracted features are also more human-readable than the ones obtained from ordinary convolutions.
- There has been significant progress in navigation of rough terrain for multilegged robots. Terrain characterisation for gait adaptation has been shown to improve locomotion efficiency when walking on rough terrain @cite . Elevation maps have been used with characterisation to plan footholds for optimal stability and obstacle avoidance @cite , as well as walking over gaps and climbing stairs @cite . Optimising robot dynamics can enable even more dynamic motions @cite ; however, none of these methods consider collisions with the body of the robot, which is necessary in confined spaces.
- For complex 3D environments such as large steps, trusses or vehicle egress, full body contact planners using random sampling such as probabilistic roadmaps (PRM) are used @cite @cite @cite . This can be done by randomly sampling a subspace of all possible contacts limited by stability and reachability as in @cite . However, this method requires accurate knowledge of the environment in advance. Short and Bandyopadhyay @cite deal with this by first pre-computing a set of possible configurations based on the robot model assuming no obstacles then selecting the best configurations for a given, dynamic environment. The computational cost of these planners are highly dependant on the complexity of the terrain and the number of joints of the robot. This makes them difficult to apply to high DOF, multilegged robots.
- As indicated above, deep iris recognition is still a developing research area and not many solutions have been presented in the literature so far. In this section, we cover only existing work related to deep iris recognition and refer the reader to @cite @cite @cite @cite for more information on deep models for other iris-related problems, such as segmentation.
- Person re-identification (re-ID) @cite @cite @cite @cite has been studied extensively in the past ten years. Currently, deep models, which adopt the identity loss @cite @cite @cite , verification loss @cite @cite @cite @cite , triplet loss @cite @cite @cite @cite or other metric functions @cite @cite @cite , are successfully employed in person re-ID and achieve promising results due to the remarkable representation ability of CNNs.
- Recently, researchers @cite @cite @cite @cite @cite attempted to exploit person structure information to learn local discriminative features. To this they trained human pose estimators on pose annotated datasets and used the pose estimators to help the feature learning of person re-ID. For example, a spindle network structure @cite is proposed to use human landmark annotations for body joint localization and body region generation, which further guided multi-stage feature decomposition and tree-structured competitive feature fusion. A pose-driven CNN model @cite leveraged human pose information and transformed a global body image into an image containing normalized part regions for feature learning. In addition, Sarfraz et. al. @cite incorporated both the fine and coarse pose information of the person to learn a discriminative embedding by explicitly including this information into the learning process of a re-ID model. However, these methods depend on huge amounts of expensive human pose annotations.
- Instead of using pose annotated datasets to train pose estimators, lots of approaches directly split person structure into stripes or grids @cite @cite @cite for local feature representations that can be concatenated into one feature vector for further global metric learning. In @cite , person images are cropped into three overlapped stripes which are used to train three independent networks. At the score level, three networks are fused for metric learning. Similarly, @cite split a person image into three stripes and jointly learned both the global full-body and local body-parts features to improve the performance of person re-ID. These models assumed the person bodies are well aligned, which may fail in real-world scenarios.
- Differently, some studies @cite @cite @cite attempted to conduct local metric learning to align the person structure. For example, Shen et. al. @cite introduced a boosting-based approach to learn a correspondence structure which indicates the patch-wise matching probabilities between images from a target camera pair. In @cite , a patch-based deformable model is proposed to combine appearance term with a deformation cost that controls relative placement of patches. Li et. al. @cite proposed a filter pairing neural network to handle misalignment and geometric transforms by integrating a patch matching layer into the neural network. Our RLD method differs a lot from these patch-based matching methods in that RLD focuses on self-structure exploration while patch-based matching methods focus on the matching between image pairs. Patch-based matching methods are not off-line algorithms and cannot extract gallery features in advance. When the gallery image set is very large, it costs too much time to extract deep features for person retrieval.
- Several models have been performed in the past to retrieve sentences of a document belonging to a particular topic @cite . Given a topic, retrieving sentences that may belong to that topic should be considered as a different task than what we aim in this paper. A graph based approach for extracting information relevant to a query is presented in @cite , where subgraphs are built using the relatedness of the sentences to the query. An incremental integrated graph to represent the sentences in a collection of documents is presented in @cite @cite . Sentences from the documents are merged into a master sequence to improve coherence and flow. The same ordering is used for sequencing the sentences in the extracted summary. Ordering of sentences in a document is discussed in @cite . In this paper, we aim to generate the sections clusters from an unordered document. To the best of our knowledge, this is a first attempt to address this problem formally.
- While contrast-invariance is indeed a highly desirable feature in many edge and ridge detection tasks, localizing a property of the frequency components in the time representation of a signal comes at a price. Indeed, it was already observed in @cite and @cite that replacing isotropic wavelets with anisotropically scaled analyzing functions in deteriorates the detection of features in images rather than improving it. This should come as a surprise, as one would assume that anisotropically scaled elements would be better suited for detecting typically anisotropic features such as edges or ridges. Finding a way to bring together the intuitions behind the phase congruency measure @math and modern constructions of anisotropic analyzing functions was in fact one of the main factors motivating the present work.
- In @cite A team of researchers in Google Brain proposed a new domain adaptation technique using generative adversarial networks. The proposed technique learns a transformation in the pixel space from one domain to another. The input domain is called the source domain and the destination domain is called the target domain. In some sense, this technique is similar to the style transfer @cite technique. However, following the style transfer approach, a style of a single image is learned which is then used to transfer the style of all the input images to the learned style whereas the pixel domain adaptation technique learns the style of an entire target domain and adapts the source domain images to appear as if they were drawn from the target domain. In @cite , the assumption is that the differences between the source and the target domains are primarily in low-level features which could include variation in noise level, texture, resolution, illumination, and color, as opposed to the high-level features such as types of objects, geometric variations, etc.
- 3D Pose Estimation from Wearable Devices: Inertial Measurement Units (IMUs) worn by the subject provide a camera-free alternative solution to first person human pose estimation. However, such systems are intrusive and complex to calibrate. While reducing the number of sensors leads to a less invasive configuration @cite recovering accurate human pose from sparse sensor readings becomes a more challenging task. An alternative approach, introduced by Shiratori al @cite consists of a multi-camera structure-from-motion (SFM) approach using 16 limb-mounted cameras. Still very intrusive, this approach suffers from motion blur, automatic white balancing, rolling shutter effects and motion in the scene, making it impractical in realistic scenarios.
- Machine learning technology has recently played an important role in improving spectrum sensing. The work in @cite presented an adversarial machine learning approach to launch jamming attacks on CR and introduces a defense strategy. Several supervised and unsupervised machine learning algorithms for cooperative spectrum sensing (CSS) were investigated in @cite . In @cite , the combination of infinite GMM and CSS was proposed to detect the primary user emulation attacks. In @cite , a convolutional neural network-based CSS scheme was developed to detect multiple bands simultaneously. A mobile CSS framework was proposed in @cite for large-scale heterogeneous cognitive networks.
- It is worth noting that the access policy design (sensing interval) in our paper is also formulated as a POMDP, but the nature of our formulation is fundamentally different from that in @cite @cite @cite @cite @cite @cite @cite . The partial observation in our work comes from imperfect multi-level sensing results and access feedback. To tackle this challenging POMDP, we reduce the infinite time horizon to a finite one, leading to a computationally tractable solution. Most importantly, we mathematically prove that such practice does not sacrifice the optimality in the utility.
- For multi-agent reinforcement learning (MARL), the work of @cite provides particle environments for both cooperative and competitive settings, as well as the training method and algorithm performances for these environments. Also, in the work of @cite , the authors show that in a simple competitive multi-agent environment, the trained agents can have very complex behaviors. The environment for their experiments is also open-sourced. For related work on StarCraft, a suite of multi-agent environments has been proposed and open-sourced in the work of @cite , together with performance baselines of several training methods.
- Besides the one in @cite , another game characterization of probabilistic bisimulation has been given in @cite . It is described later in (Table ). The latter game has a bigger arena than the one in @cite : in @cite both players have to play a subset @math , while in @cite only Spoiler does so.
- The work that is the closest to ours is the recent work @cite that studies bisimilarity games in a categorical setting. Their formalization uses (co)algebras (following the (co)algebraic generalization of the Kantorovich metric introduced in @cite ), and therefore embraces a variety of different branching types. The major differences between the two works are as follows. Our current work is fibration-based (in particular @math -fibrations), while @cite is not. As a consequence, ours accommodates an additional dimension of generality by changing fibrations, which correspond to different indistinguishability notions (relation, metric, topology, preorder, measurable structures, etc.). In contrast, the works @cite and @cite deal exclusively with two settings: binary relations and pseudometrics. A relationship to is beautifully established in @cite , while it is not done in this work. We expect our fibrational framework can accommodate modal logic too: fibrations have been used for categorical modeling of logics @cite . We leave this aspect to future work.
- The categorical generalization @cite is based on the game notion in @cite , while ours is based on that in @cite . Therefore, for some bisimulation notions (including the bisimulation metric), we obtain a game notion with a smaller arena. Compare Table (an instance of ours) and Table (an instance of @cite ).
- There are a number of categorical studies of bisimilarity notions; notable mentions include open map-based approaches @cite and coalgebraic ones @cite @cite . The fibrational approach we adopt also uses coalgebras; it was initiated in @cite and pursued, e.g., in @cite @cite @cite , and @cite . For example, in the recent work @cite , fibrational generality is exploited to study up-to techniques for bisimilarity metric. They use the of functors introduced in @cite instead of the codensity lifting that we use (it generalizes the in @cite , see Example ). It is known @cite that the Wasserstein and Kantorovich liftings can differ in general, while they coincide for some specific functors such as the distribution functor.
- Our work originated out of the search for algorithms that find semantically meaningful latent factors of data. The use of VAEs and their extensions to this end has mostly taken place in the context of @cite @cite @cite . Examples of extensions that aim at disentangling latent factors are the @math -VAE @cite , the factor-VAE @cite , the @math -TCVAE @cite and the DIP-VAE @cite .
- If a manifold has the additional structure of a Lie group, this structure allows for a more straightforward implementation of the reparametrization trick @cite . In our work, we do not assume the additional structure of a Lie group, but develop a reparametrization trick that works for general submanifolds of Euclidean space, and therefore by the Whitney (respectively Nash) embedding theorem, for general closed (Riemannian) manifolds.
- The method that we use has similarities with the approach of Hamiltonian Variational Inference @cite . Moreover, the implementation of a manifold as a latent space can be seen as enabling a particular, informative, prior distribution. In that sense, our work relates to @cite @cite . The prior distribution we implement is very degenerate, in that it is does not assign weight to points outside of the manifold.
- There are also other ways to implement approximate Bayesian inference on Riemannian manifolds. For instance, Liu and Zhu adapted the Stein variational gradient method to enable training on a Riemannian manifold @cite . However, their proposed method is rather expensive computationally.
- The family of approximate posteriors that we implement is a direct generalization of the standard choice for a Euclidean VAE. Indeed, the Gaussian distributions are solutions to the heat equations, i.e. they are transition kernels of Brownian motion. One may want to increase the flexibility of the family of approximate posterior distributions, for instance by applying normalizing flows @cite @cite @cite .
- For a long time, the task of 3D reconstruction from single-view images had largely been tackled with the help of 3D CNNs. A number of works have revolved around generating voxelized output representations @cite @cite @cite @cite @cite @cite @cite . Giridhar al @cite learnt a joint embedding of 3D voxel shapes and their corresponding 2D images. Choy al @cite trained a recurrent neural network to encode information from more than one input views. But voxel formats are computationally heavy and information-sparse, which lead to research on the octree data structure for representing 3D data @cite @cite @cite @cite @cite .
- Recently, Fan al @cite , introduced techniques for generating unordered point clouds to obtain single-view 3D reconstruction results outperforming volumetric approaches @cite . While @cite directly predict the 3D point cloud from 2D images, our approach stresses the importance of first predicting a low-resolution point cloud and super-resolving it to obtain a dense prediction. Groueix @cite represented a 3D shape as a collection of parametric surface elements and constructed a mesh from the predicted point cloud. @cite proposed a latent matching setup in a probabilistic framework to obtain diverse reconstructions. Other works utilize 2D supervision in the form of silhouettes and depth maps for 3D reconstruction @cite @cite @cite @cite @cite @cite @cite . Concurrently with us, Yuan @cite propose to deform grids for completing partial depth maps. Apart from reconstruction, other 3D perception tasks have also been performed using point clouds @cite @cite @cite .
- The concept of Laplacian pyramid networks has been previously used in 2D vision tasks for hierarchical prediction. Denton al @cite proposed a generative adversarial network to generate realistic images based on a Laplacian pyramid framework (LAPGAN). Lai al @cite extended the above by introducing a robust loss formulation and making architectural modifications for improving speed and accuracy. In the 3D vision domain, Hane al @cite proposed an octree-based method for hierarchical surface prediction. While the focus of @cite is on extending a volumetric representation into an octree-based one to enable surface prediction, we directly operate on points sampled on the object surface.
- aim to provide local explanations for specific decisions, rather than attempting to explain the whole system behavior. One of the most representative examples for classification in recent years is LIME @cite . The approach is simple: generate an explanation by approximating the underlying model by an interpretable one (e.g., a linear model with a only a few non-zero coefficients), learned on perturbations of the original instance. Typical perturbations can be removing words or hiding parts of an image. A similar model-agnostic approach is BETA @cite , which optimizes for fidelity to the black-box model and interpretability of the explanation. @cite focuses on pixel-wise decomposition of nonlinear classifiers, which allows to visualize contributions of single pixels to predictions for kernel-based classifiers. @cite extracts explanations from latent factor recommendation systems by training association rules on the output of a matrix factorization black-box model. All approaches have been applied on text and images, but are not built to take into consideration temporal progressions in time or event series.
- are interpretable by design @cite . Typical examples include decision trees, decision sets @cite @cite , fuzzy inference models @cite or additive models @cite . However, none of these fit temporal data well.
- The vast majority of explainable models for time series target their classification. @cite propose grammar-based decision trees to classify heterogeneous time series. @cite @cite extract interpretable features from series, expressed as local shapelets, while @cite learn such shapelets via stochastic gradient learning and use them for early classification. @cite , the authors propose reversible and irreversible explainable tweaking, where given a time series and an opaque classifier, the objective is to find the minimum number of changes to the time series such that the classifier changes its decision.
- Closest to our problem is the method proposed in @cite . There, the objective is to predict a future neural event based on a sequence of previously occurred events. Current approaches are mostly concerned with time-independent sequences, in which the actual time span between events is irrelevant and the difference between events is the difference between their order positions in the sequence. The authors extract and use the information provided by the time span between events in an RNN-based model to achieve some accuracy gain over baseline models. We also opt for an RNN architecture, but we additionally incorporate attention mechanisms @cite into the network to quantify how much an anomalous event contributed to a predicted critical incident.
- Particle-based methods are widely used in fluid simulation with borderline adaptability and good computational performance. @cite @cite @cite @cite @cite However, the research on the direction of blood flow simulation is scarce. M " simplified the blood flow to Newtonian fluid for coupling simulation. @cite implemented the method of M " using PPU parallel acceleration. @cite coupled SPH and particle spring models to simulate blood flow. @cite The above method lacks visual realism due to neglecting the non-Newtonian fluid characteristics of blood flow, and at the same time the simulation is small in scale and the real-time performance is not sufficient. implemented GPU-assisted blood flow simulation using the Compute Unified Device Architecture (CUDA), achieving 50 fps simulation at 9,000 particles, but did not consider the interaction of blood flow with blood vessels. @cite proposed a GPU-accelerated mixed particle-based coupling method for blood flow and vessel wall that real-time simulated at 100,000 particle size. @cite Although these studies have made some improvements in computational efficiency, a more efficient method is urgently needed due to the multi-phase coupling involved in the virtual surgery scene with flexible organs and surgical instruments.
- In recent years, data-driven methods are increasingly proposed in the field of fluid simulation. generated a large number of fluid simulations by interpolating the existing fluid simulation results. @cite used neural network prediction instead of the diffusion projection in the gridding method to achieve a speed increase of about 10 times. @cite used regression forests to simulate fluid particle state changes, which can achieve 10-1000 times faster operation on the GPU. @cite At present, there are few studies that combine data-driven methods with blood flow simulation and even virtual surgery.
- One class of alternatives to standard back-propagation aims to avoid its biologically implausible aspects, most notably the weight transport problem @cite @cite @cite @cite . Some of these methods @cite @cite can also achieve backward unlocking as they permit all parameters to be updated at the same time, but only once the signal has propagated to the top layer. None of them however solve the update locking problem or forward locking problem which we consider. Target propagation uses a local auxiliary network as in our approach, which is used to propagate backward the optimal activations computed from the layer above. Feedback alignment replaces the symmetric weights of the backward pass with random weights. Direct feedback alignment extends the idea of feedback alignment passing errors from the top to all layers, potentially permitting a simultaneous update. These approaches have also not been shown to be scalable to large datasets @cite , obtaining only $17.5
- Distributed optimization based on data parallelism is a popular area of research in machine learning beyond deep learning models and often studied in the convex setting @cite . In deep network optimization the predominant method is distributed synchronous SGD @cite and variants, as well as asynchronous @cite variants. Our approach on the other hand can be seen as closer to exploiting a type of model parallelism vs data parallelism and can be easily combined with many of these methods, particularly distributed synchronous SGD.
- The success of learning deep features in related problems like optical flow @cite , stereo fusion has motivated similar application for semantic matching. Choy al @cite propose a universal correspondence network for learning fine-grained high resolution feature representation using metric learning. The representations are then used to establish correspondences after geometric verification. Similarly, @cite uses correspondences between region proposals that pass a geometric verification check to fine tune the representations of the network. Kim al @cite introduce a CNN descriptor termed fully convolutional self-similarity which are then combined with the proposal flow based geometric consistency check. The proposed CNN based approaches are at the same level or better than hand-engineered features, but, include costly pairwise matching between candidate regions. On the other hand, @cite , @cite , @cite learn the correspondence and feature representation in an end-to-end framework.
- It is common knowledge that neural networks are data hungry models. Transfer learning alleviates the problem to certain extent, but the main challenge lies in the effective use of large amount of unlabeled data. Zhou al @cite propose a training procedure for their network that can learn to predict relative camera motion and depth without supervision using large amount of videos. Similarly, @cite propose to learn homography transformation between an image pair without using ground-truth transformation information. @cite introduced a semi-supervised paradigm based on GAN that learns optical flow from both labeled synthetic datasets and unlabeled real videos. The key recipe in all these algorithms is the idea of photometric consistency. However, in the field of semantic matching, due to large appearance variations, this color constancy constraint does not hold.
- Cycle consistency has been used to learn correspondence in a variety of settings @cite , @cite where images are defined as nodes and the pairwise flow fields define the edges. The main idea is to minimize the net distance between the key-points in source image and its estimated position obtained by traversing the cycle using respective flow fields. Zhou al @cite extends the idea to the framework of CNNs by leveraging 3D models to create cyclic graphs between the rendered synthetic views and pairs of images. The network is made to predict transformations for image-image and image-synthetic pairs. Using the 4-cycle constraint, the synthetic-synthetic transformation is estimated and compared with the ground-truth to generate gradients. However, the method necessitates the availability of 3D models and sampling appropriate synthetic views.
- Several computer vision tasks require matching keypoints across several frames or views. The keypoints need to be detected first. Harris and Stephen in @cite proposed the Harris corner detector. This detector considers differential of the corner score into account with reference to direction directly, and has been proved to be accurate in distinguishing between edges and corners.
- After keypoint detection, the descriptor is constructed. Float-point based descriptor and binary descriptor are two general types of descriptor. Lowe SIFT @cite is a state-of-the-art descriptor which belongs to float-point based category and generates robust features which are scale and rotation invariant. A grid is taken around the keypoint and histogram is generated. Finally, a 128-dimensional vector of gradients is taken into consideration. This simple descriptor provides distinctive features, but requires high computational time due to histogram generation. Another descriptor faster than SIFT @cite is The Speeded up Robust Feature (SURF) by Bay @cite . SURF uses BLOB detector based on Hessian matrix to detect keypoints. Gaussian weights are applied in all directions for orientation similar as SIFT.
- Float-point based descriptor with more accuracy requires heavy computation time. Due to increasing demand of mobile applications, a demand of low computation time is noted. Binary descriptors use haming distance and XOR operation for matching keypoints and therefore requires less computation time as compared to float-point descriptors. A sampling pattern is formed around the keypoint and a binary string is computed by comparing intensity values of sample points. BRIEF @cite is a binary descriptor in which there is no specific sampling pattern or any method for orientation calculation. The sampling pairs for forming binary strings are randomly selected. Therefore, BRIEF is less accurate for minute changes in the size or alignment of an image.
- FREAK @cite is a binary descriptor whose sampling pattern is inspired by the distribution of ganglion cells in human retina. The density of points increases moving towards the centre which mimics the human retina. Predefined pairs determine its orientation while sampling points uses coarse-to-fine approach which selects pairs from outer rings followed by pairs from inner rings. In many aspects, FREAK performs better than other binary descriptors.
- In @cite @cite have adapted quick sort for GPUs, in Bandyopadhyays adaptation @cite the researchers first partition the sequence to be sorted into sub-sequences, then sorts these sub-sequences and merges the sorted sub-sequences in parallel.
- investigate optimal block-kernel mappings of a bitonic network to the GPU stream kernel architecture @cite , their pure bitonic sort was able to beat @cite @cite the quicksort of for any number of record comparisons. The fastest GPU merge sort algorithm known at this time is presented by However s CUDA-quicksort @cite which adapts Cedermans work to more optimally access GPU memory outperforms s earlier work. s 2009 GPU-radix adaptation of radix sort to GPUs uses the radix 2 (i.e., each phase sorts on a bit of the key using the 1-bit scan primitive) and uses the parallel bitsplit technique.
- The results of and @cite indicated that the radix sort algorithm of @cite outperforms both warp sort @cite and sample sort @cite , so the radix sort of GPU-radix2 is the fastest GPU sort algorithm for 32-bit integer keys.
- It is known to all that AES models are easily to be deceived by adversarial inputs if there isn't any strategy designed to help the models detect some well-designed negative samples. In @cite , the researchers asked some experts who were familiar with e-Rater to write deceptive essays to trick e-Rater @cite , which validated the fragility of existing AES systems. As was expected, essays which were composed of some repeated well-written paragraphs achieved higher-than-deserved grades. To improve the robustness of AES systems, @cite utilized the window-based coherence model @cite to detect the adversarial crafted inputs. The results showed that their joint learning model was much more stable than other models without considering to detect adversarial samples. The adversarial samples can roughly fall into two categories, where one is based on the well-written permuted paragraphs and has been studied by @cite , and the other represents prompt-irrelevant samples which remains to be deal with.
- Existing learning based IDSs utilize ML and DL models for distinguishing different types of network data. In the category of recent ML based IDSs, @cite combined unsupervised clustering and supervised learning for robust network traffic classification. @cite proposed semi-supervised fuzzy method for intrusion detection. Those methods establish NID models by learning knowledge from a large amount of unlabeled network data. Howbeit, the performance of their methods in detecting intrusions with small sample sizes remains unknown if abundant data are unavailable. Apart from those methods, supervised classification models such as LR, SVM, and random forest (RF) have been extensively applied for improving modern IDSs @cite @cite @cite . To deal with the feature redundancy problem in training supervised classifiers, @cite introduced mutual information based algorithm to select network features. @cite adopted word embedding for extracting meaningful features of network data. In order to further alleviate the over-fitting problem, RF @cite and tree algorithms @cite have been employed to ensemble sub-classification models for robust NID. However, the data scarcity and data imbalance remain unsolved because the feature selection extraction and classifier aggregation do not increase the number of intrusion samples among different categories in the training set.
- For robots with large number of DOFs, planning in configuration space can often be computationally expensive. To speed up the computation, one approach is to split the planning problem into two lower-dimensional sub-problems - planning for the shoulder and elbow joints and that for the wrist joints @cite . Researchers have often projected the configuration space to a lower dimensional space and used topological methods for path planning. @cite shows that under some constraints the topological properties are preserved upon projection.
- Popular loss functions used for classification in deep networks include hinge loss, soft-max loss, Euclidean loss and contrastive loss @cite . A triplet loss could simultaneously perform recognition and clustering, however its training is prohibitive due to huge number of triplet combinations on large-scale datasets @cite . Since these loss functions are limited in their capability to achieve discriminability in feature space, recent literature explores the combination of multiple loss functions. To this end, @cite showed that the combination of soft-max and contrastive losses concurrently enforce intra-class compactness and inter-class separability. On a similar line, @cite proposed center loss' that uses separate objectives for classification and clustering.
- Margin-maximizing learning objectives have been traditionally used in machine learning. Hinge loss in Support vector machines is one of the pioneering max-margin learning framework @cite . Some recent works aim to integrate max-margin learning with cross-entropy loss function. Among these, Large-margin soft-max @cite enforces inter-class separability directly on the dot-product similarity while SphereFace @cite and ArcFace @cite enforce multiplicative and additive angular margins on the hypersphere manifold, respectively. The hypersphere assumption for feature space makes the resulting loss less generalizable to applications other than face recognition. Furthermore, enforcing margin based separation in angular domain is an ill-posed problem and either requires approximations or assumptions (e.g., unit sphere) @cite . THis paper proposes a new flexible loss function which simultaneously performs clustering and classification, and enables direct enforcement of the max-margin constraints. We describe the proposed loss formulation next.
- Like other stochastic processes , motion modeling is often addressed by training transition operators, also called auto-regressive models. At each time step, such a model predicts the next pose given the previous poses. Typically, training such a model involves supplying recorded frames to predict the next recorded target. This strategy -- called teacher forcing -- does not expose the model to its own errors and prevents it from recovering from them, a problem known as . To mitigate this problem, previous work suggested to add noise to the network inputs during training . Alternatively, @cite forgo teacher forcing and always inputs model predictions. This strategy however can yield slow training since the loss can be very high on long sequences.
- Due to the difficulty of long-term prediction, previous work has considered decomposing this task hierarchically. For locomotion, @cite propose to subdivide the task into three steps: define the character trajectory, annotate the trajectory with footsteps, generate pose sequences. The neural network for the last step takes trajectory and speed data as input. This strategy makes the task simpler since the network is relieved from modeling the uncertainty due to the trajectory and walk cycle drift. @cite consider a network which computes different sets of weights according to the phase in the walk cycle. Other work consider alternative metrics and human evaluation to deal with the uncertainty of the task .
- Forecasting is an active topic of research beyond the prediction of human pose sequences. Pixel-level prediction using human pose as an intermediate variable has been explored . Related work also includes the forecasting of locomotion trajectories , human instance segmentation , or future actions . Other types of conditioning have also been explored for predicting poses: for instance, @cite explore generating skeleton pose sequences of music players from audio, @cite aim at predicting future pose sequences from static images. Also relevant is the prediction of 3D poses from images or 2D joint positions . The prediction of rigid object motion for robotic applications is also relevant, e.g. @cite model object dynamics using a neural network that performs spatial transformations on point clouds.
- Our work builds on top of who investigate language modeling for pretraining Transformer encoders. Their approaches lead to drastic improvements on several classification tasks from the GLUE benchmark @cite . show that language modeling pretraining can also provide significant improvements on machine translation tasks, even for high-resource language pairs such as English-German where there exists a significant amount of parallel data. Concurrent to our work, results on cross-lingual classification using a cross-lingual language modeling approach were showcased on the BERT repository https: github.com google-research bert . We compare those results to our approach in .
- Another approach that has proved effective in ameliorating loneliness and depression in older people is reminiscence therapy @cite . A pilot study aimed at implementing a conversational agent that collects and organizes memories and stories in an engaging manner is reported in @cite . The authors suggest that a successful companionable agent needs to possess not only a model of conversation and of reminiscence, but also generic knowledge about events, habits, values, relationships, etc., to enable it to respond meaningfully to reminiscences. Robotic pets are another interesting technology offered for alleviating loneliness in older people; such pets react to user speech and petting by producing sounds and eye and body movements. Studies @cite show that such interactions can improve users' communication and interaction skills.
- Recently, several deep methods have been proposed for imbalanced data learning @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . One major direction is to integrate the sampling idea and cost-learning into an efficient end-to-end deep learning framework. @cite treated the Complementary Neural Network as an under-sampling technique, and combined it with SMOTE-based over-sampling to rebalance the data. @cite studied data resampling for training cost-sensitive neural networks. In @cite @cite , the cost-sensitive deep features and the cost parameter are jointly optimized. @cite resampled the number of foreground and background image patches for learning a convolutional neural network (CNN) for object classification. @cite proposed a selective learning(SL) method to manage the sample distribution in one batch to a target distribution and assign larger weight for minority classes for backward propagation. Another recent direction of the problem involves the metric learning into the system. @cite @cite proposed a class rectification loss (CRL) regularising algorithm to avoid the dominant effect of majority classes by discovering sparsely sampled boundaries of minority classes. More recently, LMLE CLMLE @cite @cite are proposed to preserve the local class structures by enforcing large margins between intra-class and inter-class clusters.
- The idea of curriculum learning was originally proposed in @cite , it demonstrates that the strategy of learning from easy to hard significantly improves the generalization of the deep model. Up to now, works been done via curriculum learning mainly focus on visual category discovery @cite , object tracking @cite , semi- weakly-supervised learning @cite @cite @cite @cite , etc. @cite proposed an approach that processes multiple tasks in a sequence with sharing between subsequent tasks instead of solving all tasks jointly by finding the best order of tasks to be learned. Very few works approach the imbalanced learning. @cite developed a principled learning strategy by leveraging curriculum learning in a weakly supervised framework, with the goal of effectively handling massive amount of noisy labels and data imbalance.
- Basically, secure PA here refers to confirming the authenticities of pilot tones from LUs suffering above three attacks. This includes how to detect any alteration to their authenticities and how to protect and further maintain high authenticities. Since PA also means authenticating CSIs, much work have been extensively investigated on this area, from narrow-band single-carrier systems @cite @cite @cite @cite @cite @cite @cite @cite to wide-band multi-carrier systems @cite @cite @cite @cite @cite @cite @cite .
- Video prediction has received increasing attention in the recent years. Generally speaking, the nontrivial task of video prediction has been studied based on Convolutional Neural Networks (CNNs) and Recurrent Neural Networks (RNNs). Methods based on CNNs are well-known for extracting features from images and dealing with complicated images and videos @cite . However, using pure convolutional networks is limited by an inability to deal with temporal information from input video thus, creating a crucial problem in video prediction. Apart from CNN-based methods, researchers also dig into RNN-based methods for video prediction. Convolutional LSTM @cite is an effective one within various RNN methods. Following the Long Short Term Memory model, ConvLSTM utilizes its recurrent neural network architecture to memorize temporal information in a video sequence and extracts the spatial feature maps by using convolutional operation. Therefore, we refine it for better prediction performance in this paper.
- The task of video prediction is of great value to precipitation nowcasting problem. Previous works such as @cite and @cite have studied in video prediction and applied their models to realistic precipitation forecasting, like Figure .
- Given a sequence of input frames, e.g. sequence of radar echoes, our goal is to predict the most possible sequence of future frames. In paper @cite , ConvLSTM follows Equation 1 to make prediction on Moving MNIST dataset and Hong Kong radar echo dataset. PredRNN enhances the ConvLSTM by using a spatiotemporal flow to further utilize the spatial feature and memorize the temporal information. Following ConvLSTM, PredRNN trains on Moving MNIST dataset and also deals with the precipitation nowcasting problem by training on the HK radar echo data.
- Recently, @cite proposed a novel form of normalization named Group Normalization (GN). Distinct from the widespread BN, GN normalizes feature maps by grouping channels into small groups and computing the mean and variance of each small group. In this way, GN can normalize the feature tensors within each single sample. As a result, the normalization can be independent from batch size. In the setting of small batches which is commonly seen in computer vision tasks, GN can be an effective normalization without involving the batch dimension.
- Recent advances in zero-shot learning typically learn an embedding from the image feature space to the semantic space, where the embedding is learned via a linear parameterized mapping. During testing, for an unseen class, the semantic vector is predicted and the neighbor class is assigned. The ALE @cite learns a bilinear compatibility function between the image and the attribute space using the ranking loss. The ESZSL @cite uses the square loss to learn the embedding and explicitly regularizes the objective. The SCoRe @cite adds a semantically consistent regularization to make the learned mapping perform better on test images. The SAE @cite uses a linear semantic autoencoder that its decoder acts as an additional constraint on the mapping to reconstruct the original image features.
- In addition, non-linear compatibility mapping models have also been proposed. The LATEM @cite proposes piecewise compatibility modal learning which learns nonlinear compatibility function and the CMT @cite trains a neural network with two hidden layers to learn a nonlinear mapping from image feature space to word2vec space. The DEM @cite argues that the image feature space is more discriminative than semantic space, thus it proposes an end-to-end deep embedding model which maps from semantic space into the image feature space.
- Another direction of zero-shot learning embeds the image feature and the semantic into common intermediate space. The JLSE @cite maps the image features and the semantic space into two separate latent spaces, and measures their similarity by learning another bilinear compatibility function. The LAD @cite proposes to learn a latent attribute space, which is not only discriminative but also semantic-preserving. The SYNC @cite constructs the classifier of unseen classes by taking the linear combinations of base classifiers, which are trained in a discriminative learning framework. Annadani @cite captures semantic relations defined on the categories themselves to learn the intermediate embedding space.
- The autoencoders are used for classification based on the assumption that higher dimensional features are better classification @cite . The SAE @cite model is a semantic autoencoder. Its decoder imposes an additional constraint in learning the visual to semantic mapping. This is very effective in mitigating the domain shift problem. This is because although the visual appearance of attributes may change from seen classes to unseen classes, the demand for more truthful reconstruction of the visual features is generalizable across seen and unseen domains, resulting in the learned project function less susceptible to domain shift @cite . Similarly, in our model, the encoder maps the image feature to the semantic embedding and the decoder reconstructs the original image feature to recover all the image feature and semantic information. Differently, our model proposes the discriminative feature in the embedding space and the decoder imposes a regressor feedback to the truthful and representative image feature. At test time, we use the decoder to generate the reconstructed unseen image features and then train an SVM classifier.
- Zero-shot learning has been restrictive with a strong assumption that the image used to predict can only come from unseen classes. Therefore, generalized zero-shot learning has been proposed in @cite to generalize the zero-shot learning to the case where both seen and unseen classes are used during testing. Chao @cite showed that it is nontrivial and ineffective to directly extend the current zero-shot learning approaches to solve the generalized zero-shot learning. Such a generalized setting, due to the more practical nature, is recommended as the evaluation settings for zero-shot learning @cite . We evaluate our model on the four benchmark datasets with SS and PS @cite for the two settings.
- Previous works estimate FR requirements by adopting different methodologies, from pre-defined, generation-based requirements to inertia-dependent ones. In order to assess the economic benefits of dynamic demand in providing frequency regulation, @cite constructed a unit commitment (UC) model with per-defined frequency regulation and reserve requirements. Similarly, @cite developed a comprehensive UC model for the UK power system and used historical reserve requirements to estimate future electricity and reserve prices. System inertia can capture transient frequency evolution dynamics to accurately determine hourly-based FR requirements. Chavez2014a uses the system inertia to estimate PFR adequacy. Recent works @cite successfully conducted the stochastic scheduling for generation dispatch and ancillary services with inertia-dependent constraints. Regarding the multi-speed FRs, @cite formulates the model that can simultaneously allocate inertia responses (IR), PR and EFR, and evaluates the EFR effectiveness. Considering different inertia conditions, @cite proposed a utility function for system operator (SO) to choose multi-speed FR products.
- Inferring the latent mental states of agents (e.g., beliefs, desires, and intentions) from behavior features prominently in machine learning and cognitive science (see for a recent and comprehensive review from the machine learning point of view and for a developmental perspective). Previous computational treatments similar in spirit to the approach here have focused on making inferences about other individuals acting in a single agent setting @cite @cite @cite @cite @cite @cite @cite . When these tools are applied to multi-agent and game theoretic contexts they have focused on dyadic interactions @cite @cite @cite @cite . Dyadic interactions are significantly simpler from a representational perspective since an observer must merely determine whether each agent is cooperating or competing.
- The question of testing the direct product was studied extensively when the underlying domain @math @cite @cite @cite @cite @cite . In this setting, Goldreich and Safra @cite proposed a constant query test. Dinur and Reingold @cite suggested the two-query test mentioned above and analyzed it in the high acceptance regime but with a relaxed distance measure.
- The state of the art in this context is the result of Dinur and Steurer The result in @cite is stated in the language of tuples, i.e., the domain is a subset of @math , but their result also holds when the domain is a collection of @math -sized subsets of @math . See @cite for more details. @cite dealing with the domain @math where @math varies between @math and @math . They analyze the aforementioned two-query test with @math -intersection size. They analyze it in the high acceptance regime and show that @math indeed admits a direct product testing theorem. The proof is quite involved and in particular analyzes first the low acceptance regime. Recently, in a breakthrough paper, Dinur and Kaufman @cite analyzed the two-query test when the underlying domain is obtained from the set of faces of a Ramanujan complex. Their approach crucially relies on the result of @cite . More recently @cite introduced the notion of and remarked that it might admit a direct product theorem.
- We remark that the direct product testability question was further analyzed in the low acceptance regime under the domain @math , see @cite @cite @cite and also under the domain where the universe is @math , and the domain is the set of all subspaces of @math @cite .
- Using object information in an image is the most utilized scene traits for scene recognition @cite @cite @cite . When a particular object appears in an image, the chance of the image belonging to a certain category associated with the object increases. For example, if a TV is detected, the chance of the image being in a living room increases. In the previous works, the objects features were used for scene recognition instead of detecting the objects directly. To extract the object features, large image datasets for object recognition are used and they are ImageNet @cite , PASCAL visual object classes (VOC) @cite , or Microsoft COCO @cite .
- Previous studies also used other scene traits: The analysis of object scales in the scene images was utilized in @cite and @cite . In @cite @cite @cite @cite , the number of CNN input patches was adjusted by considering several objects in the scene image. To capture recurring visual elements and salient objects in scene recognition, the deformable part-based model (DPM) was utilized in @cite . In addition, the traits that features appearing in each image region within scene images are all similar was used in @cite . A super category was proposed to solve the problem that the scene categories have label ambiguity in @cite . A deep gaze shifting kernel was developed to distinguish sceneries from different categories in @cite . As such, the traits of the scene image are very diverse, and there seem to be still many available unused scene traits in scene recognition studies.
- In order to combine scene and object features for scene recognition, the effective feature fusion is great importance and several fusion methods have been reported. For example, the two features extracted by two different CNNs were combined at feature level by summing or concatenating the features in @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . Then, the classical classifiers such as support vector machine (SVM) @cite was applied to the fused features. Unfortunately, these methods have some drawbacks that they cannot be trained in an end-to-end manner. Moreover, the simple summation or concatenation might degrade the recognition performance owing to redundancy in two feature sets.
- Word2vec is a classical method used to transform a word into a vector @cite . Methods like word2vec keep information about semantics @cite . Sent2vec @cite create embedding of sentences. It has state-of-the-art results on datasets for unsupervised sentence similarity evaluation.
- Recognizers based on the bottom-up approach first perform individual character detection and recognition using character-level classifiers, and then they integrate the results across the whole text image to get the final word recognition. In @cite @cite @cite @cite , traditional hand-crafted features ( , HOG) were employed to train classifiers for detecting and recognizing characters in the text image. With the recent success of deep learning, many methods @cite @cite @cite @cite used deep neural networks to train character classifiers. In @cite , Wang al employed a CNN to extract features from the text image for character recognition. @cite first over-segmented the whole text image into multiple character regions before using a fully-connected neural network for character recognition. Unlike @cite @cite which used a single character classifier, @cite combined a binary text no-text classifier, a character classifier and a bi-gram classifier to compute scores for candidate words in a fixed lexicon. To recognise text without a lexicon, a structured output loss was used in their extended work @cite to optimize the individual character and @math -gram predictors.
- The above bottom-up methods require the segmentation of each character, which is a highly non-trivial task due to the complex background and different font size of text in the image. Recognizers @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite which are based on the top-down approach directly recognize the entire word in the image. In @cite and @cite , Jaderberg al extracted CNN features from the entire image, and performed a 90k-way classification (90k being the size of a pre-defined dictionary). Instead of using only CNNs, @cite @cite @cite also employed recurrent neural networks to encode features of word images. All these three models were optimized by the connectionist temporal classification @cite , which does not require an alignment between the sequential features and the ground truth labelling.
- Following the success of the attention mechanism @cite in neural machine translation, recent text recognizers @cite @cite @cite @cite @cite @cite introduced a learnable attention network in their decoders to automatically select the most relevant features for recognizing individual characters. In order to handle distorted scene text, @cite @cite employed a spatial transformer network (STN) @cite to rectify the distortion of the entire text image before recognition. As it is difficult to successfully train a STN from scratch, Cheng al @cite proposed to encode the text image from multiple directions and used a filter gate to generate the final features for decoding. Unlike @cite @cite @cite which rectified the distortion of the entire image, a hierarchical attention mechanism was introduced in @cite to rectify the distortion of individual characters.
- Different from @cite @cite @cite @cite which focused on recognizing severely distorted text images, we address the problem of having character with different scales. This is a common problem that exists in both distorted and undistorted text recognition. The scale-spatial attention network (S-SAN) proposed in this paper belongs to the family of attention-based encoder-decoder neural networks. Unlike previous methods @cite @cite @cite @cite @cite @cite @cite which employed only a single CNN as their feature encoder, we introduce a scale aware feature encoder (SAFE) to extract scale-invariant features from characters with different scales. This guarantees a much more robust feature extraction for the text recognizer. Although a similar idea has been proposed for semantic segmentation @cite to merge segmentation maps from different scales, it is the first time that the scale problem has been identified and efficiently handled for text recognition using an attention-based encoder-decoder framework. Better still, SAFE can also be easily deployed in other text recognizers to further boost their performance.
- In @cite , introduced the theoretical foundations of matrix profile, and proposed a first algorithm, called STAMP, for computing the matrix profile over a time series. The algorithm uses a similarity search algorithm, called MASS, that under z-normalized Euclidean computes the distance of each subsequence to other subsequences by using the Fast Fourier Transform (FFT). Other exact algorithms such as Quick-Motif @cite , IMD @cite , or MK @cite can be fast for cooperative data (those that are relatively smooth data, short motif lengths etc.). But in data (e.g., seismology data) these algorithms are not efficient @cite .
- In @cite , proposed an algorithm, called STOMP, that is faster than STAMP. The STOMP algorithm is similar to STAMP in that it can be seen as highly optimized nested loop searches, with the repeated calculation of distance profiles as the inner loop. However, while STAMP must evaluate the distance profiles in random order (to allow its anytime behavior), STOMP performs an ordered search. STOMP exploits the locality of these searches, and reduces the time complexity by a factor of @math . In @cite , the authors proposed an extension of STOMP, called SCRIMP++, that converges much faster than STOMP. To the best of our knowledge, all most all matrix profile algorithms have been developed for z-normalized Euclidean distance. In this paper, we proposed efficient algorithms for a larger class of Euclidean functions. We also proposed an algorithm for the z-normalized case, , ACAMP, that is significantly faster than SCRIMP++, which is the fastest exact algorithm for matrix profile computation in the literature, to the best of our knowledge. Our ACAMP algorithm is designed based on an efficient incremental technique that does not need to calculate FFT (in contrast to SCRIMP++).
- To bridge the gap between two different distributions, many approaches have been proposed based on the ideas of domain adaptation @cite @cite @cite @cite . Multimodal deep learning architectures have been proposed to learn domain invariant representations in @cite . However, this method performed primarily in a generative context and did not leverage the full representation power of supervised CNN representations. In @cite , proposed pre-training with a denoising auto encoder, then train a two-layer network simultaneously with the MMD domain confusion loss. Nevertheless, the learned network is relatively shallow and therefore lacks the strong semantic representation which is learned by directly optimizing with a supervised deep CNN. In @cite , proposed a new CNN architecture which introduces an adaptation layer and an additional domain confusion loss for classification. They use MMD both to select the depth and width of the architecture while using it as a regularizer during fine-tuning, and achieved state-of-the-art performance on the standard visual domain adaptation benchmark.
- For distortion @math in planar graphs, Thorup @cite and Klein @cite constructed distance labels of size @math . Abraham and Gavoille @cite generalized this result to @math -minor-free graphs, achieving label size @math . @math is a function that depends on @math only, taken from the structure theorem of Robertson and Seymour @cite . No low-dimension embedding into @math with distortion @math is known for planar graphs or even treewidth- @math graphs. If one allows larger distortion, Krauthgamer al @cite proved that planar graphs embed with distortion @math into @math , or more generally that @math -minor-free graphs embed with distortion @math into @math . Abraham al @cite showed that @math -minor-free graphs embed with distortion @math into @math . Turning to priorities, Elkin al @cite constructed prioritized versions of distance labeling with distortion @math . Specifically, for planar and @math -minor-free graphs they achieve label sizes of @math and @math , respectively. No prioritized embeddings are known, nor lower bounds thereof.
- Another way to represent distances is a distance oracle @cite . This is a data structure that, given a pair of points, returns an estimate of their pairwise distance. The properties of interest are distortion, space and query time. A distance labeling can be viewed as a distributed version of a distance oracles, see also @cite @cite @cite @cite @cite @cite .
- Exact distance labelings were studied in the precession of bits (i.e. not asymptotically) see e.g. @cite . Another type of labeling studied is adjacency labeling @cite , where given two labels one can compute whether the vertices are adjacent. Efficiency of the labeling algorithms has also been studied @cite .
- Many distribution pole inspections are still performed by the inspection team travelling on foot @cite . They use varied methods such as visible spectrum cameras, infrared cameras, binoculars, and corona detection camera to perform the visual inspection on the pole and attached components such as conductors, transformers and insulators. This type of inspection is labour intensive and time consuming. Extreme weather and difficult terrains can also present challenges.
- To that end, helicopter inspection can be utilised @cite . The inspection can then be performed online or offline. The image size produced by this type of inspection is usually very large compared to the objects of interest such as tower components. This leads to the tiny object detection problem. Note that, the data we used in this paper was produced from this type of inspection.
- As typical algorithmic approaches to the problem, we can distinguish bottom-up versus top-down procedures @cite . Subspace clustering aims at finding while projected clustering aims at finding where each cluster can reside in a subspace (projection) that is different from the other clusters' subspace. Top-down procedures (typically solving the projected clustering problem) start in the full dimensional space with some proto-cluster @cite @cite @cite @cite , or local neighborhood point set @cite @cite , and derive some adaptive weighting of distances for the clustering procedure. Bottom-up procedures start in one-dimensional subspaces associated with single features, and then iteratively combine those features deemed relevant or interesting, assessing the importance of the feature for proto-clusters in subspaces @cite @cite @cite @cite @cite @cite or for the local neighborhoods of individual points @cite @cite @cite @cite @cite @cite . As testing all combinations would lead to an exponentially-large search space, these approaches typically identify some monotonic property that allows early pruning of less-promising combinations following principles borrowed from frequent pattern mining @cite . Even so, existing subspace clustering methods remain computationally expensive, and tend to deliver many redundant subspaces and clusters.
- Some methods consider feature combinations directly or assess (linear) correlations among features. These typically rely on locally applied PCA as a primitive to assess locally relevant feature combinations @cite @cite or on an adaptation of the Hough transform @cite , which is computationally even more expensive.
- This work is related to existing research that incorporates knowledge representation and reasoning (KRR) into sequential decision-making (SDM) in stochastic worlds. SDM can be realized via either probabilistic planning (e.g., MDPs and POMDPs @cite ) or reinforcement learning (RL) @cite .
- When world model is unavailable, one can use RL algorithms to learn an action policy. Declarative action knowledge has been used to help an agent to select only the reasonable actions in RL exploration @cite . Researchers have developed an algorithm called PEORL that integrates hierarchical RL with task planning @cite . In that work, RL (low-level) helps learn action costs for the task planner, and the task planner guides RL (high-level) to accomplish complex tasks. Also, researchers proposed an architecture that uses knowledge to find out RL components for incremental learning of unknown rules governing the domain dynamics @cite . Symbolic planning has been incorporated to deep reinforcement learning to improve interpretability of subtasks @cite . The work by griffith2013policy integrates human feedback for policy shaping in their reinforcement learning framework @cite . These works cannot learn complex representations from previous annotated decision-making experiences.
- alahi2016social proposed the social LSTM to consider trajectory prediction of people with interactions at large distance @cite . The developed architecture, automatically learns typical interactions that take place among trajectories which coincide in time by sharing LSTMs of spatially proximal sequences. fernando2017soft+ developed a method to predict human future motion based on their trajectory and trajectory of their neighbors using LSTM framework @cite . None of the works involve active perception for human intention estimation.
- Over-sampling adds repeated samples from minor classes, which could cause the model to overfit. To solve this, novel samples can be either interpolated from neighboring samples @cite or synthesized @cite @cite for minor classes. However, the model is still error-prone due to noise in the novel samples. It was argued that even if over-sampling incurs risks from removing important samples, under-sampling is still preferred over over-sampling @cite .
- Our theoretical framework is inspired by the random covering problem @cite , where the goal is to cover a large set by a sequence of i.i.d. random small sets. We simplify the problem in by making reasonable assumptions. Note that the effective number of samples proposed in this paper is different from the concept of effective sample size in statistics. The effective sample size is used to calculate variance when samples are correlated.
- Existing works that have used lyrics for genre and popularity prediction can be partitioned into two categories. First, that use lyrics in augmentation with acoustic features of the song @cite @cite and second, that do not use acoustic features @cite @cite @cite @cite . However, all of them represent lyrics using either handcrafted features or bag-of-words models. Identifying features manually requires intricate knowledge of music, and such features vary with the underlying dataset. Mikolov and Le have shown that distributed representation of words and documents is superior to bag-of-words models @cite @cite . To the best of our knowledge, this is the first work that capitalizes on such representation of lyrics for genre and popularity prediction. However, our results cannot be directly compared with existing works as datasets, set of genres, the definition of popularity, and distribution of target classes are not identical. Still, our results stand in contrast with existing works that have concluded that lyrics alone are a weak indicator of genre and popularity. These works report significantly low performance of lyrics for genre prediction task. For example, report an accuracy of 34
- There are many works on compression of neural networks. The network pruning @cite @cite @cite @cite removes the redundant channels of the network. The architecture search @cite @cite automatically finds the efficient architecture by reinforcement learning. The works @cite @cite @cite are related to designing the architecture of neural networks for more efficiency. Most of these compression methods produce one static sized model. Using these models, to cope with the requirements of computing with limited resources, it should prepare many different-sized networks to its memory, which is not desirable for the resource constrained scenario like in an embedded environment.
- The works in @cite @cite @cite @cite @cite are based on the idea of not fully using the networks entire feed forward graph, but picking a subset of the graph specific for each input. Those methods have a full capacity network as a baseline, and train an additional module which selects where to forward between channels, blocks, or other paths. These selection modules can be other external small network @cite @cite @cite , or located inside the network as a gating function @cite @cite @cite . Utilizing this may lead the network to change its complexity at runtime. However, these methods usually need to be trained by reinforcement learning which normally encounters the problem of slowness and instability. Unlike these, our method is not based on reinforcement learning.
- The works in @cite @cite @cite use condition vectors to control the network to output a desirable result. The condition vector which is inputted with data can be used as a clue for the generator network to select modes. Our method also uses a condition parameter to control the model complexity as the user's will. Using this condition, our network's computation graph still changes dynamically but can be balanced to follow the given condition.
- While the theorems of @cite @cite describe conditions under which the homotopy type of a manifold is recoverable from a Vietoris--Rips complex for sufficiently small @math , much less is known about the topological behavior of these constructions for large values of @math , even though large values of @math commonly arise in applications of persistent homology @cite . However, more is known in the specific case when the underlying manifold is the circle. The following theorem from @cite is based on @cite @cite .
- Related papers include @cite which studies the 1-dimensional persistence of C ech and Vietoris--Rips complexes of metric graphs, @cite which extends this to geodesic spaces, @cite which studies approximations of Vietoris--Rips complexes by finite samples even at higher scale parameters, and @cite which applies Bestvina--Brady discrete Morse theory to Vietoris--Rips complexes.
- This metric is also called the Kantorovich, optimal transport, or earth mover's metric @cite @cite @cite ; it provides a notion of distance between probability measures defined on a metric space. Although it exists much more generally @cite @cite @cite , the @math -Wasserstein metric on @math can be defined as follows. Given @math with @math and @math , define a @math between @math and @math to be any collection of non-negative real numbers @math such that @math and @math . Define the of the matching @math to be @math . The @math -Wasserstein distance between @math is then the infimum, varying over all matchings @math between @math and @math , of the cost of @math .
- Note that @math is isometric to @math . Contrary to the situation for an arbitrary Vietoris--Rips complex, the embedding @math into the Vietoris--Rips metric thickening given by @math is continuous. In fact, more is true: @math is an of @math @cite @cite . For this reason, we identify @math with the measure @math in the image of this embedding.
- Though the facial structure of the Barvinok--Novik orbitopes @math is not known for @math , certain neighborliness results have been established @cite . Sinn has shown that the orbitopes are simplicial @cite . Additionally, Vinzant proved that the edges of @math consist of all line segments @math with @math @cite . In other words, the edges of @math are the same as the edges of @math . The following is an immediate corollary of the work of Sinn and Vinzant.
- The classical approaches to image-manipulation detection include @cite @cite , and the deep-learning-based approaches include @cite @cite @cite @cite . These algorithms aim to locate the manipulated image regions by outputting a mask or a set of bounding boxes enclosing suspicious regions. Unfortunately, they are not directly applicable to inpainting-quality estimation because they have a different goal: whereas an objective quality-estimation metric should strive to accurately compare realistically inpainted images similar to the originals, a forgery-detection algorithm should strive to accurately tell one apart from the other.
- Environmental field estimation and mapping through mobile sensing is an active research topic. Model-based approaches have attracted attention of researchers for spatial data analysis since they can estimate the underlying structure of the data generating process and provide the spatial characteristics and global information of the environment. For example, by generalizing a Gaussian distribution in a finite vector space, to a Gaussian process in a function space of infinite dimension, the GP framework can be used to model various physical phenomena and estimate values at any location in the sensing domain @cite .
- A distribution of sampling locations provides a pattern across the study area, which indicates the estimation performance of the underlying field. Many frameworks have been proposed to select the target locations for constructing the scalar field. Among them, the experimental design theory has been studied to find the optimum sensor deployment plan by optimizing an information gain that is determined by using an established model. In the research, strategies were addressed to select the locations with high informativeness'' to achieve adaptive sensing (e.g., @cite ). However, these strategies rely on prior knowledge of the environmental model structure. Deviations of the assumptions will lead to model misspecification and unacceptable estimation performance. In other words, the field estimation results will not be robust against misspecification of the established model. In many circumstances, the assumptions may not be practically feasible. For instance, the assumption of a known and constant mean of the environmental model (e.g., @cite ) can cause inferior estimation performance in a practical situation where unknown spatial trends exist.
- For an unknown environment, the underlying environmental model is learned by utilizing observations that are taken across the study area. Coverage sampling that is done to relax unrealistic assumptions is called an exploratory design, which provides sufficient flexibility to explore an unknown field @cite . Such designs have been adopted in spatial mapping without rigid prior assumptions, for initial collection of prior knowledge, exploratory survey for long-term design improvement, or combination with optimum designs for improved estimation performance. In most existing monitoring programs, the exploratory sampling is required to learn the underlying field. Regular grid-based sampling approaches accomplish such objectives as exploratory designs @cite @cite . These approaches distribute sampling sites by decomposing the field into a grid of cells (e.g., squares, triangles, hexagons) where the samples are taken in the grid tessellation. In the absence of prior knowledge, a regular grid ensures overall coverage of the surveillance area. In particular, it provides a simple frame to distribute spatially balanced data samples across the study area for prior survey.
- Formal Language Theory, primarily developed to study the computational basis of human language is now being used extensively to analyze any rule-governed system @cite @cite @cite . Formal languages have previously been used to train RNNs and investigate their inner workings. The Reber grammar @cite was used to train various 1 st order RNNs @cite @cite . The Reber grammar was also used as a benchmarking dataset for LSTM models @cite . Regular languages, studied by Tomita @cite , were used to train 2 nd order RNNs to learn grammatical structures of the strings @cite @cite .
- Regular languages are the simplest grammars (type-3 grammars) within the Chomsky hierarchy which are driven by regular expressions. For neural network research an interesting subclass of regular languages is the Strictly -Piecewise languages. Strictly -Piecewise languages are natural and can express some of the kinds of LDDs found in natural languages @cite @cite . This presents an opportunity of using SP grammar to generate benchmarking datasets @cite @cite . In , LSTM networks were trained to recognize valid strings generated using SP, SP, SP grammar. LSTM could recognize valid strings generated using SP and SP grammars but struggled to recognize strings generated using SP grammar, exposing the performance bottleneck of LSTM networks. It has also been observed that the performance of LSTMs on SP datasets degraded when the length of the LDDs in the datasets were increased, this was done by increasing the maximum length of the generated strings of SP @cite .
- In recent years, significant research has been done to minimize power loss in the process of reconfiguring distribution power systems. The reconfiguring of distribution power system has a combinatorial nature and the search of the best computational time for supporting the decision-making process in real time has been focused on @cite , @cite , @cite .
- In @cite the authors implement an algorithm for network reconfiguration for a realistic distribution network based on a genetic algorithm (GA), taking as objective power loss minimization and load balancing index. HATSGA uses a distinct genetic algorithmic approach by using elitism to choose among potential solutions.
- Previous stack based approaches to open-loop planning maintain a fixed size stack of @math statistics over actions to sample open-loop plans with high expected return @cite @cite @cite . While these approaches work well in practice, their convergence properties remain unclear because of the non-stationarity of the sampling statistics and the underlying state distributions due to the simultaneous adaptation of each statistic. Furthermore, the required number of statistics is highly domain dependent and hard to prespecify. SYMBOL maintains an of Thompson Sampling bandits, which automatically adjusts its size according to the underlying domain without prior domain knowledge. The creation and adaptation of each bandit depends on the bandits to preserve a stationary state and reward distribution for proper convergence of all bandits.
- @cite proposed a memory bounded version of MCTS with a fixed size state pool to add, discard, or reuse states depending on their visitation frequency. However, this approach cannot be easily adapted to tree-based open-loop approaches, because it requires (belief) states to be identifiable. SYMBOL does not require a pool to reuse states or nodes but maintains an of Thompson Sampling bandits. The bandits adapt according to the between actions, while the size of the bandit stack is bounded by the planning horizon and itself according to the underlying domain.
- Collecting and annotating a big corpus requires significant effort and has the additional challenge that agents trained in a supervised manner with a given corpus cannot easily generalize to unseen out of domain input. Building a good user simulator to train against can be challenging as well, even equivalent to building a dialogue system in some cases. Directly learning from humans leads to policies of higher quality, but requires thousands of dialogues even for small domains @cite . combine such resources to train dialogue policies. Recently, model-based RL approaches to dialogue policy learning are being revisited @cite ; however, such methods still assume a stationary environment.
- concurrently learn two negotiator agents' dialogue policies in a setting where they negotiate allocation of resources. However, their agents do not interact via language, but rather via dialogue acts. They use PHC and WoLF-PHC @cite to train their agents, who use two types of dialogue acts: and , each of which takes two numerical arguments. train agents on a similar task, but their agents are modelled as end-to-end networks that learn directly from text. However, the authors train their negotiator agent on supervised data and against a fixed supervised agent. Earlier works include , the first to train policies for two conversational agents, but with single-agent RL, and who applied co-adaptation on single-agent RL, using Inverse RL to infer reward functions from data.
- Software engineering tools are bringing sophisticated search power into the development environment by extending the browsing and searching capabilities @cite @cite @cite @cite @cite . For instance, Holmes and Murphy @cite proposed a technique that recommends source code examples from a repository by matching structures of given code. XSnippet @cite provides a context-sensitive code assistant framework that provides sample source code snippets for developers. In general, these tools help locate samples of code, demonstrate the use of frameworks and fasten development by exploring the syntactic context provided mainly by the IDE to recommend code samples more relevant to developers (as in Strathcona @cite ). However, the samples provided by these systems are highly dependent of a particular development context, whereas code samples typically are complete projects that were made to facilitate and accelerate the learning process of features provided by frameworks. Therefore, it is expected that the types of code samples explored in this paper present different characteristics when compared to samples automatically generated by tools.
- Complementing the tools aforementioned, many studies confirmed the the significance of API usage examples, mainly in the context of framework APIs, and proposed approaches to mine API usage examples from open code repositories and search engines @cite @cite @cite @cite @cite @cite . Most of these work retrieve the so-called code snippets to support API learning, whereas our work focus on complete projects of framework code samples. In addition, our work is not focused on proposing an approach to mine code samples, but analyze characteristics of these code samples.
- Nasehi et al. @cite focused on finding the characteristics of a good example on Stack Overflow. They adopted an approach based on high low voted answers, number of code blocks used, the conciseness of the code, the presence of links to other resources, the presence of alternate solutions, and code comments. Yang et al. @cite assessed the usability of code snippets across four languages: C #, Java, JavaScript, and Python. The analysis was based on the standard steps of parsing, compiling and running the source code, which indicates the effort that would be required for developers to use the snippet as-is. Finally, there are studies analyzing the adoption of code snippets @cite @cite @cite . For instance, Roy and Cordy @cite analyzed code snippet clones in open source systems. They found that on average 15
- In our paper, we analyze variable importance in the context of a set of good models, namely the Rashomon set. This idea is closely related to the works that investigate variable importance without using posthoc analysis. @cite estimate the bounds of variable importance in the sense that no good predictive model has variable importance that exceed the bounds. Unlike the posthoc methods mentioned above, they do not study variable importance for a pre-selected model. Instead, the analysis is performed on the set of all good models within a model class. Our work differs from theirs because we look at the joint set of variable importance rather than only the extremes. Our approach can reveal the importance of a variable in the context of other variables, which is not what their framework is designed to do.
- Post-selection inference, though not directly related to variable importance, is another common posthoc method in statistical analysis. Inference is the process of learning what we do not know from the dataset, and is usually done with a pre-selected data analysis specification. Inference results might not always be robust since there could be other specifications that are consistent with the dataset, yet lead to results that are not necessarily the same (and researchers might inevitably choose the specifications that are in favor of their presumptions.) One way to address this issue is to use a non-posthoc method and look at the joint inference results from all specifications that are justified by the dataset, which is referred to as the hacking interval by @cite . The idea of looking for all consistent data analysis specification in their work is similar to ours and the work by @cite that look at all the almost-equally accurate predictive models, and both works are non-posthoc.
- Finally, the idea that multiple models can explain the observed dataset well, namely the Rashomon effect ( @cite ), is discussed in other works that are not related to variable importance. For example, @cite implement robust optimization to find a decision rule that works well under a uncertainty set of possible situations that are consistent with the historical data. The fields of robust optimization and chance-constrained programming use uncertainty sets to make decisions; these uncertainty sets could arise from the Rashomon set. @cite analyze a model selection problem in high-dimensional regression and identify a minimal class of models that rely on different subsets of variables but perform almost equally well in terms of prediction accuracy. In some sense, they are exploring the Rashomon set with random search algorithms rather than fully characterizing it.
- M-Aegis @cite aims to protect data from cloud providers by using a transparent window that sits atop an existing application and encrypts input data in transit. Our approach differs from M-Aegis in a number of ways.
- Another closely related work is MessageGuard @cite , which implements a system that layers end-to-end encryption on top of existing web applications, using the browser as a global control point and deploying as either a browser extension or a bookmarklet. Our approach differs from MessageGuard in the following ways.
- There are also a number of methods of ensuring data confidentiality with the help of the cloud provider, most notably based on the use of fully-homomorphic encryption (see, for example, @cite @cite @cite @cite , although there is a wealth of additional literature within their citations and reverse-citations). These methods aim to have the server agnostically compute functions of a user's data, and they are aimed toward an honest-but-curious provider. Our approach does not require any server-side modifications while maintaining transparency to the user or multiple collaborating users. We also avoid the heavy computational machinery required for these schemes.
- A closely related work is DR in smart grids. @cite studied the mechanism design on DR in smart grids. Let @math , and there is an upper bound on the BES, i.e. @math . A randomized FPTAS mechanism was given in @cite . Their idea is to combine with smooth analysis and randomize auction. Actually, Dough and Roughgarden @cite showed that if there exists an FPTAS approximation, then this algorithm can be transformed into a truthful in expectation mechanism that retains the FPTAS property. The work in @cite does not require the existence of FPTAS. However, it still remains open whether there exists a deterministic FPTAS.
- @cite presented a truthful FPTAS for the max-knapsack problem. Our problem differs from the min-knapsack in which we have BES such that the capacity of the knapsack we need to cover is soft.
- Distributed storage systems emphasize the paramount importance of high availability and resilience to failures @cite @cite . Thus eventual consistency is often adopted @cite . The notion of eventual consistency is promoted by practitioners. It is different from the rigorous and formal treatment of consistency models in the academia @cite .
- Both @math -AV and @math -AV are NP-Complete in the general case, since they include the verification of atomicity as its special case. Read-mapping is used to circumvent the NP-Completeness of atomicity verification @cite . Assuming read-mapping, the @math case for @math -AV is solved in @cite . For @math , the @math -AV problem is solved only for special cases. Similarly, we also assume the existence of read-mapping, and solve the @math -AV problem for a restricted type of histories. For fixed @math and @math , the problem of proving NP-Completeness for @math -AV and @math -AV for are principally open.
- The interpretability and the discrimination power are two crucial aspects of a CNN @cite . In recent years, different methods are developed to explore the semantics hidden inside a CNN. Our previous paper @cite provides a comprehensive survey of recent studies in exploring visual interpretability of neural networks, including (i) the visualization and diagnosis of CNN representations, (ii) approaches for disentangling CNN representations into graphs or trees, (iii) the learning of CNNs with disentangled and interpretable representations, and (iv) middle-to-end learning based on model interpretability.
- | | Visualization of filters in a CNN is the most direct way of exploring the pattern that is encoded by the filter. Gradient-based visualization @cite @cite @cite showed the appearance that maximized the score of a given unit. Furthermore, Bau @cite defined and analyzed the interpretability of each filter. Recently, @cite provided tools to visualize filters of a CNN. Dosovitskiy @cite proposed up-convolutional nets to invert feature maps of conv-layers to images. However, up-convolutional nets cannot mathematically ensure the visualization result reflects actual neural representations.
- | | Many approaches have been proposed to diagnose CNN features, including exploring semantic meanings of convolutional filters @cite , evaluating the transferability of filters @cite , and the analysis of feature distributions of different categories @cite . The LIME @cite and the SHAP @cite are general methods to extract input units of a neural network that are used for the prediction score. For CNNs oriented to visual tasks, gradient-based visualization methods @cite @cite and @cite extracted image regions that are responsible for the network output, in order to clarify the logic of network prediction. These methods require people to manually check image regions accountable for the label prediction for each testing image. @cite extracted relationships between representations of various categories from a CNN. In contrast, given an interpretable CNN, people can directly identify object parts or filters that are used for prediction.
- | | Unlike the diagnosis and visualization of pre-trained CNNs, some approaches were developed to learn meaningful feature representations in recent years. For example, @cite required people to label dimensions of the input that were related to each output, in order to learn a better model. Hu @cite designed logic rules to regularize network outputs during the learning process. Sabour @cite proposed a capsule model, where each feature dimension of a capsule may represent a specific meaning. Similarly, we invent a generic filter loss to regularize the representation of a filter to improve its interpretability.
- Furthermore, some method distilled CNN knowledge into another model with interpretable features for explanations. @cite distilled knowledge of a neural network into an additive model to explain the knowledge inside the network. @cite roughly represented the rationale of each CNN prediction using a semantic tree structure. Each node in the tree represented a decision-making mode of the CNN. Similarly, @cite used a semantic graph to summarize and explain all part knowledge hidden inside conv-layers of a CNN.
- Scheduling is one of the most important components in LTE systems: the scheduler, implemented at the eNodeB (eNB), distributes the limited available time frequency resources among the active users in order to satisfy their QoS needs @cite @cite . The 3GPP specifications do not specify scheduling policies, making the scheduling algorithm vendor-specific @cite . There are several works that have proposed and compared various scheduling algorithms; however, in general, these do not use an optimization framework to maximize a chosen metric (such as our choice of sum rate or PF rate) while meeting constraints set by GBR users. In this paper we focus on VoLTE users; the principles developed here could be extended to other GBR flows.
- The three basic scheduling algorithms are round robin, max sum rate, and proportional fair @cite . The round robin scheduler assigns the PRBs in equal TTIs and in ordered manner. It allocates resources equally, regardless of the users' CQI. The max-sum rate scheduler maximizes throughput by assigning resources to users that have the highest SINR, but ignores fairness among users. The proportional fair scheduler provides a balance between fairness and the overall system throughput. Compared to the sum rate objective, achieving rate allocations with PF provides a better tradeoff between users' satisfaction (i.e., fairness) and total system throughput @cite .
- These three basic scheduling algorithms are modelled and compared in @cite and @cite in terms of both throughput and fairness index. @cite , the authors compare six scheduling schemes including round robin, max rate, Kwan max throughput, MaxMin, PF, and resource fair (RF) via simulations. @cite a new scheduling mechanism is proposed to increase the throughput compared to PF and round robin schedulers, while losing only 20 For multi-tier networks a scheduling algorithm based on PF scheduling and frequency reuse is developed in @cite . The work in @cite develops resource allocation for Coordinated Multi-Point transmission in LTE-Advanced to ensure a good tradeoff between cell-average and cell-edge spectral efficiency. Most of these works consider non-GBR users only (while we propose novel scheduling algorithms which consider two different services non-GBR and GBR services).
- Specifically for VoLTE, @cite proposes a resource allocation algorithm based on the prediction of channel quality change for VoLTE users. Also, in @cite a semi-persistent packet scheduling algorithm is proposed for VoLTE (GBR) users over heterogenous networks of fourth generation LTE and third-generation universal mobile telecommunications service (3G UMTS). A new channel- and QoS-aware scheduling algorithm (WE-MQS) is proposed in @cite for downlink scheduling of VoLTE users which considers user perception. To reduce the waste of resources when scheduling a low-rate VoLTE user on a strong channel, a scheduling algorithm is proposed in @cite in which voice packets for different users are multiplexed within one LTE packet in the downlink. Compared to these works we consider both GBR and non-GBR users in LTE networks. Also, the authors of @cite proposed a Resource Block Preserver scheduling algorithm for both real time and non-real time flows. It includes two layers, the upper layer is designed to satisfy the QoS of real time flows based on sub-frame aggregation, while the lower layer allocates the resource blocks to users depending on flow types. @cite and @cite new scheduling algorithms are developed which can manage mixed traffic including both real time and non-real time traffic.
- Researchers propose high-quality hand-crafted feature representations to localize 3D objects @cite @cite @cite when 3D object detection just emerges. These methods can obtain acceptable results when 3D shape descriptions are available in a simple scene. However, they fail to learn potential invariant features from more complex scenes so that they have a certain gap in practical applications.
- The monocular RGB images can provide rich appearance and context information. Deep3DBox @cite finds the fact that the perspective projection of the 3D bounding boxes should fit tightly within their 2D detection boxes. It extracts the 3D bounding boxes only from monocular RGB images. @cite proposes an energy minimization method that puts candidate boxes in 3D space by exploiting the fact that objects should be on the ground-plane and perform 3D object detection from monocular images. However, due to lacking of depth information, they fail to get high accuracy on 3D object detection tasks.
- Estimating the orientations of 3D bounding boxes is of great importance as the orientation angles will directly affect the 3D detection. We can easily get the orientation information from the bird's eye view of 3D point clouds. @cite utilizes a bird's eye view representation of point clouds to generate 3D highly accurate candidate boxes. However, this region proposal method would fall behind for small object detection.
- 3D space can be discretized into a 3D voxel grid and a sliding window method can be used through all three dimensions for detection @cite . It is also demonstrated that the exhaustive window searching in 3D space can be extremely efficient by exploiting the sparsity of the problem. Voxelnet @cite divides 3D points into 3D voxel grids equally in space, putting forward a novel voxel feature encoding (VFE) layer to encode each voxel via stacked VFE layers, which enable the interaction between points within a voxel by combining point-wise features with a locally aggregated feature. Then, the region proposal network takes the voxel features and obtains the 3D detection results.
- In order to reduce the search space, F-pointnet @cite extracts the 3D frustum point clouds by extruding 2D bounding boxes from 2D image detectors. Then Pointnets @cite @cite are performed for each 3D bounding box frustum to get 3D points of interests. Finally, an amodal 3D box estimation network is applied to yield the 3D detection results.
- Chatbots aim to engage users in human-computer conversations in the open domain and are currently receiving increasing attention because they can target unstructured dialogue without a priori logical representation of the information exchanged during the conversation. Existing work on building chatbots includes generation-based methods @cite @cite and retrieval-based methods @cite @cite @cite @cite @cite @cite . Generation-based models maximize the probability of generating a response given the previous dialogue. This approach enables the incorporation of rich context when mapping between consecutive dialogue turns. Retrieval-based chatbots have the advantage of informative and fluent responses because they select a proper response for the current conversation from a repository by means of response selection algorithms.
- Graph-based semi-supervised learning (GSSL) is a well-studied topic in computer science and engineering. @cite first used min-cuts in a graph to perform clustering and @cite introduced normalized min-cuts to deal with the issue of unbalanced networks. Other spectral-based methods were later proposed, including @cite @cite . A Gaussian kernel was used to construct the graph in @cite . A random walk based method called Adsorption was proposed in @cite and was modified in @cite into something called MAD. TACO, which was proposed in @cite , introduced an additional quantity of confidence in labeling. We recommend @cite for a good review of GSSL. While many methods on GSSL were developed in the last decade, few considered the consistency theoretically. @cite is the only paper we found that discussed the consistency of the basic GSSL method theoretically.
- Since the framework's introduction in 2007 by , @cite @cite , numerous SQKD protocols have been proposed @cite @cite @cite @cite @cite @cite @cite @cite (just to list a few), some with information theoretic proofs of security @cite @cite @cite . Often one is interested in removing requirements on one or both users while still attempting to attain security against an all-powerful adversary - this is to study the effects of these resources and abilities on the secure communication rate of the resulting protocol. However, no prior SQKD protocols allow for the smooth transition from a purely classical protocol to a semi-quantum one.
- In learning-free methods, Most of works in this field suppose that deblurring is shift invariant and cause by motion @cite @cite @cite , which can be treated as a deconvolution problem @cite @cite @cite @cite . There are many ways to solve this, many works @cite used bayesian estimation to solve it, that is
- Chan and Wang @cite proposed a robust total variation minimization method which is effective for regularizing the gradient or the edge of the sharp image. @cite proposed a sparse coding method for sharp image recovering, which assumes that the natural image patch can be sparsely represented by a over-complete dictionary. @cite applied sparse representation for estimate sharp image and blur kernel at the same time. @cite found that the minimum of their loss function in many existing methods do not correspond to their real sharp images,so @cite proposed a normalized sparsity prior to handle this problems. Michaeli and Irani @cite found that multiscale properties can also be used for blind deblurring problems, so Michaeli and Iranli @cite proposed self-similarity as image prior. @cite proposed low rank prior for both images and their gradients.
- Another commonly approach to estimate motion blur process is to maximize the marginal distribution Fergus @cite proposed a motion deblurring method based on Variational Bayes method. Levin @cite proposed a Expectation Maximization (EM) method to estimate blur process. The above two approaches does have some drawbacks: it is hard to optimize, time consuming and cannot handle high frequency features well.
- One of the novel deep learning techiques is Generative Adversarial Networks, usually known as GANs, introduced by Goodfellow @cite , and inspired by the zero-sum game in game theory proposed by Nash @cite which has achieved many excited results in image inpainting @cite , style transfer @cite @cite @cite , and it can even be used in other fields such as material science @cite . The system includes a generator and a discriminator. Generator tries to capture the latent real data distribution, and output a new data sample, while discriminator tries to discriminate the input data is from real data distribution or not. Both the generator and the discriminator can build based on Convolutional Neural Nets @cite , and trained based on the above ideas. Instead of input a random noise in origin generative adversarial nets @cite , conditional GAN @cite input random noise with discrete labels or even images @cite @cite take a step further, which based on conditional GAN and trained a cycle consistency objective, which gives more realistic images in image transfer tasks. Inspired by this idea, Isola @cite proposed one of the first image deblurring algorithms based on Generative Adversarial Nets @cite .
- Fog fades the color of observed objects and reduces their contrast. Extensive research has been conducted on image defogging (dehazing) to increase the visibility of foggy scenes @cite @cite @cite @cite @cite @cite @cite . Certain works focus particularly on enhancing foggy road scenes @cite @cite . Recent approaches also rely on trainable architectures @cite , which have evolved to end-to-end models @cite @cite . For a comprehensive overview of defogging dehazing algorithms, we point the reader to @cite @cite . Our work is complementary and focuses on semantic foggy scene understanding. This work investigates the usefulness of image dehazing for semantic scene understanding under fog.
- Using additional images as input for filtering a target image has been originally studied in settings where the target image has low photometric quality @cite @cite or low resolution @cite . Compared to the bilateral filtering formulation of these approaches, subsequent works propose alternative formulations, such as the guided filter @cite and mutual structure filtering @cite , for better incorporating the reference image into the filtering process. In comparison, we extend the classical cross-bilateral filter to a dual-reference cross-bilateral filter by accepting reference images, one of which is a discrete label image that helps our filter adhere to the semantics of the scene.
- Typical examples in this line include road and lane detection @cite , traffic light detection @cite , car and pedestrian detection @cite , and a dense, pixel-level segmentation of road scenes into most of the relevant semantic classes @cite @cite . While deep recognition networks have been developed @cite @cite @cite @cite @cite and large-scale datasets have been presented @cite @cite , that research mainly focused on clear weather. There is also a large body of work on fog detection @cite @cite @cite @cite . Classification of scenes into foggy and fog-free has been tackled as well @cite . In addition, visibility estimation has been extensively studied for both daytime @cite @cite @cite and nighttime @cite , in the context of assisted and autonomous driving. The closest of these works to ours is @cite , in which synthetic fog is generated and foggy images are segmented to and . Our work differs in that our semantic scene understanding task is more complex and we tackle the problem from a different route by learning jointly from synthetic fog and real fog.
- Our work bears resemblance to transfer learning and model adaptation. Model adaptation across weather conditions to semantically segment simple road scenes is studied in @cite . More recently, domain adversarial based approaches were proposed to adapt semantic segmentation models both at pixel level and feature level from simulated to real environments @cite @cite @cite @cite . Most of these works are based on adversarial domain adaptation. Our work is complementary to methods in this vein; we adapt the model parameters with carefully generated data, leading to an algorithm whose behavior is easy to understand and whose performance is more predictable. Combining our method and adversarial domain adaptation is a promising direction.
- A very recent work @cite on semantic nighttime image segmentation shows that real images captured at twilight are helpful for supervision transfer from daytime to nighttime via model adaptation. CMAda constitutes a more complex framework, since it leverages both synthetic foggy data and real foggy data jointly for adapting semantic segmentation models to dense fog, whereas the adaptation method in @cite uses solely real data for the adaptation. Moreover, the assignment of real foggy images to the correct target foggy domain through fog density estimation is another crucial and nontrivial component of CMAda and it is a prerequisite for using these real images as training data in the method. By contrast, the partition of the real dataset in @cite into subsets that correspond to different times of day from daytime to nighttime is trivially performed by using the time of capture of the images.
- It is well known that multi-task learning @cite of deep networks is effective for many computer vision tasks; @cite @cite @cite @cite @cite @cite to name a few. To enable MTL to work, there should arguably be some relation among the tasks jointly learned; in other words, there should be overlaps among the representations to be learned for those tasks. Combinations of tasks in the successful MTL examples include scene recognition and object recognition @cite ; depth estimation and scene parsing @cite ; facial expression recognition and landmark detection @cite ; vision and language @cite . However, it remains unknown if the same holds true for image restoration tasks. Although there should be some similarity among them, different types of degradations seem to be somewhat orthogonal with each other. In fact, the aforementioned studies on restoration from combined degradations attempt to deal with different degradation factors by adaptively selecting different networks @cite or different operations @cite depending on the degradation factors in input images.
- Attention mechanisms have been developed and employed to solve various computer vision problems @cite @cite @cite @cite @cite . Hu al proposed a squeeze-and-excitation (SE) block, which produces and applies channel-wise weights on an input tensor @cite . This block has been successfully applied to various tasks such as classification @cite , super resolution @cite and single-view depth estimation @cite . A number of later studies @cite @cite @cite @cite @cite aim at improving the SE-block. Woo al propose to use channel-wise and spatial attention weights @cite . Hu al study how to efficiently combine a SE-block and a ResNet module @cite . Gao al propose to use correlation of activations of each pair of channels to generate attention Sec_order_pool . Hu al improve SE-block by replacing global average pooling with a pooling operation with trainable parameters @cite .
- Previous monocular solutions @cite @cite @cite @cite @cite to spacecraft tracking and pose estimation rely on model-based approaches @cite that align a wireframe model of the object to an edge image (typically given by a Canny detector) of the real object based on heuristics. However objects are more than just a collection of edges and geometric primitives. Convolutional Neural Networks (CNNs) can learn more complex and meaningful features to the task at hand while ignoring background features (e.g. clouds) based on context.
- As described in section , SuperNeurons has been proposed by @cite . SuperNeurons uses the hybrid method of data-swapping and recomputing. Their method statically decides classification unlike PoocH. We have shown that PoocH enables faster executions using classification algorithm based on runtime profiling.
- Recent works in scene text recognition @cite @cite use convolutional neural networks and pre-processing techniques like adjusting orientations to predict characters from a scene text. @cite proposed an end-to-end framework of recognizing characters from a document by using LSTMs for character segmentation and CNNs for character classification.
- One of the advantages of using CNNs is that inputs can be unprocessed data such as raw pixels of an image, rather than extracting specific features @cite or pen stroke grid values. Connectionist Temporal Classification (CTC) removes the need to forcefully align the input stream with character prediction locations @cite . One of the major advantages of the CTC algorithm is that you do not need properly segmented labeled data. Although the CTC algorithm takes care of the alignment of input with the output labels, it can be finicky to train and increases compute complexity during inference.
- @cite proposed hybrid RNN-HMM for English offline handwriting recognition. They introduced a new variant of a LSTM memory cell by using a scalar multiple for every gate in each layer of the RNN. The scaling technique for LSTM gates reduced Character Error Rate (CER) by 0.3 @cite proposed Multidimensional RNN using dropout to improve offline handwriting recognition accuracy by 3 Dewan and Srinivasa @cite and @cite used CNNs for offline character recognition of Telugu and Chinese characters respectively. @cite used auto encoders, where the model was trained in a greedy layer wise fashion to learn weights in an unsupervised fashion, then fine-tuned with supervised data.
- Most widely used techniques for regularizing RNN are drop-out @cite and @math 2-regularization @cite on the trainable parameters of the networks. Recently, @cite proposed an RNN regularization technique taking into account caption generation task. They introduced an additional RNN, called @math (ARNet), which encourages the current hidden state vector of the decoder RNN to embed more information from the previous one. Introducing a discriminator network to generate multiple realistic future trajectories @cite @cite can be considered as the regularization on RNN. In this paper, we propose a regularization method based on inverse reinforcement learning framework. The proposed regularization approach differs from @cite @cite @cite in the following points. First, only the encoder RNN is regularized by a reward function so that the encoder RNN learns how to efficiently encode meaningful features extracted from inputs. Second, the reward function evaluates if the predicted positions are realistic along with the scene context information caused by the positions. In contrast, the discriminator networks in @cite @cite use only the past and predicted future trajectories for the evaluation.
- The desire to develop CNNs that are fast, power efficient, and can accommodate embedded platforms has intensified @cite after work @cite . We briefly review the related prior art that our work is inspired from.
- Most of the prior work on designing computation-efficient CNNs focus on reducing the size of a trained network in order to make the inference model smaller, faster, and less energy-intensive. Taking advantage of the fact that most CNNs have a considerable amount of redundant parameters, the research community has used quantization @cite and pruning @cite to perform network compression.
- On another front, researchers work to design compact CNNs in the first place instead of compressing existing models @cite .
- Several semi-supervised learning methods have been proposed and verified their effectiveness on natural images @cite @cite @cite @cite @cite @cite . @cite used box-level annotation to achieve similar performance with pixel-level annotation. @cite organized a reconstruction head to convert segmentation output back to original images. Generally, all the previous methods focus on training one model with complex auxiliary branches. On the contrary, our semi-supervised learning framework can organize the multiple models to support each other and no extra auxiliary branch is needed. There have been lots of automatic methods for pathology, including nuclei segmentation @cite @cite @cite and specific object @cite . However, automatic signet ring cell detection has not been investigated before to our best knowledge.
- Selecting appropriate biometric data to create key pairs in decentralized digital blockchain identity is another issue that must be considered. Researchers have investigated several biometric features in biometric based cryptographic key generations @cite .
- There are very few studies that integrate biometric traits into RSA keys. In his study, Je-Gyeong proposed a method for generating keys of digital signature (public and private key) from biometric. Some others investigated Iris texture as a biometric feature for generating cryptographic key. Rathgeb and Andreas proposed an approach using bits of the iris code for deriving biometric cryptographic key @cite . Janbandhu et.al derives signature keys from the code generated by using the 512 byte iris biometric data invented by the work done by J. Daugman @cite . Similarly, study by also considers the iris texture as biometric trait @cite . In the study by Sarkar et. al, biometric authentication was used for obtaining asymmetric cryptography keys @cite .
- proposed a method using users' voice as biometric trait @cite . Their system regenerates the key from the users voice by asking the user to repeat the same pass phrase. In the study by Chen and Chandran, the image of users face was used in biometric key generation @cite . The same face image is required for regeneration of the key in the future.
- In some studies, researchers explored applying more than one biometric traits instead of using only one biometric trait. proposed multimodal biometric system that generates a 256-bit secure cryptographic key using a combination of features from iris texture and minutiae points from the users finger prints @cite . In the study of , they propose multimodal approach of biometric. For instance they use Iris and Fingerprint, Speech and Signature, Face and Voice etc. In the study, Iris &fingerprint modalities are used and evaluated under FAR, FRR and accuracy @cite . Also, the study conducted by Yik-Herng proposes multi modal biometric systems that combine iris and fingerprint with IFO hash fusion method @cite . Iris trait is unique for each individual even for identical twins. Also, false acceptance rate (FAR), the rate of invalid matches, is lower than all other biometric traits like fingerprint and face. Voice trait is a composite of both behavioral and physical biometrics. Behavioral part differentiates in time due to the factors like medical conditions and age. In contrast to token or password-based systems, biometric matching does not work well every time due to the false matching or false mismatching.
- provided a review on biometric authentication technologies @cite . They found that fingerprint based systems had 2 The study of also stated that indexing structure of iris surpass indexing structure of fingerprint. According to their study, in multi-modal approach, the hit rate was improved up to 99.8 Smart card based biometric user authentication schemes have also been proposed @cite . The biometric data and keys are stored in a smart card for regeneration of keys in the future. However, smart card based approaches have portability issues as carrying physical card is an additional burden. Also if compromised, they pose security threats for biometric data. Fingerprint technology provides very accurate results @cite . Also, Jain et. al claimed that no biometric data is better than the other traits because all have own strengths and weaknesses, and performance of biometric data selection related with the type of application @cite . However, the matching accuracy of the fingerprint has been shown to be very high @cite .
- There are mainly three types of approaches for aspect and opinion term extraction: rule based approaches, topic modeling based approaches, and learning based approaches. A commonly used rule based approach is to extract aspect and opinion terms based on dependency parsing results @cite @cite . A rule in these approaches usually involves only up to three words in a sentence @cite , which limits its flexibility. It is also labor-intensive to design the rules manually. propose an algorithm to select some rules from a set of previously designed rules, so that the selected subset of rules can perform extraction more accurately. However, different from the rule mining algorithm used in our approach, it is unable to discover rules automatically.
- Our approach is also related to the use of weakly labeled data @cite , and is similar to the distant supervision approach used in relation extraction @cite .
- Perhaps the most related work is the model in @cite which uses a type of soft decision tree to mimic the input-output functions of a trained DNN. This soft decision tree produces hierarchical decisions, which is more easier to interpret than DNN that relies on hierarchical features. It is modeled based on hierarchical mixture of experts and trained with gradient descent. The way they design the soft decision tree is quite similar to @cite . Knowledge distillation was then used to improve the soft decision tree's accuracy. The difference between their approach and ours is that we use vanilla decision tree as the student model while their student model is the soft decision tree which has similar architecture with neural networks and could be more easily adapted to the original knowledge distillation framework.
- In the health care domain, two pipelines @cite @cite are proposed to distill the knowledge from a DNN to Gradient Boosting Trees (GBTs) @cite @cite . One of them extracts the logits from a learned DNN and uses the logits and the true labels of the original training data to train a logistic regression algorithm to obtain the soft prediction scores. The next step is to train GBTs with the original training data's features and the soft predictions. The second pipeline directly applies the soft prediction scores of the trained DNN on the original training data as targets for training a mimic model with GBTs. However, GBTs lack transparency as they rely on post-hoc determinations: partial dependence @cite , which would result in bias in this process @cite . The differences between their approach and ours are apparent. The strategy we applied when training the mimic model is matching logits, not the soft targets. Also, our student model is decision tree.
- Another approach that distills neural networks into GBTs is in @cite . They tried two student models: tree-based generalized additive models (GA2Ms) @cite @cite @cite and GBTs. The teacher model they adopted is multilayer perceptrons. For the student model's training process, they applied the method of matching logits instead of soft targets in @cite @cite . They investigated both classification and regression problems. However, their model is limited to the binary class problems and their results are not conclusive yet and not published. Compared to their method, our teacher model is DNN and the student model is decision tree. We aims at multi-class classification problems.
- A most recent work @cite combines knowledge distillation and dimension reduction to visualize the results of deep classifiers. They pointed out that the method: t-distributed stochastic neighbor embedding (t-SNE) @cite commonly used for visualizing the activations of hidden layers was problematic. They propose to visualize the data points that are assigned similar probability vectors to give practitioners a sense of how the decisions are made on test cases. They train a simpler and more interpretable classifier using the soft targets generated by a deep classifier. The student model they applied is Naive Bayes.
- Regarding type systems, languages, and other means to validate and verify network programs, we've already mentioned NetKat @cite earlier in this work, and there have been many follow-up pieces regarding the language, including probabilistic variants @cite . Just recently, p4v @cite was published and is motivated by real-world examples; it attempts to find bugs in P4 programs and verify program properties by incorporating domain-specific assumptions into a constraint solver.
- Smart Grids and demand response programs are often considered to be technical in nature. However, recent research has emphasised the socio-technical aspects of Smart Grids @cite , and how demand response programs can be designed to be more bottom-up, and user-centred. It is argued that user flexibility does not only depend on the appliances and monetary incentives @cite ; but additionally it depends on individual characteristics and behaviour @cite , conventions @cite , and social practices involving the appliances @cite . However, capturing such information based on historical data can be very challenging. Thus, this paper utilises appliance-level day-ahead scheduling via a personalised scheduling agent for each user. Using this scheduling agent, users directly determine their schedules, flexibility, usage preference, scheduling constraints, and comfort vs. reliability preference, on a daily basis. While this approach does require a higher level of engagement from users, recent research has shown that values such as increased control and autonomy can improve users' acceptance and participation in such programs @cite @cite @cite .
- Finally, the complexity of coordination and optimal selection of users' schedules is combinatorial (NP-Hard) and computationally expensive @cite . Distributed optimisation and control algorithms have been utilised in Smart Grids and demand response program to cope with this complexity. Such algorithms approximate a near-optimal solution between users' demand and the supply requirements of the utility company @cite . By utilising a distributed algorithm, the demand response program can better address the increasing penetration of distributed energy resources. It can also expand the possible solution space, improve the privacy of users, and provide more resilience against adversarial users @cite . This paper utilises and expands I-EPOS to provide a distributed, privacy-preserving coordination and optimisation scheme for users' schedules @cite . I-EPOS has been used in various sharing economy scenarios such as energy management @cite @cite @cite , bike sharing @cite , and charging control of electric vehicles @cite .
- Early methods rely on hand-crafted detection and description : SIFT @cite detects 3D spatial-scale keypoints on difference of gaussians and describes them with a 3D Histogram Of Gradients (HOG). SURF @cite uses image integral to speed up the previous detection and uses a sum of Haar wavelet responses for description. KAZE @cite extends the previous multi-scale approach by detecting features in non-linear scale spaces instead of the classic Gaussian ones. ORB @cite combines the FAST @cite detection, the BRIEF @cite description, and improves them to make the pipeline scale and rotation invariant. MSER-based detector hand-crafts desired invariance properties for keypoints, and designs a fast algorithm to detect them @cite . Even though these hand-crafted methods have proven to be successful and to reach state-of-the-art performance for some applications, recent research focus on learning-based methods.
- One of the first learned detector is TILDE @cite , trained under drastic changes of light and weather on the Webcam dataset. They use supervision to learn saliency maps which maxima are keypoint locations. Ground-truth saliency maps are generated with good keypoints': they use SIFT and filter out keypoints that are not repeated in more than 100 images. One drawback of this method is the need for supervision that relies on another detector. However, there is no universal explicit definition of what a good keypoint is. This lack of specification inspires Quad-Networks @cite to adopt an unsupervised approach: they train a neural network to rank keypoints according to their robustness to random hand-crafted transformations. They keep the top bottom quantile of the ranking as keypoints. ELF is similar in that it does not requires supervision but differs in that it does not need to further train the CNN.
- The simple descriptor introduced for the sake of the matchability evaluation is taken from UCN @cite . Given a feature map and the keypoints to describe, it interpolates the feature map on the keypoints location. Using a trained CNN for feature description is one of the early applications of CNN @cite . Later, research has taken on specifically training the CNN to generate features suitable for keypoint matching either with patch-based approaches, among which @cite @cite @cite @cite , or image-based approaches @cite @cite . We choose the description method from UCN @cite , also used by SuperPoint, for its complexity is only @math compared to patch-based approaches that are @math with @math the number of keypoints. We favor UCN to InLoc @cite as it is simpler to compute. The motivation here is only to get a simple descriptor easy to integrate with all detectors for fair comparison of the matching performances. So we overlook the description performance.
- @cite @cite used deep neural networks to construct nonlinear state space models and leveraged a structured variational approximation parameterized by recurrent neural networks to mimic the posterior distribution. @cite reformulated RL and control problems using probabilistic inference, which allows us to bring to bear a large pool of approximate inference methods, and flexibly extend the models. @cite @cite exploited continuous state-space models and deep RL to obtain improved treatment policies for septic patients from observational data. @cite discussed problems when evaluating RL algorithms in observational health setting. However, all the works mentioned above do not take into account confounders in the proposed models.
- @cite attempted to learn individual-level causal effects from observational data in the non-temporal setting. They also used a variational auto-encoder to estimate the unknown confounder given a causal graph. @cite developed predictive models based on electronic medical records without using causal inference. @cite proposed a nonparametric Bayesian method to analyze clinical temporal data. @cite represented the treatment response curves using linear time-invariant dynamical systems. This provides a flexible approach to modeling response over time. Although the latter two works model sequential data, they both do not consider RL or causal inference.
- @cite considered the problem of bandits with unobserved confounders. @cite and @cite further studied contextual bandits with latent confounders. @cite circumvented some problems caused by unobserved confounders in multi-armed bandits by using counterfactual-based decision making. @cite leveraged causal inference to tackle the problem of transferring knowledge across bandit agents. Unlike our method, all these works are restricted to bandits, which corresponds to a simplified RL setting without state transitions.
- PVC @cite is originally designed for two-view data in the missing view setting. It learns a full representation from incomplete multi-view data based on the NMF. Denote @math as the examples presented in both views, and denote @math , @math as the examples only presented in the first view and the second view, respectively. The optimization problems of PVC is From codes (http: lamda.nju.edu.cn code .ashx) published by the authors, it can be known that the @math -norm in PVC @cite is actually the @math -norm of matrix. where @math @math is the latent representation for the @math -th view, and @math is the basis matrix, @math is the dimension of the latent space, and @math is the trade-off parameter for the regularization terms.
- Based on multi-view matrix completion, MVL-IV tends to recover the incomplete multi-view data @math by exploring the connection among multiple views @cite . Denote the reconstructed data matrices as @math , the formulation of MVL-IV is where @math is the basis matrix, and @math is the full representation matrix. MVL-IV is able to cope with the complex incomplete-view setting with both missing views and missing variables.
- The Incomplete Multi-modality Grouping (IMG) approach can be regarded as an enhanced version of PVC. Differently, IMG gets rid of the nonnegative constraint and considers the global structure in the latent space @cite . Using the same notations with PVC, the latent representation of all samples can be denoted as @math . With a Laplacian graph regularization (LGR) to capture the global structure, the objective function of IMG is where @math is the Laplacian matrix of similarity matrix @math , @math , @math and @math are positive parameters.
- Although our work does not fall directly under the umbrellaof transfer learning, it is similar to the transfer learningmethods in deep learning. At the domain of deep neuralnetworks for image classification, haveshown the benefits speed up of learning features from existingmodels when the datasets are similar @cite . In this work, we structured a human demonstrationconvolution network @cite , then used thepre-trained model as source, and the CNN model was thenused to initialize the RL agent's network.
- Existing research on pre-training in RL @cite @cite has shown improvementwhen using a pre-trained model on similar datasets. The capabilityof these studies were limited by the small numberof parameters learned and by the state input. In our work,we used the raw images of simulation driving domain asnetwork input from the human driving demonstration. It isworth nothing that the pre-training model needs to learn thefeatures of states as well as policy.
- Few references in cyber security utilize natural language processing. Neuhaus and Zimmermann @cite analyze the trend of vulnerability by applying latent Dirichlet allocation to vulnerability description. @cite put forward a system to automatically extract IOC items from blog posts. @cite proposed a system that automatically extracted threat actions from unstructured threat intelligence reports by utilizing a pre-defined ontology. A concurrent work by @cite automatically extracted IOC data from security technical articles and further categorized them into different stages of malicious campaigns. All of those systems consist of several components that rely heavily on manually defined rules, while our proposed model is an end-to-end model using word embedding and token features as input, which is more general and applicable to a broader area.
- In this section, we briefly discuss the related work in the areas of networks embedding and graph compression. Previous researchers consider the graph embedding as a dimensionality reduction @cite such as PCA @cite that captures linear structural information and LE (locally linear embeddings) @cite that preserves the global structure of non-linear manifolds. While these methods are effective on small graphs, scalability is the major concern for them to be applied on large-scale networks with billions of vertices, since the time complexity of these methods is at least quadratic in the number of graph vertices @cite @cite . On the other hand, recent approaches in graph representation learning focus on the scalable methods that use matrix factorization @cite or neural networks @cite @cite @cite . Many of these aim to preserve the first and second order proximity as local neighborhood with path sampling using short random walks such as and @cite @cite @cite @cite . Some studies use network embedding on node and graph classification @cite @cite @cite , some of them use it on graph clustering @cite @cite @cite .
- Although recent network embedding methods have a promising performance on the effectiveness of various applications, there are still some challenges since real-world graphs are massive in scale and this may obstruct the direct application of existing methods. On the other hand, when we consider a compressed or summary graph conserving the key structure and patterns of the original graph, many methods would be applicable to large graphs @cite .
- Log analysis, consisting of log collection, log parsing and log mining @cite , can effectively identify system problems @cite @cite .
- The log parser algorithms have been widely studied in recent years, but there still lacks evaluation methods for multiple algorithm cooperation. In @cite , the authors give four log parser algorithms (SLCT, IPLoM, LKE and LogSig) an evaluation on their effectiveness and accuracy. Their experiment codes and log datasets are publicly available. In @cite , they have made a further research on log parser algorithms evaluation of 13 log parser algorithms on a total of 16 log datasets.
- . As an important branch of log analysis, anomaly detection is widely used in large-scaled distributed systems. It has also formed a complete analysis process including log collection, log parsing, feature extraction and machine learning. Based on different design techniques, anomaly detection models are mainly divided into supervised models and unsupervised models. In order to select an efficient and accurate anomaly detection in practical applications, @cite provide a detailed review and evaluation of 6 state-of-art log anomaly detection methods.
- . As described in @cite , the authors propose the theory of conformal prediction that uses past experience to determine precise level of confidence in new prediction. Conformal prediction uses a non-conformal measure to calculate the non-conformity score. We can obtain a pvalue that can be used to make decisions or evaluation, whose outputs is a prediction set with fixed confidence level. @cite , the authors use conformal prediction to detect deviation of data sequence under the assumption that data are independent and identically distributed. Conformal evaluation uses the theory of conformal prediction to measure the non-conformity of a test object to a class compared to all other objects. By using conformal evaluation, the authors detect concept drift and identify aging classification in malware classification models @cite . Conformal evaluation provides better understanding for model quality.
- . Recently, blockchain has attracted attentions for its accomplishment in cryptocurrencies and distributed applications. There are many studies on the combination of blockchain and IoT to provide a reliable solution in IIoT systems. In @cite , the researchers use smart contracts in an industrial production audit system. As consortium blockchain uses many security mechanisms such as identity and member service provide, it can be used for secure energy trading in industrial internet of things.
- Many early prominent papers using attention mechanisms in NLP initially used the term alignment to refer to attention in the context neural machine translation. One of the most foundational of these is the 2014 Bahdanau, Cho and Bengio collaboration in @cite . In @cite , Bahdanau shows that a single neural network can be jointly tuned to maximise translation performance using attention-based encoder-decoders. In parallel, Graves, Wayne and Danihelka showed in @cite that neural networks (in particular LSTMs) can be improved by coupling them to attention processes. While the application in @cite was helping Neural Turing Machines infer copying, sorting and recall, subsequent CV and NLP applications of the very same concept were greatly inspired by techniques presented in this paper @cite . Building on the work above as well as on their own work, in 2015 Firat, Cho and Bengio in @cite had a breakthrough in multilingual Neural Machine Translation (NMT). They used single attention mechanisms shared across language pairs to outperform existing NMT benchmarks.
- Since then, a whole flurry of different types of attention have been developed including but not limited to self-attention, shared attention, local attention, hybrid attention and multi-headed attention. Indeed, the multi-headed attention model presented in the Transformer @cite by Google Brain and UofT alumni has been widely lauded as a promising novel architecture for dealing with long range dependencies without the issues of LSTMs in NMT. For this reason, that paper will serve as inspiration for our investigation on the potential of attention in FTS forecasting.
- There are also other HLS-based software simulators. The LegUp HLS @cite simulator provides speedup prediction based on the profiling of the source code and the execution cycle from its synthesis result. HLScope+ @cite describes a method to extract cycle information that is hidden by HLS abstraction and uses Vivado HLS C simulation to predict the performance for applications with dynamic behavior. These works, however, do not guarantee cycle-accuracy.
- Deep learning is successfully applied in many applications of the medical image analysis domain. Tasks related to features segmentation @cite and feature classification @cite can be performed using both traditional computer vision and deep learning. Deep learning recently captured momentum to outperform traditional features based methods used in the medical image domain. Starting from retinal analysis to cancer analysis, deep learning proved its usefulness as screening and diagnostic tools for doctors. In the eye @cite and X-ray @cite screening scenarios deep learning outperforms human practitioners.
- Deep learning is also widely used in OCT image analysis task. A deep learning based OCT image segmentation method @cite proposed pathological lesions segmentation using a fully convolutional method. Intraretinal cystoid fluid and subretinal fluid detection deep learning method @cite using OCT was successfully tested with high accuracy. In case patient referral recommendation for sight-threatening retinal diseases using OCT images @cite , this deep learning method outperforms expert ophthalmologist.
- Deep learning and convolutional neural network have become popular topics in medical image processing. There are already a lot of research works that apply deep learning to medical image processing. In terms of disease classification, there are studies on breast cancer image classification @cite @cite , lung pattern classification @cite , Alzheimers disease classification @cite . In the detection of lesion targets and diseases, there are cancerous tissue recognition @cite , detecting cardiovascular disease @cite , and melanoma recognition @cite . In the segmentation of organs and substructures, there are studies on skin lesion segmentation @cite , microvasculature segmentation of arterioles @cite , tumor segmentation @cite . In addition, there are many other applications, such as studies of visual attention of patients with Dementia @cite , diagnosis of cirrhosis stage @cite , constructing brain maps @cite .
- Action recognition is one of the core components of video understanding. In early works, 3D motion templates @cite , or features such as SIFT-3D @cite were used for representing temporal information for action recognition. Later, the introduction of dense trajectories @cite and improved dense trajectories @cite provided a significant boost in performance for feature based pipelines.
- After the early success of deep learning @cite @cite @cite on image classification, early deep learning based video recognition methods focused on utilizing deep features. @cite evaluated different fusion methods for deep features. @cite pooled deep-learned features based on trajectory constraints. @cite proposed a two-stream network which learns temporal dynamics on both stacked optical flow and appearance. @cite further analyzed different ways to fuse two-stream networks. @cite further improved the two-stream network by introducing a temporal sampling strategy and training on video-level instead of short snippets to enable efficient learning on full videos.
- Meanwhile, researchers started to design novel deep architectures specifically for video tasks. @cite first proposed to utilize 3D convolution, which naturally extends convolutional filters to temporal domain. Then, @cite proposed to build a 3D convolution variant of inception architecture and trained on a newly collected large-scale dataset of actions. This network achieved impressive performances on multiple benchmarks but was computationally costly. Very recently, @cite proposed P3D to simplify 3D convolutions with 2D convolutional filters in the spatial domain followed by 1D convolutional filters in the temporal domain. This method is most relevant to proposed work. We also decompose 3D convolutions into separate spatial and temporal convolutions. However, unlike P3D, which replaces 3D convolution with 2D + 1D convolutions, we design a dedicated temporal aggregation block to capture context among multiple temporal levels and meanwhile preserving the temporal resolution. We only place a single layer of temporal block after each downsampling layer to reduce the filters needed for performing video recognition tasks. By stacking these two types of blocks repeatedly, we effectively capture appearance and spatio-temporal motion pattern which is important for localization tasks in videos.
- Multi-label dense action recognition is a considerably harder task than action recognition as it requires labeling frames with all the actions occurring in them. Yeung @cite first proposed this task by densely labeling every frame within a popular sports action dataset @cite . They further applied standard techniques such as LSTM, two-steam networks and created a very strong baseline. @cite collected another multi-label dataset by crowd-souring everyday activities at home. For recognizing these actions, @cite proposed ActionVLAD, that aggregates appearance and motion features from two-stream networks. @cite used a fully-connected temporal CRF model for reasoning over various aspects of activities. @cite employed RNNs to sequentially make top-down predictions and later then corrected them by bottom-up observations. Most recently, @cite performed a detailed analysis on what kinds of information are needed to achieve substantial gains for activity understanding among objects, verbs, intent, and sequential reasoning.
- Unlike action recognition where we predict a label per video, action detection requires predicting temporal boundaries of an action in an untrimmed video. Recent methods mainly focus on two types of approaches: making dense predictions or predict temporal boundaries (like proposals). @cite encoded a stream of frames into a single feature vector using an LSTM which was then used to rank proposals. @cite used a bi-directional LSTM on multi-stream features and performed fine-grained action detection. @cite utilized multiple temporal scales to rank candidate segments. Later, they @cite refined this work by designing a convolution de-convolution network to generate dense predictions for refining action boundaries. @cite decomposed their model into separate classifiers for classifying actions and determining completeness. Meanwhile, due to success in object detection for localizing objects in images, recent works started applying similar ideas to videos. @cite presented a R-CNN like action detection framework from C3D features. @cite argued for the importance of sampling at two temporal scales to capture temporal context in a faster R-CNN architecture.
- Adding intents is recognized as the main bottleneck in scaling--adding functionality to--conversational agents @cite . Recent work focuses on two directions to improve the process. One is on bootstrapping--reusing available chat log data @cite --to rapidly expand the understanding and responding capabilities of agents. The other is to allow domain experts, who need not to be machine learning experts, to build intent models by working on model definition, labeling, and evaluation through user-friendly interfaces @cite . Our work targets both directions by enabling an experts-in-the-loop approach to bootstrap intents.
- The idea of using search to explicitly acquire training examples is relevant to guided learning @cite , an approach to reduce labeling effort under skewed classes, where the search approach is proved to be superior over labeling from uniform sampling and active learning. This is critical for the use case of bootstrapping conversational agents, as intents are fine-grained concepts and usually highly skewed in chat logs. We note that the same search interface can be used for guided learning, but our approach differs in that it takes the user's search queries to automatically expand the labeled set, and thus requires minimum labeling effort from the user. A contribution of our user study is to empirically compare the effectiveness of guided learning and our SLP framework.
- An Autoencoder (AE) is trained to reconstruct its input @math from a learned representation @math @cite . It consists of two parts, an encoder @math , which encodes the input @math to a learned feature representation @math , and a decoder @math which attempts to recapture the original input by decoding the representation. For a deep convolutional AE the encoder @math and decoder @math each are modeled as deep-convolutional networks with parameters @math and @math , respectively. Hence, the training of a deep AE can be formalized as: A common choice for the reconstruction error @math is the mean-squared error (MSE): @math To reconstruct the image truthfully, the encoder has to encode the information of the input into the feature vector @math . To learn more suitable representations @math different variations of AE have been proposed @cite @cite :
- Researchers have utilized temporal dependencies in video sequences over the last few years. Romero al @cite advocated a two-stream CNN model that combines optical flow and RGB information, and their result was promising. However, they used one binary classification model for each AU, which caused their approach to be time consuming to train and yield numerous model parameters. The CNN and LSTM hybrid network architectures are studied in Chu al @cite , Wei Li al @cite and He al @cite , which feed the CNN-produced features to LSTM to improve performance by capturing the temporal relationship across frames. However, their solutions are inefficient because they are not an end-to-end networks. In our experiments, we also investigate the effects of using temporal feature relationships in the time axis of videos. We use various dynamic models (including two-stream network, ConvLSTM ) that are incorporated into AU R-CNN. Such temporal dependency cannot always improve performance in all cases (Section ).
- Non-SGD methods for deep learning were also studied in the recent years. proposed the Alternating Direction Method of Multipliers (ADMM) to transform a fully-connected neural network into an equality-constrained problem to solve @cite . @cite handled deep supervised hashing (VDSH) problem by an ADMM algorithm to overcome vanishing gradients. Carreira and Wang proposed a method of auxiliary coordinates (MAC) to replace a nested neural network with a constrained problem without nesting @cite .
- A way to cope with cast shadows is to model illumination as a set of candidates, either point sources @cite or directional sources @cite . Sato al @cite propose to estimate the illumination distribution by an adaptive sampling of the directional sources. They implicitly make use of shadows by incorporating a shadow term in their Bidirectional Reflectance Distribution Function (BRDF), computed from the geometry and the potential directional light candidates. In @cite , Jiddi al approximate a set of point light sources equally distributed around the scene, and select the candidates whose cast shadows correlate with a binary mask of shadows preliminary detected in the image. Our method differs from @cite and @cite in that we perform a continuous optimization, and we do not approximate a set of candidates. Hence there is no need to compute a set of shadow maps prior to the optimization: a shadow map is rendered at each iteration given the current illumination estimate.
- From a machine learning perspective, state-of-the-art techniques aim to estimate illumination (either indoor @cite or outdoor @cite ) by learning the weights of a neural network. To train such network, a supervised strategy is often adopted: the loss function is the error between some ground truth illumination and the illumination computed by the network fed with an example image. Rematas al @cite learn to estimate an intermediate representation that mixes illumination and a single material, called a "reflectance map". In @cite this architecture is combined with a CNN that decomposes the reflectance map into an environment map and a single material. In @cite a parametric model of illumination and material is fitted to a reflectance map, via a light transport operator that is approximated by two neural networks preliminary trained on synthetic data. Mandl al @cite try to estimate the illumination only, by training a CNN for every camera pose sampled around an object used as a light probe.
- There is a large and growing literature on false discoveries in multiple testing, aimed at solving a range of problems, often addressing issues of scientific reproducibility in research @cite . Here we focus on work whose methods or objectives have the most overlap with ours. In particular, we focus on literature on online'' methods in multiple testing, and compare and contrast those solutions to the ones we propose. The most salient difference is that we address the general problem of asynchrony; when there is no asynchrony, our approach recovers a slew of existing methods, including work by , , , .
- Our work is much inspired by the classical work on runtime repair. @cite present a technique called failure oblivious computing'' to avoid illegal memory accesses by adding additional code around each memory operation during the compilation process. Assure @cite is a self-healing system based on error-virtualization. @cite proposes the concept of recovery shepherding'' in a system called RCV. Those techniques do not produce patches and do not perform regression testing in production.
- @cite presents Ares a runtime error recovery for Java exceptions using JavaPathFinder (JPF). The two majors differences with are: first is safer, it does not modify the production state of the application as Ares does, and secondly, while Ares performs runtime repair, produces source code patches that are then communicated to the developers.
- @cite propose ClearView, a system for automatically repairing errors in production. and ClearView both perform repair in production, yet they are very different: 1) ClearView does not produce source code patches while Itzal does; 2) ClearView modifies the production state, while Itzal only modifies the sandboxed shadow requests and state (this means that ClearView can mess up the application while Itzal never does so); 3) ClearView works with learned invariant-based oracles, while Itzal uses human designed request oracles.
- The concept of shadow traffic is related to the execution of multiple versions of the same software in parallel, called in the literature multi-version execution'' @cite , or parallel execution'' @cite . However, none of the related work uses shadow traffic to generate patches.
- Holistic methods: Holistic methods extract features from the human in the given bounding box and combine them with contextual features from the whole image to predict human actions @cite @cite @cite . Early works @cite @cite use a graphical model on the human body pose to infer actions. Recently, Mallya and Lazebnik @cite propose a simple fusion network that concatenates features extracted from a bounding box with features from the whole image for action prediction. Overall, holistic methods follow the most straightforward strategy and do not involve many pre-processing steps. However, holistic methods can be easily misled by the presence of irrelevant objects or backgrounds. To resolve this problem, our approach introduces a human-mask loss to guide the activations of the network into human regions, and hence suppresses the response of irrelevant contexts.
- Part-based methods: Part-based approaches detect multiple bounding boxes on various body parts and combine their features with global features to predict actions @cite @cite @cite . @cite train body part detectors on 'pool5' features in a sliding window manner and combine them with the ground-truth box to train a CNN for action classification. Recently, @cite incorporate mid-level body part actions (e.g. head: laughing) to infer body actions. However, this method requires an external human pose estimation technique to localize body keypoints and crop out part patches in both training and testing stages. Moreover, the "hard-coded attention" limits the regions to be around the human. Instead of using body parts' patches as input, our approach learns rich representations of humans by using our human-mask loss.
- Context-based methods: Contextual algorithms exploit contextual cues, such as interactive objects. CAI @cite utilizes language information of the context (i.e. subject and object) labels, and encodes them into semantic space to learn context-dependent classifier for visual relationship detection. R*CNN @cite applies selective search @cite to generate object proposals to discover proper interactive objects. However, these proposals are required for both training and testing stages, and the sampling over potential proposals might also be computationally expensive. Moreover, R*CNN uses two hyper-parameters to define the overlap between the person bounding box and the proposal box. Our approach achieves this overlapping by introducing a human-mask loss, which can automatically divert the attention into the most discriminative image regions around the human, in a soft and learnable way.
- Weakly-supervised localization: All the aforementioned methods require the prior knowledge of the ground-truth bounding boxes in both training and testing stages, making them difficult to scale to real-world applications. There have been a number of recent works exploring weakly-supervised object localization or soft attention @cite @cite @cite . @cite transfer mid-level image representations obtained from image classification to action recognition. @cite generate a foreground action mask using a five-step iterative optimization method, then extract features from the action mask for recognition purpose. However, this method suffers from high optimization complexity. Recently, Girdhar and Ramanan @cite propose a pooling method that scales the score map with a saliency map. This method potentially assumes that the salient objects are the most useful cues for identifying actions. However, there could be salient but irrelevant objects (see Fig. ) that can lead to wrong predictions. Our approach implicitly models attention via a number of feature activation maps. We show that it is unnecessary to explicitly model the attention map, but by training the network to predict the human location heatmap. Doing this, we implicitly divert the attention from the misleading contexts to the human regions.
- The task of Sketch-Based Image Retrieval (SBIR) aims at retrieving the images that are of similar semantic meaning as the query sketch. A typical solution is to learn a shared embedding space for both sketches and images. Such a common space facilitates the ranking of similarity of sketches and images. Previous methods @cite @cite @cite @cite employed the hand-craft sketch features to represent the sketches. Recent deep learning based architectures enable the cross-domain learning in an end-to-end manner @cite @cite @cite @cite . To accelerate the retrieval in a large-scale dataset, hashing based models @cite @cite @cite @cite have also been studied.
- Representation learning is studied in computer vision community @cite @cite @cite @cite @cite @cite ; but the sketch-based representation learning is relatively less studies @cite @cite . Among all the different deep architectures have been investigated and studied, such as ResNet, ResNeXt, and DenseNet. Among them, ResNeXt @cite incorporated both the residual learning and group convolution; and thus it is adopted as the building structure in this paper. The recent SE module @cite made networks capable of choosing relatively important channels with feature maps, and thus it is also used here. Remarkably, to effectively embed cross domain features, several hashing methods @cite @cite @cite @cite @cite @cite and cross-modal embedding methods @cite @cite @cite @cite have been studied.
- Many metric learning based methods @cite @cite @cite @cite proposed learning deep features by the loss functions of Euclidean distance. In order to make the learned feature more discriminative, other variants of loss functions have been investigated recently, such as contrastive loss @cite @cite and triplet loss @cite , L-softmax @cite , A-softmax @cite and LMCL @cite losses. Particularly, the contrastive and triplet losses aim at increasing the Euclidean distance margin, while L-softmax, A-softmax, and LMCL losses are designed to expand the angular margin. Remarkably, the latter three losses make simple modification on softmax loss and greatly improve the performance on face recognition tasks. Furthermore, prototypical loss @cite is also a variant of softmax which incorporates the Euclidean distance.
- @cite propose a system for extracting field information from administrative documents using a document model for extracting structural relationships between words. As the system processes more documents, it refines its structural model. More recently, @cite proposed a technique to extract data from colored documents by extracting rectangles and modeling the topological relationship between them as a graph. The technique was generalized to extraction from new cases based on topological and context similarity. An Entity Recognition model was proposed in @cite , using geometrical relationships between entities. @cite propose a system for automatic separation of static and variable content in Administrative Document Images using a probabilistic model for determining whether a pixel is static or dynamic.
- A relevant problem for extraction is that of document template identification for which several techniques have been proposed. Bruel developed a matching technique that was more robust to noise in the template instance @cite , and subsequently to capture the layout of a complex document @cite . In [9], the authors generate a hierarchical representation of documents and templates using XY-trees and utilize tree edit distance to identify the correct template for a new instance. Hamza @cite utilize a bottom-up aproach which involves keyword extraction followed by clustering to generate high level structures. The document is represented as a graph with these high level structures as nodes and edges indicative of spatial relationships. Documents are mapped via graph probing, which is an approximation of graph edit distance. @cite have developed an invoice reading system, where documents are again represented by a graph but nodes are split into key-nodes and data-nodes, corresponding to keywords and values (numbers, dates etc). Graph probing is used to find the correct template. Extraction is done by examining the nearby words of every data-node.
- The advent of sequence to sequence models has allowed for significant advancement in several text modeling tasks and these models play a crucial role in several state of the art conversational systems @cite . DeepReader utilizes a sequence to sequence model to provide users with the ability to query the system via a natural language conversational interface. The interface builds on prior work that utilizes seq2seq models for learning an NL to SQL query mapping @cite The motivation is to allow non-technical users to efficiently utilize the system for efficient data extraction from images.
- Academic citation recommendation methods can be classified based on the input that the recommendation system takes. There have been an interest in methods that recommend academic papers based on an existing manuscript, typically taking an existing manuscript as an input to annotate @cite @cite . The usage of metadata, such as authors or keywords, have been considered to build discriminative models @cite @cite @cite @cite . The problem of the technical vocabulary changing depending on communities or application area has spawned works to bridge the gap between two heterogeneous languages @cite @cite . The effects of modeling a researcher's past works and exploiting potential citation papers in recommending papers are also examined @cite @cite . Most notably this stream of research as generated a tool called RefSeer http: refseer.ist.psu.edu @cite which suggests recommendations on a paragraph or manuscript using topic-based global recommendations and citation-context-based local recommendations.
- The graph embedding methods that we will present use sampling method inspired by the classic seed papers based citation recommendation PaperRank @cite and Collaborative Filtering @cite . And we will also evaluate the recommendation results our graph embedding methods obtain against these two baselines.
- This work is also credited to recent development on language model @cite @cite @cite @cite and graph embedding @cite @cite @cite @cite @cite @cite @cite @cite @cite which represent words or vertices as vectors in a low dimensional space. On the one hand, in the language model, each word is represented by a vector which is concatenated or averaged with other word vectors in a context, and the resulting vector is used to predict other words in the context. For example, the neural network language model proposed by @cite uses the concatenation of several previous word vectors to form the input of a neural network, and tries to predict the next word. The outcome is that after the model is trained, the word vectors are mapped into a vector space such that semantically similar words have similar vector representations.
- On the other hand, the problem of graph embedding is related to two traditional research problems, i.e., graph analysis and representation learning. Particularly, graph embedding aims to represent a graph as low dimensional vectors while the graph structures are preserved. Graph analysis aims to mine useful information from graph data. And representation learning obtains data representations that make it easier to extract useful information when building classifiers or other predictors. Graph embedding lies in the overlap of the two problems and focuses on learning the low-dimensional representations. Recently, deep learning (unsupervised feature learning) techniques, which have proven successful in natural language processing, has been introduced for graph analysis. For example, DeepWalk @cite learns social representations of a graph's vertices, by modeling a stream of short random walks. Social representations are latent features of the vertices that capture neighborhood similarity and community membership. These latent representations encode social relations in a continuous vector space with a relatively small number of dimensions.
- Since then, Random walk sampling has become the most popular neighborhood construction strategy for graph embedding. Node2vec @cite extends the DeepWalk by leveraging breadth first sampling and depth first sampling and Metapath2vec @cite utilizes the random walk sampling on heterogeneous graphs. Efforts have been made to bring the idea the citation recommendation related field. @cite explore cross language citation recommendation by guiding the random walk streams. Paper2vec @cite extends the edges in citation graph based on textual similarities, then adopts the random walk sampling to learn the representations of papers in the graph. In this paper, we propose a novel strategy for neighborhood construction for citation recommendation on graph.
- 1) DGD-Based Algorithms for Network Consensus: Network consensus optimization can trace its roots to the seminal work by Tsitsiklis @cite , where the system model and the analysis framework were first developed. As mentioned earlier, a well-known method for solving network consensus optimization is the distributed (sub)gradient descent (DGD) method, which was proposed by Nedic and Ozdaglar in @cite . DGD was recently reexamined in @cite by Yuan using a new Lyapunov technique, which offers further mathematical understanding of its convergence performance. In their follow-up work @cite , the convergence behavior of DGD was further analyzed for non-convex problems. Recently, several DGD variants have been proposed to enhance the convergence performance (e.g., achieving the same @math convergence rate with constant step-size @cite or even under time-varying network graphs @cite ).
- Earlier work concerned with speech recognition from ultrasound data has mostly been focused on speaker-dependent systems @cite @cite @cite @cite . An exception is the work of @cite , which investigates the classification of tongue gestures from ultrasound data using convolutional neural networks. Some results are presented for a speaker-independent system, although the investigation is limited to two speakers generalizing to a third. @cite present a method for automatic tongue contour extraction from ultrasound data. The system is evaluated in a speaker-independent way by training on data from eight speakers and evaluating on a single held out speaker. In both of these studies, a large drop in accuracy was observed when using speaker-independent systems in comparison to speaker-dependent systems. Our investigation differs from previous work in that we focus on child speech while using a larger number of speakers (58 children). Additionally, we use cross-validation to evaluate the performance of speaker-independent systems across all speakers, rather than using a small held out subset.
- is one of the fundamental problems in computer vision. Early approaches relied on hand-crafted features @cite that track pixels over time and then aggregated their motion statistics into compact video descriptors. With the onset of deep learning these methods have been outperformed by two-stream networks @cite that take both raw images and optical flow fields as input to CNNs @cite , which are trained end-to-end on large datasets. These methods are limited by the 2D nature of CNN representations. This limitation has been addressed by @cite who extended CNN filters to the temporal dimension resulting in 3D convolutional networks. More recently, Carreira and Zisserman @cite have integrated 3D convolutions into a state-of-the-art 2D CNN architecture @cite , resulting in Inflated 3D ConvNet (I3D). @cite , have extended this architecture with non-local blocks that facilitate fine-grained action recognition. We use an I3D with non-local blocks as the video feature representation in our model.
- is a key component of most of the action detection frameworks. Traditional approaches relied on hand-crafted features and part-based models @cite . Modern deep-learning based methods are either based on RCNN-like @cite @cite @cite @cite , or SSD-like architectures @cite @cite . In our model, we use Mask-RCNN @cite for person and object detection. One limitation of this approach is that it is trained on a closed vocabulary of 80 object categories in COCO. In the context of human-object interaction recognition, we want to detect any objects that participate in interactions. To this end we propose a simple modification of the training procedure of Mask-RCNN which makes the model category-agnostic.
- is a well studied problem. Traditional tracking algorithms @cite @cite @cite used hand-crafted appearance features to perform online tracking of the bounding box in the first frame. Despite their efficiency, the performance of these methods on realistic videos is sub-optimal. State-of-the-art, deep learning-based trackers @cite @cite @cite @cite @cite demonstrate a better performance and are more robust. @cite find that the last layers of CNNs encode semantic abstraction of a target that is invariant to appearance changes. @cite use R-FCN @cite and correlation filters for simultaneous region of interest detection and tracking which is robust to target size change. Our tracking module, following the tracking by detection paradigm, first detects all humans in consecutive video frames. For efficiency reasons, instead of online fine-tuning the model on the detected actors in the first frame, we propose to train siamese-network @cite offline with a triplet loss. We then use the resulting discriminative features for matching the boxes in consecutive frames.
- The concept of utilizing an unverified, complex controller along with a simple, verified safety controller for fault tolerance was initially proposed as Simplex Architecture in @cite @cite @cite . In earlier simplex designs, fault tolerance was achieved in one of two ways. In some of these designs such as @cite @cite @cite @cite @cite , all three components (safety controller, complex controller, and decision unit) share the same computing hardware (processor) and software platform (OS, middleware). As a result, these designs only protect the safety against the faults in the application logic of the complex controller. And, there is no guarantee of the correct behavior in the presence of system-level faults. Our proposed design protects the system from both application-level and system-level faults.
- Some Simplex-based designs such as System-Level Simplex @cite , Secure System Simplex Architecture (S3A) @cite , and other variants @cite run the safety controller and the decision logic on an isolated, dedicated hardware unit. By doing so, the trusted components are protected from the faults in the complex subsystem. However, exercising System-Level Simplex design on most COTS multicore platforms SoC (system on chip) is challenging. The majority of commercial multicore platforms is not designed to achieve strong inter-core fault isolation due to the high-degree of hardware resource sharing. For instance, a fault occurring in a core with the highest privilege level may compromise power and clock configurations of the entire platform. To achieve full isolation and independence, one has to utilize two separate boards systems. In contrast, the approach proposed in this paper needs only one processor and tolerates system-level faults.
- The notion of restarting as a means of recovery from faults and improving system availability is previously proposed in the literature. These approaches are generally divided into two categories, ) , reactively restart a failed component and ) , prophylactically restart functioning components to prevent state degradation @cite . Our approach, as described in this paper, fits in the former category. However, with slight modification, our design can incorporate periodic self-triggered restarts to prevent future unscheduled unavailable times. In the second form, this work can also be categorized in the latter category.
- Most of the previous works on restarting are proposed for traditional safety-critical computing systems such as servers and switches. Authors in @cite introduce recursively restartable systems as a design paradigm for highly available systems and use a combination of revival and rejuvenation techniques. Earlier literature @cite @cite @cite illustrates the concept of microreboot which consists of having fine-grain rebootable components and trying to restart them from the smallest component to the biggest one in the presence of faults. The works in @cite @cite @cite focus on failure and fault modeling and try to find an optimal rejuvenation strategy for various systems. In this context, our previous work in Reset-Based Recovery @cite was an attempt to utilize restarting as a recovery method for computing systems in safety-critical environments. In @cite , we used System-Level Simplex architecture and proposed to restart only the complex subsystem upon the occurrence of faults. This is feasible because the safety subsystem runs on a dedicated hardware unit and is not impacted by the restarts in the complex subsystem. The approach of the current paper is significantly different and uses only one hardware unit.
- Early works for addressing aspect extraction relied on approaches such as identifying frequent nouns and noun phrases using association rule mining, dependency relations, and lexical patterns @cite @cite @cite . The SemEval workshops, over the course of three years, has included aspect-based sentiment analysis in their competitions. One of the subtasks introduced during SemEval is aspect category detection, which our proposed method is going to address. Most of the supervised approaches proposed to address this subtask utilizing machine learning algorithms and train a set of the one-vs-all classifier using hand-crafted features @cite , @cite , @cite .
- There are only a few unsupervised approaches to address the aspect category detection subtask in the literature. In @cite authors trained a network similar to auto-encoder with attention mechanism to attend to aspect-related words. @cite propose to use a double propagation technique to mine rules based on dependency relations for finding aspect terms of each category. Also, irrelevant aspect terms were pruned using stop words and the PageRank algorithm on a graph-based approach. @cite proposed an unsupervised method called spreading activation' that performs association rule mining, using a set of seed words and a co-occurrence matrix between words to form a co-occurrence digraph to detect aspect categories. One of the shortcomings of this method is the need for tuning multiple parameters. Unlike this method, our proposed method only requires a small number of parameter (a threshold, the number of clusters, and the interpolation coefficient).
- 5pt Image inpainting can be seen as a special case of the image-to-image transformation problem in that image inpainting tries to transform a cropped image to a reconstructed one. One of the typical image-to-image transformation problems is autoencoding @cite . In relation to inpainting, a seminal work in this area is the denoising autoencoder @cite . It introduces an inpainting-like scheme to autoencoding, hoping the autoencoder can learn a better feature representation by recovering the damage to the input image. Another related work is @cite . It incorporates GANs into a variational autoencoder (VAE) @cite and argues that the network trained using the adversarial loss and VAE loss can give a more representative feature vector of the input image.
- For other image-to-image transformation problems, @cite proposes to use GANs for image super-resolution. It has the standard generative image-to-image model setting: a generator that maps the low resolution input to high resolution and a discriminator that tells if the input image is a high resolution one or not. It also includes perceptual loss @cite to further regularize the realism. @cite proposes a general framework for image-to-image translation. It improves the generative image-to-image networks by introducing image-conditional GANs and PatchGAN.
- The closest work to ours is @cite . It advances the state of the art @cite by introducing a parsing network to further regularize the inpainting through semantic parsing segmentation and a local discriminator to ensure the generated contents are semantically coherent. However, the parsing network is pretrained and independent of the generator. Thus, the semantic parsing information is not shared with the generator during training. The semantic parsing loss is also not applied directly to the generator. As a result, the accuracy of computing such a loss is limited by the parsing network which further limits the final performance of the generator. Meanwhile, for the local discriminator, it requires the mask to be rectangular and hence cannot be generalized to other inpainting cases such as noise inpainting. @cite also proposes a similar global and local discriminator design and it suffers from the same disadvantage as @cite .
- The study of computational techniques for the effective and efficient management of Emergency Medical Service (EMS) resources has a relatively long history @cite . The main focus of research in this domain has been on the development of models for the placement of EMS facilities, such as ambulance stations, and on strategies for resource relocation so that specific performance metrics are maximised @cite @cite . The survey @cite provides a chronological perspective on the development of research in this area.
- In recent years, data market design has gained increasing interest from the database community. The seminal paper @cite by Balazinska discusses the implications of the emerging digital data markets, and lists the research opportunities in this direction. Koutris @cite presented a flexible data trading format, , query-based data pricing. Later, Lin and Kifer @cite designed an arbitrage-free pricing function for arbitrary query formats. For personal data sharing, Li @cite proposed a theory of pricing private data based on differential privacy. Upadhyaya @cite developed a middleware system, called DataLawyer, to formally specify data use policies, and to automatically enforce these pre-defined terms during data usage. Jung @cite focused on the dataset resale issue at the dishonest data consumers.
- However, the original intention of above works is pricing data or monitoring data usage rather than integrating data truthfulness with privacy preservation in data markets, which is the consideration of this work The early version of this paper @cite mainly focused on the profile matching service. .
- To get a tradeoff between functionality and performance, partially homomorphic encryption (PHE) schemes were exploited to enable practical computation on encrypted data. Unlike those prohibitively slow fully homomorphic encryption (FHE) schemes @cite @cite that support arbitrary operations, PHE schemes focus on specific function(s), and achieve better performance in practice. A celebrated example is the Paillier cryptosystem @cite , which preserves the group homomorphism of addition and allows multiplication by a constant. Thus, it can be utilized in data aggregation @cite and interactive personalized recommendation @cite @cite . Yet, another one is ElGamal encryption @cite , which supports homomorphic multiplication, and it is widely employed in voting @cite . Moreover, the BGN scheme @cite facilitates one extra multiplication followed by multiple additions, which in turn allows the oblivious evaluation of quadratic multivariate polynomials, , shortest distance query @cite and optimal meeting location decision @cite .
- The dominant image captioning networks are based on the encoder-decoder framework @cite . Kiros al @cite developed feed forward neural network-based multimodal neural language models to generate image descriptions. Recurrent neural networks (RNNs) were introduced in @cite @cite to enhance the capacity of neural language models. Vinyals al @cite proposed an end-to-end captioning network with a CNN encoder and LSTM-based sentence generator. Visual attention mechanisms @cite @cite @cite were explored in image captioning networks to replace fixed CNN encoders and further improve captioning performance.
- There are temporal dynamic scenes, rich visual contents, and auditory modality in videos. Therefore, video captioning is much more challenging than the image captioning task. Inspired by the early image captioning networks, Venugopalan @cite utilize a CNN encoder to extract visual features for sampled video frames and perform mean pooling over these features for video captioning. To explore temporal structures of videos, Venugopalan @cite propose a sequence-to-sequence video captioning network, which exploits LSTMs to encode visual features from different video frames. Yao al utilize a spatial temporal 3D CNN and a temporal attention mechanism to explore local and global temporal structures in videos. Pan al @cite propose a hierarchical recurrent neural encoder for exploiting temporal information. Zhang al @cite develop a dynamic fusion method to combine appearance and motion features for video captioning. Reinforcement learning is exploited in @cite and @cite .
- Excepting the visual content, auditory modality and video tags have also been demonstrated that it can help to improve video captioning performance in @cite @cite @cite @cite @cite @cite @cite . Naive fusion ( concatenation in @cite ) and attention fusion ( @cite ) are utilized in these methods to aggregate features from different modalities. Although these methods exploit the audio information, they fail to completely analyze the two modalities for video captioning due to essential difficulties existing in their RNN decoder-based frameworks.
- Recently, research topics about vision and sound have attracted a lot of attentions. Aytar al @cite propose a SoundNet to learn audio features using a visual teacher network from massive unlabeled videos. Owens al @cite adopt ambient sounds as a supervisory signal for learning visual models. To learn both a joint embeding for audio and visual content, Arandjelovic and Zisserman introduce a audio-visual corresponding prediction task. To learn audio and spatio-temporal visual representions, Owens and Efros @cite , and Korbar al @cite utilize the audio-visual temporal synchronization task as a proxy. Excepting representation learning, some works on sound source separation @cite @cite , sound source localization @cite @cite , and audio-visual event localization @cite have also been studied. Unlike the previous works, in this paper, we will investigate audio-visual video captioning task and explore whether the audio modality can infer high-level semantics of visual modality and vice versa with the captioning problem as a proxy.
- Moreover, @cite and @cite both allow for QbS word spotting using similar approaches. In @cite , the authors create a textual descriptor based on character n-grams and use Latent Semantic Analysis @cite to perform multi-modal fusion, mapping their visual BoVW and textual representations to the same space. In a similar vein, @cite use an attribute representation @cite @cite called Pyramidal Histogram of Characters (PHOC) as textual descriptor. They use Canonical Correlation Analysis (CCA) to learn a common subspace for the textual PHOC descriptor and visual Fisher Vectors. As it turns out, the approach in @cite proved to be the stronger system and the PHOC attribute representation has been widely adopted by the word spotting community, and has even been put to good use in lexicon-based text recognition @cite .
- Since then, many methods extending it have been proposed. In @cite @cite , the approach in @cite is extended to the segmentation-free setting. The method proposed in @cite replaces the Fisher Vectors by a convolutional neural network (CNN) and the PHOC representation is learned using SVMs. Two methods were simultaneously proposed where the two step Fisher Vector and CCA approach is replaced with end-to-end trainable CNNs @cite @cite . This strand of work has been consolidated and improved upon in @cite @cite .
- The second category are methods using connected components @cite @cite @cite @cite @cite . In @cite , connected components in the shape of vertical strokes are extracted by performing mathematical morphology to separate characters. A popular approach is to binarize the image, extract connected components, and group them in a bottom-up fashion using heuristics and finally extract bounding boxes @cite @cite . A similar approach is used in @cite for matching entire documents using distributions of word images. A combination of the two approaches is used in @cite . Here, extremal regions are extracted on top of text presence scores computed using a sliding window. A related approach is applied to word segmentation in @cite , where morphological closing is used using a variety of different kernel sizes to connect characters into words and extract bounding boxes. These methods still over-segment the manuscript, but the number of proposals is typically fewer than for sliding window based methods. They are also more robust to small input shifts as the connected components provide a kind of attention. The downsides include a sensitivity to physical degradations like ink blotches and difficulty with densely written manuscripts.
- A recent approach that can be seen as hybrid between sliding window and connected components is presented in @cite . Here, the authors use a Resnet encoder-decoder network to produce a heatmap denoting each pixels probability of being inside, outside, or near a bounding box. The heatmap is smoothed using a smoother network before being fed to a Proposal Generation network that produces bounding box coordinates, which are subsequently filtered using a proposal Filter network.
- The recently introduced embedding for word spotting DCToW @cite is similar to PHOC in that it is hand-engineered, but it does not consist of binary attributes, but a low-frequency, real-valued representation of a text string. It shares a greater similarity with the Spatial Pyramid of Characters introduced for text recognition in @cite , which is similar to PHOC except that character occurrences are counted, not a binary presence absence.
- The attribute and label embedding representations allow seamless retrieval of words not present in the training data, known as zero-shot learning. This same technique is used for multi-label image classification @cite , and text recognition @cite . Moreover, word spotting shares similarities with approaches for multi-modal embeddings for zero-shot learning @cite @cite , with the difference that the text modality has a fixed embedding.
- The problem set out in this paper was inspired by the work done in @cite . Here, the authors attempt to classify activities of the user by on-body sensors and video cameras. They also consider the energy consumption of the camera, utilising a Markov Decision Process (MDP) to decide whether to use weak, but efficient accelerometer and gyroscope, or strong but inefficient video cameras.
- The available pervasive health monitoring sensors, such as PIR or ES @cite differ in their usability and the quality of their readings. They also differ in how much energy it takes to operate them and process their results @cite . Low-power wearable sensors @cite are also popular within the community, providing not only the on-board acceleration observations, but also acting as a RF anchor. The fusion of these sensors @cite show that different combinations can provide improvement to localisation and activity recognition @cite performance.
- Focusing on the work on IM in computer vision, most studies made use of one of the two available large datasets, specifically designed for IM prediction, where IM was measured a few minutes after memorization @cite @cite , and consequently focused on predicting a so-called short-term IM @cite @cite @cite @cite @cite @cite @cite @cite . The pioneering work of @cite focused primarily on building computational models to predict IM from low-level visual features @cite , and showed that IM can be predicted to a certain extent. Several characteristics have also been found to be relevant for predicting memorability in subsequent work, for example saliency @cite , interestingness and aesthetics @cite , or emotions @cite . The best results were finally obtained by using fine-tuned or pre-extracted deep features, which outperformed all other features @cite @cite @cite @cite , with models achieving a Spearman's rank correlation near human consistency ( @math ) when measured for the ground truth collected in @cite @cite .
- Results obtained for VM prediction are yet far from those obtained in IM prediction. Han proposed a method which combines audio-visual and fMRI-derived features supposedly conveying part of the brain activity when memorizing videos, which in the end enables to predict VM without the use of fMRI scans @cite . However, the method would be difficult to generalize. Shekhar investigated several features, including C3D, semantic features obtained from some video captioning process, saliency features, dense trajectories, and color features, before building their memorability predictor @cite . They found that the most predictive feature combination used captioning features, dense trajectories, saliency and color features.
- Single-view, or monocular, depth estimation refers to the problem where only a single image is available at test time. @cite show that it is possible to produce pixel depth estimations using a two scale deep network trained on images and their corresponding depth values. Several methods extend this approach by introducing new components such as CRFs to increase the accuracy @cite , changing the loss from regression to classification @cite , using other more robust loss functions @cite , and by incorporating strong scene priors @cite . Recently, there are a number of methods to estimate depth in an unsupervised way. @cite introduce an unsupervised method by using an image alignment loss. @cite propose an unsupervised deep learning framework by employing loss functions which impose consistency between predicted depth maps which are obtained from different camera viewpoints. @cite adopt a semi-supervised deep method to predict depths from single images. As opposed to existing methods, in our work, we use supervised depth estimation to produce depth maps to enable the inference of 3D shapes. Our 3D-2D refinement module uses the generated full point cloud as a 3D supervision algorithm to steer the depth estimation.
- Point cloud feature extraction is a challenging problem because points of 3D point clouds lie in a non-regular space and cannot be processed easily by standard CNNs. @cite propose PointNet to extract unordered point representations by using multi-layer perceptrons and global pooling. As a follow-up work, PointNet++ abstracts local patterns by sampling representative points and recursively applying PointNet as learning blocks to obtain the final representation. @cite introduce 3DContextNet that exploits both local and global contextual cues imposed by the k-d tree to learn point cloud features hierarchically. @cite propose a folding-based decoder that deforms a canonical 2D grid onto the underlying 3D object surface of a point cloud. In our work, we leverage the PointNet layers and folding operations to build our point completion module. The PointNet layer is used as the basic learning block to build our network. The folding operation is used as the last step of our point completion module to transform the sparse full point cloud to a dense full point cloud.
- Voxel-based representations are computationally expensive and are only suitable for coarse 3D voxel resolutions. To address this issue, @cite introduce point cloud based representations for 3D reconstruction. They propose an end-to-end framework to directly regress the point location from a single image. Different from @cite , our approach sequentially predicts the depth map, infers the partial point cloud based on the camera model, and generates the full point cloud of the 3D shape. In addition, we also explicitly enforce the alignment between the generated point cloud and the estimated depth map to jointly optimize both of the components.
- Cross-modal retrieval is an active area of research and the different approaches proposed in literature can be divided into unsupervised, supervised and semi-supervised. Unsupervised approaches do not have access to the training labels and in general utilizes the correspondence between the data of the two modalities to learn a common space. Canonical Correlation Analysis (CCA) and its kernelized version (KCCA) @cite tries to project the data from the different modalities so that they become correlated. Partial Least Squares @cite linearly maps the different modalities into a common space. @cite , connections between the objects in an image with keywords in the textual queries are established to design better cross-modal associations.
- The concept of label cleaning and prediction from weakly annotated or unlabeled data for single modality has been explored in @cite @cite . Algorithms like @cite @cite @cite design noise-robust procedures and try to correct the mislabeled data. Some approaches @cite @cite @cite select a small subset of the data whose clean annotations are available and tries to learn functions so as to clean the label noise. The work in @cite jointly learns to clean noisy annotations and design an effective image classifier. Our work focuses on label prediction for cross-modal data and to the best of our knowledge is the first work in this field.
- There are two main tasks in computational fact checking: (1) monitor and spot claims @cite @cite , (2) check claims and explain outcomes. We focus on the second task and on factual facts, specifically. Related approaches try to align the fact to trusted data resources, such as KGs @cite , Web documents @cite , and databases @cite @cite . These approaches create features for binary classifiers from the data in the KG. Features exploit the structure of the training examples, in the form of paths @cite @cite or geometric properties in a multi-dimensional space with embeddings @cite @cite . As providing interpretable descriptions of the outcome of a ML model, such as SVM, is an active topic of research @cite , we argue that semantically rich rules and their evidence facts are useful explanations for a fact checking outcome.
- Markov Logic combines first-order logic and Markov networks @cite . In principle, learning in Markov Logic could learn the uncertain rules and inference can be applied to the learned rules as we do here. We tested alchemy to learn logical rules for spouse relation with only 10 positive examples, the system was not able to produce results after 2 hours of execution. This illustrates that rule learning in Markov Logic has scalability issues with large KGs such as DBpedia, let alone the quality of the rules learned.
- @cite classify ALL subtypes on a private dataset of 330 images using a pre-trained AlexNet and fine-tuning. Shafique and Tehsin @cite classify ALL subtypes on ALL-IDB augmented with 50 private images, also using a pre-trained AlexNet and fine-tuning. @cite classify ALL on ALL-IDB using a number of different pre-trained CNNs as fixed feature extractors. From these CNN features the most informative ones are selected using PCA and finally classification is performed with an ensemble of SVM, MLP, and random forest.
- Putzu and Ruberto @cite use ALL-IDB and classify a number of hand-crafted features like area, compactness, roundness or area ratio between cytoplasm and nucleus with an SVM. @cite and @cite both use (different) private datasets and classify using an ensemble of naive Bayes, KNN, MLP, SVM and a KNN classifier respectively.
- It is increasingly accepted that many natural and social systems are better described as temporal or dynamic networks @cite , with links that exist only at certain times. Some examples are: the temporal and dynamical user's behavior in social networks @cite ; the temporal evolution of disciplines guided by scientists social interaction @cite ; calculating the analytical epidemic threshold @cite or controlling the spreading of epidemics or information @cite @cite ; and, understanding the spatiotemporal evolution of wildfires according to the community identification @cite .
- Several works have developed methods to measure the engagement of individuals in social environments, like the discovering of correlations between the use of Facebook and student engagement @cite ; the role of Twitter opinion leadership on individuals engagement in politics @cite ; and measuring the engagement of businesses on Twitter and the positive effects on consumers' engagement with online word-of-mouth communication @cite . Another approach, based on content analysis, is focused on studies applying Natural Language Processing (NLP) techniques for processing User Generated Content in Opinion Mining and Sentiment Analysis @cite . Also, based on these techniques, several studies have developed methods to measure the users' engagement on marketing campaigns, products or individuals, over time @cite @cite . However, these previous work employed the content of the messages as input of the methods and are not suitable for the case of encrypted scenario. To the best of our knowledge, this is the first approach that tackle the mining of users' interaction and engagement in encrypted group message and by employing temporal networks.
- The idea of GANs @cite @cite @cite has recently enjoyed success in NLP fields @cite @cite . For example, a success application of GAN is used in @cite , where the generator is a RL-based seq2seq model, and the outputs from the discriminator are then used as rewards for the generator, pushing the system to generate dialogues that mostly resemble human usage.
- The graph alignment, which aims to find node correspondence across different graphs, has attracted lots of research interests with extensive literatures in the past decade. Graph alignment was first studied by exploring the graph structure to align the topology of the underlying graphs. For example, IsoRank @cite aligns different graphs greedily based on the pairwise topology similarities ( node similarity) calculated with spectral graph theory and finds the alignment by solving the maximum bipartite matching. NetAlign @cite proposes a distributed and efficient algorithm for the sparse bipartite graph matching problem by using the max-product belief propagation based on the graph topology. UniAlign @cite proposes to distinguish the nodes of graphs by types ( users and groups) and find the correspondences at different granularities at once for the fast bipartite or unipartite graph matching problem. Klau @cite formulates the pairwise global graph alignment problem as the graph-based maximum structural matching problem, which is solved by a Lagrangian relaxation approach. These approaches, however, ignore the rich attribute information on nodes and or edges in many real graphs and are prone to lead to sub-optimal results.
- @cite consider the problem of preserving , a class of noninterference properties similar to CVDNI in its explicit consideration for capturing timing-sensitivity. consider a wider scope of common categories of compile-time optimisations (than those performed by our ), and mechanise proofs in Coq that such optimisations preserve various constant-time security properties. The sharing of variables in our setting severely limits the scope of our optimisations, to those that the compiler can perform knowing that a shared variable is stable because it has been locked. At present, our avoids redundant loads during expression compilation, but other optimisations like loop hoisting and constant folding we are yet to implement. Their preservation proof technique, was developed independently to our original cube-shaped secure refinement definition @cite . Like ours, theirs is also a cube-shaped obligation and makes use of a pacing function analogous to our @math . Unlike our work here, do not give a general method for decomposing their cube-shaped simulation diagrams.
- Neither of the above consider per-thread compositional compilation of concurrent, shared memory programs, nor value-dependent noninterference policies -- the focus of our theory and compiler. @cite however did aim to preserve noninterference of multithreaded programs by compilation, extending a prior compilation approach @cite . Their noninterference property however was termination- and timing-, so preventing internal timing leaks relied on the scheduler disallowing certain interleavings between threads. Also, their type-preservation argument was derived from a big-step semantics preservation property for their compiler. Here we instead rely on preservation of a small-step semantics (specifically memory contents), which is necessary for us to preserve value-dependent security under compilation, as well as to avoid imposing non-standard requirements on the scheduler.
- Other recent works have improved on (surveyed @cite ) by mapping out the spectrum @cite or developing specific forms @cite of , concerned with of source program (hyper)properties to concrete contexts. Like @cite , these works differ from ours in quantifying over a wider range of hostile interference. They also focus prominently on changes to data types, which we do not support. Thus, as a 2-safety hyperproperty quantifying over a lesser range of interference, we expect CVDNI-preservation to be implied by R2HSP (robust 2-hypersafety preservation), but do not expect it to imply any other secure compilation criterion on 's @cite spectrum.
- A basic framework of the standard RVFL network @cite is shown in Fig. (a). The inputs to the output layer in RVFL consist of both non-linearly transformed features @math from the hidden layer and original input features @math . If @math be the input data features and @math be the number of hidden nodes, then there are total @math inputs for each output node. Since the hidden layer parameters are randomly generated and kept fixed during the training phase, only the output weights @math need to be computed. Thus, the resulting optimization problem can be mathematically represented as: where @math is the concatenation of hidden features and original features, @math is the regularization parameter and @math is the target vector.
- Typically, Eq. can be solved via a closed form solution using either ridge regression (i.e. @math ) or Moore-Penrose pseudoinverse (i.e. @math ) @cite . Using Moore-Penrose pseudoinverse, the solution is given by: @math while using the regularized least squares (or ridge regression), the closed form solution is given by:
- ELM @cite , developed in 2004, can be viewed as a variant of RVFL without direct links and bias term (see Figure (b)). Thus, Eq. becomes
- In a standard RVFL network, the hidden layer parameters ( @math and @math ) are randomly generated within a suitable range and kept fixed thereafter. Even though RVFL has demonstrated its efficacy in various domains, its performance is often challenged by randomly assigned hidden layer parameters. To alleviate this issue, the authors in @cite proposed an unsupervised parameter learning based RVFL known as sparse pre-trained RVFL (SP-RVFL). In an SP-RVFL network, an autoencoder with @math regularization is employed to learn the hidden layer parameters. Specifically, the optimization problem for the autoencoder is given by:
- where @math is the input, @math is the hidden layer matrix obtained via random mapping and @math is the output weight matrix of the autoencoder. The above optimization problem, Eq. is solved using a fast iterative shrinkage-thresholding algorithm (FISTA) @cite . The @math pre-trained by sparse-autoencoder is then used as the weights of the hidden layer of a standard RVFL. The hidden biases are then computed as:
- The HELM @cite is a randomized multi-layer neural network based on ELM. It consists of two components: feature encoding using ELM and an ELM based classifier. For feature extraction, it uses sparse autoencoder as defined by Eq. in the preceding section. Multiple hidden layers are then stacked on top of each other for the feature extraction part. The extracted features are then used by ELM classifier for final decision making.
- In recent years, several training strategies have been proposed to mitigate model collapse in GANs. Metz @cite , for instance, modified the training strategy, such that at each gradient update step @math , the generator's update is computed based on the discriminator's state at step @math to match those that the optimal discriminator would allow. This strategy stabilizes GAN training and decreases mode collapse, but it significantly increases the computational complexity of each update step, as the discriminator calculates derivatives @math times more than the vanilla GAN. Other strategies modify the GAN's loss function and network architecture. For instance, Zhao @cite proposed a nonstochastic alternative loss and an autoencoder-like architecture for the discriminator to learn the manifold of the training data.
- There have been many works in the literature on applications of machine learning and expert systems for classification and identification of disabilities. @cite worked on identification of students with learning disabilities that includes dyslexia, attention deficit disorder and more using ANN. They recommend their model as "second opinion" to experts who in charge of learning disabilities evaluation. Hofmeister and Lubke @cite and @cite are some other works on applications of Artificial intelligence (AI) for diagnosis and classification of learning disabilities.
- @cite worked on an intelligent decision support system for the evaluation of the degree of hearing loss. @cite worked on learning the behavior patterns of people with disabilities using machine learning methods such as Hidden markov models. @cite implemented multiple machine learning algorithms viz. Decision trees, Support vector machine and k nearest neighbors for identification of children with reading disability.
- @cite worked on the self-care identification and classification problem in children and they came up with an innovative and the first of its kind dataset for this purpose known as "SCADI (Self-Care Activities Dataset based on ICF-CY)". They used advanced machine learning algorithms viz. Artificial neural networks and Decision trees for solving the self-care problem classification.
- However, in the literature we don't find much work apart from that of @cite that focuses on machine learning algorithms for solving the self-care problem classification. Thus, there is a great opportunity for development and application of new methodologies that outperform the traditional ones in this domain.
- The growth of machine learning applications in the healthcare sector has been considerable in recent years. Indeed, due to the large amount of information encoded in X-Ray images, focused research into their analysis has been significant. Aiding this advancement of research has been an increase in availability of X-Ray image datasets, each tailored to different applications @cite @cite @cite @cite . Using such datasets, many machine learning approaches for the detection of anomalies, such as pneumonia @cite , pulmonary tuberculosis @cite , and thoracic diseased @cite have been developed, as well as a variety of diseases based on chest X-Rays @cite . Islam @cite and Qin @cite provide comparisons of a range of neural networks applied to detecting anomalies in chest X-Rays. These detection technologies usually perform image segmentation to localise the position of the anomaly, or of a certain bone structure (most commonly the rib cage), and are carefully tuned to these applications. Additionally, due to their architectures, many of the networks presented in the above literature would not be suitable for performing a pixel-level segmentation over the entirety of the image.
- Complete multi-class image segmentation is an active, high-growth area of research, most recently driven by autonomous vehicle development. Neural networks have been at the forefront of this research and take various forms including the encoder-decoder design similar to that presented in this paper @cite @cite @cite @cite @cite , and fully connected networks @cite . Several applications of these networks also utilise the technique of image augmentation to aid network generalisation and reduce the risk of overfitting. Similarly, Badrinarayanan, Handa and Cipolla @cite present a simplified version of the widely applied SegNet architecture @cite , showing improved performance on small datasets. Indeed, it is against this simplified network, known as @cite , that we benchmark our network performance. It is noted that some networks have been specifically designed for the total segmentation of medical images, yet these applications have been largely constrained to the segmentation of cell structures @cite @cite @cite . There have been examples of networks, such as U-Net @cite , being applied to other segmentation tasks in the field of X-Ray image segmentation @cite , but only in specific use cases.
- The current state of the art in activity recognition is based on the use of cameras. Cameras allow direct and easy capture of motion but this output requires significant processing for recognition of specific activities. Inertial sensing is another popular method used in HAR. To achieve the high accuracy of the inertial sensing systems shown in @cite , a system consisting of multiple sensors is required, compromising functionality and scalability. The associated signal processing is not trivial and singular value decomposition (SVD), truncated Karhunen-Lo eve transform (KLT), Random Forest (RF) and Support Vector Machines (SVM) are examples of feature extraction and machine learning methods that have been applied to HAR.
- @cite recently extended the A2D dataset with human generated sentences, describing the actors and actions in the video, and proposed the task of actor and action segmentation from a sentence. Their method uses a convolutional network for both visual as well as textual inputs and predicts localization on one frame of the video. We propose a different approach, where we make use of capsules for both visual as well as text encoding, and perform localization on the full video instead of just one frame, in order to fully utilize the spatiotemporal information captured by the video.
- first introduced the idea of capsules in @cite , and subsequently capsules were popularized in @cite , where dynamic routing for capsules was proposed. This was further extended in @cite , where a more effective EM routing algorithm was introduced. Recently, capsule networks have shown state-of-the-art results for human action localization in video @cite , object segmentation in medical images @cite , and text classification @cite . We propose to extend the use of capsule networks into the multi-modal domain, where the segmentation and localization of objects in video are conditioned on a natural language input. We introduce a novel capsule based attention mechanism for fusion of video and text capsules for text selected segmentation.
- There are many works related to the automated test input generation problem, as surveyed by @cite . The majority of them focus on unit testing, specification model-based testing, and security testing.
- AFLGo @cite extends AFL to direct fuzzing towards generating inputs that exercise a program point of interest. AFLGo could potentially serve as an alternative to validity fuzzing (Algorithm ) by directing fuzzing towards the validity-checking criteria in the test program. However, AFLGo relies on whole-program static analysis, using LLVM's link-time optimization (LTO) to construct a call graph that helps the tool find a to the fuzzing target location. In our ecosystem, this is not feasible. Constructing precise call graphs for Java is notoriously difficult due to wide-spread use of virtual methods and dynamic class loading @cite @cite . is purely dynamic, and is therefore unaffected by such language features.
- FairFuzz @cite modifies AFL to bias input generation towards branches that are rarely executed, but does not explicitly identify parts of the program that perform the core logic. In , we bias input generation towards validity even if the semantic analysis stage is exercised frequently; our objective is to maximize code coverage in this stage.
- Recently, there has been interest in generating input grammars from existing inputs, using machine learning @cite and language inference algorithms @cite . Similarly, DIFUZE @cite infers device driver interfaces from a running kernel to boostrap subsequent structured fuzzing. These techniques are complementary to ---the grammars generated by these techniques could be transformed into parametric generators for .
- Unlike , which uses coverage information as a heuristic for which inputs may yield new coverage under mutation, symbolic execution tools @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite methodically explore the program under test by capturing path constraints and directly producing inputs which fit yet-unexplored path constraints. Symbolic execution can thus be used to precisely produce valid inputs exercising new behavior. The cost of this precision is that it can lead to the path explosion problem for larger programs, which causes scalability issues. Hybrid techniques that combine symbolic execution with coverage-guided fuzzing have also been proposed @cite @cite @cite .
- Finally, domain specific languages supporting the construction of test harnesses have also been developed. UDITA @cite is such a language for Java. It aims to assist test generation and supports several test generation and test filtering strategies. TSTL @cite is a scripting language for writing test harnesses. It includes tools to support, manage and analyze test generation that share a common library interface. We share the same idea of easy and simple definition of test templates---in our case, QuickCheck-like generators---as these approaches. However, targets the test generation problem directly by automatically generating the inputs that achieve high coverage or expose faults, while UDITA and TSTL focus on the definition and management of the test inputs.
- The left ventricle appears as a blob'' object in short axis MRI. Traditionally active contours and level set based methods were used for blob object segmentation @cite . While these methods offer object shape constraints, they typically look for strong edges or statistical modeling for successful segmentation. These techniques lack a way to work with labeled images in a supervised machine learning framework. For complex segmentation tasks, such as cardiac MRI segmentation @cite these methods are inadequate. Deep learning (DL) has invigorated interest for these classic techniques in the recent years, including our present work, because starting from raw pixels DL can be trained end-to-end with labeled images. With the exception of limited literature, such as shape prior CNN @cite , DL lacks any inherent mechanism to incorporate prior knowledge about object shapes; instead, DL relies on the volume of labeled images to implicitly learn about object shapes or constraints. Hence, there is a need to combine CNN with these traditional methods so that the latter can provide adequate prior knowledge.
- Hu @cite proposed to use CNN to learn a level set function (signed distance transform) for salient object detection. Tang @cite used level set in conjunction with deep learning to segment liver CT data and left ventricle from MRI. However, their method does not use end-to-end training for this combination. Deep active contours @cite combined CNN and active contours; the work, however, fell short of an end-to-end training process.
- In this work, we present a complementary approach to work with limited amount of labeled images. Our guiding principle is to inject the learning system with prior knowledge about the solution. A similar argument was made by Ngo, Lu, and Carneiro @cite for combining level set and CNN to work with limited labeled data for left ventricle segmentation. For this segmentation task, the prior knowledge is a smooth shape, which can be modeled as a closed contour drawn through a star pattern. To inject such knowledge into the learning system, we resort to the principle of differentiable programming, where more than one differentiable algorithms are stitched together. However, the added difficulty in our case is the non-differentiable nature of DP that we overcome using SG.
- Prior work has made use of graphical models, such as Hidden Markov Models (HMMs) or Conditional Random Fields (CRFs), to predict human motion. used HMMs to retain a dynamic model of human motion and reported good results for motion completion tasks @cite . @cite , Koppula and Saxena predicted trajectories of the human hand using CRFs. Their approach samples possible trajectories by taking object affordances into account. However, while graphical approaches capture relationships between objects well they do not allow for additional constraints that may come from the environment, an issue that we address in this paper.
- Another approach for predicting human motion is Inverse Optimal Control (IOC) which aims to find a cost function underlying the observed behavior. @cite , investigated cost functions for arm movement planning and report that such movements are closely linked to the combination of two costs related to mechanical energy expenditure and joint-level smoothness. In @cite , investigated prediction of human reaching motions in shared workspaces. Using goal-set IOC and iterative replanning, the proposed method accounts for the presence of a moving collaborator and obstacles in the environment using a stochastic trajectory optimizer. IOC methods typically represent bio-kinematic processes by simplified models, which not necessarily generalize well. In contrast our method learns these processes from data using a recurrent neural network.
- Within recent literature, several strategies for altering the flow of information within the transformer have been proposed, including adaptive model depth @cite , layer-wise transparent attention @cite , and dense inter-layer connections @cite . Our investigation bears strongest resemblance to the latter work, by introducing additional connectivity to the model. However, rather than establishing new connections between layers indiscriminately, we explicitly seek to facilitate the accessibility of lexical features across network layers. As a result, our proposed shortcuts remain sparse, while performing comparably to their best, more elaborate strategies that rely on multi-layer attention and hierarchical state aggregation.
- Likewise, studies investigating the role of lexical features in NMT are highly relevant to our work. Among them, @cite note that improving accessibility of source words in the decoder benefits translation quality for low-resource settings. In a similar vein, @cite attend both encoder hidden states and source embeddings as part of decoder-to-encoder attention, while @cite provide the decoder-to-encoder attention mechanism with improved access to source word representations. We have found a variant of the latter method, which we adapted to the Transformer architecture, to be less effective than applying lexical shortcuts to self-attention, as discussed in section .
- Our view of the transformer as a model learning to refine input representations through the repeated application of attention is consistent with the iterative estimation paradigm introduced in @cite . According to this interpretation, given a stack of connected layers sharing the same dimensionality and interlinked through highway or residual connections, the initial layer generates a rough version of the stack's final output, which is iteratively refined by successive layers, e.g. by enriching localized features with information drawn from the surrounding context. The results of our probing studies support this analysis, further suggesting that different layers not only refine input features but also learn entirely new information given sufficient capacity, as evidenced by the decrease in similarity between embeddings and hidden states with increasing model depth.
- Image datasets are very important in many fields in computer vision, such as MNIST @cite and CIFAR-10 @cite . There are also some bigger datasets, like ImageNet @cite and Microsoft COCO @cite . In the video understanding domain, we can see a similar progress. Starting from KTH @cite , Hollywood 2 @cite , with a few thousand video clips, to the medium size dataset like UCF101 @cite , however, none of them is as huge as YouTube-8M that we use. Therefore, YouTube-8M shows its importance because it serves as a benchmark in the video understanding area.
- Multimodal is an emerging area which focuses on using various modalities to improve the performance of the model. More specifically, it tends to solve these five problems: Representation, Translation, Alignment, Fusion and Co-learning. Representation means learning how to represent and summarize multimodal data that exploits the complementarity and redundancy of multiple modalities. Translation studies how to translate data from one modality to another. Alignment tries to identify the direct relations between elements from over two modalities. Fusion is a challenge joining information from two or more modalities to perform a prediction. Co-learning transfers knowledge between modalities, their representation, and their predictive models @cite . What we concern in our work is Fusion, cause the whole task is about prediction with two modalities: visual and audio. It includes two types of methods: Joint and Coordinated. We mainly cares about Joint method, which consists of three types of methods, including neural networks, graphical models and sequential. We care about the first one because our model adopts end-to-end neural networks.
- Since year 2015, a mechanism called attention is widely used in several fields in natural language processing area. For example, attention is applied in machine translation in @cite , which significantly improve the BLEU score in machine translation due to the fact that when translating, the decoder knows the exact place to pay attention to. Attention can also be used when reasoning about entailment @cite , which helps to improve the accuracy of classification. In the computer vision area, attention can also be used in the task of image caption @cite , by focusing on the crucial part of an image, the caption is able to describe a picture more accurately.
- Recently, some fancy methods are applied to large video classification with visual and audio features and obtain good performance. @cite use Temporal Resnet Blocks(TRB), each TRB consists of two temporal convolutional layers (followed by batch norm and activation), and this structure is followed by a 7-layer Bi-LSTM Bi-GRU with short-cut connections. @cite introduces a new text modality by crawling the title and keywords of youtube videos to enrich the dataset. @cite adopts Vector of Locally aggregated Descriptors(VLAD) @cite or Fisher Vectors(FV) @cite as a way for pooling features. @cite proposes a network structure called chaining which separates training data into two parts and train the model in two stages. @cite uses variations of Long Short Term Memory(LSTM) model with some tricks and ensemble techniques to get good results.
- Traditional clustering algorithms such as K-means, Hierarchical Clustering, Singular Value Decomposition, Affinity Propagation have been successfully applied in the field of text clustering (see @cite for a comparison of these methods on short text clustering). Algorithms utilizing spectral graph analysis @cite , sparse matrix factorization @cite , probabilistic models @cite @cite @cite were proposed for performance improvement. As text is usually represented by a huge sparse vector, previous works have shown that feature selection @cite and dimension reduction @cite are also crucial to the task.
- Most classic methods require access to prior knowledge about the number of clusters, which is not always available in many real-world scenarios. Dirichlet Process Mixture Model (DPMM) has achieved state-of-the-art performance in text clustering with its capability to model arbitrary number of clusters @cite @cite ; number of clusters is automatically selected in the process of posterior inference. Variational inference @cite @cite and Gibbs sampling @cite @cite can be applied to infer cluster assignments in these models.
- As one of the primary research directions in action recognition, action classification has drawn far more attention than action detection. Driven by the success of deep learning in image classification @cite , many studies have explored to use convolutional neural networks (CNNs) for video classification @cite @cite . In addition to applying the 2D convolutions to individual video frames, @cite introduced the C3D model to simultaneously learn spatial and temporal features using the 3D convolutions with a buffer of video frames. @cite studied various 3D convolutional networks for video understanding tasks to be more accurate and efficient. @cite proposed an effective network architecture for spatio-temporal fusion of video snippets and studied different ways of fusing appearance and motion information. PreRNN was proposed in @cite to transform convolutional networks to recurrent networks for various video understanding tasks including action classification.
- For action detection in videos, most research focuses on either spatial or temporal detection. To localize actions in spatial, Yu and Yuan @cite integrated the actionness score with a greedy method to generate action proposals on individual frames. @cite computed spatio-temporal bounding boxes by merging a hierarchy of supervoxels and classified the candidate boxes by motion features. @cite extended the deformable part models to videos for spatial action detection. To determine the temporal boundaries of actions, a sliding window approach was introduced in @cite to build pyramid representations in order to capture motion information at multiple resolutions. @cite proposed to replace the sliding window approach with a more efficient branch-and-bound search. @cite introduced a temporal segment proposal algorithm based on C3D and LSTM. R-C3D was proposed in @cite to save computational costs by sharing convolutional features between proposal and classification stages. A recurrent policy network was recently developed to perform temporal action detection within a time budget @cite .
- Even though research on brain decoding has attracted increasing attention, a relatively limited number of studies focus on perceived image reconstructions to date. Electroencephalography (EEG) @cite and fMRI are two most widely used neural signals in these decoding tasks. EEG has a high temporal resolution but insufficient spatial resolution, and thus is difficult to locate the active regions in the brain. Contrastively, fMRI is capable of providing more abundant spatial information for higher-precision decoding.
- Traditionally, machine learning methods play significant roles in fMRI-based brain decoding tasks. Miyawaki , for the first time, proposed spares multinomial logistic regression (SMLR) by using multi-voxel patterns of fMRI signals and multi-scale visual representation to reconstruct the lower-order information such as binary contrast patterns @cite . Schoenmakers reconstructed handwritten characters using a straightforward linear Gaussian approach @cite . Fujiwara proposed to build a reconstruction model in which image bases can be automatically estimated by Bayesian canonical correlation analysis (BCCA) @cite . However, the linear hypothesis in the proposed model did not conform to the actual visual encoding-decoding process in human brain.
- considered both output and objective perturbations for privacy-preserving ERM, and gave theoretical guarantees for both privacy and utility for logistic regression and SVM @cite @cite . numerically studied the effects of learning rate and batch size in DP-ERM @cite . studied stability, learnability and other properties of DP-ERM @cite . proposed an adaptive per-iteration privacy budget in concentrated DP gradient descent @cite . Variance reduction techniques, e.g., SVRG, have also been introduced to DP-ERM @cite . The utility bound of DP-SGD has also been analyzed for both convex and nonconvex smooth objectives @cite @cite . analyzed the excess empirical risk of DP-ERM under the distributed setting @cite . Besides ERM, many other ML models have been made differentially private. These include: clustering @cite @cite @cite , matrix completion @cite , online learning @cite , sparse learning @cite @cite , and topic modeling @cite . exploited the ill-conditionedness of inverse problems to design algorithms to release differentially private measurements of the physical system @cite .
- proposed distributed selective SGD to train deep neural nets (DNNs) with a DP guarantee in a distributed system; they achieved quite a successful trade-off between privacy and utility @cite . considered applying DP-SGD to train DNNs in a centralized setting. They clipped the gradient to bound the sensitivity and invented the momentum accountant to get better privacy loss estimation @cite . proposed Private Aggregation of Teacher Ensembles PATE based on the semi-supervised transfer learning to train DNNs and to protect the privacy of the private data @cite . Recently introduced new noisy aggregation mechanisms for teacher ensembles that enable a tighter theoretical DP guarantee. The modified PATE is scalable to the large dataset and applicable to more diversified ML tasks @cite . considered general ML with a DP guarantee under federated settings @cite . numerically studied the vulnerability and privacy-utility trade-off of DNNs trained with a DP guarantee to adversarial attacks @cite .
- The analysis of protein functions in PPI networks is a hot topic in Bioinformatics research. The main research streaming for PPI networks focuses on the detection of protein functional modules and analysis of the dynamic PPI networks' characteristic @cite . Peng . @cite proposed a Udonc algorithm for identifying essential proteins based on protein domains and PPI networks. Zhu . @cite introduced a local similarity preserving embedding algorithm to identify spurious interactions in PPI networks. Sanghamitra . @cite proposed a new feature vector based on gene ontology terms for PPI prediction, in which a protein pair is considered as a document and the terms annotating the two proteins represent the words. Ji . @cite compared some functional module detection methods for PPI, and discussed the accuracy and performance of several typical algorithms. Li . @cite discussed a topology potential-based method for identifying essential proteins from PPI networks. The previous protein modules detection approaches of PPI networks achieved a certain degree of success. However, most of the existing achievements detect protein communities only according to the topological structure of PPI networks, where the identified protein sub-cluster may not effectively reflect its biological significance.
- Existing studies demonstrate that the genes or gene products with similar expression patterns tend to have the similar biological function in a period of life, and also more likely to contact each other to form a dense functional module in PPI networks @cite . Therefore, proteins' GED data are used in this work to evaluate the similarity of proteins in a PPI network @cite @cite . Ji . @cite introduced a multiple-grain model to detect functional modules from large-scale PPI networks. Spirin . @cite presented an enumeration method to find completely connected subgraphs, and then to search for protein functional module. GN algorithm @cite is a classical community discovery algorithm. Depending on high cohesion for internal communities and low cohesion among communities, structures of cohesive communities are relatively detected by gradually removing the edges among communities. Louvain @cite is a fast aggregation algorithm that is used to extract the community structure in large networks, using a heuristic method based on modularity optimization. SPICi @cite is a clustering algorithm for discovering protein complexes and functional modules from PPI networks.
- Early methods for human pose estimation localized keypoints or body parts of individuals but did not consider multiple people simultaneously @cite @cite @cite @cite @cite . Hence, these methods were not adept at localizing keypoints of highly articulated or interacting people. Person detection was used followed by single-person keypoint detection @cite @cite @cite @cite . With deep learning, human detection methods such as Mask-RCNN @cite @cite were employed to directly predict multiple human bounding boxes through ROI-pooling followed by pose estimation per person @cite . However, these methods suffered when people were in close proximity as bounding boxes got grouped together. Furthermore, these methods required more computation as the number of people increased in the image, making them inadequate for real-time pose estimation and tracking.
- In the past, the primary approach to patches comparing was using hand-crafted descriptors and comparing the squared euclidean distance, such as SIFT @cite . The original method of using CNN to extract features had many problems. But with the development of neural network in recent years, new architecture have been emerging, such as AlexNet @cite , VggNet @cite and ResNet @cite . At the same time, The method of data processing in deep learning are gradually improved, such as dropout @cite and batch normalization @cite . Utilizing CNN for feature extraction has become the major approach of image patches matching.
- As a branch of the development of convolutional network, siamese network is a common method for descriptor learning and patches comparison. In 1993, LeCun and utilized siamese networks for signature verification on American checks, that is, to verify whether the signature on checks is consistent with the bank's reserved signature @cite . This is believed to be the first appearance of siamese networks. Similarly, siamese networks were utilized for face recognition @cite . The descriptors of face images can be used to match new samples of previously unknown categories, by comparing the euclidean distances between image descriptors. However, because of the difficulty of descriptor learning in training, in the paper @cite , the author proposed 2-ch net. Instead of comparing the distance by descriptors, it learns the image similarity function to match image patches through the neural network
- In the paper @cite @cite , the triplet network is proposed to change the twice branches structure into three branches. Its advantage lies in the distinction of details, that is, if input images are similar, triple net can be used to describe the details better. The idea of triple net equivalent to adding two measures of input differences and learning a better representation of the inputs. Triplet net is also utilized for face recognition due to its advantage in descriptor learning @cite . In further work, the author of @cite improved the loss function of triple net, named PNSoft Loss.
- Similar to siamese networks, complex networks also started in the 1990s. The theory and error propagation mechanism of complex networks has been explored by researchers @cite @cite @cite @cite . In the complex number field, the characteristics of data representation are more easily to abstracted. In some tasks, the complex-valued network has achieved significant advantages @cite @cite @cite . Although the research has shown that the development potential and expandable space of complex networks, the development of complex networks has been marginalized because the training tricks of complex-valued networks need to be improved. In addition, complex networks require longer training time than the network based on real-value numbers. Recently, c. proposed a solution for initialization, activation, and batch normalization in complex networks @cite , which greatly simplified the training difficulty of complex-valued network.
- Recently, Generative Adversarial Network (GANs) @cite have achieved astounding results in image synthesis such as -- text-to-image translation @cite , image inpainting @cite , super-resolution @cite etc. Moreover, GAN is widely used in image-to-image translation, for example -- CycleGAN @cite -- that uses unpaired training data. It trains two sets of GAN to map @math and @math respectively. Recently, CartoonGAN @cite is proposed to translate real word images to cartoon images which converges faster than CycleGAN @cite and performs satisfactorily (see Figure. ).
- In @cite a C-GAN is used to synthetically augment FR datasets with some success. Their network can generate and modify images of identities in their training dataset or synthesise new identities. (Most other similar work in the literature relies on encodings of input images as the GAN's source of identity information @cite @cite .) Our main concern with @cite is that the network relies entirely on the capability of its auxiliary identity-classifier in order to separate the encoding of identity information and non-identity information. Any residual identity information not captured by the classifier will become encoded by the conditioning vector. This may limit the usefulness of the method for augmenting FR datasets. IVI-GAN, on the other hand, only isolates non-identity information that is labelled as being present in the image, and there is no particular reason why this information should be correlated with identity.
- Another method with similar objectives is @cite . In this work the GAN's generator is trained as a siamese network in which two images are generated from the same random identity'' vector but from different random observation vectors''. The GAN's discriminator then decides if the pair of images looks real compared pairs of real training images depicting the same identity but in different situations. Again, the network relies on the discriminator's capacity to learn to match identities between images in order to separate identity information from extraneous information. There is nothing preventing residual identity information from becoming encoded in the observation vectors.
- In @cite , synthetic images are generated by combining the identity from one input image with the situational factors of a second image that could be an image of a different person. They essentially implement an AC-GAN trained on images with associated ID labels. However, the generated image is forced to contain the situation factors of the second input image via a pixel-wise reconstruction loss. This pixel-wise loss between two images potentially containing different identities seems to cause generated images to be somewhat blurry. The method also doesn't allow for the generation of new synthetic identities nor allow control over image attributes without having an example input image.
- Pairwise global registration: the global methods @cite @cite @cite @cite @cite @cite do not rely on the warm start'' and can be performed on point clouds with arbitrary initial poses. Most global methods extract feature descriptors from two point clouds. These descriptor are used to establish 3D-to-3D correspondences for relative pose estimation. Robust estimations, e.g., RANSAC @cite , are typically applied to handle the mismatches. The feature descriptors are either hand-crafted such as FPFH @cite , SHOT @cite , 3D-SIFT @cite , NARF @cite , PFH @cite , spin images @cite , or learning-based such as 3DMatch @cite , PPFNet @cite , and 3DFeatNet @cite .
- Multiple registration: in addition to pairwise registration, several methods have been proposed for multiple point clouds registration @cite @cite @cite @cite @cite . One approach is to incrementally add new a point cloud to the model registered from all previous ones. The drawback of the incremental registration is the accumulated registration error. This drift can be mitigated by minimizing a global cost function over a graph of all sensor poses @cite @cite .
- Other related methods such as @cite @cite solve camera localization by training DNNs to regress camera poses and test the performance in the same environment as the training images. Related to that is DSAC @cite as a differentiable alternative to traditional RANSAC for its use in pose estimation DNNs. For place recognition, semantic scene completion is used in @cite as an auxiliary task for training an image VAE for long-term robustness. The method in @cite proposes an unsupervised approach with variational Bayesian convolutional auto-encoder to model structures from point clouds. In DeepMapping, we adopt this idea to model the scene structure but use DNN rather than Bayesian inference. Other prior works include: in @cite the generative query network (GQN) shows the ability to represent the scene from a given viewpoint and rendering it from an unobserved viewpoint in the simple synthetic environments. A neuroscience study @cite uses recurrent neural networks to predict mammalian spatial behavior.
- is a long-standing problem in computer vision, and there has been tremendous effort in designing translation-invariant features. Prior to the deep learning era, notable works include scale-invariant feature transform (SIFT) @cite , oriented FAST and rotated BRIEF (ORB) @cite , and deformable part-based models (DPM) @cite . Such works are limited by the inferior representation power of handcrafted features and the constrained family of geometric transformations they address ( , affine transformations). Spatial transformer networks (STN) @cite is the first work on learning translation-invariant features for deep CNNs. It learns to apply global affine transformations to warp feature maps, but such transformations inadequately model the more complex geometric variations encountered in many vision tasks. Instead of performing global parametric transformations and feature warping, Deformable ConvNets sample feature maps in a local and dense manner, via learnable offsets in the proposed deformable convolution and deformable RoIpooling modules. Deformable ConvNets is the first work to effectively model geometric transformations in complex vision tasks ( , object detection and semantic segmentation) on challenging benchmarks.
- For atrous convolution, the spatial support of convolutional layers has been enlarged by padding zeros in the convolutional kernels @cite . The padding parameters are handpicked and predetermined. In active convolution @cite , which is contemporary with Deformable ConvNets, convolutional kernel offsets are learned via back-propagation. But the offsets are static model parameters fixed after training and shared over different spatial locations. In a multi-path network for object detection @cite , multiple RoIpooling layers are employed for each input RoI to better exploit multi-scale and context information. The multiple RoIpooling layers are centered at the input RoI, and are of different spatial scales. A common issue with these approaches is that the spatial support is controlled by static parameters and does not adapt to image content.
- Towards better interpreting how a deep network functions, significant progress has been made in understanding which image regions contribute most to network prediction. Recent works on effective receptive fields @cite and salient regions @cite @cite @cite @cite reveal that only a small proportion of pixels in the theoretical receptive field contribute significantly to the final network prediction. The effective support region is controlled by the joint effect of network weights and sampling locations. Here we exploit the developed techniques to better understand the network behavior of Deformable ConvNets. The resulting observations guide and motivate us to improve over the original model.
- are recently introduced techniques for model acceleration and compression. Given a large teacher model, a compact student model is trained by mimicking the teacher model output or feature responses on training images @cite @cite @cite . The hope is that the compact model can be better trained by distilling knowledge from the large model.
- There exists a large body of works that addressed the resource allocation problems for OFDMA @cite @cite @cite @cite and NOMA @cite @cite @cite @cite @cite techniques in 5G. In OFDMA setup, the proposed resource allocation problems are consisting of subcarrier and power allocation. Meanwhile, in NOMA, one needs to optimize user pairing along with power and sub-carrier allocation @cite . For both NOMA and OFDMA, network optimization is often posed using non-deterministic polynomial-time (NP)-hard problems @cite @cite .
- Equation can be formulated as a boolean integer program, but the nature of @math being arbitrarly complex (e.g shortest-path function) does not allow to solve the general case, especially when @math . Therefore, other formulations are preferred : a rule-based individual linking has been proposed @cite , and @cite proposed a collective formulation for entity linking decisions, in which evidence can be reinforced into high-probability decisions.
- In this context, graph-based approaches have been developed. @cite , and @cite proposed to link efficiently mentions to their corresponding entities using the weighted undirected bipartite graph built among mentions-entities text similarities, by extracting a dense subgraph in which every mention node is connected to exactly one entity, yielding the most likely disambiguation.
- In general, this combinatorial optimization problem is with respect to the number of nodes, since they generalize Steiner-tree problem @cite . However, heuristics have been brought forward, such as @cite and @cite proposing a discarding algorithm using taboo search and local similarities with polynomial complexity.
- An interesting idea is to consider mentions as random variables and their golden true entities as hidden states. Unlike character recognition where @math for latin alphabet, the number of possible states per entity - usually @math - and Viterbi algorithm quadratic complexity ( @math , where @math is the number of states and @math the number of observations) makes the problem computationally untractable. To overcome this technical issue, a first step proposed by @cite is to establish a reduced set of candidates per mention : @math using mention context. Using annotation, an HMM is trained on the reduced set of candidates. Inference is made using message passing (Viterbi algorithm) to find the most probable named entity sequence. Another approach using probabilistic graphical model has been provided by @cite , with a factor graph that uses popularity-based prior, Bethe Approximation to decrease inference computational cost, and message passing to compute marginal probabilities. The computational complexity is @math where @math the number of average entity candidates per mention and @math the number of observations.
- Finally, another probabilistic graphical model has been proposed, similarly to latent Dirichlet allocation (LDA) @cite , where an iterative procedure @math is used above the LDA-scheme to enrich the knowledge base. Its complexity is proportional to the product between LDA complexity and the number of iterations of procedure @math @cite .
- Word embeddings are practical and used for deep learning architectures @cite . Methods such as Word2vec and Glove build a statistical distribution over words representations scalar products @cite @cite . Considering these pairwise conditional probabilities, Skip-gram model aim is to predict context words given one input word @math . Indeed, each word has a probability of appearing given words around it, with a probability being a growing function of the dot product between context word vectors representations. These word embeddings can be obtained either outside or inside of a deep learning architecture, as a first layer. Here, the embeddings represent words of mentions context and entities text description. An example of learning other representations entities is achieved in @cite and reaches state-of-the-art performance on NIST TAC-KBP 2010 Dataset.
- A disambiguation tool using pre-trained embeddings, then averaging and ranking has been proposed @cite with a @math complexity, where @math is the number of mentions and @math the number of entities.
- Recent advances in neural networks conception suggested to use word embeddings and convolutional neural networks to solve the named entity linking problem. @cite proposed to maximize a corrupted cosine similarity between a mention, its annotated gold entity and a false entity. The network is trained with polynomial complexity, and reached state-of-art performance in precision (until 2017 and @cite ) on NIST TAC-KBP datasets in 2009 and 2010.
- Long-short-term memory networks (LSTMs) recently provided remarkable results for natural language modeling in general. Recent neural network architecture have been proposed @cite @cite , the latest using a recent method using fine-grained ontology type system and reaching promising results on several datasets.
- Existing literature on unsupervised video segmentation @cite @cite @cite are mostly based on a graphical model with the exception of Brox. & Malik @cite . Most notably, Papazoglou & Ferrari @cite first obtain motion saliency maps and then refine it using Markov Random Fields. Recent success in video segmentation comes mainly from semi-supervised setting @cite . Semi-supervised methods are either tracking-based or rely on foreground propagation algorithms. Typically, one initializes such methods with ground truth annotations in the first frame and thus, differ from the main goal of this paper that is to segment videos on the fly.
- Video localization is a relatively new problem where the end goal is to localize the common object across videos. Prest @cite tackles this problem by proposing candidate tubes and selecting the best one. Joulin @cite leverages discriminative clustering and proposes an integer quadratic problem to solve video colocalization. Kwak @cite goes a step further and simultaneously tackles object discovery as well as localization in videos. Jerripothula @cite obtains state of the art results by first pooling different saliency maps and then, choosing the most salient tube. Most of the approaches @cite @cite leverage a large set of videos to discriminate or build a foreground model. In contrast, we segment and localize the foreground separately on each video, making our approach much more scalable.
- Our work builds on the discriminative framework @cite , first applied to cosegmentation in Joulin @cite and later extended for colocalization @cite @cite and other tasks @cite @cite . The success of such discriminative frameworks is strongly tied to the availability of diverse set of images where hard negative mining with enough negative(background) data separates the foreground. Our model instead explicitly models the foreground by minimizing the difference of histograms across all image frames. The idea of histogram matching originated first in image cosegmentation @cite @cite . However, we are the first one to highlight its need in discriminative clustering and connection to modelling video data.
- From the perspective of potential applications of automatic speech analysis to technology-assisted care, there is evidence @cite that it is psychologically more acceptable for a user to be aided by another person or a robot than from ambient sensors and devices which are unable to offer meaningful interaction. Therefore, the development of such assistive applications involves research on speech processing for natural conversations rather than scripted speech or monologues @cite .
- Analysis performed on similar corpora provide good insight of the performances achieved using different features. A first analysis @cite , based on a monologue corpus (DementiaBank), identified four different linguistic factors as main descriptors: syntactic, semantic, and information impairments, and acoustic abnormality. They achieved accuracy of up to 92.05 of 25 features, selected amongst an original feature set of 370 features after extensive experimentation.
- An analysis of the CCC corpus by @cite used similar linguistic features. Unlike the work presented in this paper, Guinn's analysis was focused on the differences between interviewers and subjects in the subset of patients with AD. They achieved a combined accuracy of 75-79.5 large discrepancy between AD (38-42 accuracy.
- The family of recognition tasks include image classification @cite @cite @cite , object detection @cite @cite @cite , weakly supervised object detection @cite @cite @cite and semantic segmentation @cite @cite @cite .
- In this work, we build our system upon the recently proposed state-of-the-art object detector, Faster R-CNN @cite . The basic idea is to seek evidence from other existing objects in the scene, which requires object detection as a basic building block. We also extend the expected output from a single and a single to lists of each, to allow multiple acceptable results in an information retrieval (IR) fashion.
- Image retrieval tasks aim to retrieve a list of relevant images based on keywords @cite , example images @cite , or even other abstract concepts such as sketches or layouts @cite .
- Generally, some (topic, features, color, layout, etc. ) are known about the target image, and the expected output is a list of images that satisfies these conditions. Our task is distinct to this family of tasks because our query object is generally present in the scene. Neither is it an attribute possessed by the target image. Nonetheless, we share a similar idea as the retrieval systems in two aspects: First, we adopt the similar expected output as a ranked list, and employ the metric, normalized discounted cumulative gain (nDCG) , as is widely used in previous retrieval tasks; Second, similar to content-based image retrieval systems @cite @cite @cite , we also utilize the known information of the image, typically the categories and locations of the existing objects.
- Closest to our work is the automatic person composition task proposed by @cite , which establishes a fully automatic pipeline for incorporating a person into a scene. This pipeline consists of two stages: 1) location prediction; 2) segment retrieval. Though our system is different from this work, in that we do not perform segment retrieval; while it could not make recommendations on categories or scenes. We compare our system's performance on bounding box prediction with the first stage of this work, and report both quantitative and qualitative results.
- While self-tuning for energy efficiency is a broadly studied topic, the approaches used by researchers vary. For example, Dhiman et. ,al. @cite use online learning to choose one of two energy-saving strategies (experts) during runtime. They specify two experts: one for Dynamic Power Management (DPM) and one for DVFS. The DPM expert decides whether to shut down a specific device during an idle period. The DVFS expert uses a model based on processor events to estimate the optimal frequency for the CPU. Both experts are evaluate at every OS scheduler tick. However, this approach does not necessarily fit HPC workloads, which are usually long running applications without extended idle times.
- Another example is given by Shen et. ,al. @cite , who use Q-Learning to find a trade-off between temperature, performance, and energy. They define a state as a combination of instructions per second, stall cycles, the current processor frequency, and the current processor temperature. This state is captured and evaluated every 10 ,ms. Based on the observations, they calculate a reward, which is used to update the Q-value. Based on the information, an action is taken, i.e., the frequency is changed. Shen et. ,al. use models to evaluate the performance and energy impact of their decisions. However, these models can result in wrong predictions and have to be carefully chosen. By using well-defined program regions, we can measure the energy and performance impacts, which is more precise than a model evaluated every 10 ,ms.
- Van Rijswijk- @cite surveyed a large variety of TLD servers. Their overview spanned features of DNS measurements such as duration, goals, number of vantage points, etc. However, their tests cases only examined cloud email services.
- @cite proposed an associative feature analysis approach based on statistical models to track the anomalous behavior of DNS servers. In collaboration with a major commercial ISP in China, they captured and analyzed real DNS traffic in this large-scale network environment. They used an outlier function to map malicious responses. The parameters they used were queries responses per client server specific server. The authors detected various attacks in the real world, but did not determine the real volume of the attacks. In other words, they could not evaluate their accuracy.
- @cite focused on an anomaly detection system for DNS servers. Normally, dealing with large number of hosts can consume vast amounts of computational resources and make real-time analysis difficult due to traffic overload. They proposed anomaly detection for DNS servers that frequently invoke host selection in which only potential hosts are selected. They used a FIFO (First In First Out) based method for frequent host selection along with other statistics. They categorized packets by type such as DNS mail records (MX), regular DNS packets (A), error rate, etc. They proposed several heuristics, such as the number of queries and requests, where a slightly higher rate of queries requests was considered to indicate an attack. They identified attacks such as the spam Backscatter. This kind of spam consists of incorrectly automated bounce messages sent by mail servers, typically as a side effect of incoming spam (unsolicited messages). Their attack identification system achieved 68
- @cite focused on cache poisoning attacks. They investigated a new indirect attack where they injected the victim's cache with a poisonous record which does not immediately impact the resolution process, but rather becomes effective after an authentic record expires. In this case, the next resolution request to that name returns the spoofed record. Canonical NAME record (CNAME) is a type of resource record in the DNS used to specify that a domain name is an alias for another domain called the "canonical" domain. Delegation of the NAME record (DNAME) creates an alias for an entire subtree of the domain name tree. They injected CNAME and DNAME responses in a cache poisoning attack.
- Learning from imbalanced datasets @cite is an important topic, arising very often in practice in classification problems, that may lead to misclassify most of the data as the dominant class. Many approaches @cite @cite are developed across different levels solving learning from imbalanced datasets. Data-level approaches @cite depend either on under-sampling technique for dominant classes or over-sampling technique for the non-dominant classes. Random Under-sampling @cite , and Over-sampling techniques @cite @cite are non-heuristic method that aims to balance class distribution through the random elimination of majority class examples, and through the random replication of minority class examples @cite by generating new synthetic data respectively. Cost sensitive Learning level approach @cite @cite encodes the penalty of misclassifying in terms of weights like weighted loss function. Ensemble approaches @cite such as Boosting method @cite aims to construct multiple models from the original data and then aggregate their predictions when classifying unknown samples. Focal Loss @cite applies a modulating term to the cross entropy loss in order to focus learning on non-dominant fine-grained classes.
- Recent approaches convert the 3D point cloud into different representation before feeding it into a Deep Neural Network @cite @cite . This is because 3D point cloud points are not in a structured format, so they transformed the data into 3D regular voxel-grids @cite @cite . This representation results in having unnecessarily volumes of data which lead to heavy computations. Other approaches use 3D point cloud points directly as an input to the Deep Neural Network @cite @cite . PointNet @cite is a unified architecture that takes point cloud points directly with the ground truth point wise labels as inputs. However, it fails in capturing and extracting local structure features for complex sparse 3D points in the scene. As an extension for PointNet architecture to solve its issues, PointNet++ @cite is introduced to have the ability of extracting deep local features. However, based on our experimental results, PointNet++ also suffers from misclassifying imbalanced point cloud datasets.
- Early works on multi-label image classification use hand-crafted image features and classifiers adapted to multi-label classification. TagProp @cite is an approach proposed by using a weighted nearest-neighbor model to exploit multi-label training images. Matrix completion approach was proposed by @cite , assuming that the histogram based features of a multi-label image can be decomposed as a linear combination of multiple class histogram basis. The performance of these early works is restricted due to the poor ability of hand-crafted features in representing complex visual information in multi-label images.
- Impressive progresses on multi-label image classification have been made by using deep convolutional neural networks. @cite proposed a CNN-RNN framework to explore the label co-occurrence using the long-short term memory (LSTM). Although VGG16 was employed for the CNN part, the model capacity was not fully exploited by fine-tuning the parameters. @cite extended the idea by improving the CNN part. They proposed a regional latent semantic dependencies (RLSD) model for multi-label image classification, which focused on small objects in the multi-label images by generating subregions that potentially contain mutliple objects and visual concepts. An LSTM based model was employed to generate multiple labels. Recently, the attention mechanism has been introduced to deep neural networks for multi-label image classification. It aims to explicitly or implicitly extract multiple visual representations from a single image characterizing different associated labels @cite @cite . Although improved performance has been reported by introducing more advanced frameworks, we notice that the performance of those proposed methods has marginal gains towards the vanilla deep models, and the training strategies employed in different works vary. Therefore it is necessary to set up a uniform baseline for comparison.
- The basic metrics to evaluate GWAPs @cite @cite @cite are global indicators computed as means over the entire data; while effective in summarizing the behaviour of GWAP players, those are very simple measures that do not tell the entire story: an analysis of data distribution and temporal evolution is usually required to get a deeper understanding of a GWAP.
- Some work exists on cross-feature analysis of GWAPs @cite and similarly on citizen science @cite and crowdsourcing @cite ; our goal is to contribute to making such evaluation easier to replicate and reproduce.
- Participation incentives are usually classified as intrinsic or extrinsic motivation @cite . Some comparative analysis of incentives exists for GWAPs @cite , especially in contrast to different methods like micro-working @cite @cite @cite or machine learning @cite . The effect of competition and tangible rewards on participation and quality of results has also been explored, both in the context of GWAPs @cite and online citizen science campaigns @cite , revealing the pros and cons of designing different motivation mechanisms.
- Other metrics to evaluate GWAPs can be borrowed from studies of social community @cite and citizen science evolution @cite ; in those cases, however, user participation's success'' is measured through simple indicators like number of participants and contributions, while a deeper investigation is needed to assess the effectiveness of participation. Behavioural studies in HCI research have investigated volunteer characterization in citizen science, defining engagement metrics and profiles @cite @cite , which may or may not apply to GWAP players.
- In the context of (paid) crowdsourcing, assessment is usually conducted in relation to micro-work platforms @cite , in which important features are related to cost minimization @cite @cite which is out of scope with respect to our work.
- Erhan al @cite synthesize the images to maximally activate a network unit. Mahendran al @cite and Dosovitskiy al @cite analyze the visual coding to invert latent representation. They perform image reconstruction by inverting the features with an up-convolutional neural network.
- Our framework is inspired by recent works @cite @cite @cite addressing category-specific attention maps. CAM @cite generated the class activation maps highlighting the task relevant region by replacing fully-connected layers with convolution and global average pooling. A drawback of CAM is inflexibility, requiring retraining of classifiers and feature maps to directly precede softmax layers, hindering its applicability to any feature layers. Grad-CAM @cite was proposed to address these issue, where without retraining and changing the network architecture, class activation maps were generated by weighted combination of feature maps in various channels. The weights were computed by averaging the gradient of the final prediction the pixels in the feature map. However, we observe that such simple averaging is unable to measure channel importance properly, resulting in substantial attention inconsistency among various feature layers. While Grad-CAM++ @cite proposed a better class activation map by modifying the way weights are computed, its high computational cost in calculating the second and third derivatives makes it impractical to be used during model training.
- Category-specific attention has been used in the past for weakly-supervised object localization and semantic segmentation tasks @cite @cite @cite @cite . The intuition here was that given only image-level annotations, attention from the last feature layer can be used to improve spatial localization of the target category. Compared to these methods, we model attention differently. While the goal of these methods is singular, , good target localization, our goal is two-fold, good target localization as well as discriminability from other categories. To this end, we devise novel objective functions to guide our model towards discriminative attention across different categories, leading to improved classification performance as we demonstrate later.
- There has been a growing interest in detecting false information online. Various datasets have been released for developing AI approaches that make a social impact. One line of studies has focused on detecting , a type of web content that attracts an audience and encourages them to click on a link to a particular web page @cite . One study @cite released a manually labeled dataset and developed an SVM model to predict clickbait based on linguistic patterns of news headlines. Using this dataset, researchers suggested a neural network approach that measures textual similarities between the headline and the first paragraph @cite . A national-level clickbait challenge was held, where the goal was to identify social media posts that entice its readers into clicking a link @cite .
- In terms of the network architectures during deep learning, after stacking over ten CNN layers of VGGNet @cite and GoogLeNet @cite , ResNet @cite has been one of the most successful approaches with an identity shortcut connection, as shown in Fig. (a). To achieve better performances, the stochastic depth-based network @cite improved the ResNet architecture by randomly dropping layers during training to allow better information and gradient flow. ResNeXt @cite increased the cardinality of a network without involving many parameters. However, it was assumed that stacking with deep mapping by identical mapping was a good solution. This assumption may not be entirely valid, and thus, we attempted to improve the identical mapping by modifying it in the form of the proposed AWM method.
- Fully convolutional networks (FCNs) @cite have become indispensable models for semantic image segmentation. Many successful applications of FCNs rely on atrous convolutions @cite (to increase the receptive field of the network without down-scaling the image) and dense conditional random fields (CRFs) @cite (either as post-processing @cite or as an integral part of the segmentation model @cite @cite @cite @cite ). Recent efforts have focused on encoder-decoder based models that extract long-range information using encoder networks whose output is passed to decoder networks that generate a high-resolution segmentation prediction. SegNet @cite , U-Net @cite and RefineNet @cite are examples of such models that use different mechanisms for passing information from the encoder to the decoder. SegNet @cite transfers max-pooling indices from encoder to decoder, U-Net @cite introduces skip-connections between encoder-decoder networks and RefineNet @cite proposes multipath refinement in the decoder through long-range residual blocks. Another approach for capturing long-range contextual information is spatial pyramid pooling @cite . ParseNet @cite adds global context features to the spatial features, DeepLabv2 @cite uses atrous spatial pyramid pooling (ASPP), and PSPNet @cite introduces spatial pyramid pooling on several scales for the segmentation problem.
- While other segmentation models may be used, we employ DeepLabv3+ @cite as our segmentation model because it outperforms previous CRF-based DeepLab models using simple factorial output. DeepLabv3+ replaces Deeplabv3's @cite backbone with the Xception network @cite and stacks it with a simple two-level decoder that uses lower-resolution feature maps of the encoder.
- In addition to box annotations, segmentation models may use other forms of weak supervision such as image pixel-level @cite @cite @cite @cite @cite @cite @cite , image label-level @cite , scribbles @cite @cite , point supervision @cite , or web videos @cite . Recently, adversarial learning-based methods @cite @cite have been also proposed for this problem. Our framework is complimentary to other forms of supervision or adversarial training and can be used alongside them.
- In @cite , local features such as Fisher vectors are compared with deep features for aggregation. Traditional hand-crafted features have different distributions of pairwise similarities, which requires careful evaluation of aggregation methods. In @cite , a temporal attention network (TAN) is proposed for multi-object tracking. It adaptively allocates different degrees of attention to different observations and filters out unreliable samples in the trajectory. Very recently, feature pooling has been explored to assess the quality of facial images in a set and sometimes it relies on having to carefully define weighting functions'' to produce intelligent weights. Neural Aggregation Networks (NAN) @cite fuse face features with a set of content adaptive weights using a cascaded attention mechanism to produce a compact representation. An online unsupervised method for face identity learning from unconstrained video streams is proposed in @cite by coupling CNN based face detection and descriptors with a memory based learning mechanism.
- The framework proposed in this paper is inspired from @cite . The authors propose set based feature aggregation network (FAN) for the face verification problem. By generating representative template features using metadata like yaw, pitch and face size, their system could outperform traditional pooling approaches. Instead of building a siamese network, we focus on learning the pooled feature for gallery template (trajectory) while coupling the probe metadata beside gallery metadata. This way, we derive each trajectory's representation by considering the impact or influence of the probe detection in the context of data association.
- Evading Backdoor Attacks: @cite assume existence of clean trusted test data and studied pruning and fine-tuning as two possible strategies for defending against backdoor attacks. Pruning refers to eliminating neurons that are dormant in the DNN when presented with clean data. The authors then show that it is possible to evade pruning defenses by designing pruning-aware' attacks. Finally, they show that a combination of fine-tuning on a small set of clean data together with pruning leads to a more reliable defense that withstands pruning-aware' attacks. While the presented approach in @cite is promising, it comes at the cost of a reduced accuracy of the trained model on clean data. @cite identifies the attack at test time by perturbing or superimposing input images. @cite defends by proactively injecting trapdoors into the models. Such methods, however, do not necessarily detect the existence of backdoor attacks.
- @cite follows the rationale that the neural activations for clean target samples rely on features that the network has learned from the target class. However, these activations for a backdoor triggered sample (i.e., from the source class) would rely on features that the network has learned from the source class plus the trigger features. The authors then leverage this difference in detection mechanism and perform clustering analysis on neural activations of the network to detect infected samples.
- The aforementioned defenses rely on two crucial assumptions: 1) the outliers in the clean dataset (non-infected) do not have a strong effect on the model and 2) more importantly, the user has access to the infected training dataset. These assumptions could be valid to specific scenarios, for instance, when the user trains her his own model based on the dataset provided by a third party. However, in a setting where the user outsources the model training to an untrusted third party, for instance, a Machine Learning as a Service (MLaaS) service provider, or when the user downloads a pre-trained model from an untrusted source, the assumption of having access to infected dataset is not valid. Recently, there has been several work that consider this very case, in which the user has access only to the model and clean data @cite @cite .
- Another interesting approach is Neural Cleanse @cite , in which the authors propose to attack clean images by optimizing for minimal triggers that fool the pre-trained model. The rational here is that the backdoor trigger is a consistent perturbation that produces a classification result to a target class, @math , for any input image in source class @math . Therefore, the authors seek a minimal perturbation that causes the images in the source class to be classified as the target class. The optimal perturbation then could be a potential backdoor trigger. This promising approach is computationally demanding as the attacked source class might not be a priori known and such minimal perturbations should be calculated for potentially all pairs of source and target classes. In addition, a strong prior on the type of backdoor trigger is needed to be able to discriminate a potentially benign minimal perturbation from an actual backdoor trigger.
- Similar to @cite @cite , we also seek an approach for detection of backdoor attacks without the need for the infected training data. We, however, approach the problem from a different angle. In short, we learn universal and transferable set of patterns that serve as a Litmus test for identifying networks containing backdoor Trojan attacks, hence we call them Universal Litmus Patterns. To detect whether a network is poisoned or not, the ULPs are fed through the network and the corresponding outputs (i.e., Logits) are linearly classified to reveal backdoor attacks.
- Deep learning based methods are increasingly used for medical image reconstruction, either by using deep learning for post-processing @cite @cite or by integrating deep learning into the image reconstruction @cite @cite @cite @cite @cite @cite @cite . These papers start by specifying the loss and then use a deep neural network to minimize the expected loss. This essentially amounts to directly computing a Bayes estimator with a risk is given by the loss. The loss is often the squared @math -distance, which implicitly implies that one approximates the conditional mean. Hence, the above approaches could be seen as examples of deep direct estimation. There is however an important difference, in deep direct estimation one starts by explicitly specify the estimator, which then implies the appropriate loss function.
- Regarding sampling from a posterior, conditional generative models @cite @cite have been widely used in the machine learning literature for this purpose. Typical use cases is to sample from a posterior where an image is conditioned on a text, like the bird is yellow'' @cite @cite , but also for simple applications in imaging, including image super-resolution and in-painting @cite @cite @cite . These approaches do not consider sampling from the posterior for more elaborate inverse problems that involve a physics driven data likelihood. An approach in this direction is presented in @cite where variational auto-encoders are used to sample from the posterior of possible segmentations (model parameter) given CT images (data).
- An entirely different class of methods for exploring the posterior are based on MCMC techniques, which have revolutionized mathematical computation and enabled statistical inference within many previously intractable models. Most of the techniques are rooted in solid mathematical theory, but they are limited to cases where the prior model is known in closed form, see surveys in @cite @cite @cite . Furthermore, these MCMC techniques are still computationally unfeasible for large-scale inverse problems, like 3D clinical CT .
- A computationally feasible alternative to MCMC for uncertainty quantification is to consider asymptotic characterizations of the posterior. For many inverse problems, it is possible to prove Bernstein--von Mises type of theorems that characterizes the posterior using analytic expressions assuming the prior is asymptotically uninformative @cite . Such characterizations do not hold for finite data, but assuming a Gaussian process model (data likelihood and prior are both Gaussian) allows for using numerical methods for linear inverse problems @cite . Gaussian process models are however still computationally demanding and it can be hard to design appropriate priors, so @cite @cite introduces (conditional) neural processes that incorporate deep neural networks into Gaussian process models for learning more general priors.
- In their work, @cite focus on the topic of . Therefore the framing bias that they detect has a narrow scope, whereas our work is different in the sense that we aim at capturing a broader scope of biased language. We classify statements that contain biased language which is introduced through words or phrases that are partial or are not neutrally phrased.
- @cite propose an approach for detecting a single bias-inducing word given a biased Wikipedia statement. The approach relies on linguistic features, divided into two bias classes: framing bias, including subjective language such as praising and perspective-specific words; and epistemological bias, dealing with believability of a proposition, i.e. phrasing choices that either cast doubt on a fact or try to sell an opinion as a fact. In their dataset collection, they crawl Wikipedia revisions that have a '' flag in the revision comments. We use a similar dataset collection procedure, however, we additionally use crowdsourcing to filter statements that do not contain bias ( @math 60
- An additional competitor to our work is the work by Hube and Fetahu @cite . Their task is similar to ours, where they gather biased statements from Wikipedia by extracting statements from the right-wing conservative wiki Conservapedia http: www.conservapedia.com . Their approach extends on the work of @cite by introducing features that take into account the context of certain words from lexicons, and additionally including features based on the LIWC text analysis tool @cite . The approach is supervised and one of their best features is a biased word list they construct by identifying bias word clusters in the Wikipedia word2vec word embedding space. We compare against this work, and show that long-range dependencies between words and phrases in a statement are hard to capture through hand-crafted features.
- @cite introduce a RNN model for classifying statements as either liberal or conservative. Their datasets contain statements from politicians in US Congressional floor debates and statements from ideological books about US politics. For pre-selecting biased statements from the data they make use of the features used by @cite and a simple classifier with manually selected partisan unigrams as features. For labeling the pre-selected statements they use crowdsourcing, where crowdworkers label not only the full statement but also each phrase part of the sentence separately in a hiearchical manner. These additional labels allow for a handling of semantic compositions and the correct classification of more complex sentence structures, when the sentence parts are incrementally combined. For example, the statement is classified as liberal bias, even though the term death tax'' suggests pro-conservative bias.
- @cite propose an unsupervised approach for determining the ideology of both users and content in a combined liberal-conservative latent space using Twitter data. They include features such as the surrounding network structure of users and information about content shared by users.
- @cite propose a model to detect the linguistic cues that introduce framing in political events. The results suggest that readership is often unaware of the subtle framing cue words, and that depending on the framing of an event the perception and stances towards an event may vary. The classifier relies on a set of syntactic and lexical features for identifying framing cue words. Similar is the work by @cite , where they propose a topic modeling approach to identify farming words in news articles.
- Some research also covers other types of bias, e.g. selection bias @cite , @cite and bias focusing on specific topics, such as gender bias @cite or cultural bias @cite . We do not consider open opinions to be bias. For example, the statement is not bias according to our definition because the writer makes clear that it is her own opinion.
- Research in CSPM has primarily focused on exploiting special properties of constraints, such as monotonicity or anti-monotonicity, to guarantee the feasibility of pattern extensions in the mining algorithm @cite @cite @cite @cite @cite @cite @cite @cite @cite . Constraint types that do not possess such properties remain a challenge for CSPM algorithms, although some of these have been successfully incorporated in more general item-set mining on databases where events have no specific order @cite @cite @cite @cite @cite , as well as in CSPM when items and attributes are interchangeable @cite .
- Recently, constraint programming (CP) has emerged as a successful tool for CSPM @cite @cite @cite @cite @cite . CP search techniques, albeit general, can potentially be more efficient when compared to specialized CSPM algorithms. Nonetheless, they still rely on constraint-specific properties to effectively prune undesired patterns.For example, @cite show how to effectively implement a number of prefix anti-monotone constraints into CP, but indicate that post-processing is still required to handle monotone constraints such as the minimum span.
- Graphical representations of a database have been shown to be effective in item-set mining @cite @cite @cite and SPM @cite . Previous works have also applied binary decision diagrams as a database modeling tool @cite @cite @cite @cite , which are effective when the sequences of the database are similar, but typically do not scale otherwise. We show that our MDD representation retains its size regardless of the similarity between sequences, and provides a more concise representation in the context of SPM.
- Another substantial area of the literature has been focused in semi - supervised learning. While our methodology can perform semi-supervised learning, there some significant differences. This line of research proposes variants of the same approach: if we have a small training set and a big test set, can the geometry of the latter help the first? For example there have been proposed: extensions of LDA by constructing the nearest neighbor graph for the unlabeled data by cleverly adding a data-dependent regularizer @cite ; a new discriminative least squares regression @cite ; interesting concepts of soft label propagation based on negation that reminds of the recent success of negative sampling in NLP () @cite , as well as random-walk based label propagation @cite .
- From a board point of view, the work in this paper is an endeavor to study the interplay of modal logic with graph game. On this topic, the literature @cite discusses a large number of interesting graph games in tandem with matching logics. Further more, it presents a systematical way of thinking about graph games as a meeting place with logic, and provides us inspiring ideas on research directions and questions.
- In terms of logics, extending the standard modal logic with model-changing operators have already brought about fruitful results. Besides the researches mentioned above, relevant results include dynamic epistemic logic @cite @cite , logic for local announcement @cite , global and local graph modifiers @cite . Further more, we also get some interesting ideas from other researches. Say, @cite shows that some relation-changing logics including SML can be seen as fragments of hybrid logics; and @cite presents some interesting results on a hybrid version of public announcement logic.
- show that a greedy policy could achieve at least @math approximation for the adaptive submodular function. The result could be applied to our offline adaptive problem, but by an independent analysis we show the better result that the greedy policy is optimal. Multi-armed bandit (MAB) problem is initiated by Robbins @cite and extensively studied in @cite @cite @cite . Our online learning algorithm is based on the extensively studied Upper Confidence Bound approach @cite . The non-adaptive community exploration problem in the online setting fits into the general combinatorial multi-armed bandit (CMAB) framework @cite @cite @cite @cite @cite , where the reward is a set function of base arms. The CMAB problem is first studied in @cite , and its regret bound is improved by @cite @cite . We leverage the analysis framework in @cite @cite and prove a tighter bound for our algorithm. define an adaptive submodular maximization problem in bandit setting. Our online adaptive exploration problem is a instance of the problem defined in @cite . We prove a tighter bound than the one in @cite by using the properties of our problem.
- Making existing or enhanced models interpretable , i.e. to provide a precise description of how the model determined its decision (e.g., @cite @cite @cite ).
- Creating a second, simpler-to-understand model, such as a small number of logical expressions, that mostly matches the decisions of the deployed model (e.g., @cite @cite ).
- Leveraging rationales'', explanations'', attributes'', or other privileged information'' in the training data to help improve the accuracy of the algorithms (e.g., @cite @cite @cite @cite @cite @cite @cite @cite
- Work in the natural language processing and computer vision domains that generate rationales explanations derived from input text (e.g., @cite @cite @cite ).
- The third group seeks to generate textual explanations with predictions. For text classification, this involves selecting the minimal necessary content from a text body that is sufficient to trigger the classification. For computer vision @cite , this involves utilizing textual captions in training to automatically generate new textual captions of images that are both descriptive as well as discriminative. While serving to enrich an understanding of the predictions, these systems do not necessarily facilitate an improved ability for a human user to understand system failures. Although promising, it is not clear how these techniques generalize to other domains and if the explanations will be meaningful to the variety of explanation consumers described in .
- End-to-end deep learning methods have also been introduced to take the stereo images as input and output the final disparity map. The model FlowNet ( @cite ) was the first end-to-end CNN to estimate the optical flow. Therewith the DispNet architecture ( @cite ) was applied to the disparity estimation task and presented a synthetic stereo dataset, Scene Flow, which was a large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. A two-stage network called cascade residual learning (CRL) ( @cite ) extended DispNet ( @cite ) by adding a deep residual module to learn its multi-scale residuals. Then the final disparity map was formed by summing the initial disparity with the residual. In addition, GC-Net ( @cite ) learned disparity map by the geometric information and context, and introduced geometric 3D convolution and a soft-argmin layer.
- Recently there has a novel network to extract contextual features. The novel model EdgeStereo ( @cite ) was the first stereo algorithm based on multi-task structure to estimate the disparity map. It was beneficial to both stereo matching task and edge detection task after the multi-task learning. Meanwhile, a pooling context pyramid was proposed to extract multi-scale contextual features in the model. But the model has some disadvantages. First, the pooling operations will lose some information although the pyramid is used to obtain multi-scale information. Second, the calculation of pyramid is large.
- To our knowledge, all of the above algorithms simply provide the features extracted from the earlier layers for the back part of the network except the model EdgeStereo( @cite ). And there is no method to try to obtain more contextual features by merging the element-wise addition with concatenation. Out model is the first stereo method based on fusing various multi-scale features to help stereo matching.
- It has been reported that when the goal is to improve the ASR performance, ideal binary mask (IBM) is more suitable than ideal ratio mask (IRM) or directly mapping @cite to be used to design the SE system. Therefore, we implement an IBM-based SE system in this study. For the IBM-based SE system, the input @math was filtered by IBM to obtain the enhanced output @math : where @math '' represents an element-wise multiplier, and @math is the IBM matrix, which is defined as: where @math is the unit step function applied to each element of @math .
- In deep learning approach, our work is closely related to NNVLP @cite . Being motivated by CNN-LSTM-CRF architecture that achieves state-of-the-art performance in many languages @cite , NNVLP applied this architecture to Vietnamese. In our work, instead of CNNs, we use LSTM to represent character embeddings as in @cite .
- Generative models such as Restricted Boltzmann Machines (RBMs) and Conditional RBMs (cRBMs) have also been used to model audio scenes in high-dimensional representations @cite . These models follow in the tradition of exploring robust feature representations for audio signals and can in fact reliably track multiple audio streams by encoding their regularities over new embedding spaces. In @cite , mixtures of cRBMs were shown to predict unexpected events in a noisy subway station with high precision by locating time windows that deviate from an underlying statistical structure of the scene. In the present study, we employ similar models based on RBMs in order to leverage their generative nature with a focus on onset detection unlike a discriminative method @cite .
- For social media retrieval, introduce Joint Subspace Matrix Factorization (JSMF) @cite . Focusing on the two-class setting, they assume that data points (rows of the data matrix) emerge not only from discriminative but also from common subspaces. JSMF infers for a given nonnegative data matrix and ranks @math and @math a factorization as displayed in Fig. . Multiplicative updates minimize the weighted sum of class-wise computed RSS. In Regularized JSNMF (RJSNMF), a regularization term is used to prevent that shared feature vectors swap into discriminative subspaces and vice versa @cite . The arising optimization problem is solved by the method of Lagrange multipliers. Furthermore, a provisional method to determine the rank automatically is evaluated. However, this involves multiple runs of the algorithm with increasing rank of shared and discriminative subspaces, until the approximation error barely decreases. A pioneering extension to the multi-class case is provided in @cite .
- Miettinen @cite transfers the objective of JSMF into Boolean algebra, solving [ X,Y a 1,2 2 | - ( ) | ] for binary matrices @math and @math , and normalizing constants @math . A variant of the BMF algorithm @cite governs the minimization. A provisional determination of ranks based on the Minimum Description Length (MDL) principle is proposed, computing which of the candidate rank constellations yields the lowest description length. The description length captures model complexity and data fit, and is hence suitable for model order selection @cite @cite .
- In the two-class nonnegative input matrices case, improve over RJSNMF by allowing small deviations from shared patterns in each class @cite . They found that shared patterns are often marginally altered according to the class. In this paper, we aim at finding these overlooked variations of shared patterns together with strident differences among multiple classes, combining the strengths of MDL for rank detection and the latest results in NMF.
- In spite of achieving promising performance, these advances are at the sacrifice of running time and speed. In order to overcome this problem, many lightweight networks, initially designed for image classification task @cite @cite @cite @cite @cite , have been designed to balance the segmentation accuracy and implementing efficiency @cite @cite @cite @cite @cite @cite . ENet @cite is the first work that considers the efficiency issue, where the point-wise convolution is adopted in the residual layer. Apart from this initial designment, some recent work always employ convolution factorization principle @cite @cite @cite in their network architecture, where the 2D standard convolution is replaced by depthwise separable convolution. For example, @cite investigate the high-level label cues to improve performance. ERFNet @cite leverages skip connections and 1D convolutions in residual block designment, greatly reducing network parameters while maintaining high efficiency. In @cite , design an efficient spatial pyramid convolution network for semantic segmentation. Some similar networks also use symmetrical encoder-decoder architecture @cite @cite @cite @cite , while the other approaches take the contextual clues into account @cite @cite to balance performance and efficiency. Unlike these lightweight networks, our ESNet utilizes multiple branch parallel factorized convolution, achieving real-time inference and higher accuracy.
- The overall engagement of students is indisputably one of the main covariates of academic success. For the case of mathematical statistics this has been shown on several occasions. @cite show in a meta-study that the simultaneous usage of traditional classroom lectures and e-assessment has a positive effect on students' success. @cite substantiate the previous result by analyzing the learning activity on the e-assessment platform JACK. The study reveals that learning effort and success, measured by the total number of (correct) submissions on JACK over the course, positively affects the final grade in the exam. @cite add additional R-programming exercises to the JACK framework and show that the newly introduced exercise type helps to improve the general understanding of fundamental statistical concepts and thus ultimately yields better results in the final exam.
- In contrast to previous studies which mostly rely on quantitative and qualitative learning activity measured by time-invariant variables, there is also a temporal dimension of engagement which has been studied from different perspectives throughout the academic literature. @cite use time-dependent information provided by a learning management system to predict academic performance. @cite incorporate students' response time as an additional feature into a random forest to investigate the predictive capability for students' performance and find evidence that it can indeed improve prediction accuracy. @cite and @cite further elaborate on the latter by using more sophisticated techniques and are able to support the preceding result.
- Only a few studies focus on the intraday engagement of students, that is, the actual daytime of learning, as a predictor for academic success. This topic is relevant as various studies show a significant influence of sleep quality and patterns on academic performance @cite @cite @cite @cite . Based on these insights @cite incorporate sleep variables into a prediction setting. With a stepwise regression approach they identify sleep frequency, night outings and sleep quality as among the most important predictors of academic success.
- In games inducing social dilemmas and when the dynamic is accessible as an oracle, cooperative solutions can also be obtained by self-playing and then applied to define a TFT behaviour forcing cooperation @cite , even when opponent actions are unknown, since in that case the reward function already brings sufficient information @cite .
- Closer to our setting, when the dynamic is unknown, online MARL can extract cooperative solution in some non-cooperative games, and particularly in restricted resource appropriation @cite . Using alternative objectives based on all players reward functions and their propensity to cooperate or defect improves and generalizes the emergence of cooperation in non-cooperative games and limits the risk of being exploited by purely selfish agents @cite .
- Extracting knowledge from text is a broad and highly regarded task in science and practice. One distinction is the type of text or documents that is analyzed. It ranges from social media texts, e.g., @cite over regulatory documents, e.g., @cite @cite @cite @cite and business process descriptions, e.g., @cite @cite @cite to historic text analysis as in digital humanities, e.g., @cite .
- Only few approaches target at digitalizing the GPDR such as @cite : here the GDPR is formalized in terms of a declarative notion, the so called DCR graphs.
- Amongst the graph propagation approaches, we mention the equational approaches that assign a more fine grained ranking to arguments by evaluating fixed points of functions that assign a numerical value to any given argument based on the values of its attackers. In particular, the equational approach of Gabbay and Rodrigues @cite who conjecture that their approach yields a unique solution for cyclic graphs, the compensation based semantics of Amgoud et.al. @cite , which assigns the same ranking to all arguments in cycles and yield a unique solution for cyclic graphs, and the social argumentation approach of Leite and Martin () @cite (who are concerned with propagating user votes on arguments, but also yield fine grained rankings without recourse to exogenous information).
- Other graph propagation approaches that do not use an equational fixed point approach, typically account for the attack and defense paths terminating in the argument being ranked, where these paths are sequences of, respectively even and odd, numbers of attacking arguments. Besnard and Hunter () @cite rank classical logic arguments in acyclic graphs, through a function -- @math defined as @math -- that assigns high values to arguments with low-valued attackers (and the maximum value to un-attacked arguments) and low values to arguments with high-valued attackers. Cayrol and Lagasquie-Schiex () @cite then generalises use of this function to Dung @math s to develop (in their terms) a local approach' to valuation of arguments, and then formalise a global approach' that they argue gives more intuitive outcomes. Their approach requires a highly involved transformation of cyclic graphs to infinite acyclic graphs. More recently, Amgoud and Ben-Naim @cite propose two ranking-based semantics, which they call discussion-based () and burden-based (). These semantics are also based on the processing of attack paths and their general applicability relies on conjectures concerning the processing of cyclic paths.
- Finally, Matt and Toni () @cite provide a highly original paradigm for ranking arguments, defining argument strength in terms of the value of a repeated two-person zero-sum strategic game with imperfect information.
- Properties of rankings have been proposed by the above works, and a very informative comparison of these approaches---in particular , , (for acyclic @math s), , and ---in terms of whether or not these properties are satisfied, has been provided in @cite . It is instructive to study these properties as they apply to our graded rankings, in part because such a study reveals fundamental distinctions between propagation based and Dung semantics based approaches to evaluation of arguments.
- In what follows, we refer to absolute graded rankings @math (Definition ), which are the more naturally comparable with the other existing approaches to graduality. Unless stated otherwise, we assume @math stands for any of the semantics in @math . Moreover, we assume the constraint @math and @math . Albeit not essential for the comparison, this assumption streamlines some of the proofs in this section. We now turn to the properties studied in @cite , and discuss whether our approach satisfies each of them. In doing so we will recall which approaches satisfy each property (writing if satisfied by all, and if satisfied by none).
- The long history of computational chemical reaction prediction has been extensively reviewed in @cite and @cite . Methods in the literature may be divided into two different groups, namely, template-based and template-free.
- In this work, we present a fully attention-based model adapted from @cite , the Molecular Transformer, that outperforms all previous methods while being completely atom-mapping independent and not requiring to split the input into reactants and reagents. Similar to the work of Nam & Kim @cite and @cite , our current model is trained end-to-end, fully data-driven and is free of chemical knowledge rules.
- SPAD sensors are photodetectors in which photon radiation can be detected from the resulting large avalanche currents. SPAD sensor arrays are capable of photon counting at a high speed with high timing resolution and are useful in a variety of applications such as fluorescence lifetime imaging microscopy (FLIM), positron emission tomography (PET), time-of-flight imaging etc. @cite @cite @cite . Recently, developed a SPAD array known as SwissSPAD @cite . The SwissSPAD is fabricated in a high-voltage CMOS process and features a large 512 @math 128 array with global gating. In this paper, we use data from different SwissSPAD sensor arrays for demonstrating our high speed video recovery scheme.
- Recent works in range imaging through SPAD sensors include @cite @cite @cite . SPAD cameras have been used to perform challenging tasks such as transient imaging @cite @cite @cite @cite @cite , non-line-of sight imaging @cite @cite @cite @cite and imaging through fog @cite . Closely related to the topic of single photon counting is that of Binary Quanta image sensors @cite @cite . These sensors were developed with the objective of shrinking the pixel pitch. In Quanta imaging, the densely defined sensors oversample the scene radiance to generate binary measurements @cite @cite . Image reconstruction schemes have been proposed for these sensors @cite @cite @cite @cite . The main difference between these reconstruction methods and our scheme is that these algorithms consider the availability of a higher number of samples to predict the image intensity at a pixel location. These methods have mainly focused on recovering a static image. In contrast, our focus is on recovering videos.
- Video denoising has been widely studied for many years and different kinds of algorithms have been proposed. Because of the vastness of the topic, we restrict our discussion to some of the popular and recent works. When compared with applying image denoising on each frame, video denoising has an advantage because the high temporal coherence can be leveraged to make a better prediction. Consequently, denoising algorithms adopt different strategies such as non-local means @cite @cite , motion estimation @cite @cite , 3D dictionary representations @cite etc. @cite propose a denoising method popularly known as VBM4D and is based on the collaborative filtering scheme (VBM3D) @cite . They apply this filtering scheme to a stack of 3-D spatio-temporal volumes that are obtained through non-local grouping. , @cite develop a variational denoising scheme by adaptively combining nonlocal means with total variation on spatio-temporal volumes. Their method adapts to different noise levels. The authors in @cite propose a denoising method for the scenario of mixed noise (like Gaussian noise mixed with impulsive noise). They formulate the denoising problem as low-rank matrix completion problem. This work has been extended in @cite to avoid pre-detection of outliers.
- Recently, techniques based on deep learning have been proposed for video denoising. @cite propose a deep recurrent neural network for video denoising. Their method does not assume a specific noise model and performs close to state-of-the art VBM4D scheme @cite . @cite propose task oriented flow to achieve a specific video processing objective such as denoising. Instead of trying to achieve a precise flow estimation, they train a model whose objective is to predict a motion field tailored for a specific task. In our experiments, we observe that video denoising methods fail in the scenario of SPAD imaging. This could be due to the fact that grouping of similar structures across the spatio-temporal volume fails when the number of photons in the observation is sparse.
- Some attempts have been made to integrate object recognition results into the SLAM system @cite @cite @cite @cite , so the scale information can be obtained from known objects. These methods require training a detector or classifier ahead, and assume that particular objects can be encountered by the robot, which does not usually occur in a real application scenario. Our method does not need pre-training, nor access to a model database during running. The only assumption made in our algorithm is that the line segments in an indoor environment are quantized and form tight clusters, which is usually true in a modern building.
- The specialty of indoor environments has been explored in SLAM related research for over a decade @cite @cite . At low feature level', the abundant line features in buildings are commonly used @cite @cite @cite @cite @cite @cite @cite @cite because they are more robust and contain more structure information than key points. In contrast to using lines for feature matching and structure representation, we observe that not only the appearance of line segments, but the regularity of how they appear can be used to assist SLAM. Specifically, the frequent existence of line segments of the same length and orientation gives the robot a hint to regulate its map built.
- One source of difficulty in a dataset is mislabelled items of data (noise). showed that filtering noise could produce large gains in model performance, potentially yielding larger improvements than hyperparameter optimisation @cite . We ignored noise in this work because it can be reduced with proper data cleaning and is not a part of the true signal of the dataset. We identified four other areas of potential difficulty which we attempt to measure:
- Text classification tasks to predict the 1 - 5 star rating of a review are more difficult than predicting whether a review is positive or negative @cite @cite @cite @cite @cite @cite , as reviews given four stars share many features with those given five stars. describe how as the number of classes in a dataset increases, so does the potential for "confusability" where it becomes difficult to tell classes apart, therefore making a dataset more difficult. Previous work has mostly focused on this confusability - or class interference - as a source of difficulty in machine learning tasks @cite @cite @cite @cite @cite , a common technique being to compute a minimum spanning tree on the data and count the number of edges which link different classes.
- Class diversity provides information about the composition of a dataset by measuring the relative abundances of different classes @cite . Intuitively, it gives a measure of how well a model could do on a dataset without examining any data items and always predicting the most abundant class. Datasets with a single overwhelming class are easy to achieve high accuracies on by always predicting the most abundant class. A measure of diversity is one feature used by to identify datasets which would benefit from multi-task learning.
- Most state-of-the-art methods for extreme multi-label classification can be trichotomized to tree-based, embedding-based and polished one-versus-all approaches. The first branch learns a hierarchical structure of the full label set by recursively dividing feature or label spaces. For instance, FastXML @cite searches for a sparse linear separator to split each node by optimizing an nDCG based loss function. The second branch reduces the effective number of labels under the low-rank assumption. For example, LEML @cite directly optimizes for the decompression matrices using a regularized least square objective function in a generic empirical risk minimization framework. PD-Sparse @cite and DiSMEC @cite are two methods in the third trend that attract considerable attention in the literature. PD-Sparse makes use of both primal and dual sparsity by margin-maximizing loss with @math and @math penalties. DiSMEC revisits one-versus-all paradigm and provides prominent boosts in prediction accuracy and prediction time by explicitly inducing sparsity and doubly parallel training.
- In contrast to the above methods, we develop a pretreatment for those algorithms. The following three approaches are relevant to ours. Label Partitioning for Sub-linear Ranking (LPSR) follows a two-step approach. At the training stage, it clusters the training instances, and then assigns a fixed number of potential labels to each cluster. At the testing stage, a test instance is first put in one of the clusters, and its labels are predicted only from the labels attached to that cluster @cite . While LPSR and our proposal stand on the same testing stage, three differences can be observed at the training stage. First, the optimized number of clusters in our method is selected by the algorithm, whereas the number in LPSR is predefined. Second, cardinality of our label clusters may vary across clusters, while the number of potential labels in each cluster in LPSR is fixed. Moreover, our instance and label clusters are updated via optimization, whereas assignment of training instances in LPSR solely depends on feature matrix. Consequently, our method permits more flexibility in cluster structure.
- The second method, Clustering Based Multi-Label Classification (CBMLC), groups the training instances into a user-specified number of clusters, and trains a multi-label classification model for each cluster @cite . The testing stage is identical to that of LPSR. There are several differences between CBMLC and our approach. First, our goal is to minimize the prediction time while CBMLC focuses on reducing the training time. Second, we formally form a clustering objective to achieve better prediction time using both features and labels, and expedite prediction by reducing label size, whereas CBMLC only takes features into consideration during the clustering step, and label size remains the same. Furthermore, our number of clusters is not user-specified.
- Our work follows the path of investigation proposed by @cite where they used the neural Seq2Seq model @cite to generate dialogue responses. Despite the success achieved by the Seq2Seq model in generating grammatically structured responses, the model fails to output responses that are sensitive to the context of the conversation. @cite built upon the Seq2Seq work by introducing a Hierarchical Recurrent Encoder-Decoder neural network (HRED) that accounts for the conversation history. Precisely, the model captures dependencies over a three-turn conversation history by introducing an additional on top of the RNN encoder. These works have demonstrated promising results, but they typically generate short-sighted and generic responses which lack diversity, making for unengaging conversations. To address this pitfall, @cite suggested using Maximum Mutual Information rather than the traditional Maximum-Likelihood Estimation (MLE) objective function, making the responses more interesting.
- Early work Scene text recognition has drawn lots of attentions during recent years and made significant progress in performance. Early approaches mainly work in a fashion @cite @cite @cite @cite , in which individual characters are detected firstly via sliding window or connected components, and then integrated into a word by dynamic programming or graph models. Character detection or separation by itself, however, is not a completely-solved problem due to complicated background or cursive fonts. Alternatively, @cite considered text recognition as a multi-class classification problem, which assigned a distinct label to each word in a @math k-sized dictionary. Apparently, it is difficult to extend the approach to words out of the dictionary.
- A few papers have constructed theories of SGD implicit regularization. li2017algorithmic prove the implicit regularization effect of SGD on small machine learning models based on the fact that SGD prefers some generalizable local minimas to others @cite . However, these theories does not establish universal conclusions which can be applied to deeper neural networks with more practical activations and the cross-entropy loss function. Such theory construction requires a better understanding to optimization algorithm's behavior for non-linear models and statistics. In this work, we study the unsolved problem base on large-scale and comprehensive empirical studies.
- One study shows that the logistic regression model and 2-layer neural networks using monotone decreasing loss functions tend to converge in the direction of the max-margin solution when using GD and SGD @cite . We further enhance the conclusion by doing studies on more practical deep learning systems.
- conditions for exploding gradients to appear are also provided in @cite . Their result can be understood as a special case of Theorem and is limited to a single layer of vanilla RNN. By constraining the results to this vanilla case, they do not cover interesting nonlinear behaviors, such as periodic behavior and strange attractors, that are studied here. Our results are also more general, because we include bounds on second order derivatives.
- The work in @cite presents a neural network without chaotic behavior. The results presented here, for which the chaotic behavior of LSTMs yield regions in the parameter space where the cost function is highly intricate, densely populated by local minimums and with exploding gradients, is a strong suggestion that this is indeed an interesting line of research that deserves to be further explored.
- It has previously @cite been shown that contractive RNNs can be truncated with arbitrarily small effect on both training and prediction. And, hence, contractive RNNs are not significantly different to feedforward networks. However, when restricted to contractive models, RNNs lose the ability to represent several very interesting and relevant dynamical behaviors, such as limit cycles and multiple fixed points. The work in @cite gives examples of recurrent neural networks that do manage to capture some of these rich nonlinear dynamics and provides some tools to help extract dynamic information from the model. These rich dynamics are non-contractive and constitute a good example of what can be achieved in cases for which the training difficulties presented here are overcome.
- The notion of using entropy for a dynamical system is not new. One classical definition is that of the Kolmogorov-Sinai entropy @cite which is related to how uncertainty (and information) increase with time in a chaotic attractor. However, this definition cannot be applied for regions of the parameter space that do not preserve volume. The definition presented in serves our purpose better, since it does not introduce such restrictions.
- Sentiment analysis can be defined as the process of automatically determining the sentiments, views, emotions, opinions and attitudes towards a social event. According to previous @cite opinions, views, sentiments are used interchangeably but still there is some difference between them.
- According to @cite sentiment analysis is about finding out opinions, identifying the sentiments people express and then classifying their polarity which could be positive, negative or neutral. There are two types of approaches for sentiment analysis - machine learning techniques (supervised and unsupervised) and lexicon based approach. Lexical based approach is further divided in to Dictionary based approach and Corpus based approach. Dictionary based approach relies on the terms collected and annotated manually and grows by adding synonyms and antonyms of the dictionary words, whereas corpus based approach uses a domain specific dictionary.
- In @cite , sentiment analysis has been performed on 104,003 tweets which were collected before the German national election between August 13th and September 19th, 2009. The analyses show that a small section of people are dominating the social media and more than 40 In @cite a survey is presented that talks about different features in news content like linguistic features which are helpful in detecting fake news. There are many other similar tasks, like spam detection, rumour classification, on truth discovery that paper has discussed.
- A similar paper is @cite which has presented a work that identifies fake tweets by using various features from tweets and twitter accounts.
- Reserach @cite uses Naive Bayes Algorithm to analyse public sentiments on approximately 20000 tweets collected from 27th June 2017 to 07th July 2017. The analysis shows that approximately 52 In @cite @cite a technique is used to process and analyse a huge amount of real time data using Hadoop cluster and Naive Bayes approach. The paper focuses on the speed of the sentiment analysis rather than accuracy of the analysis. It processes the data by removing stop words, converting unstructured data to structured data and replacing emotions to their corresponding words.
- In @cite sentiment analysis has been performed on over 2,50,000 tweets mentioning #Microsoft, #Windows, $MSFT etc. using supervised machine learning approach to find out the correlation between stock market movements and sentiments in the tweets.
- In @cite author finds correlation between Cricketers' performances and fans' emotions. Their work shows that fans' emotions depend on players' performance in the tournament. In @cite the author performs the Geo-spatial sentiment analysis for the UK-EU referendum over the Twitter data. It analyses the data to find out the most talked about British politicians and the public sentiments for them. We have used the same approach to remove the noise from tweets as discussed in @cite research.
- In @cite authors have witnessed correlations between the good performance with positive emotions and bad performance with negative emotions. Besides this, they found Trust (a type of positive emotion) as a most entangled emotion corresponding to each performance. Moreover, trading price of commercial brands are found to have transitive relationship with their brand ambassadors' performance in the match. As a reference, they have used Google Trends to verify the influence of players performance all over the globe.
- Parallel document mining has been extensively studied. One standard approach is to identify bitexts using metadata, such as document titles @cite , publication dates @cite @cite , or document structure @cite @cite @cite . However, the metadata related to the documents can often be sparse or unreliable @cite . More recent research has focused on embedding-based approaches, where texts are mapped to an embedding space to calculate their similarity distance and determine whether they are parallel @cite @cite @cite . has studied document-level mining from sentence embeddings using a hyperparameter tuned similarity function, but had limited success compared to the heavily engineered system proposed by .
- In early experiments using convolutional neural networks for place recognition, a feature vector is produced from a particular layer of the network, using all the information that is encoded in the activations of that layer @cite . However, such a whole-image approach is sensitive to viewpoint variations. This was addressed by developing a landmark extraction algorithm and computing the neural responses to each landmark region in a scene @cite . Intelligently selecting the useful information within an image is a valuable method of improving the localization performance. Rather than finding regions, LoST @cite creates a feature vector by extracting semantically meaningful keypoints within the feature map spatial region. @cite finds keypoints by observing the activations out of a late convolutional layer, while @cite trains a soft attention mask to select salient regions within an image to improve the selection of features used to formulate the feature vector. These keypoint feature vectors consist of the activations across all the feature maps within that layer at the spatial location of the keypoint, even if some of the feature maps are encoding visual information that is counter-productive to localizing in the current environment.
- Several experiments compared the performance across different layers @cite @cite , while a number of experiments use multiple layers simultaneously @cite @cite , to improve the visual recognition performance beyond the performance of a single layer. Different layers have been found to encode different types of visual features, such as color and texture in early layers, and objects and scenes in later layers @cite .
- Recent literature on network dissection has provided evidence that individual feature maps encode specific visual features that are relatable to the classifier outputs @cite . In their work, the hidden convolutional layers are probed by testing an individual feature map on a pixel-wise semantic segmentation task. They discover that individual feature maps activate for different objects, scenes, textures and colors. This research underpins the motivation for this work - for example, if a particular feature map activates to man-made lighting, this feature map will confuse the localization between night and day and is better removed from the feature vector.
- In the previous study, the idea of removing the top principal components has been preliminary studied. @cite firstly proposed the idea and applied it to population matrix analysis. In addition, in NLP applications, @cite assume that the maximum variance component of the symbiosis matrix is destroyed by information other than lexical semantics, the removal of the top principal components was proved to be reasonable.
- Unsupervised learning with deep neural networks (DNN) is a relatively new topic. Autoencoders @cite @cite is a typical DNN method to achieve the purpose of feature learning. In most of recent unsupervised DNN-based feature learning algorithms, autoencoders are used as a pretraining procedure to extract hierarchical data features.
- Tian @cite use DNN to optimize the reconstruction loss function between the encoder and the decoder, but the input handcrafted feature is firstly optimized by subspace learning and then optimized by DNN secondly. Later, Peng @cite also input handcrafted computer vision datasets for their DNN model resulting in that this method does not effectively utilize the representation ability of a convolutional neural network. As similar as Peng @cite 's method, Ji @cite also make use of the autoencoder as pretraining and self-expressive property to learn the affinity matrix. The subtle difference that Ji @cite 's method inputs raw image data for a convolutional neural network rather than using handcrafted data in the Peng @cite 's model. Taking inspiration from @math -SNE @cite , Xie @cite define an centroid-based auxiliary target distribution to minimize Kullback-Leibler divergence, with parameters initialized by stacked autoencoders. Based on Xie 's @cite algorithm, Dizaji @cite use cluster assignments frequency as a regularization term to balance the cluster results. Yang @cite jointly optimize a combination of the reconstruction error and @math -means objective function to achieve clustering-friendly' latent representations.
- Inspired by the fact that deep convolutional neural networks can capture feature in a hierarchical way from a low-level to a high level, Chang @cite adopt the curriculum learning to adaptively select labeled samples for training convolutional neural networks and use a strategy to adaptively choose the label features defined by the cosine similarity. Yang @cite dispose of the successive clustering operations in a recurrent process, stacking the convolutional neural networks representations stepwise. Guo @cite take the data structure into account, employing a clustering loss as prior to prevent the feature space from corruption. Tzoreff @cite lay emphasis on the initial process of deep clustering and propose a discriminative pairwise loss function in terms of the autoencoder pretraining. Based on the popular spectral clustering algorithm, Shaham @cite propose a deep neural network with a constraint in the last layer to satisfy the orthogonality property between the feature vectors.
- PD is a simple model that elucidates crucial properties of models in general @cite . As a central model for denoising, it lays the groundwork for CS, deconvolution and inpainting problems @cite . A fundamental signal recovery phase transition in CS is predicted by geometric properties of PD @cite , because the minimax risk for PD is equal to the statistical dimension of the signal class @cite . This quantity is a generalized version of @math introduced above.
- Self-attention: Recently, there have been a number of studies exploring the use of self-attention to improve language decoding . @cite attend over the embedded outputs of the decoder, allowing for non-sequential dependencies between outputs. @cite propose the self-attentive RNN, which attends over its own past hidden states. @cite present the Transformer, an architecture that produces language, without a sequential model ( an LSTM) and instead relies entirely on a number of attention mechanisms, including self-attention in the decoder. We utilize a novel combination of these self-attention mechanisms in our generation, which we refer to as dual self-attention.
- Diversified Decoding: There has been some exploration into diverse language generation, particularly for the task of image captioning. @cite uses a Conditional GAN, along with a policy gradient, to generate diverse captions. @cite presents GroupTalk, a framework which simultaneously learns multiple image caption distributions, to mimic human diversity.
- Crowd management based on data from GPS handheld devices (e.g., smartphones, smartwatches, and personal fitness trackers) has been widely studied. @cite adopted the crowd pressure technique to visualize this information as heat maps, offered a global view of the crowd situation, and assessed different crowd conditions instantaneously throughout an event. @cite implemented a smartphone based crowd management system, which also uses a heat map representation of the crowd state and its evolution. @cite proposed a solution based on Baidu map and developed a prediction model to perceive the crowd anomaly and to assess the risk of the crowd event.
- Agent-based pedestrian crowd simulation is a well-known solution for crowd management. Thus, a variety of simulation frameworks were proposed. @cite proposed a multi-agent framework to simulate emergency evacuation scenarios. @cite introduced an open-source, cross-platform and agent-based crowd simulation framework---Menge. @cite proposed an agent-based crowd simulation and analysis framework and used a case study of Hajj as an example for the assessment of crowd evacuation strategies.
- To analyze the risk of stampedes, a number of stampede measuring techniques were presented in the literature. @cite , @cite derived crowd pressure formulas (by analyzing video recordings of the crowd disaster during the Hajj in 2006) and discovered crowd turbulence phenomenon, in which physical contacts among people are transferred and accumulated. People try to escape the crowd, which causes crowd panic. Such phenomenon may trigger crowd disasters, such as stampedes. Lee and Hughes @cite developed a crowd pressure measuring technique using the standard forward-backward auto-regressive modeling approach. Elliott and Smith @cite examined some sporting disasters and found the relationship between the accidents and the inter-person forces. @cite introduced social and physical forces among pedestrians and then treated each pedestrian as a particle abiding Newton's laws. Lee and Hughes @cite studied the relationship between crushing situation and people density, which gives the typical threshold of people density of different nationalities. However, none of these work has taken into account the impact of GPS noise on the prediction of stampedes.
- Recent research has attempted to combine data from GPS and external sensors to improve the tracking accuracy. @cite developed a positioning technique by integrating GPS and Wi-Fi data (to improve positioning indoors) and accelerometers (to distinguish pedestrian behavior). proposed an approach to utilize overhead Microsoft Kinect depth sensors placed in a walkway to provide reliable automatic pedestrian positioning for tracking @cite . @cite proposed to use infrared cameras to eliminate the multi-path errors which are caused by different positioning systems. Assisted-GPS (A-GPS) technique is also widely used to improve the performance of GPS-enabled smartphones @cite . In general, combining GPS, WiFi, and sensors data improves tracking accuracy indoors (see @cite for review). Besides employing external sensors, various algorithms were developed to mitigate GPS measurement noise, e.g., the Kalman filter @cite @cite and the autoregressive moving average filter @cite .
- As a rule of thumb, the accuracy of GPS can be hugely affected by the environment. The U.S. government claimed that GPS-enabled smartphones are typically accurate to within 4.9 m; however, their accuracy worsens near buildings or other obstacles @cite . Zandbergen and Barbeau's research showed that the horizontal error of GPS-enabled smartphones ranges from 5.0 m to 8.5 m @cite . In 2015, Garnett and Stewart tested two personal GPS devices and two GPS-enabled mobile devices, and they found that the mean relative accuracy ranges from 3.65 m to 6.50 m @cite .
- The idea of using machine learning methods to determine neural network architectures has been popularized by the seminal work of @cite , who have demonstrated that (NAS) is able to find networks that outperform the best human-designed neural architectures. The most obvious drawback of the original NAS method is its resource requirement, as the learning process involves generating a large number of deep neural networks, whose performance is evaluated by fully training them with the given training dataset and evaluating them with the test set, which takes several hours or even days per candidate architecture.
- There is a large body of follow-up work studying attempts to speed up the search. Much attention has been gained by @cite , who have proposed the Efficient Neural Architecture Search method based on the idea to design the search space such that each candidate network is a subgraph of a joint network. The weights of the joint network are trained while an architecture generator (controller) is trained to select the best sub-network for the given dataset. Using this approach, the authors were able to improve the time complexity from thousands of GPU days to less than a single GPU day for the CIFAR-10 dataset while reaching a performance similar to the original NAS results.
- Our findings are consistent with the very recent results of @cite , who have analyzed the behavior of the ENAS controller during search and found that its hidden state does not encode any properties of the generated architecture, providing an explanation of the observation made by @cite that ENAS does not perform better than simple random architecture search. Our work is complementing that line of research by providing an experimental study of the learning progress of ENAS, demonstrating that good performance is already achieved before any search has taken place. This is also in consistent with recent results of @cite , who have experimented with neural network generators for image recognition and have found that, without search, the randomly generated networks achieve state-of-the-art performance.
- Rendering of 3D shapes has many applications. For example, this can be used for designing therapeutic interfaces @cite , interactive pedagogy @cite @cite , shape learning by kindergarten students @cite , learning art and music @cite , virtual and augmented reality applications @cite etc. Users can perform rendering and transformation of 3D shapes by means of interactive participation through natural gestures.
- Lately, gesture based 3D shape creation and recognition has been a topic of research. The VIDEODESK system proposed by @cite can be considered as the pioneering work in this field. They have designed a system that allows a user to control an object's shape by using his her own hands. Whereas utilization of both hands of the user is of good advantage, however, their system uses predefined points of a user's hands. Thus, the system fails to take advantage of full expressive power of both hands. Researchers have shown that, the accuracy can be improved using both hands @cite . Though the improved framework can assign different responsibility to each hand and the method suggests to use bi-manual actions than Krueger's method in the context of 3D shape modeling, the object deformation is only controlled by position and orientation of both hands. Therefore, shape of the hand is not used in true sense. This has been improved by @cite . They have shown that, representation of 3D objects can be improved through bi-manual actions. However, above mentioned methodologies assume that, palm including fingers are detected and tracked precisely for gesture recognition to be effective.
- Though, normal camera-based systems are popular in gesture recognition, however, they have certain disadvantages as compared to IR camera-based or sensor-based systems. For example, vision-based gesture recognition system proposed by @cite suffers from segmentation error. The method proposed by @cite assumes a simple background to avoid segmentation error, which is not realistic. On the other hand, sensor-based systems are more accurate since the signals acquired by IMU sensor are less affected by variations in illumination or segmentation error @cite .
- Despite good accuracy of the sensor-based systems, they have certain drawbacks; (i) Contact-based systems are a burden to the users because often they feel uncomfortable with such artificial attachments @cite (ii) Some of the existing systems require external power through battery @cite . Therefore, contact-less vision-guided systems are preferred for such applications. Researchers have shown that Leap Motion device can be successfully used for palm rehabilitation @cite , upper limb rehabilitation @cite , stroke rehabilitation @cite etc.
- Apart from the operational planning, planning methods have to consider the bidding in electricity markets. Nowadays, producers often base their offers on the given electricity price forecast, which is very volatile due to the variability of RES production and uncertain one day before the energy is delivered @cite . Additionally, the production from RES in the DH system itself is uncertain. Consequently, tools that optimize the operation of DH systems and propose bidding strategies need to consider the uncertainty given by price and production. Despite several bidding strategies for price-taker power producers in the day-ahead market have been proposed, (see references in @cite ), the authors in @cite demonstrate that under high uncertainty of electricity prices the use of stochastic programming @cite for creating bidding curves for the day-ahead market renders good solutions that consider the uncertainty involved in the bidding process. Based on the representation of the uncertain electricity prices as scenarios, the authors use non-anticipativity constraints that order the bids presented to the market in a step-wise manner to create price dependent bids.
- The so far presented methods focus on the day-ahead market trading only. The consideration of bidding in sequential markets is considered, e.g., in @cite , who created bids using stochastic programming in both day-ahead and intra-day markets for an aggregator combining decentralised RES production and consumption without any connection to DH systems. The presented approach first creates bids for the day-ahead market. After this market is cleared, the already committed power production or consumption in the day-ahead market is used to formulate optimal bids for each intra-day market auction throughout the day. Additionally, the standalone participation of different units in sequential electricity markets (especially day-ahead and balancing markets) has been widely discussed in literature (see for instance these sequence bidding strategies for thermal generators @cite , microgrids @cite , wind farms @cite , hydropower @cite or CHP units @cite ).
- . Image quality assessment (IQA) is an important research area. It can be accomplished in three ways: full reference image quality assessment (FR-IQA), reduced reference image quality assessment (RR-IQA), and no reference image quality assessment (NR-IQA). NR-IQA algorithms measure the quality of an image without the need for any reference image or its features. Recently, various strategies have been proposed to measure image quality, including edge detection @cite , natural scene statistics @cite , wavelet decomposition @cite @cite , and human visual system model @cite . In this work, since the rescaling procedure is the major source of visual degradation, we evaluate the resolution of images with their sharpness.
- . Generative adversarial network (GAN) contains two sub-networks: a generator and a discriminator. The framework of GANs is first proposed by Goodfellow al @cite . After that, many researchers focus on improving the stability and visual quality of GANs @cite @cite @cite . In the field of computer vision, GANs are widely used in applications ranging from motion deblurring (DeblurGAN) @cite to texture recovering (SRGAN) @cite . To generate the antithetical training set, we adopt SRGAN @cite for recovering the fine texture details from low-quality images.
- . Person re-identification (ReID) can be split into two subproblems: feature representations and distance metric learning. Over the past decades, many studies focus on designing discriminative features @cite @cite @cite @cite , while others focus on constructing more robust metric learning algorithms @cite @cite @cite . With the rise of deep learning, deeply-learned models have dominated person ReID tasks. Several early works @cite @cite take advantage of the two-stream siamese network and perform the pair-wise comparison in three steps: 1) extracting features from a given pair of images, 2) splitting feature cubes manually and comparing corresponding fractions across images, 3) determining whether these two images belong to the same identity. Attention-based methods @cite @cite provide a more adaptive way for locating different human parts. Unlike these methods which focus on handling the variations of human pose and viewpoint changes, the proposed method tackles another common but crucial problem: the biased image resolution discrepancies in the training data.
- Many research efforts about big data or AI benchmarks have been proposed in recent years. BigBench @cite @cite models a product retailer business model based on TPC-DS and provides a set of queries covering different categories of big data analytics. BigDataBench @cite is a benchmark suite providing dozens of big data workloads, and its latest version 4.0 @cite provides a suite of micro and component big data and AI workloads. CloudSuite @cite is a benchmark suite of emerging scale-out workloads and consists of eight applications. Also, much work about machine learning benchmarks has been presented. Fathom @cite provides an AI benchmark suite consisting of eight deep learning workloads implemented with TensorFlow. BenchNN @cite develops and evaluates the neural network implementations of 5 (out of 12) high performance applications from the PARSEC Benchmark Suite. However, it is frustrating to run these benchmarks on simulators because of their complex software stacks and long running time. In addition, the simulators have limited supports for distributed deployments.
- Using reduced data input is one way to reduce execution time. Previous work @cite @cite adopts reduced data set for the SPEC benchmark and maintains similar architecture behaviors with that of using the full reference data sets. However, it does not suit for big data and AI workloads, since they are data centric computing.
- In terms of micro-architectural simulation, many previous studies generate synthetic benchmarks as proxies @cite @cite . Statistical simulation @cite @cite @cite @cite @cite @cite generates synthetic trace or synthetic benchmarks to mimic micro-architectural performance of long-running real-world workloads, which targets one workload on a specific architecture with the certain configurations, and thus each benchmark needs to be generated on the other architectures with different configurations @cite . Sampled simulation selects a series of sample units for simulation instead of entire instruction stream, which are sampled randomly @cite , periodically @cite @cite or based on phase behavior @cite . @cite accelerate the full-system simulation through characterizing and predicting the performance behavior of OS services. For emerging big data workloads, PerfProx @cite proposes a proxy benchmark generation framework for real-world database applications through characterizing low-level dynamic execution characteristics. The proxy benchmark of PerfProx also needs to be regenerated under different configurations.
- The speaker component is the most crucial aspect of a speaker adaptation methodology as it directly affects the speaker footprint and performance of the adapted model. Here, we could adapt either the entire neural network @cite or all but the output layer @cite of a pre-trained model. However, as mentioned above, this approach is vulnerable to overfitting, so techniques like regularization @cite or early stopping @cite are often introduced in the adaptation stage. Instead of using regularization, the number of adaptable parameters could be reduced as a way to prevent overfitting. The speaker component can be reduced to to just one @cite or a few layers @cite . These layers can be further factorized @cite to discourage the adapted model of the target speaker from straying too far from the initial state. Below, we categorize these factorized methods on the basic of the type of transformation they model within a single token layer of the neural network:
- Most model-based IfO algorithms use an inverse dynamics model, i.e., a mapping from state-transitions to actions. The most related work to ours may be the work of DBLP:journals corr NairCAIAML17 ( DBLP:journals corr NairCAIAML17 ), where they show the learner a single demonstration of an expert performing some task with the intention of the learner replicating the task. They do this by allowing the learner to undergo self-supervision and collect states and actions, which is then used to train a neural network inverse dynamics model. The learned model is then applied on the expert demonstration to infer the expert actions. The actions are then executed to replicate the demonstrated behavior. Another method of this type is behavioral cloning from observation (BCO) by IJCAI2018-torabi ( IJCAI2018-torabi ), which, similarly, first trains an inverse dynamics model in a self-supervised fashion, and applies the learned model on the expert demonstration(s) to infer the expert actions. However, BCO then trains a policy by behavioral cloning (BC) @cite , which maps the expert states to the inferred actions.
- There has been much success of using covariance matrix adaptation evolution strategy (CMA-ES) @cite for derivative-free optimization in reinforcement learning. Salimans2017EvolutionSA ( Salimans2017EvolutionSA ) have noted the scalability of evolutionary algorithms for reinforcement learning tasks. We have also seen much success of applying CMA-ES to skill learning in robot soccer . For example, for walking, AAAI12-MacAlpine ( AAAI12-MacAlpine ) have used CMA-ES to learn an omnidirectional walk engine, which is currently among the best in the RoboCup 3D simulation league. For kicking, LNAI14-Depinet ( LNAI14-Depinet ) develop a method called KSOBI (keyframe sampling, optimization, and behavior integration) that uses CMA-ES to learn a @math m long distance kick.
- In the area of cross-lingual syntactic parsing, there is a notation of universal POS tagset @cite . This tagset is a collection of coarse tags that exists in similar form across languages. Utilizing this tagset and a mapping from language-specific fine-tags, it becomes possible to train a single model in a cross-lingual setting. In this case, the mapping is many-to-one, i.e., a fine-category to a coarse category, thus the models are limited to predict a coarse-grained label.
- CrossCheck training can be viewed as an application of dropout @cite . Each CrossCheck set is trained by dropping out'' output nodes not in its set, and the CrossCheck output can be viewed as averaging a set of sub-models. The voting method to obtain a classification from CrossCheck sets can be seen as an ensemble method, which is known to provide additional robustness and accuracy @cite .
- Traditionally, narrative generation systems like were devised to generate macro-level plot and character elements without connecting them into a comprehensive story in natural language. Later work has sought to combine plot and NLG into a unified system @cite . These full pipelines generate story elements and employ rule-based language models that produce naturally sounding narratives. While these approaches make use of text embellishment, the disadvantage of these rule-based approaches is that rules for text enrichment have to be conceived of in advance by the system's architect. The work presented here, in contrast, seeks to learn a natural language representation from human data to automatically embellish priorly generated narratives.
- This approach can be located in the field of natural language processing. Within this area of research several related subfields can be identified, including text summarisation, statistical machine translation, and most notably, TS. Both, text summarisation and simplification, seek to extract the relevant information of a phrase and lose all linguistic embellishment that is deemed unnecessary for its understanding. Our work has a complementary objective, which is enriching text while maintaining its original meaning. This generative task is related to recent developments in computer vision using Generative Adversarial Networks for problems like image super resolution @cite . Therein similar challenges are faced as new information has to be hallucinated'', requiring particular caution in content sensitive domains.
- UDA problems have been widely investigated for their importance in a number of real world tasks. A major idea to perform domain adaption is to learn domain invariant embeddings by reducing the difference between source and target domain distributions @cite @cite @cite @cite . Among them, Maximum Mean Discrepancy (MMD) and its kernel variants has been a popular target towards minimizing the cross-domain difference of feature distributions. More recently, there has been an increasing interest in using adversarial training based methods to reduce the domain gaps @cite @cite @cite @cite @cite .
- Self-training methods have been widely studied in semi-supervised learning @cite @cite , with applications to vision and natural language processing @cite @cite @cite @cite . Given the inherent relation between UDA and SSL in their forms, @cite addressed image-to-video detector adaptation, by incorporating self-paced sample selection into self-training with a weight shifting policy to gradually increase target domain samples. @cite proposed a variant of co-training @cite for domain adaptation, by jointly learning target predictor, view split, and subset selection in a single optimization problem.
- Recent advances in deep learning have aroused broad interests in semantic segmentation using convolutional neural networks (CNNs). @cite proposed fully convolutional network (FCN) towards pixel-level dense prediction. Since then, several powerful segmentation networks have been proposed, including DeepLab v2 v3 @cite @cite , ResNet-38 @cite , PSPNet @cite etc.
- It is commonly observed that the representation power of a segmentation model does not transfer well to its cross-domain performance. As a result, domain adaptation for semantic segmentation recently emerged as a hot topic. A number of adaptation methods are proposed, including adversarial training at input image level @cite , feature level @cite @cite @cite , and network output level @cite . Specifically, @cite seeks to reduce domain gap by first transferring source images to target style with a cycle consistency loss, and then aligning the cross-domain feature distributions of the task network through adversarial training. In addition, @cite propose to detect non-discriminative samples near decision boundaries through a critic network, and let the generator learn to generate more discriminative features by fooling the critic network with adversarial training. @cite proposed a curriculum adaption method to regularize the distributions of predicted labels in the target domain such that they follow the label distributions in source domain.
- Focusing on the performance of classification algorithms for large-scale data, numerous studies on the intersection of parallel distributed computing and the learning of tree models were proposed. @cite proposed a COMET algorithm based on MapReduce, in which multiple RF ensembles are built on distributed blocks of data. @cite proposed a boosted decision tree ranking algorithm, which addresses the speed and memory constraints by distributed computing. @cite introduced a scalable distributed framework based on MapReduce for the parallel learning of tree models over large datasets. A parallel boosted regression tree algorithm was proposed in @cite for web search ranking, in which a novel method for parallelizing the training of GBRT was performed based on data partitioning and distributed computing.
- Focusing on resource allocation and task-parallel execution in a parallel and distributed environment, @cite implemented a dynamic resource allocation for efficient parallel data processing in a cloud environment. @cite carried out an energy-aware scheduling of MapReduce jobs for big data applications. @cite proposed a robust resource allocation of data processing on a heterogeneous parallel system, in which the arrival time of datasets are uncertainty. @cite proposed an evolutionary scheduling of dynamic multitasking workloads for big data analysis in an elastic cloud. Meanwhile, our team also focused on parallel tasks scheduling on heterogeneous cluster and distributed systems and achieved positive results @cite @cite .
- The combination of Deep Neural Networks and Reinforcement Learning has been the most successful approach to Reinforcement Learning in the last ten years. Most famously DRL has solved ATARI games @cite and the game of Go @cite . ATARI and Go both are essentially discrete state and action space Markov Decision Problems. But, DRL has also been applied to continuous control problems with multiple proposals existing for continuous state and action spaces @cite @cite @cite . In principle our approach is compatible with all discrete time Reinforcement Learning problems. Our approach combines rich (continuous) input from images or other sensori information with symbolic information and we then apply any DRL learning system. Our overall system differs from pure DRL systems by being able to easily incorporate prior and background knowledge available in a first-order grounded language.
- How to combine such systems effectively has led recently to new work on how to use traditional symbolic knowledge representations in Reinforcement Learning. For instance, some recent work introduces symbolic front ends on top of neural back ends @cite . The neural back-end is responsible of conceptual abstraction from the image and maps the raw input to symbolic representations. A symbolic layer then represents information in separate streams for each symbol, before a decision module aggregates them using heuristics. Others have tried to add common sense priors in the heuristic aggregation @cite . The system presented in this paper integrates different sources of information in a single representation before a DRL algorithm can learn to make choices using either symbolic information or raw pixel data.
- @cite constructed a five-layer 3D convolutional deep belief network (CDBN), namely 3D ShapeNet, to learn the probability distribution of 3D voxel grids. @cite considered 3D object classification as a multi-task problem by introducing object orientation prediction. This model achieved excellent performance, which demonstrates that orientation is also an important aspect for 3D object classification @cite @cite .
- 2D images based methods are also important for 3D object classification problem. @cite proposed a multi-view CNN (MVCNN) based technique to aggregate multiple images into concise descriptors in a view pooling layer, which lies in the middle of a 2D CNN framework pre-trained on ImageNet @cite . Multi-view images are also used in 3D object retrieval applications @cite . @cite conducted a comprehensive study on the voxel based and multi-view based CNNs for 3D object classification. According to these works, there are two important factors affecting the model performance: architecture and volume resolution. Therefore, two distinct volumetric networks and multi-resolution filtering technique are proposed. In particular, @cite proposed a group-view convolutional framework which is composed of a hierarchical view-group-shape architecture for correlation modeling towards discriminative 3D shape description. Currently, the state-of-the-art method is @cite , called DSCNN, which can learn feature vectors from multiple views by using a recurrent cluster strategy. In addition to the above methods, a novel method which can directly work on point cloud data attracts increasing attention @cite @cite , but the performance is still worse than multi-view images based approaches.
- Before deep learning era, there are also works that concern several aspects of NER, like leveraging unlabeled data for NER @cite , leveraging external knowledge for NER @cite @cite @cite , nested NER @cite @cite , and NER in informal text @cite @cite .
- In deep learning era, researchers use neural networks and word embeddings to develop variants of models on CoNLL03 dataset @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite .
- Co-training is also an effecitve way to use unlabelled data in supervised learning task, Li and use a bilingual co-training and maximum entropy model to carry out English and Chinese NER. modify the original co-training to cover knowledge from unlabeled data to recognize bio named entities in text. Besides, clustering whould be another research direction for this problem @cite @cite @cite . Our interest is domain NER for Chinese high school news texts. In this paper we also put our proposed ERNN into a co-training approach to see how much improvement can be obtained through large amounts of unlabeled data.
- The idea of off-chain scaling originated from @cite , which has emerged as the most promising PCN practice for Bitcoin, followed by and for Ethereum. There exist extensive literature proposing constructions to improve PCN. Revive @cite , Spider @cite , and Flash @cite propose dynamic routing algorithms to maximize the throughput and success volume of PCN, while Perun @cite introduces to avoid the involvement of the intermediary for each individual payment, thereby, significantly reducing communication complexity.
- Recent literature finds that UCS framework has been widely used to prove the security of off-chain solutions @cite @cite @cite @cite @cite @cite . Given the serious monetary loss when off-chain protocols are massively applied, we suggest that the security of each off-chain protocol should be formally proved.
- Brzozowski and Tamm @cite showed that every regular language defines a unique NFA, which they call . The tomaton is built upon the minimal DFA ( ^ DM ) for the language, defining its states as non-empty intersections of complemented or uncomplemented right languages of ( ^ DM ), i.e., the atoms of the language. They also observed that the atoms correspond to intersections of complemented or uncomplemented left quotients of the language. Then they proved that the tomaton is isomorphic to the reverse automaton of the minimal deterministic DFA for the reverse language.
- In conclusion, we establish a connection between well-known independent minimization methods through Theorem . Given a DFA, the left languages of its states form a partition on words, (P ), and thus, each left language is identified by a state. Intuitively, Moore's algorithm merges states to enforce the condition of Theorem , which results in merging blocks of (P ) that belong to the same Nerode's equivalence class. Note that Hopcroft's partition refinement method @cite achieves the same goal at the end of its execution though, stepwise, the partition computed may differ from Moore's. On the other hand, any co-deterministic NFA satisfies the right-hand side of Equation hence, by Lemma , satisfies the condition of Theorem . Therefore, the double-reversal method, which essentially determinizes a co-determinized NFA, yields the minimal DFA. Finally, the left-right duality (Lemma ) of the language-based equivalences shows that the condition of Theorem is equivalent to that of Brzozowski and Tamm @cite .
- Some of these connections have already been studied in order to offer a better understanding of Brzozowski's double-reversal method @cite @cite @cite @cite . In particular, @cite and @cite offer an alternative view of minimization and determinization methods in a uniform way from a category-theoretical perspective. In contrast, our work revisits these well-known minimization techniques relying on simple language-theoretical notions.
- In @cite @cite @cite , attacks were generated by manipulating the Mel-frequency cepstral coefficients (MFCC) and the coefficients back to the time domain. However, due to the lossy nature of the MFCC computation, some of the information of the original audio signal is lost, limiting the attack space. The raw signal was attacked in @cite @cite in a complete end-to-end fashion. These attacks focused on limiting the size of the adversary based on the signal-to-noise ratio (SNR) only, and did not account for the perceptual distortion introduced by the adversary.
- In @cite these attacks were extended to incorporate typical transformations in the adversarial generation process that an audio signal encounters from source to receiver. This was shown to improve the robustness of the attacks for over-the-air broadcasting but were more perceptible to listeners. Psychoacoustic properties were used to reduce the perceptibility of the adversarial signal in @cite @cite , but both methods encountered instability during back-propagation in the frequency domain. These methods were also not explored in the physical domain, i.e., they were conducted in the pure digital domain. Due to path effects from the speaker to the receiver, as well as additive noise, the success of the adversary may be severely affected in an over-the air scenario.
- Supervised end-to-end approaches have been successfully applied to a variety of challenging control scenarios: off-road obstacle avoidance @cite @cite ; autonomous driving @cite @cite @cite @cite @cite ; vision-based manipulation @cite ; quadrotor control in forested environments @cite @cite @cite , cities @cite @cite , and cluttered environments @cite .
- These can be considered instances of imitation learning (also known as learning-from-demonstration): in fact, one trains a policy to solve a sequential task using only demonstrations by an expert (represented as state action pairs), as training data @cite . In end-to-end approaches, the state coincides with the robot sensing; this yields reactive controllers unless recurrent models are used to capture temporal dynamics @cite .
- The ground truth used for learning may have various origins: skilled drivers or operators @cite @cite @cite @cite ; people walking @cite @cite or driving a vehicle that is not the target robot @cite ; random controllers that sometimes lead to collisions, which the model learns to avoid @cite ; hand-designed controllers @cite ; controllers learned through reinforcement learning @cite ; or the future position of vehicles in a large driving dataset @cite . Most of these approaches are stateless and reactive, but applications of Deep Recurrent Neural Networks allows to capture temporal dynamics in end-to-end approaches @cite .
- The task we consider in this paper is aimed to proximity human-robot interaction @cite @cite : a drone should be able to fly at an appropriate distance in front of people, following them @cite while waiting for possible command gestures @cite . The use of visual markers @cite simplifies this task but, in general, more sophisticated techniques, like Tracking-Learning-Detection @cite , are needed @cite . For instance, several deep learning approaches have been proposed to estimate the 6D pose of a person's head @cite or body @cite from monocular cameras (as well of general objects @cite ). Adopting such perception modules would be a reasonable alternative to solve our task; however, in this paper the task acts as a model of a larger class of tasks.
- We review here four of the best known state-of-the-art student modelling methods for estimating student's performance, either for their predominance in psychometrics (IRT) or Educational Data Mining (BKT), or because they are best performers (PFA, DKT). See @cite for a general review.
- IRT assumes the student knowledge state is static and represented by her proficiency when completing an assessment during an exam @cite @cite @cite @cite . IRT models a single skill and assumes the test items are . It assigns student @math with a static proficiency @math . Each item @math has its own difficulty @math . The main idea of IRT is estimating a probability that student @math answers item @math correctly by using student's ability and item's difficulty. The widely used one-parameter version of IRT, known as the Rasch model, is @size 9 @mathfonts Recently, Wilson @cite proposed an IRT model that outperforms state-of-the-art knowledge tracing models. In which, maximum a posteriori (MAP) estimates of @math and @math are computed using the Newton-Raphson method.
- PFA, which was proposed as an alternative to BKT, also relaxes the static knowledge assumption and models multiple skills simultaneously @cite with its basic structure. It defines the probability of success to an item @math by student @math as: @size 9 @mathfonts where @math is the bias for the skill @math , and @math and @math represent the learning gain per success and failure attempt to skill @math , respectively. @math is the number of successful attempts and @math is the number of failure attempts made by student @math on skill @math @cite .
- The knowledge representation based models originate from representation learning which has many representative structures, e.g., Structured Embedding @cite , Unstructured Model @cite and Trans-family model @cite @cite @cite . In general, the knowledge representation based methods vectorize the entities and relations, and achieve the purpose of KB completion by ranking candidate relations.
- Different from knowledge representation methods, PR is a RW based inference technique. Initially, PR is proposed by Lao2010ML to determine the missing edges by planning the optimal RW path between entities and calculating the joint probabilities of selected paths @cite . The innovation points in the follow-up variant methods can be mainly summarized as the contextual information feature extraction of path @cite , the planning of RW @cite , and the optimization of path similarity calculation @cite .
- There is a vast amount of literature on single image super-resolution using deep learning techniques. @cite is one of the pioneers that demonstrated the potential for convolutional networks to be applied to this research domain, achieving then state-of-art reconstruction quality while maintaining a lightweight structure. The model consists of 3 convolutional layers, each responsible for feature extraction, non-linear mapping to high-resolution patch representations and reconstruction, respectively. Although SRCNN learns an end-to-end mapping between LR and HR images, it is still a fairly shallow network by current standards, and superior performances have been reported with deeper models and other CNN architecture variants. Moreover, since SRCNN can be trained only on a single scale of magnification, it is constrained to work only on the specific scale on which it has been trained. This led us to consider other techniques.
- In @cite , also greedy routing with a modification of face routing is used. To overcome potential bottlenecks which avoid a guaranteed delivery, a restricted flooding procedure is used. In a slightly different setting, namely nodes on the grid, a packet is routed along multiple paths and is hence comparable to a restricted flooding procedure @cite . In their model, alive node and crashed nodes exist on the grid. The crashed nodes behave like obstacles on the grid which have to be avoided by routing paths.
- In addition to the just mentioned local routing, there are also routing strategies that use a portion of global knowledge about the network. BoundHole @cite , for instance, uses a preprocessing phase at each node which is located at the boundary of a hole. These hole nodes send out a packet which is routed using the right hand rule around the perimeter of the hole until it reaches the source of the message. On the way, the packet collects information about the boundary of the hole. With knowledge about the boundary, the authors are able to find better paths than strategies which only use local information. For a survey on all mentioned strategies, we refer the reader to @cite .
- To combine local and global routing strategies, where the goal is to use only few global knowledge, Hybrid Communication Networks have been introduced @cite . Hybrid Communication Networks have also been proposed in different contexts. In practical applications, the term Hybrid Communication Network usually combines wired with wireless networks like in @cite @cite . Closer to our application is the scenario presented in @cite . The authors assume an external network which is not under control of the network participants. The participants can, however, control an internal network. The authors show that the combination of both networks allows to evaluate monitoring problems of the external network much faster than in classical approaches which only use the links of the external network.
- The approach we extend in this work makes use of global information as well @cite . The global information is gathered via a Hybrid Communication Network. In a Hybrid Communication Network, nodes can communicate with other nodes in their ad hoc range for free. In addition, they can use long-range links to communicate with any other node of the network. These long-range links, the Cellular Infrastructure, are costly. The solution they propose is to compute an Overlay Network in which holes are represented by their convex hulls. It is assumed that the convex hulls of the holes do not intersect. The storage requirements for some nodes are asymptotically in the size of the sum of all holes. In this work, we aim to reduce the storage requirements for these nodes and investigate also the challenging question of @math -competitive routing through intersections of hole abstractions.
- Meta-learning models are trained by being subjected to a variety of tasks in training and are then tested in their ability to learn new tasks. The concept is not new @cite @cite , but has become increasingly relevant in modern deep reinforcement learning and imitation learning algorithms @cite @cite @cite @cite @cite @cite @cite @cite . Model-Agnostic Meta Learning (MAML) @cite @cite @cite provides a framework for rapidly adapting gradient-based planners to different (new) tasks by performing a few gradient steps. On a high level, our approach is inspired by MAML in the sense that we have a two-stage computation through gradient descent during training. The inner stage computes a plan given the planner, while the outer stage updates the parameters of the planner, including the weights of the neural network used as the inner stage loss function.
- has been a well studied field of research. show that features extracted from the penultimate layers of convolutional neural networks previously trained on ImageNet @cite outperform handcrafted features for retrieving semantically similar images when using euclidean distance as the distance metric. They further show that it is possible to train these same neural networks using classification data to improve retrieval performance for a given domain of interest.
- The two most related works to this study are @cite and @cite . @cite introduces the hard concrete distribution. They show that incorporating it into a deep neural network sparsifies the learned weights. The authors demonstrate how this sparsification leads to fast convergence and improved generalization. In this study, we focus on a different goal. By applying the stochastic gates only to the input layer, we demonstrate how the hard concrete distribution is useful for parametric feature selection. This allows us to increase the feature size up to a number of thousands of features (as shown in the Section ); this high dimensional regime is common in a field such as bioinformatics. In @cite , the Gumbel-softmax trick is used to develop a framework for interpreting a pre-trained model @cite . The main difference between our work and @cite is that their method is focused on finding a subset of features given a particular instance, whereas our method aims to construct a subset of features based on all the training examples. This approach is more appropriate when we want to apply feature selection methods to data that are known to have consistent important features such as gene data.
- Some authors tackle embedded feature selection problems by extending LASSO and group LASSO to neural network models. Although @cite and @cite have a similar goal as ours, their empirical result does not achieve enough sparsity as practitioners would like. Our approach, which utilizes stochastic gates instead of relying on regularizing @math norms, has an advantage in terms of achieving high sparsity level while maintaining good performance.
- A number of unsupervised object matching methods have been proposed @cite @cite @cite @cite @cite @cite , such as kernelized sorting @cite and Bayesian object matching @cite . However, these methods are not for relational data. In addition, these methods do not scale to large data since they find correspondence by estimating a (probabilistic) permutation matrix with the size of the square of the number of objects. On the other hand, the proposed method scales well since it estimates a projection matrix with the size of the square of the latent space dimensionality, which is much smaller than the number of objects.
- The ReMatch method is an unsupervised cluster matching method for relational data @cite . The ReMatch assigns each object into a cluster that is shared across all datasets, and finds correspondence based on the cluster assignments. Therefore, multiple objects assigned into the same cluster cannot be distinguished, and there would be many ties when objects are ranked based on the estimated correspondence. In contrast, the proposed method estimates different continuous feature representations for different objects.
- The kernel embeddings of distributions and the MMD have been successfully used for various applications, such as a statistical independence test @cite @cite @cite , discriminative learning on probability measures @cite , anomaly detection for group data @cite , density estimation @cite , a three-variable interaction test @cite , a goodness of fit test @cite , supervised object matching @cite , domain adaptation @cite , and deep generative modeling @cite @cite .
- The proposed method is related to graph matching and network alignment methods, which find correspondence so that matched objects have the same edge relation and or the similar attributes @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . The proposed method is different from them, in a way that the proposed method considers not only directly connected edge relation but also the intrinsic hidden structure of the given relational dataset without attributes by embedding objects onto a latent space. The proposed method is based on the recent advancements of graph representation learning methods, such as DeepWalk @cite , LINE @cite and node2vec @cite . These methods obtain node representations that contain structural information in the graph, and have been successfully used in link prediction and node classification. Although spectral methods for graph matching have been proposed @cite @cite , DeepWalk and node2vec achieved better performance than the spectral methods in graph representation learning @cite @cite .
- Approaches are generally either physics-based, statistical, or utilize a combination of both methods. Numerous detailed physics-based models derived from first principles have been developed and used for residential end-use simulation and control @cite @cite @cite . However, the accuracy of physics-based models may suffer due to nonidealities and heterogeneity in appliance characteristics and consumer behavior.
- Various statistical approaches have been developed for non-intrustive load monitoring @cite @cite @cite . Several variants of a factorial hidden Markov model are considered in @cite for load disaggregation. In particular, a conditional factorial hidden semi-Markov model was shown to produce the best performance. In @cite , the authors develop a second order model called a explicit-duration hidden Markov model with differential observations. A hierarchical hidden Markov model was proposed in @cite to represent appliances with multiple operating states. Although these studies develop variants of a HSMM for load modeling, the application is mainly for load disaggregation which usually focuses on a single household. Our goal, on the other hand, is to develop a generic model which can be applied to a broad set of appliances from different households with a particular interest in short-term load prediction.
- Other studies have taken a bottom-up approach to model residential load at the household, substation, or distribution network level, using both statistical and physics-based approaches @cite @cite @cite @cite . These models explicitly model occupancy and different occupant activities using probabilistic methods, but rely on consumer time-of-use surveys to characterize typical'' use patterns. Thus, these methods cannot accurately model use patterns at sufficiently high resolution or account for heterogeneity within the population. Instead, our approach is to implicitly learn use patterns through model parameter estimation, using only historical power consumption data.
- This paper originates from the work described in @cite , where the memory-anonymous mutex problem is introduced, and where first results are presented on mutual exclusion in read write memory-anonymous systems, namely a symmetric deadlock-free algorithm for two processes (the notion of symmetric'' is related to the use of process identifiers; it will be defined in ), a theorem stating there is no deadlock-free algorithm if the number of processes is not known, a necessary condition stating that any symmetric deadlock-free mutex algorithm for @math processes, which uses @math read write registers, requires that @math belongs to the set @math (i.e., @math and @math are relatively prime). Let us observe that @math is infinite (among other values, it contains an infinite number of prime numbers).
- The question about internal organization inside of the list of roles can be rephrased in a more general form: are there any types of relations across roles? Many linguists pointed out that roles are related in some way. They paid attention to the fact that some semantic roles co-occur while the others do not. For example, the role frame Agent, Patient, Instrument represents lots of verbs but the role frames Patient, Experiencer or Beneficiary, Goal do not represent any verb at all. In FrameNet @cite there are 2 types of role-to-role relations -- "requires" and "excludes" -- that indicate for a specific role in a frame which roles are required and which are excluded.
- The second consequence follows from the first one: if it is unable to explain why some frames of roles are valid while the others are not, the obvious conclusion is that roles are not primitive elements in semantic representation. They are not unanalyzable @cite . Treating semantic roles as primitives that are unrelated to one another makes valid role combination purely accidental @cite .
- Verification models usually take a pair of input data and calculate a similarity score between low-dimensional features to determine whether they belong to the same person, which can be penalized by the contrastive loss. @cite extract pedestrian image features by three part-CNNs and compute cosine distance of the images features as their similarity. adds a patch-matching layer to find similar locations and penalizes the similarity regression by softmax loss. What's more, takes triplet samples as input so that images from the same person and the different person can be considered in the mean time. However, verification models only take advantage of weak re-id labels and lack the consideration of all the annotated information.
- Word embeddings are compact vector representations of words learned from a training corpus, and are actively deployed in a number of domains. They not only preserve statistical relationships present in the training data, generally placing commonly co-occurring words close to each other, but they also preserve higher-order syntactic and semantic structure, capturing relationships such as : :: : and : :: : @cite . However, they have been shown to also preserve problematic relationships in the training data, such as : :: : @cite .
- A recent line of work has begun to develop measures to document these biases as well as algorithms to correct for them. introduced a measure of bias in word embeddings and used it to show that word embeddings trained on large public corpora (e.g., Wikipedia, Google News) consistently replicate the known human biases measured by the Implicit Association Test @cite @cite . For example, female terms (e.g., her'', she'', woman'') are closer to family and arts terms than they are to career and math terms, whereas the reverse is true for male terms. developed algorithms to de-bias word embeddings so that problematic relationships are no longer preserved, but unproblematic relationships remain @cite . We build upon this line of work by developing methodology to understand the sources of these biases in word embeddings.
- Recently, Koh and Liang proposed a methodology for using influence functions, a technique from robust statistics, to explain the predictions of a black-box model by tracing the learned state of a model back to individual training examples @cite . Influence functions allow us to efficiently approximate the effect on model parameters of perturbing a training data point. Other efforts to increase the explainability of machine learning models have largely focused on providing visual or textual information to the user as justification for classification or reinforcement learning decisions @cite @cite @cite .
- MAX-SMTI is known to be @math -hard even if each tie occurs at the end of some agent's preference list, ties occur on one side only and each tie is of length two @cite . The special case of MAX-SMTI that asks if an instance of SMTI has a stable matching that matches every man and woman is also @math -complete @cite , and this result holds even when preference lists have lengths of at most 3 and ties occur on one side only @cite .
- MAX-SMTI is also not approximable within a factor of 21 19 @cite unless @math , even if preferences on one side are strictly ordered, and on the other side are either strictly ordered or a tie of length two. The best currently-known performance guarantee is 3 2, achieved first in non-linear running time @cite and later improved to linear time @cite @cite , although better guarantees can be achieved in certain restrictions @cite . Kir 'aly @cite shows how to extend his 3 2-approximation algorithm for MAX-SMTI to HRT.
- The Stable Marriage problem with Ties, Incomplete lists and Symmetric preferences (SMTI-SYM) is a restriction of SMTI-GRP such that (i) for each man--woman pair @math , the rank of @math in @math 's list, i.e., the integer @math such that @math belongs to the @math th tie in @math 's list, is equal to the rank of @math in @math 's list, and (ii) the weight of @math is precisely this integer @math . Finding a maximum size stable matching in an instance of SMTI-SYM is @math -hard, and therefore the same result holds for SMTI-GRP @cite . Given an instance of SMTI-GRP, if the goal is to find a matching that maximises the total weight rather than the total size, this problem is @math -hard also @cite .
- Diebold and Bichler @cite performed a thorough experimental study of eight algorithms for HRT, giving a comparison of these algorithms when applied to real-world HRT instances derived from a course allocation system at the Technical University of Munich. These datasets ranged in size from 18-733 students (the doctors'') and 3-43 courses (the hospitals''). The authors measured a number of attributes of the algorithms, including the sizes of the computed stable matchings. The algorithms that they considered included three exact algorithms for MAX-HRT based on the ILP model presented in @cite .
- Let @math be a network or graph with @math nodes with its adjacency matrix @math , which is an @math matrix. For a binary network, @math if node @math and @math has an observed edge between two nodes and @math otherwise for @math . The graph Laplacian of a graph @math is defined as @math , where @math is a degree matrix such that @math for @math and @math otherwise @cite .
- Several measures have been proposed to describe dissimilarity based on direct observables of the network. One simple way is to count the number of matching edges from two networks @cite , the popular Hamming distance. In @cite , Wilson and Zhu suggested to use the Euclidean distance between the spectra of two adjacency matrices. From a network-theoretic perspective, @cite claimed the discrepancy of node-defined centrality measures be a candidate for dissimilarity measure.
- The graph Laplacian has been known as an approximation of the Laplace-Beltrami operator on smooth manifold underlying observed objects @cite . Since @math contains geometric and topological information of the data via its spectrum, many strategies have been proposed.
- Since the graph Laplacian matrix is symmetric and positive-semidefinite, eigenvalues of @math are nonnegative real numbers. Jakobson and Rivin exploited such phenomenon by defining the distance measure by taking normalized sum of squared differences for top eigenvalues @cite . Instead of using eigenvalues directly, some chose to compute the disparity of two approximated distributions of spectrum. Ipsen and Mikhailov, in @cite , suggested to apply kernel density estimation by convolving narrow Lorentz distributions with computed eigenvalues, while employed discrete histogram through binning @cite .
- Recently, an interesting work adopted diffusion dynamics on a graph to characterize and distinguish networks. The work is based on the manifold learning method called Diffusion Maps @cite , where the distance between two nodes takes all possible paths in between into account across different timescale. This implies that each network and its topological properties can be well presented by the diffusion process. In @cite , adopted such idea to define graph diffusion distance that measures dissimilarity of two networks and showed it is indeed a metric.
- Recently, the strength of deep learning methods for feature learning and pattern recognition @cite @cite , and the strength of CNNs for image processing @cite @cite have provoked their application to medical image analysis. Many initial studies validated the applicability of deep learning to the medical domain using approaches that transfered features learned from a non-medical domain and tuning them to a specific medical task @cite @cite , e.g., classification of the modality of the medical images depicted in research literature @cite and the localization of planes in fet al ultrasound images @cite . Later studies designed new CNNs for specific clinical challenges, such as the classification of interstitial lung disease in CT images @cite .
- Simulation-to-real transfer learning approaches include randomizing the dynamic parameters of the simulation @cite , and varying the visual appearance of the environment @cite , both of which scale linearly or quadratically the amount of computation needed to learn a transfer policy. Other strategies, such as that of Barrett al @cite reuse models trained in simulation to make sim-to-real transfer more efficient, similar to our method, however this work requires an explicit pre-defined mapping between seen and unseen tasks. Saemundson al @cite use meta-learning and learned representations to generalize from pre-trained seen tasks to unseen tasks, however their approach requires that the unseen tasks be very similar to the pre-trained tasks, and is few-shot rather than zero-shot. Our method is zero-shot with respect to real environment samples, and can be used to learn unseen tasks which are significantly out-of-distribution, as well as for composing learned skills in the time domain to achieve unseen tasks which are more complex than the underlying pre-trained task set.
- Our work is closely related to simultaneous work performed by Co-Reyes al @cite . Whereas our method learns an explicit skill representations using pre-chosen skills identified by a known ID, @cite learn an implicit skill representation by clustering trajectories of states and rewards in a latent space. Furthermore, we focus on MPC-based planning in the latent space to achieve robotic tasks learned online with a real robot, while their analysis focuses on the machine learning behind this family of methods and uses simulation experiments.
- Up to this point (and despite their empirical success) there has been very little theoretical work on GNNs---with the notable exceptions of Li et al.'s @cite work connecting GNNs to a special form Laplacian smoothing and 's @ @cite work showing that the feature maps generated by GNNs lie in the same Hilbert space as some popular graph kernels. Moreover, @cite investigates the approximation capabilities of GNNs.
- There are several key assumptions underpinning our results. To guarantee the likelihood can be factored, we need to assume that transitions are uncorrelated. Had we modeled serial correlation in the noise process, @math would be distributed with a tridiagonal covariance @cite , which cannot be factored directly. It is possible to obtain a factorable model by applying a whitening transform. However, this changes the observation process to Monte Carlo samples, which are known to be noisy @cite . Under our simpler set of assumptions, we obtain an efficiently-computable, sparse representation of the value posterior that is amenable to smooth evidence maximization. Section details how to select the hyperparameters and pseudo inputs with gradient-based optimization.
- When it comes to other approximate methods, the state-of-the-art uses a low-rank approximation to the full covariance matrix @cite : @math , where @math is a projection matrix. Before adding new data to the active set, checks if new data points increase the conditional covariance by a desired error threshold, @math . In Section , we compare the approximation quality of and .
- Experience replay was proposed by in 1992 and became popular due to the success of DQN @cite in 2015. In the same year, prioritized experience replay was introduced by as an improvement of the experience replay in DQN. It prioritized the transitions with higher TD-error in the replay buffer to speed up training. This idea is complementary to our method.
- Our method can be considered as a form of curriculum learning @cite @cite @cite @cite @cite @cite . The essence of our method is to assign priority to the achieved trajectories with higher energy, which are relatively difficult to achieve. In RL, curriculum generation can be traced back to 2004. used a program search to construct an asymptotically optimal algorithm to approach the problem. Recently, trained the agent reversely, from the start states near the goal states, gradually to the states far from the goals. Our method bares a similar motivation, but is orthogonal to these previous works and can be combined with them.
- CARSs aim to take into account the users' contextual information, in the most efficient way, in order to propose more relevant and personalized recommendations @cite . So instead of the 2D rating function of traditional RSs ( @math ), in CARSs we have the multidimensional function, @math @cite . The context of a user is composed of a number of context factors like . To each one of these context factors some values can be associated, called context conditions. For example possible context conditions for could be and , and for could be . The integration of contextual information in CARSs can be done by relying on either , or @cite :
- In approaches, contextual information is used to select only appropriate data for the target user's context situation, and then a traditional recommendation technique is applied on this selection. Numerous approaches have been proposed in this category. We can mention: the approach @cite , with its two variants and ; the splitting approaches: @cite , @cite and @cite ; the approach, and its two variants (DCR) and (DCW), proposed in @cite ; and the approach @cite .
- Among these previous approaches, we are especially interested into the following approaches: , and , which are the most similar to our approach. In fact, they try to model the influence of the context on their model, but based on different points of view: @cite models the influence of context on ratings based on the difference between context-free rating and the rating given in the specific context. But, differently from our technique, this influence computation is not user-centric enough because of the way the context-free rating is estimated @cite . In fact they have represented each context condition ( @math ) by a vector ( @math ) containing the item-based or user-based influence of the context condition in ratings. For example in the item-based case, the influence of the context condition @math in ratings of item @math , noted as @math , is computed based on the difference between the rating done by a user @math for item @math in this context situation, @math , and the predicted context-free rating, @math , as follows.
- CAMF @cite is an extension of matrix factorization @cite , that tries to take into account the context situation of users by integrating additional model parameters in the matrix factorization equation. @cite proposed a similarity-based version of this approach.
- For , most of the current state-of-the-art methods exploit the differential flatness of the quadrotors, and solve the planning problem minimizing the squared norm of a derivative of the position @cite , @cite to find a dynamically feasible smooth trajectory @cite . On the one hand, there are approaches where the obstacle constraints, and sometimes also input constraints, are checked after solving the optimization problem: Some of them use stitched polynomial trajectories that pass through several waypoints obtained running RRT-based methods @cite , @cite , @cite . Others use Pontryagin's Minimum Principle to find closed-form solutions @cite . These closed-form solutions are also used to search over the state space @cite , @cite . Alternatively, there are works that use the cost function to penalize the distance to the obstacles @cite , @cite , requiring usually computationally expensive distance fields representations. Finally, there are also approaches that add the obstacle constraints in the optimization problem, like Mixed-Integer Programming (MIP) @cite , convex decompositions @cite , @cite , @cite and Successive Convexification @cite , @cite , @cite .
- Moreover, two main categories can be highlighted in the methods proposed in the literature: memory-less and fused-based methods. The first category includes the approaches that rely only on instantaneous sensing data, using only the last measurement, or weighting the data @cite , @cite . These approaches are in general unable to reason about obstacles observed in the past @cite , @cite . The second category is the fusion-based approach, in which the sensing data are fused into a map, usually in the form of an occupancy grid or distance fields @cite , @cite . Two drawbacks of these approaches are the influence of the estimation error, and the fusion time (which means that the planner usually uses an out-of-date fused map).
- Finally, several approaches have been proposed for the between the planner and the mapper: reactive and map-based planners. Reactive planners often use a memory-less representation of the environment, and closed-form primitives are usually chosen for planning @cite and @cite . These approaches often fail in complex cluttered scenarios. On the other hand, map-based planners usually use occupancy grids or distance fields to represent the environment. These planners either plan all the trajectory at once or implement a Receding Horizon Planning Framework, optimizing trajectories locally and based on a global planner. Moreover, when unknown space is also taken into consideration, several approaches are possible: @cite and @cite used optimistic planners (considering unknown space as free), while in @cite and @cite , an optimistic global planner is used combined with a conservative local planner.
- The transport of DLOs has been recently studied @cite . The feasibility of this task, with three UAVs, was demonstrated in simulation using a particle swarm optimization to get the proper set of PID gains before flight. This work, as with most of the literature of flexible payload transport with UAVs, adopted a centralized controller. Similarly, the impressive body of work of D'Andrea's group at ETH Zurich includes works such three UAVs throwing and catching a ball in a net @cite and the transport of a flexible ring with six UAVs @cite ; but they rely on a fully centralized and latency-free state estimator and controller.
- Distributed or decentralized approaches to the problem are seen less frequently. Distributed collaborative transport was achieved for wheeled robots, pushing a rigid geometric object @cite with a combination of potential field forces. More recently, two UAVs were designed to cooperate in a fully decentralized configuration for the transport of a rigid body using only inertial measurement and vision @cite . Both are inspiring works, but are still far from the target of this work.
- The same type of micro-UAVs used in our experiments, the Crazyflie, was subject to a detailed swarm controller design @cite . We leverage the stability of their on-board control and the scalability of their configuration to deploy our solution. Nevertheless, collaborative payload transport has never been attempted with such a small and sensitive device. Other solutions for controlling a swarm of Crazyflies has been considered @cite @cite , but they are centralized. As with , their scripts are specific to their implementation for this platform, whereas we leveraged a domain-specific programming language for swarms @cite to guarantee portability to other platforms.
- Other application areas of HCA include electronics and computer vision. have employed a HCA to design a test pattern generator for testing very large scale integration circuits specified in hierarchical structural description @cite . The HCA in that study was designed based on the theory of the extension field. In @cite the Authors have introduced a HCA, which consists of single-layer cellular automata and cuboid cellular automata. That HCA was used to detect salient objects, i.e., the most important parts of an image.
- Recovering 3-D transparent objects is known to be a challenging research problem in computer vision, and numerous techniques have been proposed to address this problem. Previous techniques utilized the laser scan with polarization @cite or with a fluorescent liquid @cite for the accurate reconstruction of translucent objects. @cite recovered the 3D shape of transparent objects utilizing a known refractive index and the images captured under controlling the background patterns. @cite analyzed the images recorded by projecting several background patterns, and then reconstructed the shape of axially-symmetric transparent objects. Despite of the impressive quality, their capture systems rely on the controlled environment, unlikely applyable to practical applications.
- Several approaches observe that the depth distortions caused by transparent objects provide meaningful clues for understanding their shapes, even in practical situations. @cite analyzed the light field distortion caused by transparent objects (e.g, a glass) and used it for recognizing the object. However, detecting depth distortion features heavily depends on the type of background; for instance, a textureless background or a scene with repeating patterns degrades the detectability. Torres-G 'omez and Mayol-Cuevas @cite segmented and roughly reconstructed transparent object from multiple color images by carefully stitching hand-crafted features for translucent objects. Yet, their algorithm is limited to a glass object with a spatially smooth surface.
- In any case of depth acquisition, the recorded signal might be the result of combining multiple reflected signals from the source, each travels from different paths. This is namely the multi-path inference (MPI), which causes significant depth errors in a concave object or corner of the scene (indirect bounces), in mildly translucent material such as skin or wax (subsurface scattering), or along dense participating media. Alleviating those errors is an active research topic for various imaging systems @cite @cite @cite . In case of the ToF depth sensors, MPI leads to the depth value farther than ground truth because the longer traveling path results in the larger depth. @cite corrected MPI using a single ToF depth image and without any additional information, by learning from the simulated data, such as simulated indirect bounces and global illumination. @cite used high-frequency illumination patterns to resolve ambiguities in multiple possible travel paths, reducing errors caused by sub-surface scattering.
- Shim and Lee @cite also reported the same analysis, and suggested the depth distortion model based on a ToF depth sensing principle for reconstructing translucent objects. They showed that their model is effective to restore the 3D shape of translucent objects assuming a known background and ignoring the effect of light refraction. Later, Kim and Shim @cite extended the idea of @cite and improved the performance by integrating user interactions. Still, these models assume no refraction, limiting the pose of target object being frontal and the shape being planar. Such assumptions are too strict for practical applications.
- Since AlexNet @cite was announced with outstanding performance in image classification, CNN has been successfully applied in visual recognition tasks. While increasing the number of stacked layers (network depth) of CNN is expected to improve high level feature extraction and boost the performance, the tradeoff is to hinder the training process; it can lead to the gradient vanishing during backpropagation. @cite overcame this drawback by stacking residual blocks, while each block includes an identity mapping to directly link between its input and output. Their invention is called a ResNet architecture, and it is widely applied not only to a visual recognition task or segmentation, but also in more complicated tasks including image restoration @cite @cite , style domain transfer @cite @cite , and depth estimation @cite @cite @cite .
- Image and video understanding has recently gained a lot of attention in deep learning research. Image classification @cite @cite @cite , object detection @cite @cite , semantic segmentation @cite @cite , image captioning @cite @cite , and localized image description @cite tasks have witnessed tremendous progress in the last few years.
- The notion of a space where similar points are close to each other is a key principle of metric learning. The representations obtained from this formulation need to generalize well when the test data has unseen labels. Thus models based on metric learning have been used extensively in the domain of face verification @cite , image retrieval @cite , person-re-identification @cite and zero shot learning @cite . @cite used an auto-encoder model to learn cross modal representations and show results with audio and video datasets. Recently, @cite leveraged this concept to associate data from different modalities. Our work can be seen as an extension of this as we extend it to visual data while linking image and caption spaces to improve image generation.
- Personality impression is an active research topic in psychology and cognitive science. Researchers are interested in studying how different factors, , face, body, profile, motion, influence the formation of personality impression on others @cite . Recent work @cite suggests that facial appearances play an important role in giving personality impressions.
- Data-driven techniques have been successfully applied for 3D modeling @cite @cite . devised deeply-learned generative models for 3D shape synthesis. used Sequential Monte Carlo to guide the procedural generation of 3D models in an efficient manner. Along the direction of face modeling, Saito et al used deep neural networks trained with a high-resolution face database to automatically infer a high-fidelity texture map of an input face image.
- Automatic Annotation. Some prior works annotate objects or gestures by physically tagging them with RFIDs @cite or magnetic sensors @cite . These require active participation by the target and significantly limit scalability and applicable uses. In contrast, our goal is to scalably produce labeled images of passive targets ( e.g. , people, pets with collars, vehicles that already carry WiFi devices). Our work also differs from automatic image annotation that uses visual features ( e.g. , color, texture, shape) to generate image labels. It requires complex generative models that are hard to build @cite .
- New Model Architecture. Transfer learning and semi-supervised learning use local labeled data to adapt well-trained generic models to new scenarios. Self-taught learning and unsupervised feature learning learn features from unlabeled data, but still require a sizable amount of labeled data to train the classifier. Finally, @cite reduces labeling complexity by using coarse-grained labels ( e.g. , image-level labels without object bounding box), but has limited applicability. Our work differs by taking a different (and complementary) perspective, i.e. removing labeling overhead via automation.
- The concept of co-creative PCGML has been previously discussed in the literature @cite @cite , but no prior approaches or systems exist. Comparatively there exist many prior approaches to co-creative or mixed-initiative level design agents without machine learning @cite @cite @cite . Instead these approaches rely upon search or grammar-based approaches @cite @cite @cite . Thus these approaches require significant developer effort to adapt to a novel game.
- It is widely accepted that structural properties of a network play a significant role in determining its actors' behavior @cite @cite @cite @cite @cite @cite . The last decade's abundance of temporal information paved the path to a further understanding of the dynamics of networks @cite , and findings corroborate that structural properties have a prominent affect on the longitudinal dynamics of networks and their actors @cite @cite @cite @cite @cite .
- In stochastic models of networks, change points are points in time where a change in the system's norm is detected in a manner that can be significantly differentiated from plain stochastic noise @cite @cite @cite @cite @cite . McCulloh & Carley @cite convert the series of networks to a time series of scalar values for different network measures, and looked for a stable change in these values (as opposed to temporal change, when looking for anomaly detection) using process mining techniques for change points detection @cite @cite @cite . Change point detection frameworks @cite commonly consist of three phases: (1) generate longitudinal graphs; (2) capture representative features of each graph; (3) detect significant distance between graph snapshots, or window of snapshots, to specify changes.
- PAYL @cite uses unsupervised learning, creating profiles of byte frequencies distributions per host and port connected to an application. They then decide whether the behavior is anomalous by testing if the Mahalanobis distance of the operational profile from the precomputed one is higher than a threshold set during the training period. Thus, this method is domain specific and uses only features extracted from the network activity of each application.
- Deep learning is already being used for malware classification successfully. @cite presented a successful malware classifier that uses random projections for initial dimensionality reduction, and a neural network classifier to distinguish between malicious families and benign files. However, their method does not address the task of differentiating between previously seen and unseen malware families.
- Baquero @cite proposed a classifier to predict the programming language of a Stack Overflow question. They extracted a set of @math questions from Stack Overflow that contained text and code snippets, @math questions for each of @math programming languages. They trained two classifiers using a Support Vector Machine model on two different datasets which are text body and code snippet features. The evaluation achieved an accuracy of 60 summarizes our results in comparison to @cite as, to the best of our knowledge, it is the only previous work in the literature that tackles the problem of predicting programming language tags for Stack Overflow questions.
- Rekha @cite proposed a hybrid auto-tagging system that suggests tags to users who create questions. When the post contains a code snippet, the system detects the programming language based on the code snippets and suggests many tags to users. Multinomial Naive Bayes (MNB) was trained and tested for the proposed classifier which achieved 72 @cite converted Stack Overflow questions into vectors, and then trained a Support Vector Machine using these vectors and suggested tags used the model obtained. The tag prediction accuracy with this model is 68.47 it works well for some specific tags, it is not effective with some popular tags such as Java. Stanley and Byrne @cite used a cognitive-inspired Bayesian probabilistic model to choose the most suitable tag for a post. This is the tag with the highest probability of being correct given the a priori tag probabilities. However, this model normalizes the top for all questions, so it is unable to differentiate between a post where the top predicted tag is certain, and a post where the top predicted tag is questionable. As a consequence, the accuracy is only 65
- Rank aggregation problem (RAP) targets an optimal rank that best represents a set of given base ranks. RAP is NP-hard for more than three input ranks @cite . From a theoretical perspective, some heuristics have been proposed to find approximate solutions, based on, among other strategies, branch-and-bound search @cite and evolutionary algorithms @cite @cite @cite @cite .
- Unsupervised rank aggregation functions work without relying on labeled training data. For that, they can be based on data discrimination or summarization strategies, such as rank position averaging @cite @cite @cite , retrieval score combination @cite @cite , correlation analysis @cite @cite , or clustering @cite .
- Existing graph-based methods are mostly targeted at modelling the whole collection of objects as a graph, from which the ranks can be derived @cite @cite @cite @cite @cite . Among these graph-based methods, a state-of-the-art unsupervised rank aggregation approach has been proposed recently, based on graph-based rank aggregation representations followed by similarity computations over those structures @cite . This initiative -- referred to here as -- was novel in establishing an aggregation representation, and overcame most previous baselines in retrieval effectiveness. Its shortcoming, as in other previous works, is about the retrieval performance, as the query retrieval times are asymptotic linear to the collection size. In this paper, we adopt as our main baseline, for effectiveness and efficiency comparative analysis.
- Graph embedding approaches have been effective in multiple scenarios involving graph databases, because vector representations from graphs usually promote better scalability, and they also have more existing mining and search functions at disposal. An embedding acts as a mapping function from a graph domain to a multidimensional vector space. proposed a map that roughly preserves distances between those domains, but to spaces of high dimensionality. This map can also be based on statistics of vertex attributes and edge attributes @cite , prototypes @cite , or graph kernels @cite . Among these initiatives, Bag of Graphs (BoG) @cite was introduced as a general unsupervised framework for graph embedding that allows graphs to be represented as vectors based on common local graph patterns. Despite BoG is targeted for any graph scenario, it requires some functions to explicitly defined concerning the target scenario. BoG has been extended for some scenarios already, such as for text classification and retrieval @cite . proposed a learning method for embedding representations of entities and relations previously modeled by graphs.
- Since making dense depth measurements is slow and expensive, it is useful to be able to recover a high quality dense depth map from a small number of direct measurements, by exploiting monocular cues from a color image. A popular way of combining color information with partial measurements is by requiring color and depth edges to co-occur. This approach is often successful for depth inpainting'', i.e., filling in gaps of missing measurements in a depth map (common in measurements from structured light sensors). A notable and commonly-used example is the colorization method of Levin al @cite . Other methods along this line include @cite @cite @cite @cite @cite , while Zhang and Funkhouser @cite used a neural network to predict normals and occlusion boundaries to aid inpainting.
- However, when working with a very small number of measurements, the task is significantly more challenging (see discussion in @cite ) and requires relying more heavily on the monocular cue. In this regime, the solution has been to train a network that takes the color image and the provided sparse samples as input. Researchers have demonstrated the efficacy of this approach with measurements along a single horizontal line from a line sensor @cite , random sparse measurements @cite @cite @cite @cite @cite , and sub-sampled measurements on a regular grid @cite @cite @cite . Moreover, each of these methods also train separate networks for different settings, such as different networks for different sparsity levels in @cite , and different resolution grids in @cite .
- Monocular depth estimators commonly output a single estimate of the depth value at each pixel, preventing their use in different estimation settings. Some existing methods do produce distributional outputs, but as per-pixel variance maps @cite @cite or per-pixel probability distributions @cite . Note that depth values at different locations are not statistically independent, i.e., different values at different locations may be plausible independently, but not in combination. Thus, per-pixel distributions provide only a limited characterization that, while useful in some applications, can not be used more generally, , to infer relative depth, or spatially propagate information from sparse measurements.
- Like us, Chakrabarti al @cite also consider joint distributions over local depth values, albeit to eventually produce a depth map. They use a factorization into independent distributions for different depth derivatives, and train a network to output these distributions. But, their outputs do not provide a way to solve other inference tasks (this was not their goal). Also, their factorization into pre-selected derivatives with a fixed parametric form is still a restrictive assumption that does not fully capture local depth dependencies.
- In this work, we use a more general form for the conditional joint distribution of depth values in local regions. We train a conditional GAN @cite @cite to produce multiple estimates of depth in local patches from an image input. Conditional GANs have been used to produce outputs that are more natural'' than those from networks trained with regression loss alone @cite . In our case, we run our GAN model multiple times to generate multiple plausible estimates, treat these as samples from a distribution, and use these samples to approximate the distribution itself.
- Recently, end-to-end deep reinforcement learning has been shown to learn complex behaviours in virtual environments @cite @cite @cite . However, these approaches require large amounts of experience and can be unstable due to the complex optimization challenges of training large neural networks on reward signals alone. Such approaches are continually being developed, with improvements including improved sample-efficiency and stability @cite @cite @cite @cite @cite .
- The application of deep reinforcement learning to robots in the real world is an emerging research area, with significant potential to provide robots with intelligent decision-making tools @cite @cite @cite with robotic manipulation being a prominent challenge @cite . Sample-efficiency remains a significant limitation, with these approaches requiring millions of training samples, translating to a restrictive amount of training time and hardware resources @cite @cite . Simulations provide a platform to train robot policies, which can then be transferred, without the implications of training robots in the real world @cite @cite @cite . Restricted simulation accuracy results in an unavoidable reality gap @cite when attempting to simulate perception and control at a fine-grained level. Ongoing research in this field is concerned with exploring means to improve robustness to environmental changes to more easily handle the intricacies of the real world, such as domain randomization @cite @cite @cite . Despite impressive progress, these approaches are still limited significantly by the resources required.
- There are various approaches to detection of inappropriate comments in online discussions @cite . The most common approach is to detect inappropriate texts through machine learning. Features used include bag of words @cite , lexicons @cite , linguistic, syntactic and sentiment features @cite , Latent Dirichlet Allocation features @cite or comment embeddings @cite . Deep learning was also considered to tackle this issue @cite @cite .
- Apart from detecting inappropriate texts, multiple works focus on detecting users that should be banned @cite by analyzing their posts and their activity in general @cite @cite , their relationships with other users @cite and the reaction of other users @cite or moderators @cite towards them.
- Most of the works deal with interpretability of computer vision models @cite , but progress in interpretable text processing was also made. Several works try to analyze the dynamics of what is happening inside the neural network. focus on memory cells activations. compute how much individual input units contribute to the final decision. Other techniques rely on attention mechanisms @cite , contextual decomposition @cite , representation erasure @cite or relevance propagation @cite . Our work uses the method by , which selects coherent subset of words responsible for neural network decision. The authors use this model to explain multi-aspect sentiment analysis over beer reviews and information retrieval over CQA system.
- Broadcasting algorithms have been widely optimized for speed, throughput, and energy consumption. A lot of algorithms apply MAC (medium access control) protocols like TDMA (Time Division Multiple Access) @cite @cite @cite @cite , CDMA (Code Division Multiple Access) @cite @cite , FDMA (Frequency Division Multiple Access) @cite to increase spatial reuse. Physical models with high path loss exponent @math are beneficial here and increase the spatial reuse with only local interference. With spatial reuse, parallel point-to-point communications are possible which either spread the same broadcast message in the network or pipeline multiple broadcast messages at the same time. The latter can achieve a constant broadcasting rate for path loss exponent @math . Cooperative transmission with MISO (Multiple Input Single Output) is applied to increase the transmission range and broadcast speed by a constant factor (where underlying MAC protocols still work).
- Broadcasting has been first considered for a graph based model, where interference prevents communication and a choice has to be made which link should be used for propagation. Since we do not consider interference and allow the usage of all links a simple flooding algorithm achieves the optimal bound of the diameter of the network. So, these works (see @cite for a survey) do not apply here. However, even if interference is considered there is only a constant factor slow down in the Unit-Disk-Graph model @cite . Note that Unit-Disk-Graphs are connected, when the node density of the randomly placed nodes is large enough @cite .
- Launched by the seminal paper of @cite , the SNR (Signal to Noise Ratio) model has gained a lot of interest. Here, signals can be received if the energy of the sending nodes is a constant factor larger than the sum of noise energy and interference. This model leads to a smooth receiver area with near convexity properties @cite .
- When the energy of each sender is constrained, in @cite it is shown that the SNR-model does not give much improvement compared to the UDG-model. So, @cite incorporates the unit disk model with the SINR (Signal to Interference and Noise) model. The focus are TDMA scheduling schemes to enhance the network capacity. The path-loss exponent in the SINR model is chosen with @math and interference has only local effects for unsynchronized transmitters. In this context, the SNR model is used for each sender separately. So, the problem of broadcast mainly reduces to range assignment and scheduling problem, for which the number of rounds approaches the diameter @cite .
- A transmission with cooperative beamforming requires phase synchronization of the collaborating transmitters to produce a beam and sharing the data to transmit. The authors of @cite present therefore a two phase scheme: in phase one, the message is spread among nodes in a disk in the plane around the node holding the original message. The open-loop and closed-loop approach can be used to synchronize nodes to the destination or a known node position and time synchronization. In phase two, the synchronized nodes jointly transmit the message towards the destination.
- For the MIMO model in @cite and @cite the authors give a recursive construction, which allows a capacity of @math for @math senders using MIMO communication. However, in @cite , an upper bound of @math has been proved. This seeming contradiction has been addressed in @cite , where they address the question whether distributed MIMO provides significant capacity gain over traditional multi-hop in large ad hoc networks with @math source-destination pairs randomly distributed over an area A. It turns out that the capacity depends on the ratio @math , it describes the spatial degree of freedom. If it is larger than @math it allows @math degrees of freedom @cite , if it is less than @math the bound of @cite holds. For all regimes optimal constructions are provided in these papers. While in @cite path loss exponents @math are considered, for @math the regularity of the node placement must be taken into account @cite .
- While this research is largely concerned with the diversity gain, we study the physical limitations of the energy gain. In @cite @cite , a method is presented to amplify the signal by using spatially distributed nodes. They explore the trade-off between energy efficiency and spectral efficiency with respect to network size. In @cite , a distributed algorithm is presented in which rectangular collaborative clusters of increasing size are used to produce stronger signal beams. In @cite , the asymptotic behavior of the rounds for a unicast has been analyzed in great detail and an upper and lower bound of @math rounds has been proven. If the nodes are placed on the line it takes an exponential number of rounds @cite . The generalization of these observations for different path loss models can be found in @cite . In @cite it is shown that the sum of all cooperating sender power can be reduced to the order of one sender, while maintaining a logarithmic number of rounds to send a message over an @math hop distance.
- The authors of @cite consider a system model similar to our work. They use two phase opportunistic broadcasting to achieve linear increase in propagation distance. In Phase 1, nodes inside a disk of specific radius broadcast message with different random phases while in Phase 2, a node broadcasts the message to its neighboring nodes. These phases are performed repeatedly to broadcast the message. Improving on this work we obtain better bounds by coordinating the phase of the nodes, while we consider only the path loss factor of @math .
- This work involves around two broad strategies, namely community structure analysis and structural network vulnerability assessment. Numerous approaches have been developed and applied to detect community structure. For instance, a hierarchical agglomerative algorithm was proposed by @cite . A much more extensive literature can be found in an excellent survey which describes all approaches derived to solve the problem @cite .
- Assessing the structural network vulnerability has received increasing attention. For example, @cite have proposed a Community Vulnerability Assessment (CVA) problem and suggested multiple heuristic based algorithms based on the modularity measure of communities in the network. These approaches are restricted to the scope of online social networks and do not cater to general network structures. Another work by @cite explored the number of connected triplets in a network as they capture the strong connection of communities in social networks.
- @cite proposed an approach where the resilience of the community structure is quantified by first introducing disruption in the original network and measuring the change in the community structure temporally i.e., after the disconnection and during the restoration process. @cite provide a review of various approaches that make use of the facility importance concept to understand the system wide vulnerability. @cite proposed a new vulnerability metric where they considered a combination of external and internal factors such as connection density etc. These methods allow us to quantify the vulnerability of a community, but does not provide us with a set of nodes that contribute to the vulnerability of the community itself.
- Most related to our work is a recent work @cite addressing the same task as ours, that of converting a time series dataset into a set of vector embeddings. Starting from a dataset of @math time series, they build an @math matrix of pairwise similarities that is defined using DTW distances. This is done by performing @math DTW computations, and approximating the other entries in the matrix using a low-rank assumption. If a time series is of length @math , a DTW computation takes time of the order of @math . This makes just the matrix creation step a process in @math . The similarity matrix is then subject to symmetric matrix factorization so that a vector embedding matrix X is learnt such that XX @math approximates the similarity matrix well. This calls for eigen decomposition which is superlinear in @math as well. This makes it impractical for most large datasets. In contrast to this work, the method we propose is linear in both @math and @math , making our method significantly more scalable.
- There are two more recent methods that consider learning embeddings to approximate DTW distances. The first, called Jiffy @cite , targets the case of multi-variate time series, and proposes a convolutional neural architecture. We differ from their task setting in that we consider the univariate setting for devising our method. The second method @cite models the embedding task as one of identifying a specified number of shapelets of specified lengths (both being method parameters) that enable transforming time series data into embeddings closely approximating DTW distances. Our model is not constrained to produce shapelets, and targets to train a neural architecture that embodies the transformation within itself.
- Another recent work @cite makes use of convolutional neural networks in a framework heavily inspired by word2vec @cite . They train a neural network so that it learns an embedding for a time series subsequence that would be closer to the embedding of the larger sequence containing it (positive example) than to a different random series (negative example). SVM classifiers trained over such embeddings are shown to be better than DTW 1-NN classifier in accuracy. In contrast with their intent of learning representations that are more suitable to train SVMs, our intent is to ensure that the distances between embeddings are meaningful locally. This intent entails a different evaluation target, that of optimizing for performance of a 1-NN (or k-NN) classifier over the embeddings.
- Aggelopoulou and colleagues presented in @cite one of the first works using computer vision techniques to detect flowers. That method is based on color thresholding and requires image acquisition at specific daylight times, with the presence of a black cloth screen behind the trees. Thus, although its reported error in predicted yield is relatively low ( @math 84.3 More advanced computer vision techniques have been employed for fruit quantification @cite . A multi-class image segmentation for agrovision is proposed by in @cite , classifying image pixels into leaves, almonds, trunk, ground and sky. Their method combines sparse autoencoders for feature extraction, logistic regression for label associations and conditional random fields to model correlations between pixels. Some other methods are based on support vector machine (SVM) classifiers that use information obtained from different shape descriptors and color spaces as input @cite @cite . Compared to existing methods for flower detection, these methods are more robust since morphological characteristics are taken into account. As many other shape-based and spectral-based approaches @cite @cite @cite @cite , these techniques are, however, still limited by background clutter and variable lighting conditions in orchards @cite .
- Recent works on fruit quantification include the use of metadata information. Bargoti and colleagues in @cite built on @cite to propose an approach that considers pixel positions, orchard row numbers and the position of the sun relative to the camera. Similarly, @cite proposed the use of information such as fruit number, fruit area, area of apple clusters and foliage area to improve accuracy of early yield prediction, especially in scenarios with significant occlusion. However, the inclusion of metadata is highly prone to overfitting, particularly when limited training data is available and the variability of the training set is hence low @cite .
- Following the success of Krizhevsky's model @cite in the ImageNet 2012 Challenge, deep learning methods based on CNNs became the dominant approach in many computer vision tasks. The architecture of traditional CNNs consists of a fixed-size input, multiple convolutional layers, pooling (downsampling) layers and fully connected layers @cite . Winner of the ImageNet 2013 Classification task, the Clarifai model is one such network @cite . Illustrated on the right side of Figure , it takes input image portraits of size @math pixels, which traverse a composition of @math convolutional layers (C1-C5) and @math fully connected layers (FC6-FC7 and the softmax FC8). Each type of layer plays a different role within the CNN architecture: while convolutional layers allow feature extraction, the latter fully connected layers act on this information to perform classification.
- Traditional deep CNNs are composed of millions of learned parameters (over @math million in AlexNet @cite ), such that large amounts of labeled data are required for network training. Deep learning models became feasible relatively recently, after the introduction of large publicly available datasets, of graphics processing units (GPUs), and of training algorithms that exploit GPUs to efficiently handle large amounts of data @cite . Nevertheless, gathering domain specific training data is an expensive task. One alternative to reduce the required amount of labeled data is data augmentation, a technique proven to benefit the training of multiple machine learning models @cite . It is typically performed by applying transformations such as translation, rotation and color space shifts to pre-labeled data.
- In addition, transfer learning approaches such as fine-tuning have been investigated. Earlier layers of a deep network tend to contain more generic information (low-level features), which are then combined by the latter layers into task specific objects of interest. Thus, a network that can recognize different objects present in a large dataset must contain a set of low-level descriptors robust enough to characterize a wide range of patterns. Under this premise, fine-tuning procedures typically aim at adjusting the higher-level part of a network pre-trained on a large generic dataset, rather than training the full network from scratch. This greatly reduces the need for task-specific data, since only a smaller set of parameters has to be refined for the particular application @cite .
- At the classification side, most CNN architectures employ fully connected layers for final categorization. They determine which features are mostly correlated to each specific class employing a logistic regression classifier. For scenarios in which the output is binary, consistent albeit small improvements on popular datasets have been demonstrated by replacing the final CNN layer by a SVM classifier @cite . SVM models tend to generalize better than logistic regression, since they target a solution that not only minimizes the training error, but also maximizes the margin distance between classes.
- Following the success of CNNs on image classification tasks, the work of @cite introduced the concept of region-based CNNs (R-CNN), outperforming by a large margin previous hand-engineered methods for object detection. In that work, a CNN is first pre-trained on a large auxiliary dataset (ImageNet) and then fine-tuned using a smaller but more specific dataset (PASCAL dataset for object detection). The Faster R-CNN proposed in @cite improved this model by replacing selective search @cite with the concept of Region Proposal Network (RPN), which shares convolutional layers with the classification network. Both modules compose a single, unified network for object detection.
- Recent works adapt the Faster R-CNN for fruit detection. Bargoti and Underwood in @cite present a Faster R-CNN trained for detection of mangoes, almonds and apples fruits on trees. in @cite extended this model for tracking and localization of mangoes, combining it with a monocular multi-view tracking module that relies on a GPS system. in @cite applied the Faster R-CNN to RGB and near-infrared multi-modal images. Each modality was fine-tuned independently, with optimal results obtained using a late fusion approach. Still in the context of agricultural applications, CNNs have been also successfully used for plant identification from leaf vein patterns @cite .
- Research in blockchain and cryptocurrencies has gained a significant momentum over the years @cite . In what follows, we present the background and related work and contrast it to ours.
- Blockchain analysis systems parse and analyze raw transaction data for many applications. Recently, proposed BlockSci @cite , an open-source, scalable blockchain analysis system that supports various blockchains and analysis tasks. BlockSci incorporates an in-memory, analytical database, which makes it several hundred times faster than its contenders. While there is a minimal support for tagging in its programming interface, BlockSci is designed for analysis of core blockchain data. At the cost of performance, annotation and tagging can be integrated into the analysis pipeline through a centralized, transactional database. For example, proposed BitIodine @cite , an open-source blockchain analysis system that supports tagging through address labels. However, BitIodine, relies on Neo4j @cite , a general-purpose graph database that is not designed for blockchain data and its append-only nature, which makes it inefficient for common blockchain analysis tasks, such as address linking. In contrast, BlockTag is the first open-source tagging system that fills this role.
- The impact of Bitcoin address linking on user anonymity and privacy has been known for a while now @cite @cite @cite @cite . Fergal and Martin @cite showed that passive analysis of public Bitcoin information can lead to a serious information leakage. They constructed two graphs representing transactions and users from Bitcoin's blockchain data and annotated the graphs with auxiliary data, such as user accounts from BitcoinTalk and Twitter. The authors used visual content discovery and flow analysis techniques to investigate Bitcoin theft. Alternatively, @cite explored the level of anonymity in the Bitcoin network. The authors annotated addresses in the transaction graph with user accounts collected from BitcoinTalk in order to show that users can be linked to transactions through their public Bitcoin addresses. These studies show the value of using public data sources for Bitcoin privacy research and law enforcement, which is our goal behind designing BlockTag.
- Tor hidden services have become a breeding ground for black markets, such as Silk Road and Agora, which offer illicit merchandise and services @cite @cite . Moore and Rid @cite studied how hidden services are used in practice, and noted that Bitcoin was the dominant choice for accepting payments. Although multiple studies @cite @cite showed that Bitcoin transactions are not as anonymous as previously thought, Bitcoin remains the most popular digital currency on the Dark Web @cite , and many users choose to use it despite its false sense of anonymity. Recent research explored the intersection between Bitcoin and Tor privacy @cite @cite , and found that legitimate hidden service users and providers are one class of Bitcoin users whose anonymity is particularly important. Moreover, @cite found that hidden services devoted to anonymity, security, human rights, and freedom of speech are as popular as illegal services. While BlockTag makes it possible to link users to such services, we designed it to help analysts understand the privacy threats, identify malicious actors, and enforce the law.
- Previous research showed that cryptocurrencies, Bitcoin in particular, have a thriving market for fraudulent services, such as fake mining, wallets, exchanges, and Ponzi schemes @cite @cite . Recently, @cite proposed a data mining approach to detect Bitcoin addresses that are involved in Ponzi schemes. The authors manually collected and labeled Bitcoin addresses from public data sources, defined a set of features, and trained multiple classifiers using supervised machine learning. The best classifier correctly labelling 31 addresses out of 32 with 1
- In order to avoid the constraints of expensive pixel-level annotations, weakly-supervised approaches train the segmentation models with only weak annotations, including image-level labels @cite , @cite , @cite , @cite , @cite , @cite , @cite , @cite , @cite , @cite , @cite , @cite , @cite , points @cite , scribbles @cite , @cite , and bboxes @cite , @cite , @cite . All of these methods are dedicated to converting the weak annotations to the latent pixel-level supervision and training the FCNs by iteratively inferring and refining segmentation mask. However, the latent supervisions are either incomplete or noisy without guiding with any strong annotations. Therefore, the segmentation performance of these techniques is still inferior.
- In semi-supervised learning of semantic segmentation, the assumption is that a large number of weak annotations and a small number of strong (pixel-level) annotations are available, which is usually satisfied in practice. Various types of weak annotations, such as image-level labels @cite , @cite , scribbles @cite , and bboxes @cite , @cite , have been explored in the semi-supervised setting. Intuitively, @cite , @cite , @cite augment the weak annotations with the small number of strong annotations for training the FCNs as in the weakly-supervised settings and achieve better performance than the weakly supervised counterparts. In contrast, @cite decouples the semantic segmentation into two sub-tasks: classification and class-agnostic segmentation, supervised with image-level labels and pixel-level masks respectively. This approach shows an impressive performance even with a very small number of strong annotations.
- It is noted that the common pre-training strategy (e.g., pre-training on ImageNet @cite for classification task) in fully supervised setting can be also regarded as a way for alleviating the requirements of strong annotations. However, its main role is to promote the convergence of the segmentation models @cite .
- In this work, we assume that there exists a large number of box annotations and a small number of mask annotations. Our method is most related to @cite , @cite , @cite . The box annotations in @cite , @cite are converted to mask labels using unsupervised methods @cite , @cite , @cite and a priori knowledge for training FCNs. In contrast, we utilize box annotations in an efficient and accurate manner by training the detection module. Motivated by @cite , we also decouple the semantic segmentation into sub-tasks. However, the class-specific features generated from the classification component in @cite are usually sparse and noisy, since the classification network tends to focus only on small discriminative parts (e.g. the head of an animal). In this work, by exploring the detection model, multiple complete objects of multiple scales are focused on the class-specific features with an attention module.
- In biological systems, experience replay has played an important role for learning behaviors. Various studies show that there is frequent experience replay in the hippocampus of rodents of both awake and sleeping animals. Research also shows that disrupting experience replay might lead to impairing spatial memory @cite @cite , and some show that experience replay can contribute to better memory consolidation and retrieval @cite , both are instrumental in boosting learning ability.
- In machine learning, the idea of using was first introduced in @cite when it is used to speed up learning by repeating past experiences. Experience replay helps break the temporal correlation by mixing old and new experiences, and frequent experiences therefore might be replayed more often. Since then, experience replay has been widely adapted, and it becomes the norm in many success in RL research @cite @cite .
- However, the current way to use experience replay is to uniformly sampled from first-in-first-out (FIFO) buffers of experiences @cite @cite . By doing this way, the importance of each transition is neglected, and we might replay experience that is not very helpful for the learning at all. When faced with the design choice of which experiences to restore, neuroscience research show that surprising @cite and rewarding experiences are more preferred @cite .
- In RL, attempts to prioritized experiences have been made in @cite where temporal difference (TD) error is used as a way to measure the priority. Our approach is reward-favored and more direct. We will look at the state of the training episode and use a pre-trained neural network to rank that episode and decide whether the associated experience is worth being stored for future learning.
- In @cite a concept drift detection method is proposed that uses statistical tests over feature vectors. These feature vectors are based on the order in which activities occur in the workflow. They record for example how many activities always, sometimes, or never occur after a given activity. Other measures can be used to add extra features that may incorporate the other attributes in the log. The method does require manual selection of the exact features to be used in the statistical tests.
- @cite propose an automated method for detecting drifts. They focus only on the activities within the process, and transform the traces in the log into partial order runs. These runs summarize the different workflows possible for the process. In order to detect drifts they search for statistically significant changes in the distribution of the most recent runs.
- @cite create an abstract representation of the process in the form of a polyhedron, which is afterwards used to test how well the representation includes the traces in from log. When the traces differ from the representation a drift is detected. To find other drifts the entire method needs to be repeated. This method is able to work in an online environment where we can check for drifts during the execution of the process, whereas other methods require a log file containing completed traces.
- investigate the learnability of semantic universals from a computational point of view. In their study, they train a LSTM network @cite to perform a related sequence classification task: the input to the network is a sequence in which each element represents a set-theoretic model and a quantifier, and the output are two possible truth-values given the input. The model is a sequence of entities represented by one-hot encodings of four zones in a Venn diagram: A @math B, A @math B, B @math A, and @math . For example, an entity in A @math B is encoded in a vector @math 0,1,0,0 @math . Two encoded quantifiers are used in each experiment: one exhibiting the semantic universal, e.g. the conservative ( @math A @math B @math 0), and one not exhibiting the universal, e.g. the non-conservative ( @math B @math A @math 0).
- Unsupervised network embedding methods only utilize network structure information to construct node embeddings. The learned node embeddings are treated as generic feature vectors, which can then be used for downstream tasks in networks. A representative method is DeepWalk @cite , which is a two-phase method for learning node embeddings. In the first step, DeepWalk constructs node neighborhoods by performing fixed-length random walks in the input network. Then, DeepWalk employs the skip-gram @cite model to preserve the co-occurrences between nodes and their neighbors. Many later studies follow this two-step framework for learning network embeddings, by proposing different strategies for constructing node neighborhoods @cite @cite or modeling co-occurrences between nodes @cite @cite .
- There are also a number of methods that utilize edge labels for learning better node representations. SNE @cite presents a log-bilinear model that incorporates two vectors to capture the positive or negative relationships between nodes respectively. TransR @cite learns a projection matrix for each relation type in knowledge graph, which is used for projecting entity embeddings from entity space to relation space. These work generally assume that each edge is associated with a single type of label: it could be the sign of edges in signed networks, or relation types in knowledge graphs. Different from these work, our method is capable of dealing with edges with multiple labels, which is a common scenario in real-world networks.
- Another direction of work utilize label information to enhance network embedding methods. These works typically assume that there is a feature vector associated with each node, and that nodes in the input network are partially labeled. Planetoid @cite simultaneously minimizes the unsupervised loss over network structure and the supervised loss of predicting node labels. GCN @cite and GraphSAGE @cite adopt a neural message passing strategy to propagate node features in the network, with supervision from the labeled nodes.
- Different from these work, our method does not rely on extra node features to work. Additionally, we aim at preserving social relations associated with edges, while these methods are only capable of learning from node labels. DeepGL @cite is an exception in that it is capable of utilizing edge labels for node edge representation learning, but it assumes that label information is available for all edges, which is not often the case.
- Applications with similar functionality as the enemy processes have been used before in the literature. They have been referred to as @cite , @cite or @cite . @cite were the first to utilise such techniques by deploying assembly code to measure multi-core interference on real application workloads. They propose a framework for quantifying the maximum slowdown obtained during simultaneous execution by stressing a single shared resource at a time. Their work examines several Intel processors, exploring the extent that the interference from resource stressing benchmarks can slow down real-time software. @cite perform a similar experiment on a multi-core PowerPC-based processor platform and focus specifically on the memory system. The platform allows for different memory configurations and provides several methods for interference mitigation. Regardless of configuration, SUT slowdowns are still observed when executing the resource stressing kernels on distinct cores. @cite evaluate a multi-core LEON-based processor and run experiments with both a Linux and real-time operating system. Unsurprisingly, the slowdown is mitigated on the real-time operating system, but not eliminated.
- argue that most resource stressing benchmarks may fail at producing bounds @cite . Under heavy contention, arbitration policies of shared resources such as round robin and first in first out produce a so-called "synchrony effect" that causes the SUT to suffer a delay that is not as severe as the potential worst-case. They propose a method to improve the bound by varying the injection time between requests to the shared hardware resources. Approaches such as @cite @cite rely on randomisation of the source code to produce different memory mapping and therefore gather a large set of possible execution times. They utilise a statistical approach called "extreme value theory" and can provide multiple worst-case execution times alongside a confidence factor.
- Tuning strategies have been used to optimise different computational aspects, with @cite showing how such an approach can be used for a variety of domain-specific issues. @cite use genetic algorithms to find the inputs that cause the longest or shortest execution time. To do so, they formulate the search for such inputs as an optimisation problem. @cite use simulated annealing on a single core processor to maximise code coverage and therefore obtain an estimate of the WCET.
- According to the classical multi-task learning paradigm, forcing a single model to perform several related tasks simultaneously can improve generalisation via imposing an inductive bias on the learned representations @cite @cite . Such an approach assumes that all the tasks use a shared representation before learning task-specific parameters. Multiple works in computer vision have been following this strategy; in particular, Eigen & Fergus @cite trained a single architecture (but with different copies) to predict depth, surface normals and semantic segmentation, Kokkinos @cite proposed a universal network to tackle @math different vision tasks, Dvornik al @cite found it beneficial to do joint semantic segmentation and object detection, while Kendall al @cite learned optimal weights to perform instance segmentation, semantic segmentation and depth estimation all at once. Chen al @cite built a single network with the ResNet-50 @cite backbone performing joint semantic segmentation, depth estimation and object detection. To alleviate the problem of imbalanced annotations, Kokkinos @cite chose to accumulate the gradients for each task until a certain number of examples per task is seen, while Dvornik al @cite simply resorted to keeping the branch with no ground truth available intact until at least one example of that modality is seen.
- We note that none of these methods makes any use of already existing models for each separate task, and none of them, with the exception of BlitzNet @cite , achieves real-time performance. In contrast, we show how to exploit large pre-trained models to acquire better results, and how to do inference in real-time.
- Semantic segmentation is a task of per-pixel label classification, and most approaches in recent years have been centered around the idea of adapting image classification networks into fully convolutional ones able to operate on inputs of different sizes @cite @cite @cite . Real-time usage of such networks with decent performance is a non-trivial problem, and few approaches are currently available @cite @cite @cite @cite . We have chosen recently proposed Light-Weight RefineNet @cite on top of MobileNet-v2 @cite as our baseline architecture as it exhibits solid performance on the standard benchmark dataset, PASCAL VOC @cite in real-time, while having fewer than @math M parameters.
- Finally, we briefly touch upon the knowledge distillation approach @cite @cite @cite @cite that is based on the idea of having a large pre-trained teacher (expert) network (or an ensemble of networks), and using its logits, or predictions directly, as a guiding signal for a small network along with original labels. Several previous works relied on knowledge distillation to either acquire missing data @cite , or as a regulariser term @cite @cite . While those are relevant to our work, we differ along several axes: most notably, Zamir al @cite require separate network copies for different tasks, while Hoffman al @cite and Li & Hoiem @cite only consider a single task learning (object detection and image classification, respectively).
- Pioneering work on playback attacks was reported by Lindberg and Blomberg in 1999 @cite . They pre-recorded the numbers one to ten of two speakers and then concatenated various combinations of them to attack a hidden Markov model (HMM) @cite -based text-dependent ASV system. They demonstrated considerable increase in both the EER and false acceptance rate (FAR) compared to verification without attacks. More recently, investigated the effect of playback attacks against ASV systems and also achieved a large increase in FAR @cite . Compared to these conventional playback attacks, our method further degrades the performance of playback spoofing CMs by enhancing the speech.
- There are a few attack methods similar to our enhanced stolen speech method. improved the naturalness of synthesized and converted speech before attacking a phase-based synthetic speech detector and an ASV system @cite . The synthesized and converted speech signals were firstly analyzed frame by frame, and each frame was replaced with one containing the most similar natural speech selected from a dataset. A complex cepstrum vocoder was used to re-synthesize these frames so as to improve speech naturalness. Finally, the speech was directly fed into an ASV system. They reported that their method fooled four out of nine detectors. Our method can be thought of as an extension of their method as it further transforms synthesized speech close to natural speech.
- reported an attack method that transforms computer-generated (CG) images into natural images before feeding them into a facial authentication system @cite . The transformation model is trained using a generative adversarial network (GAN) @cite . The GAN discriminator, which mimics a spoofing detector, is used to distinguish CG natural images. The discriminator is pre-trained and fixed during training of the transformation model. In contrast, we treat the authentication system as a black box, and anything regarding playback spoofing CMs and ASV systems is unknown.
- In @cite , the authors propose a visualization technique to give some insight into the function of the intermediate feature maps of a trained CNN, by attaching a deconvolutional network to each of its convolutional layers. While a CNN maps the input from the image space to a feature space, a deconvolutional network does the opposite (mapping from a feature space back to the image space), by reversing its operations. This is done by a series of unpooling, rectifying and filtering operations. The authors use a deconvolutional network to visualize the features that result in the highest activations in a given feature map. Furthermore, they evaluate the sensitivity of a feature map, to the occlusion of a certain part of the input image, and the effect it has on the class score for the correct class.
- Two other visualization techniques are presented in @cite that are based on optimization. The first technique iteratively generates a canonical image representing a class of interest. To generate this image, the authors start from a zero image and pass it through a trained CNN. Optimization is done by means of the back-propagation algorithm, by calculating the derivative of the class score, with respect to the image, while keeping the parameters of the network fixed. The second technique aims to visualize the image-specific class saliency. For a given input image and a class of interest, they calculate the derivative of the class score, with respect to the input image. The per-pixel derivatives of the input image give an estimate of the importance of these pixels regarding the class score. More specifically, the magnitude of the derivate indicates which pixels affect the class score the most when they are changed.
- Concluding, typical visualization techniques either generate a single output image, in case of the feature visualization and the generation of the class representative, or function at the pixel level of the input image, in case of the region occlusion and the image-specific class saliency visualization. However, dermatologists typically scan a lesion for the presence of different individual features, such as asymmetry, border, color and structures, i.e. the so-called ABCD-score @cite . Therefore, we inspect and visualize the intermediary feature maps of the CNN on a per-image basis, aiming to provide more familiar insights to dermatologists.
- Sennrich and Haddow @cite generalized the embedding layer to support linguistic features such as morphological features, part-of-speech tags, and syntactic dependency labels. Press and Wolf @cite proposed tier embedding and argued that weight tying reduces the size of neural translation models. However, no attempt has been made to reduce the size of the target dictionary through word embedding. Significant reduction in model complexity is expected considering words are on the order of hundreds of thousands or more in a typical dictionary.
- @cite proposed to segment words of source and target sentences into smaller subword units using byte pair encoding (BPE) compression @cite . They showed an improvement in the BLEU scores of 1.1 and 1.3 for English-German and English-Russian translations, respectively. Despite its advantage, BPE splits an alphabetic word to multiple letter groups, and thus it is intrinsically not applicable to logographic languages such as Chinese, Chorti, and Demotic (Ancient Egyptian) where a word is a glyph rather than alphabetic letters. Garc ' a-Mart ' @cite proposed to decompose words morphologically and grammatically into factored representations such as lemmas, part-of-speech tag, tense, person, gender, and number. Their approach reduced training time and out of vocabulary (OOV) rates with improved translation performance, but also introduces unnecessary grammatical dependencies, (e.g. there are hundreds of tenseless languages), and is not optimized in all scenarios.
- Natural Language Interface to Database aims at providing an interactive bridge between humans and machines, where human users can issue a question in natural language text, which would then be translated to a structured query that can be executed by a database engine. first explores this task with concrete examples defining this problem and highlights the separation between linguistic and database-derived information. Later proposes to identify questions whose answers are tractable solely from the database, and incorporates tree kernels in ranking candidate queries. Many recent advances can be categorized into two groups. The first group uses semantic parsing @cite @cite @cite as well as some extensions that support cross-domain semantic parsing @cite @cite . However, due to the idiosyncrasy and complexity of natural languages, most works in this group are confined in narrow domains. The other group relies on neural based methods where sequence to sequence models are leveraged to translate input questions to SQL queries, optional combined with the help of user feedback @cite and reinforcement learning @cite .
- Closest to our proposed work is @cite , which employs a sketch-based approach that represents an SQL as a template with slots, and the model predicts values from a limited candidate set to be filled in each slot. This is different from our work that focuses on annotation and does not restrict SQL to a particular template-based form. Another close work is TypeSQL @cite that enriches the inference of columns and values using domain-specific knowledge-based model that searches five types of entities on Freebase, an extra large database, which is contrast to our work which does not rely on extra database knowledge.
- Sequence to Sequence (referred to as seq2seq in the rest of the paper) learning @cite has led to many advances in neural semantic parsing models. Notable advances in sequence learning include attention mechanism @cite and pointer network @cite that boost performance for sequence learning and enable it to handle long sequence and rare words. They have seen successful applications on language model @cite , text summarization @cite , text understanding @cite , and neural computing @cite . Our model also benefits from these techniques since our model needs to see both information packed in long sequence and rare words that only appear in few tables.
- Molecular optimization has a longstanding history especially in drug discovery, and mostly combinatorial methods have been used to generate novel molecules @cite . The paper by , for the first time, applies modern machine learning techniques to this problem, and since then, growing number of papers have been tackling this problem.
- There are two complementary approaches to molecular optimization. One is the combination of VAE and Bayesian optimization, originally developed by , and the other is reinforcement learning, where the construction of a molecule is modeled as a Markov decision process @cite @cite @cite . These two approaches cover complementary application areas, and therefore, they are not conflicting with each other. The key difference is the assumption on function evaluation cost. The former assumes that the cost is so high that the number of function evaluations should be kept at a minimum, while the latter assumes that the cost is negligible and a number of trial-and-errors are allowed. Therefore, the former is more favorable when the evaluation requires wet-lab experiments or computationally heavy first-principles calculation, and the latter is more favorable when the evaluation can be carried out by light-weight computer simulation. Since our paper focuses on the former setting, in the following, we will discuss its literature.
- Open set recognition is a challenging task initially proposed in face recognition task @cite , where test faces have limited overlap IDs with the training faces. It demands a robust system with good generalizability. In this work, we view the open set recognition as a retrieval task. In some early works @cite @cite @cite @cite , the intermediate semantic representation is usually learned from the source dataset and applied to the target dataset. Recently the progress in this field has been due to two factors: the availability of large-scale source set and the learned representation using the deep neural network. Most state-of-the-art methods apply Convolutional Neural Network (CNN) to extract the visual feature and rank the images according to the feature similarity @cite @cite @cite . Despite the impressive performance, no prior works have explored the robustness of the open set system. In this paper, we do not intend to achieve the state-of-the-art accuracy. We train the baseline CNN on several datasets, which yields competitive results and then attack these models with adversarial queries.
- Recursive neural networks (RvNN) are a kind of neural architecture which model sentences by exploiting syntactic structure. While earlier RvNN models proposed utilizing diverse composition functions, including feed-forward neural networks @cite , matrix-vector multiplication @cite , and tensor computation @cite , tree-LSTMs @cite remain the standard for several sentence-level tasks.
- To enable dynamic compositionality in recursive neural networks, many previous works @cite @cite @cite @cite @cite @cite @cite have proposed various methods.
- Another way of building dynamic compositionality into RvNNs is to take advantage of a meta-network (or hyper-network). Inspired by recent works on dynamic parameter prediction, DC-TreeLSTMs @cite dynamically create the parameters for compositional functions in a tree-LSTM. Specifically, the model has two separate tree-LSTM networks whose architectures are similar, but the smaller of the two is utilized to calculate the weights of the bigger one. A possible problem for this model is that it may be easy to be trained such that the role of each tree-LSTM is ambiguous, as they share the same input, i.e. word information. Therefore, we design two disentangled tree-LSTMs in our model so that one focuses on extracting useful features from only syntactic information while the other composes semantic units with the aid of the features. Furthermore, our model reduces the complexity of computation by utilizing typical tree-LSTM frameworks instead of computing the weights for each example.
- With algorithms increasingly running our world, there has been substantial recent interest on incorporating fairness systematically into algorithms and machine learning. One such notion that has gained importance is that of : in addition to requiring that such as gender or race not be used (explicitly) in decisions, this asks that decisions not be disproportionately different for diverse protected classes @cite . This is developed further in the context of clustering in the work of @cite . Such notions of are considered along with -- treating similar individuals similarly -- in @cite . See @cite for earlier work that developed foundations and connections for several such notions of fairness.
- Similar stochastic approximation guarantees have appeared in the context of approximation algorithms for static problems, particularly @math -median problems. In @cite , Charikar & Li discussed a randomized procedure for converting a linear-programming relaxation in which a client has distance @math , into a distribution @math satisfying @math . This property can be used, among other things, to achieve a @math -approximation for @math -median. However, many other randomized rounding algorithms for @math -median only seek to preserve the value @math , without our type of per-point guarantee.
- To the best of our knowledge, Pop-Art @cite is the only framework that directly deals with reward scales. Their work is mainly for value-based RL approaches while we focus on actor-critic approaches, which is capable of achieving competitive performances @cite such as DDPG @cite , PPO @cite , and A2C @cite . The main difference between Pop-Art and our approach is that ANS finds the best reward scale in terms of learning performance whereas Pop-Art maintains a normalized reward distribution, which may not guarantee to improve the outcome of learning.
- Given the importance of chemical design to the pharmaceutical development process, a considerable body of work has been devoted to computational methods for improving it (e.g. @cite @cite ). Much recent research on constructing continuous embeddings of chemicals has used Bayesian optimization to generate molecules with desirable properties @cite @cite @cite @cite @cite @cite . However, these methods do not perform chemical design, aimed at a particular molecule of interest.
- Inspired by the success of stochastic gradient-based optimization in training deep neural networks, we also take advantage of the continuous nature of the latent chemical space, using ADAM @cite for optimization, rather than a Bayesian approach. This has the disadvantage of requiring differentiable models for all of the quantities we wish to optimize; however, deep learning models can be used to satisfy this requirement.
- Separately, methods of predicting affinity from structural information have also been recently developed @cite @cite . Such networks take a PLC as input, and use the structural information from the complex to score the affinity of the bound pair. The main difference between our method and these approaches is that our affinity prediction network does not receive structural information on the interaction; i.e. the protein site and putative binding chemical are received independently, as embedded vectors. This makes the estimation problem more difficult, since detailed atomic interaction information is not available to the predictor, but more general, since it can be applied to any protein site-chemical pair. This allows us to avoid a docking step (i.e. to generate a PLC) during our optimization.
- Hate Speech on Web Communities. Several studies focus on understanding the degree of hate speech that exists in various Web communities. Specifically, @cite focus on 4chan's Politically Incorrect board ( ) by analyzing 8M posts during the course of two and a half months. Using the Hatebase database they find that 12 , @cite undertake a similar analysis on Gab finding that Gab exhibits two times less the hate speech of , whereas when compared to Twitter it has two times more hateful posts. @cite use the Hatebase database to study hate speech on two Web communities, namely Twitter and Whisper. Their quantitative analysis sheds light on the targets (recipients) of hate speech on the two Web communities. Similarly, @cite use the same Web communities to understand the prevalence of hate speech, the effects of anonymity, as well as identify the forms of hate speech in each community.
- Hate Speech Detection. A substantial body of prior work focus on the detection of hate speech on Web communities. Specifically, Warner and Hirschberg @cite use decision lists in conjunction with an SVM classifier to detect hateful content. They evaluate the proposed approach on a classification pilot that aim to distinguish antisemitic content, highlighting that their approach has acceptable accuracy (94 and Wang @cite use a Naive Bayes classifier on tweets to classify them as either racist against blacks or non-racist. Their classifier achieves an accuracy of 76 @cite leverage a continuous bag of words (CBOW) model within doc2vec embeddings to generate low-dimensional text representations from comments posted on the Yahoo finance website. These representations are then fed to a binary classifier that classifies comments as hateful or not; they find that the proposed model outperforms BOW baselines models.
- @cite use subjectivity and sentiment metrics to build a hate lexicon that is subsequently used in a classifier that determines whether content is hateful. Waseem and Hovy @cite annotate 16K tweets as racist, sexist or neither. They also assess which features of tweets contribute more on the detection task, finding that character n-grams along with a gender feature provide the best performance. Del @cite propose the use of Support Vector Machines (SVMs) and Recurrent Neural Networks (RNN) for the detection of hateful Italian comments on Facebook, while @cite provide a German hate speech corpus for the refugee crisis.
- @cite use the error signal of class-based language models as a feature to a neural classifier, hence allowing to capture online behavior that uses new or misspelled words. This approach help outperform other baselines on hate speech detection by 4 @cite propose the use of a unified deep learning model for the classification of tweets into different forms of hate speech like hate, sexism, bullying, and sarcasm. The proposed model is able to perform inference on the aforementioned facets of abusive content without fine tuning, while at the same time it outperforms state-of-the-art models.
- @cite approach the problem through the lens of multiple Web communities by proposing a community-driven model for hate speech detection. Their evaluation on Reddit, Voat, and Web forums data highlight that their model can be trained on one community and applied on another, while outperforming keyword-based approaches. @cite leverage the Hatebase database and crowdsourcing to annotate tweets that may contain hateful or offensive language. Using this dataset, they built a detection model using Logistic Regression. Their analysis highlights that racist and homophobic tweets are likely to be classified as hate speech, while sexist tweets are usually classified as offensive.
- Burnap and Williams @cite propose a set of classification tools that aim to assess hateful content with respect to race, sexuality, and disability, while at the same time proposing a blended model that classifies hateful content that may contain multiple classes (e.g., race and sexuality). @cite compare a wide variety of machine and deep learning models for the task of detecting hate speech. They conclude that the use of deep learning models provide a substantial performance boost when compared with character and words n-grams.
- @cite perform a personality analysis on instigators and recipients of hate speech on Twitter. They conclude that both groups comprises eccentric individuals, and that instigators mainly target popular users with (possibly) a goal to get more visibility within the platform. In their subsequent work, @cite perform a linguistic-driven analysis of hate speech on social media. Specifically, they differentiate hate speech in targeted hate (e.g., towards a specific individual) and generalized (e.g., towards a specific race) and find that targeted hate is angrier and more informal while generalized hate is mainly about religion.
- Finally, @cite propose the use of user-centered metrics (e.g., users' overall perception of classification quality) for the evaluation of hate speech detection systems.
- Case Studies. @cite undertake a case study on Operation Google, a movement that aimed to use benign words in hateful contexts to trick Google's automated systems. Specifically, they build a model that is able to detect posts that use benign words in hateful contexts and undertake an analysis on the set of Twitter users that were involved in Operation Google. @cite focus on Jihadist hate speech by proposing a hate detection model using Natural Language Processing and Machine Learning techniques. Furthermore, they undertake a quantitative and qualitative analysis on a corpus of 45K tweets and examine the users involved in Jihadist hate speech.
- Apparently we are not the first who concerns transformation estimation in visual object tracking. In correlation filter based trackers, DSST @cite and SAMF @cite are early work that estimates the scale change of the tracked object. DSST @cite does so by learning separate discriminative correlation filters for translation and scale estimation. SAMF @cite uses a scale pyramid to search corresponding target scale. Recently, RAJSSC @cite proposes to perform both scale and angle estimation in a unified correlation tracking framework by using the Log-Polar transformation. In SiamFC-based trackers, while the scale estimation has been considered in the original SiamFC tracker, angle estimation has not been considered before.
- Some of the remarkable recent advances in deep learning @cite happened in the area of generative models. For static data, such as generating images, the work done using @cite and @cite has shown remarkable results.
- The generation of continuous data has always been tricky. Graves @cite combined (LSTM) networks with , MDN @cite , to generate continuous handwritten characters, using @cite . While the results are impressive, the MDN approach are quite difficult to train. Another possible approach for generating continuous tracings is , GSM @cite .
- In order to simplify the procedure, and focus on our investigation of styles, we discretized the tracings using for direction, and speed - see Section more details -, and apply to the output of the last layer, instead of MDN. This was inspired by the results reported in @cite @cite , where they show impressive results on the discrete domain, given a good discretization policy. Having a categorical distribution is more flexible and generic that a continuous distribution, and requires no assumption about the data distribution shape. Recently, interesting work has been done concerning style extraction in the area of speech synthesis @cite @cite . In their work, they extract a number of . They evaluated the performance of their system via classical subjective rating of voice, and show these token relate to some aspects of the speech prosody and the speaker's voice.
- The optimal approximation results of the ordinary @math -center clustering appeared in the 80's: Gonazlez @cite and Hochbaum and Shmoys @cite respectively provided a 2-approximation and proved that any approximation ratio @math would imply @math . If @math is fixed, we have the PTAS (Polynomial Time Approximation Scheme) for @math -center clustering through constructing the core-set of minimum enclosing ball @cite @cite .
- Motivated by the applications from big data, we assume that both the number of input points @math and dimension @math are large, and propose faster algorithms for @math -BCenter, @math -BMedian, @math -BMeans, and their metric counterparts. In addition, we assume that the number of clusters @math is a constant. Actually, @math is usually a small number in practice (e.g., the data is distributed over less than @math machines). Moreover, the existing research on large-scale clustering problems often assume that either @math or @math is a constant, due to their hardness results mentioned above @cite @cite @cite @cite @cite @cite @cite @cite .
- Clustering problems commonly involve two key steps: ). determine the @math cluster centers and ). partition the input data into @math clusters. For ordinary clustering problems, step ( 2 ) is trivial. Namely, we just need to assign each data point to its nearest cluster center to minimize the total clustering cost. However, step ( 2 ) can be complicated for constrained clustering problems. In general, we need to compute the matching between input points @math and the obtained cluster centers, such that the objective value is minimized and the constraint (e.g., the balancedness) can be satisfied simultaneously. For example, the algorithms in @cite @cite @cite all need to reduce it to be a max flow or min cost flow problem. They build a bipartite graph between @math and the @math cluster centers, and the numbers of the vertices @math and edges @math are both linear on @math ; therefore, their running times will be @math that is at least quadratic on the input size. The authors of @cite proposed the open problem: can we avoid computing the high complexity matching in step ( 2 )?
- In this paper, we answer their question in the affirmative. Specifically, we provide a novel method to complete step ( 2 ) in linear time. For @math -BCenter, we apply a spatial partition idea to build a system of linear equations and inequalities that has the size independent of @math , and round the feasible solution to be an integral solution (i.e., a feasible partition on @math ) efficiently without increasing the objective value. Comparing with the existing method for @math -BCenter @cite , we improve the approximation ratio from @math to @math and significantly reduce the running time by avoiding to solve the large-scale matching problem. To solve @math -BMedian and @math -BMeans, we generalize the spatial partition idea and replace the system of linear equations and inequalities by a model of linear programming. More importantly, a feasible partition on @math can be efficiently obtained as well. We adopt the results from @cite @cite and obtain the same constant factor and @math -approximations, while our running times are much lower.
- The literature on semantic parsing has focused on various types of semantic formalisms. The @math -calculus expressions @cite have been popular and widely used in semantic parsing tasks over recent years @cite @cite @cite @cite @cite @cite . Dependency-based compositional semantics (DCS) Unlike ours, their work captures dependencies between semantic units but not natural language words. was introduced by , whose extension, @math -DCS, was later proposed by . Various models @cite @cite @cite on semantic parsing with the @math -DCS formalism were proposed. In this work, we focus on the tree-structured semantic formalism which has been examined by various research efforts @cite @cite @cite @cite @cite @cite @cite .
- proposed the semantic parser that regards the task as a phrase-based machine translation problem. proposed a generative process to generate natural language words and semantic units in a joint model. The resulting representation is called where both natural language words and semantics are encoded into a joint representation. The -s @cite parser applied the CCG grammar @cite to model the joint representation of both semantic units and contiguous word sequences which do not overlap with one another. applied a generative process with Bayesian tree transducer and their model also simultaneously generates the meaning representations and natural language words. proposed a discriminative version of the hybrid tree model of @cite where richer features can be captured. proposed a sequence-to-tree model using recurrent neural networks where the decoder can branch out to produce tree structures. augmented the discriminative hybrid tree model with multilayer perceptron and achieved state-of-the-art performance.
- (left) @cite and our dependency-based hybrid tree (right) as well as the flat representation (bottom right) of the example in Figure .
- First publicly available databases to present iris images obtained in visible light were the UPOL @cite , UBIRISv1 @cite and UBIRISv2 @cite datasets. The first one contains images collected with an ophthalmology device, the second - scaled down photographs taken with Nikon E5700 handheld camera (resampling was introduced to simulated poor quality of unconstrained imaging), and the third - images captured and , also in visible light. Recently, a dataset of iris images and iris image printouts ( fake' iris images) obtained using various mobile devices has been released @cite , however, these are also images taken without built-in flash illumination.
- Upon the UBIRISv2 database, the NICE.I competition was founded by Proenca al @cite , focusing on the noisy iris images and independent evaluation of segmentation and noise recognition stages, as those two are acknowledged by the authors to be the most likely source of errors. Following that, Proenca discusses challenges associated with unconstrained iris recognition in visible light @cite , most notably the amount of information that can be captured using visible light acquisition setups. Santos al explore configurations of visible light illumination best for unconstrained iris recognition @cite , such as the choice of illuminant and its luminance in respect to different eye pigmentation types. Proenca also proposes a new method for segmenting poor quality iris images captured in visible illumination @cite , and methods for assessing image quality in order to improve an overall system performance by discarding samples of substandard quality @cite .
- Raja investigate visible light iris recognition using a light field camera @cite , and also smartphones and tablets @cite , reporting promising results. In the latter scenario, EER (Equal Error Rate) below 2 Segmentation stage is recognized as the most challenging part of the recognition process using noisy images in a study by Radu @cite , where authors manage to achieve 7
- Diverse appearances of real world objects can be encoded by a BRDF @math , which relates the observed intensity @math to the associated surface normal @math , the @math -th incoming lighting direction @math , its intensity @math , and the outgoing viewing direction @math via where @math accounts for attached shadows and @math is an additive error to the model. eq:img_form1 is generally called image formation model . Most photometric stereo algorithms assumed the specific shape of @math and recovered the surface normals of a scene by inversely solving eq:img_form1 from a collection of observations under @math different lighting conditions @math . All the effects that are not represented by a BRDF (image noises, cast shadows, inter-reflections and so on) are typically put together in @math . Note that when the BRDF is Lambertian and the additive error is removed, it is simplified to the traditional Lambertian image formation model @cite .
- Many photometric stereo algorithms recover surface normals of a scene via a simple diffuse reflectance modeling ( , Lambertian) while treating other effects as outliers. For instance, Wu al @cite have proposed a rank-minimization based approach to decompose images into the low-rank Lambertian image and non-Lambertian sparse corruptions. Ikehata al extended their method by constraining the rank-3 Lambertian structure @cite (or the general diffuse structure @cite ) for better computational stability. Recently, Queau al @cite have presented a robust variational approach for inaccurate lighting as well as various non-Lambertian corruptions. While effective, a drawback of this approach is that if it were not for dense diffuse inliers, the estimation fails.
- @cite attempted to explain the mechanism of LISTA by re-factorizing the Gram matrix of dictionary, which tries to nearly diagonalize the Gram matrix with a basis that produces a small perturbation of the @math ball. They re-parameterized LISTA into a new factorized architecture that achieved similar acceleration gain to LISTA. Using an indirect'' proof, @cite was able to show that LISTA can converge faster than ISTA, but still sublinearly. Lately, @cite tried to relate LISTA to a projected gradient descent descent (PGD) relying on inaccurate projections, where a trade-off between approximation error and convergence speed was made possible.
- In @cite , a similar learning-based model inspired by another iterative algorithm solve LASSO, approximated message passing (AMP), was studied. The idea was advanced in @cite to substituting the AMP proximal operator (soft-thresholding) with a learnable Gaussian denoiser. The resulting model, called Learned Denoising AMP (L-DAMP), has theoretical guarantees under the asymptotic assumption named state evolution.'' While the assumption is common in analyzing AMP algorithms, the tool is not directly applicable to ISTA. Moreover, @cite shows L-DAMP is MMSE optimal, but there is no result on its convergence rate. Besides, we also note the empirical effort in @cite that introduces an Onsager correction to LISTA to make it resemble AMP.
- We have referenced much of the related work throughout the paper. We now review another orthogonally related field of work. Grammatical error correction (GEC) is the task of correcting the grammatical errors (if any) in a piece of text @cite . As GEC includes not just identification of ungrammatical text but also correcting the text to produce grammatical text, its a more complex task. However, grammatical error prediction @cite @cite is the task of classifying whether or not a sentence is grammatical, which is more closely related to our task as classifying a question as well-formed requires making judgement on both the style and grammar of the text.
- Bilingual word embeddings have been extensively studied in the literature in recent years. Their nature varies with respect to the supervision signals used for training @cite @cite . Some common signals to learn bilingual embeddings come from parallel @cite @cite @cite or comparable corpora @cite @cite @cite , or lexical resources such as WordNet, ConceptNet or BabelNet @cite @cite @cite . However, these sources of supervision may be scarce, limited to certain domains or may not be directly available for certain language pairs.
- Another branch of research exploits pre-trained monolingual embeddings with weak signals such as bilingual lexicons for learning bilingual embeddings @cite @cite @cite @cite . was one of the first attempts into this line of research, applying a linear transformation in order to map the embeddings from one monolingual space into another. They also noted that more sophisticated approaches, such as using multilayer perceptrons, do not improve with respect to their linear counterparts. built upon this work by normalizing word embeddings during training and adding an orthogonality constraint. In a complementary direction, put forward a technique based on canonical correlation analysis to obtain linear mappings for both monolingual embedding spaces into a new shared space. proposed a similar linear mapping to , generalizing it and providing theoretical justifications which also served to reinterpret the methods of and . further showed how orthogonality was required to improve the consistency of bilingual mappings, making them more robust to noise. Finally, a more complete generalization providing further insights on the linear transformations used in all these models can be found in .
- We discuss power and performance variations of two generations of Intel processors in this paper. describe node-level power variations for more than 260 nodes equipped with AMD Magny Cours executing mprime @cite and Intel Sandy Bridge processors executing FIRESTARTER @cite in @cite . They propose to change the batch systems partition to increase the utilization of less power consuming nodes and decrease the utilization of inefficient ones. describe power consumption variations over time and between nodes for several systems based on different processors (Intel Nehalem-EP, Nehalem-EX, Sandy Bridge-EP, and AMD FirePro GPUs) with a focus on improving large-scale power measurement methodologies @cite .
- However, processor performance variations occur due to power and thermal constraints for certain processors that can be the result of insufficient cooling, artificially low power limits, or hardware over-provisioning. Starting with Intel Sandy Bridge processors, a power limit below the TDP can be enforced via the running average power limitation (RAPL) @cite . To estimate the current power consumption, RAPL uses an energy model on Sandy Bridge processors @cite and measurements on Haswell processors @cite to throttle the performance of the processor to achieve the given power limit. The quality of the RAPL power enforcement in terms of stability, accuracy, settling time and maximum overshoot has been analyzed by Zhang and Hoffman for Sandy Bridge processors @cite . show how an enforced power limit turns power variations into performance variations for Intel Sandy Bridge processors @cite . investigate in @cite the influence of such power limits on power consumption, performance, and energy efficiency of four parallel benchmarks and compare it to the usage of processor P-States. They describe that an enforced common power cap regulation can limit scalability due to performance variability.
- The major changes of the Intel Haswell processor generation with respect to on-chip energy management have been summarized previously @cite . It was predicted that these changes significantly influence power consumption and performance variations, which is investigated in detail in the following sections.
- This paper builds on the work of Diehl and Cook @cite where a spiking neural network (SNN) is used to classify the MNIST handwritten digits after learning network weights without supervision and with several (STDP) rules. Classification performance of their networks increases with the number of spiking neurons used, ultimately achieving approximately 95 A number of other SNNs trained with STDP are used to classify image data @cite , @cite . The former uses Gabor filter features as a pre-processing input to their network, uses the rank-order coding scheme for input spikes, and classifies data with the winner-take-all strategy on the output layer. The latter is comprised of a difference of Gaussians pre-processing step, followed by convolutional and pooling layers, and whose output is trained on a linear SVM to perform classification. Other systems use spiking neurons, but are trained with supervision; e.g. @cite , which was first trained as a deep neural network using back-propagation and later transferred to a spiking neural network without much loss in performance.
- Numerous matrix factorization approaches to link prediction have been proposed. An early model, RESCAL @cite , tackles the link prediction task by optimizing a scoring function containing a bilinear product between vectors for each of the subject and object entities and a full rank matrix for each relation. DistMult @cite can be viewed as a special case of RESCAL with a diagonal matrix per relation type, which limits the linear transformation performed on entity vectors to a stretch. ComplEx @cite extends DistMult to the complex domain. TransE @cite is an affine model that represents a relation as a translation operation between subject and object entity vectors.
- A somewhat separate line of link prediction research introduces Relational Graph Convolutional Networks (R-GCNs) @cite . R-GCNs use a convolution operator to capture locality information in graphs. The model closest to our own and which we draw inspiration from, is ConvE @cite , where a convolution operation is performed on the subject entity vector and the relation vector, after they are each reshaped to a matrix and lengthwise concatenated. The obtained feature maps are flattened, put through a fully connected layer, and the inner product is taken with all object entity vectors to generate a score for each triple. Advantages of ConvE over previous approaches include its expressiveness, achieved by using multiple layers of non-linear features, its scalability to large knowledge graphs, and its robustness to overfitting. However, it is not intuitive why convolving across concatenated and reshaped subject entity and relation vectors should be effective.
- The most similar concept with wrapped loss function is cost-sensitive learning and label re-balancing. The target of cost-sensitive learning is to find a minimal cost in the imbalanced labels problems. In general, the errors are from the class with rare label. In the cost-sensitive learning, some methods add weights on loss function to learn its features. For example, there are the Prior Scaling @cite and Minimization of the misclassification costs method @cite . Or just simply pre-set label weights before model training. like Median Frequency Balancing: @cite @cite @cite . ( @math , where freq(c) is the number of data in the class c divided by the total number of data, and @math is the median of the frequencies.
- Another noteworthy modify weights on data usage is AdaBoost @cite . While AdaBoost boosts the performance of a collection weak learner, it also modifies the weights of data depending on their current classification performance so that the final classifier achieves better performance.
- For the multi-task learning, there are two or more related tasks jointly learned, and their loss functions are combined into a loss function. Then the weights can be added to each tasks to generate a loss function. Huy @cite uses convolutional neural networks and deep neural network coupled with weighted loss function to solve audio event detection, and Sankaran @cite shows that the combination of 3 techniques: LSCSR-initialization, Multi-task training and Class-weighted cross entropy training gives the best results on keyword spotting. Unlike above cases use pre-set weights, Kendall @cite applies IRLS to find maximum likelihood estimates of multi-task learning model for scene geometry and semantics.
- Most previous works on accelerating CNNs can be roughly divided into three categories, namely, , , and . In particular, the of deep CNN tensors is approximated by the product of two low-rank matrices @cite @cite @cite . This can save the computational cost. Some works @cite @cite focus on compressing the CNNs by using . -based approaches aim to remove the unnecessary connections of the neural network @cite @cite . Essentially, the work of this paper is based on the idea of pruning techniques; and the approaches of matrix decomposition and low-precision weights are orthogonal but potentially useful here  it may be still worth simplifying the weight matrix after pruning filters, which would be taken as future work.
- Many recent works @cite @cite @cite pruning weights of neural network resulting in small models. For example, @cite proposed an iterative weight pruning method by discarding the small weights whose values are below the threshold. @cite proposed the dynamic network surgery to reduce the training iteration while maintaining a good prediction accuracy. @cite @cite leveraged the sparsity property of feature maps or weight parameters to accelerate the CNN models. A special case of weight pruning is neuron pruning. However, pruning weights always leads to unstructured models, so the model cannot leverage the existing efficient BLAS libraries in practice. Therefore, it is difficult for weight pruning to achieve realistic speedup.
- Due to the deep networks @cite , image generation has dramatic progress in recent years. Although the early work such as @cite can only produce synthetic images which are easy to distinguish from real samples, the recent work @cite @cite @cite can synthesize photo-realistic images. There are some typical deep image generation approaches promoting this progress. propose Variational Autoencoders (VAE) @cite by using probabilistic graphical models and maximizing the lower bound of data likelihood to formulate the generation problem. propose Generative Adversarial Networks (GAN) @cite to train a generative model with a discriminative model in an adversarial paradigm. Deep Recurrent Attention Writer (DRAW) method @cite can generate photo-realistic images with the recurrent variational auto-encoder and the attention mechanism. As an autoregressive method, PixelRNN @cite realizes image synthesis by modeling the conditional distribution in the pixel space. In addition, it has been proven that conditional image generation based on these generative approaches can be realized, which has more flexible application @cite @cite .
- Instead of adversarial training widely used in previous methods, the generative model of our proposed SDN has a feedforward structure and learns under the guidance of generic discriminative model. With this paradigm, SDN does not need minimax optimization and careful parameter adjustment of two adversarial models, which is more stable than contemporary GAN based methods @cite .
- Deep Q-Network (DQN) has enabled an agent to achieve human-level performance on playing Atari games @cite . RL algorithms, such as DQN, are generally data intensive and frequently require huge numbers of interactions with the environments. To better use the interaction experience, experience replay (ER), that suggests storing and reusing samples at training time, has been widely used for speeding up the RL agent's training process @cite @cite . Prioritized ER (PER) further accelerates training by assigning a weight based on temporal difference error (TD-error) to each sample @cite , so as to increase the likelihood of selecting samples with a high TD-error. While PER enables more effective sample selections @cite , the applicability of PER is limited when very few successful samples are available. We develop HER methods that generate artificial successful'' samples to improve the learning rate of dialogue agents.
- Reward shaping has been used for speeding up the learning of dialogue policies by adding a new dense reward function @cite . It has been proved that a well designed (dense) reward function does not alter the optimal policy @cite . Recently, su2015reward applied Recurrent Neural Networks (RNNs) to predict the intermediate rewards for dialogues to reduce training time, and developed an on-line learning framework where dialogue policy and reward function were jointly trained via active learning @cite . However, generating such complex reward functions require either considerable human effort or large datasets.
- Another line of research focuses on developing effective exploration strategies for RL algorithms, enabling more sample-efficient dialogue learning. For instance, pietquin2011sampleA integrated Least-Squares Policy Iteration @cite and Fitted-Q @cite for dialogue policy optimization @cite . Other examples include Gaussian Process (GP)-based sample-efficient dialogue learning @cite , the Bayes-by-Backprop Q-network (BBQN) @cite , and trust-region and gradient-based algorithms @cite . Our HER methods have the potential to be combined with the above sample-efficient RL methods to produce a more efficient learning process.
- Pre-training has been used in dialogue learning for computing an initial policy from a corpus using supervised learning (SL) @cite @cite . After that, a dialogue agent can further improve the policy via learning from interactions with users. In line with past research on dialogue systems (as listed above), we use pre-training in this work (unless stated otherwise) to give our dialogue agent a warm start''.
- Work closest to this research is the original HER method @cite that manipulates goals based on resulting states. But that method is only applicable to domains where goals are explicit to the agents, e.g., target positions in manipulation tasks. In dialogue domains, goals are not fully observable and must be identified via language actions, which motivates the development of complex HER methods in this work.
- Park al @cite formalise the notion of provenance-based access control (PBAC) systems along three dimensions: 1) the type of data used to make decisions (observed vs. disclosed provenance @cite ); 2) object dependencies (information flow between objects) vs. user dependencies (information flow between users); and 3) whether policies are available to the system or learnt through the traversal of provenance graphs; , like most PBAC systems in the literature ( @cite @cite ), uses observed provenance, although it could be augmented by disclosed provenance. Layering of provenance systems @cite could enable such a capability, although we are not aware of any layered PBAC enforcement model. We plan to explore this approach in future work, with both application level @cite and network level provenance @cite .
- Information Flow Control Systems. Previous work on information flow control enforcement at the OS level, such as HiStar @cite , Flume @cite , and Weir @cite , uses labels to define security and integrity contexts that constrain information flows between kernel objects. Labels map to kernel objects, and a process requires decentralised management capabilities to modify its labels. Point-to-point access control decisions are made to evaluate the validity of an information flow. Through transitivity, it is possible to express constraints on a workflow ( collected user information can only be shared with third parties as an aggregate). SELinux @cite provides a similar information flow control mechanism but without decentralised management. A typical way of representing and thinking about information flow in a system is through a directed graph. However, current object labelling abstractions do not take advantage of this representation, and it is difficult to reason about when defining policies. differs from these systems in that it allows the implementation of such mechanisms directly on the graph abstraction.
- Taint Tracking Systems. Techniques such as colouring'' @cite or tainting @cite of data and resources have been proposed as a means to detect data misuse. TaintDroid @cite implements such an approach in the Android OS to detect applications disclosing personal information to an unexpected third party ( disclosing the owner's contact list to advertisers). can be used to achieve similar results as taint tracking systems but provides more control through its expressive query mechanism on how taints are propagated within the system. Furthermore, the provenance records, kept as forensic evidence, provide a rich resource that can be mined to identify, understand, and explain the source of a disclosure.
- Security Monitoring. In today's enterprise environments, security incidents occur when a primary indicator of compromise is triggered from security monitoring software such as an anti-virus detection alert or a blacklisted URL in the organisation's network logs @cite . In current security products, such indicators report only limited context as to the circumstances under which the alert occurred, process ID or packet header information, but do not report the historical chain of events that led to the suspicious activity. Past work has attempted to compensate for this lack of lineage through the fusion @cite @cite or correlation @cite @cite @cite @cite of multiple indicators of the compromise. However, it does not address the fundamental limitation that security monitoring tools lack the ability to reason over the entire context of a system execution. Thus, attack reconstruction has typically been relegated to (offline) forensic analysis @cite @cite @cite @cite @cite @cite @cite @cite @cite . In contrast, provides a mechanism to build runtime security monitoring based on the entire history of system execution, thus representing a significant step forward compared to the state-of-the-art.
- The Private Information Retrieval (PIR) problem was first studied in @cite from a computational complexity perspective. Recently, the PIR problem attracted considerable attention in the information theory society and many works study this problem from an information-theoretic point of view @cite @cite @cite @cite . A single user wants to privately download one message from a database. To achieve perfect privacy in the information-theoretic sense, if the database is only stored at one server, the user has to download all messages. The problem becomes more interesting if one supposes that the database is stored in multiple servers and there is no collusion between these servers. By exploiting the advantages of replications of the database in non-colluding servers, private information retrieval can be achieved without downloading all messages and the capacity of this problem is characterized in @cite . Ensuing work has studied many variations of this theme, including databases coded by erasure codes @cite @cite @cite @cite @cite @cite , partial colluding servers @cite @cite @cite , side information messages available at users @cite @cite @cite @cite @cite and multiple messages @cite @cite ,
- With further examination of the attacker's capabilities, in addition to the potential control over the training data, a powerful attacker may also know the internal architecture and parameters of the classifier. Therefore, a fourth dimension can be added to the above taxonomy according to attacker information: in , the adversary generates malicious instances against the target classifier directly; while in , since the attacker does not possess full knowledge about the model, they first approximate the target's model by training over a dataset from a mixture of samples obtained by observing the target's performance, and synthetically generating inputs and label pairs. Then if the reconstructed model generalises well, the crafted adversarial examples against this model can be transferred to the target network and induce misclassifications. Papernot al @cite @cite have demonstrated the effectiveness of the black-box attack in certain specific domains. Specifically, they investigate intra- and cross-technique transferability between deep neural networks (DNNs), logistic regression, support vector machines (SVMs), decision trees and the @math -nearest neighbour algorithm.
- In more recent studies, several papers have begun to study whether attacks against classifiers can also be applied to RL-based systems. Huang al @cite have shown that deep RL is vulnerable to adversarial samples generated by the Fast Gradient Sign Method @cite . Their experimental results demonstrate that both white-box and black-box attacks are effective, even though the less knowledge the adversary has, the less effective the adversarial samples are.
- Behzadan & Munir @cite establish that adversaries can interfere with the training process of DQNs, preventing the victim from learning the correct policy. Specifically, the attacker applies minimum perturbation to the state observed by the target, so that a different action is chosen as the optimal action at the next state. The perturbation is generated using the same techniques proposed against DNN classifiers. In addition, the authors demonstrate the possibility of policy manipulation, where the victim ends up with choosing the actions selected by the adversarial policy.
- Lin al @cite propose two kinds of attacks against deep reinforcement learning agents. In , instead of crafting the state at each time step, the adversary identifies a subset of most vulnerable steps, and uses the C &W attack @cite to perturb the corresponding states. In , the adversary uses sampling to iteratively find a sequence of actions that will take the agent to the target state, and craft the current state so that the agent will follow the next required action.
- A lot of research in recent years have been focused on the autonomous driving domain with nVidia releasing dedicated cards like the nVidia PX2 for the purpose. @cite has described an interesting method that shows how semantic features learned by Convolutional Neural Networks(CNN) can be transferred. They were motivated by the absence of perfectly annotated large datasets for the autonomous driving program. @cite have shown that reinforcement learning and transfer learning can be used to train models for autonomous vehicles.
- A comprehensive paper by @cite reveals the problems faced by autonomous driving, the available datasets and the state-of-art methods. The paper provides an in-depth analysis of different methods used in autonomous vehicles such as pedestrian detection, optical flow, 3-D reconstruction, object recognition and segmentation etc. The paper highlights problems such as the limited availability of high quality optical flow datasets that directly affect the quality of the trained models.
- Semantic Segmentation plays a very important role in understanding and interpreting a scene. @cite considers an ensemble model incorporating knowledge transfer based on drones for the semantic segmentation of aerial images.
- Previous work include those of Chatterjee, A. @cite for using a Neuro-Fuzzy EKF-based approach for SLAM problems, and Ventura, J. @cite for performing localization using mobile phone. Most of these projects were done mostly with a keyframe-based approach, where if a good set of points in the image stream is detected to be tracked in the next frame, those points are designated as . A good keyframe algorithm used in earlier works is called the Harris Corner Detector @cite . The basic idea is to come up with an image region which could be used for tracking if there are shifts in all directions. Mathematically this could be formulated by recording any change in approaches when shifting the window by (x,y) large for every (x,y) on the unit circle. This change is a weighted sum of square differences and denoted by S(x,y). When used with Faussian kernel with fixed variance @math , it could be written as:
- Further, a few more algorithms which are also good in detecting keypoints like FAST Corner detector presented by Roston and Drummond in 2006 @cite and Laplacian of Gaussian (LoG) blob detector @cite which finds blobs in image instead of corners identified using Laplacian on the smoothed image.
- Additionally, CNN based SLAM @cite can be used for identifying the depth predictions and feeding information to the keyframe initializer and for semantic label fusion, but efficiently most of the work is done using the depth or stereo camera. In particular, the predicted depth map is used as input for Kellar's Point-Based Fusion (RGB-D SLAM) algorithm, but it lacks sharp details mostly due to blurring artifacts.
- Automatic fact-checking was envisioned in @cite as a multi-step process that includes () identifying check-worthy statements @cite @cite @cite , () generating questions to be asked about these statements @cite , () retrieving relevant information to create a knowledge base @cite , and () inferring the veracity of the statements, e.g., using text analysis @cite @cite or external sources @cite @cite .
- In follow-up work, @cite developed ClaimRank, which can mimic the claim selection strategies for each and any of the nine fact-checking organizations, as well as for the union of them all. Even though trained on English, it further supports Arabic, which is achieved via cross-language English-Arabic embeddings.
- Recently, learning-based methods have achieved dramatic advantages against the model based methods. With the seminal exploration of employing deep learning in SR task @cite @cite , the variational approaches with deep neural networks have been dominated single image SR. Dong @cite propose to use a deeper network with low-resolution image as input to learn the SR mapping. Kim @cite propose VDSR -- a very deep network with residual learning and show the performance improvement by using deep networks. Ledig @cite introduce residual blocks into SR network and propose SRResNet, which makes it possible to train deeper networks. Lim @cite further expand the network size and improve the residual block by removing the Batch Normalization Layers. Zhang @cite propose a deep network with dense connection and Wang @cite propose to use residual in residual dense block to improve the training stability and network size. Zhang @cite propose residual channel attention blocks and indicate that deeper networks may be easier to achieve better performance than wider networks. As can be seen, most recently successful SR methods employ very deep networks with a large number of parameters, which leads to a high risk of overfitting.
- The method of choice to train on similar but different examples to the training data is known as data augmentation @cite . The most common methods of data augmentation include some basic image processing operations, e.g., random scale, random crop, horizontal vertical flip and image affine transformation. In addition to the basic image processing operations, Zhong @cite propose to augment data by randomly erasing part of the image. Inoue @cite propose to synthesize a new sample from one image by overlaying another image randomly chosen from the training data. Zhang @cite propose to synthesize new samples using the linear combination of training samples. DeVries @cite improves regularization of networks by masking out square region of training images. Geirhos @cite reduces bias toward textures by introducing stylized image data for training. Cubuk @cite presents AutoAugment to learn the best augmentation policies from data. Besides, Generative adversarial networks (GANs) have also been used for the purpose of generating additional data @cite @cite @cite @cite @cite @cite . Most of the existing data augmentation methods are proposed and studied for high-level tasks, and there exists few work to study the effects of different data augmentation methods on the low-level task such as SR.
- Multimodal learning is an established technique in DL research. Our work takes inspiration from earlier research that demonstrated using different data modalities can improve model accuracy @cite . However, to the best of our knowledge existing multimodal learning models operate primarily on different streams of synchronous raw data, for example a video stream and its corresponding audio stream, or an image and its respective text caption. In contrast, there has been limited research in using multimodal learning to combine traditional feature engineering with representation learning, and there currently exist no examples of multimodal learning in chemistry.
- While adapting MT systems to new languages is a long-standing challenge @cite @cite , multilingual NMT is highly promising in its ability to abstract across language boundaries @cite @cite @cite . Results on multilingual training for low-resource translation @cite @cite further demonstrates this potential, although these works do not consider adaptation to new languages, the main focus of our work. Notably, we did not examine partial freezing of parameters, another method proven useful for cross-lingual adaptation @cite ; this is orthogonal to our multi-lingual training approach but the two methods could potentially be combined. Finally, unsupervised NMT approaches @cite @cite @cite require no parallel data, but rest on strong assumptions about high-quality comparable monolingual data. As we show, when this assumption breaks down these methods fail to function, while our cold-start methods achieve non-trivial accuracies even with no monolingual data.
- Research on the predictability of stock markets has a long history in the financial literature e.g., @cite @cite . Although opinions differ regarding the efficiency of markets, many widely accepted studies show that financial markets are to some extent predictable @cite @cite @cite @cite . Two major classes of work which attempt to forecast financial time-series are, broadly speaking, statistical parametric models and data-driven machine learning approaches @cite . Traditional statistical methods generally assume that the time-series under study are generated from a parametric process @cite . There is, however, agreement that stock returns behave in more complex ways, typically highly nonlinearly @cite @cite . Machine learning techniques are able to capture such arbitrary nonlinear relationships with little, or no, prior knowledge regarding the input data @cite .
- Arguably, one of the key contributions of modern deep learning is the addition of feature extraction and representation as part of the learned model. The Convolutional Neural Network (CNN) @cite is a prime example, in which information extraction, in the form of filter banks, is automatically tuned to the utility function that the entire network aims to optimise. CNNs have been successfully applied to various application domains, for example, object tracking @cite , object-detection @cite and segmentation @cite . However, there have been but a few published works that adopt CNNs to analyse financial microstructure data @cite @cite @cite and the existing CNN architectures are rather unsophisticated and lack of thorough investigation. Just like when moving from AlexNet'' @cite to VGGNet'' @cite , we show that a careful design of network archiecture can lead to better results compared with all existing methods.
- The Long Short-Term Memory (LSTM) @cite was originally proposed to solve the vanishing gradients problem @cite of recurrent neural networks, and has been largely used in applications such as language modelling @cite and sequence to sequence learning @cite . Unlike CNNs which are less widely applied in financial markets, the LSTM has been popular in recent years, @cite @cite @cite @cite @cite @cite @cite @cite all utilising LSTMs to analyse financial data. In particular, @cite uses limit order data from 1000 stocks to test a four layer LSTM model. Their results show a stable out-of-sample prediction accuracy across time, indicating the potential benefits of deep learning methods. To the best of our knowledge, there is no work that combines CNNs with LSTMs to predict stock price movements and this is the first extensive study to apply a nested CNN-LSTM model to raw market data. In particular, the usage of the Inception Model in this context is novel and is essential in inferring the optimal decay rates'' of the extracted features.
- There are several algorithms to compute a longest increasing subsequence of a sequence @math , if no comparison errors happen. Typically, they are based on a common underlying algorithmic idea: They process the elements one by one and maintain for each length found so far the increasing subsequence of this length that ends with the smallest possible element seen so far. We shall call this algorithmic idea the to compute a longest increasing subsequence. The running time of the Core-Algorithm is @math in the decision-tree model (see for instance @cite @cite @cite ). This time complexity is tight, as shown in @cite . In the RAM model, where one can also inspect the values, the algorithm can be implemented to run in @math time @cite @cite . All the results can be parameterized to @math or @math , respectively, where @math is the length of the longest increasing subsequence.
- The longest increasing subsequence of @math is also the between @math and the sorted sequence of the elements in @math . This implies an @math time (or @math time if optimized) algorithm to find the longest increasing subsequence when using the standard dynamic programming technique that is used to find longest common subsequences @cite @cite .
- Activity recognition is an important research topic and has been extensively studied for a long time. In the past few years, tremendous progress has been made due to the introduction of large datasets @cite @cite and the developments on deep neural networks @cite @cite @cite @cite @cite . Two-stream network @cite learned both spatial and temporal features by operating 2D ConvNet on single frames and stacked optical flows. C3D @cite used Conv3D filters to capture both spatial and temporal information directly from raw video frames. More recently, improvements on top of the C3D architecture @cite @cite @cite as well as advanced temporal building blocks such as non-local modules @cite were proposed to further boost the performance. However, the assumption of well-trimmed videos limits the application of these approaches in real scenarios, where the videos are usually long and untrimmed. Although they do not consider the difficult task of localizing activity instances, these methods are widely used as the backbone network for the detection task.
- Recognizing objects at vastly different scales is a fundamental challenge in computer vision. To alleviate the problems arising from scale variation, multi-scale pyramidal modeling forms the basis of a standard solution @cite and has been extensively studied in the spatial domain. For example, independent predictions at layers of different resolutions are used to capture objects of different sizes @cite , training is performed over multiple scales @cite , inference is performed on multiple scales of an image pyramid @cite , feature pyramid is directly constructed from the input image @cite .
- Meanwhile, the multi-scale modeling for temporal activity detection is still under-explored: Shou @cite used a multi-scale sliding window to generate snippets of different lengths, however, such method is often inefficient during runtime due to the nature of sliding window; Zhao @cite used temporal pyramid pooling for modeling multi-scale structures without considering complex motion dynamics, since those features were directly pooled at different levels. In this paper, we provide a comprehensive study on temporal multi-scale modeling and propose an efficient end-to-end solution.
- Previous works on activity detection mainly use sliding windows as candidates and classify video clips inside the window with activity classifiers trained on multiple features @cite . Many recent works adopt a proposal-plus-classification framework @cite @cite @cite @cite @cite by generating segment proposals and classifying activity categories for each proposal: some of them focus on designing better proposal schemes @cite @cite @cite , while others focus on building more accurate activity classifiers @cite @cite @cite . Along this line of attack, Xu @cite proposed an end-to-end trainable activity detector based on Faster-RCNN @cite . Buch @cite investigated the use of gated recurrent memory module in a single-stream temporal detection framework. However, all these methods rely on feature maps with a fixed temporal resolution and fail to utilize a multi-scale pyramidal architecture for handling instances with varying temporal scales.
- A few very recent works @cite @cite @cite @cite have started to model temporal scales with a multi-tower network @cite or a multi-scale feature hierarchy @cite @cite , and incorporated temporal contextual information @cite @cite . Our method differs from all these approaches in that we identify three unique modeling problems specific to temporal activity detection and propose to solve them in one single multi-scale pyramidal network. We detail our contributions below.
- Multi-modal approaches have been widely implemented for emotion recognition @cite @cite @cite @cite @cite @cite @cite . E.g., @cite used a multi-modal deep belief network (DBN) to extract features from face, body gesture, voice and physiological signals for emotion classification. @cite classified the combination of EEG and eye movement signals into three affective states. But, very few of them explored SSL. To the best of our knowledge, only @cite proposed an enhanced multi-modal co-training algorithm for semi-supervised emotion recognition, but its shallow structure is hard to capture the high-level correlation between different modalities. In addition, most prior work in this field assumes that all modalities are available at all times @cite @cite , which is not realistic in practical environments. In contrast to the above methods, our framework naturally allows us to perform multi-modal emotion recognition within SSL and incomplete-data situations.
- The variational autoencoder (VAE) @cite @cite is one of the most popular deep generative models (DGMs). VAE has shown great advantages in semi-supervised classification @cite @cite . E.g., @cite proposed a semi-supervised VAE (M2) by modeling the joint distribution over data and labels. maaloe2016auxiliary proposed the auxiliary DGMs (ADGM and SDGM) @cite by introducing auxiliary variables, which improve the variational approximation. However, these models cannot effectively deal with multi-view data, especially in incomplete-view case. Our proposed semi-supervised multi-view DGMs distinguish our method from all existing ones using VAE framework @cite @cite @cite @cite @cite .
- For a recent survey on molecular communication, see @cite . Although there was much earlier work on demodulation, see e.g. @cite @cite @cite , this paper differs from the earlier work in two key aspects. First, most earlier work assumed that the demodulation is based on one sample point per symbol; however, this work assumes that demodulation is based on the continuous history of the number of active receptors. Second, most earlier work did not consider a demodulator which is made entirely from chemical reactions.
- Our assumption of using the continuous history of the number of active receptors for demodulation leads to a demodulator which uses analog filters. The use of analog filters for demodulation was studied in @cite @cite @cite , but no molecular circuit realisation was provided. A recent work @cite presented two different molecular circuits for demodulation but their circuits used one sample point per symbol for demodulation rather than continuous history.
- There were other examples of using molecular circuits for molecular communication. The paper @cite presented a biological circuit for molecular communication from a system-theoretic perspective. There was also work on using analog circuits for soft detection @cite and parity check decoder @cite . The key difference between these few pieces of work and ours is that they use one sample point per symbol. Lastly, there is also work on using chemical reactions to produce transmission signals for molecular communication, see @cite .
- The use of chemical reactions to implement analog computation is an active area of research in molecular computing and synthetic biology, see @cite @cite @cite @cite . However, the problem of using chemical reactions to implement an analog filter based demodulator does not seem to have been done before.
- Recent approaches to learn from noisy web data can be roughly classified into two categories. (1) Methods aim to directly learn from noisy labels. This group of approaches mainly focus on noise-robust algorithms @cite @cite @cite , and label cleansing methods which aim to remove or correct mislabeled data @cite @cite . However, they generally suffer from the main challenge of identifying mislabeled samples from hard training samples, which are crucial to improve model capability. (2) Semi-supervised learning approaches have also been developed to handle these shortcomings, by combining the noisy labels with a small set of clean labels @cite @cite @cite . A transfer learning approach solves the label noise by transferring correctness of labels to other classes @cite . The models trained on this subset are generalized to a larger dataset with unlabelled or weakly-labelled data @cite . Unlike these approaches, we do not propose a noise-cleansing or noise-robust or semi-supervised algorithm. Instead, we investigate improving model capability of the standard neural networks, by introducing a new training strategy that alleviates negative impact of the noisy labels.
- Our work is closely related to the work of @cite , which is able to model noise arising from missing, but visually present labels. The method in @cite is conditioned on the input image, and was designed for multiple labels per image. It does not take advantage of cleaning labels, and the focus is on missing labels, while our approach works reliably on the highly noisy labels, without any cleaned (manual annotation). Our learning curriculum is designed in a completely unsupervised manner.
- Given a joint probability distribution @math of input data @math and the observed relevant random variable @math , the information bottleneck (IB) method seeks a representation @math such that the mutual information @math is minimized, while preserving the mutual information @math @cite . @math can be seen as a measure of the predicative power of @math on @math , and @math can be seen as a compression measure. Hence, the information bottleneck is designed to find the trade off between the accuracy and compression. @cite first used the information bottleneck principle to analysis the deep neural networks theoretically, but no practical models are derived from the IB model. @cite presents a variational approximation to the information bottleneck so that the IB-based models can be parameterized by the neural networks.
- Recommender systems play an essential role in many Internet-based services @cite , such as e-commerces, and have arouse the great attention from both industry and academia. The relevant works of this study can be concluded into two main paradigms: the and the .
- Nichols and Wyman @cite describe a real-time technique for rendering indirect illumination using multi-resolution splatting. They use min-max mipmaps to find the discontinuities in the geometry. Using these discontinuities, the image space is hierarchically divided into smaller squares, so that areas with higher-frequency components obtain a finer resolution. After the image is completely split into such splats' of an appropriate size, the indirect illumination is rendered in all resolutions and the layers are then combined by upsampling to produce the final image. Our technique differs from the algorithm presented by Nichols and Wyman among other things in the method used to decide which resolution to render in. We can apply more flexible filters depending on the situation, while their approach using min-max mipmaps can only find geometric discontinuities. We also use a different approach to combine the final images that prevents visible artifacts. Finally, our technique is not only specialized for indirect illumination using Reflective Shadow Maps, but can also be applied and optimized for various lighting effects due to its high flexibility.
- @cite use variable resolutions for soft shadow mapping in screen space. Again our approach is more flexible and can be applied to a multitude of screen space effects.
- Screen Space Ambient Occlusion (SSAO) is a real-time approximation of the occlusion of ambient light by local geometry. The technique was first presented by Mittring @cite and further developed and improved (e.g. by @cite ).
- Screen Space Global Illumination as, for example, described by @cite generalizes SSAO to not only dim ambient illumination but also add indirect illumination from other surfaces visible on the screen. The light transport between chosen samples close to a pixel is calculated inducing information from the G-Buffer.
- During the past few years, many community detection algorithms have been proposed, see @cite @cite @cite @cite for an overview. In this section, we take a closer look at the algorithms and concepts used in the current research.
- is a widely used measure optimized by many community detection algorithms. It was first proposed in @cite and is defined as follows where @math is a resolution parameter @cite . The intuition behind modularity is the following: the first term in is the fraction of intra-cluster edges, which is expected to be relatively high for good partitions, while the second term penalizes this value for having too large communities. Namely, the value @math is the expected fraction of intra-cluster edges if we preserve the degree sequence but connect all vertices randomly, i.e., if we assume that our graph is constructed according to the configuration model @cite .
- Modularity was originally introduced with @math and many community detection algorithms maximizing this measure were proposed. However, it was shown in @cite that modularity has a resolution limit, i.e., algorithms based on modularity maximization are unable to detect communities smaller than some size. Adding a resolution parameter allows to overcome this problem: larger values of @math in general lead to smaller communities. However, tuning @math is a challenging task. In this paper, we propose a solution to this problem.
- Likelihood optimization algorithms are also widely used in community detection. Such methods are mathematically sound and have theoretical guarantees under some model assumptions @cite . The main idea is to assume some underlying random graph model parameterized by community assignments and find a partition @math that maximizes the likelihood @math , which is the probability that a graph generated according to the model with communities @math exactly equals @math .
- The standard random graph model assumed by likelihood maximization methods is the stochastic block model (SBM) or its simplified version --- planted partition model (PPM). In these models, the probability that two vertices are connected by an edge depends only on their community assignments. Recently, the degree-corrected stochastic block model (DCSBM) together with the degree-corrected planted partition model (DCPPM) were proposed @cite . These models take into account the observed degree sequence of a graph, and, as a result, they are more realistic. It was also noticed that if we fix the parameters of DCPPM, then likelihood maximization based on this model is equivalent to modularity optimization with some @math @cite . Finally, in a recent paper @cite the independent LFR model (ILFR) was proposed and analyzed. It was shown that ILFR gives a better fit for a variety of real-world networks @cite . In this paper, to illustrate the generalization ability of the proposed hyperparameter tuning strategy, in addition to the Louvain algorithm, we also use parametric likelihood maximization methods based on PPM and ILFR.
- In the early stage, video captioning methods are mainly template-based language models @cite @cite @cite . These methods follow a bottom-up paradigm, which first predicts semantic concepts or words, like objects, scenes and activities, and then generates sentences according to pre-defined language templates. These methods heavily rely on the template definition and the predicted video concepts, which limits the diversities of generated sentences. Recently, inspired by the development of deep learning and neural machine translation (NMT) @cite , many sequence learning based models @cite @cite @cite @cite are proposed to address video captioning problem. Regarding video captioning as a translating'' process, these methods construct the encoder-decoder structures to directly generate sentences from the video content.
- @cite make the early attempt to generate video descriptions using encoder-decoder structure, but they simply apply mean pooling over individual frame features to obtain video representation, which ignores the temporal information of ordered video frames. For addressing this issue, the following works @cite @cite @cite make advances by using temporal attention mechanism, as well as taking LSTM-based encoders to learn long-term temporal structures. @cite achieve the progress by further considering the different characteristics of video frames. They propose to adaptively capture the regions-of-interests in each frame, then learn discriminative features based on these regions-of-interests for better video captioning. @cite propose the SeqVLAD method, which performs feature aggregation on frame features to exploit fine spatial information in video content. However, these methods mainly work on the global frame or salient regions without discrimination on specific object instances, which cannot well capture the temporal evolution of each object in video. In this work, we propose the OA-BTG approach, which constructs bidirectional temporal graph on the objects across video frames to captures their temporal trajectories. In addition, we also perform representation learning on the temporal trajectories, which exploits the object-awareness to boost video captioning.
- There are also some works @cite @cite @cite @cite @cite that exploit multi-modal features for video captioning. Besides frame features extracted by popular 2D CNNs, they also exploit motion features extracted by C3D @cite or audio features @cite , where they mine the complementarities among multi-modal information to boost the video captioning performance. Different from them, our OA-BTG approach takes only visual features, which mainly focuses on capturing detailed temporal evolutions of objects by bidirectional temporal graph and learning discriminative features through object-aware feature aggregation.
- @cite used the pre-trained 2D VGG model @cite and Inception V4 model @cite to train on a set of 6,400 slices from the axial view of 200 patients' MRI scans from the OASIS dataset to perform two-class (Normal vs. Alzheimer's) classification, where 32 slices are extracted from each patient's MRI scans based on the entropy of the image.
- @cite followed a similar approach using a 2D VGG model @cite on the ADNI dataset to train on a set of 4,800 brain images augmented from 150 subjects' scans. For each subject's scan, they selected 32 slices based on the entropy of the slices to compose the dataset. They then proceed to shuffle the data and split it with a 4:1 training to testing ratio to perform the more complicated task of three-class classification for Normal, Mild Cognitive Impairment, and Alzheimer.
- @cite pretrained a sparse autoencoder on randomly generated @math patches and used the weights as part of a 3D CNN to perform classification on the ADNI dataset, splitting randomly by MRI. @cite employed a similar approach of using sparse autoencoder to pretrain on scans, but instead of randomly selecting patches from the training dataset, they used scans from the CADDementia dataset @cite , and performed ten-fold cross-validation on the ADNI dataset.
- @cite extracted regions of interest around the lobes of the hippocampus using atlas Automated Anatomical Labeling (AAL), augmented their data by applying random shifts of their data by up to two pixels and random Gaussian blur with @math up to 1.2, and classified using a Siamese network on each of the regions of interest.
- More recently, @cite trained a 3D DensetNet with ensemble voting on the ADNI dataset to achieve the state-of-the-art accuracy on three-class classification. They split by patients but treated MRIs of the same patient that are over three years apart as different patients, giving away some information from the training to testing process.
- Unlike most of the aforementioned papers that report high performance but do not explicitly mention their training and testing data split methodology, @cite pointed out the problem of potentially giving away information from the training set into the testing set when splitting randomly by MRI scans, and showed that splitting MRI data by patient led to worse model performance. However, they only report two-class classification (Normal vs. Alzheimer's) in their results. They also only used a subset that is less than half the size of the MRIs available in ADNI, even though they reported empirical studies showing that .
- @cite also reported on the automatic detection of bug fix patterns at the AST level. The main differences between their work and our work are the following. First, they focused on 18 bug fix patterns from @cite while we focused on 25 patterns from @cite . Second, they used the ChangeDistiller AST differencing algorithm @cite while we use GumTree @cite . The latter outperforms the former by maximizing the number of AST node mappings, minimizing the edit script size, and detecting better move actions @cite . Moreover, they pointed out that ChangeDistiller works at the statement level, preventing the detection of certain fine-grain patterns. Third, they formalized a representation for change patterns and used this representation to specify patterns. Then, to detect a pattern, a match of its specification must happen in a given edit script. However, such representation is based on change type (e.g. addition) over code elements (e.g. if ), which does not support the specification of patterns such as .
- The paper that is most closely related to ours is the one mentioned above, by @cite . Using an interesting reduction from the local search problem on a class of graphs known as Kneser graphs, they show that the problem of finding an EFX allocation requires an exponential number of queries, even for two agents with identical valuations. They also examine when EFX can be achieved in conjunction with other properties such as Pareto optimality, and establish the existence of allocations satisfying an approximate version of EFX for agents with subadditive valuations.
- From hand-designed to data-driven features, deep learning has played a significant role in diverse fields where the artificial intelligence (AI) community has struggled for many years. Certainly, bioinformatics can also benefit from deep learning. In recent years, many public reviews @cite @cite have been proposed to discuss deep learning applications in bioinformatics research. For example, @cite applying deep belief networks (DBN) to the frequency components of EEG signal to classify left-hand and right-hand motor imagery skills. @cite used CNN to decode P300 patterns, and @cite used CNN to recognize rhythm stimuli. @cite conducted an emotion detection and facial expressions study with both EEG signal and face images by RNN.
- Recent work proposes visualizing multiple facets' of the neuron's selectivity by obtaining multiple images from different random initializations @cite , using a diverse set of highly activating images as initializations @cite , or using a generative image model to sample highly-activating images @cite .
- These methods do not explicitly specify an objective to produce a diverse set of images. In contrast, we optimize a batch of images to drive the neuron of interest strongly while simultaneously being as distinct from each other as possible. Recent concurrent work @cite introduces a similar idea, albeit with a different loss function based on texture representations @cite @cite .
- Long al @cite first introduced fully convolutional networks (FCN), while U-Net was introduced by Ronneberger al @cite . They both share a key idea: skip connections. In FCN, up-sampled feature maps are summed with feature maps skipped from the encoder, while U-Net concatenates them and add convolutions and non-linearities between each up-sampling step. The skip connections have shown to help recover the full spatial resolution at the network output, making fully convolutional methods suitable for semantic segmentation. Inspired by DenseNet architecture @cite , Li al @cite proposed H-denseunet for liver and liver tumor segmentation. In the same spirit, Drozdzal al @cite systematically investigated the importance of skip connections, and introduced short skip connections within the encoder. Despite the minor differences between the above architectures, they all tend to fuse semantically dissimilar feature maps from the encoder and decoder sub-networks, which, according to our experiments, can degrade segmentation performance.
- Then, extra corpus such as social relationship is incorporated into recommendation for a further improvement, @cite . However, because the additional corpus is difficult to obtain and is often full of noise, this methodology is still under limitation.
- One of the most common ways to evaluate GANs is the Inception score @cite . It uses an Inception network @cite pre-trained on ImageNet to compute logits of generated images. The score is given by: where @math is a generated image sampled from the learned generator distribution @math , @math is the expectation over the set of generated images, @math is the KL-divergence between the conditional class distribution @math (for label @math , according to the Inception network) and the marginal class distribution @math ]. By definition, Inception score does not consider real images at all, and so cannot measure how well the generator approximates the real distribution. This score is limited to measuring only the diversity of generated images. Some of its other limitations, as noted in @cite , are: high sensitivity to small changes in weights of the Inception network, and large variance of scores.
- chet Inception distance The recently proposed Fr ' e chet Inception distance (FID) @cite compares the distributions of Inception embeddings (activations from the penultimate layer of the Inception network) of real ( @math ) and generated ( @math ) images. Both these distributions as modeled as multi-dimensional Gaussians parameterized by their respective mean and covariance. The distance measure is defined between the two Gaussian distributions as: where @math , @math denote the mean and covariance of the real and generated image distributions respectively. FID is inversely correlated with Inception score, and suffers from the same issues discussed earlier.
- Sliced Wasserstein distance (SWD) @cite was used to evaluate high-resolution GANs. It is a multi-scale statistical similarity computed on local image patches extracted from the Laplacian pyramid representation of real and generated images. A total of 128 @math local patches for each level of the Laplacian pyramid are extracted per image. While SWD is an efficient approximation, using randomized projections @cite , of the Wasserstein-1 distance between the real and generated images, its utility is limited when comparing a variety of GAN models, with not all of them producing high-resolution images (see our evaluation in ).
- Precision and recall measures were introduced @cite in the context of GANs, by constructing a synthetic data manifold. This makes it possible to compute the distance of an image sample (generated or real) to the manifold, by finding its distance to the closest point from the manifold. In this synthetic setup, precision is defined as the fraction of the generated samples whose distance to the manifold is below a certain threshold. Recall, on the other hand, is computed by considering a set of test samples. First, the latent representation @math of each test sample @math is estimated, through gradient descent, by inverting the generator @math . Recall is then given by the fraction of test samples whose L2-distance to @math is below the threshold. High recall is equivalent to the GAN capturing most of the manifold, and high precision implies that the generated samples are close to the manifold. Although these measures bring the flavor of techniques used widely to evaluate discriminative models to GANs, they are impractical for real images as the data manifold is unknown, and their use is limited to evaluations on synthetic data @cite .
- In summary, evaluation of generative models is not a easy task @cite , especially for models like GANs. We bring a new dimension to this problem with our GAN-train and GAN-test performance-based measures, and show through our extensive analysis that they are complementary to all the above schemes.
- With the introduction of DeepPose'' by Toshev al @cite , research on human pose estimation began to shift from classic approaches based on pictorial structures @cite @cite @cite @cite @cite @cite @cite @cite @cite to deep networks. Subsequent methods include @cite , which simultaneously captures features at a variety of scales using heatmaps, and @cite , which employs a hierarchical model to capture the relationships between joints. A popular approach by Newell al @cite uses conv-deconv architecture and residual model to efficiently generate the heatmap without the need for any hierarchical processing. This approach has been further extended by using visual attention @cite and feature pyramid @cite . However, these methods rely on the network capacity to capture the highly articulated human pose and to handle occlusion, without modeling the uncertainty in pose explicitly.
- The second paradigm, used in several state-of-the-art methods @cite @cite @cite @cite , involves propagating the mask from the previous frame using optical flow and then refining these estimates using a fully convolutional network. The methods proposed in @cite and @cite expand this idea by using a network to calculate a re-identification (ReID) embedding vector for proposed masks and using this to improve the object re-identification after an object has been occluded. @cite improves upon the mask propagation paradigm by training on a huge set of augmented images generated from the first-frame ground truth.
- All of the above networks required significant engineering effort. The increasing complexity of neural network designs has encouraged the development of methods for automating (NAS). use an RNN to generate network descriptions and filter the options using reinforcement learning. Storing such a large quantity of possible networks is expensive, and their evaluation strategy utilised 450 GPUs over the course of 3 days. To address this, @cite propose giving all models access to a shared set of weights, achieving similar performance to with a single GPU in less than 24 hours. Subsequent works have made extensive use of this technique . However, it has been observed that under the constrained architecture search space of the above methods, random architecture search provides a competitive baseline . In particular, @cite show that weight sharing hampers the ability of candidate networks to learn and causes many NAS techniques to find suboptimal architectures.
- In @cite , Gerber uses statistical topic modeling on tweets that have geolocation to predict how likely 20 different types of crimes are to happen in individual cells of a grid that covers the city of Chicago. This work is a large scale approach for predicting future crime locations, while we detect codes in individual tweets related to future violence. Another important difference is that @cite is meant to assist criminal justice decision makers, whereas our efforts are community based and have solid grounding in social work research.
- Using the pixel-wise classification loss, CNN usually ignores the micro context between pixels and the macro context between semantic parts. Conditional random fields (CRFs) @cite @cite @cite are one of the common methods to enforce spatial contiguity in the output label maps. Served as a post-process procedure for image segmentation, CRFs further fine-tune the output map. However, the most common used CRFs are with pair-wise potentials @cite @cite , which has very limited parameters and handles low-level inconsistencies with a small scope. Higher-order potentials @cite @cite have also been observed to be effective in enforcing the semantic validity, but the corresponding energy pattern and the clique form are usually difficult to design. In summary, the utilization of context in CNN remains an open problem.
- Adversarial networks have demonstrated the effectiveness in image synthesis @cite @cite @cite @cite @cite . By minimizing the adversarial loss, the discriminator leads the generator to produce high-fidelity images. In @cite , Luc add the adversarial loss for training semantic segmentation and yield the competitive results. Similar idea then has been applied in street scene segmentation @cite and medical image segmentation @cite @cite . Contemporarily, an increasing body of literature @cite @cite report the difficulty of training the adversarial networks on the high-resolution images. Discriminator can easily recognize the fake high-resolution image, which leads to the training unbalance. The generator and discriminator are prone to stuck in a local minimum.
- Person search has drawn much research interest since the publication of two large scale datasets: CUHK-SYSU @cite and PRW @cite . Zheng al @cite conduct a detailed survey on various and propose to solve the person search problem in two separate models, one for detection and another for re-ID. Other works propose to solve the problem in an by employing the Faster R-CNN detector @cite for pedestrian detection and share the base network between detection and re-identification @cite . In @cite , feature discriminative power is increased by introducing center loss @cite during training. Liu al @cite improve the localization policy of Faster R-CNN by recursively shrink the search area from the whole image till achieving precise location of the target person. In this paper, we first made a systematic comparison between and , and show that a separated solution improves both the detection and re-identification results.
- Pedestrian detection is canonical object detection, especially when hand-crafted features are widely used. The classic HOG descriptor @cite is based on local image differences, and successfully represents the special head-shoulder shape of pedestrians. A deformable part model (DPM) @cite is proposed to handle deformations and still uses HOG as basic features. More recently, the integral channel feature (ICF) detectors @cite @cite @cite become popular, as they achieve remarkable improvements while running fast. In recent years, convnets are also employed in pedestrian detection and further push forward the progress @cite @cite @cite @cite . Some works use the R-CNN architecture, which relies on ICF for proposal generation @cite @cite . Aiming for an end-to-end procedure, Faster R-CNN @cite is adopted and it achieves top results by applying proper adaptations @cite @cite . Therefore, we use the adapted Faster R-CNN detector in this paper.
- Recently, attention mechanism @cite @cite @cite @cite @cite has been adopted to learn better features for person re-ID. For instance, HydraPlus-Net @cite aggregates multiple feature layers within the spatial attentive regions extracted from multiple layers in the network. PDC model @cite enriches the person representation with pose-normalized images and re-weights the features by channel-wise attention. In this paper, we also formulate person re-identification as a classification problem, and we propose to emphasize foreground information in the aggregated representation by adding an axillary stream with spatial attention (instance mask) and channel-wise re-weighting (SEBlock), which is similar to HydraPlus-Net and PDC model. However, our work differs from them in that the attention mechanism in our work is introduced with a different motivation, which is to consider the foreground-background relationship instead of local-global or part-whole relationship. In addition, the architecture of our model is more clear and concise, along with more practical training strategy without multi-staged fine-tuning.
- Additionally, several relation-based systems such as Hexastore @cite , RDF-3x @cite @cite @cite , BitMat @cite and TripleBit @cite , employ specialized optimization techniques based on the features of RDF data and SPARQL queries. Hexastore @cite and RDF-3x @cite @cite @cite build a set of indices that cover all possible permutations of S, P and O, in order to speed up the joins. TripleBit @cite uses a two-dimension matrix to represent RDF triples, with subjects and objects as row and predicates as column. Then, it uses '1' to label the relation, otherwise uses '0'. Thus, there are only two '1' in each column, which are easy to be recorded. Furthermore, the triple matrix is divided into submatrices by the same properties and stored by column. Moreover, BitMat @cite numbers every elements of RDF triples and builds bitmap indexes to collect the candidates of queries. Similar to property table, TripleBit and BitMat suffer from the large waste of space. Instead, we save the storage space and facilitate the processing in a way that represents the data as a sparse matrix and only maintains the actual relations in RDF graph.
- Recently, a number of graph-based approaches were proposed to store RDF triples in graph models, such as gStore @cite @cite , dipLODocus @math @cite , Turbo @math and AMBER @cite . These graph-based approaches typically see SPARQL query processing as subgraph matching, which help reserve and query semantic information. gStore @cite @cite maps all predicates and predicate values to binary bit strings which are then organized as a VS*-tree. Since every layer of VS*-tree is a summary of the whole RDF graph, gStore has capacity to process SPARQL query efficiently. dipLODocus @math starts by a mixed storage considering both graph structure of RDF data and requirement of data analysis, in order to find molecule clusters and help accelerate queries through clustering related data. Turbo @math @cite develops TurboISO @cite by transforming RDF graphs into normal data graphs while AMBER @cite represents RDF data and SPARQL query as multigraphs.
- We now briefly survey the techniques that use GPUs to improve the performance of database operations. Current database researches identify the computational power of GPUs as a way to increase the performance of database systems. Since GPU algorithms are not necessarily faster than their CPU counterparts, it is important to use the GPU only if it is beneficial for query processing. S. @cite @cite extend CPU GPU scheduling framework to support hybrid query processing in database systems. @cite focuses on accelerating SELECT queries and describes the considerations in an efficient GPU implementation. @cite accelerates the search in big RDF data by exploiting modern multi-core architectures based on GPU. @cite , a new efficient and scalable index is proposed. These data structures have an edge over others in terms of their implementation as a parallel algorithm using the CUDA (Compute Unified Device Architecture) framework. @cite shows that usage of graphic card for intensive calculations over large knowledge bases in RDF format can be another way to decrease computational time.
- There are various approaches to pruning neural networks. Pruning may be performed after training a model to convergence @cite @cite @cite @cite , or throughout the training process such that there are multiple pruning events as the model trains @cite @cite @cite . While these methods share the goal of pruning parameters that appear unimportant to the function computed by the neural network, they differ greatly in their execution. For instance, magnitude pruning uses small-magnitude to indicate unimportance. Despite its simplicity, magnitude pruning has been shown to perform competitively with more sophisticated approaches to pruning @cite .
- As stated in @cite , one role of generalization theory is to provide theoretical insights to guide the search over model classes.'' VC dimension (a measure of model capacity) motivated pruning as a regularizer in the early pruning techniques Optimal Brain Damage (OBD) @cite and Optimal Brain Surgeon (OBS) @cite . The logic was that overfitting can be bounded above by a function of VC dimension, which increases with parameter counts, so fewer parameters guarantees a better bound @cite @cite . Unfortunately, such bounds are so loose that tightening them by reducing parameter counts need not translate to better generalization in practice @cite . Furthermore, more recent generalization bounds suggest that fewer parameters actually makes typical neural networks generalize worse @cite .
- Resource management has been actively researched in many communities. Several works have been applying deep RL to optimally allocate resources, distribute tasks, and optimize power management decisions @cite . DeepRM uses standard deep Q-learning algorithm to formalize resource management as a Tetris game, however, it only work with homogeneous settings and not consider task dependency @cite . A variant of DeepRM leverages convolution neural networks as a backbone network to improve performance in scheduling @cite . Subsequent work in DeepRM, Pensieve applies a resource managing algorithm to video streaming to optimally control the bitrate and successfully reduced buffering @cite . Moreover, Hopfield neural network has been applied to design heterogeneous multiprocessor architecture scheduler @cite . More recent work combines heuristic and learning algorithms, starting from an existing plan and iteratively improving it and successfully applying it in heterogeneous job scheduling task @cite . However, their work follows the general MDP setting where, again, the agent chooses action at every timestep. From the perspective of hardware, recent work has proposed new accelerator architectures which have potential advances @cite .
- Constraining motion to lie on a sphere was first exploited by Ventura @cite to determine the essential matrix (and thus relative pose) between two images with known camera calibration for this particular motion. The assumption of spherical motion reduces the degrees of freedom in the problem from 5 for the general case @cite to 3, enabling an efficient and accurate algorithm for determining relative motion. The author demonstrates the applicability of this solver in a spherical SfM system, though he notes that the accuracy for outward-facing sequences can be poor and deviations from the spherical motion assumption cause the system to fail.
- We build on Ventura's work @cite to design three new solvers for determining the fundamental matrix and calibration when the motion between cameras is constrained to a sphere. Whereas the fundamental matrix for general motion is estimated from 7 @cite or 8-point correspondences @cite , our spherical motions solvers compute the fundamental matrix from as few as 4 point correspondences. We can additionally compute a common radial distortion parameter with 5 or 6 correspondences. These fundamental matrix solvers are integrated into an SfM pipeline suitable for reconstructing panorama-style captures from handheld video sequences. Unlike @cite , our method does not require prior calibration, allowing for more general use, and our system is robust to deviations from spherical motion.
- The majority of ellipse estimation methods fit a curve to a planar set of points. One distinguishes between point-based ellipse fitting methods by considering the nature of the cost function that the algorithms minimise. Methods which explicitly decrease the distance between the points and the ellipse curve are considered geometric methods. The quintessential geometric method is orthogonal distance regression, which minimises the orthogonal distance from a point to the curve @cite @cite @cite @cite @cite @cite @cite . Algebraic methods, on the other hand, try to ensure that the data points satisfy an ellipse implicit equation as accurately as possible. One differentiates between algebraic methods by considering how they penalise the degree to which a data point fails to satisfy an implicit equation @cite @cite @cite @cite . Algebraic methods, in particular, have been the focus of considerable study, and recent works have concentrated on improving their statistical efficacy and accuracy @cite @cite @cite @cite .
- It is possible to obtain data points with sub-pixel coordinates by using sub-pixel edge detection methods @cite . However, sub-pixel techniques usually do not characterise the uncertainty or bias of their estimates, and so one cannot attribute meaningful covariance matrices to the sub-pixel data points. The inability to characterise the uncertainty and bias of the sub-pixel points is a severe limitation and, effectively, violates the modelling assumptions associated with point-based ellipse fitting methods. Moreover, the sub-pixel estimation methods are themselves sensitive to noise and do not take quantisation into account.
- In this section, we review related work on writer identification that considered different data augmentation approaches to address cutting-edge challenges. Some researchers considered data augmentation in intrasets @cite @cite @cite @cite , but this easily led to model overfitting. Two recent studies added extra labeled data into the original data to enlarge the training set, which in turn required a vast amount of extra data to improve the identification results @cite @cite .
- In @cite , Christlein created a combined dataset (MERGED) consisting of 559 scribes with four documents per writer, resulting in 2236 documents from the ICDAR2013 and CVL datasets. Thereby, the training set was enlarged, and the outcomes on the MERGED datasets slightly differ from the image vocabularies that can be calculated from the ICDAR2013 experimental set or the CVL dataset. Furthermore, @cite showed that the identification rate on the CVL test set could be improved by adding additional datasets (ICDAR2011 and IAM @cite ) into the CVL training set. Although existing data augmentation approaches have the capability to improve the identification performance using the extra data, we can imagine that it requires a large amount of extra labeled data. In practice, however, we do not have access to collect a large number of samples for writer identification.
- The representation of a 3D local structure used to rely on traditional geometric descriptors such as Spin Images @cite , PFH @cite , FPFH @cite , SHOT @cite , USC @cite and al, which are mainly produced based on the histograms over local geometric attributes. Recent studies seek to learn descriptors from different representations of local geometries, like volumetric representations of 3D patches @cite , point sets @cite and depth maps @cite . The CGF @cite still leverages the traditional spherical histograms to capture the local geometry but learns to map the high-dimensional histograms to a low-dimensional space for compactness.
- Rather than only using geometric properties, some existing works refer to extracting descriptors from RGB images that are commonly co-registered with point clouds as in scanning datasets @cite @cite @cite and 3D reconstruction datasets @cite @cite . Registration frameworks like @cite @cite @cite @cite use SIFT descriptors @cite as the representations of 3D keypoints based on their projections in single-view RGB images. Besides, the other state-of-the-art 2D descriptors like DeepDesc @cite , L2-Net @cite and al @cite @cite @cite can easily migrate here for the description of 3D local structures.
- The multi-view fusion technique is used to integrate information from multiple views into a single representation. It has been widely proved by the literature that the technique effectively boosts the performance of instance-level detection @cite , recognition @cite @cite and classification @cite compared with a single view. Su al @cite first propose a probabilistic representation of a 3D-object class model for the scenario where an object is positioned at the center of a dense viewing sphere. A more general strategy of multi-view fusion is @cite @cite @cite @cite , which aggregates the feature maps of multiple views via element-wise maximum operation.
- Previous approaches for depth estimation from single images can be categorized into two main groups, methods operating on hand-crafted features, and methods adopting deep neural networks. Earlier works addressing the depth estimation task belong to the first category. @cite introduced photo pop-up, a fully automatic method for creating a basic 3D model from a single photograph. @cite developed Depth Transfer, a non-parametric approach where the depth of an input image is reconstructed by transferring the depth of multiple similar images with warping and optimizing procedures. Ladicky @cite demonstrated the benefit of combining semantic object labels with depth features. @cite introduced a multi-scale conditional random field (CRF) to extract multi-scale context information for depth estimation.
- More recent approaches for depth estimation are based on convolutional neural networks. In the pioneer work @cite , introduced a coarse-to-fine network, and utilized the scale-invariant loss to improve the accuracy of the estimated depth map. This work is further extended in @cite , where the depth estimation, surface normal estimation, and semantic segmentation are integrated into one unified network. @cite considered a loss function with components in both the depth domain and the gradient of depth domain. Fu. et al @cite introduced a spacing-increasing discretization strategy to discretize depth and re-casted depth network learning as an ordinal regression problem. Their method achieves significant accuracy improvement compared to previous methods.
- Most of above works use some backbones with fully connected layer. This will increase the model complexity and computation cost. In addition, the input image size is restricted during the testing. To solve this problem, a fully convolutional network was proposed by @cite . A revised version of this work is introduced in @cite , where randomly sampled sparse depth is adopted together with the RGB image to predict a dense depth map. @cite used a fully-convolutional network to predict the relative depth map, e.g., a relationship between the depths of any two pixels. @cite utilized semi-supervised learning for SIDE, where supervised learning on sparse measurements is complemented with unsupervised learning of the left-right consistency in stereo images. @cite proposed a spatial propagation network to learn the affinity matrix and showed its effectiveness to improve the performance of existing SIDE networks. All of the above CNNs are constructed based on some computationally expensive architecture. In contrast, our DS-SIDENet is constructed by depthwise separable convolution @cite , which is more efficient during the prediction.
- In this section, we review related work in feature attribution, adversarial attack, adversarial defense and detection. A variety of methods have been proposed to assign feature attribution scores. For each specific instance where the model is applied, an attribution method assigns an importance score for each feature, by approximating the target model via a linear model locally around the instance. One popular class of methods assumes the differentiability of the model, and propagates the prediction to features through gradients. Examples include direct use of gradient (Saliency Map) @cite , Layer-wise Relevance Propagation (LWRP) @cite and its improved version DeepLIFT @cite , and Integrated Gradients @cite .
- Another class is perturbation-based and thus model-agnostic. Given an instance, multiple perturbed samples are generated by masking different groups of features with a pre-specified reference value. The feature attribution of the instance is computed according to the prediction scores of a model on these samples. Popular perturbation based methods include Leave-One-Out @cite @cite , LIME @cite and KernelSHAP @cite .
- Note that for these methods, the amount of regularization introduced can be modulated by at least one parameter, for instance the degree of rotation applied to the input image, or the percentage of neurons dropped in Dropout @cite . With most methods, the more regularization is applied, the longer the optimization takes until convergence. Introducing too much regularization can make the optimization unreasonably long, and force stopping before convergence of the validation loss, which leads to a sub-optimal performance. Choosing the right amount of regularization is therefore a key factor. In the proposed method, the regularization effect can be controlled by varying the average number of samples used to create combinations.
- Recently, methods employing neural network regressors trained with global labels have been proposed for biomarker extraction @cite @cite @cite . @cite predict brain age from 3D images of grey matter density computed from MRI scans. @cite predict the number of enlarged perivascular spaces in a 3D region of interest in the brain extracted from a MRI scan. And @cite predict Agatston scores to quantify coronary artery calcifications from 3D non-contrast non-ECG gated chest CT scans. These networks needed hundreds training samples to be trained accurately. With our method, we can achieve the level of interrater agreement only using 25 training scans, which could for instance only require one to two hours of labeling from an expert rater.
- Engineered microbial and mammalian cells are used as production platforms to synthesize commodity or specialty chemicals and pharmaceuticals. Accurate computational models of cell metabolism and protein expression are important to design cell factories and to maximize product yield and quality @cite @cite . Similarly, mathematical models of cell metabolism have been used to identify strategies to improve the efficacy of existing antibiotics @cite .
- The ability of engineers to predict microbial behavior was facilitated in 1990 by the observation that overflow metabolism---an industrially-relevant metabolic behavior---in could be predicted by a relatively simple network of reactions with capacity-constrained flows @cite . Since then, this constrained optimization model of cellular metabolism (often called COBRA) has been applied to over @math species across the tree of life @cite . Metabolic reconstructions today are genome-scale''---i.e., they account for the majority of metabolic genes annotated in the organism's genome---and consist of hundreds to thousands of biochemical reactions and metabolites. For example, the most recent reconstruction of human metabolism includes 13,543 metabolic reactions involving 4,140 metabolites @cite , while the latest multiscale model of metabolism and protein expression for @cite consists of 12,655 reactions and 7,031 components including macromolecules like protein, RNA (ribonucleic acid), and ribosome.
- As of mid-2013, over 640 published studies used COBRA for experimental investigation in various domains of engineering and health sciences @cite . Successful applications of COBRA include engineering microbes to produce commodity or valuable chemicals @cite , developing novel antimicrobials against infectious disease @cite , and discovering new drug targets against cancer @cite .
- In order to make accurate predictions, these models require an accurate @math matrix, which is nowadays reliably reconstructed from extensive knowledgebases of enzyme biochemistry, and genome annotations for thousands of species and strains organisms. The other key ingredient is the cellular objective function, @math . Other objectives, including nonlinear functions, have been tested @cite . For many microbes cultured in nutrient-limiting conditions, maximization of cell growth rate (per unit of limiting nutrient) is an accurate objective function @cite . This particular function corresponds to a @math that is an indicator vector with a @math in the component associated with the reaction that corresponds to the cell growth, and zero everywhere else.
- Currently, system-level understanding of living organisms requires the analysis of large-scale measurements originating from disparate biological processes operating at multiple length and time scales. Such measurements are collectively referred to as omics'', as they involve measuring the complete makeup of a given biological variable (e.g., proteomics attempts to measure the complete protein composition of a cell). Analysis of such omics measurements has shown that, despite the complexity inherent in living systems, relatively simple models are accurate enough for several biological studies. E.g., the gene expression profiles of various cell types can be described in terms of relatively few biological functions @cite .
- Similarly, the metabolic fluxes in a cell can be predicted by assuming that the cell is solving an optimization problem shaped by its evolutionary history @cite . The problem includes constraints that model biochemical reactions'' consuming and producing metabolites at specific stoichiometric ratios, and an objective that depends on the fluxes through the reactions. For example, the metabolism of fast-growing microbes (mid-log phase) is predicted accurately by a linear problem such as . As another example, the metabolism of mammalian cells (e.g., hybridoma), is explained well by minimization of the flux through the reaction that makes reactive oxygen species @cite . Studies have identified alternative objective functions that best predict microbial metabolism under different growth conditions @cite . These objectives include maximizing ATP yield per flux unit and maximizing ATP or biomass yield.
- While the aforementioned studies have focused on using pre-defined optimization problems, a number of studies have investigated data-driven estimation of cellular goals. Two types of method exist. Methods of the first type estimate how important the different chemical reactions are for the cell's operation, i.e., estimate @math in . For example, ObjFind @cite does this through a nonconvex optimization formulation, and the more recent invFBA @cite solves a linear program. The second type, and the focus of our work, estimate the stoichiometric coefficients of a new chemical reaction, i.e., estimate a new column for matrix @math in . This approach is important because often the known biochemical reactions are not enough to explain observed data. We want to extend the model complexity automatically by learning new chemical reactions, i.e., new columns for @math .
- The main drawback of estimating new reactions is that it requires solving nonconvex optimization problems. Currently, only the BOSS tool @cite does this. BOSS was shown to recover known biomass reactions successfully in synthetic experiments involving less than @math reactions and @math metabolites. This is not large enough for real-life applications, which can involve thousands of reactions metabolites. Note that BOSS (a) uses an off-the-shelf solver (e.g. MINOS) that cannot exploit parallelism, (b) cannot induce sparsity on the learnt reaction, and (c) has not been tested on large problems.
- Pursuits were also used in mixed reality. VR benefits from using Pursuits during interaction, especially when moving in VR @cite , and when interacting with occluded targets @cite . Pursuits was also employed in augmented reality glasses @cite .
- There are two predominant implementations of Pursuits detection for interaction, one of which uses the Euclidean distance between the gaze estimates and target positions @cite @cite @cite @cite , while the other one employ Pearson's product moment correlation @cite @cite @cite @cite @cite @cite @cite @cite @cite .
- Although there has been a great deal of efforts to detect communities, to the best of our knowledge, no previous work has been proposed to uncover the existence of antagonism and alliance between communities. However, some efforts have been made @cite @cite @cite @cite @cite @cite to detect only antagonistic communities. These methods can be roughly divided into two main categories. First category includes the methods @cite @cite utilizing frequent patterns in users' ratings to mine antagonistic communities. Second category includes the methods @cite @cite @cite @cite utilizing signed networks, having trust and distrust links, to detect antagonistic communities. A majority of these methods @cite @cite @cite detect a pair of subgraphs with most trust links preserved between the members of each subgraph and most distrust links remained between the members of different subgraphs. These methods are limited to detecting only a pair of antagonistic communities. To address this limitation, another method @cite has been proposed to detect multiple antagonistic communities by finding several dense subgraphs with the mentioned property. However, as experiments in @cite show such methods usually end up with large number of small subgraphs due to high sparsity of users' interactions in social media.
- Several works apply adversarial training for the purpose of creating a representation free of a specific attribute @cite @cite @cite @cite (in fact, the original purpose of the method of adversarial training can also be seen as such). To the best of our knowledge, the current paper represents the first application of these ideas in the domain of recommender systems. Furthermore, while all the aforementioned applications experiment with solely one feature at a time, in this work we aim to create a representation free of multiple demographic features.
- Distant supervision is another element of IE that is employed in this research to generate training data for police killing detection. In particular, distant supervision has been used to produce training data for relation extraction @cite @cite @cite @cite @cite and event extraction @cite .
- In another variation of prophet inequalities, the gambler can pick the random variables in her desired order. This variation is known as . We first note, as recently shown by @cite , that any approximation for SPMs implies the same approximation for free-order prophets. The bound on the best possible approximation by also holds for this setting. give a @math approximation for the SPM problem. Recall that under SPMs, the seller approaches the buyers in decreasing order of their prices. This bound of @math was recently surpassed by @cite to @math , and their bound also extends to , in which the random variables arrive in a uniformly random order. Our bound of @math for SPMs, which implies the same bound for free-order prophets by the result of , is the best known bound for free-order prophets so far. More precisely, just like for SPMs and ESP auctions, ours is the first paper to go beyond the @math bound for every @math in the free-order prophets setting.
- false Moved these two paragraphs here. Todo Finally, we compare our result with the results of @cite . Though they have a worse factor, their posted price result is applicable for a more general setting: i.e., applicable both for a fixed-seller-chosen ordering of buyers, and random ordering of buyers, whereas our posted pricing result is applicable only for a fixed-seller-chosen-ordering of buyers (namely, descending order of prices). While the random order setting is more general, note that a random order model for posted pricing does not give any additional benefit over the fixed-ordering when it comes to approximation for second price auctions with eager reserves.
- false The random order prophet inequality problem, or the prophet secretary problem was first studied by , where they show a @math approximation for large @math . They also show that with a single threshold it is impossible to get better than a @math -approximation. show that one can obtain a @math approximation for every @math with non-adaptive thresholds. Finally @cite show that one can obtain a @math approximation for any @math .
- false study the question of approximating the optimal mechanism's revenue via Vickrey auction with personalized reserve prices, and show that for regular distributions, the second price auction with the so-called monopoly reserve prices yield a @math -approximation for regular distributions, and that for irregular distributions no constant factor approximation is possible with the monopoly reserves. study the question of computing the optimal personalized reserve prices in a correlated distribution setting, and show that the problem is NP-hard. The hardness status of this problem is unknown for independent distributions. show that this problem is APX-hard for correlated distributions and give a @math -approximation. Second price auctions with lazy personalized reserve prices are incomparable to second price auctions with eager reserve prices for general correlated distributions, but are within a factor @math of each other @cite . However, when the distributions are independent, or when the buyers are symmetric, show that the optimal eager auction dominates the revenue of the optimal lazy auction.
- Carl @cite explored the common challenges that prevent the reproducibility of many research projects, and examined how the Docker container technology can solve them. For example, many projects require specific dependencies to reproduce similar results as the original researchers, but it is difficult to simply provide a installation script due to different underlying OS and hardwares the project is running on. Docker solves this issue by providing a lightweight binary image in which all the softwares have already been installed. @cite investigated the advantages of using Docker to chain together the execution of multiple containers to run the pipeline application such as Genomic pipelines. @cite suggested that the flexibility and the reproducibility of the containers will drive its adoption in HPC environments. Many researches have also discussed the advantages of using the container technology in cloud computing. For example, David @cite proposed that the Platform-As-A-Service(PaaS) cloud can benefit from containers by its easy deployment, configuration, as well as convenient management of the applications in the cloud.
- In terms of container performance and security, Roberto @cite presented a performance evaluation of using containers in the field of Internet of Things. By running containers on top of a single board computer device such as Raspberry Pi 2, Roberto showed an almost negligible impact of the containerized layer compared to native execution. @cite revealed the possibility of using container technologies, such as Linux VServer, OpenVZ, and Linux Containers, to achieve a very low overhead HPC environment compared to the native setups. Thanh @cite analyzed the security of Docker from its internal mechanisms as well as its interacts with the security features of the Linux kernel. @cite enhanced the container security by using Intel SGX trusted execution support to prevent outside attacks.
- Traditional methods for logo detection rely on hand-crafted features and sliding window based localisation @cite @cite @cite @cite @cite . Recently, deep learning methods @cite @cite @cite @cite @cite have been proposed which use generic object detection models @cite @cite @cite @cite . However, these methods are not scalable to realistic large deployments due to the need for: (1) Accurately labelled training data per logo class; (2) Strong object-level bounding box annotations. One exception is @cite @cite where noisy web logo images are exploited without manual labelling of object instance boxes. This method exploits a huge quantity of data to mine sufficient correct logo images, and is restricted for non-popular and new brand logos which may lack web data. Moreover, all the above-mentioned methods assume the availability of real training images for ensuring model generalisation. This further reduces their scalability and usability in real-world scenarios when many logo classes have no training images from real scenes such as those newly introduced logos. In this work, we investigate this under-studied Open Logo Detection setting, where the majority of logo classes have no training data.
- There are previous attempts to exploit synthetic data for training deep CNN models. @cite used 3D CAD object models to generate 2D images by varying the projections and orientations to augment the training data in few-shot learning scenarios. This method is based on the R-CNN model @cite with the proposal generation component independent from fine-tuning the classifier, making the correlation between objects and background context suboptimal. The work of @cite used synthetic data rendered from 3D models against varying background to enhance the training images of a pose model. @cite similarly generated synthetic images by overlaying logo instances with appearance changes on random background images. Rather than randomly placing exemplar objects @cite @cite @cite , @cite performed object-scene compositing based on accurate scene segmentation, similar as @cite for text localisation. These existing works mostly aim to generate images with varying object appearance. In contrast, we consider the consistency between objects and the surrounding context for generating appearance coherent synthetic images. Conceptually, our method is complementary to the aforementioned approaches when applied concurrently.
- Traditional methods for person re-identification mainly consist of two aspects: feature learning and metric learning. Many feature learning methods have been proposed for person re-identification in single still images or video sequences. Matsukawa al @cite presented a descriptor on a hierarchical distribution of pixel features and used Gaussian distribution to describe a local image region. Liao al @cite utilized the horizontal occurrence of local features and maximized the occurrence for stable representation, called Local Maximal Occurrence (LOMO). Wang al @cite proposed a Discriminative Video Ranking model (DVR) to select the most discriminative video fragments, from which more reliable space-time features can be extracted. The method of Bag-of-Words (BoW) @cite aimed to learn a mapping function that converted frame-wise features to a global vector. The metric learning methods have also been widely invested and made some positive achievements, such as Relaxed Pairwise Learning (RPL) @cite , Large Margin Nearest-Neighbour (LMNN) @cite , Relevance Component Analysis (RCA) @cite , Locally Adaptive Decision Function (LADF) @cite , and RankSVM @cite .
- Deep neural network (DNN) has achieved significant successes in computer vision, and the DNN-based methods have been studied and applied in person re-identification task @cite @cite @cite @cite @cite @cite @cite @cite . The DNN-based models are trained with a pair of inputs for learning a direct mapping from image or video sequence to feature space. Mclaughlin al @cite combined CNN, RNN and Siamese network together, which is the first time applying DNN to the video-based re-identification. Attention mechanism @cite @cite has gained huge achievement in deep learning. Xu al @cite proposed a joint Spatial and Temporal Attention Pooling Network (ASTPN) to extract sequence-level features by selecting informative frames and notable regions of each frame. Zhou al @cite used Temporal Attention Model (TAM) to measure the importance of each frame in video sequence and applied Spatial Recurrent Model (SRM) to explore contextual information.
- Weight prediction means a mechanism in neural networks where weights are predicted by another structure rather than directly learned, which is mainly used in the fields of @cite @cite @cite , few zero-shot learning @cite @cite and transfer learning @cite . For object detection there are a few related works, for example, @cite proposes to predict mask weights from box weights. There are mainly two differences from ours: first, in our MetaAnchor the purpose of weight prediction is to generate anchor functions, while in @cite it is used for domain adaption (from object box to segmentation mask); second, in our work weights are generated almost from scratch'', while in @cite the source is the learned box weights.
- Spatio-temporal action localization consists in finding action instances in a video volume, i.e. both in space and time. Initial attempts @cite @cite scanned the video clip with a 3D sliding window detector on top of volumetric features. Next, @cite @cite adapted the idea of object proposals @cite @cite to video action proposals. Currently, the dominant strategy @cite @cite @cite @cite @cite is to obtain per-frame action detections and then to link them into continuous spatio-temporal tracks. The most recent methods @cite @cite @cite @cite @cite operate on multiple frames and leverage temporal information to determine actions. Examples include stacking features from several frames @cite and applying the I3D features @cite to spatio-temporal volumes @cite . These methods are fully supervised and necessitate a large quantity of annotations.
- Weakly supervised learning for action understanding is promising since it can enable a significant reduction of required annotation efforts. Prior work has explored the use of readily-available sources of information such as movie scripts @cite @cite to discover actions in clips using text analysis. Recent work has also explored more complex forms of weak supervision such as the ordering of actions @cite @cite @cite .
- Most related to us, are the works from @cite and @cite studying the trade-off between the annotation cost and final performance. @cite shows that one can simply use spatial points instead of bounding boxes and still obtain reasonable performance. @cite instead demonstrates that only a few frames with bounding box annotation are necessary for good performance. Here, we introduce a method that can leverage varying levels of supervision.
- Discriminative clustering @cite @cite is a learning method that consists in clustering the data so that the separation is easily recoverable by a classifier. In this paper, we employ the method proposed in @cite which is appealing due to its simple analytic form, its flexibility and its recent successes for weakly supervised methods in computer vision @cite @cite @cite . To use this method, we rely on a convex relaxation technique which transforms the initial NP-hard problem into a convex quadratic program under linear constraints. The Frank-Wolfe algorithm @cite has shown to be very effective for solving such problems. Recently, @cite proposed a block coordinate @cite version of Frank-Wolfe for discriminative clustering. We leverage this algorithm to scale our method to hundreds of videos.
- With the formalization of OSR developed in @cite , the openness of a particular problem or data universe is defined by considering the number of training, target, and testing classes: Larger openness corresponds to more open problems, while the problem is completely closed when @math Furthermore, the OSR problem can be defined as follows: given a set of training data @math , an open space risk @math , and an empirical risk @math , the goal of OSR is to find a measurable recognition function @math defined by minimizing the following open set risk where @math is a regularization constant. Thanks to the guidance of this definition, a large number of OSR algorithms have been proposed. Next, we will briefly review the relevant representative approaches.
- In SLSS users are able to stream their own live shows in real time as broadcasters, and to join the live shows of other users as viewers audience. The audience is able to interact with the streamers through a chat and reward them with virtual rewards, e.g., points, gifts, badges (some of which are purchasable), or money. Also, various SLSS give broadcasters the opportunity to monetize part of the virtual gifts they receive from the audience during their brodcasts. Users of SLSS employ their own mobile devices (e.g. smartphones, tablets) or their PCs and webcams for broadcasting. In contrast to other social media, SLSS are mostly synchronous @cite @cite , but they can also support asynchronous interactions between users, like direct messages and comments on broadcast video replays.
- Since SLSS are quite new, the literature in the field is rather limited. Some of these studies investigate the performance of such services, e.g., Meerkat and Periscope @cite @cite , Periscope @cite and Twitch @cite . Human factors and user experience were studied in @cite . Having access to a large dataset of Inke, a Chinese SLSS, @cite identified several patterns in the users, e.g., fast interest shifts, user dedication to broadcasters as well as the locality bonds between users.
- @cite analyzed traffic patterns and user characteristics of YouNow. @cite crawled Inke and identifed that the main reasons that users are hooked in these services are the follower-followee model, the awards incentivisation, and the multi-dimensional interaction between broadcasters and viewers. Similar results, but with real users, were also reported by @cite for the case of Facebook Live, Periscope, and Snapchat.
- Legal and ethical questions about SLSS were raised by @cite . Recently, @cite performed an empirical study on law infringements in several SLSS. While the focus was not on adult content, the researchers found that around 17.9
- Contrary to other popular SLSS like Periscope @cite , all the broadcasts in LM and LL are public. All active broadcasts are visible on a global public list. In both platforms, the concept of re-sharing re-posting broadcasted content across different users is not present. Nevertheless, users are able to get shareable links to live shows that can be used for promoting broadcasters on other social media.
- This work focus on question answering for unstructured textual content in English. Earlier systems of this type comprise various modules such as, for example, query reformulation [ ,][] Brill.2002 , question classification @cite , passage retrieval [ ,][] Harabagiu.2000 , or answer extraction @cite . However, the aforementioned modules have been reduced to two consecutive steps with the advent of neural QA.
- Neural QA systems, such as DrQA @cite or @math @cite , are usually designed as pipelines of two consecutive stages, namely a module for information retrieval and a module for machine comprehension. The overall performance depends on how many top- @math passages are fed into the module for machine comprehension, which then essentially generates multiple candidate answers out of which the one with the highest answer probability score is chosen. However, this gives rise to a noise-information trade-off @cite . That is, selecting a large @math generates many candidate answers, but increases the probability of selecting the wrong final answer. Similarly, retrieving a small number of top- @math passages reduces the chance that the candidate answers contain the correct answer at all.
- Most of the previous work for feature learning and motif identification has been in the context of supervised learning. @cite proposed a method for learning distributed feature representation of splice junctions using k-mers. Spline transformation was introduced by @cite to improve upon traditional neural networks to learn better representations and improve prediction accuracy. @cite used a memory matching network to dynamically learn a memory bank of motifs using sequence classification task. @cite proposed a Restricted Boltzmann Machine based model with a new training method called boosted Contrastive Divergence to predict non-canonical splice sites and learn non-canonical feature vectors that couldn't be identified by traditional methods. Some works have also used Convolutional Neural Networks(CNN) in order to learn representations implicitly @cite @cite @cite @cite @cite .
- Generative models @cite @cite @cite @cite @cite @cite @cite have shown promising results for both ZSL and GZSL setups. Another advantage of the generative approach is that by using synthesized samples, we can convert the ZSL problem to the conventional supervised learning problem that can handle the biases towards the seen classes. The @cite used a simple generative model based on the exponential family framework while @cite synthesize the classifier. While recent generative approaches for the ZSL are deep generative models based on the VAE @cite and GAN @cite . The approach @cite @cite @cite is based on the VAE architecture while @cite @cite @cite used the adversarial sample generation based on the class conditioned attribute.
- In ZSL, the train and test classes are disjoint and hence there is a high probability of domain shift for the unseen classes. This is another challenge in the ZSL setup and needs to be handled. Previously, very few works have handled the domain shift problem and worked on both the transductive as well as inductive settings. @cite adapted to the new domain by simple Gaussian mixture model updates. @cite used the unbiased embedding in the transductive setting. @cite @cite proposed unsupervised domain adaption for the ZSL. @cite used the structural SVM formulation for domain adaption.
- The first works dedicated to the study of RWDRE focused on the case were the underlying medium exhibit fast mixing conditions. A broad range of such conditions have been considered such as: time independence @cite @cite , strong mixing conditions @cite @cite @cite , exponential mixing rate @cite @cite @cite @cite , @cite and fast decay of covariances @cite . In all the above circumstances, one expects the random walk to exhibit Gaussian fluctuations and to satisfy a functional central limit theorem.
- In @cite , a (continuous-time) random walk on the SSEP was studied by means of simulations. There the authors investigated the limiting behavior as a function of three parameters: the density @math of the SSEP, the rate @math of the SSEP and the local drift @math of the random walk on occupied sites. They restrict themselves to the case where the local drift on vacant sites satisfy @math .
- We have divided earlier works into different categories as shown in Table . First we briefly discuss works considering backhaul and access networks separately, then we summarize the works that consider both access and backhaul jointly. & & Limited to theoretical modeling & No & No & QoS-aware @cite & & Increases average cell load & No & Both & QoS and channel-aware @cite , @cite & & No system-level procedure & No & Both, Only QoS & Delay-aware @cite & Efficient load balancing & Does not guarantee QoS & Yes & QoS & Channel-aware @cite & & & & Yes & No & & & & No & No 4 * Backhaul Network & Congestion-aware @cite & & & No & Yes & QoS-aware @cite & QoS in LTE backhaul & No access network & Yes & Yes & & & & No & NonQoS & & & & No & Only nonQoS 3 * & Mobility-driven @cite & Overload detection & & No & Only QoS & & Handover delay reduction & Limited study & Yes & Not stated
- As shown in Table , for backhaul networks @cite @cite propose solutions that do not consider SDN. @cite two load detection approaches are proposed in backhaul constrained LTE networks, first runs locally, while the second runs globally. The designed approach enables cells to dynamically change their network coverage for better LB. Results show that the local scheme can balance an individual BS scheduler load, while the global scheme enables different types of cells to adjust their coverage based on the available resources resulting in better load distribution across different cells. This work only assume elastic traffic and it does not evaluate the benefits achieved when both local and global LB approaches work together. A backhaul network congestion detection and control strategy is described in @cite , which periodically injects some packets into the network to avoid non-guaranteed bit rate (NGBR) packets to be randomly dropped upon backhaul congestion. The main drawback of this approach is the large overhead due to the probe packets sent between the gateway and the LTE eNodeB for congestion detection.
- @cite @cite , SDN based backhaul network management is considered. A recent effort of SDN-based backhaul is made in @cite , where the authors proposed an OpenFlow based mechanism to prioritize traffic and control rate in LTE network. The essence of their work enables backhaul networks to guarantee the rate for GBR traffic during congestion due to the bottleneck link. Similarly, the work in @cite proposes an approach to tackle backhaul network congestion by utilizing OpenFlow. The main purpose of using a programmable backhaul is to allow network operators to share each others network infrastructure to achieve flexibility upon congestion.
- An enhanced mobility management approach in traditional LTE network is proposed in @cite , where the handover offset is tuned for the overloaded cells. A major setback of using it is that it presumes that the last mile link connecting the BS to the core network is the bottleneck. However, this is not always true, since the backhaul network usually has a ring topology @cite where the bottleneck can exist anywhere in the network. @cite author outlines a framework that enables efficient traffic forwarding and QoS control within a wireless network by using SDN concepts.
- Among the initial efforts for programmable cellular networks, @cite presents a SDCN design considering both the access and backhaul networks, simultaneously. Another recent work in @cite studies the impact of joint access-backhaul on mobility management solutions in 5G networks. Specifically for various QoS flows, different schemes can be used during cell selection based on the number of hops and the available links capacities. @cite presents a three tier SDN-based LTE network design for efficient mobility management. The results shows a significant delay reduction due to reduced signaling during handover event, however it lacks implementation on a system-level LTE simulator that enables to explore further benefits from the resulting integration.
- = -1 Many of techniques to the prime compilation are based on branch and bound backtrack search procedures @cite @cite @cite @cite . They take full advantage of powerful SAT solvers, while these methods cannot generate the primes for non-clausal formulae. In addition, a number of approaches based on binary decision diagrams (BDD) @cite or zero-suppressed BDD (ZBDD) @cite have been proposed. These methods can encode primes in a compact space thanks to BDD. Given the complexity of the problem, however, these methods may still suffer from time or memory limitations in practice. Almost simultaneously, a 0-1 integer linear programming (ILP) formulation @cite @cite @cite @cite was proposed to compute primes of CNF formulae. Although these approaches can naturally encode the minimal constraints utilizing ILP, their efficiency is questionable.
- Kogan @cite compared different methods for winter wheat yield forecasting: using remote sensing observations, meteorological data and biophysical models. The two former methods consisted respectively of linear regression models using NDVI data at 250m resolution and data from 180 weather stations for a 13-year period as predictors. The third method is based on the application of a biophysical process-based crop model, an algorithm that models phenology, canopy development, biomass accumulation, water stress and many other plant components. In this case, the World Food Studies (WOFOST) model @cite was used. All three approaches were used to perform forecast 2--3 months before harvest and the biophysical model showed the best results in terms of root mean squared error (RMSE). The NDVI-based and the meteorological data-based methods showed similar performance when minimum input data requirements were met.
- In studying dryland maize in South Africa, Estes @cite developed three empirical models that were compared against the CERES-maize model of the DSSAT platform @cite . Two of the empirical models were based on maximum entropy (MAXENT) @cite : one trained on all national crop distributions points and the other trained with the top producing localities. The third method used a generalized additive model (GAM) trained with yield data derived from NDVI @. GAM and CERES results showed linear correlation to measured yield (R @math = 0.75 and 0.37, respectively) as did the MAXENT model trained with high-productivity points (R @math = 0.62).
- Gonzalez-Sanchez @cite compared the predictive accuracy of several techniques (Multiple linear regression, M5-Prime regression trees, perceptron multilayer neural networks, support vector regression and k-nearest-neighbors KNN) for crop yield prediction in ten crop datasets from western Mexico. Predictors came from typical atmospheric data (solar radiation, rainfall, temperature, etc) and some genetic and farm management information like season-duration cultivar and planting area. For these specific conditions, the regression trees and KNN showed the lowest error metrics.
- Kumar @cite performed rice yield forecasting by adaptive neuro fuzzy inference system (ANFIS) technique. For that, they used 27 years time series data of yield and weather. ANFIS is an effort to integrate the benefits of neural networks and fuzzy logic in a single framework by using linguistic information from the fuzzy logic and learning capabilities of an artificial neural network (ANN). Quantitative performance assessment for rice and wheat yield observations in India showed good applicability of the technique in yield prediction.
- . IRL is a method that infers a reward function given a set of expert demonstrations @cite @cite . One of the key assumptions of IRL is that the observed behavior is optimal (maximizes the sum of rewards). Maximum entropy inverse reinforcement learning @cite employs the principle of maximum entropy to learn a reward function that maximizes the posterior probability of expert trajectories. Though @cite relaxes the optimality constraints, it cannot handle significantly suboptimal demonstrations. @cite also does not consider the redundancy of demonstrations. In our case, since we have both agent's failure experience as defined later and expert's demonstrations, we can leverage the failure experience to improve the current reward. By using human feedback interactively in the training, our method aims to ultimately improve the reward inference process. By interacting with the human only when needed, we are also able to reduce the amount of human involvement ( redundant demonstration data).
- . Hierarchical reinforcement learning @cite was proved to be effective in learning to perform challenging tasks with sparse feedback by learning to optimize different levels of temporal reward functions. Hierarchical IRL @cite was recently proposed to learn the reward function for complex tasks with delayed feedback. The work of @cite shows that by segmenting complex tasks into a sequence of subtasks with shorter horizons, it is possible to obtain optimal policy more efficiently. However, since @cite does not get expert feedback during learning, and does not explicitly leverages partial demonstrations, it may still involve redundant demonstrations.
- . Traditional IRL assumes the demonstrations by experts are optimal in the sense that it optimizes the sum of reward @cite @cite @cite . Recently, learning from failure experience has been proven to be beneficial with properly defined objective functions @cite @cite . Inspired by @cite , we complement the human-in-the-loop training process with learning from failure experience experienced by agents, as we find it to improve reward function inference.
- Complex networks are used to shape real-world systems, e.g. networks of protein interaction, street meshes, and subway lines. These networks, as mathematical models, stand out due to their algebraic properties and computing potential, with analytical applicability to support cognitive processes of decision-making @cite . Through metrics and methods based on topology and or geometry, it is possible to identify characteristics of interest that are not obvious for human inspections based on reading; this is because the networks may be wide (high number of vertices), intricate (high number of edges), or may hold non-trivial patterns and attributes whose observation depends on the application of algorithms.
- Aiming to solve questions related to the urban scenario, a vast number of studies have been conducted to explain cities considering their intense flow of vehicles @cite and collective behaviors @cite , while others analyzed the accidents density in street networks @cite and the discrepancies between cities driven by their urban indicators @cite . Furthermore, some authors investigated metrical and analytical methods applied to cities @cite @cite , others approached the assistance to the urban planning and design @cite @cite @cite , and there are those who advanced with facility-location analysis and planning in street meshes @cite . However, although cluster analysis has been less focused @cite @cite , it is still an important toolset @cite .
- Structured Output Prediction. Stochastic feed-forward neural networks (SFNN) @cite model multi-modal conditional distributions through binary stochastic hidden variables. During training multiple samples are drawn and weighted according to importance-weights. However, due to the latent variables being binary SFNNs are hard to train on large datasets. There has been several efforts to make training more efficient for binary latent variables @cite @cite @cite @cite . However, not all tasks can be efficiently modelled with binary hidden variables. In @cite , Gaussian hidden variables are considered where the re-parameterization trick can be used for learning on large datasets using stochastic optimization. Inspired by this technique we model Gaussian hidden variables for structured sequence prediction tasks.
- In @cite a multi-sample objective for Conditional Generative Models is also proposed in order to better capture multi-modal distributions. However, the key difference to our proposed objective is that during training samples are drawn from the prior. Sampling directly from the prior leads to high variance model updates. In contrast, we perform importance sampling through a jointly learned proposal distribution in order to deal with this problem.
- Multiple Choice Learning. In @cite a Multiple Choice Learning framework is presented where multiple models are learned to produce diverse predictions. During training, training examples are iteratively reassigned to the minimum loss model. The models are trained to convergence using these samples. Similarly in @cite multiple diverse models are iteratively learned to solve submodular optimization tasks. As these frameworks require expensive retraining, in @cite a Stochastic Multiple Choice Learning framework is proposed. Here instead of iterative reassignment, for each example in a mini-batch, the error is back-propagated to only the model with the minimum loss (best) model. In contrast to our approach, these approaches require the learning of multiple models while our approach efficiently learns only one model without compromising on diversity of predictions.
- Recurrent Neural Networks. Recurrent Neural Networks (RNNs) are state of the art methods for variety of sequence learning tasks @cite @cite . In this work, we focus on sequence to sequence regression tasks, in particular, trajectory prediction and image sequence prediction. RNNs have been used for pedestrian trajectory prediction. In @cite , trajectories of multiple people in a scene are jointly modelled in a social context. However, even though the distribution of pedestrian trajectories are highly multimodal (with diverse futures), only one mean estimate is modelled. @cite jointly models multiple future pedestrian trajectories using a recurrent CVAE sampling module. Samples generated are refined and ranked using image and social context features. While our trajectory prediction model is similar to the sampling module of @cite , we focus on improving the sampling module by our novel multi-sample objective function. Convolutional RNNs @cite have been used for image sequence prediction. Examples include, robotic arm movement prediction @cite and precipitation now-casting @cite @cite . In this work, we extend the model of @cite for structured sequence prediction by conditioning predictions on Gaussian latent variables. Furthermore, we show that optimization using our novel multi-sample objective leads to improved results over the standard CVAE objective.
- The closest prior work in intent inference and action understanding comes from inverse planning @cite and inverse reinforcement learning @cite , which use observations of a user's actions to estimate the user's goal or reward function. We take a fundamentally different approach to intent inference: using action observations to estimate the user's beliefs about the world dynamics.
- @cite propose an internal model estimation (IME) framework for brain-machine interface (BMI) control that learns an internal dynamics model from control demonstrations on tasks with linear-Gaussian dynamics and quadratic reward functions. Our work is (1) more general in that it places no restrictions on the functional form of the dynamics or the reward function, and (2) does not assume sensory feedback delay, which is the fundamental premise of using IME for BMI control.
- Modeling human error has a rich history in the behavioral sciences. Procrastination and other time-inconsistent human behaviors have been characterized as rational with respect to a cost model that discounts the cost of future action relative to that of immediate action @cite @cite . Systematic errors in human predictions about the future have been partially explained by cognitive biases like the availability heuristic and regression to the mean @cite . Imperfect intuitive physics judgments have been characterized as approximate probabilistic inferences made by a resource-bounded observer @cite . We take an orthogonal approach in which we assume that suboptimal behavior is primarily caused by incorrect beliefs of the dynamics, rather than uncertainty or biases in planning and judgment.
- Humans are resource-bounded agents that must take into account the computational cost of their planning algorithm when selecting actions @cite . One way to trade-off the ability to find high-value actions for lower computational cost is to plan using a simplified, low-dimensional model of the dynamics @cite @cite . Evidence from the cognitive science literature suggests humans find it difficult to predict the motion of objects when multiple information dimensions are involved @cite . Thus, we arrive at an alternative explanation for why humans may behave near-optimally with respect to a dynamics model that differs from the real dynamics: even if users have perfect knowledge of the real dynamics, they may not have the computational resources to plan under the real dynamics, and instead choose to plan using a simplified model.
- To compress a deep net, a natural direction is to approximate each of its weight matrices, @math , by a low-rank approximation of the matrix using SVD. Based on this idea, @cite compressed the fully connected layers in neural nets. For convolution layers, the kernels can be viewed as 3D tensors. Thus, @cite @cite applied higher-order tensor decomposition to compress CNN. In the same vein, @cite developed another structural approximation. @cite proposed an algorithm to select rank for each layer. More recently, @cite reconstructed the weight matrices by using sparse plus low-rank approximation.
- Algorithms have been proposed to remove unimportant weights in deep neural nets. In order to do this, one needs to define the importance of each weight. For example, @cite showed that the importance can be estimated by using the Hessian of loss function. @cite considered adding @math or @math regularization and applied iterative thresholding approaches to achieve very good compression rates. Later on, @cite demonstrated that state-of-the-art CNNs can be compressed by combining pruning, weight sharing and quantization.
- Although model compression has been studied extensively for CNN models, less works have focused on the compression for recurrent neural nets (RNNs), another widely-used category of deep models in NLP applications. Since RNN involves a collection of fully connected layers, many of the aforementioned approaches can be naturally applied. For example, @cite applied their quantization and retraining procedure to compress a LSTM (a popular type of RNN) language model on Penn Tree Bank (PTB) dataset. @cite applied a matrix tensor factorization approach to compress the transition matrix of LSTM and GRU, and tested their algorithm on image and music classification problems (which does not need word embedding matrices). @cite @cite proposed pruning algorithms for LSTM models compression.
- Among the previous work, we found only @cite @cite tried to compress the word embedding matrix in NLP applications. @cite showed that the quantization-plus-retraining approach can only achieve less than @math times compression rate on PTB data with no performance loss. @cite showed that for word-level LSTM models, the pruning approach can only achieve @math 26 before submitting this work, we found another very recent paper @cite applying compositional coding to compress the input embedding matrix of LSTM. However, as they explicitly mentioned in OpenReview https: openreview.net forum?id=BJRZzFlRb , their algorithm is not able to compress the softmax (output) layer matrix. As a result, the overall compressed model from this approach is still large. One main issue of the approach is that multiple words share the same coding, which makes these words indistinguishable in the output layer during inference.
- . In @cite , analyzed the whitening and decorrelation by ZCA operation. ZCA opeartion is used to project a random vector into a irrelevant sub-space which is also named whitening. In image processing field, ZCA is a very useful tool to process the features which can obtain useful features to improve algorithm performance. We will introduce ZCA operation briefly.
- @cite proposed a universal style transfer algorithm using ZCA operation to transfer the style of artistic image into content image. The encoder network is used to obtain the style features( @math ) and content features( @math ). Then authors use ZCA operation to project @math and @math into the same space. The final transferred features will be obtained by a coloring transform method which is a reverse operation to the ZCA operation. Finally, the styled image is obtained by transferred features and a decoder network.
- Seizure nonseizure classification distinguishes seizure segments from nonseizure segments, which can be used to detect whether a data segment contains a seizure or not. For this task, extensive studies have been performed. Because seizure detection, which is often of a real-time flavor, is often treated as the seizure nonseizure classification problem, many machine learning methods have been developed @cite @cite @cite @cite @cite @cite @cite @cite @cite . Recently, deep learning techniques have been applied to the seizure detection problem @cite @cite @cite @cite @cite @cite @cite .
- proposed a method to automatically detect the normal, pre-ictal, and ictal conditions from EEG signals @cite . The method firstly extracted four entropy features, including approximate entropy, sample entropy and two phase entropies, then fed the four features to classifier to do classification.
- designed a seizure detection algorithm based on lacunarity and Bayesian linear discriminant analysis (BLDA) @cite . The critical step in the algorithm was feature extraction. Firstly, EEGs were performed wavelet decomposition with five scales, and the wavelet coefficients at scales 3, 4, and 5 were selected. At the three scales, features including lacunarity and fluctuation index were extracted. Then they were passed on to the BLDA for training and classification. Over intracranial EEG data from the Epilepsy Center of the University Hospital of Freiburg, the obtained average sensitivity was 96.25 Fan and Chou leveraged a complex network model to represent EEG signals, and integrated it with spectral graph theory to extract spectral graph theoretic features for detecting seizure onsets in real-time @cite . The method was evaluated over the CHB-MIT data set. The resulting patient-specific average sensitivity was 98 The methods developed in @cite @cite @cite @cite @cite @cite are mostly based on signal processing methods and traditional machine learning methods. They often need crafted features, which may not be optimal.
- The approaches developed in @cite @cite @cite @cite @cite extracted multichannel information to do seizure detection. However, they did not differentiate channels.
- presented a recurrent convolutional neural network to capture spectral, spatial and temporal features of seizures @cite . EEG signals were firstly transformed into images. Created images were fed to CNN. Output vectors of the CNN were organized to be sequences in the chronological order. The sequences were passed on to the bidirectional RNN to make classification. Both patient-specific experiments and cross-patient experiments were performed. In the cross-patient testing, the obtained average sensitivity was 85 proposed a deep recurrent architecture by combining cellular neural network with bidirectional RNN @cite . The bidirectional RNN was deployed into each cell of the cellular neural network to extract temporal features in the forward and the backward directions. Each cell interacts with its neighboring cells to extract local spatial-temporal features. The method was evaluated in patient-specific experiments over five patients from CHB-MIT. The obtained sensitivities are 100 explored two kinds of neural networks over TUH EEG Corpus @cite . Their experimental results showed that convolutional LSTM network outperformed convolutional GRU network. Different initialization and regularization methods were also tested.
- Our setup is closely related to that of , who extend work on pixel-level autoregressive image generation @cite @cite to videos. However, whereas they model the temporal and spatial dimensions separately with dilated convolutions and convolutional LSTMs, respectively, our model is conceptually simpler in that we do not make any distinction between temporal and spatial dimensions and instead rely almost entirely on multi-head self-attention @cite within the 3D video volume. For comparability, we provide results on Moving MNIST and an older Robotic Pushing dataset for which we achieve an almost 50 To keep memory requirements feasible, we use block-local self-attention, generalizing the image generation approaches of and to 3D volumes. The concurrent work of instead use sparse attention, by linearizing images to a sequence of pixels. However, this approach would fail to capture local 3D neighborhoods directly and they only apply their model to images. To further reduce memory requirements, we exploit ideas of sub-scaling which were recently introduced in . Another approach is (hierarchical) multi-scale generation, which has recently been explored for both image- @cite @cite and video @cite generation.
- Earlier work on video generation mostly focused on deterministic approaches @cite @cite @cite @cite @cite , which fail to capture the high degree of stochasticity inherent in video. In response, a popular research direction has been that of generative latent-variable video models. In contrast to pixel-level autoregressive models, these posit an underlying latent process in tandem with the observed pixel values. Work in this category include variants of variational autoencoders @cite @cite . To tackle the issues inherent in these models, most notably the tendency to generate blurry outputs due to restricted modeling power, inadequate prior distributions, or optimization of a lower bound in place of the true likelihood, these have been combined with adversarial objectives @cite @cite @cite , hierarchical latent-variables @cite , or flow-based modeling @cite . These approaches have the benefit that they admit fast generation, but are restricted in that they tend to only focus on a subset of the modes in the empirical distribution in the adversarial case, or that they struggle with limited modeling power even when using a large number of layers in the case of flow-based models.
- The conceptual simplicity of our model is in line with recent approaches to video classification that models videos by means of 3D convolutions @cite @cite or spatiotemporal self-attention @cite . In contrast, much earlier work on video generation has encoded specific intuitions about videos, such as explicit modeling of motion @cite @cite or generation of optical flow @cite .
- Current approaches to quantifying uncertainty have mostly been Bayesian where a prior distribution is specified over the parameters of the Neural Network and then using the training data, the computed posterior distribution over the parameters is used to calculate the uncertainty @cite . Since this form of Bayesian inference is computationally intractable, approaches have ranged from Laplace Approximation @cite , Markov Chain Monte Carlo methods @cite to Variational Bayesian Inference methods @cite @cite @cite . These methods however suffer from issues due to bounds of computational power and over-reliance on the correctness of the prior probability distribution over the parameters. Having priors of convenience can in fact lead to unreasonable uncertainty estimates @cite . In order to overcome these challenges and produce a more robust uncertainty estimate, @cite proposed using an ensemble of Neural Networks trained under a defined scoring rule. This approach when compared to Bayesian approaches is much simpler, has parallelization advantages and achieves state-of-the-art or better performance. State-of-the-art performance before this was achieved by MC-dropout which can also in essence be considered an ensemble approach where the predictions are averaged over an ensemble of Neural Networks with parameter sharing @cite .
- This ensemble approach for uncertainty estimation in Neural Networks motivated its use for uncertainty estimation for exploration in the case of Deep Reinforcement Learning in the form of Q-Ensembles @cite . Specifically, an ensemble voting algorithm is proposed where the agent takes action based on a majority vote over the Q-ensemble. The exploration strategy described uses the estimate of the confidence interval to then optimistically explore in the direction of the largest confidence interval (highest uncertainty). This approach was demonstrated to improve significantly over an Atari benchmark. The Q-Ensemble approach is an example of a model-free reinforcement learning approach where we do not need to infer the environment in the learning process. Model-free approaches are however generally high in sample complexity. Sampling from a learned model of the environment can help us mitigate this problem.
- Region-based networks take images as a set of regions and extract features of them to predict their labels. Mostajabi al @cite proposed zoom-out features which combines features of local, proximal, distant neighboring superpixels and the entire scene to classify each superpixel.
- Similar to DOC @cite , our method detects object boundaries and estimates the occlusion relationships from a single image, which is referred as the object occlusion boundary detection. Notably, we adapt a single stream network architecture simultaneously predicting both object boundary and occlusion orientation in a single step by sharing convolutional features.
- In recent years, significant progress has been made towards vision based classification systems, both using handcrafted features @cite , @cite , @cite and end-to-end methods based on convolutional neural networks (CNN) @cite , @cite , @cite . However, none of these methods estimate the stem locations or other information which can be directly used for targeted intervention. With our work, we aim to bridge this gap by developing a system which integrates both the task of plant classification and stem detection in an end-to-end manner with the goal of targeted treatment in mind.
- Other approaches have been developed to classify individual plants and identify their stem locations. Most of these approaches are developed based on manually designed heuristics with specific use cases in mind. Kiani and Jafari @cite use hand-crafted shape features selected on the basis of a discriminant analysis to differentiate corn plants from weeds. They identify the stem position of the plant as the centroid of the detected vegetation. This leads to sub-optimal results particularly when the plant shapes are not symmetric or multiple plants are overlapping. Midtiby al @cite present an approach for sugar beet by detecting individual leaves and use the contours of the leaves for finding the stem locations. This approach fails to locate the stems in the presence of occluded leaves or overlapping plants.
- Moving towards a machine learning based approach, Haug al @cite propose a system to detect plant stems using keypoint-based random forests. They employ a sliding window based classifier to predict stem regions by using several hand-crafted geometric and statistical features. Their evaluation shows that the approach often misses several stems for overlapping plants or generates false positives for leaf areas which locally appear to be stem regions. Kraemer al @cite aim at addressing this issue by increasing the field of view of the classifier using a fully convolutional networks (FCN) @cite . The goal of their work is to identify crop stems over a temporal period allowing them to use the stem locations as landmarks for localization.
- Early phrase-based approaches to multilingual MT focused on multi-source translation. used a simple product or max rule to select at the sentence-level the single best hypothesis with the highest translation score from multiple decoders. There is no sharing of parameters. Consensus network decoding @cite can also be used to combine the word-level output of translations from multiple source languages. Such system combination techniques allow sharing of the words in the candidate translations, but still require training individual models for each language pair of interest.
- Sequence to sequence models with attention are no exception. Each set of parameters provides different levels of generalization @cite , which is evidenced in the synergistic task of training multilingual translation models. For example, jointly train decoders while the rest of the parameters are task-specific; jointly train the encoders while the rest of the parameters are task-specific, and train both encoders and decoders jointly with language-specific tokens to guide learning as in @cite . These latter approaches are the ones that we build on. We augment our decoder with a task-specific attention mechanism intended to better capture word order and language-specific nuances while continuing to share the rest of the model parameters (including token embeddings).
- More formally, a document @math can be represented by a vector in @math , where each dimension represents a different term. A term can be a single word, constituting the conventional bag-of-words, or combinations of @math words, constituting the bag-of-N-grams. If a term occurs in the document, its position in the vector will have a non-zero value, also known as term weight. Two documents in the VSM can be compared to each other by taking the cosine distance between them @cite .
- Our Loss Max-Pooling (LMP) loss is based on @cite , where we fix their @math -norm parameter to @math for simplicity. Our experiments further support the effectiveness of LMP in the intrinsically class imbalanced fingernail segmentation task.
- Our cascaded architecture is related to ICNet @cite in that our neural network model combines shallow high-resolution and deep low-resolution branches. Unlike ICNet, our model is designed to run on mobile devices, and therefore we completely redesigned the encoder and decoder based on this requirement.
- For a linear code @math , the minimum weight is equivalent to the minimum distance, @math . This is of interest to both coding theorists and cryptographers, and the algorithms used are of a somewhat affine nature to the one here proposed, so we briefly discuss the topic - mostly following the summary in @cite and @cite . In one specific instance @cite , the Stern algorithm for the minimum distance @cite was extended to the estimation of the whole weight distribution of LDPC codes.
- Lee and Brickell observed @cite that the best cryptanalytic attack consisted in choosing a set @math of @math random elements of @math , selecting the corresponding columns of @math as a @math matrix @math , and computing @math . If @math , this procedure returns the correct @math . They also observe that not only is it costly to invert a random @math matrix, there is also a cost associated with checking that the result is correct: if @math , then @math . One would therefore also have to compute @math to be able to claim that @math .
- The Lee-Brickell algorithm, as well as others, can be interpreted in the context of information set decoding, an equivalent definition of which can be given in terms of the generator matrix or the parity check matrix - see for instance @cite and @cite . with @math the @math th column of @math .
- The Lee-Brickell algorithm can be written as follows (see @cite ): Compute the permuted syndrome @math . Fix a weight @math . If then we can choose another set @math of @math columns from @math to obtain a set @math of columns of @math that sum to @math . Otherwise, restart with a different @math .
- It has been verified that physical layer security (PLS) technology can prevent illegitimate receivers from eavesdropping due to the time-varying nature of the wireless medium @cite - @cite . Numerical studies of PLS over FSO satellite ground systems were performed by Endo @cite . They showed that secrecy communications were possible and that there can be a complementary technologies to balance security and usability issues. But in their study, Endo only considered some idealistic conditions and assumed that the channels were fading-free. Lopez-Martinez @cite studied PLS based on Wyner's FSO model and used the probability of strict secrecy capacity to evaluate the secrecy performance. But they considered only two special cases: when the eavesdropper is either near the source or the destination. Sun and Djordievic @cite studied a secure orbital angular momentum multiplexing FSO system and numerically simulated its secrecy capacity. Their results showed that secrecy performance depends on the location of eavesdroppers and that orbital angular momentum multiplexing technology could improve the secrecy in weak and medium turbulence regimes.
- The work that is most recent and most closely related to ours is that of Scheitle al @cite , who compared Alexa's, Majestic's and Umbrella's lists on their structure and stability over time, discussed their usage in (Internet measurement) research through a survey of recent studies, calculated the potential impact on their results, and drafted guidelines for using the rankings. We focus on the implications of these lists for security research, expanding the analysis to include representativeness, responsiveness and benignness. Moreover, we are the first to empirically demonstrate the possibility of malicious large-scale manipulation, and propose a concrete solution to these shortcomings by providing researchers with improved and publicly available rankings.
- In 2006, Lo and Sedhain @cite studied the reliability of website rankings in terms of agreement, from the standpoint of advertisers and consumers looking for the most relevant sites. They discussed three ranking methods (traffic data, incoming links and opinion polls) and analyzed the top 100 websites for six providers, all of which are still online but, except for Alexa, have since stopped updating their rankings.
- In his analysis of DNS traffic from a Tor exit node, Sonntag @cite finds that popularity according to Alexa does not imply regular traffic over Tor, listing several domains with a good Alexa rank but that are barely seen in the DNS traffic. These conclusions confirm that different sources show a different view of popularity, and that the Alexa list may not be the most appropriate for all types of research (e.g. into Tor).
- The conventional approach to the variable selection was the information criterion such as AIC or the sequential test called forward and backward step-wise selection. For high-dimensional models, however, the information criterion does not work because of the computational issue, i.e., the combinatorial complexity appears in the choice of variables. Also, the statistical test needs to repeat the computation of sample statistics many times. For high-dimensional linear models, @cite proposed the so-called Lasso estimator, in which the @math -norm of the coefficients was incorporated into the squared loss.
- Also, the non-negative garrote (NNG) was proposed by @cite as a modification of the standard least square (LS) estimator for linear models. In the NNG, each coefficient of the LS estimator is shrunk towards zero, and its intensity is controlled by the @math -regularization. The variable selection consistency of Lasso was proved under the so-called irrepresentable condition @cite @cite . On the other hand, @cite and @cite proved that the NNG has the variable selection consistency without the irrepresentable condition. For some kernel-based estimators, @cite and @cite employed the NNG as the adaptive scaling for the variable selection. Also, @cite proved the variable selection consistency of the scaled kernel-ridge estimator under a variant of the irrepresentable condition.
- Since the seminal work on SVRG @cite , stochastic average gradient @cite and stochastic dual coordinate ascent @cite , variance reduction has been successfully applied to solve problem , in the special case of @math for all @math , in a variety of settings @cite @cite @cite @cite @cite @cite . Allen-Zhu @cite provided an improved IFO complexity of @math for convex objectives, and Reddi al @cite and Allen-Zhu and Hazan @cite obtained an IFO complexity of @math for nonconvex objectives. However, all of these methods are computationally inefficient as putative solutions to problem in general because they need to compute @math at each iteration.
- @cite used the moving human body in the video as a spatial-temporal model to present Motion Energy Image (MEI) and Motion History Image (MHI), which could reflect the temporal changes. @cite @cite proposed Dense Trajectories (DT) and improved Dense Trajectories (iDT) to extract apparent features and motion information from the detected spatial-temporal salient regions. The commonly used descriptors are pixel intensity, pixel distribution, optical flow characteristics, gradient direction, gradient strength. @cite extracted the Histogram of Oriented Gradient (HOG) and the Histogram of Optical Flow (HOF) features, and combined them into one feature vector HOG HOF. The HOG HOF feature describes the appearance and movement information in salient points and regions.
- @cite extended 2DCNN to 3DCNN. Multiple levels of operations such as 3D convolution and pooling are performed on each channel, then the information of each channel is merged to obtain the final feature description of the video, which includes the spatial and temporal characteristics of the video. @cite proposed the Slow Fusion Model to fuse each frame and use the correlation of neighbor frames in the video to improve the expression ability. @cite proposed Two-Stream ConvNets, which simultaneously constructed both spatial and temporal streams. Spatial stream processes still frame to get spatial and shape information. Time stream processes multiple optical streams stacked in succession to obtain time and motion information. @cite proposed the LRCN algorithm, which sent the intra-frame spatial features extracted by AlexNet into LSTM to model temporal relationships. @cite combined sparse time sampling strategy and video level supervision to use the entire action video to achieve efficient and effective learning.
- There is still no universal theory in explaining why a deep model works better than a shallow one. Many of the current attempts for this question are based on the conjecture that it is the hierarchical distributed representations learned from data are the driven forces behind the effectiveness of deep models. Similar works such as conjectured that better representations can be exploited to produce faster-mixing Markov chains, therefore, a deeper model always helps. Tishby and Zaslavsky @cite treated the hidden layers as a successive refinement of relevant information and a deeper structure helps to speed up such process exponentially. Nevertheless, it seems for a deep model to work well, it is critical to obtain a better feature re-representation from intermediate layers.
- The mirror descent algorithm was introduced in @cite . The formulation was proposed in @cite , which is equivalent to the original one under standard assumptions. The interior gradient method studied in @cite is also of the form ; the difference lies in the technical conditions. Standard convergence analyses of the mirror descent, as discussed above, assume either bounded gradient or relative smoothness @cite @cite @cite @cite @cite . The exponentiated gradient method was proposed in @cite ; it is also known as the entropic mirror descent @cite .
- A different approach to subgroup fairness is studied by Dwork @cite . This work investigates the question of how to learn a decoupled'' classifier, where separate classifiers are learned for each subgroup and then combined to achieve a desired notion of fairness. While applicable in some settings, at times, this approach may be untenable. First, decoupling the classification problem requires that we have race, age, and other attributes of interest in the dataset and that the groups we wish to protect are partitioned by these attributes; this information is often not available. Even if this information is available, , it may not always be obvious which subpopulations require special attention. In contrast, the multiaccuracy approach allows us to protect a rich class of overlapping subpopulations without explicit knowledge of the vulnerable populations. An interesting direction for future investigation could try to pair multiaccuracy auditing (to identify subpopulations in need of protection) with the decoupled classification techniques of @cite .
- 3mm Popular approaches in learning task-oriented dialog systems include modeling the task as a partially observable Markov Decision Process (POMDP) @cite . Reinforcement learning can be applied in the POMDP framework to learn dialog policy online by interacting with users @cite . Recent efforts have been made in designing end-to-end solutions @cite @cite @cite @cite for task-oriented dialogs. designed a supervised training end-to-end neural dialog model with modularly connected components. Bordes and Weston proposed a neural dialog model using end-to-end memory networks. These models are trained offline using fixed dialog corpora, and thus it is unknown how well the model performance generalizes to online user interactions. proposed a hybrid code network for task-oriented dialog that can be trained with supervised and reinforcement learning. proposed an RL dialog agent for information access. Such models are trained against rule-based user simulators. A dialog reward from the user simulator is expected at the end of each turn or each dialog.
- 3mm Dialog reward estimation is an essential step for policy optimization in task-oriented dialogs. proposed PARADISE framework in which user satisfaction is estimated using a number of dialog features such as number of turns and elapsed time. proposed a collaborative filtering based method in estimating user satisfaction in dialogs. studied using convolutional neural networks in rating dialog success. further proposed an online active learning method based on Gaussian process for dialog reward learning. These methods still require various levels of annotations of dialog ratings by users, either offline or online. On the other side of the spectrum, Paek and Pieraccini proposed inferring a reward directly from dialog corpora with inverse reinforcement learning (IRL) @cite . However, most of the IRL algorithms are very expensive to run @cite , requiring reinforcement learning in an inner loop. This hinders IRL based dialog reward estimation methods to scale to complex dialog scenarios.
- Our work is also related to the joint multimodal variational autoencoder (JMVAE) @cite , which learns representations from multimodal data and allows bi-directional generation from one modality to another. The authors assumed that the same set of explanatory factors is involved in the generative process of all modalities, and the generation of each modality is conditioned on the same latent variable. Representations learned from the JMVAE are hence not disentangled. Furthermore, the modalities that are considered in the JMVAE are images and their attribute labels, having a relatively deterministic mapping from the former to the latter.
- proposed for atom selection, which basically corresponds to the case where @math is quadratic. However, its theoretical guarantee has not been proved. We extend their algorithm to the case where @math is RSC RSM and prove its guarantee. Many recent studies on @math -constrained minimization have been devoted to projected-gradient-based methods such as and @cite @cite @cite , which are typically faster than greedy-style methods. However, their theoretical guarantees imply that projected-gradient-based methods can be affected by the ill condition more negatively than greedy-style methods. For example, to achieve @math -error guarantees, forward greedy selection and require @math and @math , respectively (see, @cite @cite ), where @math is a condition number ; a larger @math implies a worse condition.
- In domain adaptation, some work has been applied to find a unified representation between domains, for example, by applying domain invariant training @cite and semantic similarity loss @cite . However, our approach does not involve multiple domains -- we only handle data from one domain. Here, the term domain' refers to how the data are naturally generated, whereas the term modality' refers to how different aspects of data are observed.
- There are two main approaches to network interpretation in the literature: filter-level interpretation @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite and holistic-level interpretation @cite @cite @cite . The goal of filter-level interpretation is to understand and visualize the features that specific neurons learn. While it's easy to directly visualize the first convolutional layer filter weight to get a sense of the patterns they detect, it makes little sense to directly visualize deeper layer filter weights because they act as complex composite functions of lower layers' operations. Early examples of filter-level understanding include finding the maximally activated input patches @cite and visualizing the guided back propagation gradients @cite . Some works @cite try to synthesize visually pleasant preferred input image of each neuron through back-propagation into the image space. @cite applies a generator network to generate images conditioned on maximally activating certain last-layer neurons. The Plug and Play paper @cite further extends @cite to introduce a generalized adversarial learning framework for filter-guided image generation. Network dissection @cite measures the interpretability of each neuron by annotating them with predefined attributes like color, texture, part, etc. @cite proposes represent the image content and structure by knowledge graph.
- Attempts at holistic summarization mainly focus on visualizing important image subregions by re-weighting final convolutional layer feature maps. Examples include CAM @cite and Grad-CAM @cite . However, the visualization based method only provides coarse-level information, and it remains hard to intuitively know what feature or pattern the network has learned to detect. More importantly, the holistic heat map representation is sometimes insufficient to justify why the network favors certain classes over others when the attentional maps for different classes overlap heavily. See Figure for example.
- Fine grained recognition aims to discriminate between subcategories like species of birds, dogs and different make and model of cars, aircrafts, etc. The difficulty of fine grained recognition lies in the extremely large intra-class variance and small inter-class variance. Representative works include Bilinear Pooling @cite , which computes the outer product of the final layer feature maps. Attention based models @cite works by focusing attention to discriminative parts of an image object. Part based models @cite works by decomposing the image into part features to be readily compared. Fine grained recognition is special because it usually performs better than non-expert humans. It's therefore interesting to unveil the knowledge it learns towards decision making.
- Gated-Recurrent-Units (GRUs) @cite were suggested in order to reduce the number of parameters of the traditional and commonly used Long-Short-Term-Memory (LSTM) cell (). Similarly to HSG, in GRUs the new state is a weighted sum of the previous state and a non-linear transition of the current input and the previous state. The main difference is that the transition is of a depth of a single layer and, therefore, less robust.
- introduced a different variant of the LSTM cell which is inspired by Resnet @cite . They proposed adding to the LSTM cell a residual connection from its input to the reset gate projection output. By that they allowed another route for the information to flow directly through. They managed to train a net of 10 residual LSTM layers which outperformed other architectures. In their work, they focused on the way that the information passes through layers in the feed-forward manner, and not on the way it passes through time.
- Another article, relating to Zoneout regularization () also relates to information flow through time. The authors introduced a new regularization method for RNNs, where the idea is very similar to dropout @cite . The difference is that the dropped neurons in the state vectors get their values in the former time-step, instead of being zeroed. They mentioned that one of the benefits of this method is that the BPTT skips a time-step on its path back through time. In our work, there is a direct (weighted) connection between the current state and the former one, which is used similarly both for training and inference.
- : The essence of deep learning is to compute hierarchical features or representations of the observational data @cite @cite . With the surge of deep learning research and applications in recent years, lots of research works have appeared to apply the deep learning methods, like deep belief network @cite , deep Boltzmann machine @cite , Deep neural network @cite @cite and Deep autoencoder model @cite , in various applications, like speech and audio processing @cite @cite , language modeling and processing @cite @cite , information retrieval @cite @cite , objective recognition and computer vision @cite , as well as multimodal and multi-task learning @cite @cite .
- : Text generation has been an important problem in both text mining and natural language processing. Depending on the input information, the text generation problem can be categorized into text generation from @cite , @cite , @cite , @cite and @cite . In @cite , the authors propose a method consisting of candidate-text construction and evaluation for sentence generation from keywords headwords. @cite introduced a global model for concept-to-text generation, which refers to the task of automatically producing textual output from non-linguistic input. In terms of the objective output, the text generation problem includes @cite , @cite and @cite . Various models have been used in the text generation problems, including @cite , @cite and @cite .
- : The problem studied in this paper is also closely related with the problem studied in software engineering. Formally, the goal of software program synthesis is to generate programs automatically from high-level specifications, lots of research works have been done on this topic already. Program synthesis is a challenging problem, which may require external supervisions from either @cite , and @cite , and @cite . In @cite , the authors present a novel approach to automatic synthesis of loop-free programs based on a combination of oracle-guided learning from examples. Based on the program templates, @cite introduces an approach to generate the programs from the templates. @cite introduce an algorithm for synthesizing recursive functions that process algebraic datatypes, which exploits both type information and input-output examples to prune the search space. Some other program synthesis works address the problem with @cite , @cite , @cite , and @cite .
- A new trend related to network interpretability is to learn networks with disentangled, interpretable representations @cite @cite @cite . Many studies learn interpretable representations in a weakly-supervised or unsupervised manner. For example, capsule nets @cite and interpretable RCNN @cite learned interpretable middle-layer features. InfoGAN @cite and @math -VAE @cite learned meaningful input codes of generative networks. The study of interpretable CNNs @cite developed a loss to push each middle-layer filter towards the representation of a specific object part during the learning process without given part annotations, which is the closest to our research. However, as mentioned in @cite , an interpretable model cannot always ensure a high discrimination power, which limits the applicability of above interpretable models. Therefore, instead of directly boosting the interpretability of the performer network, we propose to learn an explainer network in an unsupervised fashion.
- Our study is also related to meta-learning @cite @cite @cite @cite . Meta-learning uses an additional model to guide the learning of the target model. In contrast, our research uses an additional explainer network to interpret middle-layer features of the target performer network.
- ASP Modulo extensions for handling specialised domains and abstraction mechanisms provides a powerful means for the utilising ASP as a foundational knowledge representation and reasoning (KR) method for a wide-range of application contexts. This approach is clearly demonstrated in work such as ASPMT , @cite , ASPMT(QS) . Most closely related to our research is the ASPMT founded system @cite . Whereas provides a valuable blueprint for the integration and formulation of geometric and spatial reasoning within answer set programming modulo theories, the developed system is a first-step and lacks support for a rich spatio-temporal ontology or an elaborate characterisation of complex space-time' objects as native (the focus there has been on enabling non-monotonicity with a basic spatial and temporal ontology). In addition to the ontological extensions for a much richer space-time' component, our system pipeline --based on --- has the following additional advantages over the standard ASPMT ASPMT( ) pipeline: . we generate spatially consistent models compared to only one model in the standard ASPMT pipeline; . we compute optimal answer sets, e.g. add support preferences, which allows us to rank models, specify weak constraints; . unlike ASPMT( ) we support quantification of space-time regions.
- In recent years first partially feasible algorithms for fully self-driving cars, capable of executing lane changes, have been developed and tested on the road. Mostly these were rule-based systems. A probabilistic framework for decision making was developed by and tested during autonomous drives on highways @cite . Ulbrich and Maurer @cite used Dynamic Bayesian Networks (DBNs) to evaluate situations to answer the question whether they are feasible and beneficial with respect to lane changes. Men ' e ndez- introduced a multi-level planning framework which makes use of semantic and numeric reasoning @cite . used deep reinforcement learning for making lane change decisions @cite .
- Another important and related problem is the prediction of future driving situations. Recently LSTMs have been adopted by the autonomous driving community for this, leading to good results in predicting trajectories and future driving maneuvers, outperforming non-recurrent architectures @cite .
- Most related and comparable to our work are binary classification problems assessing the suitability of a situation for a lane change, as the previously mentioned work from Ulbrich @cite . @cite described a gap assessment model for discretionary lane changes using SVMs. For evaluation purposes a selected subset of the NGSIM US Highway 101 (US 101) dataset was used. Their approach outperformed previous ones, correctly classifying 97.48 @cite addressed the problem using fuzzy logic rules, with the aim of supporting the driver in lane change decisions. The NGSIM Interstate 80 (I-80) and the US 101 dataset were used while considering only certain lanes and time periods. Correct recommendations were given in 90.50
- To avoid parameter tuning problem, some methods use a nonparametric scheme to initialize the background image. Liu @cite present a background initialization method based on nonparametric model. The most reliable background mode is calculated based on mean shift clustering and the value is taken as the estimated background. Elgammal @cite introduce a nonparametric background modeling by estimating the probability of pixel intensity values based on kernel estimator. The pixel is then considered whether to be a background based on estimated probability. @cite , Zhang propose a two-stage background initialization method. The first stage monitors pixel intensity to identify background variations and creates a look-up table as the intensity distribution. Then in the second stage, based on whether current pixel is in the look-up table, the final background is determined.
- Several attempts have already been made to study the supply-demand levels and imbalances in taxi services @cite @cite . Identification and modeling of passenger hot spots for rerouting taxi drivers is also a widely researched area. Various methodologies have been proposed to this end, with Auto Regressive Integrated Moving Average model (ARIMA) and its variants @cite @cite , Exponential Weighted Average models @cite , Nearest Neighbour clustering @cite @cite , Neural Networks (NN) @cite being some of the commonly used modeling techniques. Irrespective of the modeling technique used, the preliminary step in modeling a location based entity such as passenger demand is to spatially partition the space. In the transportation literature, two common tessellation strategies are used. Spatial aggregations are either performed using grids, where the space is partitioned into square or rectangular grids of fixed area @cite @cite @cite , or using polygons, where the space is partitioned into regular or irregular polygons of variable area @cite @cite @cite . It is common practice in the transportation literature to consider either one of these tessellation styles for spatial partitioning, without motivating the choice of the tessellation technique.
- In our previous work @cite , we tessellated the city of Bengaluru, India, into fixed-sized partitions known as geohashes. We observed that for those regions with low demand density, fixed sized partitions resulted in data scarcity, which led to low model accuracy. To improve the accuracy, we explored the spatial correlation between the neighbouring geohashes to enhance the performance of the models. The performance limitation due to the chosen tessellation strategy motivated us to conduct a comparison study of various tessellation strategies. In this scenario, a few significant questions arise: (i) How can one decide the tessellation scheme to be used?, (ii) How sensitive is the performance of the models to the tessellation strategy?, (iii) Can we arrive at a tessellation strategy that works for a broad range of datasets? We aim to address these questions through this paper. To the best of our knowledge, an extensive study of the tessellation techniques and their effects on the model performance have not been conducted in the past. In this work, we explore the relationships between tessellation strategies, demand densities and city geographies. This is one of the features that distinguishes our work from the existing literature.
- The Average Common Substring (ACS), proposed by Burstein @cite , is a simple alignment-free sequence comparison method. This measure and its various extensions @cite @cite @cite @cite @cite @cite @cite have proven to be useful in multiple applications @cite @cite @cite @cite @cite @cite . Formally, ACS of a sequence @math w.r.t. another sequence @math , denoted by @math , is @math The @math of two input sequences is the length of their longest common prefix. The (symmetric) distance based on ACS is @cite : @math
- The computation of ACS is straightforward in @math space and time using the generalized suffix tree of @math and @math , where @math is the input size @cite . In this paper, we study the problem of computing ACS, where the input sequences are @math and @math , where @math (resp., @math ) is the sequence corresponding to the run-length encoding of @math (resp., @math ). Run-length encoding is a simple algorithm used for data compression in which runs of data (occurring of the same character on consecutive positions) are stored as a single charter followed by the count of its consecutive occurrences. The challenge here is to design an algorithm for computing ACS in space and time close to @math instead of @math , where @math . We answer this question positively by presenting the following theorem.
- CNNs dominate recent computer vision challenges and especially residual networks @cite are among the most successful architectures. Residual networks were the first deep architectures that did not suffer from a performance degradation due to the increased difficulty of optimizing a large number of layers. This is achieved by adding regularly spaced shortcut connections to an otherwise linear stack of layers.
- Object detection is the problem of detecting possibly multiple objects of various classes in an image and returning their locations. This is somewhat related to the problem at hand: surgical tools must be detected in an image but the location is irrelevant. However, all state-of-the-art object detectors reviewed in @cite need bounding box labels for their training, which are not available in the CATARACTS dataset. Consequently, existing object detection networks are unsuitable for this problem.
- In the dataset at hand zero to three surgical tools can be visible in each video frame. Therefore, one image is associated with a set of labels instead of exactly one label. This setting is called multi-label learning. Using neural networks for multi-label problems is mostly a matter of choosing an appropriate last layer, loss function and (often implicitly) problem transformation. Specifically for CNNs, @cite compare different loss functions in a multi-label setting. @cite and @cite both perform multi-label image classification by treating the task as @math independent binary classification problems. As such, their CNNs have @math output nodes which is an implicit way of applying the binary relevance transformation @cite .
- Training CNNs with millions of weights requires a large amount of labeled training data. This is expensive or even infeasible to collect for many tasks. However, it has been shown that the lower layers of deep CNNs used for image classification learn very general filters that are applicable to different datasets as well @cite . The idea behind transfer learning is to pretrain a network on a very large image datasets, such as ImageNet. The network can then be used as a fixed feature extractor or fine-tuned by training it further on the target dataset. Both of these transfer learning approaches will be explored and compared in this paper.
- The CATARACTS dataset is drastically imbalanced since some tools are used much more often during cataract surgery than others. This makes classification accuracy meaningless as a performance metric. Receiver operating characteristic (ROC) graphs @cite are a viable alternative that can be aggregated into a scalar performance metric by calculating the area under curve (AUC).
- Furthermore, the class imbalance influences a neural network's stochastic gradient descent training. Since training examples of the majority classes are much more common, the gradient direction is dominated by examples of these majority classes @cite @cite . A possible remedy is to scale the loss by class prevalence, i.e. increasing the loss and therefore gradient length for low-prevalence classes. This approach is explored in this paper as well.
- Much of the work related to visual surgical tool detection from 2000 to 2015 has been reviewed in @cite . While the goals vary from detecting presence to estimating tool poses in 3D, all 28 reviewed methods have in common that they use hand-crafted image features as input to different modeling strategies. The same holds true for more recent publications @cite @cite @cite @cite that also work with cataract surgery datasets. Additionally, @cite employs preprocessing methods specifically tuned to the cataract surgery dataset that prevents easy application to different tool detection problems. Work using CNNs in this application domain is just beginning to emerge. EndoNet @cite is similar to this work: AlexNet is fine-tuned to detect different tools and surgical phases in laparoscopic videos. The main difference is a three-times smaller amount of tools in their dataset. Furthermore, there is concurrent work with CNNs being done by other authors for the CATARACTS challenge.
- The greedy approach is also called one-step-lookahead policy, which can be arbitrarily worse than optimal @math -step-lookahead ( @math -step myopic active search) policies for @math . Arbitrary @math -step myopic active search is hard to compute, as was shown by @cite , which also proved that nonmyopic active search is computationally hard even to approximate and proposed an efficient searching algorithm. This algorithm is potentially useful in the preference elicitation context and is an interesting future direction.
- Most previous works in preference elicitation assumed that people's preferences are deterministic. For example, @cite proposed an even swap algorithm to reveal a single decision maker's most preferred alternative; @cite @cite elicited preferences from a group of people in order to make a group decision under a (deterministic) voting rule. In contrast, we consider non-deterministic preferences of people, which is often the case in real-world. Moreover, we use randomized voting rules, which output the probability of each alternative to be the winner. These probabilities, which can be viewed as normalized scores over all alternatives, provide a quantitative measure of the quality of each alternative. For example, an alternative that wins with probability 0.8 can be seen as being much better than other alternatives.
- Non-deterministic preferences were modeled by general random utility models by @cite . They proposed a preference elicitation framework for personalized choice and social choice (aggregated preference). We use the Plackett-Luce model with features, which is a special case of general random utility models but has easy-to-compute probabilities. More importantly, we use randomized voting rules for aggregation, which is very different from parametric modeling of social choices employed by .
- Pairwise elicitation questions may be the most widely explored in the literature due to their simplicity . In contrast, @cite focused on elicitation of full rankings, though their proposed framework also allows for partial orders. @cite @cite studied a larger set of queries, which includes asking a person to rank her top @math choices over all alternatives. In this paper, we consider an even broader set of queries, asking an agent to rank her top @math choices over a subset of @math alternatives ( @math ). This enables us to elicit preferences in a more cost-effective manner.
- As a key role in preference elicitation, information criteria have been widely investigated for different applications. Standard information criteria include D-optimality (used in ) and E-optimality. @cite and @cite use minimax-regret-based criterion for stable matching and aggregation respectively. @cite proposed yet another criterion, defined on the certainty of the least certain pairwise comparison over the intrinsic utilities (part of the parameter of their general random utility models) of all alternatives. Our MPC criterion extends the criterion by @cite . To predict a single agent's top @math preference, we search over a subset of all pairwise comparisons (see ). To help make a group decision, we search over all pairwise comparisons of all agents in the key group to find the least certain pairwise comparison (Equation ).
- Some architectures address longer-term memory by storing information verbatim over time intervals. NARX @cite derives new outputs from a queue ("delay line") of prior network inputs and outputs. More recently, Clockwork RNN @cite has units that only update at set intervals and otherwise hold their state. In contrast to these fully-connected networks with non-lossy storage, LP-RNN accepts information loss within a preset arrangement of pooled filtering units specifically built to obtain the temporally-tiled impulse behaviour in Figure .
- Filtering has been applied to simpler recurrent networks with some minor improvements @cite . @cite adds low-pass filtered versions of the network's input to the inputs to the network's recurrent and output layers. The same filtering is used throughout, so this approach is akin to having the first pool in a Concrete memory project everywhere, provided the pool directly filters the input and not some hidden state.
- @cite by contrast does present a recurrent model with multiple filterings of hidden state, but these project via fully-connected recurrent connections back into the model. The fact that they are computed in parallel implies that models impulse response would be quite different from the Concrete architecture. The motivation of their approach is also quite different from ours. More recently, the neurologically-inspired SITH model @cite proposes a somewhat more restricted variant of this concept that limits projections of multiply-filtered to those yielding localised impulse responses similar to the ones in Figure , but this approach still employs full matrix multiplications to compute unit values.
- Finally, there is recent work on online learning for recurrent models. Much of it adapts Real Time Recurrent Learning @cite @cite , which applies forward (instead of reverse) accumulation of the chain rule. Learning can be purely online this way, but the method requires explicit computation and storage of the Jacobian of the recurrent model's transition function. This can be impracticable for large networks. @cite explores various low-rank Jacobian approximations. Taking a different approach, @cite replaces BPTT with a learnt parameterised model of the gradient. These approaches show promise, but none appear ready to replace truncated BPTT in practical applications.
- Style transfer is a kind of non-realistic rendering techniques @cite that is closely related to texture synthesis @cite @cite . It usually exploits local statistics for efficient cross-view dense correspondences and texture quilting @cite @cite @cite @cite . Although these methods produce appealing stylized images @cite @cite , dense correspondences are limited to a pair of images with similar contents, thus inapplicable to zero-shot style transfer.
- Gatys al @cite @cite at the first time formulated the style as multi-level feature correlations ( , Gram matrix) from a trained neural network for image classification, and defined the style transfer as an iterative optimization problem that balances the content similarity and style affinity in the feature level (or termed as perceptual loss @cite ). A number of variants have been developed thereafter to adapt this framework to different scenarios and requirements, including photorealistic rendering @cite , semantically composite transfer @cite , temporal coherence @cite and so on. Changing the global style statistics into Markovian feature assembling, a similar framework is also proposed for semantic image synthesis @cite . Despite visual appealing performances for arbitrary style, the results are not stable @cite @cite and the perceptual loss usually requires careful parameter tunning for different styles @cite . Moreover, the inefficiency of the optimization-based approaches restrains real-time applications.
- Recent researches @cite @cite @cite @cite @cite @cite @cite @cite @cite try to tackle the complexity issue by approximating the iterative back-propagating procedure to feed-forward neural networks, either trained by the perceptual loss or Markovian generative adversarial loss @cite . However, @cite @cite @cite have to train a independent network for every style. To strengthen the representation power for multiple styles, StyleBank @cite learns filtering kernels for styles and Li al @cite transfered styles by binary selection units, as well as Dumoulin al proposed conditional instance normalization @cite controlled by channel-wise statistics learned for each style. But the manually designed style abstractions are often short for the representation of unseen styles and the combinational optimization over multiple styles often compromises the rendering quality @cite @cite .
- To achieve zero-shot style transfer in a feed-forward network, Huang al @cite adjusted channel-wise statistics of the content features by adaptive instance normalization (AdaIN), and trained a feature decoder by a combinational scale-adapted content and style losses. Chen al @cite swapped the content feature patches with the closest style features ( Style-Swap) at the intermediate layer of a trained auto-encoder, while Li al @cite transferred multi-level style patterns by recursively applying whitening and coloring transformation (WCT) to a set of trained auto-encoders with different levels. However, AdaIN over-simplifies the transferring procedure and Style-Swap cannot parse sufficient style features when content and style images are semantically different. WCT is the state-of-the-art method, but the holistic transformation may results in distorted style patterns in the transferred features, and generate unwanted patterns in the output image.
- The proposed zero-shot style transfer is related to @cite @cite @cite . But the improves AdaIN and WCT as it reserves the detailed style patterns rather than the parameterized feature statistics. It also outperforms Style-Swap as it effectively parses the complete set of style features regardless of their domain gap. Moreover, the proposed network performs multi-scale style adaptation in one feed-forward pass, which surpasses AdaIN and WCT since AdaIN requires a style-oriented image decoder and WCT needs a set of recursive feed-forward passes to enable multi-scale style transfer.
- . The cascade face detector @cite , which uses Haar-like features and AdaBoost to train a cascade of binary classifiers, was the seminal work for real-time detection of near-frontal faces. The cascade face detector has since been improved by using: (1) robust hand-crafted features, e.g., HOG @cite , MB-LBP @cite , SURF @cite , ACF @cite , CCF @cite , NPD @cite , and LDF @cite ; (2) asymmetric feature selection approaches, e.g., FFS @cite and SAFS @cite ; (3) boosting algorithms, e.g., MILBoost @cite , RealBoost @cite , and LACBoost @cite ; (4) new cascade structures, e.g., boosting chain @cite , Closed-Loop @cite , and soft-cascade @cite ; and (5) multi-task learning, e.g., face non-face classification, bounding box regression @cite , facial landmark localization @cite , and face pose estimation @cite @cite .
- . CNNs have recently contributed significantly to the progress in image classification @cite . Inspired by this, CNNs have been introduced to cascade face detection @cite and improved by jointly training all stages @cite . Recently, CNN-based cascade face detectors have been further improved by using multi-task learning, i.e., jointly learning face non-face classification, bounding box regression as well facial key point detection @cite .
- . Anchor-based detectors, which use multi-scale anchors at each sliding window position to simultaneously predict multiple different candidate regions, were first proposed in Faster R-CNN @cite . After that, SSD @cite tried to assign different anchors to feature maps with different receptive fields. These anchor-based detectors have shown impressive performance in face detection @cite . Recently, anchor-based face detectors have been further improved by addressing (1) the mismatch between the receptive fields and anchor sizes @cite , (2) the positions of facial landmarks @cite @cite , and the scale distribution histogram of faces @cite @cite .
- . Contextual information has turned out to be crucial for object detection @cite @cite and recognition @cite @cite @cite @cite . Recently, @cite analyzed the relationship between templates with different context regions. To explore contextual information in anchor-based face detectors, @cite combined the features from different region of interests (RoIs), while @cite designed a context module using multiple different convolutional filters, e.g., 5x5 and 7x7 filters. Specifically, @cite cascaded the predictions from different RoIs to efficiently boost the performance. To explore contextual information for cascade face detectors, @cite used an additional network to extract features with body information.
- Partitioning an area into polygons of fixed shape or area is a class of problems which has been regularly studied in the operations research and computational geometry literature. @cite @cite defined two optimization problems that seek to partition the unit square into a number of rectangles with given areas, so as to optimize parallel matrix-multiplication algorithms in heterogeneous parallel computing platforms. The first problem aims to minimize the sum of all rectangle perimeters, whereas the second problem aims to minimize the largest perimeter. These problems are special cases of and where the general rectangular region is a square. The authors introduced an @math -approximate algorithm and an @math -approximate algorithm for these problems, respectively.
- Other close variants of have been studied. @cite @cite considered the problem of decomposing a square or a rectangle into a number of rectangles of equal area so as to minimize the maximum rectangle perimeter. VLSI floorplan design and facility location applications also led to a number of related studies . @cite proposed a local search and mathematical programming algorithm to solve rectangular packing problems where the shapes of the rectangles are adjustable within given perimeter and area constraints.
- Finally, @cite considered and as a building block to design approximation algorithms for and when the general rectangular region is a square. The authors introduced an exact @math algorithm for and two approximation algorithms for . The complexity status of remained open. Moreover, has not been studied to this date. These methodological gaps along with the relevance of these problems for the Vietnamese land reform are a strong motivation for additional research.
- Apart from sentence level sentiment classification @cite @cite , aspect target level sentiment classification is also an important research topic in the field of sentiment analysis. The early methods mostly adopted supervised learning approach with extensive hand-coded features @cite @cite @cite @cite @cite @cite @cite , and they fail to model the semantic relatedness between a target and its context which is critical for target sentiment analysis. incorporate the target information into the feature learning using dependency trees. As observed in previous works, the performance heavily relies on the quality of dependency parsing. propose to split the context into two parts and associate target with contextual features separately. Similar to @cite , develop a three-way gated neural network to model the interaction between the target and its surrounding contexts. Despite the advantages of jointly modeling target and context, they are not capable of capturing long-range information when some critical context information is far from the target. To overcome this limitation, researchers bring in the attention mechanism to model target-context association @cite @cite @cite @cite @cite @cite @cite @cite @cite . Compared with these methods, our TNet avoids using attention for feature extraction so as to alleviate the attended noise.
- Furthermore, there was another type of self-attention mechanism capturing the contribution and dependency of each token to the entire source sequence for a specific task, which can be used on sentence encoding or sequence compression task. liu2016learning proposed an intra-sentence attention mechanism where the pooling result of the input sequence is used as the query attending to each token from the same sequence. They applied it to sentence embedding tasks. lin2017structured proposed a self-attentive model using a matrix to represent the sentence embedding, with each row of the matrix attending to a different part of the sentence. It shares a similar idea with the multi-head attention @cite . shen2017disan proposed a source2token self-attention mechanism that removes the query from the multi-dim compatibility function, for the purpose of directly modeling the feature-wise contribution of each token to the entire input source on a specific task.
- Another way to deal with heavy-tailed distributions is the median-of-means estimator . The basic idea is to divide the data into several groups, calculate the sample mean within each group, and take the median of these means. Recently, @cite @cite generalize the median-of-means estimator to arbitrary metric spaces, and apply it to the minimization of smooth and strongly convex losses. Specifically, for @math -regression with heavy-tailed distributions, a high-probability @math excess risk is established, under slightly stronger assumptions than these of @cite . For regression problem, @cite introduce a new procedure, the so-called median-of-means tournament, which achieves the optimal tradeoff between accuracy and confidence under minimal assumptions. The setting of @cite is general in the sense that the function space could be any convex class of functions, not necessary linear, but the performance is only measured by the squared loss. Compared with truncation based approaches, the advantage of median-of-means approaches is that they do not require prior knowledge of distributional properties. However, the current theoretical results of median-of-means are restricted to the squared loss or strongly convex losses, and thus cannot be applied to @math -regression considered in this paper.
- black Designing trustworthy cloud service systems has been investigated extensively in the literature. Various methods, including a feedback evaluation component, Bayesian game, and domain partition have been proposed @cite @cite @cite . Trust models to predict the cloud trust values (or reputation) can be mainly divided into objective and subjective classes. The first are based on the quality of service parameters, and the second are based on feedback from cloud service users @cite @cite .
- black In the IoCT, however, agents may not have sufficient number of interactions, which makes reputation challenging to obtain @cite . In addition, trust value-based cloud trust management systems can be compromised by reputation attacks through fake feedback which can severely degrade the system performance @cite @cite . Therefore, in this work, we aim to design a strategic trust mechanism which is predictive rather than reactive through an integrative game-theoretic framework. Rather than using trust value @cite @cite , IoCT devices in our iSTRICT model make decisions based on the strategies of players at the cloud layer as well as based on the physical system performance. This multi-layer design provides resilience to reputation attacks.
- black In terms of the technical framework, iSTRICT builds on existing achievements in IoCT architecture design @cite @cite @cite @cite , which describe the roles of different layers of the IoCT at which data is collected, processed, and accessed by devices @cite . Each layer of the IoCT consists of different enabling technologies such as wireless sensor networks and data management systems @cite . Our perspective, however, is distinct from this literature because we emphasize an integrated mathematical framework. iSTRICT leverages game theory to obtain optimal defense strategies for IoCT components, and it uses control theory to quantify the performance of devices.
- Image generation has been widely explored in recent years. Models based on variational autoencoder (VAE) @cite aim to improve the quality and efficiency of image generation by learning an inference network. GANs @cite were firstly proposed to generate images from random variables by a two-player minimax game. Researchers have been exploited the capability of GANs for various image generation tasks. @cite proposed to synthesize images at multiple resolutions with a Laplacian pyramid of adversarial generators and discriminators, and can condition on class labels for controllable generation. @cite introduced a class of deep convolutional generative networks (DCGANs) for high-quality image generation and unsupervised image classification tasks.
- Instead of learning to generate image samples from scratch (i.e., random vectors), the basic idea of image-to-image translation is to learn a parametric translation function that transforms an input image in a source domain to an image in a target domain. @cite proposed a fully convolutional network (FCN) for image-to-segmentation translation. Pix2pix @cite extended the basic FCN framework to other image-to-image translation tasks, including label-to-street scene and aerial-to-map. Meanwhile, pix2pix utilized adversarial training technique to ensure high-level domain similarity of the translation results.
- The image-to-image models mentioned above require paired training data between the source and target domains. There is another line of works studying unpaired domain translation. Based on adversarial training, @cite and @cite proposed algorithms to jointly learn to map latent space to data space and project the data space back to latent space. @cite presented a domain transfer network (DTN) for unsupervised cross-domain image generation employing a compound loss function including multiclass adversarial loss and @math -constancy component, which could generate convincing novel images of previously unseen entities and preserve their identity. @cite developed a dual learning mechanism which can enable a neural machine translation system to automatically learn from unlabeled data through a dual learning game. Following the idea of dual learning, DualGAN @cite , DiscoGAN @cite and CycleGAN @cite were proposed to tackle the unpaired image translation problem by training two cross domain transfer GANs at the same time. @cite proposed to utilize dual learning for semantic image segmentation. @cite further proposed a conditional CycleGAN for face super-resolution by adding facial attributes obtained from human annotation. However, collecting a large amount of such human annotated data can be hard and expensive.
- Visual-inertial fusion on rolling shutter cameras has classically been done using Extended Kalman-filters (EKF). @cite use an EKF to track cell-phone orientation for the purpose of video stabilization. @cite extend this to full device motion tracking, and @cite add tracking of changes in relative pose between the sensors and changes in linear camera intrinsics.
- Also related to bundle adjustment is the factor graph approach. @cite study visual-inertial fusion with preintegration of IMU measurements between keyframes, with a global shutter camera model.
- Generative adversarial network (GAN) @cite was first proposed in 2014 by Goodfellow which is able to generate photo-realistic images and has gained extraordinary popularity within recent two years. Compared with other generative models, such as variational autoencoder (VAE), GAN can generate much sharper and more photo-realistic images. Compared with the vanilla GAN, condition GANs can generate images under controllable factors rather than generating images from random noise. For example, Pix2pix @cite and CycleGAN @cite generated images with different color, style, and content conditioning on given reference images. CAAE related works @cite @cite generated face with aging effect conditioning on different age labels. SRGAN @cite generated super-resolution images conditioning on low resolution images. @cite @cite filled in the incomplete holes of an image conditioning on the surrounding pixels. Most aforementioned works conduct image generation or translation within the same modality ( image modality). Few works conducted image generation between different modalities with large variations. For example, the text to image generation works @cite @cite synthesized image content based on given text description. @cite predicted instrument images by providing the music audio played by some instruments.
- Domain adaptation for image recognition has attracted attention for transferring the knowledge between different domains and reducing the cost for annotating a large number of images in diverse domains. Benchmark datasets are released @cite , and many methods for unsupervised domain adaptation and semi-supervised domain adaptation have been proposed @cite @cite @cite @cite @cite @cite @cite @cite . As previously indicated, unsupervised and semi-supervised domain adaptation focus on the situation where different domains completely share the class of their samples, which may not be practical especially in unsupervised domain adaptation.
- A wide variety of research has been conducted to reject outliers while correctly classifying inliers during testing. Multi-class open set SVM is proposed by @cite . They propose to reject unknown samples by learning SVMs that assign probabilistic decision scores. The aim is to reject unknown samples using a threshold probability value. In addition, method of harnessing deep neural networks for open set recognition was proposed @cite . They introduced OpenMax layer, which estimates the probability of an input being from an unknown class. Moreover, to give supervision of the unknown samples, a method to generate these samples was proposed @cite . The method utilizes GAN to generate unknown samples and use it to train neural networks, then combined it with OpenMax layer. In order to recognize unknown samples as unknown during testing, these methods defined a threshold value to reject unknown samples. Also, they do not assume that they can utilize unlabeled samples including known and unknown classes during training.
- An alternative i-vector framework is based on deep neural networks. DNN i-vector frameworks provides the best speaker recognition performance in "clean" speech conditions @cite @cite . In the DNN-based i-vector framework a deep neural network substitutes a UBM in calculation of Baum-Welch statistics, followed by total variability factor analysis. Alternatively, DNN can be used for extracting bottleneck (BN) features, which are used together with MFCC in the standard UBM i-vector framework. This also gives impressive speaker recognition performance @cite .
- In this section, we discuss the existing methods that have attempted to develop control laws for speed tracking. The initial work includes the development of control laws for vehicle following at both low speed and high speed scenarios @cite . A non-linear vehicle system is modeled to obtain the dynamic solution with safety and comfort constraints in @cite . In @cite , @cite , @cite they even explored the hybrid system framework, where the vehicle system is divided into local subsystems.
- There is a large body of literature on submodular maximization, thus we mention only a few which is most relevant to our work. Besides the aforementioned results, there are also some other well-known results towards more practical algorithm design. For the simple cardinality constraint, there is also a stochastic greedy algorithm which uses @math value oracle queries while achieving @math -approximation @cite . As for the general matroid constraint, Badanidiyuru and Vondr @math k @cite also proposed an accelerated continuous greedy algorithm which uses @math value oracle queries and @math matroid independence queries. Later @cite improve this result while exhibit an tradeoff between the value oracle query and independence query. They also give a @math -approximation algorithm for cardinality-constrained non-monotone submodular maximizing problem, which requires @math function value query.
- The STS task was first introduced by . Early methods focused on lexical semantics, surface form matching and basic syntactic similarity @cite @cite . More recently, deep learning based methods became competitive @cite @cite . One approach to this task is to train a general purpose sentence encoder and then calculate the cosine similarity between the encoded vectors for the pair of sentences. The encoding model can be directly trained on the STS task @cite or it can be trained on an alternative supervised @cite or unsupervised @cite task that produces sentence-level embeddings. The work described in our paper falls into the latter category, introducing a new unsupervised task based on conversational data that achieves good performance on predicting semantic similarity scores. Training on conversational data has been previously shown to be effective at email response prediction @cite @cite . We extend prior work by exploring the effectiveness of representations learned from conversational data to capture more general-purpose semantic information. The approach is similar to Skip-Thought vectors @cite , which learn sentence-level representations through prior and next sentence prediction within a document, but with our prior and next sentences being pulled from turns in a conversation.
- In we outlined the limitations of several related solutions. In this section we review additional related work. For a general comparison and classification of blockchain consensus, we refer the reader to @cite .
- Ouroboros Praos @cite is another PoS scheme that leverages VRFs for new random value generation on each round, similar to Algorand @cite . The main limitation of this approach is that such randomness can be biased and thus the solution does not provide fairness.
- RapidChain @cite samples a reference committee from all consensus participants. The reference committee is then responsible for running a distributed randomness generation protocol in the beginning of each epoch to create new randomness for that epoch. The randomness protocol is based on verifiable secret sharing (VSS). The main limitations of this approach is that the reference committee becomes an obvious target for attacks and the distributed random generation protocol is expensive.
- DFINITY @cite introduces a novel decentralized an random beacon that leverages BLS threshold signatures for periodic unbiased random values generation. This scheme requires a setup phase during which an expensive distributed key generation (DKG) protocol is run. Once this is done, new random values can be derived by collecting signature shares from sufficiently many participants. In this approach, the per round or per epoch randomness generation has low communication complexity, but the main cost is the expensive DKG protocol in the setup phase that needs to be repeated when new participants join or leave the system.
- Proof of Luck (PoL) @cite is SGX-based solution that has the same basic idea and the same main limitations as PoET (recall ).
- @cite considered the edge-weighted with random arrivals, and proposed a @math -competitive algorithm. The competitive ratio is tight as it matches the lower bound on the classical secretary problem @cite . Wang and Wong @cite considered a different model of with both sides of vertices arriving online (in an arbitrary order): A vertex can only actively match other vertices at its arrival; if it fails to match at its arrival, it may still get matched passively by other vertices later. They showed a @math -competitive algorithm for a fractional version of the problem.
- Recently, Cohen and Wajc @cite considered the (with arbitrary arrival order) on regular graphs, and provided a @math -competitive algorithm, where @math is the degree of vertices. Very recently, @cite proposed a fully online matching model, in which all vertices of the graph arrive online (in an arbitrary order). Extending the randomized primal-dual technique, they obtained competitive ratios above @math for both bipartite graphs and general graphs.
- Similar but different from the with random arrivals, in the stochastic , the online vertices arrive according to some known probability distribution (with repetition). Competitive ratios breaking the @math barrier have been achieved for the unweighted case @cite @cite @cite and the vertex-weighted case @cite @cite @cite .
- The with random arrivals is closely related to the oblivious matching problem @cite @cite @cite (on bipartite graphs). It can be easily shown that has equivalent performance on the two problems. Thus competitive ratios above @math @cite @cite directly translate to the oblivious matching problem. Generalizations of the problem to arbitrary graphs have also been considered, and competitive ratios above half are achieved for the unweighted case @cite @cite and vertex-weighted case @cite .
- GANs proposed by Goodfellow al @cite , generated a more realistic image than the previous studies. It contains two components: a discriminator, which distinguishes real images from fake ones, competes with a generator, which generates the fake images that look like real. In their adversarial training procedure, the generator generates outputs whose distribution gets closer to the real data distribution. GANs have been applied to various tasks, such as text-to-image conversion, super-resolution, or data augmentation @cite @cite @cite .
- Isola al @cite successfully performed image translation on varied datasets by using the GAN-based pix2pix'' framework, which allows a generator to learn the mapping function between two paired images. In their framework, the fake pair consists of the input and the generated output. And the discriminator tries to distinguish between the real and fake pairs. However, pix2pix framework needs paired data, which is difficult to obtain. CycleGAN @cite enabled unpaired image to image translation, using the idea of cycle consistency. For two sets of images, @math and @math , they trained two translators @math and @math so that @math becomes the same as original image @math . However, CycleGAN requires more than one model to deal with multiple sets.
- There have also been many attempts to change an image's attributes, rather than the whole image. InfoGAN @cite sets the loss to maximize mutual information between code vector @math and generated outputs, leading to a network that can learn image attributes. The DiscoGAN @cite framework is similar to that of CycleGAN, but its target task is to manipulate attributes of image such as hair color or sunglasses. Deep feature interpolation (DFI) @cite achieved high-resolution semantic transformations by linear interpolation of features obtained from a pre-trained convolutional neural networks.
- The approaches in this stream of research represent an image as a combination of local descriptors, where each local descriptor represents a spatial partition such as grid cell @cite @cite @cite and horizontal stripe @cite @cite @cite . They work well under a strict assumption that the location of a certain body part is consistent across images. This assumption is often violated under realistic conditions, thereby causing the methods to fail. An extreme case is that no spatial partition is used and a global representation is computed over the whole image @cite @cite @cite @cite @cite @cite .
- @cite also introduced part maps for person re-identification to solve the multi-people tracking problem. They used part maps to augment appearances as another feature, rather than to generate part-aligned representations, which is different from our method. Some works @cite @cite proposed the use of attention maps, which are expected to attend to informative body parts. They often fail to produce reliable attentions as the attention maps are estimated from the appearance maps; guidance from body part locations is lacking, resulting in a limited performance.
- Recommender systems Recommender systems aim at learning user preferences on unknown items from their past history. Content-based recommendations are based on the matching between user profiles and item descriptions @cite . It is difficult to build the profile for each user when there is no few content. Collaborative filtering (CF) alleviates this issue by predicting user preferences based on the user-item interaction behavior, agnostic to the content @cite . Latent factor models learn feature vectors for users and items mainly based on matrix factorization (MF) @cite which has probabilistic interpretations @cite @cite . MF is also flexible to integrate text @cite @cite , social relations @cite @cite , and implicit feedback @cite @cite . Factorization machines can mimic MF @cite . Some hierarchical methods can reduce to factorize a specific matrix @cite . Random walk and heterogeneous networks are adapted for recommendation @cite @cite . Neural networks are proposed to push the learning of feature vectors towards non-linear representations @cite @cite @cite . CF models, however, suffer from the data sparsity issue.
- Cross-domain recommendation @cite is an effective technique to alleviate sparse issue. A class of methods are based on MF applied to each domain, including collective MF (CMF) @cite with its heterogeneous variants @cite and codebook transfer @cite @cite . Active learning @cite can construct entity correspondence with limited budget. Heterogeneous cross-domain @cite and multiple source domains @cite are also proposed to account for different cases of input. These are all shallow methods and have the difficulty in learning complex (highly nonlinear) user-item interaction relationship @cite @cite @cite . We follow this research thread by using deep networks to learn the nonlinear interaction function.
- Transfer and multitask learning Transfer learning (TL) aims at improving the performance of the target domain by exploiting knowledge from source domains @cite . The typical TL technique in neural networks is two-step: initialize a target network with transferred features from a pre-trained source network @cite @cite . Different from this approach, we transfer knowledge in a deep way such that two base networks benefit from each other during the learning procedure. Similar to TL, the multitask learning (MTL) is to leverage useful knowledge in multiple related tasks to help each other @cite @cite . Multi-view learning @cite is closely related to MTL. The cross-stitch network (CSN) @cite enables information sharing between two base networks. We generalize CSN by relaxing the underlying assumption, especially via the idea of selecting representations to transfer.
- Content Delivery Network (CDN) and Round-Robin DNS (RRDNS) are legitimate techniques which are used by large websites to distribute the load of incoming requests to several servers. The response to a DNS query is evaluated by an algorithm which chooses a pool of IPs from a large list of available servers whose number can be of the order of thousands (see Sect. for some examples). As a result, the behaviour in terms of DNS traffic is very similar to the one of a FFSN, and indeed CDNs and RRDNSs represent the typical false positives in fast flux detection algorithms @cite @cite @cite .
- A large number of approaches have been proposed to detect FFSNs and to distinguish them from legitimate CDNs and RRDNSs. Most of them rely on active DNS analysis, which allows for the collection of a large number of IPs associated with a domain, thus simplifying the FFSNs detection, but they require the resolutions of domains that may be associated with malicious activities @cite @cite @cite @cite @cite . These methods, despite being appropriate for a deep analysis of FFSNs, have relevant drawbacks in implementations oriented to the monitoring of corporate networks @cite @cite .
- Some FFSN detection methods based on passive DNS analysis have been proposed. Some of them analyse the DNS traffic of a whole Internet Service Provider (ISP), thus taking in input the DNS traffic generated by many different networks. @cite , in particular, performed a large-scale passive analysis of DNS traffic. They extract some relevant features from the DNS traffic and classified the domains via a C4.5 decision tree classifier. @cite and @cite proposed two other approaches to analyse the DNS traffic of an ISP. Both methods are based on a tool called DNSMap and classify the bipartite graphs formed by the collected fully qualified domain names and the associated IPs. The first method searches for generic malicious usage of DNS, while the latter focuses on FFSNs.
- Visual odometry and SLAM datasets: The TUM RGB-D dataset @cite is focused on the evaluation of RGB-D odometry and SLAM algorithms and has been extensively used by the research community. It provides 47 RGB-D sequences with ground-truth pose trajectories recorded with a motion capture system. It also comes with evaluation tools for measuring drift and SLAM trajectory alignment. For evaluating monocular odometry, recently the TUM MonoVO dataset @cite has been proposed. The dataset contains 50 sequences in indoor and outdoor environments and has been photometrically calibrated for exposure times, lens vignetting and camera response function. Drift can be assessed by comparing the start and end position of the trajectory which coincide for the recordings. We also provide photometric calibration for our dataset, but additionally recorded motion capture ground truth in parts of the trajectories for better pose accuracy assessment. Furthermore, the above datasets do not include time-synchronized IMU measurements with the camera images like our benchmark.
- For research on autonomous driving, visual odometry and SLAM datasets have been proposed such as Kitti @cite , Malaga Urban dataset @cite , or the Robot Oxford car dataset @cite . The Kitti and Malaga Urban datasets also include low-frequency IMU information which is, however, not time-synchronized with the camera images. While Kitti provides a GPS INS-based ground truth with accuracy below 10 ,cm, the Malaga Urban dataset only includes a coarse position for reference from a low-cost GPS sensor. Our dataset contains 20 camera images and hardware time-synchronized 3-axis accelerometer and gyro measurements at 200 . Ground-truth poses are recorded at 120 and are accurately time-aligned with the sensor measurements as well.
- Visual-inertial odometry and SLAM datasets: Similar to our benchmark, some recent datasets also provide time-synchronized IMU measurements with visual data and have been designed for the evaluation of visual-inertial (VI) odometry and SLAM approaches. The EuRoC MAV dataset @cite includes 11 indoor sequences recorded with a Skybotix stereo VI sensor from a MAV. Accurate ground truth (approx. 1mm) is recorded using a laser tracker or a motion capture system. Compared to our benchmark, the sequences in EuRoC MAV are shorter and have less variety as they only contain recordings in one machine hall and one lab room. Furthermore, EuRoC MAV does not include a photometric calibration which is important to benchmark direct methods. Further datasets for visual-inertial SLAM are the PennCOSYVIO dataset @cite and the Zurich Urban MAV dataset @cite . However, they do not contain photometric calibration and as accurate ground truth or time-synchronization of IMU and camera images like our benchmark (cf. tab:relateddatasets ).
- Feature fusion is frequently employed in semantic segmentation for different purposes and concepts. A lot of methods fuse low-level but high-resolution features and high-level low-resolution features together @cite @cite @cite @cite @cite @cite . Besides, module is proposed in DeepLab @cite @cite @cite to fuse multi-scale features to tackle objects of different size. Pyramid pooling module in PSPNet @cite serves the same purpose through different implementation. BoxSup @cite empirically fuses feature maps of bounding boxes and segmentation maps to further enhance segmentation.
- There are mainly three approaches to upsample a feature map. The first one is bilinear interpolation, which is widely used in @cite @cite @cite @cite . The second method is deconvolution, which is initially proposed in FCN @cite and utilized in later work such as @cite @cite @cite @cite @cite . The third one is called sub-pixel convolution", which derives from @cite @cite in super resolution task and is widely broadcast to other tasks such as semantic segmentation. For instance, @cite employs it to replace the traditional deconvolution operation.
- A multivariate graph is a graph where the nodes and or edges are associated with attributes @cite . Although most graphs have some attributes, such as a node type, multivariate graph visualization techniques are concerned with graphs with several or even hundreds of associated attributes. A common goal of multivariate graph visualization techniques is to allow analysts to jointly analyze topology and attributes and reason about their relationship. @cite discuss four different types of multivariate graph visualization techniques, based on node-link layouts, which we use to structure this section. We also discuss matrix-based techniques as a fifth type.
- refers to modifying the visual appearance of a node (size, color), or embedding marks in it (bar charts, line charts, etc.) Color coding is a common choice to encode a single data value or a node type; the latter is also often encoded using node shapes or icons. @cite review techniques used in systems biology for visualizing multivariate networks, many of which make use of on-node encoding using embedded charts, such as line charts, box plots, etc. On-node encoding is also widely supported by common graph visualization tools such as Cytoscape @cite and Gephi @cite . Van den Elzen and van Wijk @cite use embedded visualizations to show distributions of values aggregated in a super-node. On-node encoding supports the integration of topology and attribute-based tasks well; however, it comes with scalability trade-offs. Even for a modest number of nodes in a node-link layout, node size has to be limited; hence little space is available to encode attributes. When details about nodes are shown, as, for example, in MoireGraphs @cite , the number of nodes that can be displayed simultaneously is limited.
- works by adjusting the layout so that a direct association between the nodes edges and their attributes can be established. This is a broad category that includes placing the nodes in a scatterplot defined by two attributes as in GraphDice @cite or aggregating nodes into bar charts as in GraphTrail @cite . Another strategy is to linearize (parts of) a node-link layout so that it can be easily juxtaposed with a table visualizing node or edge attributes. Examples of this approach include Pathline @cite , where a whole network including cycles and branching is linearized and juxtaposed with an attribute visualization; enRoute @cite , which linearizes a user-chosen path; and Pathfinder @cite , which queries for paths in networks and juxtaposes those paths with attribute visualizations. All these approaches make compromises between the readability of the topology of the graph and the association of the attributes to the network.
- have both favorable and unfavorable properties compared to node-link layouts when judging topology @cite . Various attempts have been made to combine node-link layouts with matrices to find a compromise between these trade-offs. Examples are NodeTrix @cite , which embeds adjacency matrices for subgraphs of a node-link layout, and MatLink @cite , which enhances matrices with links. For attribute visualization, however, adjacency matrices are superior to node-link diagrams. For example, adjacency matrices can naturally encode edge attributes in matrix cells. Although this is mostly done with a single color value, multiple edge attributes can be visualized as nested graphs @cite . Similar to the on-node encoding in node-link diagrams, however, the small space available for a matrix cell limits how much can be encoded. For node attributes, in contrast, it is easy to juxtapose multiple attribute visualizations with the rows or columns of the matrix. This has been done, for example in Graffinity @cite and in MapTrix @cite .
- In contrast to general graphs, trees can also be visualized using implicit layouts, such as tree maps @cite , sunburst plots @cite , or icicle plots @cite . Implicit techniques can use on-node encoding, such as color-coding on the node set, but they cannot be used to visualize edge attributes, as the edges are implicit.
- The idea of tree-based graph drawing goes back at least two decades. Munzner uses a spanning tree as the structure to lay out a graph in hyperbolic space @cite and shows links that are not part of the tree on demand. @cite take a similar approach, but they also introduce duplicates to resolve some ambiguities. Similarly, Ontorama @cite uses a hyperbolic layout for a spanning tree and supplements it with a second view showing a linear tree that allows duplicate nodes.
- @cite introduce a radial layout for graphs based on spanning trees. A focus node is used as the root of a spanning tree and shown at the center, immediate neighbors are shown circling the focus nodes, neighbors once removed are shown on a second circle, etc. The edges of the spanning tree and other non-tree edges are shown in a different color. Animated transitions are used to dynamically update the focus node. MoireGraphs @cite follow the same principle but combine the radial layout with rich on-node attribute visualizations.
- The works most closely related to ours are TreePlus by @cite and the application-specific variant of TreePlus, GOTreePlus @cite . TreePlus introduces the plant a seed and watch it grow'' principle. Based on an initial, user-chosen node, analysts can grow the spanning tree by successively revealing subtrees. TreePlus shows hidden links between the tree nodes on demand using a combination of highlighting, a separate view of neighboring nodes, and explicit cross-links. evaluated TreePlus by comparing it to a traditional node-link diagram in a controlled study and found that TreePlus outperforms the node-link layout for most tasks and is preferred by most participants. For a detailed discussion of the differences of TreePlus and Juniper, refer to . Most of these techniques, including Munzner's hyperbolic tree, the radial layouts, and TreePlus, also encode node attributes, but they limit attribute visualization to on-node encoding of one or few attributes.
- Another type of technique visualizes compound graphs that have both a tree and a secondary graph structure. @cite , for example, visualize a tree structure in a compound graph as a tree map and render cross-links between the tree nodes on top of it. Holten @cite uses a compound graph as an example for his hierarchical edge bundling technique. Gou and Zhang @cite render a tree structure in a sunburst layout and supplement edges connecting different levels of the layout.
- A common strategy to explore large graphs is a bottom-up approach, where the analysis begins with a search or a query, and then more context is added as needed @cite @cite . Flavors of this approach range from explicitly revealing neighborhoods of nodes @cite @cite , to querying for paths or connectivity in a network @cite @cite , to querying based on a degree-of-interest function @cite , to associative browsing and complex queries @cite @cite . All these examples are designed to return or expand a single subgraph, in contrast to techniques such as VIGOR @cite that are used to analyze (typically structural) queries that return many different subgraphs. Although we do not contribute novel concepts to graph querying methods, we make use of many of these approaches.
- The issue of censorship on the Web has been faced from different perspectives: in the early 2000's, some information leakage was already possible to circumvent national censorship @cite @cite . In this work, we specifically consider censored texts. Work in @cite proposes a method to make textual documents resilient to censorship sharing them over a P2P network, one of the most frequently used decentralised sharing mechanism. However, when considering centralised systems, like Facebook, censorship might occur on document instances (i.e., posts and comments). Thus, strategic information may be altered - or censored- by malicious users of P2P networks, as well as by authors of single posts on social media.
- For the sake of completeness, we acknowledge the occurrence of different kinds of censorship, from DNS to router level ones @cite . Differently from veiling single terms, an entire domain might not be accessible, thus requiring different approaches to circumvent the block @cite . Work in @cite @cite @cite propose a survey on different topics related to censorship, either on detection or possible countermeasures. Also, monitoring tools exist, to trace the diffusion of the phenomenon @cite @cite . Finally, emails or other communication channels different from social media might be affected by censorship @cite .
- In relation to other approaches that incorporate rotation invariance covariance in the network design, such as harmonic networks @cite , local transformation invariance learning @cite , deep symmetry nets @cite , scattering CNNs @cite @cite , and warped convolutions @cite , the group convolution approaches @cite @cite @cite @cite @cite @cite most naturally extend the standard CNNs by simply replacing the convolution operators.
- In work by @cite , an effective template matching method was proposed using group correlations in orientation scores, which are @math images obtained from a 2D image via lifting convolutions with a specific choice of kernel @cite . The @math templates were put in a B-spline basis (allowing for exact kernel rotations) and optimized via logistic regression. Their architecture fits within our framework as a single channel G-CNN of depth 2 with a fixed lifting kernel.
- Cho, et al @cite proposed to weight links based on in-links count and partial PageRank calculation. Baeza-Yates, et al @cite indicated using historically crawled data is effective in ordering a crawling queue, as well as estimating the PageRank of unseen links. OPIC @cite utilizes the concept of cash flow and estimates the importance of a link by cash'' that is distributed equally by pages that point to it. RankMass @cite uses all available links during crawling to calculate a lower bound on PageRank to prioritize the crawling queue. Fractional PageRank @cite incorporates features such as host domain and page title in computing PageRank scores and proposes to reduce the computational cost by skipping the path towards a set of already downloaded pages.
- The recent importance of social media requires crawlers that are effective within specific sites. iRobot @cite clusters pages using both repetitive regions and URL patterns, and filters out unimportant clusters via an informativeness measure. In its crawling phase, a traversal path is generated, which is a spanning tree on the graph, to guide crawlers. However, the process of generating a traversal path requires human supervision. Importance of clusters can be estimated by hub and authority scores @cite . However, this work utilizes search logs, which is an additional external resource.
- . Many studies have introduced prior information to help address the ill-posed SR problem. Early methods explore a smoothing prior such as bicubic interpolation and Lanczos resampling @cite . Image priors such as edge features @cite @cite , statistics @cite @cite and internal patch recurrence @cite are employed to improve performance. Dong al @cite train domain specific dictionaries to better recover local structures in a sparse representation framework. Sun al @cite propose context-constrained super-resolution by learning from texturally similar training segments. Timofte al @cite investigate semantic priors by training specialized models separately for each semantic category on exemplar-based methods @cite @cite . In contrast to these studies, we explore categorical priors in the form of segmentation probability maps in a CNN framework.
- According to output representations, MVS methods can be categorized into 1) direct point cloud reconstructions @cite @cite , 2) volumetric reconstructions @cite @cite @cite @cite and 3) depth map reconstructions @cite @cite @cite @cite @cite . Point cloud based methods operate directly on 3D points, usually relying on the propagation strategy to gradually densify the reconstruction @cite @cite . As the propagation of point clouds is proceeded sequentially, these methods are difficult to be fully parallelized and usually take a long time in processing. Volumetric based methods divide the 3D space into regular grids and then estimate if each voxel is adhere to the surface. The downsides for this representation are the space discretization error and the high memory consumption. In contrast, depth map is the most flexible representation among all. It decouples the complex MVS problem into relatively small problems of per-view depth map estimation, which focuses on only one reference and a few source images at a time. Also, depth maps can be easily fused to the point cloud @cite or the volumetric reconstructions @cite . According to the recent MVS benchmarks @cite @cite , current best MVS algorithms @cite @cite are both depth map based approaches.
- Rather than using traditional handcrafted image features and matching metrics @cite , recent studies on stereo apply the deep learning technique for better pair-wise patch matching. Han al @cite first propose a deep network to match two image patches. Zbontar al @cite and Luo al @cite use the learned features for stereo matching and semi-global matching (SGM) @cite for post-processing. Beyond the pair-wise matching cost, the learning technique is also applied in cost regularization. SGMNet @cite learns to adjust the parameters used in SGM, while CNN-CRF @cite integrates the conditional random field optimization in the network for the end-to-end stereo learning. The recent state-of-the-art method is GCNet @cite , which applies 3D CNN to regularize the cost volume and regress the disparity by the soft argmin operation. It has been reported in KITTI banchmark @cite that, learning-based stereos, especially those end-to-end learning algorithms @cite @cite @cite , significantly outperform the traditional stereo approaches.
- As assistive robotics has developed, various types of control interfaces have been explored. The modalities of these interfaces range from devices that provide a binary signal (such as a sip-and-puff) to speech recognition to a brain-computer interface (BCI) @cite . Recent work in designing a HitL system that specializes in path planning under interface based constraints utilizes a sip-and-puff to allow for multiple inputs based on the state of the current interface. This was shown to be effective in assisting those with lower motor capabilities move around a room without requiring elaborate control over their wheelchair @cite .
- Other work in speech recognition in the context of assistive robotics has shown that many of the challenges facing assistive systems focus on making sure the system can understand the user and create an engaging system that performs the designated task well @cite .
- Not only is the form of input into the system important, but so is the design of the interface. Much work and care must be put into making sure the system is robust to user input and also allows the user to accurately see what is happening in the environment as a result of the system @cite . Because HitL software is so focused on getting accurate input from the user, there is not much room for ambiguity @cite .
- In order to evaluate the effectiveness of this system we used a pick and place task that was developed as a modification of the Box and Blocks Test of manual dexterity @cite . This test is used to evaluate physically handicapped individuals to develop a normative metric for adults. We believe a test such as this is appropriate for evaluating whether the system developed is achieving a useful and or meaningful task. We opted for this instead of the Action Research Arm Test (ARAT) because it requires less than 1cm of noise for detecting the objects to be manipulated in that test and the equipment required for such precise point cloud measurements is not cost effective for an ordinary user at the time of this writing.
- Prior to Theorem , it was known that for all @math -vertex graphs @math , @math is increasing in @math @cite and that @math [Lemma 1] DGMRSS2014 . Thus @math for all @math , and @math for all @math . To our knowledge, these were the best previously known bounds that apply to all graphs, and the only known bound for @math . When @math , given a fixed initial mutant @math , Mertzios and Spirakis [Theorem 4] MS2013 showed that fixation occurs with probability at least @math , and a result of Giakkoupis [Lemma 6] Gia2016 implies that fixation occurs with probability at least @math . These results both imply better bounds than @math in the case where @math is sparse or has high minimum degree.
- We have already discussed the previously best-known family of suppressors from @cite ; note that using [Lemma 6] Gia2016 , one can show that they have fixation probability @math and so are improved on by Theorem . Theorem provides the first known family of digraphs with fixation probability @math when @math .
- In , we alluded to the fact that the problem of finding the strongest possible amplifier had essentially been solved. Any directed graph has extinction probability @math @cite , which is tight up to a polylogarithmic factor @cite , and any undirected graph has extinction probability @math , which is tight up to a constant factor @cite . In fact, the results of @cite generalise to sparse graphs; any @math -edge undirected graph has extinction probability @math , which is also tight up to a constant factor. (See also Giakkoupis @cite for a lower bound of @math in the dense undirected setting, which is proved to be tight to within a factor of @math .)
- Several generalisations of the Moran process are also studied @cite @cite . Some versions of the process allow edge weights, which is equivalent to allowing multiple edges in the graph, and others determine the fitness of a vertex partly or fully by game payoffs with its neighbours. More recently, a variant has been proposed @cite in which the mutants and non-mutants interact along different graphs. In this paper, we consider the original graph Moran process.
- The occupancy grid map @cite is one of the most popular local metric map representations for mobile robots. Besides range sensors such as RaDAR and LiDAR, occupancy grid maps can also be generated from RGB-D cameras @cite , stereo vision @cite , and from fusion of multiple sensors @cite . However, the classical occupancy grid maps are without semantics, cells only have two possible states: occupied or not occupied.
- More efficient and reliable navigation can be realized if semantics of the environment are utilized. Semantic segmentation is a potential approach to provide additional semantic scene understandings. Most semantic segmentation research has been carried out on RGB images with the goal to estimate a semantic class label for each individual pixel. For this particular task, it can be noted that deep learning methods are surpassing other classical methods in terms of both accuracy and efficiency. One state-of-the-art framework is the fully convolutional network (FCN) @cite that utilizes the convolutional feature extractor from other classification networks, such as VGG @cite or ResNet @cite . Another framework named SegNet @cite , has a similar structure of auto-encoders. Further research shows that the segmentation quality can be enhanced by applying a conditional random field (CRF) as a post-processing step @cite . To integrate this in an end-to-end manner, CRFasRNN @cite is proposed to form a CRF as a recurrent neural network (RNN) that can be trained directly. Recent research has also performed semantic segmentation in an adversarial manner to produce improved result in terms of segmentation accuracy @cite .
- A few shallow hand-crafted feature representations @cite @cite have been proposed for sketch recognition. Albeit seeing some sketch-specific design, they are largely built from popular photo feature representations. The ground-breaking work of Yu al @cite , for the first time beats human performance on sketch recognition task by utilizing the discriminative power of a deep convolutional neural network. Subsequent work further exploited stroke-level temporal information by applying heuristic data augmentation @cite . Our approach jointly explores static sketch visual characteristics and dynamic temporal sketching information in a single deep model. We show that it is superior to all existing models when re-purposed for the sketch recognition task.
- Hashing is an important research topic for fast image retrieval. Conventional hashing methods @cite @cite @cite mainly utilize hand-crafted features as image representations and propose various projections and quantization strategies to learn the hashing codes. Recently, deep hashing learning has shown superiority on better preserving the semantic information when compared with shallow methods @cite @cite @cite . In the initial attempt, feature representation and hashing codes were learned in separate stages @cite , where subsequent work @cite @cite @cite suggested superior practice through joint end-to-end training. To our best knowledge, only one previous work @cite has specifically designed a deep hashing framework targeted on sketch data. They introduced a semi-heterogeneous deep architecture by incorporating cross-view similarity and a cross-category semantic loss. Despite its superior performance, sketch specific traits such as stroke ordering and drawing abstraction were not accommodated for. The dataset @cite they evaluated on is also arguable too small to truly show for the practical value of a deep hashing framework. We address these issues by working with a much larger human sketch dataset, and designing sketch-specific solutions that are crucial for million-scale retrieval.
- The problem of Offline Signature Verification is either formulated as Writer-Dependent, with one classification task defined for each user enrolled to the system, or as a Writer-Independent problem, where we consider a single problem, of comparing a questioned signature to a reference signature. In the literature, Writer-Dependent classification is most commonly used: for each user, a set of reference (genuine) signatures are used as positive samples, and a set of genuine signatures from other users (in this context called Random forgeries") are used as negative samples, and a binary classifier is trained. Alternatively, some authors propose using one-class classifiers for the Writer-Dependent formulation, using only genuine signatures from the user as positive samples (e.g. @cite ). Writer-Independent classification, on the other hand, is often used by training a binary classifier on a dissimilarity space, where the inputs are the absolute difference of two feature vectors: @math , where @math and @math are feature vectors extracted from two signatures, and we consider a binary label: @math if both signatures are from the same user, and @math otherwise @cite @cite @cite .
- The security of machine learning has received a lot of attention in different communities (e.g., @cite @cite @cite @cite @cite @cite ). Different types of attacks against learning algorithms have been designed and analyzed, including (e.g., @cite @cite @cite @cite @cite @cite @cite ), and (e.g., @cite @cite @cite ). In the attacker manipulates training data to violate system or , , to cause a denial of service or the misclassification of specific data points, respectively @cite @cite @cite @cite @cite .
- In the security community, practical poisoning attacks have been demonstrated in worm signature generation @cite @cite , spam filters @cite , network traffic analysis systems for detection of DoS attacks @cite , sentiment analysis on social networks @cite , crowdsourcing @cite , and health-care @cite . In supervised learning settings, @cite have proposed that add spurious words (features) to reduce the maliciousness score of an instance. These attacks work against conjunctive and Bayes learners for worm signature generation. @cite practically demonstrate how an attacker can inject noise in the form of suspicious flows to mislead worm signature classification. @cite present both availability and targeted poisoning attacks against the public SpamBayes spam classifier. @cite analyze the theoretical limits of poisoning attacks against signature generation algorithms by proving bounds on false positives and false negatives for certain adversarial capabilities.
- In unsupervised settings, Rubinstein al @cite examined how an attacker can systematically inject traffic to mislead a PCA anomaly detection system for DoS attacks. Kloft and Laskov @cite demonstrated on centroid anomaly detection that involve incremental contamination of systems using retraining. Theoretical online centroid anomaly detection analysis has been discussed in @cite . @cite discuss sanitization methods against time-based anomaly detectors in which multiple micro-models are built and compared over time to identify poisoned data. The assumption in their system is that the attacker only controls data generated during a limited time window.
- In the machine learning and statistics communities, earliest treatments consider the robustness of learning to noise, including the extension of the PAC model by Kearns and Li @cite , as well as work on robust statistics @cite @cite @cite @cite . In adversarial settings, robust methods for dealing with arbitrary corruptions of data have been proposed in the context of linear regression @cite , high-dimensional sparse regression @cite , logistic regression @cite , and linear regression with low rank feature matrix @cite . These methods are based on assumptions on training data such as sub-Gaussian distribution, independent features, and low-rank feature space. @cite pioneered the research of optimizing poisoning attacks for kernel-based learning algorithms such as SVM. Similar techniques were later generalized to optimize data poisoning attacks for several other important learning algorithms, such as feature selection for classification @cite , topic modeling @cite , autoregressive models @cite , collaborative filtering @cite , and simple neural network architectures @cite .
- In contrary to region-based detection methods ( , Faster R-CNN and its variants), alternative research pipeline is designing region-free detection methods. Among them, YOLO @cite @cite and SSD @cite @cite are two representative methods. YOLO @cite utilizes one single neural network to predict bounding boxes and class probabilities from the full images directly, which trains the network with a loss function in term of detection performance. Different from YOLO, SSD @cite discretizes the space of prediction of bounding boxes into a set of default boxes over several specific convolution layers. For inference, they compute the scores of each default box being to different object categories. Although region-free methods have faster training and inference speed than region-based ones, these methods discard generation of region proposals so that they often struggle with small objects and cannot filter out the negative samples belonging to the background. Furthermore, experimental results show our method can obtain higher accuracy than state-of-the-art region-free detection methods (See Sec. for more details). Note that it is indirect to incorporate our MLKP into region-free methods, where no object proposals can be represented by MLKP. This interesting problem is worth to be investigated in future.
- Recent works have shown that the integration of high-order statistics with deep CNNs can improve classification performance @cite @cite @cite @cite @cite @cite . Thereinto, the global second-order pooling methods @cite @cite @cite are plugged into deep CNNs to represent whole images, in which the sum of outer product of convolutional features is firstly computed, then element-wise power normalization @cite , matrix logarithm normalization @cite and matrix power normalization @cite are performed, respectively. Wang al. @cite embed a trainable global Gaussian distribution into deep CNNs, which exploits first-order and second-order statistics of deep convolutional features. However, all these methods generate very high dimensional orderless representations, which can not be directly adopted to object detection. The methods in @cite @cite adopt polynomial and Gaussian RBF kernel functions to approximate high-order statistics, respectively. Such methods can efficiently generate low-dimensional high-order representations. However, different from methods that are designed for whole image classification, our MLKP is location retentive and sensitive to guarantee that it can be flexibly adopted to object detection.
- In his unpublished habilitation thesis @cite , Kuznetsov developed a version of HPD which works for input categories @math that are realizable (compatibly with their @math -linear structure) as an admissible subcategory of the derived category of a variety. Due to my inadequate Russian I was not able to read @cite , but its existence served as an inspiration for this paper.
- @cite , Thomas reproved Theorem using a reinterpretation of Kuznetsov's original proof. He handles the case where @math need not be geometric, but @math is required to be a genuine variety (not noncommutative'') and he works with a special class of Lefschetz decompositions (the rectangular'' ones).
- During the preparation of this paper, two other works on HPD appeared. @cite , Jiang--Leung--Xie build on the argument from @cite to prove a generalization of Theorem , where the the pair @math is replaced by any HPD pair. We independently discovered (a more general form of) this result, which appears as an application in @cite .
- There have been many recent development on deep generative modeling, including deterministic generative models @cite @cite , Generative Adversarial Networks (GAN) @cite @cite , Variational Auto-encoders (VAE) @cite @cite , and autoregression networks @cite , to list a few. Among them, the most popular one is perhaps GANs @cite @cite .
- Due to its abundant useful applications, the face is a major focus of image generation. Antipov @cite propose a variant of GANs for face aging. Li @cite propose a method for attribute-driven and identity-preserving face generation. However, the attribute is only limited to some simple ones. TP-GAN @cite adopt a two-pathway generative network to synthesize frontal faces. Both DR-GAN and TP-GAN obtained impressive results on face frontalization, but they need to explicitly label the frontal faces.
- Prior work also explores disentangled representation learning. For example, the DC-IGN @cite uses a variational auto-encoder based method to learn the disentangled presentation. However, DC-IGN needs to fix one attribute in one batch training, which also needs explicit annotations of the attributes. Luan @cite proposed DR-GAN to learn a generative and discriminative representation, which explicitly disentangles the pose leveraging the pose annotations.
- In contrast, this paper proposes an Identity Preserving Generative Adversarial Network framework, which does not require any attribute annotations. This framework disentangles the identity and attributes representations, and then uses different recombinations of representations for identity preserving face synthesis. This disentaglement allows us to synthesize faces with identities outside what is presented in the training datasets. This addresses a serious limitation of a previous deep generative model-based identity preserving face synthesis method @cite . It simply can not generate faces of identities outside the training dataset.
- Event detection and evaluation are complex tasks, where in intrusion detection it is often referred to as threshold-based methods for triggering alarms, such as presented in articles @cite @cite . Threshold-based detection are considered as reliable and is often applied in statistics, data mining or game-theoretical evaluation approaches @cite @cite @cite @cite . Beside the evaluation techniques of data, the data itself provides a large amount of information, such as IP-addresses, protocols, timestamps, etc, that play a major role in information sharing and a lot of evaluation techniques are investigated @cite @cite @cite .
- Discrete logic is a fundamental component of human visual reasoning: we present a general neural framework for differentiable reasoning over discrete data structures, including stacks and trees. Prior work has demonstrated some success for individual data structures and settings. StackRNN @cite allows recurrent architectures to push and pop from a stack without explicit supervision. However, implicit learning only goes so far: the hardest task it was tested on is binary addition. Approaches such as recursive NN @cite and TreeRNN @cite enable explicit tree structure supervision, but only when the structure is also known at test time.
- Prior work on CLEVR is largely categorized by dynamic and static approaches. IEP and N2NMN both generalized the original neural module networks architecture and used the functional annotations in CLEVR to predict a static program which is then assembled into a tree of discrete modules and executed. IEP further demonstrated success when program annotations are available for only a few percent of questions. These are most similar to our approach; we focus largely upon comparison to IEP, which performs significantly better. RN @cite and FiLM @cite , the latter being the direct successor of CBN @cite are both static architectures which incorporate some form of implicit reasoning module in order to achieve high performance without program annotations. In contrast, our architecture uses program annotations to explicitly model the underlying question structure and jointly executes the corresponding functional representation. As a result, our architecture performs comparably on questions requiring only a sequence of filtering operations and significantly better on questions involving higher level operations such as counting and numerical comparison.
- Human behavior has been studied from a crowd perspective in or from a individual perspective in (the focus of our work). One example of microscopic model is the Social Forces by Helbing and Molnar @cite which models pedestrian behavior with attractive forces guiding them towards their goal and repulsive forces encouraging collision avoidance. Over the past decades, this method has been often revisited @cite @cite @cite @cite @cite @cite @cite @cite . Tools popular in economics have also been used such as the Discrete Choice framework by Antonini @cite . Treuille @cite use continuum dynamics, and Wang @cite , Tay @cite use Gaussian processes. Such functions have also been used to study stationary groups @cite @cite . However, all these methods use hand crafted energy potentials based on relative distances and specific rules. In contrast, over the past two years, data-driven methods based on RNNs have been used to outperform the above traditional ones.
- Recurrent Neural Networks are a rich class of dynamic models which extend feedforward networks for sequence generation in diverse domains like speech recognition @cite @cite @cite , machine translation @cite and image captioning @cite @cite @cite @cite . However, they lack high-level and spatio-temporal structure @cite . Several attempts have been made to use multiple networks to capture complex interactions @cite @cite @cite . Alahi al @cite use a social pooling layer that models nearby pedestrians. In the rest of this paper, we show that using a Multi-Layer Perceptron (MLP) followed by max pooling is computationally more efficient and works as well or better than the social pooling method from @cite . Lee @cite introduce a RNN Encoder-Decoder framework which uses variational auto-encoder (VAE) for trajectory prediction. However, they did not model human-human interactions in crowded scenes.
- Spatial relations between cameras are either explicitly mapped in 3D @cite @cite , learned by tracking known identities @cite @cite @cite , or obtained by comparing entry exit rates across pairs of cameras @cite @cite @cite . Pre-processing methods may fuse data from partially overlapping views @cite , while some systems rely on completely overlapping and unobstructed views @cite @cite @cite @cite @cite . People may be explicitly modeled on the ground @cite @cite @cite @cite or image plane @cite @cite . is also modeled, either parametrically @cite @cite or not @cite @cite @cite @cite @cite . We use time constraints to rule out unlikely inter-camera associations. Similarly to @cite we decay correlations to zero as the time distance between observations increases. Correlation decay ensures that time-distant observations are associated if there is a chain of positively-correlated observations that connect them. The idea is similar to lifted multicuts @cite , although we employ no threshold or hard constraints.
- Trying to study a practically useful algorithm, @cite proves that SGD learns a function that approximates the best function in the conjugate kernel space derived from the network architecture. Although this work provides guarantees for a wide range of deep architectures, there is no empirical evidence that the best function in the conjugate kernel space performs at the same ballpark as CNNs. The work of @cite shows guarantees on learning low-degree polynomials, which is again learnable via SVM or direct feature mapping. Other works study shallow (one-hidden-layer) networks under some significant assumptions. The works of @cite @cite study the convergence of SGD trained on linearly separable data, which could be learned with the Perceptron algorithm, and the works of @cite @cite @cite @cite assume that the data is generated from Gaussian distribution, an assumption that clearly does not hold in real-world data. The work of @cite extends the results in @cite , showing recovery of convolutional kernels without assuming Gaussian distribution, but is still limited to the regime of shallow two-layer network.
- The development of deep neural networks including deep convolutional neural networks (CNN) @cite has improved the visual recognition dramatically. Recent studies have shown that deep neural networks can learn more transferable features @cite @cite @cite , by disentangling explanatory factors of variations underlying data samples, and grouping deep features hierarchically according to their relatedness to invariant factors.
- Recent research has shown that explicitly reducing domain divergence upon the deep learning framework can further exploit domain invariant features. Three main approaches are identified among the literature. The first is statistic moment matching based approach, i.e. maximum mean discrepancy (MMD) @cite @cite @cite @cite , Central Moment Discrepancy (CMD) @cite , and second-order statistics matching @cite . The second commonly used approach is based on an adversarial loss, which encourages samples from different domains to be non-discriminative with respect to domain labels, i.e. domain adversarial nets-based adaptation methods @cite @cite @cite @cite borrowing the idea of GAN @cite . The third approach uses Batch Normalization statistics @cite @cite , which aligns the source and target distributions to a canonical one. However, all of these approaches rely on the marginal distribution matching in the feature space and thus the label spaces between domains are assumed to be identical for feasible adaptation.
- The method proposed by Ganin al @cite is related to our work. They use a single domain classifier to regularize the extracted features to be indiscriminate with respect to the different domains. However, they assume the existence of a shared feature space between domains where the distribution divergence is small. By contrast, we use two different feature extractors for respective domains to learn more domain specific features. In addition, we weight the source domain samples when learning the two domain classifiers, such that the outlier samples from the source domain will be ignored for more effective transfer, especially when the target domain only contains a subset of classes of the source domain. Another related work is @cite , which also learns two different feature extractors by unsharing the weights in the adversarial nets-based framework. However, it assumes the identical label space between domains and cannot deal with the partial domain adaptation as addressed by this paper.
- A recent report by Cao al @cite also addresses the problem of transferring from big source domain to the target domain with a subset of classes. SAN trains a separate domain classifier for each class and introduces both instance-level and class-level weights according to the class probabilities given by label predictor. There are fundamental differences between the proposed method and the methods in @cite . Firstly, their method uses a shared feature extractor for both domains. Secondly, our method only requires two domain classifiers rather than multiple domain classifiers (one per source class) which makes their method hardly scalable to a source data with a large number of classes and is computationally expensive. Lastly, our method does not require class level weight and hence be able to deal with imbalanced target data because if a class level weight is applied, the target classes with a smaller number of samples may not be able to be classified well after adaptation.
- Extensive research on automated sewing has been performed in the apparel industry focusing on incorporating sensors and robots to augment the ability of conventional sewing machines. Relevant topics include fabric tension control for robot-assisted fabric feeding @cite , sewing seam tracking using an optical sensor @cite or a camera system @cite , multi-arm robotic sewing @cite , and automatic sewing worklines @cite . These systems, however, constrained by the incorporation of conventional sewing machines, work only on a flat sewing table. Recently, innovation in 3D structured object sewing is an important topic for industrial manufacturing. KSL Keilmann (Lorsch, Germany) has developed different single-sided sewing heads, such as KSL double needle RS 530, for sewing 3D fabric-reinforced structures for aircraft parts. Different to a conventional sewing machine, which applies stitches by using synchronized parts working on both side of the fabric, the single-sided sewing head features two needles, which are able to apply interwoven chain stitches from the outer surface of a 3D object. These single-sided sewing heads, however, are designed to sew large and heavy objects.
- Current state-of-the-art studies focus on how to accurately compute the matching cost using CNNs and how to apply semi-global matching (SGM) @cite to refine the disparity map. Zbontar and LeCun @cite introduce a deep Siamese network to compute matching cost. Using a pair of @math image patches, the network is trained to learn to predict the similarity between image patches. Their method also exploits typical stereo matching procedures, including cost aggregation, SGM, and other disparity map refinements to improve matching results. Further studies improve stereo depth estimation. Luo al @cite propose a notably faster Siamese network in which the computation of matching costs is treated as a multi-label classification. Shaked and Wolf @cite propose a highway network for matching cost computation and a global disparity network for the prediction of disparity confidence scores, which facilitate the further refinement of disparity maps.
- Some studies focus on the post-processing of the disparity map. The Displets @cite method is proposed based on the fact that objects generally exhibit regular structures, and are not arbitrarily shaped. In the Displets @cite method, 3D models of vehicles are used to resolve matching ambiguities in reflective and textureless regions. Moreover, Gidaris and Komodakis @cite propose a network architecture which improves the labels by detecting incorrect labels, replacing incorrect labels with new ones, and refining the renewed labels (DRR). Gidaris and Komodakis @cite use the DRR network on disparity maps and achieve good performance without other post-processing. The SGM-Net @cite learns to predict SGM penalties instead of manually-tuned penalties for regularization.
- Recently, end-to-end networks have been developed to predict whole disparity maps without post-processing. Mayer al @cite present end-to-end networks for the estimation of disparity (DispNet) and optical flow (FlowNet). They also offer a large synthetic dataset, Scene Flow, for network training. Pang al @cite extend DispNet @cite and introduce a two-stage network called cascade residual learning (CRL). The first and second stages calculate the disparity map and its multi-scale residuals, respectively. Then the outputs of both stages are summed to form the final disparity map. Also, Kendall al @cite introduce GC-Net, an end-to-end network for cost volume regularization using 3D convolutions. The above-mentioned end-to-end approaches exploit multiscale features for disparity estimation. Both DispNet @cite and CRL @cite reuse hierarchical information, concatenating features from lower layers with those from higher layers. CRL @cite also uses hierarchical supervision to calculate disparity in multiple resolutions. GC-Net @cite applies the encoder-decoder architecture to regularize the cost volume. The main idea of these methods is to incorporate context information to reduce mismatch in ambiguous regions and thus improve depth estimation.
- In the field of semantic segmentation, aggregating context information is also essential for labeling object classes. There are two main approaches to exploiting global context information: the encoder-decoder architecture and pyramid pooling. The main idea of the encoder-decoder architecture is to integrate top-down and bottom-up information via skip connections. The fully convolutional network (FCN) @cite was first proposed to aggregate coarse-to-fine predictions to improve segmentation results. U-Net @cite , instead of aggregating coarse-to-fine predictions, aggregates coarse-to-fine features and achieves good segmentation results for biomedical images. Further studies including SharpMask @cite , RefineNet @cite , and the label refinement network @cite follow this core idea and propose more complex architectures for the merging of multiscale features. Moreover, stacked multiple encoder-decoder networks such as @cite and @cite were introduced to improve feature fusion. @cite , the encoder-decoder architecture is termed the architecture.
- Pyramid pooling was proposed based on the fact that the empirical receptive field is much smaller than the theoretical receptive field in deep networks @cite . ParseNet @cite demonstrates that global pooling with FCN enlarges the empirical receptive field to extract information at the whole-image level and thus improves semantic segmentation results. DeepLab v2 @cite proposes atrous spatial pyramid pooling (ASPP) for multiscale feature embedding, containing parallel dilated convolutions with different dilated rates. PSPNet @cite presents a pyramid pooling module to collect the effective multiscale contextual prior. Inspired by PSPNet @cite , DeepLab v3 @cite proposes a new ASPP module augmented with global pooling.
- Similar ideas of spatial pyramids have been used in context of optical flow. SPyNet @cite introduces image pyramids to estimate optical flow in a coarse-to-fine approach. PWCNet @cite improves optical flow estimation by using feature pyramids.
- Assuming a photometric linear image @math captured using a digital camera, with pixels below black level and above saturation level corrected, the simplified imaging formation under one global illumination source can be expressed as @cite : where @math is the measured image color value at spatial location @math , @math the wavelength distribution of the global light source, @math the spectral response of the color sensor, @math the surface reflectance and @math the wavelength.
- @cite @cite @cite @cite @cite @cite @cite aim at building a model that relates the captured image @math and the sought illumination @math from extensive training data. Among the best-performing state-of-the-art approaches, the CCC method discriminatively learns convolutional filters in a 2D log-chroma space @cite . This framework was subsequently accelerated using the Fast Fourier Transform on a chroma torus @cite . Chakrabarti @cite leverage the normalized luminance for illumination prediction by learning a conditional chromaticity distribution. DS-Net @cite and FC @math Net @cite are two representative methods using deep learning. The former network chooses an estimate from multiple illumination guesses using a two-branch CNN architecture, while the later addresses local estimation ambiguities of patches using a segmentation-like framework. Learning-based methods achieve great success in predicting pre-recorded ground-truth'' illumination color to a fairly high accurate level, but heavily depending on the same cameras being used in both training and testing images (see Sections and ). The Corrected-Moment method @cite can also be considered as a learning-based method as it needs to train a corrected matrix for each dataset.
- estimate illumination by making some assumptions about the local or global regularity of the illumination and reflectance of the input image. The simplest such method is @cite , that assumes that the global average of reflectance is achromatic. The generalization of this assumption by restricting it to local patches and higher-order gradients has led to some classical and recent statistics-based methods, such as White Patch @cite , General Gray World @cite , Gray Edge @cite , Shades-of-Gray @cite and LSRS @cite , among others @cite . The closest works to ours are Xiong @cite and Gray Pixel @cite . Xiong @cite finds gray surfaces based on a special LIS space, but this method is camera-dependent. The Gray Pixel method will be discussed in .
- @cite @cite @cite estimate illumination from the understanding of the physical process of image formation ( the Dichromatic Model), thus being able to model highlights and inter-reflections. Most physics-based methods estimate illumination based on intersection of multiple dichromatic lines, making them work well on toy images and images with only a few surfaces but not very reliable on natural images @cite . The latest physics-based method is @cite , which relies on the longest dichromatic line segment assuming Phong reflection model holds and an ambient light exists. Although our method is based on the Dichromatic Model, we classify our approach as since the core of the method is finding gray pixels based on some observed image statistics. We refer readers to @cite for more details about physics-based methods.
- With recent success of large-scale recognition systems @cite , the focus has now shifted to scaling these systems in terms of categories. As more realistic and practical settings are considered, the need for zero-shot recognition -- training visual classifiers without any examples -- has increased. Specifically, the problem of mapping text to visual classifiers is very interesting.
- Early work on zero-shot learning used attributes @cite @cite @cite to represent categories as vector indicating presence absence of attributes. This vector representation can then be mapped to learn visual classifiers. Instead of using manually defined attribute-class relationships, @cite @cite mined these associations from different internet sources. @cite used attributes as side-information to learn a semantic embedding which helps in zero-shot recognition. Recently, there have been approaches such as @cite which trys to match Wikipedia text to images by modeling noise in the text description.
- The second popular way to distill the knowledge is to use knowledge graph (explicit knowledge representations). Researchers have proposed several approaches on how to use knowledge graphs for object recognition @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . For example, @cite used WordNet to share the representations among different object classifiers so that objects with few training examples can borrow statistical strength from related objects. On the other hand, the knowledge graph can also be used to model the mutual exclusion among different classes. @cite applied these exclusion rules as a constraint in the loss for training object classifiers (e.g. an object will not be a dog and a cat at the same time). They have also shown zero-shot applications by adding object-attribute relations into the graph. In contrast to these methods of using graph as constraints, our approach used the graph to directly generate novel object classifiers @cite @cite @cite .
- In our work, we propose to distill information both via semantic embeddings and knowledge graphs. Specifically, given a word embedding of an unseen category and the knowledge graph that encodes explicit relationships, our approach predicts the visual classifiers of unseen categories. To model the knowledge graph, our work builds upon the Graph Convolutional Networks @cite . It was originally proposed for semi-supervised learning in language processing. We extend it to our zero-short learning problem by changing the model architecture and training loss.
- The planners discussed so far @cite @cite @cite @cite adopt an approach of motion planning followed by open-loop execution to solve the problem of manipulation in clutter. In particular, @cite adopts sampling-based planning to solve this problem. They propose reducing the search space of kinodynamic Rapidly exploring Random Trees (RRT) by planning over statically stable environment states while allowing for physical interaction in-between these states. In this paper, we use a similar kinodynamic RRT planner to generate plans to different manipulation problem instances. There are planners which also take uncertainty into account before the generation of the motion trajectory @cite @cite @cite @cite , but these planners typically rely on uncertainty reducing actions which generate a conservative sequence of actions, limiting the robot from using the complete dynamics of the domain.
- In this paper, however, we are mainly interested in real-time planning which can be used in a closed-loop system to respond dynamically to changes during execution. @cite present a learning approach for planar pushing tasks in closed-loop form. They train a neural network, which takes the visual state of the environment as input, and feeds the appropriate physical properties extracted from the scene to an analytical model of the task. Hogan and Rodriguez @cite apply a feedback control scheme that can alternate between different interaction modes to control a tool pushing a slider on a planar surface. To avoid learning a behavior that exploits the idiosyncrasies of the physics model in the simulation environment, @cite propose randomizing the physics parameters in the simulation environment during the learning phase for a pushing task. These approaches have proven capable in real world manipulation. However, they focus on manipulating a single object whereas we are interested in multi-object interaction present in cluttered spaces. @cite tackles this problem by relying on a expert human demonstrator and a DNN to control 2-DOF of a robot arm to reach a target object on a cluttered surface.
- @cite @cite @cite , methods extended from GAN are used to synthesize detailed images with more information from given materials. These models are usually easier to train while the result are more fuzzy to some extent. Chen proposed an end-to-end cascaded refinement networks in @cite to synthesize large-size reality image from semantic layouts, whose result has a high resolution and accuracy. But this method is highly dependent on training datasets. @cite , the author propose an image synthesis model based on Laplacian pyramid, which has a lower computation complexity. @cite , Context Encoders, a GAN based model, is promoted to generate more information from the surroundings, in which artist style could be applied.
- Most researches about style transfer focus on the combination of content and single style. @cite , initial content is transformed into oil style by pixel-level analysis. @cite proposed a style transfer scheme based on CNN whose results are quite appealing. The model in @cite combines markov random fields with @cite . @cite , a faster feed-forward style image synthesis network is proposed. Texture synthesis is used to transfer image style in @cite . In this paper, multiple styles are combined together to synthesize appealing results with PageRank @cite in undirected graph @cite .
- Fortunately, recent years have witnessed the success of heterogenous knowledge base embedding techniques @cite @cite @cite , which can help to learn the embeddings of very different entities to support various application scenarios such as question answering @cite and relation extraction from text @cite . We believe that learning knowledge base embeddings while preserving the structure of knowledge for reasoning is vital for knowledge-enhanced AI in recommendation systems.
- In the literature there are a few examples of learning from features extracted from images compressed by codecs. Classification of compressed hyperspectral images was studied in . Recently, @cite proposed an algorithm based on Discrete Cosine Transform (DCT) to compress the images before feeding them to a neural net for reportedly a 2 to 10 @math speed up of the training with minor image classification accuracy loss. @cite provide a critical review on document image analysis techniques directly in the compressed domain. To our knowledge, inference from compressed representations produced by image compression algorithms has not been considered before.
- One limitation of soft attention is that it only reweighs the convolutional features and therefore everything is always attended to, although not with the same relative importance (hence attention). In contrast, models only process part of the input, which is assumed to be the most important region. Because hard attention is not differentiable, it is more challenging to optimise. @cite proposes to learn hard attention from reinforcement learning. There are two crucial component in their network: The first one is a glimpse sensor, which can be used to extract a retina-like representation centred at a given location in the input; The second component is a glimpse network, it is used to process the retina-like representation extracted from the glimpse sensor, and the processed information is then fed into a recurrent neural network (RNN) which estimates the attention focus for the glimpse sensor at the next iteration.
- This article is especially concerned with active tasks, and in particular the problem of estimating steering from vision. Pugeault & Bowden @cite developed a pre-attentive model using gist @cite and random forests, while the deep network models are CNN or CNN @math LSTM based, @cite used a convolutional neural network to map images to steering command. @cite explore two different models for steering angle prediction. The first is a 3D convolutional model with residual connections and LSTM cell. The second is uses transfer learning to fine-tune a pre-trained CNN and predict steering angles for individual images.
- Much less work has been done on the application of attention mechanism in autonomous driving. In this article: i) we propose a new sparse attention model, based on the sparsemax function @cite , yields better performance; ii) we demonstrate that bagging multiple sparse attention models can provide a significant performance improvement over single models; iii) we show that the proposed architecture performs better than the state-of-the-art soft attention model, CNN, CNN+LSTM for steering angle prediction.
- Content-based image retrieval involves retrieving and ranking images given either a text or image query. The former can be used to retrieve images that incorporate some of the query requirements, while the latter can be used to retrieve images relevantly similar to the query --- here evaluation is context-specific, e.g. whether images have similar objects, similar colors, similar poses, or whether the images are exactly the same. Some implementations include Google Reverse Image Search and TinEye. Our focus is on the former approach, as we feel text-based queries allow users more freedom in specifying images to retrieve. It is not the case that a user always has a sample of the image she would like, or can sketch a faithful representation in, e.g., sketch-based image retrieval. @cite 's key contribution is as follows: the authors develop a framework for generating accurate groundings of a query scene graph (either complete or partial), and use grounding likelihood to rank images for retrieval and display.
- An integral aspect of our image retrieval pipeline is approximate query generation. We work with canonical relationship phrases of the form <subject - predicate - object> to generate query approximates. As the Visual Genome dataset maps each object to available WordNet synsets, we incorporate WordNet based semantic similarity measures. @cite proposes a domain-specific corpus-based training method to identify correct word sens and derives more accurate cosine-similarity measures between source and target words. @cite shows a simple baseline for WordNet synset similarity using vector embedding cosine similarity averages. @cite shows a sentence-based similarity measure that uses a TF-IDF analogue to compute similarities between a source word and its synset lemmas.
- Yeung al @cite proposed an interactive way to estimate an environment matte given an image containing a transparent object. Their method requires users to manually mark the foreground and background in the image, and models the refractive effect using a thin-plate-spline transformation. Their method does not produce an accurate environment matte, but instead a visually pleasing refractive effect. Our method shares the same spirit, but does not involve any human interaction.
- Although the potential of CNN on transparent object matting has not yet been explored, some existing works have adopted CNNs for solving the general image matting problem. Shen al @cite introduced a CNN for image matting of color portrait images. Cho al @cite proposed a network to predict a better alpha matte by taking the matting results of the traditional method and normalized color image as input. Xu al @cite introduced a deep learning framework that can estimate an alpha matte from an image and its trimap. However, none of these methods can be applied directly to the task of transparent object matting as object opacity alone is not sufficient to model the refractive effect.
- Solution methods proposed in the literature for graph alignment can be roughly classified into four basic categories @cite @cite : spectral methods @cite @cite @cite @cite , graph structure similarity methods @cite @cite @cite @cite @cite , tree search or tabu search methods @cite @cite @cite @cite , and integer linear programming (ILP) methods @cite @cite @cite . All of these works have scalability issues. Our algorithms leverage global graph structure and reduces the problem space and augment that with semantic information to alleviate most of the scalability issues.
- As an example of spectral methods, IsoRank @cite ---one of the earliest global alignment work in computational biology---suggests an eigenvalue problem that approximates the objective of finding the maximum common subgraph. After finding the vertex similarity matrix, IsoRank finds the alignment by solving the maximum weighted bipartite matching. IsoRank finds a 1 2-approximate matching using a greedy method, which aligns pair of vertices in the order of highest estimated similarity. IsoRank was extended to multiple networks in @cite , where pairwise similarity matrices are computed and an iterative spectral clustering is used to output set of vertices, one from each network, that aligns with each other. It is noted that it cannot handle more than five networks.
- @cite , named as Klau in our experiments, the problem of finding the mapping with the maximum score is posed as an integer quadratic program. It is solved by an integer linear programming (ILP) formulation via a sequence of max-weight matching problems. Authors use Lagrangian relaxation to solve this problem approximately in a more reasonable time. However, the ILP based solutions will not scale to larger problem sizes.
- NetAlign @cite formulates the network alignment problem as an integer quadratic programming problem to maximize the number of squares''. A near-optimal solution is obtained by finding the maximum a posteriori assignment using belief propagation heuristic and message-passing algorithms which yield near optimal results in practice. Another message passing network alignment algorithm on top of belief propagation is proposed by @cite . @cite propose to align two bipartite graphs with a fast projected gradient descent algorithm which exploits the structural properties of the graphs.
- In a more recent work, propose Final @cite to solve attributed network alignment problem. Final extends the concept of IsoRank @cite , and make it capable to benefit from attribute information of the vertices and edges to solve this problem. In addition to graph's vertex, edge and attribute sets Final adds an optional input called prior knowledge matrix (H) in which each entry gives likelihood to align two vertices. Final is one of the most recent works which solves attributed graph alignment problem and outperforms @cite @cite @cite @cite .
- Regarding (heterogeneous) network structures, it is proposed to rank the candidates within an online forum via a propagation-based approach @cite . Besides, the problem is formalized as searching for reliable users and contents for the task of community-based query answering in a co-training fashion @cite . Regarding collaborative tagging recommendation, assess the expertise of users using a graph-based ranking method similar to the HITS algorithm @cite . propose a joint optimization framework to rank candidates based on the consistency implied by the network structure @cite . Moreover, there are some other relevant studies, such as co-rank @cite where authors and their publications are ranked based on a coupled random walk algorithm; NetClus @cite simultaneously ranks and clusters strongly-typed objects with mutual enhancement in a heterogeneous information network; and RankClus @cite applies similar philosophy to classification and ranking. Nevertheless, these works are either query independent or consider the query-document relatedness based on global semantic mapping, which loses information for specific queries. Our method not only considers the network structure, but also captures query expansion for specific queries based on locally-trained embedding learning.
- The idea of query expansion regarding local document analysis has been previously studied for information retrieval @cite . Global analysis and local feedback are combined for query expansion with a new weight ranking function for query expansion. Recently, propose to perform query expansion based on locally-trained embeddings for queries with ambiguous semantic meanings @cite . In contrast, our locally-trained embedding is designed for query expansion for specific queries, which is of particular importance for the task of expert finding; while theirs is for ad-hoc retrieval task @cite . In addition, our locally-trained embeddings are learned with guidance from a concept hierarchy. The details will be discussed in .
- One of the key challenges of IoT networks and platforms is the plethora of co-existing and overlapping standards, and the need to interface across them. @cite highlight the need to operate over diverse communications technologies and network protocols as requirements for opportunistic IoT scenarios. Specifically, they examine the use of smart phones as mobile gateways to act as a bridge between communication protocols like ZigBee, Bluetooth, WiFi and 3G 4G. This abstracts the data access by the applications and user interface from the underlying technologies. Such a model is well suited for generalizing our crowd-sourced data collection using mobile apps, and offers a parallel with the sensor data management in our Pi gateways.
- Yet another dimension of large scale IoT deployments is the ability to plan the deployment ahead of time, and with limited field explorations. Here, modeling and simulation environments are useful design tools @cite . While our SmartConnect tool @cite helps with WSN design planning, more comprehensive tools exist to allow one to span sensing, networking, device management and data management design within the IoT ecosystem @cite . Large scale deployments will benefit from mapping the proposed solutions to such simulation environments to evaluate specific technologies.
- The testbed is one of the more progressive Smart City deployments, and it offers insights on traffic and human mobility from Spain @cite @cite . They offer their design requirements, and a software architecture for managing the testbed. This includes gateway and server runtimes, registry services and resource management. Authentication, Authorization and Accounting (AAA) services, and sensor, actuator and application deployment through a service interface is provided as well. They offer examples of the potential data sources and analytics, such as environment monitoring, landscape irrigation, traffic and parking management. Many of our requirements and architectural design exhibit similarities.
- The end-to-end pose regression has been adapted by Clark et.al. @cite to take advantage of the temporal smoothness between video frames to improve pose estimates. The authors stack a series of Long Short-term memory (LSTM) layers after the convolutional part of the network, which are able to integrate features from previous time steps to improve the robustness of the pose estimates. They show that this information provides large accuracy improvement over the original PoseNet method. Recurrent neural networks (RNNs) are also used in @cite where they are applied to visual odometry. The authors state that the recurrent units implicitly learn the motion dynamics of the system, eliminating the need for complex geometrical pipelines.
- The work in @cite addresses these problems by choosing not to directly regress the pose of the camera, and instead uses a CNN to extract model feature points in the form of a set of heatmaps. The peaks of the heatmaps are chosen as feature locations, and the values of the peaks indicate the location uncertainty. Given a known 3D model, the pose of the camera is then estimated through a minimization process. To generate the heatmaps the authors borrow the stacked hourglass network architecture @cite , which combines multiple encoder-decoder networks one after the other. This enables the learning of long range relationships between feature points. This network architecture was initially proposed for the task of human pose estimation which is where we find the state-of-the-art in feature point estimation.
- The architecture above loosens the tie between information and location, as it treats an IP address, FQDN or URL as services, decoupled from the location of their actuators. Hence matching supply with demand is independent from location, where the latter's awareness is limited to the TM function to create edge-to-edge paths or multicast trees. This renders the DNS service irrelevant and not required to facilitate the communication paradigm. Furthermore, the Pub Sub model is more aligned with the transport semantics of existing CDNs, which already uses a Pub Sub model in the transport overlay; particularly that of streaming CDNs @cite . Forwarding in the core is enabled through fast functions (FW), such as that proposed by @cite and its variant suitable for SDN @cite . We utilise this architecture to propose our novel, flexible CDN (fCDN) solution in the following section. We show that by using the RV for matching supply with demand, we no longer need DNS redirects or DNS extension to provide mapping. Furthermore, the network is natively supporting multicast, thereby eliminating the need for content splitters.
- * Klee's Measure Problem To compute the volume of the union of @math (not necessarily anchored) axis-aligned boxes in @math is known as Klee's measure problem. The fastest known algorithm takes time In @math -notation, we always assume @math to be a constant, and @math is to be understood as @math . @math , which can be improved to @math if all boxes are cubes @cite . By a simple reduction @cite , the same running time as on cubes can be obtained on anchored boxes, which can be improved to @math for @math @cite . These results are relevant to this paper because Klee's measure problem on anchored boxes (spanned by the points in @math ) is a special case of (by calling @math ).
- Chan @cite gave a reduction from @math -Clique to Klee's measure problem in @math dimensions. This proves NP-hardness of Klee's measure problem when @math is part of the input (and thus @math can be as large as @math ). Moreover, since @math -Clique has no @math -time algorithm under the Exponential Time Hypothesis @cite , Klee's measure problem has no @math -time algorithm under the same assumption. The same hardness results also hold for Klee's measure problem on anchored boxes, by a reduction in @cite (NP-hardness was first proven in @cite ).
- Finally, we mention that Klee's measure problem has a very efficient randomized @math -approximation algorithm in time @math with error probability @math @cite .
- * Known Results for Volume Selection As mentioned above, 2-dimensional can be solved in polynomial time; the initial @math algorithm @cite was later improved to @math @cite @cite . In higher dimensions, by enumerating all size- @math subsets and solving an instance of Klee's measure problem on anchored boxes for each one, there is an @math algorithm. For small @math , this can be improved to @math @cite . is NP-hard when @math is part of the input, since the same holds already for Klee's measure problem on anchored boxes. However, this does not explain the exponential dependence on @math for constant @math .
- Since the volume of the union of boxes is a submodular function (see, e.g., @cite ), the greedy algorithm for submodular function maximization @cite yields a @math -approximation of @math . This algorithm solves @math instances of Klee's measure problem on at most @math anchored boxes, and thus runs in time @math . Using @cite , this running time improves to @math , at the cost of decreasing the approximation ratio to @math and introducing an error probability @math . See @cite for related results in @math dimensions.
- An alternative way of defending such attacks is to train a detector, to detect and reject adversarial examples. Metzen al @cite utilize a binary classifier which takes intermediate representations as input for detection, and Lu al @cite propose to invoke an RBF-SVM operating on discrete codes from late stage ReLUs. However, it is possible to perform attacks on the joint system if an adversary has access to the parameters of such a detector. Furthermore, it is still in doubt whether the adversarial examples are intrinsically different from the benign ones @cite .
- Relation extraction (RE) was researched originally as an sub-field of information extraction. The major research methods in the traditional RE has the knowledge of a (small) pre-defined relation set, then given a text sequence and two target entities, the goal of these methods is to choose a relation or none which means if this relation or no relation holds between the two target entities. Thus from another perspective, RE methods are usually described as a . Most of these RE methods need a step to manually pick large amount of features @cite @cite @cite . Due to recent machine learning and especially deep learning advances, many recent proposed RE approaches begin to explore the benefits of deep learning instead of using hand-crafted features. The main benefits ranging from pre-trained word embeddings @cite @cite to deep neural networks like convolutional neural networks (CNN) and long-short term memories (LSTMs) @cite @cite @cite and attention models @cite @cite which is shown to be key for a lot of other NLP tasks, such as machine translation, named entity recognition, reading comprehension, etc.
- One strong assumption mentioned above in the most RE methods is that a fixed (i.e., closed) set of relation types is given as an prior knowledge, thus no zero-shot learning capability (i.e. detecting new relations that did not occur during training) is required. Another commonality among these RE methods is that the relation set is usually not large. Here are some examples. The widely used ACE2005 has 11 32 coarse fine-grained relations; SemEval2010 Task8 has 19 relations; TAC-KBP2015 has 74 relations although it considers open-domain Wikipedia relations. Compared to that, KBQA usually has thousands of relations. Thus most RE approaches may not work well by directly being adapted to large number of relations or unseen relations. The relation embeddings in a low-rank tensor method were used @cite . However it is still using supervised way to train their relation embeddings and relation set used in the experiments is still not large.
- Another significant difference between relation detection in KBQA and general RE is that general RE research works on the condition that the two argument entities are both available. Thus it usually can learn from features @cite @cite or attention mechanisms @cite based on the entity information (e.g. entity types or entity embeddings). In contrast relation detection for KBQA mostly does not have this information: (1) one question usually contains single argument (the topic entity) and (2) one KB entity could have multiple types (type vocabulary size larger than 1,500). This makes KB entity typing itself a difficult problem so no previous used entity information in the relation detection model. Such entity information has been used in some KBQA systems as features for the final answer re-rankers.
- The skeleton for @math dimensional pair-of-pants @math has been studied by Nadler @cite , where a higher dimensional analog of Figure is constructued. A @math -symmetric skeleton for @math is constructed by Gammage-Nadler @cite , where @math is the symmetric group. With the technique of tropical phase variety of Kerr-Zharkov @cite , we hope to find other Lagrangian skeleta @math for @math , where @math indicating the number of dominant terms in the defining equation of @math .
- In Gammage-Shende @cite , as one ingredient in proving HMS for the toric boundary of a toric variety, they constructed the Liouville skeleton for the same hypersurface as considered here. However, their results [Theorem 3.4.2] GS depends on the following hypothesis that, there exists some tropicalization function @math and some identification @math , such that the tropical amoeba polytope @math contains @math as an interior point, and @math restricts to each face @math of @math has a minimum in the interior of @math . This hypothesis is true in two-dimension, and can be verified in certain examples, e.g. mirror to weighted projective spaces. But in general, one does not know if it is always true, it would be interesting to find a proof or construct a counter-example. We thank Gammage and Shende for this clarification. Our approach here does not rely on this hypothesis, which is equivalent to " @math is adapted to the tropical amoeba polytope @math " for some choice of @math . We avoid this by considering a more flexible choice of Kahler potentials on than @math , and our approach works for any choice of @math compatible with @math .
- Traditional approaches for detecting sarcasm in text have considered rule-based techniques @cite , lexical and pragmatic features @cite , stylistic features @cite , situational disparity @cite , incongruity @cite , or user-provided annotations such as hashtags @cite .
- Resources in this domain are collected using Twitter as a primary data source and are annotated using two main strategies: manual annotation @cite @cite and distant supervision through hashtags @cite @cite . Other research leverages context to acquire shared knowledge between the speaker and the audience @cite @cite . A variety of contextual features have been explored, including speaker's background and behavior in online platforms @cite , embeddings of expressed sentiment and speaker's personality traits @cite , learning of user-specific representations @cite @cite , user-community features @cite , as well as stylistic and discourse features @cite . In our dataset, we capitalize on the conversational format and provide context by including preceding utterances along with speaker identities. To the best of our knowledge, there is no prior work which deals with the task of sarcasm detection in conversation.
- Sarcasm detection in speech has mainly focused on the identification of prosodic cues in the form of acoustic patterns that are related to sarcastic behavior. Studied features include mean amplitude, amplitude range, speech rate, harmonics-to-noise ratio, and others @cite . presented one of the initial approaches to this problem that studied the vocal tonalities of sarcastic speech. They found slower speaking rates and greater intensity as probable markers for sarcasm. Later, studied prosodic and spectral features of sound --- both in and out of context --- to determine sarcasm. In general, prosodic features such as intonation and stress are considered important indicators of sarcasm @cite @cite . We take motivation from this previous research and include similar speech parameters as features in our dataset and baseline experiments.
- Neural networks have been widely applied in natural language modeling and generation @cite @cite for both encoding and decoding. Among different neural architectures, the most popular models are recurrent neural networks [RNNs;][] mikolov2010recurrent , long short-term memory networks [LSTMs;][] hochreiter1997long , and convolutional neural networks [CNNs;][] bai2018empirical,dauphin2017language .
- Many modifications of network structures have been made based on these architectures. LSTMs with self-attention can improve the performance of language modeling @cite @cite . As an extension of simple self-attention, transformers @cite apply multi-head self-attention and have achieved competitive performance compared with recurrent neural language models. A current state-of-the-art model, Transformer-XL @cite , applied both a recurrent architecture and a multi-head attention mechanism. To improve the quality of input word embeddings, character-level information is also considered @cite . It has also been shown that context encoders can learn syntactic information @cite .
- However, instead of introducing architectural changes, for example a self-attention mechanism or character-level information, previous studies have shown that careful hyper-parameter tuning and regularization techniques on standard LSTM language models can obtain significant improvements @cite @cite . Similarly, applying more careful dropout strategies can also improve the language models @cite @cite . LSTM language models can be improved with these approaches because LSTMs suffer from serious over-fitting problems.
- Recently, researchers have also attempted to improve language models at the decoding phase. showed that reusing the input word embeddings in the decoder can reduce the perplexity of language models. showed the low-rank issue in factorizing the context-to-word mutual information matrix and proposed a multi-head softmax decoder to solve the problem. Instead of predicting the next word by using only similarities between contexts and words, the neural cache model @cite can significantly improve language modeling by considering the global word distributions conditioned on the same contexts in other parts of the corpus.
- To learn the grammar and syntax in natural languages, proposed the recurrent neural network grammar (RNNG) that models language incorporating a transition parsing model. Syntax annotations are required in this model. To utilize the constituent structures in language modeling without syntax annotation, parse-read-predict networks [PRPNs;][] shen2017neural calculate syntactic distances among words and computes self-attentions. Syntactic distances have been proved effective in constituent parsing tasks @cite . In this work, we learn phrase segmentation with a model based on this method and our model does not require syntax annotation.
- Estimating the height of objects in images is closely related to 3-D location estimation. As is described in Section , height estimations can be combined with camera parameters to establish a projection matrix from 3-D world coordinates to the 2-D image plane. Vester @cite provides an overview and analysis of common techniques for height estimation. The first algorithm presented utilizes two user-defined parallel planes to determine an object's height. The second algorithm presented utilizes knowledge of the camera's position and orientation relative to the ground plane as well as a known object height to perform a back projection to 3-D coordinates. Although the formulation slightly differs, this process is most similar to the process used in this paper, with the caveat that this paper estimates the camera's position and orientation and object heights. The final algorithm presented involves setting up 5 reference objects with known heights, one at each corner of the picture and one in the center of the picture. These heights are then used to calculate the height of an unknown object.
- Limited work has also been done for 3-D location estimation for pedestrians from 2-D images. @cite present a method that relies on two cameras for stereo refinement of bounding boxes before estimation.
- Camera pose estimation for a static camera has been thoroughly researched @cite @cite @cite @cite @cite @cite @cite @cite @cite . Abbas and Zisserman developed the Birds Eye View Network @cite for estimation of focal length, camera pitch, and camera roll using image data without specific features. Other methods for featureless estimation of camera pose exist, but do not have code publicly available @cite @cite . There are also featureless convolutional neural networks that focus only on estimating the horizon line @cite @cite or focal length @cite of the camera. Feature-based methods for camera pose estimation have also been created. Methods using pedestrian data as features to predict camera pose exist @cite @cite @cite , but do not have code publicly available. The aforementioned paper on 3-D location estimation for vehicles @cite also predicts camera pose from vehicle data, but requires the previously mentioned assumptions.
- In addition to the use of speech for communicating and coordinating their next actions, humans rely extensively on non-verbal cues for action and movement prediction @cite . Situations where fast cooperation is essential, for example cooperative assembly, require the understanding of subtle non-verbal cues @cite about the human intention and future action. In these scenarios, it is not enough to merely recognize the current action. Instead, it is fundamental to predict actions and anticipate the intent in order to guarantee seamless cooperation @cite .
- Past works have focused on either gaze @cite @cite @cite or body pose @cite cues and their relation to action recognition and prediction. Both are important in understanding human behaviour and give information about the human's action goal. Research on non-verbal cues in human-robot cooperation has a long history, including the bulk of work on mirror neurons @cite and its computational and robotic models and implementations @cite . Relevant work include Admoni @cite use of human gaze as a means of estimating the human intent, modelling the relation between the gaze and the action goal by their relative distance. Huang @cite quantified the importance of gaze features, successfully demonstrating the importance of gaze by proactively planning actions according to the human intent.
- At a higher level of abstraction, models can predict actions in a discrete space @cite @cite where the actions are symbolic in nature and can represent underlying movement patterns, e.g. press-button'' or grab-object''. On a lower level of abstraction, movement can be directly anticipated in a continuous space @cite , e.g. human walking trajectories.
- Predicting in continuous space has been addressed in the context of body pose and human trajectory prediction. Relevant work include the use of Recurrent Neural Networks by Martinez @cite as a means of predicting coherent future joint trajectories.
- The dual problem is action prediction in discrete outcome space. Relevant work include a Conditional Random Field based approach by Koppula @cite to capture temporal dependencies and Saponaro's Hidden Markov Model based approach @cite . Recently, Recurrent Neural Networks, without limiting Markovian assumptions, have shown excellent results [16], [17], [20]. Relevant work include, the structural RNN as a means of encoding past contextual information and predicting a fixed number of steps in the future by Jain @cite . While the field has had a rapid evolution in the last couple of years, there are two shortcomings in the literature this paper addresses.
- The first is concerned with predicting a fixed versus a variable number of steps into the future. While models like @cite have a remarkable ability to condense contextual past information, their scope is limited to fixed step ahead prediction length. This paper aims at extending discriminative recurrent models in a classification setting with variable length action sequence prediction.
- The second shortcoming is related to the single future action sequence versus multiple future action sequences. While models like the one introduced in @cite are able to effectively use recurrent models to predict a variable number of steps into the future, their scope is limited to a regression setting, where sampling multiple future action sequences is a non-trivial problem. This paper explores a multiple future action sequence predictor in the classification setting.
- One way to restore efficiency is by means of centralized pricing. @cite and Dafermos and Sparrow @cite showed that marginal cost pricing induces an equilibrium flow that is optimal. Despite the effectiveness of marginal cost tolls, there are two drawbacks. First, potentially each edge in the network is tolled; an issue that is considered by @cite and @cite . Second, the imposed tolls can be arbitrary large; an issue that is considered by @cite , @cite , @cite and Kleer and Sch " a fer @cite . @cite show that optimal tolls also exist when users are heterogeneous with respect to the tradeoff between time and money. Later this result was extended to general networks by @cite , Karakostas and Kolliopoulos @cite and Yang and Huang @cite .
- Instead of improving efficiency, tolls can also be used to minimize or maximize the profit of one or multiple leaders. The model that computes tolls that induce the optimal flow at minimal profit was analyzed by Dial @cite @cite . The model with one profit maximizing leader and no congestion effects, was first analyzed by Labb ' e et al @cite and later by @cite .
- Acemoglu and Ozdaglar @cite introduce a model of price competition between link owners in a parallel network that is very similar to ours. The main difference is that in their model users have a reservation value for travel. This implies that if links become too expensive users choose not to travel, whereas in our model users always travel through the network. Their main result is a (tight) bound on the inefficiency of equilibria. Later several generalizations of the model were introduced. The follow-up work of Acemoglu and Ozdaglar @cite allow for a slightly more general topology, namely parallel paths with multiple links, and show that equilibria can be arbitrarily inefficient. @cite consider the model with elastic traffic demand. Their bounds on inefficiency, however, are not tight, which is improved upon by Ozdaglar @cite .
- Recently, following our question on competition regulation of price competition between edge owners, @cite consider the setting in which a central authority is allowed to put different toll caps on different edges of the network. Their main result is that for all network topologies there are toll caps so that firms are willing to put their toll equal to the cap, and the induced equilibrium flow is the optimal flow. An example of such caps are marginal tolls as introduced by @cite . Notice that the assumption of individual toll caps is important. In practice, however, toll cap discrimination is often not allowed, which motivates our work on uniform toll caps.
- The model of @cite studies competition in both tolls and capacities and finds that tolls are higher, but capacities are lower than socially desired. Other recent models of Bertrand competition in a network setting that use different ways of modelling congestion effects are Anshelevich and Sekar @cite , Chawla and Niu @cite , Chawla and Roughgarden @cite , Papadimitriou and Valiant @cite and @cite .
- It is known that removing the redundant model parameters reduces the computational complexity of networks @cite @cite @cite @cite @cite . At the very beginning, Hanson & Pratt @cite applied the weight decay method to prune the network, then Optimal Brain Damage (OBD) @cite and Optimal Brain Surgeon (OBS) @cite pruned the parameters using the Hessian of the loss function. Recently, Han @cite @cite showed that they could even reduce the model parameters by an order of magnitude in deep models while maintaining the performance. They devised an efficient inference engine @cite to speed up the models. Instead of pruning model weights, our RRM framework focuses on factorizing the input at each layer, then further speeds up the model based on the pruning methods.
- Reducing the filter redundancy in convolution layers is an effective method to simplify the CNN models @cite @cite @cite . Luo al @cite pruned filters and set the output feature maps as the optimization objective to minimize the loss of information. Howard al @cite developed MobileNet which applied depth-wise separable convolution to decompose a standard convolution operation and showed an effectiveness. He al @cite proposed an iterative algorithm to jointly learn additional filters for filter selection and scalar masks for each output channel. They achieved 13 @math speedup on AlexNet.
- It is most related to our method. Obviously, sparsity can significantly accelerate the convolutional networks both in training and testing @cite @cite @cite @cite . There are many previous works showing that they can save the energy @cite @cite and accelerate the convolution @cite @cite @cite by skipping the zeros or elements close to zero in the sparse input. Albericio @cite proposed an efficient convolution accelerator utilizing the sparsity of inputs, while Shi & Chu @cite sped up the convolution on CPUs by eliminating the zero values in the output of ReLUs. Graham & Maaten @cite @cite introduced a sparse convolution that eliminated the computation of values in some inactive output positions by recognizing the input cells in the ground state. Recently, Han @cite devised an efficient inference engine (EIE) that can exploit the dynamic sparsity of the input feature maps to accelerate the inference. Our RRM integrates EIE as a step to further optimize the model weight.
- Our Recurrent Residual Module works in a recurrent manner. The most similar architecture to ours is the Predictive-Corrective Networks @cite , which derives a series of recurrent neural networks to make prediction about feature and then correct them with some bottom-up observations. The key difference, also the most innovative point of our model, is that we utilize the recurrent framework to accelerate CNN models using sparsity and Efficient Inference Engine, which is much more efficient than the Predictive-Corrective Networks @cite . Besides our method is a generic framework that could be plugged in a variety of CNN models without retraining to speed up the forward pass.
- There are two demanding goals in medical image synthesis. The first is synthesizing realistic cross-modality images @cite @cite , and second is to use synthetic data from other modalities with sufficient labeled data to help classification tasks (e.g. domain adaption @cite ).
- In computer vision, recent image-to-image translation is formulated as a pixel-to-pixel mapping using encoder-decoder CNNs @cite @cite @cite @cite @cite @cite @cite . Several studies have explored cross-modality translation for medical images, using sparse coding @cite @cite , GANs @cite @cite , CNN @cite , etc. GANs have attracted wide interests in helping addressing such tasks to generate high-quality, less blurry results @cite @cite @cite @cite . More recent studies apply pixel-to-pixel GANs for brain MRI to CT image translation @cite @cite and retinal vessel annotation to image translation @cite . However, these methods presume targeting images have paired cross-domain data. Learning from unpaired cross-domain data is an attractive yet not well explored problem @cite @cite .
- Synthesizing medical data to overcome insufficient labeled data attracted wide interests recently @cite @cite @cite . Due to the diversity of medical modalities, learning an unsupervised translation between modalities is a promising direction @cite . @cite demonstrates the benefits on brain (MRI and CT) images, by using synthetic data as augmented training data to help lesion segmentation.
- Apart from synthesizing data, several studies @cite @cite @cite @cite use adversarial learning as an extra supervision on the segmentation or detection networks. The adversarial loss plays a role of constraining the prediction to be close to the distribution of groundtruth. However, such strategy is a refinement process, so it is less likely to remedy the cost of data insufficiency.
- Task oriented methods are also proposed. @cite propose a system called T-Finder, which recommends to taxi drivers places where as many potential customers exist as possible, and to end users places where taxis are expected to be find. To this end, they estimate taxi locations based on taxi driving trajectories and segments the trajectories as pre-processing. @cite proposes TRAOD, an algorithm for finding outliers in trajectories based on segmentation by using the Minimum Description Length (MDL) principle. estimates Transportation Mode @cite @cite @cite such as walk, car, bus, and bike used for semantic segmentation in terms of a mode of transportation.
- Several PDDL extensions such as PDDL2.1 @cite and PDDL+ @cite support planning with numeric variables that evolve as a function of time. Most planners are restricted to problems with linear or polynomial dynamics governing these variables @cite @cite @cite ; however, some planners can handle non-polynomial dynamics by discretizing time @cite @cite . While it may be technically possible to analytically model, for example, collision constraints among three-dimensional meshes using PDDL+, the resulting encoding would be enormous, far exceeding the demonstrated capabilities of current numeric planners.
- Semantic attachments @cite @cite , functions computed by an external module, are an existing way of integrating blackbox procedures and PDDL planners. Condition-checker modules are used to test operator preconditions, and effect-applicator modules modify numeric state variables. Operators must be parameterized by variables with finite types, restricting the technique to domains with finite branching factors. Semantic attachments can only produce a single tuple of numeric outputs. Planning with semantic attachments requires modifying existing PDDL planners to evaluate the attachments during the search. This also results in many unneeded procedure calls, resulting in poor planner performance when the attachments are expensive. Planning Modulo Theories (PMT) @cite generalizes semantic attachments by supporting the composition of several modules through custom first-order theories.
- Our work builds upon previous research on sleep condition, fatigue studies, and computer vision. @cite have found that the sleep-deprived fatigue rate is correlated with eight facial cues, and our study is based on the assumption that sleep-deprived fatigue is indeed mainly reflected by those eight facial cues, and the fatigue rate can somehow imply sleep conditions. In terms of computer vision research, our work is related to face detection @cite , gender, race and age identification @cite , facial landmarks location @cite , and face grouping @cite .
- In this work, we present Enhanced PeerHunter, a network-level flow-based system that relies on community behavior analysis to detect P2P botnets. We compared Enhanced PeerHunter with PeerHunter @cite and Zhang @cite on a more challenging and comprehensive experimental datasets, and showed that our system outperforms both systems in terms of detection rate, false positives and the performance under the proposed mimicking legitimate P2P application attacks.
- The concept of transparent OpenGL interception popularized by WireGL and Chromium @cite has received little attention since 2009. While some commercial implementations such as TechViz and MechDyne Conduit continue to exist, on the research side only ClusterGL @cite has been presented. ClusterGL employs the same approach as Chromium , but delivers a significantly faster implementation of transparent OpenGL interception and distribution for parallel rendering. CGLX @cite tries to bring parallel execution transparently to OpenGL applications, by emulating the GLUT API and intercepting certain OpenGL calls. In contrast to frameworks like Chromium and ClusterGL which distribute OpenGL calls, CGLX follows the distributed application approach. This works transparently for trivial applications, but quickly requires the application developer to address the complexities of a distributed application, when mutable application state needs to be synchronized across processes. For realistic applications, writing parallel applications remains the only viable approach for scalable parallel rendering, as shown by the success of Paraview , Visit and various Equalizer -based applications.
- Equalizer itself has received significant attention within the research community. Various algorithms to improve the parallel rendering performance have been proposed: compression and region of interest during compositing @cite , load-balancing resources for multi-display installations @cite , asynchronous compositing and NUMA optimizations @cite , as well as work queueing @cite . Additionally, complex large scale and out-of-core multiresolution rendering approaches have been parallelized and implemented with Equalizer @cite @cite , demonstrating the feasibility of the framework to be used with complex rendering algorithms and 3D model representations.
- Furthermore, various applications and frameworks have used Equalizer for new research in visualization. On the application side, RTT Deltagen , Bino , Livre and RTNeuron @cite are the most mature examples and are presented in sApplications . On the framework side, Omegalib @cite , a framework used in the Cave2, made significant progress in integrating 2D collaborative workspaces like Sage 2 with 3D immersive content. Lambers et.al. developed a framework for visualizing remote sensing data @cite on large displays and immersive installations.
- Generative adversarial networks. Since the GAN framework with its theoretical foundation is proposed @cite , it draws significant attention with several improvements in implementation @cite @cite @cite @cite @cite and applciations including image generation @cite , super-resolution @cite @cite , optical flow @cite , object detection @cite , domain adaptation @cite @cite @cite and semantic segmentation @cite @cite . The work closest in scope to ours is the one proposed by @cite , where the adversarial network is used to help the training process for semantic segmentation. However, this method does not achieve substantial improvement over the baseline scheme and does not tackle the semi-supervised setting. On the other hand, Souly al @cite propose to generate adversarial examples using GAN for semi-supervised semantic segmentation. However, these generated examples may not be sufficiently close to real images to help the segmentation network since view synthesis from dense labels is still challenging.
- When specialized to stochastic mean payoff games with perfect information, our bounds should be compared with the one of Boros, Elbassioni, Gurvich, and Makino @cite . The authors of @cite generalize the pumping'' algorithm, developed for deterministic games by Gurvich, Karzanov, and Khachiyan @cite , to the case of stochastic games. The resulting algorithm is also pseudopolynomial if the number of random positions is fixed. The algorithm of Ibsen-Jensen and Miltersen @cite yields a stronger bound in the case of simple stochastic games, still assuming that the number of random positions is fixed.
- The relatively recent development of DCNNs trained on large publicly available datasets represented an inflection point in image understanding research. Compared to the period when models based on hand-engineered features (e.g. HOG @cite , SIFT @cite ) were the norm, currently the state-of-the-art in object recognition tasks is being improved at a dramatically faster pace. Between 2011 and 2015, the top- @math classification error on the ImageNet dataset was reduced from @math 1 32 @math 1 8 @math 28$ state-of-the-art algorithms. One of the most widely-used method is the Simple Linear Iterative Clustering (SLIC) algorithm @cite , a -based method that groups pixels into clusters according to their proximity in both spatial and color spaces. Semantic segmentation models using superpixels traditionally employ them as a pre-processing step. The image is first divided into superpixels, followed by a prediction step in which each element is individually evaluated using engineered hierarchical features @cite or DCNNs @cite @cite .
- In addition to the generation of region proposals or pre-segmentation elements, local-appearance information has been also employed in post-processing steps to improve segmentations obtained with deep CNN models. One such post-processing approach is to model the problem as a CRF defined over neighboring pixels or small patches, which however might lead to excessive smoothing of boundaries due to the lack of long-range connections @cite . Kr "a henb "uhl et al in @cite introduces an efficient algorithm for fully connected CRFs containing pairwise potentials that associate all pairs of pixels in a image. This algorithm is successfully exploited by the Deeplab model @cite to produce fine-grained segmentations. However, this refinement model contains parameters that have to be optimized in a supervised manner when applied to different datasets.
- SLAM is a process in which an agent needs to localize itself in an unknown environment and build a map of this environment at the same time, with uncertainties in both its motions and observations. SLAM has evolved from filter-based to graph-based (optimization-based) approaches. Some EKF-based systems have demonstrated state-of-the-art performance, such as the Multi-State Constraint Kalman Filter @cite , the VIN @cite , and the system of @cite . Those methods, even though efficient, heavily depend on linearization and Gaussian assumptions, and thus under-perform their optimization-based counterparts, such as OK-VIS @cite , ORB-SLAM @cite , and LSD-SLAM @cite .
- Graph-based SLAM can be categorized either as feature-based or direct methods depending on the type of front-end. Feature-based methods rely on local features (e.g. SIFT, SURF, FAST, ORB, etc.) for pose estimation. For example, ORB-SLAM @cite performs data association and camera relocalization with ORB features and DBoW2 @cite . RANSAC @cite is commonly used for geometric verification and outlier rejection, and there are also prioritized feature matching approaches @cite . However, hand-engineered feature detector and descriptors are not robust to motion blur, illumination changes, or strong viewpoint changes, any of which can cause localization to fail.
- To avoid some of the aforementioned drawbacks of feature-based approaches, direct methods, such as LSD-SLAM @cite , utilize extensive photometric information from the images to determine the pose, by minimizing the photometric error between corresponding pixels. This approach is in contrast to feature-based methods, which minimize the reprojection error. However, such methods are usually not applicable to wide baseline settings @cite during large viewpoint changes. Recent work in @cite @cite combines feature and direct methods by minimizing the photometric error of features lying on intensity corners and edges. Some methods focus on dense recontruction of the scene, for instance @cite builds dense globally consistent surfel-based maps of room scale environments explored using an RGB-D camera, without pose graph optimisation, while KinectFusion @cite obtains depth measurements directly using active sensors and fuse them over time to recover high-quality surface maps. These approaches still suffer from strict calibration and synchronization requirements, and the data association modules require extensive parameter tuning in order to work correctly for a given scenario.
- Recently, there has been an increasing interest in combining navigation and plannning in an end-to-end deep reinforcement learning (DRL) framework. The efforts to date can be divided into two categories depending on the presence of external memory in the architecture or not. Target-driven visual navigation takes a visual observation and an image of the target @cite or range findings @cite as input, and plans goal seeking actions in a 3D indoor simulated environment as the output.
- In simulated environments, @cite uses stacked LSTM in a goal-driven RL problem with auxilary tasks of depth prediction and loop-closure classification, while @cite added successor features to ease transfer from previously mastered navigation tasks to new ones. Work in @cite augmented DRL with Faster-RCNN for object detection and SLAM (ORB-SLAM2) for pose estimation; observing images and depth from VizDoom, they built semantic maps with 3D reconstruction and bounding boxes as input to a RL policy.
- To deal with the limited memory of standard recurrent architures (such as LSTM) more structured external memories have been developed to take the spatial relations of memories into account. @cite assumes known ego-motion and constructs a metric egocentric multi-scale belief map (top-down-view latent representation of free space) of the world with a 2D spatial memory, upon which RL plans a sequence of actions towards goals in the environment with a value iteration network. Neural Map in @cite is a writable structured 2D external memory map for an agent to learn to navigate within 2D and 3D maze environments. These works all assume precise egomotion and thus perfect localization, a prerequisite that can rarely be met in real-world scenarios. Relaxing this assumption and resembling traditional occupancy grid SLAM, Neural SLAM @cite uses an occupancy-grid-like memory map, assuming only an initial pose is provided, and updates the pose beliefs and grid map using end-to-end DRL.
- Recently Skowron et al. @cite characterized the class of committee scoring rules using the axioms of consistency, symmetry, continuity, and weak efficiency. Our paper can be seen as complementary to theirs: They study committee scoring rules as opposed to all the other multiwinner rules, whereas we focus on the internal structure of the class.
- @cite @cite studied a class of approval-based rules that is very similar to the class of committee scoring rules (the class was first introduced by Thiele @cite in the 19th century, but was forgotten for some time; some of Thiele's rules were recalled by Kilgour @cite and then by ). Lackner and Skowron @cite studied axiomatic properties of these rules and highlighted their axiomatic similarity to committee scoring rules. Recently, monotonicity notions similar to those studied in this paper were also considered in the context of approval-based multiwinner rules @cite @cite . For more general discussions of the properties of approval-based rules we point the reader to the work of Kilgour and Marshall @cite .
- Proportional Approval Voting (PAV) received a bit less attention than the Chamberlin--Courant rule, but due to the work of @cite on justified representation, it is now being studied with increasing interest (briefly put, @. have shown that PAV is remarkably good at providing committees that represent the voters proportionally, as also confirmed by @cite ; see the work of @cite for another rule with similar properties). The rule was shown to be @math -hard to compute @cite @cite , but the standard greedy @math -approximation algorithm works for it. Very recently, @cite have shown a different, apparently much more powerful algorithm. The rule was also considered in the context of restricted domains @cite . FPT approximation schemes for PAV and other OWA-based rules were provided by Skowron @cite .
- Naturally, there exist many interesting multiwinner rules beyond the class of committee scoring rules. These include, for example, Single Transferable Vote (see, e.g., the work of Tideman and Richardson @cite ), a number of rules based on the Condorcet principle @cite @cite @cite @cite @cite @cite @cite @cite @cite , Monroe's rule @cite , and different variants of the rule invented by Phragm 'en @cite @cite @cite @cite @cite . For an overview of electoral systems used to select committees of representatives in practice, we refer the reader to the book of Lijphart and Grofman @cite .
- Other likelihood-free approaches have emerged from the machine learning community and have been applied to population genetics, such as support vector machines (SVMs) @cite @cite , single-layer neural networks @cite , and deep learning @cite . The connection between likelihood-free Bayesian inference and neural networks has also been studied previously by and . An attractive property of these methods is that, unlike ABC, they can be applied to multiple datasets without repeating the training process, which is commonly referred to as amortized inference. However, current practice in population genetics collapses the data to a set of summary statistics before passing it through the machine learning models. Therefore, the performance still rests on the ability to laboriously hand-engineer informative statistics, and must be repeated from scratch for each new problem setting.
- The inferential accuracy and scalability of these methods can be improved by exploiting symmetries in the input data. Permutation-invariant models have been previously studied in machine learning for SVMs @cite and, recently, gained a surge of interest in the deep learning literature. Recent work on designing architectures for exchangeable data include , , and , which exploit parameter sharing to encode invariances. To our knowledge, no prior work has been done on learning feature representations for exchangeable population genetic data.
- There has also been some recent work on the online circle packing. However, this work on online circle packing focuses on minimising the number of square bins used to pack an online sequence of circles. @cite provides an asymptotic competitive ratio of @math , and gives a lower bound of @math for the problem. The upper bound was improved to @math in @cite . We have not found existing work that takes into account the possibility of reallocations for online circle packing.
- Other groups focus on using sets of objects to aid object detection. @cite discovers groups of objects of arbitrary size, model these groups using deformable parts models, and directly detects these groups in images. @cite constructs classifiers that operate over sets of objects using object-object and object-scene relations to re-score and remove noisy detections.
- A simpler task is learning differences of undirected graphical models. Let @math and @math denote the precision matrices corresponding to @math and @math . The support of @math consists of the edges in the undirected graph (UG) models corresponding to @math . We define the difference-UG ( D-UG ) by @math , with @math if and only if @math for @math . Two recent methods that directly learn the difference of two UG models are KLIEP @cite and DPM @cite ; for a review and comparison of these methods see @cite . These methods can be used as a first step towards estimating the D-DAG @math : under genericity assumptions, the formulae for @math in imply that if @math then @math . Hence, the skeleton of @math is a subgraph of @math , i.e., @math . In the following section we present our algorithm showing how to obtain @math and determine some of the edge directions in @math . We end this section with a piece of notation needed for introducing our algorithm; we define the to be @math .
- Driven by widespread efforts to automate patient monitoring, there has been a recent surge in works applying machine learning to the vast amounts of data generated in ICUs. One notable driver is the MIMIC @cite dataset that has made ICU data accessible to a large number of researchers. Related works have, for example, explored the use of ICU data for tasks such as mortality modelling @cite , illness assessment and forecasting @cite , diagnostic support @cite , patient state prediction @cite and learning weaning policies for mechanical ventilation @cite . Applying machine-learning approaches to clinical and physiological data is challenging, because it is heterogenous, noisy, confounded, sparse and of high temporal resolution over long periods of time. These properties are in stark contrast to many of the benchmark datasets that machine-learning approaches are typically developed and evaluated on. Several works therefore deal with adapting existing machine-learning approaches to the idiosyncrasies of clinical and physiological data, such as missingness @cite @cite , long-term temporal dependencies @cite , noise @cite , heterogeneity @cite and sparsity @cite . We build on several of these innovations in this work.
- Beside distant supervision, other state-of-the-art approaches to semi-supervised learning in neural networks include, broadly, methods based on (i) reconstruction objectives, such as Variational Auto-Encoders (VAEs) @cite and Ladder Networks @cite , and (ii) adversarial learning @cite @cite @cite . However, with standard benchmarks consisting primarily of low-resolution image datasets, it is yet unclear to what degree these method's results generalise to heterogenous, long-term and high-resolution time series datasets with informative missingness, as commonly encountered in healthcare applications. -0.2in
- One of the first works to address API obfuscation problems was the Eureka framework @cite . This is a generic malware analysis framework and API deobfuscation is only one side of it. Eureka's API deobfuscation workflow includes the following steps:
- There are several ways to achieve local model interpretation. First, people structure the model in a way that the output is linear in terms of input variables so that the weights could be used as a measure of importance. For example, @cite uses the neural attention mechanism @cite to generate interpretable attention weights in recurrent neural networks. However, due to the stochastic training process used, these attentions are shown to be unstable @cite . The second way uses model specific heuristics to decompose the predicted scores by input variables. @cite described such methods for regularized regression and gradient boosted machine. Third, locally approximating sophisticated models using simple interpretable model could explain individual predictions. Gradient vector and sparse linear methods are tried as the local explainer @cite @cite @cite @cite . Finally, influence functions from robust statistics can be used to track a particular prediction back to training data that are responsible for it @cite . In conclusion, all local model interpretation methods work at the single data sample level, generating the contributions of input variables to the final predicted score for a specific data sample.
- People also try to directly build a globally interpretable model, including additive models for predicting pneumonia risk @cite and rule sets generated from sparse Bayesian generative model @cite . However, these models are usually specifically structured thus limited in predictability to preserve interpretability. @cite uses queries to build a tree to approximate neural networks. @cite generally discusses presenting machine learning models in different levels.
- Recursive partitioning and its resulting tree structure is an intuitive way to present rule sets and model interactions between input variables. It has been used for a long time to analyze heterogeneity for subgroup analysis in survey data @cite . Recently it is applied to study heterogeneous causal or treatment effects @cite @cite . It is a good fit for our global model interpretation task because we want to extract the rules that machine learning model finds and these rules are affected by the interactions between input variables.
- Feature selection methods select a subset of important features from the input variables when the machine learning model is trained. The model interpretability could be benefited from this process because it reduces the dimension of input variables, making the model compact and easier to be presented @cite . This is very useful when the input is very high dimensional @cite . The feature selection process could either be conducted before the model fitting @cite or embedded into it @cite @cite . Though feature selection and global model interpretation tasks both aim to extract the most important variables or their combinations, they are different because global model interpretation is a post model fitting process. We represent the trained model in a compact and comprehensible way with good fidelity to the original model. The goal is not to make predictions using this representation but understand how it predicts. In contrast, feature selection discards unimportant variables and predictions will be solely based on selected ones.
- There are many follow-up studies on RC along different directions, such as practical implementation @cite @cite @cite @cite and the repair problem with heterogeneous structures @cite @cite @cite @cite @cite @cite @cite .
- Flexible RC @cite is designed for heterogeneous storage systems that can achieve the lower bound of repair bandwidth. Combined with tree-structured regeneration topology, it is shown that RC can further save the network bandwidth @cite @cite . Some studies @cite @cite focus on the capacity bound for a heterogeneous model. However, all the studies do not distinguish the costs between intra-rack and cross-rack communications in data centers.
- Although the costs between cross-rack and intra-rack communications are distinguished by previous studies @cite @cite @cite , their system models are fundamentally different. DRC @cite @cite can be viewed as a special case of our MSRR codes. Sohn @cite consider a different repair model and give the optimal trade-off between storage and cross-rack repair bandwidth. In their repair process, there is no information encoding between two nodes in the same rack, while in our model, the symbols downloaded from other rack are combinations of all the symbols in the rack as in DRC @cite @cite . The cross-rack repair bandwidth of RRC is less than that of the codes in @cite for most parameters.
- The closest related work to ours is by Prakash @cite . In their model, a file needs to be retrieved from a certain number of racks, and hence @math must be a multiple of number of nodes in each rack. On the other hand, our model allows a file to be retrieved from any @math nodes. Therefore, our RRC can tolerate more failure patterns than the codes in @cite . We show that the trade-off curve of RRC coincides with the optimal trade-off curve in @cite when @math is a multiple of number of nodes in each rack, yet our exact repair constructions for MSRR codes and MBRR codes can support much more parameters than that of the minimum storage codes and the minimum bandwidth codes in @cite , respectively (see for details). More importantly, the cross-rack repair bandwidth of our MSRR codes with additional parameters is strictly less than that of MSRR codes with the nearest @math that is a multiple of number of nodes in each rack (see the remark in ). Table compares our RRC with the other related work for erasure-coded data centers.
- Notable schemes for discretizing the fractional Laplacian based on different ideas include work based on the Caffarelli-Silvestre extension @cite @cite @cite , finite-element-based approaches @cite @cite @cite @cite @cite , and work on spectral approaches @cite @cite @cite . General references for fractional Laplacians on bounded domains include, e.g., Ros-Oton @cite , D'Elia and Gunzburger @cite , Felsinger @cite , and @cite .
- Much recent work has been dedicated to the study of how people use WhatsApp and the role of this new application in social communication. Most works to date have analyzed peoples' behavior through conducting surveys and targeted interviews. For example, work by Church and Oliveira conducted an online study asking targeting questions of users which were aimed at understanding differences between WhatsApp and SMS usage @cite . Pielot et. al @cite created a survey focusing on the question whether people expected an answer to their WhatsApp and SMS messages within several minutes. O'Hara et. al interviewed 20 WhatsApp users for nearly an hour each, asking them semi-structured questions aimed at determining the nature of relationships forged with the people with whom they communicated @cite . Mudliar and Rangaswamy @cite spent over 350 hours observing 109 students, as well as conducted surveys to understand gender differences within Indian students' use of WhatsApp. All of these studies can be characterized as being formed through a desire to answer specific questions by conducting targeted surveys and interviews.
- In theory, even more accurate models could have been constructed had we also analyzed the messages' content. Specifically, models have been previously developed which can predict an author's gender, age, native language or personality @cite @cite based on content. Examples include work by Argamon @cite which focused on creating models that identify word usage differences between men and women on Internet blogs. Similarly, Wagner et. al @cite focused on content differences between men and women in Wikipedia, and Wang, Burke and Kraut performed a study of content differences between genders on Facebook @cite . However, as the WhatsApp network is inherently private, such approaches could not be applied in our case due to privacy concerns. As we now detail, even despite not having this information we were indeed similarly successful in predicting a user's demographic and group behavior.
- A good overview of transfer learning research and terminology can be found at: @cite , we follow this terminology.
- There are two prior works that form the basis for the TAN framework: 1) Generative Adversarial Networks (GAN) @cite and 2) Adversarially Learned Inference (ALI) @cite (which is equivalent to BiGAN @cite ). TAN leverages the general theoretical results from the GAN framework (the ALI framework leverages the GAN results as well) but utilises the ALI framework's training procedure as a component of the unique TAN algorithm.
- The ALI framework (and also the BiGAN framework) extends the GAN framework by simultaneously learning a reverse transfer function that maps inputted data back to the Gaussian-noise vector which generated it, allowing the ability to finely control the features of the generated data with interpolations in the Gaussian-noise vector. The ALI framework by itself does not allow the ability to learn conditional probability distributions on inputted conditional data pairs @cite .
- The GAN framework can directly learn conditional probability distributions, and has also previously been formulated for the standard domain-adaptation learning scenario @cite . However, the GAN framework and its variants are not suited to the generalised domain-adaptation learning scenario because the discriminator requires label-vectors Label-vector' here means the actual data that the discriminator discerns as being real or fake, and not the real fake label for the data inputted to the discriminator. Also, the input-vector for the domain-adaptation problem is inputted to the generator along with the Gaussian noise vector. that come from the same marginalised probability distribution across the real and fake (i.e. source and target in this case) domains. TAN allows the label-vectors to come from different marginalised probability distributions across the real and fake domains.
- @math -GAN @cite is structurally similar to TAN in that it also leverages the GAN theoretical results and the ALI training procedure, however it is built for the more restrictive inductive transfer learning task and cannot handle the transductive transfer learning task. This means that @math -GAN requires paired input label training data in both the source and target domain, whereas TAN only requires paired input label training data in the source domain, unlabelled input data in the target domain and a marginalised prior distribution on the label-vector distribution in the target domain.
- The sheer volume of video data makes the automatic video content understanding difficulty intrinsically. Very recent, deep architectures have been utilized to extract feature representations effectively in the video domain. While the development of image representation techniques has matured quickly in recent years @cite @cite @cite @cite @cite , more advanced architectures were conducted for video understanding @cite @cite @cite , including Convolutional Networks (ConvNets) with Long Short-Term Memory (LSTMs) @cite @cite and 3D Convolutional Networks @cite for visual recognition and action classification, two-stream network fusion for video action recognition @cite @cite , Convolutional Networks learning spatiotemporal features @cite @cite . We discuss these previous works in each subsection.
- To reduce such computational burden, Sun . proposed to factorize spatial-temporal convolutions @cite . It is worth noting that videos can be naturally considered as an ensemble of spatial and temporal components. Motivated by this observation, Simonyan and Zisserman introduced a two-stream framework, which learn the spatial and temporal feature representations concurrently with two convolutional networks @cite . Such a two stream approach achieved the state-of-the-art performance on many benchmarks. Furthermore, several important variants of fusing two streams are proposed, such as @cite @cite @cite @cite @cite @cite @cite @cite
- In video categorization systems, two types of feature fusion strategies are widely used, , the early fusion and the late fusion. Multiple kernel learning @cite was utilized to estimate fusion weights @cite @cite , which are needed in both early fusion and late fusion. To efficiently exploit the relationships of features, several more advanced feature fusion techniques were conducted. An optimization framework in @cite applied a shared low-rank matrix to reduce noises in the fusion. An audio-visual joint codebook proposed by Jiang @cite discovered and fused the correlations of audio and visual features for video classification. The dynamic fusion is utilized in @cite as the best feature combination strategy.
- With the rapid growth of deep neural networks, the combination of multiple futures in neural networks gradually comes into sight. In multimodal deep learning, a deep de-noised auto-encoder @cite and Boltzmann machines @cite were employed to fuse the features of different modalities. More recently, Recurrent Neural Networks have also been utilized to fuse the video representation. red Wu . @cite modeled videos into three streams including frames, optical flow and audio spectrogram and fuse classification scores adaptively from different streams with learned weights. Ng . @cite employed time domain convolution or LSTM to handle video structure and use late fusion after the two-stream aggregation. Comparing with this work, we propose a fusion network to efficiently fuse the local and global sequential information learned by the self-attentive and M-LSTM models.
- The tasks of learning to score the sports have less been studied with only two exceptions @cite @cite . Pirsiavash . @cite introduced a learning-based framework evaluating on two distinct types of actions (diving and figure skating) by training a regression model from spatiotemporal pose features to scores obtained from expert judges. Parmar @cite applied Support Vector Regression (SVR) and Long Short-Term Memory (LSTM) on C3D features of videos to obtain scores on the same dataset. In both @cite @cite , the regression model is learned from the features of video clips actions to the sport scores. Comparing with @cite @cite , our model is capable of modeling the nature of figure skating. In particular, our model learns to model both the local and global sequential information which is essential in modeling the TES and PCS. Furthermore, our self-attentive and M-LSTM model can alleviate the problem that figure skating videos are too long for an ordinary LSTM to get processed.
- Mismatched decoding has an extensive literature ( @cite has many relevant references on the subject). The approach, however, is essentially information theoretical, guided by the fundamental question of determining . This means that most of the work in the area aims to understand what is achievable asymptotically, for example, what are the achievable rates for families of channels with the input-output sets' size going to infinity. Those are very difficult questions and hence a significant part of the effort is directed to find bounds for those rates (and other significant invariants).
- Our approach to study the geometry of the space of channels has an intersection with the concept of , as introduced by Shannon @cite and as presented, for example, by Makur and Polyanskiy @cite . Using the notation of Makur and Polyanskiy, given two channels with transition matrices @math and @math of size @math and @math respectively, with @math and @math , one says that @math @math if there are two families @math and @math of channels (with @math being an @math transition matrix and @math an @math transition matrix) and probability mass function @math over the set @math such that [ Q= k=1 ^ m g ( k ) B_ k PA_ k . ]
- After the seminal work of Fergus al @cite and Shan al @cite , many deblurring methods were proposed towards both restoration quality and adaptiveness to different situations. Natural image priors were designed to suppress artifacts and improve quality. They include total variation (TV) @cite , sparse image priors @cite , heavy-tailed gradient prior @cite , hyper-Laplacian prior @cite , @math -norm gradient prior @cite , Most of these traditional methods follow the coarse-to-fine framework. Exceptions include frequency-domain methods @cite @cite , which are only applicable to limited situations.
- Image deblurring also benefits from recent advance of deep CNN. Sun al @cite used the network to predict blur direction. Schuler al @cite stacked multiple CNNs in a coarse-to-fine manner to simulate iterative optimization. Chakrabarti @cite predicted deconvolution kernel in frequency domain. These methods follow the traditional framework with several parts replaced to the CNN version. Su al @cite used an encoder-decoder network with skip-connections to learn video deblurring. Nah al @cite trained a multi-scale deep network to progressively restore sharp images. These end-to-end methods make use of multi-scale information via different structures.
- Different from classification tasks, networks for image processing require special design. As one of the earliest methods, SRCNN @cite used 3 flat convolution layers (with the same feature map size) for super-resolution. Improvement was yielded by U-net @cite (as shown in Fig. (a)), also termed as encoder-decoder networks @cite , which greatly increases regression ability and is widely used in recent work of FlowNet @cite , video deblurring @cite , video super-resolution @cite , frame synthesis @cite , Multi-scale CNN @cite and cascaded refinement network (CRN) @cite (Fig. (b)) simplified training by progressively refining output starting from a very small scale. They are successful in image deblurring and image synthesis, respectively. Fig. (c) shows a different structure @cite that used dilated convolution layers with increasing rates, which approximates increasing kernel sizes.
- We build on shared autonomy work in which the system is initially unaware of the user's goal @cite @cite @cite @cite @cite @cite @cite and explore problem statements with unknown dynamics, unknown user policy, and unknown goal representation. The parallel autonomy @cite and outer-loop stabilization @cite frameworks approach shared-control teleoperation from a different angle: instead of predicting user intent, they minimally adjust user input to achieve safe trajectories for tasks like semi-autonomous driving. Our agent's policy of executing a near-optimal action closest to the human's suggestion is inspired by this approach. Existing work in parallel autonomy requires analytic descriptions of the environment, such as the explicit locations of road boundaries and a model of the behavior of other cars. Outer-loop stabilization requires knowledge of the user's goal. Our method is analogous, but for environments in which we do not have a dynamics model or a goal representation.
- Shared autonomy enables a semi-autonomous agent to interpret user input at test time. In contrast, human-in-the-loop reinforcement learning frameworks leverage human feedback to train autonomous agents that operate independently of the user at test time @cite @cite @cite @cite . These frameworks are applicable to settings where the agent has access to all task-relevant information (e.g., goals), but the reward function is initially unknown or training can be sped up by human guidance. We focus on the orthogonal setting where the agent does not have direct access to the information that is private to the user and relevant to the task, and will always need to leverage user input to accomplish the task; even after training. This is also the key difference between our method and inverse reinforcement learning @cite and learning from demonstration @cite , which generally require user interaction during training time but not at test time.
- There is a richer body of literature on modeling voter preferences. Most of this research separates into two primary methodologies. In the first, researchers are interested in the relationship between voter characteristics and ballot preferences. To obtain labeled datasets, researchers use voter surveys @cite and exit polls @cite . They then fit models via simple GLMs, like the multinomial probit or multinomial logit model, which represent voter choice as a decision among unordered alternatives (parties, candidates) as a function of chooser (voter) attributes" @cite .
- Commonly used ranking methods for CQA that have shown to achieve state-of-the-art performance are pairwise methods @cite @cite @cite and point-wise methods @cite @cite . The use of tensor-based methods has also been used by previous researchers to achieve state-of-the-art performance on different CQA tasks @cite @cite .
- Several variants of deep learning models have been proposed by previous researchers to tackle the sentence matching problem in CQA @cite @cite @cite . @cite proposed a bag-of-vectors approach and used CNN and attention-based LSTMs to rank QA pairs. In other works, @cite used convolution neural network and @cite used recurrent neural networks with attention to find the relevance to answers for a given question.
- Most of the previous work on automatic question answering for CQA sites show the advantage of feature engineering when used with deep models @cite @cite @cite @cite . Feature engineering introduces additional semantic and lexical rules which help the underlying learning model to gain an overall increase in performance. @cite @cite @cite .
- There are several interesting analysis of traditional IT botnets. @cite , the authors managed to act as fake servers and collected information about the Torpig botnet. @cite , the authors presented a system able to capture and track more than 100 unique IRC-based botnets to measure the percentage of malicious traffic attributed to those botnets on the Internet. @cite , the authors proposed a botnet take-down analysis and recommendation system. However, none of those papers analyzes a cyber-physical botnet with suitable quantitative metrics such as latency and size of the packets.
- We have seen also attempts to detect botnets for CPS. In particular, in @cite the authors are trying to detect P2P SCADA botnets by means of custom network monitoring. However, they assume to be attacked by a traditional P2P botnet.
- The first moment formula is derived from the Grothendieck-Lefschetz trace formula for ' e tale cohomology with twisted coefficients'' from which the name is borrowed. The author and Lagarias @cite use Theorem to establish a representation theoretic interpretation of the @math , where @math is the probability of a random squarefree polynomial having factorization type @math .
- There have been other generalizations of Theorem from squarefree polynomials to all polynomials. Gadish [Sec. 1.3] Gadish and Hast, Matei @cite both study expected values of functions defined on the set of all polynomials; their functions depend on both the degree of the irreducible factors and their multiplicities. We call these . Gadish [Cor. 1.4] Gadish shows that the expected value of a weighted factorization statistic @math on @math matches the expected value of @math on @math viewed as a class function. Stated geometrically, the expected values of weighted factorization statistics on degree @math polynomials correspond to the cohomology of @math as an @math -representation, while the expected values of our factorization statistics correspond to the cohomology of @math as an @math -representation.
- Recently, there is a trend to incorporate visual features into the research of personalized recommendation. Specifically, @cite introduced the concept of visual recommendation into e-commerce, and released a large dataset for this task. @cite represented each product image as a fixed length vector, and infused it into the bayesian personalized ranking (BPR) framwork @cite to improve the performance of Top-N recommendation. To make use of both visual- and textual- features, @cite integrated the product images and item descriptions together to make dynamic Top-N recommendation. @cite adopted neural modeling based on product images to model the style of items, which led to improved recommendation performance. @cite introduced image features into point-of-interest (POI) recommendation, and proposed a graphical framework to model visual content in the context of POI recommendation. Shankar @cite designed a unified end-to-end neural model (named VisNet) to build a large scale visual recommendation system for e-commerce. Chen @cite introduced the attention mechanism into CF to model both item- and component-level implicit feedback for multimedia recommendation.
- The formal analysis of weak memory model hardware implementations has typically been done using SAT-based techniques @cite @cite . In @cite , a formal analysis based on Coq is used in order to evaluate SC, TSO, PSO, and RMO memory models. The DIY tool developed in @cite generates assembly programs to run against Power and x86 architectures. In contrast, in this work we concentrate on the analysis of the ECMAScript memory model, assuming the processor behavior is correct.
- MemSAT @cite is a formal tool, based on Alloy @cite , that allows for the verification of axiomatic memory models. Given a program enriched with assertions, MemSAT finds a trace execution (if it exists) where both assertions and the axioms in the memory model are satisfied.
- An analysis of the C++ memory model is presented in @cite . The formalization is based on the LEM language @cite , and the CPPMem software provides all possible interpretations of a C C++ program consistent with the memory model. More recently, an approach based on Alloy and oriented towards synthesizing litmus tests is proposed in @cite .
- Despite the above-mentioned prominent applications in IoT, the area of the large-scale B-WPC network with multiple reader-tag links has been largely uncharted. A very recent contribution @cite considers the network coverage probability and transmission capacity of a B-WPC network with power beacons (PBs), where the network is modeled as a random Poisson cluster process by leveraging stochastic geometry. Most recently, the network model in @cite is further extended to a more generic network setup with the energy from multiple nearby PBs taken into consideration in @cite . However, the work in @cite and @cite only focuses on the scenario where the energy is harvested from dedicated power beacons, while the performance of the decentralized B-WPC MANET remains unknown. Most importantly, all the above work fails to capture the nature of the B-WPC network in pervasive IoT and mMTC applications with asynchronous transmissions and the incurred time-varying interference. The above reasons motivate our work.
- Social media context for labeling . A set of tags associated with each image is commonly used in multimodal classification settings. Guillamumin @cite explored the relationship between tags and manual annotations to recover annotations using a combination of tags and image content. Lindstaedt @cite and Sigurbjornsson @cite studied the problem of recommending tags that were obtained from similar images and similar users. Sawant @cite and Stone @cite investigates friendship information between users for tag recommendation in social networks. EXIF and GPS are two commonly used sources of metadata that come directly from the camera @cite @cite @cite @cite . Such metadata can be used to help determine who captured the photo and where, and also provide informative signals for image labeling tasks. Our method differs from all these and also @cite in that we use a much larger range of social media information, including free-form text as well as links, with deep learning based pixel descriptors incorporated into our novel deep learning fully connected CRF framework.
- Tutorials can enhance different kinds of learning, learning by example or learning by principle. Eiriksdottir and Catrambone conducted a review of instructions for procedural tasks @cite . They suggest that specific procedural instructions grounded with realistic examples and limited reference to more general principles produce better primary task performance but poor learning and transfer to other tasks. In many cases, users turn to a how-to or tutorial video to obtain specific information without particularly needing or wanting to learn about more general principles -- for example, when fixing their printer. Thus it is critical that tutorial structure supports initial performance, implying a focus on step-by-step instructions. Other work has shown that higher quality examples enhance task performance @cite and that learning can improve with the incorporation of video-based examples in particular @cite . Coupled with Clark's and Mayer's finding that multimedia is especially useful for learners who have low knowledge of a domain'' @cite , this suggests that tools for creating tutorial and how-to video should support links to concrete examples and complementary multimedia materials.
- In summary, Torrey found that how-to sharing occurs within and across a collection of communication tools without any centralized control'' @cite and that people tend to find information by browsing as much as by more directed search @cite . Carter previously found that tools for the capture, creation, and access of how-to guides were similarly decentralized @cite .
- Containerization systems for service delivery in 5G networks is a widely studied topic. In particular, works such as @cite evaluate Docker as a platform for Edge Computing concluding that it provides fast deployment, small footprint and good performance. Other studies, e.g., @cite , claim that distributed service delivery will be one of the hottest topics in 5G networks, and demonstrate the efficiency of Docker as containerization system in the implementation of Virtual Network Functions. Furthermore, Linux containers have been widely analyzed and compared against Virtual Machines under multiple aspects: @cite explores the performance of traditional VM deployments, and compares this approach to the use of Linux containers. In line with our results, @cite also demonstrates that containers lead to equal or better performance than VMs in all cases.
- Other works, such as @cite , focus on the advantages of Docker containers rather than VMs, in the deployment of cloud gaming, i.e., architectures where the game is rendered on a distant cloud server and the resulting video stream is sent back to the user. We underline that in our work, instead, a dockerized multiplayer client-server architecture is analyzed. The authors of @cite investigate Docker in overlay-network mode and in host-network mode for the provisioning of a large-scale streaming system, finding that the advantages in terms of latency in host mode are balanced by the higher stability of an overlay network. Although related to their study, our work focuses on characterizing the overhead cost of the containerized solution for different applications.
- In auto-regressive models, such as PixelRNN @cite , the contextual dependencies between spatio- or temporaly-proximate parts of a given data sample are learned by a generating network. Thus, the model learns how to generate a realistic looking segment @math of the data sample given a set of already generated parts @math (context), such that @math is maximized w.r.t. the model parameters @math .
- Besides stabilizing training by allowing for @math to be trained to optimality, a further advantage of the WGAN objective is that the Wasserstein distance can be monitored during training and can indicate convergence. However, it is required that @math (here called a ) is K-Lipschitz continuous for all @math , where K is a positive constant. To satisfy this prerequisite, previous works have proposed weight clipping of the critic parameters @cite or a gradient penalty regularization term @cite . As of recent, @cite have shown that of all @math parameters outperforms other K-Lipschitzness enforcing methods in terms of stability and computation time.
- In the domain of unsupervised adversarial learning of sequential data, SeqGAN @cite has shown that GANs can be applied to the generation of short sequences of natural language using reinforcement learning. Recently, that line of work was continued in @cite @cite and the authors of @cite specifically express the need for more elaborate training heuristics when generating natural language sentences with RNN generator and RNN discriminator. This illustrates the increased difficulty of the generation task when dealing with temporal dependencies in the data, particularly for the NLP domain where features are discrete and gradients are difficult to propagate back. A maximum-likelihood augmentation of the GAN framework is proposed in @cite to stabilize training when discrete data (e.g. language) is modeled. Furthermore, GANs have been used for music generation in @cite @cite , where the sequences are also represented by discrete tokens.
- The present paper is an extended version of the results in @cite : more proofs and examples have been provided. An erratum has been provided: the interpretation of the case construct has been slightly modified so that we can consider non decreasing functions (and not only strictly increasing functions). There are two lines of work that are related to our approach. @cite , Van De Pol introduced higher order interpretation for showing the termination of higher order term rewrite systems. @cite @cite , Baillot and Dal Lago introduce a higher order interpretations for complexity analysis of term rewrite systems. While the first work only deals with termination properties, the second work is restricted to a family of higher order term rewrite systems called simply typed term rewrite systems.
- Locality Sensitive Hashing (LSH) @cite aims to use several hash functions to randomly project the data into a Hamming space, so as to ensure the probability of collision is much higher for data points which are close to each other than for those which are far apart. Consider the non-linearity existing in many real-world datasets, LSH was generated to accommodate arbitrary kernel functions (KLSH) in @cite . Some other priors, such as @math -stable distributions @cite and shift-invariant kernels @cite , are also embedded to extend LSH for performance improvement.
- In contrast to unsupervised hashing methods, supervised hashing learning utilizes the label information to encourage the binary codes in the Hamming space to preserve the semantic relationship existing in the raw data. For instance, @cite introduced a hinge-like loss function to exploit the semantic information. Besides, @cite projected the raw data into a latent subspace, and the label information is embedded on this subspace to preserve the semantic structure. The Jensen Shannon Divergence is also utilized in @cite to learn the binary codes within a probabilistic framework, in which an upper bound is derived for various hash functions. Being similar to KLSH, @cite proposed a supervised hashing with kernels, in which the similar pairs are minimized while the dissimilar pairs are maximized. Consider the discrete constraint, the supervised discrete hashing (SDH) @cite was proposed to not only preserve the semantic structure, but also discretely learn the hash codes without any relaxation. However, this discrete optimization is time-consuming and unscalable. To tackle this problem, a novel method named column sample based discrete supervised hashing (COSDISH) was presented to directly obtain the binary codes from semantic information.
- @cite proposed an encoder-decoder network for image inpainting called Context Encoders (CEs). CEs are trained by minimizing a function of @math loss and adversarial loss @cite to inpaint occluded image regions. Although CEs are able to generate promising inpainting results, they make use of structures of occluded regions during training but not during inference. Thus, inpainting results of CEs are sometimes visually unrealistic.
- @cite proposed a semantic image inpainting framework which includes contextual and perceptual losses. Their framework leverages a pre-trained Deep Convolutional GAN (DCGAN) @cite , and aims to find the closest mapping in the latent space. However, inpainting results of their framework usually have differences in color along mask boundaries. Therefore, some post-processing methods, such as Poisson blending @cite , are used to eliminate color differences after inpainted images are generated by the framework. Moreover, their method requires many iterations to find the closest mapping in the latent space during testing phase. On the other hand, our method can generate realistic and visually consistent contents, and requires no post-processing.
- @cite proposed an approach for image completion using deep neural networks (DNNs). Their method is non-blind inpainting , that is, it requires not only an input image but a mask indicating regions to be inpainted. The above two methods @cite @cite including ours perform blind inpainting where no mask is necessary.
- @cite proposed input convex neural networks (ICNNs) which share similar architectural properties compared to SPENs. ICNNs add constraints to the parameters of SPENs, such that their energy function is convex with respect to some parameters of the energy function and the optimization can be performed globally. Our architecture is similar to that of SPENs and ICNNs. Unlike SPENs, which only consider multi-label classification problems, we address image inpainting problems. Employment of end-to-end learning methods enables SPENs to handle more complex tasks such as depth image denoising. We employ a specific CNN which includes connections from input layer to hidden layers in order to achieve realistic image inpainting results. Such connections have been recently discussed in deep residual networks @cite and densely connected convolutional networks @cite . ICNNs employ such connections in order to restrict the energy function to be a convex function with respect to some parameters of the energy function. However, convexity gives a strong restriction on the expressivity of the energy function. In our experiments, we show that convexity constraints of ICNNs hinder the networks from generating visually better image inpainting results, and thus our method performs better than ICNNs.
- . LRA methods @cite @cite are based on one key observation that most of CNN filters or features are of low rank and can be decomposed into to lightweight layers by matrix factorization. @cite made one of the early attempts at applying LRA methods such as Single Value Decomposition (SVD) for network simplification. @cite decomposed @math filters into @math and @math filters, which effectively saved the computations. @cite investigated two different optimization schemes, one for filter-based approximation and one for feature-based approximation. Similarly, @cite used low-cost collaborative kernels for acceleration. @cite used Generalized SVD for the non-linearity in networks and achieved promising results in very deep CNNs. Recently, @cite combined the low-rank approximation with channel pruning and @cite proposed to use Force Regularization to train neural networks towards low-rank spaces.
- . Methods in this category mainly focus on increasing the sparsity in CNNs. @cite @cite introduced a three-step training pipeline to convert a dense network into a sparse one. They achieved this by removing connections with small weights. Their method showed promising compression rates on various CNNs. However, it required an additional mask to mask out pruned parameters and handle the sparsity, which actually does not save computations. @cite proposed a dynamic network surgery algorithm to compress CNNs by making them more sparse. @cite @cite @cite also proposed to remove the neurons in networks to increase the sparsity. Usually, these methods require specially designed software or hardware @cite to handle the sparsity in CNNs in order to gain real speedup.
- . On top of the method in @cite , @cite used additional quantization and Huffman encoding to further compress the storage requirement. These methods can also be applied on top of our filter pruning algorithm. Another group of methods tried to use bit-wise operations in CNNs to reduce the computations. @cite introduced an algorithm to train networks with binary weights and activations. In @cite , the authors suggested to also binarize the inputs to save more computations and memories. However, experiments showed that the performance of these binarized networks are worse than their full prevision counterparts @cite .
- Generalizing neural networks to data with graph structures is an emerging topic in deep learning research. The discussed neural network architectures include both recurrent neural networks @cite @cite and convolutional neural networks (CNNs) @cite @cite @cite @cite @cite . This work is more related to the generalization of CNNs, or graph convolutional networks (GCNs). The principle of constructing GCNs on graph generally follows two streams: 1) the , where the locality of the graph convolution is considered in the form of spectral analysis @cite @cite @cite @cite ; 2) the , where the convolution filters are applied directly on the graph nodes and their neighbors @cite @cite . This work follows the spirit of the second stream. We construct the CNN filters on the spatial domain, by limiting the application of each filter to the 1-neighbor of each node.
- Activity recognition comprises the challenge to recognise human activities from the input of sensor data. A broad range of sensors can be applied for this task. Traditionally, accelerometer devices have evolved as the standard equipment for activity recognition both for their high diffusion and convincing recognition rates @cite @cite . General research challenges for activity recognition regard the accurate classification of noisy data captured under real world conditions @cite or the automation of recognition systems @cite . Another problem that is addressed in depth only recently is the creation of classification systems that scale to a large user base. With increasing penetration of sensor enriched environments and devices, the diversity in user population poses new challenges to activity recognition. for instance address this challenge by maintaining several groups of similar users during training to identify inter-user differences without the need for individual classifiers @cite .
- Sohn et. al describe a system that extracts seven features from GSM signal strength measurements to distinguish six velocity levels with an accuracy of @math @cite . The features mainly build on distinct measures of variation in signal strength and the frequency of cell-tower changes in the active set.
- While all previously mentioned results considered special installations of the wireless transmitters, presented a system that allows the localisation of a wireless device with an accuracy of about 1 meter from WiFi physical layer information even when the receiver is carried by a person that might induce additional noise to the captured features @cite .
- The simultaneous localisation of multiple individuals at the same time was first mentioned and studied by Patwari and Wilson in @cite . The authors derive a statistical model to approximate the position of a person based on RSSI variance which can be extended to multiple persons. This aspect together with the previously untackled problem that environmental changes over time might necessitate frequent calibration of the location system was approached by Zhang and others in @cite . The authors isolate the LoS path by extracting phase information from the differences in the RSS on various frequency spectrums at distributed nodes. Their experimental system is with this approach able to simultaneously and continuously localise up to 5 persons in a changing environment with an accuracy of 1 meter.
- monitor breathing based on RSS analysis @cite . The monitored area was surrounded by twenty 2.4 GHz nodes and the two-way RSSI was measured. Using a maximum likelihood estimator they approximated the breathing rate within @math to @math beats accuracy.
- Recently, we also conducted preliminary studies regarding the use of features from a RF-transceiver to classify static environmental changes such as opened or closed doors, presence, location and count of persons with an accuracy of @math to @math @cite @cite @cite @cite . We utilised USRP Software defined radio devices (SDR) http: www.ettus.com from which one constantly transmits a signal that is read and analysed by other nodes. Devices were equipped with 900 MHz transceiver boards. With the software radios a higher sampling frequency than in previous studies is possible and we can also sample the actual channel instead of only tracking the RSSI. In these studies we concentrated on features related to the signal amplitude and derivation of the instantaneous amplitude from its mean. Furthermore, we conducted preliminary studies on passive device free situation awareness by utilising ambient signals from a FM radio station not under the control of the recognition system. In these studies, static environmental changes such as opened doors have been detected with an accuracy of about @math @cite and a first study on suitable features to detect human activities could achieve an accuracy of about @math with a two stage recognition approach @cite .
- Recognition utilising signals on the wireless channel has been generalised in @cite to activities and we can further imagine also situations @cite , gestures @cite or attention @cite to be identified by RF-based device-free implementations. These systems can be grouped into active and passive approaches conditioned on the presence of an active transmitter. Most previous work in this direction uses SDR devices.
- sense traffic situations by tracking frequency and speed of passing cars that intercept the direct line of sight between a pair of nodes @cite . The authors of @cite classify simple activities in an SDR-based active device-free system by extracting and interpreting features from a continuous signal between two nodes. Their approach explores also the multipath effects induced by persons that are not intercepting the direct path between nodes. It was later demonstrated that also simultaneously conducted activities from multiple persons can be distinguished by leveraging purely signal-strength based features @cite . Furthermore, it was shown by Pu and others that simultaneous detection of gestures from multiple individuals is possible utilising multi-antenna nodes and micro Doppler fluctuations @cite @cite . In a related system, Adib and Katabi employ MIMO interference nulling and combine samples taken over time to achieve the same result while compensating for the missing spatial diversity in a single-antenna system @cite .
- Also, active systems utilising non-SDR nodes have been studied. Most notably, Patwari and others estimated the breathing frequency of an individual surrounded by nodes from the RSSI of exchanged packets @cite . Following other directions, have counted crowd @cite from RSSI within a field of sensor nodes. Their unsupervised learning approach is able to predict the count of up to 10 stationary or moving individuals. Recently, the recognition of general activities from RSSI in a sensor network has been considered @cite . In particular, the activities standing, sitting, lying, walking and empty have been distinguished with an accuracy of 0.8-0.9.
- In the literature, several authors consider spontaneous authentication or the establishing of a secure communication channel among mobile and ad-hoc devices based on environmental stimuli @cite @cite @cite @cite . So far, shaking processes from accelerometer data and RF-channel measurements have been utilised as unique context source that contains shared characteristic information.
- Another sensor class utilised for context-based device authentication is the RF-channel. present a technique to authenticate co-located devices based on RF-measurements since channel measurements from devices in near proximity are sufficiently similar to authenticate devices against each other @cite . utilise physical layer features to derive secret keys for a pair of devices @cite . In the absence of interference and non-linear components, transmitter and receiver experience identical channel response @cite . This information is utilised to generate a secret key among a node pair. Since channel characteristics are spatially sharply concentrated and not predictable at a remote location @cite , an eavesdropper is not capable of guessing information about the secret. This scheme was validated in an indoor environment in @cite . Although we consider the keys generated by this scheme as strong, it does not preserve spatial properties. A device at arbitrary distance could pretend to be a nearby communication partner.
- Kunze and Lukowicz recently demonstrated, that audio information indeed suffices to derive spatial information @cite . They combine audio readings with accelerometer data to classify locations of mobile devices. In their work, the noise emitted by a vibrating mobile phone was utilised to distinguish among 35 specific locations in three different rooms with over 90 , Instead, we utilise purely ambient noise to establish a secure communication channel among devices in spatial proximity. We record NTP-synchronised audio samples at two locations, generate a characteristic audio-fingerprint and map this fingerprint to a unique secret key with the help of error correcting codes.
- The last step is necessary since the similarity between fingerprints is typically not sufficient to establish a secure channel. With fuzzy-cryptography schemes, the generation of an identical key based on noisy input data @cite is possible. analyse the usage of biometric or multimedia data as part of an authentication process and propose a protocol @cite . Due to the use of error-tolerant cryptographic techniques, this protocol is robust against noise in the input data. The authors utilise a secure sketch @cite to produce public information about an input without revealing it. The input can then be recovered given another value that is close to it. A similar study is presented by @cite . The authors establish a key distribution based on a fuzzy vault @cite using data measured by devices worn on the human body. The fuzzy vault scheme, also utilised in @cite , enables the decryption of a secret with any key that is substantially similar to the key used for encryption.
- Contextual or sensor information of mobile devices can be incorporated as a solution for authentication @cite . When the seed to the key is implicit with the context, no information that could be used to reconstruct the key is transmitted during key generation. For instance, @cite introduced , utilizing the camera of a mobile device to capture a 2D barcode which is displayed on the screen of another device. of @cite implements a similar scheme but exploits spoken audio. A user reads aloud a text message displayed on one device and a second device recognizes the speech for authentication. A further example mechanism by @cite uses accelerometer readings when devices are shaken simultaneously by a single person. Also, Mayrhofer derived in @cite that the sharing of secret keys is possible with a similar protocol by repeatedly exchanging hashes of key-sub-sequences until a common secret is found. generalise this approach to noisy acceleration readings @cite @cite . They utilize a hash function that maps similar acceleration patterns to identical key sequences. These approaches require explicit user interaction.
- By utilising a context source that provides a sufficient amount of unique, context-related information, such as audio or radio frequency (RF), it is possible to get the user out of the loop. introduced ProxiMate that enables wireless devices in proximity to pair automatically and securely using their shared ambient RF-signals @cite . They generate fingerprints from RF-channel fluctuations and map these onto a codespace of an error-correcting code. By correcting potential errors in the fingerprints, they are mapped onto the closest regular codeword in the codespace. When the similarity between fingerprints is high, codewords are identical. Sigg et. al proposed to use audio instead of RF in a similar implementation @cite . They study the entropy of audio fingerprints and identify time synchronisation as a main hindrance to practically apply the method for mobile devices. Their instrumentation requires idealized conditions regarding the synchronisation of devices and to account for this a high number of fingerprints must be created (201 in their experiments) in order to find one matching fingerprint. For extensive computational load, this is feasible only in an offline approach. The high number of fingerprints created, however, was necessary since the utilized NTP synchronisation is not sufficiently accurate.
- Although cross-language and cross-corpus speech emotion recognition is an interesting problem, relatively few studies have addressed this topic. Existing studies have mostly studied the preliminary feasibility of cross-corpus learning and pointed to the need for further in-depth research. For example, @cite used six different corpora to analyse cross-corpora emotion recognition using support vector machines (SVM) and highlighted the limitations of current systems for cross-corpus emotion recognition. @cite used four corpora to evaluate some pilot experiments on cross-corpus emotion recognition while using SVM. They used three datasets for training and a fourth for testing, and showed that the cross-corpus emotion recognition is feasible. To explore the universal cues of emotions across languages, @cite investigated cross-language emotion recognition for Mandarin vs. Western languages (i.e., German, and Danish). The authors focused on gender-specific speech emotion recognition and achieved the classification rates higher than the chance level but less than baseline accuracy. @cite developed an ensemble SVM for emotion detection with a focus on emotion recognition in unseen languages.
- In their paper Edelman and Jamison @cite have developed the foundations of a combinatorial abstraction of convexity. Similar ideas were studied by Dilworth @cite and later by Korte, Lov 'a sz and Schrader @cite via the notion of , a concept dual to the one of a finite convex geometry. Today, the concept of convex geometry appears in many fields of mathematics such as formal language theory @cite , choice theory @cite and mathematical psychology @cite among others. @cite showed that any abstract convex geometry can be represented as generalized shelling'' in @math for some @math . Richters and Rogers @cite reproved this theorem giving a better bound on the dimension. Different representations of finite convex geometries using different shapes than points for the ground set have been studied @cite @cite @cite .
- There are mainly two different way to perform the task of image captioning. These two types are basically retrieval based method and generative method. From that most of work is done based on retrieval based method. One of the best model of retrieval based method is Im2Txt model @cite . It was proposed by Vicente Ordonez, Girish Kulkarni and Tamara L Berg. Their system is divided into mainly two part 1) Image matching and 2) Caption generation. First we will provide our input image to model. Matching image will be retrieved from database containing images and its appropriate caption. Once we find matching images we will compare extracted high level objects from original image and matching images. Images will then reranked based on the content matched. Once it is reranked caption of top-n ranked images will be returned. The main limitation of these retrieval based method is that it can only produce captions which are already present in database. It can not generate novel captions.
- This limitation of retrieval based method is solved in generative models. Using generative models we can create novel sentences. Generative models can be of two types either pipeline based model or end to end model. Pipeline type models uses two separate learning process, one for language modeling and and one for image recognition. They first identify objects in image and provides the result of it to language modeling task. While in end-to-end models we combine both language modeling and image recognition models in single end to end model @cite . Both part of model learn at the same time in end-to-end system. They are typically created by combination of convolutional and recurrent neural networks.
- Show & Tell model proposed by is of generative type end-to-end model. Show & Tell model uses recent advancement in image recognition and neural machine translation for image captioning task. It uses combination of Inception-v3 model and LSTM cells @cite . Here Inception-v3 model will provides object recognition capability while LSTM cell provides it language modeling capability @cite @cite .
- Manual transcription of historical handwritten documents requires highly skilled experts, and is typically a time consuming process. Manual transcription is clearly not a feasible solution due to large amounts of data waiting to be transcribed. Fully automatic transcription using HTR techniques offers a cost-effective alternative, but often fails in delivering the required level of transcription accuracy @cite . Instead, semi-automatic or semi-supervised transcription methods have gained importance in the recent past @cite @cite @cite @cite @cite .
- An active learning based handwritten text transcription method is proposed in @cite that performs a sequential line-by-line transcription of the document, and a continuously re-trained system interacts with the end-user to efficiently transcribe each line.
- In practical scenarios, such methods are not appropriate as a system should ideally accept a full document page as an input and generate full transcription of the words as an output. An end-to-end system for handwritten text transcription is presented in @cite @cite that also uses HMM-based text image modeling with interactive computer assisted transcription. The transcription method proposed in this work addresses these issues and introduces @math for quick transcription of handwritten text using a segmentation-free word spotting algorithm @cite . The following section explains the proposed method and its advantages in detail.
- A lattice code consists of the intersection of a lattice (a discrete additive subgroup of the Euclidean space) and a bounded region, also called a shaping region. While it is well-known that lattice codes can achieve the capacity of the AWGN channel (see @cite and references therein), renewed interest in the topic can be traced to the seminal paper by Erez and Zamir @cite , who showed that capacity can be achieved by nested lattice codes with lattice decoding (a suboptimal decoding approach which effectively ignores the shaping region). Since then, several applications of lattice codes to multiterminal information theory have been proposed based on their results, including: distributed source coding @cite , physical-layer security @cite , and communication over Gaussian networks @cite ---in particular, lattice codes are essential to the compute-and-forward strategy for relay networks @cite and to integer-forcing methods for MIMO channels @cite .
- Multilevel lattice constructions based on binary codes are potentially harder to design, but have the promise of complexity that scales linearly with the number of levels. Moreover, they are known to be AWGN-good under multistage decoding @cite . Construction D has been used in @cite to produce turbo lattices and in @cite to construct polar lattices; the latter are shown to be AWGN-good with encoding and decoding complexity @math . Construction D has also been used in @cite to construct spatially-coupled LDPC lattices, which were shown to be AWGN-good under multistage belief propagation decoding. However, both the encoding and the cancellation step that has to be performed at each decoding stage rely on the generator matrices of the component LDPC codes, which are generally dense, leading to an overall high complexity.
- LRP is an inverse method which calculates the contribution of a single pixel to the prediction made by the network in the image classification task. The overall idea of pixel-wise decomposition is explained in @cite . Here we briefly reiterate some basic concepts of LRP using a simple example.
- Visual saliency is a biologically inspired model of measuring which information stands out relative to its neighbors and so attracts human attention @cite . It was originally promoted by psychologists in the study of attention in infancy @cite . Itti @cite presented a computational architecture to introduce the basic Koch and Ullman model @cite to the field of computer vision. There is an extensive body of literature on various applications of visual saliency, such as @cite @cite @cite @cite to name just a few. Recently, researchers have also integrated saliency with the latest deep learning techniques in salient object detection and category-specific object detection @cite . As is summarized in @cite , most visual saliency models are implemented in three stages:
- Saliency methods have garnered attention among deep learning researchers @cite @cite @cite @cite because they can address all the desirabilities mentioned above. Most of this existing work tries to calculate heatmaps from the predictions of the network, albeit using different equations.
- Most recently, Kindermans pointed out that methods such as LRP may suffer from the problem of input variance @cite . We conducted our studies independently at around the same time than these authors but pursued a different and presumably more powerful variant of this approach. In our work. we go further and beyond LRP by adding saliency detection directly into the deep CNN understanding and interpretation scheme. The results we obtained using our framework show that this proposed approach is indeed highly effective.
- . Convenient approaches for video hashing usually select frames from a video, treat the selected frames as separate images and then employ image hashing techniques on them @cite @cite @cite @cite @cite @cite . For example, Weng and Preneel proposed to extract feature from each frame and then generate hash code based on the extracted statistical feature vector, while and utilized multiple frame sets and multiple key frames to learn hash functions. proposed a hash learning method via deep belief network, where a fusion of visual-appearance and visual-attention features are used as inputs. All the above methods employed hand-crafted features which were fixed during hash learning process. In @cite , CNN features were extracted to learn hash function. presented an unsupervised video hash learning framework, by using a binary LSTM module and a normal LSTM module as the encoder and the decoder respectively. Frame-level features are extracted via a deep CNN. Still, the feature generation and the hash code generation are processed separately. proposed an integrated framework in which feature extraction, binary code learning and hash function learning are optimized in a self-taught manner. Yet this method doesn't learn binary code and hash function as part of the deep architecture.
- We propose to learn both video feature representation and hash functions within a deep learning pipeline. As far as we know, only one other approach has provided an end-to-end deep hash learning pipeline @cite . Our approach differs from that work both in neural network structure and in supervision learning metric. We also investigate the influence of category on hash code bits distribution and devise a category mask based hash code generation method for efficient video retrieval.
- There have been a significant amount effort devoted to develop models and algorithms that can effectively take the anatomy structure similarities across modalities contrasts into account during joint reconstructions. As widely accepted, these structure similarities can be exploited using the locations and directions of the edges @cite @cite @cite characterized by the magnitude and direction of the gradient of an image. The active researches on multi-modal contrast image reconstructions have focused mainly on how to effectively utilize the complimentary information on these structure similarities to improve the accuracy and robustness of the reconstructions.
- Besides joint TV or tensor based regularization, the parallelism of level sets across multi-modality contrast images are also proposed as joint image regularization in @cite @cite @cite @cite . The main idea is to exploit the structural similarity of two images @math and @math measured by the parallelism of their gradients @math and @math at each point using @math , for some functions @math and @math . Then the regularization in joint reconstruction takes form @math Several different choices of @math and @math have been studied in these works. For instance, in @cite , @math and @math are taken as identities, or @math and @math . In @cite , @math is the identity and @math . In @cite the side information on the level set, namely, the location and direction of the gradients of a reference image, is available to assist the reconstruction.
- Another joint reconstruction approach different from aforementioned methods is to recover gradient information for each of the underlying multi-modal images from the measurements in Fourier domain, then use the recovered gradients to reconstruct images. This approach is motivated by the idea of Bayesian compressed sensing and applied to joint multi-contrast MR image reconstruction in @cite . In @cite , gradients of the multi-contrasts images are reconstructed jointly from their measurements in Fourier space under a hierarchical Bayesian framework, where the joint sparsity on gradients across multi-contrast MRI is exploited by sharing the hyper-parameter in the their maximum a posteriori (MAP) estimations. Their experiments show the advantage of using joint sparsity on gradients over conventional sparsity. However, their method requires extensive computational cost. A two-step gradient reconstruction of MR images is also proposed in @cite , however, only single-modality contrast image is considered. In @cite , the authors showed that this two-step gradient reconstruction approach allows to reconstruct image with fewer number of measurements than required by standard TV minimization method.
- . Similar problems to the semantic loss have been reported in other terms. Domain shift @cite @cite is a generic problem that resides in all types of visual recognition, where the data from train and test are in different distributions. Hubness @cite states the phenomenon that the mapped semantic embeddings from images would be collapsed to hubs, which are near many other points without being similar to the class label in any meaningful way. We believe that semantic loss is one of the main reason for hubness, which can be alleviated by reconstruction @cite @cite @cite @cite . In this paper, we find that jointly training @cite reconstruction and classification is not effective to preserve semantics. Another way of countering semantic loss is to learn independent attribute classifiers @cite , which is not applicable when attribute annotation is unavailable.
- . We seek algorithms that can generate perceptually realistic images @cite @cite @cite @cite @cite @cite . Besides pixel-level loss, these methods impose feature-level reconstruction loss for preserving perceptual similarity or adversarial loss to remove unreal artifacts. However, they are based on image-to-image transformation while we requires that the reconstruction is from the semantic embedding. Our reconstruction network relates to image generation from a bottleneck layer @cite @cite @cite @cite .
- RGB RGB-D camera plus 2D LiDAR is the most frequently used combination in the literature. @cite presented two different methods for mobile robot tracking and following of a fast-moving person in an outdoor environment. The robot was equipped with an omnidirectional camera and a 2D LiDAR. @cite presented an integrated system to detect and track people and cars in outdoor scenarios, based on the information retrieved from a camera and a 2D LiDAR on an autonomous car. @cite introduced a people tracking system for mobile robots in very crowded and dynamic environments. Their system was evaluated with a robot equipped with two RGB-D cameras, a stereo camera and two 2D LiDARs.
- Our previous work with the aforementioned sensor combination includes @cite and @cite . The former presented a human tracking system for mobile service robots in populated environments, while the latter extended this system to a fully integrated perception pipeline for people detection and tracking in close vicinity to the robot. The proposed tracking system tracks people by detecting legs extracted from a 2D LiDAR and fusing this with the faces or the upper-bodies detected with a camera using a sequential implementation of the Unscented Kalman Filter (UKF).
- The combination with 3D LiDAR is increasing with the development of the 3D LiDAR technology. Taking advantage of its high accuracy, @cite developed an algorithm to align 3D LiDAR data with high-resolution camera images obtained from five cameras, in order to accurately track moving vehicles. Other reported results include @cite and @cite , which mainly focused on pedestrian detection rather than tracking. In addition, earlier work presented multitarget tracking with a mobile robot equipped with two 2D LiDARs, respectively located at the front and back @cite . Thus the robot can have a 360 @math horizontal field of view, where each scan of these two sensors covers the whole surrounding of the robot at an angular resolution of 1 @math .
- The use of machine learning algorithms for tracking has particular advantages. The closest work to ours is @cite , where the authors proposed a semi-supervised learning approach to the problem of track classification in 3D LiDAR data, based on Expectation-Maximization (EM) algorithm. In contrast to our approach, their learning procedure needs a small set of seed tracks and a large set of background tracks, that need to be manually or semi-manually labeled at first, whereas we do not need any hand-labeled data.
- The entity and relation linking challenge has attracted a wide variety of solutions over time. Linking natural language phrases to DBpedia resources, Spotlight @cite breaks down the process of entity spotting into four phases. It identifies the entity using a list of surface forms and then generates DBpedia resources candidates. It then disambiguates the entity based on surrounding context. AGDISTIS @cite follows the inherent structure of the target knowledge base more closely to solve the problem. Being a graph-based disambiguation system, AGDISTIS performs disambiguation based on the hop-distance between the candidates for the entities in a given text, where multiple entities are present. Babelfy @cite uses word sense disambiguation for entity linking. On the other hand, S-MART @cite is often appropriated as an entity linking system over Freebase resources. It generates multiple regression trees and then applies sophisticated structured prediction techniques to link entities to resources.
- As relation linking is generally considered to be a problem-specific task, only a few general purpose relation linking systems are in use. Iterative bootstrapping strategies for extracting RDF resources from unstructured text have been explored in BOA @cite and PATTY @cite . It consists of natural language patterns corresponding to relations present in the knowledge graph. Word embedding models are also frequently used to overcome the linguistic gap for relation linking. RelMatch @cite improves the accuracy of the PATTY dataset for relation linking. There are tools such as ReMatch @cite which uses wordnet similarity for relation linking.
- Reconstruction is a standard concept in auto-encoder, that guides towards learning representations that captures the underlying explanatory factors for the observed input @cite @cite . An auto-encoder model consists of an encoding function to compute a representation from an input, and a decoding function to reconstruct the input from the representation. The parameters involved in the two functions are trained to maximize the reconstruction score , which measures the similarity between the original input and reconstructed input. Inspired by the concept of reconstruction , tu2017neural proposed guiding decoder hidden states to embed complete source information by reconstructing the hidden states back to the original source sentence. Our approach differs at: (1) we introduced not only decoder-side reconstructor but also encoder-side reconstructor to learn enhanced hidden states of both encoder and decoder; and (2) we guide the hidden states to embed complete source information as well as the labelled DP information.
- Recently, it was shown that NMT can be improved by feeding auxiliary information sources beyond the original input sentence. The additional sources can be in various forms, such as parallel sentences in other languages @cite @cite , cross-sentence contexts @cite @cite @cite , generation recommendations from other translation models @cite @cite @cite @cite , syntax information @cite @cite . Along the same direction, we provide complementary information in terms of source sentences labelled with DPs.
- Modeling geometric transformation with neural networks was first explored by capsules,'' computational units that locally transform their input for modeling 2D and 3D geometric changes . Later, @cite demonstrated that similar computational units, named spatial transformers, can benefit many visual recognition tasks. @cite adopted the spatial transformers for synthesizing novel views of the same object and has shown that a geometric method can produce more realistic results compared to pure pixel-based methods. Inspired by these successes, we also use the spatial transformers to deform the input images, but with a different goal: to generate realistic adversarial examples.
- Following the emergence of adversarial examples, various defense methods have been studied, including adversarial training , distillation , gradient masking and feature squeezing . However, these defenses can either be evaded by attacks or only provide marginal improvements . Among these defenses, adversarial training has achieved the state-of-the-art performance. @cite proposed to use the fast gradient sign attack as an adversary to perform adversarial training, which is much faster, followed by ensemble adversarial training and projected gradient descent (PGD) adversarial training . In this work, we explicitly analyze how effective the spatial transformation based adversarial examples are under these adversarial training based defense methods.
- Recently, deep learning has achieved very promising results in visual object tracking @cite @cite @cite @cite @cite @cite . Usually, deep neural networks require a lot of training data. However, only the first frame of a test sequence is annotated in visual object tracking. To overcome this problem, some deep learning based trackers pre-train their neural networks by using auxiliary data, @math million tiny images dataset @cite , face detection dataset @cite and Hans van Hateren natural scene videos @cite . In contrast, our feature learning method does not require any network pre-training on auxiliary data. Some other deep learning based trackers adopt existing deep neural networks, R-CNN model built upon Caffe Library @cite and VGG network pre-trained on ImageNet @cite , and then fine-tune network parameters based on training data of specific target objects. Different from these methods, we learn RNN parameters in the first frame of a test sequence and fix the parameters in the subsequent frames. Therefore, our method does not suffer from fine-tuning network parameters.
- RNN has been successfully applied to natural language processing for sentiment analysis @cite , phrase and sentence modeling @cite and paraphrase detection @cite . Also, RNN is applied for parsing natural scene @cite and @math D object classification @cite . To our best knowledge, we are the first to learn hierarchical features by using RNN for visual object tracking.
- @cite consider a collision-free scheduled access to formulate throughput for both saturated and non-saturated queues in multi-hop wireless networks. However, in case of random access scheme, their analytical model is limited to saturated queues. In this paper, instead of limiting nodes to scheduled access, we study the performance using IEEE 802.11 MAC layer, where collision can occur without assuming saturated queues. In addition, we provide simulation results to verify our model.
- In a similar theoretical-based approach for multicast sessions, @cite derive throughput for multi-hop wireless networks. However, in the coding process, they assume the decoding probability is equal to one and also they postpone transmission of the native packet at a node until receiving a packet from another flow to be combined with the first packet, and thus, causing a long delay.
- Furthermore, @cite propose an analytical framework for bidirectional unicast flows in multi-hop wireless networks. Their work considers collision and different interference levels in CSMA CA (Carrier Sense Multiple Access with Collision Avoidance) by varying carrier-sensing range and signal-to-interference ratio to maximize the throughput in different retransmission schemes. Although their scenario is similar to ours, our work is different in the following aspects: 1) in contrast to their approach, in our model, if a node has a transmission opportunity, it does not delay forwarding native packets to generate coded packets; 2) their focus is on saturated queues, while we work on stable queues.
- Neural Episodic Control(NEC) is one of the method of efficient sampling from Replay Buffer. It is the algorithm based on episodic memory and improved Model-Free Episodic Control (MFEC) @cite to learn end-to-end from state mappings to estimations of Q-values.
- We can perform two kinds of operations for DND, @math and @math . In @math , when @math featured by CNN and corresponding action @math are entered, we lookup the top @math -nearest neighbors for @math in @math using kd-trees @cite . We weight the value @math corresponding to @math keys and we set it as @math .
- Human Activity recognition has been a popular research topic in pervasive computing @cite for its competence in learning profound high-level knowledge about human activity from raw sensor inputs. Several survey articles have elaborated the recent advance of activity recognition using conventional machine learning @cite @cite and deep learning @cite approaches.
- Conventional machine learning approaches have made tremendous progress on HAR by adopting machine learning algorithms such as similarity-based approach @cite @cite , active learning @cite , crowdsourcing @cite , and other semi-supervised methods @cite @cite . Those methods typically treat HAR as a standard time series classification problem. And they tend to solve it by subsequently performing preprocessing procedures, feature extraction, model building, and activity inference. However, they all assume that the training and test data are with the same distribution. As for CDAR where the training (source) and the test (target) data are from different feature distributions, those conventional methods are prune to under-fitting since their generalization ability will be undermined @cite .
- Deep learning based HAR @cite achieves the state-of-the-art performance than conventional machine learning approaches. The reason is that deep learning is capable of automatically extracting high-level features from the raw sensor readings @cite . Therefore, the features are likely to be more domain-invariant and tend to perform better for cross-domain tasks. A recent work evaluated deep models for cross-domain HAR @cite , which provides some experience for future research on this area. There are stll many open problems for deep learning based CDAR. In this paper, we mainly focus on the traditional approaches.
- Transfer learning has been successfully applied in many applications such as Wi-Fi localization @cite , natural language processing @cite , and visual object recognition @cite . According to the literature survey @cite , transfer learning can be categorized into 3 types: instance-based, parameter-based, and feature-based methods.
- Our framework belongs to the feature based category, which brings the features of source and target domain into the same subspace where the data distributions can be the same. A fruitful line of work has been done in this area @cite @cite @cite . The proposed STL differs from existing feature-based methods in the following aspects:
- @cite proposed structural correspondence learning (SCL) to generatively learn the relation of features. @cite applied a feature-level transfer model to learn the dependence between domains, then trained a domain-adapted classifier. Instead of modeling the relationship of domain features, STL transforms the domain data into a new subspace, which does not depend on the domain knowledge in modeling features.
- @cite proposed maximum mean discrepancy embedding (MMDE) to learn latent features in the reproducing kernel Hilbert space (RKHS). MMDE requires solving a semidefinite programming (SDP) problem, which is computationally prohibitive. @cite extended MMDE by Transfer Component Analysis (TCA), which learns a kernel in RKHS. @cite adopted a similar idea. @cite learns target predictive function with a low variance. @cite sampled the domain features by viewing the data in a Grassmann manifold to obtain subspaces. @cite exploited the low-dimensional structure to integrate the domains according to geodesic flow kernel (GFK). Long proposed joint distribution adaptation (JDA) based on minimizing joint distribution between domains, while STL focuses on the marginal distribution. @cite proposed transfer kernel learning (TKL), which learned a domain-invariant kernel in RKHS. @cite studied the conditional transfer components between domains. Methods in those literature tend to learn some common representations in the new feature space, then a global domain shift can be achieved. However, the difference between individual classes is ignored.
- Typical works on text normalization have relied primarily upon string and phonetic similarity in an hierarchical candidate generation and filtering procedure for identifying lexical variations of IV words (). For example, @cite present a technique that generates a confusion set' for IV words by filtering out OOV words using edit distance and phonetic measures, followed by ranking based on a tri-gram language model.
- Multi-person pose estimation in video: Among the most dominant approaches to pose estimation from videos is a two-stage approach, which first deploys a frame-level keypoint estimator, and then connects these keypoints in space and time using optimization techniques. @cite @cite , it is shown that a state of the art pose model followed by an integer programming optimization problem can result in very competitive performance in complex videos. While these approaches can handle both space-time smoothing and identity assignment, they are not applicable to long videos due to the NP-hardness of the IP optimization. @cite propose a CRF with space-time edges and jointly optimize for the pose predictions. Although they show an improvement over frame-level predictions, their method does not consider body identities and does not address the challenging task of pose tracking. In addition, their approach is hard to generalize to an unknown number of person instances, a number that might vary even between consecutive frames due to occlusions and disocclusions. Our approach also follows a two-stage pipeline, albeit with a much less computationally expensive tracking stage, and is able to handle any number of instances per frame in a video.
- Multi-object tracking in video: There has been significant effort towards multi-object tracking from video @cite @cite . Prior to deep learning, the proposed solutions to tracking consisted of systems implementing a pipeline of several steps, using computationally expensive hand-crafted features and separate optimization objectives @cite for each of the proposed steps. With the advent of deep learning, end-to-end approaches for tracking have emerged. Examples include @cite @cite which use recurrent neural networks (RNNs) on potentially diverse visual cues, such as appearance and motion, in order to track objects. @cite , a tracker is built upon the state of the art object detection system by adding correlation features between pair of consecutive frames in order to predict frame-level candidate boxes as well as their time deformations. More recent works have attempted to tackle detection and tracking in end-to-end fashion @cite @cite @cite , and some works have further used such architectures for down-stream tasks such as action recognition @cite . Our work is inspired by these recent efforts but extends the task of object box tracking to address for the finer task of tracking poses in time.
- @cite formulated an influence maximization problem for a given seed set size with two basic diffusion models, namely the Independent Cascade (IC) and Linear Threshold (LT) models. They showed that the influence spreads under these models are submodular and monotone. Thus, they proposed a simple hill-climbing greedy algorithm to address the problem, which can provide a @math -approximation guarantee @cite . The follow-up studies have mainly concentrated on improving the efficiency of the algorithm implementation for large-scale OSNs @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . Different from the above studies, we target at maximizing the profit that accounts for both the benefit of influence spread and the cost of influence propagation in viral marketing.
- @cite presented a markup language to describe biochemical network models named SBML. One of their primary goals was to create a common format that can be used across simulators. Several implementation exist for this specification @cite , @cite .
- The work presented in @cite uses differential equations in order to model changes in the concentration of proteins as well. Although, this work uses gene expression in a specific task, i.e. studying the influence of different protein pathways in the spreading of cancer cells.
- The research @cite introduces a new statistical gene network estimation method based on the dynamic Bayesian network and nonparametric regression model with advantages over Bayesian and Boolean networks. For example, it can detect nonlinear dependencies. This article can be taken into consideration in further work if the statistical approach will be implemented in BioDynaMo.
- Many algorithms on recovering holes in images or videos have been proposed @cite , @cite , @cite , @cite , @cite , @cite , @cite , @cite . Some existing methods for image completion are related to texture synthesis @cite , @cite or patch-based synthesis @cite , @cite , @cite . Efros and Leung @cite proposed a method for predicting pixels from the context boundary while @cite searches for matching patches and quilts them properly. Drori et.al. @cite computed a confidence map to guide filling while Komodakis et.al. @cite proposed a priority belief propagation method. However, these exemplar based approaches struggle to generate globally consistent structures despite producing seamless high-frequency textures. Hays and Efros @cite filled large missing regions using millions of photographs and presented seamless results. However, in this method, missing regions need to be prepared carefully by completely removing partially occluded objects. Synthesizing content for arbitrary missing regions remains a challenging task (e.g., recovering body parts for a partially occluded object).
- Fully convolutional networks (FCNs), which was first used in @cite for semantic image segmentation, provides an end-to-end learnable neural network solution for pixel-level image comprehension. Without fully connected layers, FCNs occupy less memory and can learn and predict more efficiently. Besides, FCNs preserve spatial information and extract location sensitive features. Recently FCNs have achieved excellent results on semantic segmentation @cite , edge detection @cite , saliency detection @cite and other pixel-wise labeling tasks. In this paper, we exploit the idea of FCN in GAN-based inpainting to better capture object contours, preserve spatial information in features, and infer coherent visual content from context.
- Perceptual loss is a feature reconstruction loss defined by deep neural networks @cite . It guides neural models to generate images visually similar to their corresponding targets (e.g., ground truth) and has been widely utilized in style transfer @cite . Dosovitskiy and Brox et.al. @cite presented a similar concept, called DeePSiM, which successfully generates images with sharp details. So far perceptual loss has been applied to style transfer @cite @cite , super resolution @cite and texture synthesis @cite . However, these topics primarily use the texture network'', a part of the VGG network @cite to extract middle-level features while high-level features from the fully connected layers have not been investigated for image completion. In this paper we exploit high-level deep features in the definition of perceptual loss to synthesize regions semantically consistent with their contexts.
- The runtime performance of SearchMC , like that of ApproxMC (2), is highly dependent on the performance of SAT solvers on CNF-XOR formulas. Some roots of the difficulty of this problem have been investigated by @cite @cite .
- Non-randomized approximate model counting using techniques similar to static program analysis is generally faster than randomized approximate model counting techniques, and such systems can give good approximations for some problem classes. However, they cannot provide a precision guarantee for arbitrary problems, and it is not possible to give more effort to have more refined results. Castro @cite compute an upper bound on the number of bits about an input that are revealed by an error report. They measure the entropy loss of an error report by computing the number of bits revealed by subsets of path conditions first and then combining these partial results to get the final result. Meng and Smith @cite use two-bit-pattern SMT entailment queries to calculate a propositional overapproximation and count its instances with a model counter from the computer algebra system Mathematica. Luu @cite propose a model counting technique over an expressive string constraint language. Their tool computes the bounds on the cardinality of the valid string set and uses generating functions for reasoning about the cardinality of string sets.
- Rainbow cycles and paths have also been studied in graphs other than flip graphs. A well-known conjecture in this context due to Andersen @cite asserts that every properly edge-coloured complete graph on @math vertices has a rainbow path of length @math , i.e., a path that has distinct colors along each of its edges. Progress towards resolving this conjecture was recently made by Alon, Pokrovskiy and Sudakov @cite , and Balogh and Molla @cite .
- All the above mentioned works treat the unsupervised domain adaptation problem as a batch transition without exploiting temporal coherence commonly available to robots in continuous deployment. Continuous refinement has however been actively researched in supervised learning for many years ( @cite @cite @cite ), yet there has been little work on methods for unsupervised domain adaptation. One notable exception is the work by Hoffman al @cite , which addresses the problem with predefined features and focuses on the challenges of aligning to a continuously reshaping target domain. This work seeks to extend the recently developed approach of adversarial domain adaption to a continuously evolving target domain by capitalising on the perpetual observations made by a robot.
- Several ensemble methods are proposed in the literature to handle imbalanced datasets @cite @cite . An ensemble method was proposed by @cite which converts a imbalance binary class problem to multiple learning process. The proposed method divided the majority class instances into several sub datasets. Here each sub datasets holds almost same number of minority class instances. Thus several balances datasets were created and used to create a binary classifiers. Then a combination of those classifiers were used to learn an ensemble classifier.
- RUSBoost @cite is a hybrid boosting algorithm using Adaboost with random under sampling as sampling method. From an imbalanced data random under sampling randomly removes instances from major class in each iteration. A Adaboost @cite was used with random under sampling to create the RUSBoost algorithm. Similarly SMOTEBoost was created using Adaboost and a over sampling technique called SMOTE. This method was proposed in @cite . It over-samples the minority class instances using an over sampling technique called SMOTE @cite . By employing @math nearest neighbors of minority class, synthetic instances are generated by operating in feature space. The above mentioned methods which uses sampling inside Adaboost showed impressive performance in terms of area under Receiver Operating Characteristic (ROC) curve. Investigation on the behavior of SMOTEBoost was performed by Blagus and Lusa @cite on imbalanced datasets with high dimensions. Here dataset with high dimension means where there are more features than the instances. They came to the conclusion that, as SMOTE biases the classifier towards minority class, it is necessary to do feature selection.
- DataBoost algorithm or DataBoost-IM method was presented by Hongyu Guo @cite . He proposed an ensemble model which uses data generation. In this algorithm, hard majority and minority class instances are identified during the execution of boosting. Then those hard examples are chosen separately and used to create synthetic instances of respective class. After that those created instances are added to the main dataset. Easy Ensemble is an ensemble method was proposed by Xu-Yung Liu @cite . They create several subsets of majority class instances. Then using each of those datasets it trains a learner. These subsets are created using random under sampling. However, it creates several sub datasets to overcome the main limitation of random under sampling which is it discard instances from majority class randomly regardless of its importance.
- In model-based approaches, GPs are commonly used to learn transition models for agents moving in the real world @cite and have been used in RL to learn the transition function @cite and the reward function @cite . GPs have also been used in model-free approaches for approximating the @math -values in continuous state-action spaces @cite . This was extended to the GP-SARSA algorithm which includes an online action selection and policy improvement steps @cite . The authors used the GP variance to compute confidence intervals around the value estimate. However, they also note that this measure could be used for various exploration strategies.
- The general-purpose approaches we are aware of are Clipper @cite , LASER @cite and Velox @cite . Of these, Clipper may be the closest effort to TensorFlow-Serving; the two systems were developed concurrently. Clipper and TensorFlow-Serving share a focus on remaining largely agnostic to the specific ML technology of the models being served, and have some similar components e.g. batching. Whereas Clipper is a research system used as a vehicle to pursue speculative ideas, TensorFlow-Serving encapsulates the state of the art of production infrastructure as used inside Google and Google Cloud Platform.
- To the best of our knowledge, detecting low rating Android apps with only static program and UI information is a problem that has not been addressed before. Monett and colleagues proposed a technique to predict the star number of Android apps from the user reviews @cite . Similar technique is also used to predict movie scores or other sentiment analysis in natural language processing @cite @cite @cite @cite @cite . The problem of these techniques is that they still require users to provide reviews for Android apps. It suffers the same problem of the current star rating system.
- Another related work to us is Mou and colleagues' work @cite . This approach first embeds the keywords of a programming language as vectors so that similar keywords have closer vectors in the Euclidean space. Then, it uses the token level embeddings to classify the programs from the online judgment platform of the Peking University. To classify the programs, this approach represents the nodes in the ast of a program with the token level embeddings and uses a tree-based convolution neural networks to classify the ast . This task is similar to what we have done in the preliminary study.
- White and colleagues proposed a neural network based techniques to detect program clones @cite . This work first uses recursive neural network based approach to embed tokens of program source code as vectors @cite . It then encodes the source code of program snippets as vectors with a recursive autoencoder @cite . Finally, the program vectors are used to detect program clones. Deckard @cite @cite summarizes the patterns in ast of programs and counts these patterns as vectors. Then it uses machine learning techniques to detect code clones with the tree patterns. Chen and colleagues embeds cfg of programs as a matrix @cite to detect program clones. Unlike , these approaches focus on detecting program clones. They do not detect low rating Android apps. Further, similar to Mou and colleague's work @cite , these approaches also need the source code, while our approach does not.
- Mu and colleagues proposed a machine learning based technique, DroidSIFT, to detect malwares @cite . This approach builds the API dependency pattern database from benign and malicious Android apps. Then it encodes new apps based on its similarities to the API dependency patterns in the databases. Finally, it detects malwares by learning a model from the app encodings. The limitation of this work is that it encodes apps based on the dependency graph for each API. It is very expensive to build the dependency graph for all APIs used by an app. For malware classification, this problem can be alleviated by focusing on a small set of permission related APIs. However, for detecting low rating apps, people cannot only analyze a small set of APIs. In this case, DroidSIFT will encounter the scalability issue.
- Wang and colleagues proposed a neural network based approach for defect prediction on the source code file level @cite . This approach first encodes programs based on the token vector of AST nodes. Then it uses a deep belief network for feature dimension reduction. Finally, it trains a model to classify buggy files with the features with reduced dimension. This technique cannot be directly used to detect low rating apps because of two reasons. First, it requires source code. Second, the size of the token vector can be too large for machine learning models on the whole application level.
- Bag of word approaches are used for malware detection @cite @cite @cite @cite @cite @cite @cite @cite @cite . Compared with these approaches, addresses a very different problem. Furthermore, as we evaluated in sec:rq1 , our approach significantly outperforms the bag of word approach regarding classification accuracy.
- Many approaches also use machine learning techniques to generate code snippets from natural language queries @cite @cite @cite . These techniques learn a translation model from the API sequences and their comments. Then, when people provide the model a natural language query, the model will generate the API sequence. Other techniques are also proposed for API patterns mining @cite @cite @cite . Despite the usefulness of these techniques, they address very different problems as we addressed in this paper.
- There are also a group of studies that examine the relationship between the rating and features of apps. Tian and colleagues @cite found that the size, code complexity, and other 15 features have positive correlations with the star ratings. Linares-Vasquez and colleagues @cite studied the relationships of API changes in Android apps and star ratings. Ruiz and colleagues @cite studied the correlation between ad libraries and ratings of Android apps. Gui and colleagues @cite studied the relationship between energy consumption and ratings of Android apps. Although the conclusions of these techniques are interesting, they do not have a method to predict or classify low rating apps.
- There are also hybrid approaches that combine overview and tabular approaches or overview and projection approaches. In hybrid overview-tabular approaches, the rows are preserved within subsets of the data, but the relationships between subsets are visualized using an overview technique. Examples of this class include @cite , @cite , @cite , @cite , and @cite . In hybrid overview-projection approaches, selected attributes are plotted on top of a plot of projected data, as in the technique developed by @cite . In the following, we limit our discussion to tabular and hybrid techniques.
- Domino @cite is a hybrid tabular overview technique. It is based on the concept of placing subsets of a dataset on a canvas and choosing a suitable representation for it ). Multiple subsets can then be connected to show their relationships in various ways. @cite , VisBricks @cite and StratomeX @cite @cite are related hybrid techniques, but are more restricted with respect to the selection of subsets.
- Both Matchmaker and VisBricks focus on numerical matrices and facilitate the comparison of different groupings or clusterings. Other techniques, such as the hierarchical cluster explorer @cite , @cite , @cite , and @cite , focus on visualizing clustered matrices. Taggle, in contrast, supports both numerical matrices and heterogeneous tables in a single visualization.
- Bertifier @cite is a table-visualization technique inspired by Jacques Bertin's matrix analysis methods. It uses various visual encodings for cells , ) that can be interactively re-ordered based on similarities between rows and columns. Other features, such as styling options for the table grid, indicate that the technique is intended mainly for presenting small or medium-sized tables. Bertifier does not support aggregation ) or grouping ) and is limited to numerical data.
- Complementary to the three types of tabular data visualization techniques introduced in the previous section, there is an option of representing (groups of) attributes of a tabular dataset independently of each other in a multiple coordinated view system. Representative systems in this category include @cite and the recent @cite . These systems allow users to choose representations that are suitable for the subset of data represented by a single view, and usually rely on linked highlighting to re-introduce connections. Common configurations of Keshif, for example, use a tabular view to identify specific items, but represent other attributes in other views using, for instance, histograms or bar charts.
- While MCV systems can leverage visualization techniques that are ideal for certain attributes and that would potentially not fit into the confines of a tabular layout, they also add complexity and increase the cognitive load for the user @cite . Tabular layouts, in contrast, make the association of all attributes to their item easy, but make it harder to see correlations between attributes or trends across the whole dataset.
- Orthogonal to the design space discussed above are aggregation methods for the items within a table: representing the underlying distribution or statistical measures of a set of items is an important approach to increasing the scalability of visualization techniques. Aggregation can be applied to a whole dataset or to multiple groups of items and or attributes separately. Examples of overview techniques are hierarchical parallel coordinates @cite , which visualize cluster centroids rather than individual items, and VisBricks @cite , which can visualize clusters using various techniques, including aggregation methods. An example that predominantly uses aggregations is Keshif @cite , where a table of items is supplemented with multiple views showing distributions for interaction-driven exploration. To our knowledge, there is currently no interactive general tabular visualization technique that allows aggregation.
- Elmqvist and Fekete @cite proposed several design guidelines for aggregation, including: ---aggregates should convey information about the underlying data; ---aggregates can easily be distinguished from individual data items; and ---measures are taken to counteract artifacts of the aggregation process that misrepresent true effects. The aggregation techniques in Taggle were designed with these guidelines in mind.
- There are various specialized tabular visualization tools that use aggregation in tabular layouts. @cite aggregates amino acid sequences and associated metadata using the most frequent category or the average to represent aggregated items, depending on the data type. @cite use the average for numerical values for aggregates. Both techniques employ transparency to communicate fidelity (the higher the variation in a cell, the higher the transparency), but neither addresses fidelity well. The technique by Conklin and North @cite aggregates rows or columns of a table based on a pre-existing aggregation hierarchy. Users can traverse the hierarchy and pivot through intersecting hierarchies.
- Pivot tables are another way of dealing with the analysis of large multidimensional data. They enable rotation, that is, pivoting of various data dimensions, and with use of data aggregation, such as sum, average, or count, they provide a quick data overview. Pivot tables are widely available in spreadsheet tools such as Excel and Google Sheets, and similar operations are available as part of libraries and programming languages used to create static tabular figures. Tableau @cite employs pivot table principles for constructing nested matrices, where each matrix can then be represented visually in different ways. While pivot tables are well suited to, for instance, summing up the sales figures of multiple departments, they do not commonly support more nuanced aggregations, such as histograms and box plots.
- Visual aggregation is often employed not only to minimize screen space, but also to provide additional contextual information. For example, tabular techniques such as LineUp @cite and DataComb @cite show histograms of column data atop each column, while @cite shows histograms of items that were filtered out at the bottom of the table.
- Several methods for jointly sparse recovery have been proposed in the literature @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . amp was introduced in @cite @cite @cite as a large system relaxation of loopy belief propagation to solve a random linear system with sparsity constraint. Scalar bamp , its Bayesian version @cite @cite , uses the signal prior explicitly and is an efficient approximate mmse estimator. The turbo bamp methods in @cite @cite @cite , and their generalization in @cite for clustered sparse signals, improve the recovery performance by exchanging extrinsic information about the current support estimate in each message passing iteration. In @cite @cite @cite , joint sparsity is directly enforced by an appropriate vector estimator (denoiser) function for the bg prior.
- The se formalism developed in @cite @cite @cite analytically predicts the recovery performance of (B) amp algorithms. se was employed to analyze bamp for joint sparsity with a vector estimator and to point out the difference between the dcs and mmv scenarios in @cite . Recent works rigorously prove the se for non-separable non-linearities @cite and a class of sliding-window denoisers @cite with Gaussian i.i.d. measurement matrices. Furthermore, the se of the Vector AMP has been derived for a large class of right orthogonally invariant random sensing matrices @cite . (We highlight that the acronym Vector AMP should not to be confused with the vector-prior version of BAMP, considered in this paper for the MMV DCS problems.)
- In @cite , the replica method (a statistical physics tool for large disordered systems) is used to calculate the mmse of the cs measurement (note that @cite refers to mmv and dcs as MMV-2 and MMV-1, respectively). The replica trick non-rigorously simplifies the high-dimensional integral for the mmse of the Bayesian estimator of the cs channel, thereby leading to the free energy as a function of the mse . The local maxima in the free energy function correspond to stable fixed points of bp and bamp and thus predict the expected mse of bamp . The replica analysis in @cite is performed for the bg signal prior with uncorrelated isotropic unitary signal and uncorrelated isotropic Gaussian noise distribution, i.e., with a single noise parameter.
- Existing research literature focused mostly on retrieval, detection, and recognition of a reduced number of logos @cite @cite @cite @cite @cite @cite and, consequently, a number of datasets were introduced. The most representative large public logo datasets are shown in Table . Due to the low diversity of the contained logos, these datasets are not suitable for learning and validating automatic logo generators. At the same time a number of web pages allow (paid) access to a large number of icons, such as iconsdb.com (4135+ icons), icons8.com (59900+), iconfinder.com (7473+), iconarchive.com (450k+) and thenounproject.com (1m+). However, the diversity of these icons is limited by the number of sources, namely designers artists, themes (categories) and design patterns (many are black and white icons). Therefore, we crawl a highly diverse dataset -- the Large Logo Dataset (LLD) -- of real logos in the wild' from the Internet. As shown in Table our LLD proposes thousands of times more distinct logos than the largest public logo dataset to date, WebLogo-2M @cite .
- @cite proposed an ensemble method for addressing binary-class imbalance problems by converting an imbalanced binary learning process into multiple balanced learning processes. In this method, the majority class instances were divided into several groups sub-data sets, where each sub-set has the similar number of minority class instances. So, several balanced data sets were generated. Then, each balanced data set was employed to build a binary classifier. Finally, these binary classifiers were combined into an ensemble classifier to classify new data.
- @cite proposed an over-sampling approach called SMOTE (Synthetic Minority Over-sampling TEchnique) algorithm in which the minority class is over-sampled by creating synthetic minority class instances rather than by over-sampling with replacement. SMOTE generated synthetic instances by operating in rather than employing @math minority class nearest neighbors. Their result showed that the combination of over-sampling with under-sampling performed better in Receiver Operating Characteristic (ROC) space. @cite implemented a cluster-based (k-means) over-sampling approach where SMOTE was adapted to oversample clusters with reduced sizes. This work considered merging the minority class instances from the multiple over-sampled datasets. Blagus and Lusa @cite investigated the behavior of SMOTE on high-dimensional imbalanced data, where the number of features greatly exceeds the number of training instances. They found that feature selection is necessary for SMOTE with k-nearest neighbors (kNN), as SMOTE strongly biases the classifier towards the minority class.
- @cite presented a new hybrid sampling boosting algorithm, called RUSBoost, which applied random under-sampling (RUS) with AdaBoost algorithm. RUS randomly removes the majority class instances to form a balanced data. RUSBoost was built based on the SMOTEBoost (synthetic minority over-sampling with AdaBoost) algorithm @cite . SMOTEBoost was built upon over-sampling approach with AdaBoost algorithm. @cite presented an ensemble algorithm by evolutionary under-sampling (EUA) approach, called EUSBoost, to classify highly imbalanced datasets. EUS generated several sub-datasets using randomly under-sampling technique and obtained a best under-sampled dataset of the original dataset that cannot be improve further. RUSBoost, SMOTEBoost, and EUSBoost applied data sampling techniques into the AdaBoost.M2 algorithm by considering the minority class instances and the majority class instances. Yen and Lee @cite presented a cluster-based under-sampling approach to cluster all the training instances (majority class instances and minority class instances) into some clusters. This approach selected a suitable number of majority class instances from each cluster by considering the ratio of the number of majority class instances to the number of minority class instances in the cluster.
- Early work on IMEs include trigram statistical language model @cite and joint source-channel model @cite that is applicable to Chinese, Japanese and Korean languages. Recent studies pay more attention on IME input typo correction @cite @cite along with various approaches on Chinese spell checking @cite @cite @cite . There are also stuides on Janpanese IME which focus on kana-kanji conversion, including probabilistic language model based method @cite , discriminative method @cite @cite @cite , improved @math -pos model @cite , and ensemble method @cite . However, all the above are based on predefined vocabulary and did not exploit the interactive property of IME inputting. @cite and @cite tried to address unknown word problem, but they still require an initial vocabulary or a segmented training corpus.
- This work differs from all existing scientific and engineering solutions for the following aspects. First, we focus on adaptive word acquisition for IMEs, but not for any standard machine learning tasks such as all the recent work about Chinese word segmentation, as IME word definition is quite different from all these tasks @cite @cite @cite @cite @cite @cite @cite @cite @cite . Second, the interactive property of IMEs is to be fully exploited while was never seriously taken into account in previous work. Third, the proposed method will be optimized for each user but not all users' statistics and keep updates nearly at real time.
- There is currently very little literature on autonomous drones applied to cinematography. Recently, @cite proposed an interesting approach to control quadrotors for lighting purposes. They present a solution to automatically achieve a specific lighting effect on dynamic subjects using a drone equipped with a fixed portable light source. Their solution processes the images from a static camera to compute the 3D motion commands for the UAV. Closer to our work, in @cite , the authors address the challenge of autonomously performing camera shots with quadrotors. They present an interactive tool that allows users to design physically plausible trajectories by visually specifying shots. They use a virtual environment to compose and preview the shots. Their tool however remains limited to outdoor environment. It also requires to manually craft the path beforehand and does not allow to track targets in real-time.
- Path planning has been challenging the research community for decades and the amount of literature on the matter is significant. In the robotic field especially, a number of approaches have addressed the problem. However, a large amount of this research was dedicated to ground vehicles and therefore did not fully exploit the capacities of UAV. Looking at the research conducted on path planning by the computer graphics community, the specific task of virtual camera control happens to be strongly related to our research topic due to the similar properties of drones and virtual cameras. In @cite , Christie review a large spectrum of the literature on intelligent camera control, mainly consisting of optimization-based or constraint-based approaches. More recently, in @cite , Lino proposed an algebraic solution to the problem of placing a camera given a set of visual properties. This seminal work on static camera placement was later used to propose camera path planning solutions. In Lino2015 and @cite , the authors detail offline solutions to the problem. Closer to our problem, @cite @cite detailed a reactive approach based on steering behaviors.
- Early work by @cite explored trusted hardware (Java iButton) to protect the initial PSK of the Schneier and Kelsey scheme @cite . Later, @cite suggested a using a TPM with a forward integrity scheme based on branched key chaining. Logs are divided into epochs (blocks), each comprising a sequence of hash-chained log entries (sub-epochs). The root entries of each epoch are hash-chained with past epochs, which creates a two-dimensional hash chain to prevent in which an attacker re-orders log blocks to mislead auditors. For each new epoch, the previous epoch's logs are securely stored using the TPM's seal functionality, which encrypts the logs with a TPM-bound key so only that particular TPM can decrypt unseal' them. B "o @cite explore the use of AMD's Secure Virtual Machine (SVM) -- an early inception of the TEE -- for launching a syslog client daemon and logging application from the TPM's secure boot chain. The logger executes with access to TPM-bound key-pairs for encrypting and signing log entries. Upon request, the logs are decrypted and transmitted to the verifying party; the TPM keys are certified for authenticating that signed logs originated from the SVM.
- @cite propose streaming medical logs to a server application in Intel SGX (see Section ) that applies the tamper-resistance. Logs are sent to the Intel SGX application ( enclave') over TLS, which computes a hash chain comprising a signature of each record; TPMs are used to authenticate the medical devices to the server, and on the server's end to securely store log hash chains using its sealing mechanism. @cite introduce SGX-Log, which protects server-side device logs received from remote devices. SGX-Log, like @cite , uses block-based hash chains with SGX's secure storage for log integrity and confidentiality. The authors note that continual sealing also provides resilience to attacks in which large volumes of logs in memory are lost due to an unauthorised power loss. Remote attestation is also suggested to authenticate the server enclave before transmitting the logs. The proposed scheme is evaluated using three datasets, yielding a small ($<7
- Modern TPM- and TEE-based approaches @cite @cite @cite @cite still fall short of satisfying many desirable properties identified in past work. Public verifiability of origin, as in @cite , has not been addressed in recent TEE loggers, which could be potentially useful to authenticate system data from remote devices, e.g. generating trust scores from log data for access control @cite and continuous authentication @cite @cite . Recent TEE-based schemes, i.e. @cite and @cite , focus primarily on protecting logs being received by a server-side log processing application; an attacker on the source device may simply tamper the logs before reaching the server that applies some tamper-resistance algorithm. To complicate matters, source devices are unlikely to transmit logs in real-time to minimise network and computational overhead, and so secure storage methods should be used to preserve unsent logs. Additionally, TEEs typically contain other security-critical applications, e.g. for fingerprint matching (as in Android https: source.android.com security authentication fingerprint-hal ) and payment tokenisation (see Samsung Pay http: developer.samsung.com tech-insights pay device-side-security ). As a result, a TEE-based logging mechanism should operate with reasonable resource consumption, e.g. run-time memory, to limit the rise of Denial of Service (DoS) conditions.
- Another popular approach for ZSL is based on learning a shared embedding of seen and unseen class instances into the class-attribute space @cite @cite . After projection, nearest neighbor methods can be used to find the most similar class attribute vector for the (projected) test instance, which corresponds to the most likely class. While conceptually simple and easy to implement, these methods suffer from shortcomings such as the hubness problem @cite .
- Using the fact that the class-attributes can be used to compute relationships between seen and unseen classes (e.g., using a class-attribute based similarity measures), a number of ZSL methods have been proposed that are based on representing the parameters representing each unseen class as a similarity-weighted combination of the parameters representing the seen classes @cite @cite @cite .
- The generalized ZSL problem @cite @cite where the training and test classes are not disjoint is considerably more challenging as compared to the traditional ZSL, and a recent focus has been to design ZSL methods that can work robustly in this setting without being biased towards predicting seen classes. Generative models @cite @cite @cite @cite are promising in this setting. One of the ways these models can solve the GZSL problem is by generating synthetic labeled examples from the unseen classes and then using these examples (and the labeled examples from other seen classes) to train a classification model.
- Following this approach, and in a similar spirit to our work, a number of recent works @cite @cite have tried to use synthesized examples both for the seen as well as unseen classes to perform the generalized zero-shot task. @cite synthesize samples for each class by approximating the class conditional distribution of the unseen classes based on the learned class conditional distributions of the seen classes and their corresponding attribute vectors. On the other hand @cite perform adversarial training to train generators and use the domain adapted samples for perform classification.
- Finally, the ability to generate exemplars from unseen class and use them in training classification models can also help mitigate the domain-shift problem @cite encountered by traditional ZSL methods if the distribution of seen classes and unseen classes are not the same. Given labeled examples from the seen classes and the synthetic labeled examples from the unseen classes, supervised semi-supervised domain adaptation methods can be readily applied to address the domain shift problem.
- A short and constructive proof is available at @cite . In fact, the Erds-Gallai condition is not the only sufficient and necessary condition for an integer sequence to be graphical ; there are some other (equivalent) conditions, notably listed in @cite . The corresponding realization problem for had also been solved quite early ; see the interesting note @cite for a complete history and presentation of the many variants.
- For @math , the number of labeled graphs with a given degree sequence is asymptotically known in many asymptotic regimes, see notably @cite , [Theorem 2.16] MR1864966 and references therein. For general @math , this question has been recently adressed in @cite in the regime where the maximal degree is uniformly bounded. The motivation came from the Benjamini-Schramm topology of rooted graphs.
- A very effective and powerful set of models are based on translation vectors. These models represent entities as vectors in @math -dimensional space, @math and relations as translation vectors from head entity to tail entity, in either same or a projected space. TransE @cite is one of the initial works, which was later improved by many works [ @cite , @cite , @cite , @cite , @cite , @cite ]. Also, there are methods which are able to incorporate text data while learning KG embeddings. @cite is one such method, which assumes a combined universal schema of relations from KG as well as text. @cite further improves the performance by sharing parameters among similar textual relations.
- While the vector space models perform well in many tasks, the semantics of learned representations are not directly clear. This problem for word embeddings was addressed by @cite where they proposed a set of constraints inducing interpretability. However, its adaptation for KG embeddings hasn't been addressed. A recent work @cite addressed a similar problem, where they learn coherent semantic features for entities and relations in KG. Our method differs from theirs in the following two aspects. Firstly, we use vector space modeling leading directly to KG embeddings while they need to infer KG embeddings from their probabilistic model. Second, we incorporate additional information about entities which helps in learning interpretable embeddings.
- In the past, various filtering approaches have been proposed to process mesh geometry. Early work from Taubin and applied low-pass filters on meshes, which remove high-frequency noises but also attenuate sharp features. Later, Taubin proposed a two-step approach that first performs smoothing on face normals, followed by vertex position updates using anisotropic filters. To enhance crease edges, applied anisotropic diffusion to mesh normals before updating vertex positions. Chuang and Kazhdan @cite developed a framework for curvature-aware mesh filtering based on the screened Poisson equation.
- Feature-preserving signal smoothing can also be achieved via optimization. Notable examples include image smoothing algorithms that induce sparsity of image gradients via @math -norm @cite or @math -norm @cite regularization. These approaches were later adapted for mesh smoothing and denoising @cite @cite @cite . Although effective in many cases, their optimization formulation only regularizes the signal difference between immediately neighboring faces. In comparison, our optimization compares signals within a neighborhood with user-specified size, which provides more flexibility and achieves better preservation of large-scale features.
- From a signal processing point of view, meshes can be seen as a combination of signals with multiple frequency bands, which also relates with the scale space analysis @cite . Previous work separate geometry signals of different frequencies using eigenfunctions of the heat kernel @cite or the Laplace operator @cite @cite . Although developed with sound theoretical foundations, such approaches are computationally expensive. Moreover, as specific geometric features can span across a wide range of frequencies, it is not easy to preserve or manipulate them with such approaches. The recent work from @cite provides an efficient way to separate and edit geometric features of different scales, harnessing the scale-aware property of the rolling guidance filter. Our SD filter also supports scale-aware processing of geometry signals, with more robustness than RGNF thanks to the incorporation of both static and dynamic guidances.
- Non-uniform coordinate selection has been proposed first for constant (non-adaptive) probability distributions @math over @math . In @cite , @math is proportional to the Lipschitz constant of @math . Similar distributions are used in @cite @cite for strongly convex @math in .
- In contrast, our principled approach leverages a bandit algorithm to learn a good estimate of @math ; this allows for theoretical guarantees and outperforms the state-of-the-art methods, as we will see in . Furthermore, our approach does not require the cost function to be strongly convex (contrary to e.g., @cite @cite )
- Bandit approaches have very recently been used to accelerate various stochastic optimization algorithms; among these works @cite @cite @cite @cite focus on improving the convergence of SGD by reducing the variance of the estimator for the gradient. A bandit approach is also used in @cite to sample for CD. However, instead of using the bandit to minimize the cost function directly as in B , it is used to minimize the variance of the estimated gradient. This results in a @math convergence, whereas the approach in our paper attains an @math rate of convergence. In @cite bandits are used to find the coordinate @math whose gradient has the largest magnitude (similar to GS). At each round @math a stochastic bandit problem is solved from scratch, ignoring all past information prior to @math , which, depending on the number of datapoints, might require many iterations. In contrast, our method incorporates past information and needs only one sample per iteration.
- Recovering 3D properties from a single image is one of the most fundamental problems of computer vision. Early works mostly focused on developing analytical solutions and optimization techniques, with zero or minimal learning @cite @cite @cite @cite @cite . Recent successes in this direction include the SIRFS algorithm by Barron and Malik @cite , the local shape from shading method by Xiong al @cite , and polynomial SFS'' algorithm by Ecker and Jepson @cite . All these methods have interpretable, glass box'' models with elegant insights, but in order to maintain analytical tractability, they have to make substantial assumptions that may not hold in unconstrained settings. For example, SIRFS @cite assumes a known object boundary, which is often unavailable in practice. The method by Xiong al assumes quadratically parameterized surfaces, which has difficulties approximating sharp edges or depth discontinuities.
- Learning-based methods are less interpretable but more flexible. Seminal works include an MRF-based method proposed by Hoiem al @cite and the Make3D @cite system by Saxena al. Cole al @cite proposed a data-driven method for 3D shape interpretation by retrieving similar image patches from a training set and stitching the local shapes together. Richter and Roth @cite used a discriminative learning approach to recover shape from shading in unknown illumination. Some recent works have used deep neural networks for predicting surface normals @cite @cite or depth @cite @cite @cite and have shown state-of-the-art results.
- Learning-based methods cannot succeed without high-quality training data. Recent years have seen many efforts to acquire 3D ground truth from the real world, including ScanNet @cite , NYU Depth @cite , the KITTI Vision Benchmark Suite @cite , SUN RGB-D @cite , B3DO @cite , and Make3D @cite , all of which offer RGB-D images captured by depth sensors. The MIT-Berkeley Intrinsic Images dataset @cite provides real world images with ground truth on shading, reflectance, normals in addition to depth.
- In addition to real world data, synthetic imagery has also been explored as a source of supervision. Promising results have been demonstrated on diverse 3D tasks such as pose estimation @cite @cite @cite , optical flow @cite , object reconstruction @cite @cite , and surface normal estimation @cite . Such advances have been made possible by concomitant efforts to collect 3D content needed for rendering. In particular, the 3D shapes have come from a variety of sources, including online CAD model repositories @cite @cite , interior design sites @cite , video games @cite @cite , and movies @cite .
- The majority of eye center localization methods are hand-crafted approaches and can be divided into shape and appearance based methods. In the iris recognition literature there are also many segmentation based approaches, such as methods that employ active contours. An extensive overview is given by Hansen and Li @cite . Shape-based techniques make use of the circular or elliptical nature of the iris and pupil. Early methods attempted to detect irises or pupils directly by fitting circles or ellipses. Many techniques have roots in the iris recognition and are based on the integrodifferential operator @cite . Others, such as Kawaguchi al @cite , use blob detection to extract iris candidates and use Hough transform to fit circles to these blobs. Toennies al @cite also employ generalized Hough transform to detect irises, but assume that every pixel is a potential edge point and cast votes proportional to gradient strength. Li al @cite propose the Startburst algorithm, where rays are iteratively cast from the current pupil center estimate to detect pupil boundaries and RANSAC is used for robust ellipse fitting.
- Recently, some authors focused on robust eye center localization without an explicit segmentation of the iris or the pupil. Typically, these are either voting or learning-based approaches. The method of Timm and Barth @cite is a popular voting based approach where pixels cast votes for the eye center based on agreement in the direction of their gradient with the direction of radial rays. A similar voting scheme is suggested by Valenti and Gevers @cite , who also cast votes based on the aforementioned alignment but rely on isophote curvatures in the intensity image to cast votes at the right distance. Skodras and Fakotakis @cite propose a similar method but use color to better distinguish between the eye and the skin. Ahuja al @cite improve the voting using radius constraints, better weights, and contrast normalization.
- While the above methods are accurate, they still lack robustness in challenging in-the-wild scenarios. The success of discriminative cascaded regression for facial feature alignment prompted the use of such methods for eye center localization. @cite @cite start by detecting the face and initializing the eye center estimates using anthropometric relations. Subsequently, they use a cascade of regression forests with binary pixel difference features to estimate the eye centers. Inspired by the recent success of the SDM method for facial feature alignment Zhou al @cite propose a similar method for eye center localization. Unlike the original SDM work, their regressor is based on a combination of SIFT and LBP features. Moreover, unlike @cite @cite who regress individual eye centers, Zhou al estimate a shape vector that includes both eye centers and eye contours. In line with this trend we develop a new regression-based eye center estimator, but additionally employ circle-based refinement and voting-based techniques to get an accurate detector that is easy to train.
- CNNs have been used in driving like scenarios since 2005 when Muller @cite proposed a CNN based off road driving robot, DAVE, that mapped images to steering angles. In a more recent publication in 2015, Huval @cite described a CNN system that detects vehicles and lane markings for highway driving showing that CNNs have promise in autonomous driving.
- Virtual data has had a successful history in computer vision. (2007) used a computer game Half-Life to create a system for evaluating tracking in surveillance systems @cite . In a paper released in 2016 @cite video game data was used to augment real datasets to provide coverage for scenarios that were difficult to find data for in the real world.
- Since early on in the development of self-driving technology physical test tracks have been used to evaluate self-driving algorithms @cite . The final benchmark for system's performance will be real world testing, but with rigorous and complex examples of labeled datasets it is possible to evaluate many corner cases. In 2012 released the KITTI dataset which has become the preeminent dataset in testing self-driving technologies. @cite The value in the labeled data stems from the complexity and the number of examples it contains. Just as the KITTI dataset provided immensely useful labeled data to train and test on, it is possible to use a virtual environment to create varied and difficult test cases for self-driving technologies.
- Traditionally, approaches to model a system's reaction to visual input include behavior reflex and mediated perception. The behavior reflex approach uses learning models which internalize the world model. Pomerleau utilized this method to map images directly to the system's actions such as steering angles and speeds @cite . This scheme extensively exploits the capability of deep learning. Nevertheless, it obscures the decision process being made by the network, leaving only a black-box system that handles exceptionally high-risk tasks. This is certainly an undesirable property to manufacturers and users.
- On the other end of the spectrum, mediated perception detects important features using learned models and then builds a world model based on these features. For example, a mediated perception system uses a combination of many subsystems such as vehicle detectors, and pedestrian detectors to locate as many objects as it can in the driving scene. The approach often creates unnecessary complexity by extracting information irrelevant to the driving task. With consideration for the shortcoming of these approaches, @cite proposed a direct perception approach.
- Software verification is a very active research field. However, the area of games has been only partially investigated. In @cite , a case study of verification of role-playing game (RPG) is discussed. The authors use the Algebraic Petri Nets Analyzer (ALPiNA) model checker to verify reachability properties of the model, represented by the Petri Nets. The results of the work are very promising, however the game model has not been formally defined, and the necessary steps for constructing it from the game specification were not explicitly discussed. Moreover, examples of more complex game properties have not been studied.
- Robotic system can consist of interacting intelligent components or agents. Such systems are called multi-agent. Gaia methodology was introduced in @cite in order to provide an instrument for building a model of the multi-agent system from its specification. The model is based on role notion with attached responsibilities, permissions and activities. Responsibilities are introduced in order to specify goals of each role reaching which guarantees liveness (something good eventually happen) or safety (something bad never happens) properties of the multi-agent system. Target model gives high level understanding of the system and can help to insure logical correctness but does not impact significantly on reliability of the code.
- Model Checking approach can be also a valuable tool for formal simulation and verification in swarms and swarm robotics, where multi-agent dynamic system behavior is frequently described by directed and undirected graphs and their geometric representations in a particular motion space @cite . These graphs @math typically take into account the communication, information flow and interaction scenarios between agents @math in a swarm @math by a weighted graph representation @math , where @math is a vertex (node) set of agents @math with @math , @math is an edge set, which describes an information link @math between a pair @math of agents, and a weighting set @math defines the desired weights @math that represent a control goal or a critical parameter (e.g., a distance between @math ). The structure and properties of @math strictly depend on the hardware and software features of the swarm agents.
- Other recent work built upon semantic parsing by explicitly considering perception at the same time as parsing . These approaches used limited supervision to learn models that connect language to sets of objects, attributes and relations. Other work used attention-based neural networks to answer questions about images @cite @cite @cite , which do not generally use an intermediate parse. Although uses full supervision to train its semantic parser and grounding classifiers, it is able to additionally learn to understand verb phrases such as pick up'' and put,'' which are challenging because such learning involves parameter estimation and inference over state sequences.
- Supplementary information based methods usually require additional knowledge such as 3D geographical models @cite , scene depth @cite , multiple images of the scene under different weather conditions @cite , polarization filters @cite , and so forth. Nevertheless, these methods are mostly computationally intensive and not applicable to dynamic scenes. Much attention, therefore, has been devoted to single image dehazing methods @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . In @cite , He al observed an interesting phenomenon of haze-free outdoors images that at least one channel has some pixels with very low intensities. Based upon this prior, the transmission and global atmospheric light were roughly estimated. After that, the dehazed image was achieved based on the refined transmission by soft-matting @cite or guided filter @cite as well as the estimated global atmospheric light. In @cite , Berman al proposed a nonlocal image dehazing method, which relied on the assumption that colors of a haze-free image are well approximated by a few hundred distinct colors that form tight clusters in RGB space.
- There have been several interesting research works in recent times, in the image processing and computer vision fields, indicating the superior performance of dictionaries constructed via supervised learning for pattern classification tasks, mostly by incorporating a discriminative criterion into the objective function. These algorithms fall under two categories. The first of these @cite @cite @cite @cite @cite treats dictionary learning and classifier training as two separate processes, which uses classifiers like SVM @cite trained on sparse dictionary based features in the final stage.
- More sophisticated approaches @cite @cite unify these two processes into a mixed reconstructive and discriminative formulation. They learn simultaneously an over-complete dictionary and multiple linear classification models for each class. Supervised sparse representation methods like Discriminative KSVD (DKSVD) @cite and its extension Label Consistent K-SVD (LCKSVD) @cite , learn atoms of dictionary based on the traditional K-SVD algorithm. It incorporated the classification error into the objective functions in order to enhance the classification ability of coefficients.
- In this paper, we will be learning class specific dictionaries that are adversarial in nature. By adversarial we mean each of these class specific dictionaries is learned in such a manner that apart from doing a good job at representing samples of their own class, they also have to do a poor job at representing samples belonging to any other class. This allows us to learn a structured dictionary where atoms have correspondence to the class labels so that the reconstruction error associated with each class can be used for classification and hence can act as a stand-alone classifier as opposed to @cite @cite @cite @cite @cite , which relies on expensive classifiers like RBF Kernal SVM trained on sparse coefficients for making final predictions. We propose a direct method to train these dictionaries in a multi-class setting, unlike @cite @cite , that deal with multiclass problem into multiple independent binary classification tasks and these might not scale well for large number of classes and are often computationally expensive. Our method is unique as compared to previous approaches as they are designed to capture inter-class differences in a much better manner even for multi-class problem instances.
- For textual data, @cite propose the reverse of the architecture we present here; an LDA model is used to produce a contextual feature vector that is input into a recurrent neural network for contextually-aware language modeling. While powerful, this model does not address the model's dependence on standard word features and employs a BOW assumption. In @cite , the BOW assumption is relaxed. Instead, a convolution operation maps variable length text sequences into a low-dimensional latent space. Unlike the work presented here, simple distance-based clustering is applied to discover semantically similar documents in place of a Bayesian topic model. For image data, @cite introduce a hybrid neural-Bayesian topic model based on a Deep Boltzmann Machine (DBM). As in this work, the feature representation discovered by the DBM is fed directly into an HDP topic model to discover visual topics. However, instead of discovering features directly from image data, SIFT features are extracted from the image and the neural network learns an image representation based on these features, thereby not reducing the dependence on human-designed features.
- There has been a great deal of research in finding more biologically-plausible alternatives to back-propagation of errors. Classically, the online alternative to back-propagation of errors was real-time recurrent learning (RTRL, @cite ), which employs forward-mode differentiation to compute gradients. However, this algorithm scales poorly, i.e., quadratically in the number of parameters. Some algorithms have been proposed to reduce the complexity of RTRL, including the @cite and @cite algorithms, but are noisy and slow down the learning procedure in trying to approximate the way back-propagation through time works in an online fashion.
- The contrastive divergence recipe , well-known for being the primary learning algorithm of restricted Boltzmann machines, and the Wake-Sleep algorithm for training deeper Boltzmann-based architectures, were largely inspired by the role of sleep in human learning. However, these approaches to learning, which rely on Markov Chain Monte Carlo methods, suffer from a variety of problems including slow convergence to steady-state distributions due to poor mixing of modes and the constraint that the weights of the model must be symmetric. With some success, Boltzmann-based architectures have been applied to stateful problems @cite @cite @cite @cite , but require hybridizing the Contrastive Divergence approach with back-propagation through time, incurring the limitations and criticisms of both algorithms. Other approaches inspired by Boltzmann-based learning (and energy-based learning in general) include the algorithm and . However, these algorithms have only been investigated on static modeling problems and it is not clear how one might extend them to stateful problems.
- The desire for useful local learning, of which target propagation represents a strong modern step towards, is not new, and saw a small resurgence in the early days of training deeper networks in the form of layer-wise training of unsupervised models , supervised models , and semi-supervised models @cite @cite (also known as hybrid training). However, among the many problems with these early approaches to deep learning was the lack of global coordination. Global coordination means that higher-level layers essentially direct lower-level layers in what patterns they should be extracting. A lower-level feature detector might be able to find different aspects of structure in its input since multiple patterns might satisfy its layer-wise objective. However, this detector will only locate the right bit of structure needed for the whole model to make sense, at any time-step, if a higher-level layer signals what pattern it should be looking for. Since greedy layer-wise approaches build the model from the bottom-up, freezing the learned lower-level parameters, this coordination is impossible to achieve.
- The TNCN's localized learning approach was also motivated by the simple learning algorithm , which showed that a stack of Boltzmann network modules (and other simple, auto-associative variations) could be learned in a pseudo-joint'' layerwise fashion. However, in order to build in some form of global coordination, a variation of back-propagation of errors was used, ultimately creating a global feedback path as part of the overall learning procedure. A more global approach was later presented in @cite , incorporating top-down information much like that in , however, these algorithms were only built for and studied on stateless problems. Furthermore, these approaches would be difficult to scale when extended to sequential modeling problems given their strong dependence on Markov Chain Monte Carlo sampling.
- Predictive coding theories posit that the brain is in a continuous process of creating and updating hypotheses that predict the sensory input it receives, directly influencing conscious experience . Models of sparse coding and predictive coding embody the idea that the brain is a directed generative model where the processes of generation (top-down mechanisms) and inference (bottom-up mechanisms) are intertwined and interact to perform a sort of iterative inference of latent variables states. Furthermore, nesting the ideas of predictive coding within the Kalman Filter framework @cite can create dynamic models that handle time-varying data. Many variations and implementations of predictive coding have been developed . Some approaches, such as predictive sparse decomposition , attempt to speed up the iterative inference by introducing an inference network, but this again, creates a problem similar to that of variational inference--the generative model is constrained by the quality of the approximate inference model.
- Translating photo to cartoon by computer algorithms has been studied for a long time because of the corresponding interesting and meaningful applications. The earlier work mainly relied on the analysis of facial features @cite @cite @cite , which is hard to be applicable for large-scale faces in the wild. Thanks to the invention of GANs @cite , automatic photo-to-cartoon translation becomes feasible. But most of the current related work mainly focused on the generation of anime @cite or emoji @cite with specific style We consider that anime, emoji, and caricature are three types of cartoon or three subsets of cartoon. . And these works have nothing to do with caricature creation that needs to be exaggerated, lifelike and artistic.
- Other 3D house datasets could also be turned into interactive platforms, but these datasets are not as large-scale as SUNCG, which consists of 45622 house layouts. These datasets include Stanford Scenes (1723 layouts) @cite , Matterport3D @cite (90 layouts), sceneNN (100 layouts) @cite , SceneNet (57 layouts) @cite , and SceneNet RGB-D (57 layouts) @cite . We used SUNCG, as scale and diversity in data have proven critical for machine learning algorithms to generalize @cite @cite and transfer, such as from simulation to real @cite . SUNCG's simpler graphics also allow for faster rendering.
- In computer graphics and vision, photometric Stereo (PS) @cite is the widely adopted technique for inferring the normal map of the face. The normal map can then be integrated (e.g, using Poisson completion @cite ) to reconstruct the point cloud and then mesh. We refer the readers to the comprehensive survey @cite for the benefits and problems of the state-of-the-art methods. In general, recovering high quality 3D geometry requires using complex setups. The most notable work is the USC Light Stage @cite @cite that utilizes 156 dedicatedly controlled light sources simulating the first-order spherical harmonics function. Their solution can produce very high-quality normal map using near point light sources and the results are superb and have been adopted in movie productions. The setup, however, is rather expensive in cost and labor. Developing cheaper solutions capable of producing similar quality reconstruction is highly desirable , but by far few solutions can match the Light Stage.
- There is an emerging interest on directly converting a 2D face image to a 3D face model. Most prior works can be categorized into 3D-morphable faces and learning-based techniques. @cite automatically synthesized a 3D morphable model from over 10,000 3D faces. Bolkar @cite utilized a multiline model based learning framework that uses much smaller training datasets. @cite proposed a Surrey Face Model which provides high resolution 3D morphable model and landmarks alignment. Face models obtained from these approaches are sensitive to pose, expression, illumination, etc, and the problem can be mitigated by using more images @cite @cite or special facial feature decoders @cite .
- In the past few years, a large volume of deep learning based approaches have shown great success on face pose and geometry estimations @cite @cite @cite . @cite tailored a deep CNN to estimate face normal map 'in the wild' and then inferred the face shape. @cite applied regression to recover discriminative 3D morphable face models. The main goal of these approaches is face recognition and the recovered geometry is generally highly smooth. Most recent techniques @cite can recover certain medium-scale details such as deep wrinkles but the quality is still not comparable to professional solutions.
- In a similar vein as ours, @cite combined a low resolution depth with high resolution photometric stereo where the depth map is obtained via structured light. Compared with the structured light results that are highly noisy, the morphable 3D face geometry is smoother but less accurate. We further conduct optimization and semantic segmentations for refinement.
- Some non-uniform probability measures have already been studied in other areas of computer science, where problems concern the computation of a weighted sum over the set of solutions to a combinatorial problem. E.g. computing the partition function of the hard-core model and the Potts-model from statistical physics. There are Markov chains associated to these problems, that converge to non-uniform distributions in general. E.g. Glauber dynamics, Gibbs sampling @cite , Metropolis-Hastings algorithm @cite @cite etc. However, for the special cases where weights are in @math , the problems correspond to conventional counting problems in #P, and the associated stationary distributions are uniform, again. The literature on this areas is enormous, e.g. @cite @cite @cite @cite @cite @cite .
- Separate-and-conquer strategy @cite is likely the most commonly approach to rule learning. It provides a basis for the seminal RIPPER algorithm @cite as well as for the state-of-the-art FURIA algorithm @cite . Association rule learning is algorithmically different approach, which was originally designed to discover interesting patterns in very large and sparse instance spaces @cite . It yields a set of conjunctive rules that correspond to high density regions in the data. Unlike in the typical separate-and-conquer approach, cardinal features need to be discretized prior to the execution of association rule learning and converted along with all nominal attributes to binary-valued features. The resulting rules correspond to hypercubes with boundaries aligned to the discretization breakpoints. This impaired precision is offset by computationally efficiency on high-dimensional data, allowing association rule learning to succeed where other approaches fail [p. 505] friedman2009elements .
- Phrase localization. Object grounding with natural language descriptions has recently drawn much attention and several tasks and approaches have been proposed for it @cite @cite @cite @cite @cite . The most related task to ours is the phrase localization introduced by @cite , whose goal is to localize objects that corresponds to noun phrases in textual descriptions from an image. @cite is the closest to our work in terms of learning region proposals and performing regression conditioned upon a query. However, most phrase localization methods are not scalable and cannot be used for retrieval tasks. Some approaches @cite @cite learn a common subspace between the text and image for phrase localization. Instead of learning the subspace between the image and sentence as in standard cross-modal searches, they learn the subspace between a region and a phrase. In particular, @cite use a deep neural network to learn the joint embedding of images and text; their training uses structure-preserving constraints based on structured matching. Although these approaches can be used for large-scale retrieval, their accuracy is not as good as recent state-of-the-art methods.
- Parameter prediction by neural network. Query-Adaptive R-CNN generates the weights of the detector from the query instead of learning them by backpropagation. The dynamic filter network @cite is one of the first methods that generate neural network parameters dynamically conditioned on an input. Several subsequent approaches use this idea in zero-shot learning @cite and visual question answering @cite . @cite integrates this idea into the Fast R-CNN framework by dynamically generating the classifier from the text in a similar manner to @cite . We extend this work to the case of large-scale retrieval. The proposed Query-Adaptive R-CNN generates the regressor weights and learn the region proposal network following Faster R-CNN. It enables precise localization with fewer proposals, which makes the retrieval system more memory efficient. In addition, we propose a novel hard negative mining approach, called negative phrase augmentation, which makes the generated classifier more discriminative.
- There are transfer learning methods reported in literature where previously learnt features are directly used. @cite , the convolutional layers of a learnt CNN are used as fixed feature extractors for the new task and the training data of the new task, represented as feature vectors like this, is then used for training a new classifier, e.g., an SVM. In our method, the filter-trees represent pre-learnt features; but they come from multiple source networks. Also, since the target network can have one or more convolutional layers on top of the filter-trees, in the new task we can learn useful ways of combining previously learnt features. Some earlier works in transfer learning in neural networks such as @cite @cite also use multiple source networks for transfer. However, here, each of these networks is previously trained on a subtask of the target task and then these networks are simply fused together to learn the whole of the target task. Our method based on an ensemble of filter-trees is a much more general method of effecting transfer learning from multiple source networks.
- Studies on visual relationships have been emerging in the field of computer vision. This line of research includes detection of visual relationships @cite @cite and generation of a scene graph @cite . Most of these approaches are based on algorithms for grouping elements by relationships, and aiming to find relationships among the elements. Recently, this research field has focused on the scene graph analysis algorithm, which tackles the problem of understanding general scenes in natural images. Lu al @cite incorporated language prior to reasoning over a pair of objects and Xu al @cite solved scene graph inference using GRUs via iterative message passing. Whereas most of the previous studies dealt with natural images, we aim to infer visual relationships and generate a graph based on these relationships. Moreover, our method extracts knowledge from an abstracted diagram by inferring human's general perception.
- Generalization of neural networks for arbitrarily structured graphs has drawn attention in the last few years. (GNNs) @cite were introduced as an RNN-based model that iteratively propagates nodes in the graph until the nodes reach a stable fixed point. Later, Li al @cite proposed (GG-NNs), which apply GRU as an RNN model for the task. In contrast to the RNN-based models, Marino al @cite proposed (GSNN) to build knowledge graphs for multi-label classification problems. GSNN iteratively predicts nodes based on current knowledge by a way of pre-trained importance network'. The main difference between previous methods and ours is that our model can generate a graph structure based on relationships between nodes. Since generating a graph should involve in dynamic establishment or removal of edges between nodes, we also adopt RNN for DGGN as most neural-network-based methods for a graph. The proposed DGGN not only works by message-passing between nodes, but also builds the edges of a graph online, which provides great potential for graph generation and solution of inference problems.
- Since Weston al @cite proposed a memory network for the question answering problem, a memory augmented network became popular in natural language processing. This memory component has shown great potential to tackle many problems of neural networks such as catastrophic forgetting. In particular, Graves al applied the memory component in Neural Turing machine @cite , and showed that the neural network can update and retrieve memory dynamically. Using this concept, a number of dynamic memory models have been proposed to solve multi-modal problems such as visual question answering @cite @cite . In this paper, we incorporate the scheme of the dynamic memory in DGGN to easily capture and store the graph structure.
- Clustering aims at grouping similar objects in the same cluster and many different clustering methods have also been proposed. One type is the hierarchical clustering methods @cite , which include agglomerative hierarchical clustering methods @cite and divisive hierarchical clustering methods @cite . Meanwhile, according to the manner that the similarity measure is calculated, the hierarchical clustering methods can be further divided into single-link clustering @cite , complete-link clustering @cite and average-link clustering @cite . Another type of clustering methods is partition-based methods, which include K-Means for instances with numerical attributes @cite , K-Medoids for instances with categorical attributes @cite , probabilistic clustering @cite , as well as density-based clustering methods @cite . Other clustering methods include grid-based clustering methods @cite , constraint-based clustering @cite and fuzzy clustering @cite .
- Clustering method has also been widely used to detect communities in networks. introduce a modularity function measuring the quality of a division of networks @cite . introduce the concept of normalized cut and discover that the eigenvectors of the Laplace matrix provide a solution to the normalized cut objective function @cite . In addition, many community detection works have been done on heterogeneous online social networks. @cite propose to study the clustering problem with complete link information but incomplete attribute information. @cite try to detect the communities in networks with incomplete relational information but complete attribute information. In recent years, many works have propose to detect communities across aligned networks. @cite propose to detect the communities in emerging networks with extremely sparse relational information and little attribute information by propagating information from other aligned networks (with abundant of information). @cite propose to study the synergistic partition problem of multiple large scale partially aligned social networks based on the Map-Reduce.
- In this work, we model the RSS of narrowband communication systems for small periodic perturbations with unknown frequency, direction, and amplitude. It is shown that, similarly to signals in radar systems, the RF signal frequency modulate the periodic vital signals. The resultant model shows explicit relation between the respiration signal and initial position of the person, her orientation with respect to link-line, amplitude of the breathing, and the wavelength of the communication system. The SNR depends on these parameters as well as the noise power which dictates the performance of single tone parameter estimation techniques @cite . This result is coherent with empirical results of @cite , real-time spectrum analyzer measurements in @cite , and findings of @cite . The model also shows various interesting situations arising in practical deployments including the measurements showing only odd or even harmonics. It is further shown that the logarithmic transformation taking place in typical RSS measurement systems @cite does not change the signal type. Therefore, the developed model can be used for evaluating various deployment conditions that enable successful breathing rate monitoring.
- Reproduction of visual content on devices of different gamuts is typically divided into two categories, namely and . Traditionally the first class of techniques deals with mapping the color gamut between devices, attempting to produce the most accurate reproduction of colors possible given the restrictions of a given device or medium @cite . Tone mapping, on the other hand, is primarily concerned with compressing the luminance range of an HDR image or video such that the media can be visualized on a low dynamic range (LDR) display device @cite @cite .
- The study of gender and ethnic stereotypes is a large focus in linguistics and sociology, and is too extensive to be surveyed here @cite @cite @cite @cite @cite . Our main innovation is the use of word embeddings which provides a new lens to measure and quantify biases. Another related field in linguistics studies how language changes over time and has also recently employed word embeddings as a tool @cite @cite @cite . However, this literature primarily studies changes, such as how the word used to primarily mean and now means predominantly means @cite @cite , and does not investigate bias.
- The access to the shared storage by the physical machines (i.e. the hypervisor nodes) will consume resources and create undesirable VMM Noise. This VMM noise has been shown to have impact in the virtualized guests running on those hosts and is something to avoid in scientific computing environments @cite @cite @cite @cite .
- Before the invention of action abstractions induced by scripts, state-of-the-art algorithms included search methods for un-abstracted spaces such as Monte Carlo @cite @cite @cite @cite and Alpha-Beta @cite . Due to the large number of actions available during search, Alpha-Beta and Monte Carlo methods perform well only when controlling a small number of units. Some search algorithms cited are more general than the algorithms we consider in this paper, e.g., @cite @cite . This is because such algorithms can be used to control a playing agent throughout a complete RTS game. By contrast, the algorithms we consider in this paper are specialized for combat scenarios.
- Recently, aligning local features by pose estimation has become a popular approach. For instance, pose invariant embedding (PIE) aligns pedestrians to a standard pose to reduce the impact of pose @cite variation. A Global-Local-Alignment Descriptor (GLAD) @cite does not directly align pedestrians, but rather detects key pose points and extracts local features from corresponding regions. SpindleNet @cite uses a region proposed network (RPN) to generate several body regions, gradually combining the response maps from adjacent body regions at different stages. These methods require extra pose annotation and have to deal with the errors introduced by pose estimation.
- @cite presents a deep mutual learning strategy where an ensemble of students learn collaboratively and teach each other throughout the training process. DarkRank @cite introduces a new type of knowledge-cross sample similarity for model compression and acceleration, achieving state-of-the-art performance. These methods use mutual learning in classification. In this work, we study mutual learning in the metric learning setting.
- There are many non academic works regarding the need or lack thereof for a . However, authors agree that there would be not such a unique standard to rule all the cloud aspects. Some preliminary work regarding the need of standards for the cloud has been done in the past @cite @cite .
- G. Lewis @cite report tackles several standardization areas such as workload management, data and cloud management APIs, concluding that there will be not a single standard for the cloud due to pressures and the influences of existing vendors. The author states that an agreement on a set of standards for each of the needed areas would reduce the migration efforts and enable the third generation of cloud systems.
- @cite work surveyed the existing standards for the management of cloud computing services and infrastructure within the Contrail project so as to avoid vendor lock-in issues and ensure interoperability. In the same line, @cite performed a quite complete survey regarding Infrastructure as a Service access, management and interoperability, studying OVF, CDMI and OCCI @cite . However, they have not entered into other details and challenges such as accounting or information discovery.
- Boulanger- @cite proposed a model that predicts polyphonic music (multiple independent notes) with no distinction between chords and melodies, but since the predicted music is polyphonic it can form chords. The resulting music sounds pleasing and contains some long term structure. Since the music samples are a bit short it is not possible to tell if the structure spans over multiple bars.
- Other approaches that create polyphonic music are @cite , which create nice sounding Bach chorales that always have exactly 4 voices, and Johnson @cite which generates pleasing sounding music also with some long term structure.
- Recently there have been some approaches that take chord progressions into account. @cite propose a text based LSTM that learns relationships within text documents that represent chord progressions. @cite present a hierarchical recurrent neural network where at first a monophonic melody is generated, and based on the melody chords and drums are added. It is worth noting that @cite incorporates the circle of fifths as a rule for generating the chord progressions, whereas our model is able to extract the circle of fifths from the data.
- Huang and Wu @cite also experiment with learning embeddings for the notes. The visualized embeddings show that the model learned to distinguish between low and high pitches.
- In the stochastic composition optimization literature, all algorithms are based on biased stochastic gradient. @cite first proposed a generic algorithm for solving with a convergence rate @math for convex objectives and @math for strongly convex objectives. This result has been improved to @math for strongly convex objectives by @cite . Recently, @cite further improves the convergence rate to @math for the finite sum problem by utilizing a stochastic variance reduced gradient algorithm (SVRG). However, in this paper, we proposed a unbiased gradient simulation method that combines recent development in @cite @cite . In particular, we employ the methods proposed in @cite which combines a bias removal randomization scheme into Multilevel Monte Carlo method proposed in @cite . We then further make use of the SVRG @cite algorithm which can greatly reduce variance for ERM problem that achieves linear convergence. SVRG has been extended and improved in many works including but not limited to @cite , @cite , @cite , @cite , @cite , @cite . SAG @cite and SAGA @cite are two examples of incremental gradient methods that achieve linear convergence.
- Safe autonomous driving at moderate speeds has been demonstrated for example in the context of Autonomous Highway Systems (AHS) in 1997 @cite within the Californian PATH programme, or in contests such as the DARPA Grand Challenge in 2005 @cite and the Urban Challenge in 2007 @cite by numerous research groups. In these projects, the main goal was do develop autonomous driving systems for public traffic situations. Autonomous racing has received less attention, but impressive advances have been made recently also in this discipline. For instance, a fully autonomous Audi TTS has been reported to race at high speeds in @cite using a trajectory tracking controller, which follows a pre-computed trajectory. However, to the best knowledge of the authors, fully automatic racing including obstacle avoidance in dynamic racing situations has not been demonstrated yet in practice.
- From a control perspective, the fundamental building blocks for automatic racing can be grouped into three categories: drift control, reference tracking, and path planning. Research associated with the first group focuses on gaining a better understanding of the behavior of the car near the friction limit for designing safety control systems that try to prevent loss-of-control accidents. By analyzing the nonlinear car model, it can be shown that steady state motions corresponding to drifting can be generated, see e.g. @cite and @cite . As these approaches are not designed for trajectory tracking, the controlled car drifts in a circle, which represents a steady state drift equilibrium.
- Reference tracking controllers usually are designed to operate the vehicle within the linear tire force region for maximum safety and robustness. Approaches based on NMPC, which allows one to incorporate the latter constraint in a systematic manner, deal with the nonlinearities of the model either directly by a nonlinear programming (NLP) solver @cite , or use LTV @cite or piece-wise affine (PWA) approximations @cite , resulting in a convex quadratic program (QP) or a mixed-integer QP, respectively. Approaches without optimization in real-time include nonlinear robust control @cite and flatness-based control @cite , which however lack the ability to incorporate constraints. One approach that is explicitly designed for operating with saturated tire forces is @cite , where the center of percussion (CoP) is used to design a state feedback steering controller for reference tracking. At the CoP, the rear tire forces do not influence the dynamics by definition, which allows for a simple linear steering controller.
- In order to avoid obstacles, reference tracking schemes rely on a higher-level path planner. A simple point mass model in the high-level path planner is used in @cite to limit the computational complexity. This can be problematic if the generated trajectories are infeasible with respect to the car's physical dynamics, which can lead to preventable collisions @cite . The latter reference suggests to use a library of steady-state movements (trims) with parametrizable length, which are generated from a more complex dynamical model, and to connect them by maneuvers which allow the transition from one trim to another, similar to the idea of @cite . This path planning has the advantage that it generates feasible reference trajectories for the low level tracking controller, and that drifting trims can be included to increase the agility of the car. However, the resulting optimization problem is a mixed-integer program that is too complex to be solved within the typical sampling times on embedded platforms. Consequently, the approach is not well suited for real-time autonomous racing.
- In order to avoid relying on the feasibility of the path planner, one-level approaches have been investigated e.g. in @cite , where optimal trajectories and the associated open-loop control inputs for rally racing maneuvers are calculated numerically in simulation. @cite , obstacle avoidance is incorporated into the tracking controller by using an additional cost term that penalizes coming too close to the obstacle. However, the controller is reported to perform worse than its two-level counterpart, especially if the car has to diverge from the reference trajectory. A one-level approach similar to @cite is studied in @cite in simulation, where obstacle avoidance is incorporated into the problem formulation by using a spatial reformulation and imposing constraints on the transformed variables. While in @cite @cite the solution to the NLP is obtained by a standard nonlinear solver with prohibitively long solve times, @cite uses real-time iterations @cite to achieve the low computation times needed for a real-time implementation. However, it is assumed that there is only one side where the obstacle can be overtaken, which may not be the case in practical racing situations.
- An interesting alternative to optimization-based methods are sampling-based methods. For example, rapidly exploring random trees (RRTs) have been investigated in @cite and @cite to generate time optimal trajectories through a 180 @math curve. The advantage is that such algorithms tend to quickly find feasible trajectories. In @cite , the differential flatness of the system is exploited to speed up the convergence of the algorithm, generating obstacle-free trajectories even in complex situations. However, the reported computation times are not yet low enough to allow for a real-time implementation.
- Multiplicative and Tensor composition models Tensor based composition models have been shown to be useful for other NLP tasks such as sentiment analysis @cite @cite , knowledge base completion @cite , and in general for demonstrating compositional semantics in measuring sentence similarity @cite @cite @cite @cite .
- Schema script learning Unsupervised learning of script knowledge can be traced back to @cite , which introduced a count based technique for inducing narrative chains. extends this idea to creating full on narrative schemas by merging together narrative chains with argument overlap. Other unsupervised induction approaches include a relation n-gram based method @cite , and generative latent variable models @cite @cite @cite . All of these models worked on discrete representations for capturing co-occurrence statistics. In this work, we show that higher quality scripts can be produced using continuous representations instead.
- SGD @cite is the most widely used DL solver due to its simplicity, whose learning rate ( , step size for gradient) is predefined. In general, SGD suffers from slow convergence, and thus its learning rate needs to be carefully tuned. To improve the efficiency of SGD, several DL solvers with adaptive learning rates have been proposed, including Adagrad @cite , Adadelta @cite , RMSProp @cite and Adam @cite . These solvers integrate the advantages from both stochastic and batch methods where small mini-batches are used to estimate diagonal second-order information heuristically. These solvers have the capability of escaping saddle points and often yield faster convergence empirically.
- Specifically, Adagrad is well suited for dealing with sparse data, as it adapts the learning rate to the parameters, performing smaller updates on frequent parameters and larger updates on infrequent parameters. However, it suffers from shrinking on the learning rate, which motivates Adadelta, RMSProp and Adam. Adadelta accumulates squared gradients to be fixed values rather than over time in Adagrad, RMSProp updates the parameters based on the rescaled gradients, and Adam does so based on the estimated mean and variance of the gradients. Very recently, Mukkamala al @cite proposed variants of RMSProp and Adagrad with logarithmic regret bounds.
- There are a few other extensions to the variational autoencoder that allow for the data to be conditioned on a discrete class variable. This includes both the conditional variational autoencoder @cite @cite and a semi-supervised variational autoencoder @cite . Our work differs from these previous approaches as follows. First, in contrast to the conditional variational autoencoder @cite @cite , we do not assume that we are provided with the conditioning class variable at test time (e.g., when generating samples). This would be beneficial in a live music generation setting. For example, our model could play along with a musician without having to be explicitly told which key to play in.
- An interesting variation is given by the observation that in the above problem the decoder is completely determined by the encoder and the data distribution. We can however consider that both @math have to be optimized: which can be seen as the optimization of a penalized metric The cross-entropy is a very common and popular cost function in machine learning (see @cite and references therein) . Different from ), this problem does not lead (at least to our knowledge) to an information theory operational meaning as ). However, it is easy to check that given an arbitrary encoder @math , the optimal decoder choice is given by ). Therefore, expression ) is --from the point of view of the optimization problem-- not more general than ). For this reason and the connection with noisy source coding with log-loss fidelity, we will be concentrate our efforts in ).
- The work @cite is an effort to find a good circuit representation of a function, in order to minimize the total computation time. Bootstrapping is an operation that refreshes the so-called noise in FHE ciphertexts. It is an essential yet expensive operation. The two papers @cite and @cite aim at minimizing the total number of bootstrapping operations in a circuit, while keeping the noise from overflowing in order to ensure the final result is correct. In their work, the authors implicitly assume the relinearization is done after every multiplication. Similarly, we will make a simplifying assumption that the boostrapping time is a constant, so that it does not factor into our optimizatoin problem. It will be interesting to combine these works in order to achieve an overall optimization that targets both operations. Acknowledgement The author thanks Rebecca Hoberg and Mohit Singh for helpful discussions in preparing this work. We modify the usual definition of the arithmetic circuits to include the squaring operations in FHE.
- The challenge of data scarcity has been addressed in various computer vision research @cite @cite @cite . In particular, data augmentation techniques have been utilized to improve the training performance especially for deep learning-based methods @cite @cite @cite @cite . Conventional approaches mostly rely on simple transformations such as rotation @cite , random cropping @cite , random flipping @cite @cite @cite and altering RGB channel intensities @cite . The amount of new information introduced in such operations is limited as no latent manipulation (e.g., varying the illumination) is involved.
- Pennington et. al. @cite introduced the GloVe (Global Vectors for Word Representations) model in which they analyzed the properties responsible for the morphological regularities in the earlier models. They combined the effects of local context window and global matrix factorization methods resulting in a global log-bilinear regression model. The proposed model out-performed SG and CBOW on standard test sets.
- Word Representations, as learned by the above methods, deal with word as the basic unit and do not exploit the morphological relations present between words. However, these relations are present in the embeddings as regularities in the vector space @cite . These are specially useful for words which are unseen or have a low frequency in the training set and are not trained well. Luong et. al. @cite and Botha and Blunsom @cite used Morfessor @cite for word segmentation and used a combination of morpheme and word level models. Both of these approaches handled rare and unseen words using their basic morpheme units. Luong et. al. @cite used morphological recursive neural networks (RNNs) for constructing representation for a word using its morphemes. Botha and Blunsom @cite used log-bilinear models for building and combining representations of morphemes for constructing representations of rare or unseen words. Luong et. al. @cite also introduced the Stanford Rare-Word Dataset which contains a large number of rare and morphologically complex words.
- In contrast to Luong et. al. @cite and Botha and Blunsom @cite where an external morphological analyzer was used, Soricut and Och @cite induced morphological transformations in an unsupervised manner using SG (Skip-gram) word embeddings. These morphological transformations were represented as word pairs in the same embeddings space and they used simple vector arithmetic to calculate vectors for rare and unseen words. For example, for building the embedding for nationalism'', they evaluated the expression functionalism - function + nation''.
- In recent years, the feature selection technique plays an important role in machine learning community. This technique is desired to extract numerous useful features and to eliminate redundancies, in order to construct a simple model architecture. To overcome the limitations of conventional feature selection approaches such as Lasso and Ridge, etc., a combined feature selection approach, termed matrix elastic net (MEN), has been successfully used for different learning algorithms @cite @cite @cite .
- In addition to the @math norm in the RMEN, the nuclear norm (or trace norm) is a popular low-rank learning approach with widespread applications @cite @cite @cite @cite . The nuclear norm can yield a low rank solution, thus simplifying significantly the model architecture. It is a common perspective that only few elements of the instances contribute to a task. Moreover, different from previous work of the nuclear norm @cite , the nuclear norm in the RMEN is implemented over all projected instances rather than only adjustable parameters. As a consequence, the nuclear norm can enforce the relevance of projected samples with connections.
- Visual attention can be classified into and components. Covert attention precedes eye movements, and is intuitively used to monitor the environment and guide eye movements to salient regions @cite @cite . Two particular functions of covert attention motivate the Gaussian attention mechanism proposed below: noise exclusion, which modifies perceptual filters to enhance the signal portion of the stimulus and mitigate the noise; and distractor suppression, which refers to suppressing the representation strength outside an attention area @cite . Further inspiring the proposed attention mechanism is evidence from cueing @cite , multiple object tracking @cite , and fMRI @cite studies, which indicate that covert attention can be deployed to , regions that and can be conceptually viewed as multiple "spotlights".
- Overt attention is associated with an eye movement, so that the attentional focus coincides with the fovea's line of sight. The planning of eye movements is thought to be influenced by bottom-up (scene dependent) saliency as well as top-down (goal relevant) factors @cite . In particular, one major view is that two types of maps, the saliency map and the priority map, encode measures used to determine the target of attention @cite . Under this view, visual input is processed into a feature-agnostic saliency map that quantifies distinctiveness of a location relative to other locations in the scene based on bottom-up properties. The saliency map is then integrated to include top-down information, resulting in a priority map.
- The saliency map was initially proposed by Koch & Ullman @cite , then implemented in a computational model by Itti @cite . In their model, saliency is determined by relative feature differences and compiled into a "master saliency map". Attentional selection then consists of directing a fixed-sized attentional region to the area of highest saliency, i.e. in a "winner-take-all" process. The attended location's saliency is then suppressed, and the process repeats, so that multiple attentional shifts can occur following a single feed-forward computation.
- Subsequent research effort has been directed at finding neural correlates of the saliency map and priority map. Some proposed areas for salience computation include the superficial layers of the superior colliculus (sSC) and inferior sections of the pulvinar (PI), and for priority map computation include the frontal eye field (FEF) and deeper layers of the superior colliculus (dSC) @cite . Here, we need to only assume existence of the maps as conceptual mechanisms involved in influencing visual attention and refer the reader to @cite for a recent review.
- Visual attention is a major area of interest in deep learning; existing work can be separated into sequential attention and bottom-up feed-forward attention. Sequential attention models choose a series of attention regions. Larochelle & Hinton @cite used a RBM to classify images with a sequence of fovea-like glimpses, while the Recurrent Attention Model (RAM) of @cite posed single-object image classification as a reinforcement learning problem, where a policy chooses the sequence of glimpses that maximizes classification accuracy. This "hard attention" mechanism developed in @cite has since been widely used @cite @cite @cite @cite . Notably, an extension to multiple objects was made in the DRAM model @cite , but DRAM is limited to datasets with a natural label ordering, such as SVHN @cite . Recently, @cite developed a variable-sized glimpse inspired by biological vision, incorporating it into a simple RNN for single object recognition. Due to the fovea-like attention which shifts based on task-specific objectives, the above models can be seen as having overt, top-down attention mechanisms.
- An alternative approach is to alter the structure of a feed-forward network so that the convolutional activations are modified as the image moves through the network, i.e. in a bottom-up fashion. Spatial transformer networks @cite learn parameters of a transformation that can have the effect of stretching, rotating, and cropping activations between layers. Progressive Attention Networks @cite learn attention filters placed at each layer of a CNN to progressively focus on an arbitrary subset of the input, while Residual Attention Networks @cite learn feature-specific filters. Here, we consider an attentional stage that a feed-forward stage, i.e. a saliency map and image representation are produced in a feed-forward stage, then an attention mechanism determines which parts of the image representation are relevant using the saliency map.
- Saliency is typically studied in the context of saliency modeling, in which a model outputs a saliency map for an image that matches human fixation data, or salient object segmentation @cite . Separately, several works have considered extracting a saliency map for understanding classification network decisions @cite @cite . @cite formulate a loss function that causes a student network to have similar "saliency" to a teacher network. They model saliency as a reduction operation @math applied to a volume of convolutional activations, which we adopt due to its simplicity. Here, we investigate using a saliency map for a downstream task. Recent work has begun to explore saliency maps as inputs for prominent object detection @cite and image captioning @cite , pointing to further uses of saliency-based vision models.
- While we focus on using reinforcement learning for multiset prediction with only class labels as annotation, RL has been applied to other computer vision tasks, including modeling eye movements based on annotated human scan paths @cite , optimizing prediction performance subject to a computational budget @cite , describing classification decisions with natural language @cite , and object detection @cite @cite @cite . Finally, our architecture is inspired by works in hierarchical reinforcement learning. The model distinguishes between the upper level task of choosing an image region to focus on and the lower level task of classifying the object related to that region. The tasks are handled by separate networks that operate at different time-scales, with the upper level network specifying the task of the lower level network. This hierarchical modularity relates to the meta-controller controller architecture of @cite and feudal reinforcement learning @cite @cite . Here, we apply a hierarchical architecture to multi-label image classification, with the two levels linked by a differentiable operation.
- In @cite among others PMD and FindBugs are compared based on their warnings which were not all checked for false positives. The findings are that although there is some overlap the warnings generated by the tools are mostly distinct. We can support this result with our data.
- Engler and Musuvathi discuss in @cite the comparison of their bug finding tool with model checking techniques. They argument that static analysis is able to check larger amounts of code and find more defects but model checking can check the implications of the code not just properties that are on the surface.
- In @cite a static analysis tools for C code is discussed. The authors state that sophisticated analysis of, for example, pointers leads to far less false positives than simple syntactical checks.
- An interesting combination of static analysis tools and testing in described in @cite . It is proposed to use static analysis to find potential problems and automatically generate test cases to verify if there is a real defect. However, the approach obviously does not work with maintenance-related defects.
- report in @cite on a static analyser for C and C++ code which is able to find several more dynamic programming errors. However, a comparison with tests was not done. Nevertheless, our observation that the defect-finding capabilities depend strongly on the coding styles of different programmers is supported in this paper.
- In @cite , an evaluation of static analysis tools for C code regarding buffer overflows is described. The defects were injected and the fraction of buffer overflows found by each technique was measured. It is also noted that the rates of false positives or false alarms are unacceptably high.
- Palsberg describes in @cite some bug finding tools that use type-based analysis. He shows that they are able to detect race conditions or memory leaks in programs.
- The most basic aspect tackled in the literature is the problem that determines the presence or absence of food in an image. This problem is also called food non-food classification or food detection @cite . The first approximation was proposed by @cite , who through the combination of a BoF model and an SVM achieve a high accuracy on a tiny dataset of 600 images. An improvement of about 4
- In food analysis, once images containing food are identified, is usually the next step to apply. Again, CNN-based models have been able to progressively improve the results of food recognition models reaching an accuracy of about 90 Most of the approaches focused on food recognition only exploit the visual content, but they ignore the context. However, geolocation and other information have also been explored in the literature for restaurant-oriented food recognition: on-line restaurant information is used in @cite , similarly to @cite in which nutritional information is also retrieved; whilst the menu, the location and user images of dished are used in @cite . On the other hand, @cite go a step further since their target is not only to improve both classification performance and efficiency, but also to better model contextual data and its relation with the other elements.
- To date, most food recognition algorithms and datasets focus on classifying images that include only one dish @cite @cite @cite . However, in some cases, there may be more than one dish in the image and, in some cases, the dish can contain several kinds of food. and are two tasks intended to cope with these problems. The former consists in extracting the regions of the images where the food is located. Up to our knowledge, the only available approach that does not require segmenting the food before extracting the bounding boxes is the one proposed by @cite . The task of food segmentation consists in classifying each pixel of the images representing a food. The latest research for food segmentation proposes an automatic weakly supervised methods @cite @cite , which are based on Deep Convolutional Neural Networks and Distinct Class-specific Saliency Maps, respectively.
- In this paper, we deal with the identification of different foods placed on a food tray, by integrating the four food analysis problems mentioned above. To the best of our knowledge, only one approach with this purpose has been evidenced in the literature @cite . The authors there introduced an additional food dataset composed by images taken in a canteen environment named UNIMIB2016. In addition, they proposed a pipeline for food recognition that performs classification based on the candidate regions obtained by combining two separate images segmentation processes, through saturation and color texture (JSEG). The best result was achieved by combining global and local (path-based) approaches using an SVM as classifier. However, the proposed approach performs the segmentation based on generic methods rather than learning the best discriminant features between different foods based on the dataset. Furthermore, it requires several sequential steps to first segment and then classify, which implies a high processing time. Instead, our method is able to perform the food segmentation and detection processes in parallel, allowing to speed up the processing time.
- The special case of the model described in the previous subsection where every agent is Bayesian (i.e., with @math for every @math ) is identical to the model described in the exposition of Easley and Kleinberg [Chapter 16] easley2010networks . The original model of Bikhchandani, Hirshleifer, and Welch @cite differs only in its tie-breaking rule (breaking ties by flipping a fair coin), while that of Banerjee @cite differs in the signal distribution (false signals are drawn from a continuous distribution). Despite these minor differences, these models all share the same phenomenological behavior as described in the introductory paragraphs.
- In particular, Bikhchandani et al. @cite emphasize the of information cascades with respect to different types of shocks, as prior work on conforming behavior could not explain this phenomenon. They show examples from numerous fields (e.g., politics, zoology, medicine, and finance) where cascades occur and are fragile. The current paper can be viewed as a more detailed quantitative exploration of the fragility of cascades. What amount of additional information is needed to break wrong cascades? What is the optimal rate of learning that can be achieved?
- One possible source of additional information comes from people not acting in a rational, Bayesian manner. It is well documented that human behavior is often irrational (see, e.g., @cite ). In the information cascades setting, laboratory experiments by Anderson and Holt @cite show that while most participants act rationally, many do not. When deviations from rational behavior occur, participants often act mainly or solely based on their private information, disregarding the information in the actions of those before them. See also related experiments and results by C elen and Kariv @cite for a setting with continuous signals. Such individuals effectively reveal their private signal, which is valuable information for those coming after them. The model described in , which contains and , captures this empirically observed behavioral phenomenon.
- The framework of this paper also fits into the broader field of social learning. In particular, there is a large literature on learning in social networks. Acemoglu, Dahleh, Lobel, and Ozdaglar @cite consider a model of sequential decision making where agents act only once, but each agent can only observe a of previous actions, based on a stochastic social network. One of their results is that asymptotic learning occurs even if private signals have bounded informativeness if there are sufficiently many individuals whose neighborhoods are non-persuasive and hence whose action will necessarily be influenced by their private signal. Similar to revealers in the model described in , these individuals provide a sufficient amount of information for those coming after them to lead to asymptotic learning.
- Lastly, a related framework of finding minimax risk was also studied in @cite for the purpose of preventing attacks on privacy. We discuss how the attack on classification in this paper and the attack on privacy are the two sides of the same optimization problem with the opposite goals.
- The Restricted Boltzmann Machine (RBM) is an unsupervised two-layer neural network for the estimation of distribution of binary input data. This generative probabilistic model was first introduced in 1986 by Smolensky @cite and was later developed by Hinton in 2002 @cite . Inspired by the RBM model, in 2011 introduced the Neural Autoregressive Distribution Estimation (NADE) @cite , which is an unsupervised generative probabilistic method for modeling the probability of discrete data. NADE eliminated the limitation of RBM in high dimensional joint probability estimations by the use of fully visible Bayes networks for probability calculations. The earliest neural network-based topic model is Replicated Softmax Model (RS) introduced by Hinton and Salakhutdinov in 2009 @cite , which is an extension of the RBM model used to detect the distribution of topics in text data @cite . In 2012, Larochelle and Lauly combined NADE and RS to develop an unsupervised neural network-based topic modeling method called the Document Neural Autoregressive Distribution Estimation (DocNADE) @cite .
- In the category of Bayesian topic models, the well-known Latent Dirichlet Allocation (LDA) model introduced by in 2003 @cite has long served as the basis of all methods of this category. LDA is a generative probabilistic method in which text document is considered as a mixed distribution over topics, where each topic is characterized by a distribution over words.
- All of the aforementioned models are capable of detecting the topics in text data. There is however another group of topic models that can detect the topics as well as the sentiments associated with each one. This group includes the Aspect-Sentiment Unification Model (ASUM) introduced by Jo and Oh in 2011 @cite for detecting topics and sentiments in online reviews. ASUM is an extension of LDA and falls in the category of generative probabilistic graph models. In 2012, introduced the weakly supervised joint sentiment-topic (JST) detection model @cite . The advantage of JST over its competitors is in its weakly supervised nature, which allows it to be easy adapted for other domains without noticeable decrease in performance.
- The second component of each layer of the Transformer network is a feed-forward network. The authors propose using a two-layered network with a ReLU activation. Given trainable weights @math , the sub-layer is defined as: The dimension of the inner layer is @math which is set to @math in their experiments. For the sake of brevity, we refer the reader to @cite for additional details regarding the architecture.
- The structure adaptation (SA) approach exploits network (e.g., distribution of weights) to incorporate architecture selection into the training process. Existing SA methods can be further divided into two categories: constructive and destructive. A constructive approach starts with a small network and iteratively adds connections neurons @cite @cite . A destructive approach, on the other hand, starts with a large network and iteratively removes connections neurons. This can effectively reduce model redundancy. For example, recent pruning methods, such as network pruning @cite @cite @cite @cite , layer-wise surgeon @cite , sparsity learning @cite @cite @cite @cite , and dynamic network surgery @cite , can offer extreme compactness for existing DNNs with no or little accuracy loss.
- Finally, so-called self-supervised approaches in the computer vision community are analogous to what we propose in this paper for audio. There, constraints based on egomotion @cite , spatial compositional context @cite @cite , object tracking @cite , and colorization @cite have all been evaluated. Recent efforts have extended this principle of self-supervision to joint audio-visual models that learn speech or audio embeddings using semantic constraints imposed by the companion visual signal @cite @cite @cite @cite .
- Real life events logs often contain all sorts of data quality issues @cite , include incorrectly logged events, events that are logged in the wrong order, and events that took place without being logged. Instances of such data quality issues are often referred to as . Many event log filtering techniques have been proposed to address the problem of noise. Existing filtering techniques in the process mining field can be classified into four categories: 1) event filtering techniques, 2) process discovery techniques that have an integrated filtering mechanism build in, 3) trace filtering techniques, and 4) activity filtering techniques. We use these categories to discuss and structure related work.
- @cite proposed a technique to identify outlier traces from the event log that consists of two steps: 1) mining frequent patterns from the event log, and 2) applying MCL clustering @cite on the traces, where the similarity measure for traces is defined on the number of patterns that jointly characterize the execution of the traces. Traces that are not assigned to a cluster by the MCL clustering algorithm are considered to be outlier traces and are filtered from the event log. It is easy to see that trace filtering techniques address a fundamentally different problem than chaotic activity filtering: in the event log shown in Figure there are only two traces that do not contain an instance of chaotic activity @math , therefore, even if a trace filtering technique would be able to perfectly filter out traces that contain a chaotic event, the number of remaining traces will become too small to mine a fitting and precise process model when the chaotic activity is frequent.
- Here, we focus on generic stochastic bandits with a finite but potentially large number of arms. Both continuous as well as adversarial versions of the problem have been investigated, see survey @cite .
- Models for NLG can be divided into two groups: statistical and rule-based ones @cite . The latter employ linguistic expertise and work in three different phases:. During document planning the information that will be communicated in the text is selected and organised (i.e. document structuring). The output of the document planner is used by the microplanner to decide how this information should be linguistically expressed in the generated text. Subsequently, the realiser generates the actual text by applying a specific template that satisfies the linguistic requirements that are set by the microplanner, and expresses the information as it is structured by the document planner. Each one of the above mentioned stages is associated almost explicitly not only with the domain of the end-application but, in most cases, with the application itself.
- Most of the previous work on NLG with Semantic Web data has focused on the verbalisation of domain ontologies by using rules. Examples include systems that generate text in domains with limited linguistic variability, such as clinical narratives @cite , summaries of football matches @cite , and, descriptions of museum's exhibits @cite . Further Semantic Web oriented NLG applications can be found in @cite . Our work naturally lies on the path opened by recent unsupervised @cite and @cite based approaches for the extraction of RDF verbalisation templates using parallel data-to-text corpora. However, rather than making a prediction about the template that would be the most appropriate to verbalise a set of input triples, our model jointly performs content selection and surface realisation, without the inclusion of any hand-coded rules or templates.
- Previous work on neural network approaches shows their great potential at tackling a wide variety of NLP tasks ranging from machine translation @cite @cite to automatic response generation @cite @cite , and to computing vector representations of words in a continuous semantic space @cite . Our approach is inspired by the general encoder-decoder framework @cite @cite with multi-gated Recurrent Neural Network (RNN) variants, such as the Gated Recurrent Unit (GRU) @cite and the Long Short-Term Memory (LSTM) cell @cite . Adaptations of this framework have demonstrated state-of-the-art performance in many generative tasks, such as machine translation @cite @cite @cite , and conversation modelling and response generation @cite @cite .
- In parallel to our work, there has been a string of recent developments on action-dependent baselines for policy-gradient methods in reinforcement learning. Such works include and which train an action-dependent baseline which incorporates off-policy data. @cite independently develop a method similar to LAX applied to continuous control. exploit per-dimension independence of the action distribution in continuous control tasks to produce an action-dependent unbiased baseline.
- Other works have aimed at disentangling content and view style without any supervision, i.e based on unlabelled samples. In the Info-GAN model ( @cite ), a code is passed along a random noise to generate an image. An encoder is then used to retrieve the code so that the mutual information between the generated image and the code is maximized. The beta-VAE model proposed in @cite is based on the VAE model, where the dimensions of the latent representation are forced to be as independent as possible using a strong regularization. The independent dimensions can then correspond to independent factors of the outputs. For these two models, the ability to disentangle content and view is not obvious since there is no supervision that can guide the learning. More specifically, these models disentangle low-level visual attributes but struggle to grasp higher level concepts.
- The work closest to ours is the model proposed in @cite in which they use an encoder to extract both view and content vectors from a datapoint and a decoder that combines both vectors to produce an output. They use both a reconstruction loss and a discriminator that serves multiple purpose. However, this work is mostly centered on disentangling the factors, and their purely generative abilities are limited.
- Camera relocalization has been widely studied in large scale global localization @cite @cite @cite , recovery from tracking failure @cite @cite , loop closure detection in visual SLAM @cite @cite and global localization in mobile robotics @cite @cite , and sports camera calibration @cite @cite . Various methods have been proposed to advance camera relocalization performance. We provide a review of random forests based methods here and refer to @cite for a review of other methods.
- There has been a lot of research currently to address these loss functions, where researchers have tried incorporating reinforcement policies where the model is rewarded for producing more diverse and longer responses @cite . Adversarial strategies have also been adopted where a discriminative network is stacked on a generator and the discriminator classifies whether the generated response resembles a human response or not @cite .
- With all this said, there has been very little focus on incorporating the context of the conversation that has happened so far in order to generate responses. @cite propose a naive approach where they incorporate the previous set of utterance, responses as a bag of words model and use a feed forward neural network to inject a fixed sized context vector as a into the LSTM cell of the encoder. @cite proposed a modified LSTM cell with an additional gate that incorporates the previous context as input during encoding. The weights of the gate are learned exactly in the same way the weights for the input, forget and output gates are learned.
- @cite propose a context topic sensitive question answering system where a convolution neural network is pre-trained to predict one among 40 topics. The convolutional model is given as input a window of the previous conversation and the hidden state of the encoder before the softmax layer is fed as input(context) to a sequence to sequence model along with the current utterance for coherent answer generation.
- Building autonomous robots that assist people in human environments has been a long-standing goal @cite and an appealing inspiration of research in robotics. Robots have demonstrated various levels of success but still face challenges.
- RHINO @cite integrates a distributed autonomous navigation architecture dedicated to human robot interaction with various sensors. Minerva @cite focuses on the probabilistic paradigm of environment, robot , sensors and models, setting the foundation for the probabilistic robotics epoch. Besides autonomous navigation, Robox @cite integrated multi-modal interaction, speech recognition, and multi-robot coordination. Curious George @cite could use visual attention locate the objects of interest in an environment. Jinny @cite can select the proper motion strategy according to different environments. Sonar or laser range finders are usually positioned on these robot to build 2D maps along a horizontal slice of the world @cite . However, just one slice of the space can not represent the environment, especially uneven environments with slopes and staircases.
- Some work focuses on uneven outdoor terrain. A rule-based fuzzy traversability index has been used @cite to quantify the ease-of-traversal of a terrain by a mobile robot based on measurements from image data. Online kernel-based learning is proposed to estimate a continuous surface over the area of interest while providing upper and lower bounds on that surface with 3D laser @cite . However, these uneven outdoor terrains are very different from indoor uneven environments. Indoor environments tend to be mostly even, and often contain slopes for wheelchair accessibility. With robots or smart wheelchairs are operating in shared spaces indoors with people more and more, challenges operating in uneven and unstructured environments, especially in those designed for wheelchair mobility shall be addressed in the robotics community.
- The first is to partition the graph into a subgraph per machine. Each subgraph is then clustered independently on one machine. Then, all nodes of each cluster are merged summing up weights of parallel edges. The resulting coarser graph is clustered on a single machine. This assumes the coarsened graph fits in the memory of a single machine. @cite , for the partitioning, the input node ID range is chunked into equally sized parts. This can work well, but is problematic if input node IDs do not reflect the graph structure. @cite , the input graph is partitioned using the non-distributed, parallel graph partitioning algorithm ParMETIS @cite . While this is independent of node IDs, it requires that the graph fits into the memory of one machine for the partitioning.
- The second approach consists of distributing the clustering algorithm itself. Using MPI, @cite have introduced a distributed extension of the Louvain algorithm @cite . Similarly, @cite have presented an algorithm that uses the GraphX framework. Another algorithm named GossipMap is presented in @cite which uses the GraphLab framework. Our algorithms also use this second approach.
- Stochastic gradient methods have often been used to minimize the large-scale finite-sum problem. However, stochastic gradient methods are unsuitable for the family of nonlinear functions with two finite-sum structures. @cite first proposed the first-order stochastic method SCGD to solve such problems, which used two steps to alternately update the variable and inner function. SCGD achieved a convergence rate of @math for the general function and @math for the strongly convex function, where @math is the number of queries to the stochastic first-order oracle. Furthermore, in the special case that the inner function @math is a linear mapping, @cite also proposed an accelerated stochastic composition proximal gradient method with a convergence rate of @math .
- Recently, variance-reduced stochastic gradient methods have attracted attention due to their fast convergence. @cite @cite proposed a stochastic average gradient method with a sublinear convergence rates. Two popular gradient estimator methods, SVRG @cite and SAGA @cite , were later introduced, both of which have linear convergence rates. @cite went on to introduce the proximal-SVRG method to the regularization problem and in doing so provided a more succinct convergence analysis. Other related SVRG-based or SGAG-based methods have also been proposed, including @cite who applied SVRG to the ADMM method. @cite reported practical SVRG to improve the performance of SVRG , @cite introduced the Katyusha method to accelerate the variance-reduction based algorithm, and @cite used the SVRG-based algorithm to explore the non-strongly convex objective and the sum-of-non-convex objective. Moreover, @cite first applied the SVRG-based method to the stochastic composition optimization and obtained a linear convergence rate.
- Dual stochastic and primal-dual stochastic methods have also been proposed, and these also included "variance reduction" procedure. SDCA @cite randomly selected the coordinate of the dual variable to maximize the dual function and performed the update between the dual and primal variables. Accelerated SDCA @cite dealt with the ill-conditioned problem by adding a quadratic term to the objective problem, such that it could be conducted on the modified strongly convex subproblem. Accelerated randomized proximal coordinate (APCG) @cite @cite was also based on SDCA but used a different accelerated method. Duality-free SDCA @cite exploited the primal and dual variable relationship to approximately reduce the gradient variance. SPDC @cite is based on the primal-dual algorithm, which alternately updates the primal and dual variables. However, these methods can only be applied to the single finite-sum structure problem. @cite proposed the dual-based method for stochastic composition problem but with additional assumptions that limited the general composition function to two finite-sum structures.
- Finally, @cite considered corrupted samples with Markov noise and proved that SCGD could almost always converge to an optimal solution. @cite applied the ADMM-based method to the stochastic composition optimization problem and provide an analysis of the convex function without requiring Lipschitz smoothness.
- Though the exploration of intelligence processors continues to grow, a suitable benchmark for the evaluation and optimization of such hardware is still absent. BenchNN and DeepBench are two benchmarks that partially address this problem. However, due to their non-diversity and nonrepresentativeness, they cannot be used for designing and optimizing state-of-the-art intelligence processors. In the computer architecture community, several benchmarks have already been used. Chen @cite used 10 layers all extracted from AlexNet and two customized RBMs for evaluation. Chi @cite employed six networks, five extracted from LeNet-5 and one from VGG. Han employed nine DNNs extracted from AlexNet, VGG, and Neural Talk @cite . Shafiee @cite employed seven CNNs (four from VGG, three from MSRA @cite ), and two DNNs (one from DeepFace @cite and the other one from @cite ). Due to the lack of a standard benchmark suite, employing such personalized benchmarks in the design of existing neural network accelerators is relatively casual, which is unsuitable for benchmarking intelligence processors.
- The problem of designing a distribution rule so as to maximise @math has been studied in @cite and @cite . Both works impose a natural constraint on the admissible @math , requiring @math and @math to be non-increasing. The optimal distribution rule is explicitly derived in the former work, while the latter shows how @math is fully characterised by a single scalar quantity @math defined in , measuring how fast the distribution rule @math decreases. We intend to build upon these results, which are summarised in the following proposition. Given @math and a distribution rule @math , we define j k-1 j f(j)-f(j+1),(k-1) f(k) ,.
- Sequence-to-sequence models encode a variable-length input into hidden states, which are then processed by a decoder to produce a target sequence. An attention mechanism allows a decoder to adaptively select encoder hidden states to focus on while generating the target sequence . Attention-based sequence-to-sequence models are widely applied in machine translation , speech recognition , and text summarization . Recent improvements in attention mechanisms relevant to Deep Voice 3 include enforced-monotonic attention during training , fully-attentional non-recurrent architectures , and convolutional sequence-to-sequence models . Deep Voice 3 demonstrates the utility of monotonic attention during training in TTS, a new domain where monotonicity is expected. Alternatively, we show that with a simple heuristic to only enforce monotonicity during inference, a standard attention mechanism can work just as well or even better. Deep Voice 3 also builds upon the convolutional sequence-to-sequence architecture from @cite by introducing a positional encoding similar to that used in @cite , augmented with a rate adjustment to account for the mismatch between input and output domain lengths.
- The use of UAVs as flying base stations is attracting growing interests from researchers @cite @cite @cite . The literature on UAV-enabled communications focus on developing the air-to-ground transmission model and explore the line of sight opportunity @cite @cite . Further, @cite consider the co-channel interference effect and study the UAV coverage maximization problem.
- In computer vision and pattern recognition, on the other hand, models integrated context into many problems such as object recognition @cite @cite , activity recognition @cite using probabilistic graphical models, such as Markov Random Field, Conditional Random Field, or Bayesian Networks. In these models, contextual information was provided mostly through local interactions between predictions.
- There are several studies in robotics that integrate context into various robot problems, examples including @cite , which used context in determining where to place a new object in the scene; @cite , which modeled local interactions between objects (as context) in determining their labels; and, @cite , which proposed using context in modulating object detections in a scene and planning. Our model differs from all these studies by being incremental and hierarchical.
- There are incremental context or topic modeling efforts in text modeling @cite , computer vision @cite and in robotics @cite @cite . These methods look at the errors or the entropy (perplexity) of the system to determine when to increment. Moreover, they are not hierarchical. There are also other methods such as Hierarchical Dirichlet Processes @cite or its nested version @cite that assume the availability of all data to estimate the number of topics or assume infinite number of topics, which are both unrealistic for robots continually interacting with the environment and getting into new contexts through their lifetime.
- Researchers analyzed crowdfunding platforms @cite @cite . For example, @cite showed the dynamics of Kickstarter donors. @cite studied the dynamics of crowdfunding and revealed that personal networks and underlying project quality were related to the crowdfunding success. @cite analyzed why people created and or backed projects in crowdfunding platforms. @cite @cite showed various factors to make a project successful in terms of the fundraising. Joenssen and M "u llerleile @cite analyzed 42k Indiegogo projects, and discovered that scarcity management was problematic and reduced the chances of projects to successfully achieve the fundraising goal. Researchers @cite @cite @cite studied the impact of social media and social communities in raising fund. @cite showed that timing and communication were two key factors to make projects successful.
- @cite examined 16k Kickstarter projects and proposed a model based on pledged money features, and project and backer graph features to predict the success of the projects. @cite extracted 13 features from each of 13,000 Kickstarter projects and developed classifiers to predict project success. In @cite @cite , the authors used different feature traits to predict the success of projects. @cite analyzed 18K Kickstarter projects and built logistic and log-logistic based models to predict the chance of successfully achieving goal. @cite discovered that early donation played an important role in making the project successful. @cite proposed text features of project pages for the project success prediction.
- Recently, @cite interviewed crowdfunding participants and found various factors which influenced backers' trust. They also conducted analysis for 4,089 delayed projects to understand how 8 factors were related to delayed duration. However, they did not study which project will pass estimated delivery date, nor how long a reward delivery including production will take.
- While most of the previous research works focused on , we focus on the next phase -- . Our work is also different from @cite . In particular, we analyze characteristics of on-time and late delivery projects, and build a predictive model to predict whether rewards in a project will be delivered on-time or not. In addition, we build a regression model to recommend accurate delivery duration to creators.
- Constituency-based and dependency-based parse trees have been explored and applied to improve performance of neural nets for the task of sentiment analysis and semantic relation extraction @cite @cite @cite . The focus of these prior studies is on designing new neural network architectures (e.g., tree-structured LSTMs) corresponding to the parse tree structure. In contrast, our method aims at extracting appropriate event-centered data representations from dependency trees so that the neural net models can effectively concentrate on relevant regions of contexts.
- Similar to our dependency chains, dependency paths between two nodes in a dependency tree have been widely used as features for various NLP tasks and applications, including relation extraction @cite , temporal relation identification @cite semantic parsing @cite and question answering @cite . Differently, our dependency chains are generated with respect to an event word and include words that govern or depend on the event, which therefore are not bounded by two pre-identified nodes in a dependency tree.
- Previous work by shows the evaluation of algorithms for the visual localization subsystem and establishes the histogram based approach to be the most effective in determining the location and orientation of the USV from the UAV's video feed @cite . This paper focuses on the extension of Dufek's work by adding an autonomous system to control the camera's pan tilt to track the USV. In this section, we evaluate multiple techniques to use pan-tilt movements for visual servoing.
- A camera-control approach by UAV was proposed in @cite , where the UAV camera movements are based on a biomimetic eye based control, being able to simulate the human eye movements resulting in stable and smooth camera movements. The algorithm is computationally heavy, which is not suitable for our purposes.
- , in @cite shows the usage of a camera and PIR sensor fusion to track a moving object (human). However, this method has a range limitation posed by the Passive Infrared sensor. For the UAV to track the USV, an algorithm that can track the USV irrespective of the distance of the UAV from the USV, as long as it is in sight of the camera is needed. This is because, the UAV is not stationary as it keeps moving due to the effects of wind and coastal currents in the sea. Also, the PIR sensor consumes additional power and is not reliable in sunlight.
- Another line of research focuses on drawing metro maps, for example, by restricting the representation of transit lines to octilinear polylines @cite or B 'ezier Curves @cite . See also @cite for a recent survey on automated metro map layout methods. These approaches strongly abstract from the geographical course of the lines (and often also from station positions), and the minimization of line crossings or separations is not part of the problem. In particular, the resulting maps cannot be used for tiles or overlays in typical map services.
- Our work is also related to Bayesian non-parametric approaches in the sense that the number of clusters is open-ended, and will be inferred based on the observed data. Some good reviews on non-parametric approaches in statistics can be found, for example, in @cite @cite @cite . In particular, @cite has proposed a unified non-parametric framework for model-based clustering with the use of hierarchical Dirichlet mixtures. Almost all of the studies reviewed so far share a common property: at their best, authors have only proved convergence to a sub-optimal likelihood value, rather than providing guarantees on the accuracy of the final clustering learning.
- It was not until @cite that neural networks were tried out for the task as the authors used the same news dataset as @cite to develop a deep learning based model to detect clickbait. They used distributional semantics to represent article titles, and BiLSTM to model sequential data and its dependencies. Since then, @cite has also experimented with Twitter data @cite deploying a BiLSTM for each of the textual features (post-text, target-title, target-paragraphs, target-description, target-keywords, post-time) available in the corpus, and finally concatenating the dense output layers of the network before forwarding it to a fully connected layer. Since it was proposed in @cite , the attention mechanism has been used for a variety of text-classification tasks, such as fake news detection and aspect-based sentiment analysis. @cite used a self-attentive BiGRU to infer the importance of tweet tokens in predicting the annotation distribution of the task.
- Since Sweeney et. al. @cite proposed the @math -anonymity model, numerous extensions to that model have been proposed to address its weaknesses. As an example, @cite proposes @math -diversity and @cite proposes @math -closeness, both of which protect against inference-based attacks. None of the proposed models consider the combination of other anonymisation protocols.
- @cite the authors propose a way to achieve differential privacy and @math -anonymity on the same data release. In order to add a stochastic element to @math -anonymity (a deterministic system), the authors propose taking a sample at random from the original dataset and then processing this sample to achieve @math -anonymity prior to publication. The uncertainty introduced by randomly selecting users to be present in the released (but anonymised) dataset ensures differential privacy can also be achieved.
- In contrast, @cite addresses the issue of one-time publishing of non-overlapping counts with @math -differential privacy by defining GS, a method that pre-processes the counts by grouping and smoothing them via averaging, and then treating the counts with a differential privacy algorithm.
- @cite the authors present a comparison between the privacy guarantees provided by applying @math -anonymity and @math -differential privacy. Also, the authors provide a mechanism to approximate the equivalent @math parameter of a @math -closeness setting and vice-versa.
- In @cite , the authors provide a survey on vehicle detection techniques, with a focus on computer vision. The sensors are classified into two groups: active (such as lasers, radars and lidars) and passive (such as cameras, and acoustic sensors), and then compared to each other in terms of range, cost and many other features. The radar is considered as the best active sensor, since it provides long-range ( @math ) real-time detection even under very bad weather (e.g., foggy, rainy) conditions. On the other hand, a radar is not able to determine the shape of the object, which can be done with lidar, a costly alternative. These problems encouraged authors to focus on passive sensors, such as cameras. Cameras are low-cost sensors, able to provide a very precise information about the objects. However, their main drawback is a high complexity of data processing, low range during nights, and sensitivity to weather conditions. Note that authors did not consider any kind of communication between vehicles that would resolve some of the sensors' problems.
- In @cite , the authors use V2V for decentralized and cooperative collision avoidance for semi-autonomous vehicles, in which the control is taken from the driver once the car enters a critical area. The algorithm is tested using vehicles equipped with: differential GPS (DGPS), inertial measurement unit (IMU), dedicated short-range communication (DSRC) unit, and an interface with actuators. Their solution aims to compute an optimal throttle brake control to avoid entering the capture area, in which no control action can prevent a collision. The estimation of longitudinal displacement, velocity and acceleration is performed using Kalman filtering. This estimation takes into account a bounded communication delay found experimentally (based on their experimental results, the worst case delay is 0.4s). Their experimental results showed that all collisions can be avoided, and that the algorithm does not introduce a significant delay.
- The work in @cite develops a reliable and efficient intersection crossing protocols using V2V communication. The proposed solutions are able to avoid deadlocks and collisions at intersections. The protocols are fully distributed since they do not rely on any centralized unit such as intersection manager. The autonomous vehicles are equipped with a similar set of sensors as in @cite , and also a DSRC unit for V2V communication. The vehicles interact with each other using standardized basic safety messages (BSM) adapted for intersection crossing problem. The communication failures are not explicitly handled since it is assumed that local sensors would be able to handle this problem. The proposed protocols are tested using AutoSim simulator emulator, which utilizes a real city topography, and consists of control, communication and mobility modules. The results showed that the proposed protocols outperform the traditional traffic light protocols in terms of trip delay, especially when the traffic volume is asymmetric. Finally, this work is extended in @cite to account for GPS position inaccuracies, and also deal with roundabout intersections.
- Cooperative collision avoidance with imperfect vehicle-to-infrastructure (and vice-versa) communication is analyzed in @cite . The centralized supervisor, located at the intersection, acquires the positions, velocities, and accelerations of the incoming stream of vehicles, and then decides either to allow vehicles' desired inputs, or to override them with a safe set of inputs. The communication is subject to failures, with the success reception probability based on the Rayleigh fading channel model. According to simulation results, the mean time between the accidents is significantly increased, but a collision may still happen if the override message has been lost.
- In @cite , authors propose a novel centralized intersection crossing method (reffered to as AIM) for both autonomous and semi-autonomous vehicles. The vehicles and the intersections are considered as autonomous agents, which communicate via V2I I2V communication links. The intersection agent uses its internal reservation policy to grant, reject or request modification of the vehicles' requests. Since this policy ensures that there will never be more than one vehicle in the conflict area, the collisions are not possible. However, in case of unbounded communication failures, an indefinite delay may happen. According to their results, their approach is much faster than traditional methods based on traffic lights and stop signs.
- A hybrid centralized distributed architecture that ensures both the safety (no collisions), and the liveness (a finite crossing time), is proposed in @cite . They assumed that there are no stop signs and traffic lights at the intersection, and that the vehicles are equipped with a positioning unit, internal sensors, and a V2V communication unit. To resolve the problem with a bounded communication delay and packet losses, the rear car needs to break with maximum deceleration. They compared the proposed solution with stop-sign and traffic-light technologies and found that the average travel time is significantly reduced.
- In @cite , the authors consider intersection crossing using the controller located in the intersection center. The analysis is provided for both uplink and downlink of the imperfect communication channel used to relay the positions, velocities and the destination of all approaching vehicles. The communication is carried out via frequency division duplexing, and multipath fading is modeled with Rayleigh distribution. Then, by minimizing the failure probability, they found the optimal values of the transmit power, the number of channels, and the communication rate.
- The work in @cite focuses on semi-autonomous vehicles with the controller that can override the driver and prevent potential collision at the intersections. They define this problem as a scheduling problem and then solve it by determining the largest set of states for which there exists a control that guarantees collision avoidance (known as maximal controlled invariant set). The original problem is NP-complete, but the proposed approximation has a polynomial complexity.
- Finally, in @cite , the main contribution is a novel decentralized navigation function for autonomous vehicles with a predefined path. It takes into account the expected arrival time to the intersection, which results in a fluent traffic without unnecessary stops. The weighting factors are introduced to give a higher priority to heavier vehicles. The proposed decentralized solution is compared with a centralized one and traffic lights, in terms of energy consumption and the maximum throughput. The key result is a higher throughput, but the price is higher energy consumption than a centralized solution.
- Since this is not an exhaustive list of contributions in this area, we also refer the readers to a recent survey @cite on cooperative intersection management.
- Research on automated theorem proving has a long history @cite . Decades of research has resulted in a variety of well-developed automated theorem provers such as Coq @cite , Isabelle @cite , and E @cite . However, no existing automated provers can scale to large mathematical libraries due to combinatorial explosion of the search space. This limitation gave rise to the development of interactive theorem proving @cite , which combines humans and machines in theorem proving and has led to impressive achievements such as the proof of the Kepler conjecture @cite and the formal proof of the Feit-Thompson problem @cite .
- Premise selection as a machine learning problem was introduced by @cite , who constructed a corpus of proofs to train a kernelized classifier using bag-of-word features that represent the occurrences of terms in a vocabulary. Deep learning techniques were first applied to premise selection in the DeepMath work by @cite , who applied recurrent networks and convolutional to formulas represented as textual sequences, and showed that deep learning approaches can achieve competitive results against baselines using hand-engineered features. Serving the needs for large datasets for training deep models, @cite introduced the HolStep dataset that consists of 2M statements and 10K conjectures, an order of magnitude larger than the DeepMath dataset @cite .
- @cite were the first to apply deep networks to proof guidance. They experimented with both sequential representations and tree representations (recursive neural networks @cite @cite ). Note that their tree representations are simply the parse trees, which, unlike our graphs, are not invariant to variable renaming and do not capture how quantifiers bind variables. @cite used GRU networks to guide the exploration of partial proof trees, with formulas represented as sequences of tokens.
- Our graph embedding method is related to a large body of prior work on embeddings and graphs. Deepwalk @cite , LINE @cite and Node2Vec @cite focus on learning node embeddings. Similar to Word2Vec @cite @cite , they optimize the embedding of a node to predict nodes in a neighborhood. Recursive neural networks @cite @cite and Tree LSTMs @cite consider embeddings of trees, a special type of graphs. Neural networks on general graphs were first introduced by @cite and @cite . Many follow-up works @cite @cite @cite @cite @cite @cite proposed specific architectures to handle graph-based input by extending recurrent neural network to graph data @cite @cite @cite or making use of graph convolutions based on spectral graph theories @cite @cite @cite @cite @cite . Our approach is most similar to the work of @cite , where they encode molecular fragments as neural fingerprints with graph-based convolutions for chemical applications. But to the best of our knowledge, no previous deep learning approaches on general graphs preserve the order of edges. In contrast, we propose a novel way of graph embedding that can preserve the information of edge ordering, and demonstrate its effectiveness for premise selection.
- Automated activity recognition has been studied in many different application contexts @cite . Traditionally, a dominant approach is based on extracting representative features from video frames. Laptev introduced a bag-of-features (BoF) approach by extracting space-time interest points (STIPs) and aggregating feature descriptors by computing histograms of the visual words @cite . Wang suggested descriptors based on dense trajectories and motion boundary histograms @cite . Recently, deep learning architectures have begun to be employed and been shown to yield improved recognition performance. Ji extended the convolutional neural networks to three-dimension to deal with both spatial and temporal characteristics of activities in videos @cite . Donahue built a deep learning model having both convolutional and recurrent neural networks to capture the spatial and temporal characteristics, respectively @cite .
- In general, videos can have various combinations of spatial, temporal, and quality resolutions, which is noted as video scalability. Previous studies in several fields investigated how it affects the performance of the video systems, including surveillance @cite @cite , face recognition @cite @cite , visual perception of users @cite @cite @cite , and so on. The video scalability has been also considered in human activity recognition tasks. Harjanto investigated the impact of different frame rates on human activity recognition in several feature extraction-based approaches, and observed degradation of recognition performance for reduced frame rates @cite . See and Rahman analyzed the impact of video scalability on activity recognition using extremely low quality videos @cite . Srinivasan analyzed the performance of a method using compression-domain features (i.e., motion vectors), and confirmed the fluctuation of performance with respect to spatial and quality variations @cite . However, these studies focused only on feature extraction-based methods and furthermore, did not consider various combinations of the video scalability dimensions thoroughly.
- This work follows a great deal of recent caption-generation literature in exploiting end-to-end deep learning with a CNN image-analysis front end producing a distributed representation that is then used to drive a natural-language generation process, typically using RNNs @cite @cite @cite @cite @cite @cite @cite @cite . Our grammatical interpretation of the structural roles of words in sentences makes contact with other work that incorporates deep learning into grammatically-structured networks @cite @cite @cite @cite @cite @cite @cite @cite . Here, the network is not itself structured to match the grammatical structure of sentences being processed; the structure is fixed, but is designed to support the learning of distributed representations that incorporate structure internal to the representations themselves --- filler role structure.
- A number of prior publications have extended VAEs to model structured data by altering the underlying graphical model to dynamic Bayesian networks, such as SRNN @cite and VRNN @cite , or to hierarchical models, such as neural statistician @cite and SVAE @cite . These models have shown success in quantitatively increasing the log-likelihood, or qualitatively generating reasonable structured data by sampling. However, it remains unclear whether independent attributes are disentangled in the latent space. Moreover, the learned latent variables in these models are not interpretable without manually inspecting or using labeled data. In contrast, our work presents a VAE framework that addresses both problems by explicitly modeling the difference in the rate of temporal variation of the attributes that operate at different scales.
- Our work is also related to @math -VAE @cite with respect to unsupervised learning of disentangled representations with VAEs. The boosted KL-divergence penalty imposed in @math -VAE training encourages disentanglement of independent attributes, but does not provide interpretability without supervision. We demonstrate in our domain invariant ASR experiments that learning interpretable representations is important for such applications, and can be achieved by our FHVAE model. In addition, the idea of boosting KL-divergence regularization is complimentary to our model, which can be potentially integrated for better disentanglement.
- For the uniform sampling of simple directed graphs with a given degree sequence, the most used switch algorithm is the version, We will address this version as well. see Greenhill @cite , who gives a polynomial upper bound on the mixing time for the case of @math -regular directed graphs, and Greenhill and Sfragara @cite for some recent results on certain irregular degree sequences. The latter paper @cite only considers degree sequences for which the edge-switch Markov chain is irreducible for a given degree sequence. The Curveball chain has also been formulated for (un)directed graphs, see Carstens, Berger and Strona @cite . A theoretical analysis for the mixing time of the Curveball Markov chain was raised as an open problem there.
- All the results regarding rapid mixing mentioned above rely on the multi-commodity flow method developed by Sinclair @cite . In this work we omit multi-commodity flow techniques in order to compare the switch and Curveball Markov chains, but rather take a more elementary approach based on comparing eigenvalues of transition matrices. One seeming advantage of the eigenvalue comparison is that it allows us to compare the switch and Curveball chains for arbitrary fixed row and column sums. Our spectral gap comparisons are special cases of the classical comparison framework developed largely by Diaconis and Saloff-Coste and is based on so-called Dirichlet form comparisons of Markov chains, see, e.g., @cite @cite , and also Quastel @cite . See also the expository paper by Dyer, Goldberg, Jerrum and Martin @cite . As the stationary distributions are the same for all our Markov chains, we use a more direct, but equivalent, framework based on positive semidefiniteness. We briefly elaborate on this in Appendix for the interested reader.
- The transition matrix of the Curveball Markov chain is a special case of a heat-bath Markov chain, as introduced by Dyer, Greenhill and Ullrich @cite . Our work partially builds on @cite in the sense that we compare a Markov chain, with a similar decomposition property as in the definition of a heat-bath chain, to its heat-bath variant. We explain these ideas in the next section.
- Learning monotonic single-layer neural nets by constraining the neural net weights to be positive dates back to Archer and Wang in 1993 @cite , and that basic idea has been re-visited by others @cite @cite @cite @cite , but with some negative results about the obtainable flexibility even with multiple hidden layers @cite . Sill @cite proposed a three-layer monotonic network that used an early form of monotonic linear embedding and max-and-min-pooling. Daniels and Velikova @cite extended Sill's result to learn a partial monotonic function by combining min-max-pooling, also known as adaptive logic networks @cite , with partial monotonic linear embedding, and show that their proposed architecture is an universal approximator for partial monotone functions. None of these prior neural networks were demonstrated on problems with more than @math features, nor trained on more than a few thousand examples. For our experiments we implemented a positive neural network and a min-max-pooling network with TensorFlow.
- Fusion Methods. The joint problem of video segmentation and flow estimation has been studied by layered models @cite @cite . Nevertheless, such methods rely on complicated optimization during inference, thereby limiting their applications. Recently, significant efforts have been made along the direction of video object segmentation while considering optical flow. In @cite , a network that uses pre-computed optical flow as an additional input to improve segmentation results is developed. Different from this work, our model only requires images as the input, and we aim to jointly learn useful motion representations to help segmentation.
- Closest in scope to our work is the ObjectFlow algorithm (OFL) @cite that formulates an objective function to iteratively optimize segmentation and optical flow energy functions. However, this method is optimized online and is thus computationally expensive. In addition, it requires the segmentation results before updating the estimation for optical flow. In contrast, we propose an end-to-end trainable framework for simultaneously predicting pixel-wise foreground object segmentation and optical flow.
- Only a quick overview of the large literature related to spatial representation learning is given here, leaving aside approaches where the spatial structure of the problem is largely hard-coded @cite .
- The problem is often conceptualized as the learning of grid or place cells, inspired by neuroscience @cite . Place cells have been built as a way to compress sensory information @cite , or to improve sensorimotor and reward predictability @cite @cite @cite . Grid cells have been built as an intermediary representations in recurrent networks trained to predict an agent's position @cite @cite . Both place and grid cells have also been extracted by processing the internal state of a reservoir @cite . Representations akin to place cells and displacements have also been built from low-level sensorimotor interaction @cite .
- In RL, state representation learning is often used to solve spatial tasks (ex: navigation). Some noteworthy works build states based on physical priors @cite , system controllability @cite , action sequencing @cite , or disentanglement of controllable factors in the data @cite . Many end-to-end approaches are also applied to spatial problems without explicitly building spatial representations @cite @cite , although auxiliary tasks are sometimes used to induce spatial constraints during training @cite .
- Different flavors of Variational Auto-Encoders have been used to encode spatial factors of variation in a latent representation @cite ; a work that more recently led to the definition of disentanglement of spatial factors as in an agent's experience @cite .
- @math Traditional models @cite @cite @cite @cite @cite usually present general global features to represent the visual information, which may contain a lot of irrelevant and noisy information when predicting the answer. Attention mechanisms are therefore proposed to address this problem as they help models to focus only on the relevant region of the image according to the given question. @cite proposed stacked attention networks which employ the attention networks for multi-step reasoning, narrowing down the selection of visual information. Dynamic memory networks @cite integrated an attention mechanism with a memory model. Multi-model compact bilinear pooling @cite combined visual and textual representations to a joint representation. Hierarchical co-attention network @cite recursively fused the attended question and image features to output the answers. Dual attention networks @cite employed multiple reasoning steps based on the memory of previous attention. @cite proposed multimodal low-rank bilinear pooling (MLB) which uses element-wise multiplication to reduce complex computations of the original bilinear pooling model.
- @math Attention supervision has recently been explored in computer vision. @cite leveraged gaze tracking information to provide spatial and temporal attention for video caption generation. In image captioning, @cite proposed a method to improve the correctness of visual attention using object segments as supervision. @cite proposed to use segmentations which are manually labeled by humans to supervise attention generated during training in VQA. However, using segmentations as attention supervision is potentially inaccurate, especially for some questions that need more reasoning, for example, Or It is ineffective to segment a specific object to answer the questions. Apart from the accurate attention area, the model also needs more global information to analyze. We propose to add human-like attention supervision to models, as human-like attention contains lots of information indicating the important areas that should be focused.
- @math Research about human attention and eye tracking has already been carried out @cite @cite @cite @cite . @cite used mouse-tracking to collect large-scale human attention annotations for MS COCO. Recently, @cite published the VQA-HAT (Human ATtention) dataset which contains human attention annotations for around 10$ datasets indicate the attentive area of the human when staring at the images and are very useful for evaluation purposes. However, as mentioned in @cite @cite , collecting datasets is always a very time-consuming task and it requires a vast amount of effort. In this work, instead of collecting human labeled attention data, the HAN is proposed to generate attention maps automatically, training on the VQA-HAT dataset. The pre-trained HAN can be generally used to produce attention maps for image-question pairs.
- Computationally-secure quantum encryption has garnered significant interest in the past few years, beginning with basic security notions like and @cite @cite , and then with more advanced concepts such as quantum fully-homomorphic encryption (QFHE) @cite @cite . For authentication, uncloneability, and non-malleability, the one-time setting has received considerable attention (see, e.g., @cite @cite @cite @cite @cite @cite @cite @cite .) We will make use of the authentication definition of @cite , a characterization lemma of @cite , and a simulation adversary of @cite . For classical notions of unforgeability and chosen-ciphertext security, see e.g. @cite .
- A number of approaches for transforming the bag-of-words model or modifying linear models to increase the classification accuracy have been proposed in previous works. These approaches generally involve: 1) Reducing the dimensionality of models by removing stop words @cite , stemming terms @cite and removing the least frequent words @cite ; 2) Transforming word frequencies by TF-IDF and other measures to identify the key terms in particular classes @cite ; 3) Limiting classes to a maximum vocabulary size through methods such as mutual information metrics @cite ; 4) Using dictionaries @cite @cite @cite and encyclopaedias @cite @cite to find synonyms for rare terms. However, the names of people and organisations are unlikely to occur in dictionaries @cite @cite and may be used rarely in training text.
- @cite follow a different approach with their system called (ROSCo). Generic parametrised skills are developed by experts in the respective field. End-users can combine them by visual programming in the form of hierarchical finite state machines (HFSM). These state machines are a generic representation of a skill which then can be deployed. Other users can load these state machines and adapt the skill to their local environment, e.g. by teaching where certain behaviours should be applied to. Our system uses a mixed paradigm in which the generic skills are either created by the end-user by visual programming and subsequent autonomous playing or by field experts which make the corresponding controller available.
- In the last few years, C. Fefferman, A. Israel, and G.K. Luli @cite have been investigating Whitney type extension problems for @math maps, finding conditions to fit smooth functions to data.
- Recently NLP techniques have been successfully applied to computational social science. Combined with social network analysis, textual content analysis has shown promise in applications such as prediction of moral value @cite @cite , power @cite @cite @cite , expertise @cite , leadership role @cite , personality @cite @cite , gender @cite @cite , hate speech @cite @cite , and social interaction @cite @cite . This work has extensively studied textual (e.g., @math -gram and LIWC) and structural features (e.g., Twitter relationships) on a variety of online platforms.
- Deep convolutional neural networks have been applied successfully to a wide range of computer vision problems, including image recognition @cite @cite @cite @cite , semantic segmentation @cite @cite @cite , object detection @cite @cite @cite @cite @cite , etc . For object detection, one of the most popular pipeline @cite @cite involved first extracting a number of regions named object proposals @cite @cite @cite @cite , and then determining if each of them belongs to the target class. Bounding-box regression and non-maximum suppression were attached for post-processing. This framework significantly outperforms the deformable part-based model @cite trained on top of a set of handcrafted features @cite .
- Detecting semantic parts under occlusion is an important problem but was less studied before. @cite combined multiple visual concepts via the geometric constraints, i.e. , the spatial distribution of the visual concepts related to the target semantic parts, to obtain a strong detector. Different from @cite , DeepVoting implements visual concept extraction and the geometric relationships as two layers, and attach them directly to the intermediate outputs of a deep neural network to perform an end-to-end training. This yields much better performance compared to @cite .
- : Most previous attack work focuses on adversarial examples for computer vision tasks. Multiple techniques to create such adversarial examples have been developed recently. Broadly, such attacks can be categorized as either using costs gradients @cite @cite @cite @cite or the forward gradient of the neural network @cite and perturbing along most promising direction or directly solving an optimization problem (possibly using gradient ascent descent) to find a perturbation @cite @cite . In addition, adversarial examples have been shown to transfer between different network architectures, and networks trained on disjoint subsets of data @cite @cite . Adversarial examples have also been shown to translate to the real world @cite , that is, adversarial images can remain adversarial even after being printed and recaptured with a cell phone camera. Attacks on non-neural networks have also been explored in literature @cite . Our approach is distinctly different from all these approaches as we pose the problem of generating adversarial samples as a generative learning problem, and demonstrate generation of adversarial examples given access to any given classifier. Our approach also applies to any classifier that output class probabilities and not just neural networks.
- More closely related to our work are some defense techniques that have focused on detecting and filtering out adversarial samples @cite @cite or de-noising input @cite ; here the filter or de-noiser with the classifier could be considered as a larger neural network. However, unlike these work, our goal for DLN is targeted sanitization. Moreover, recent attack work @cite have produced attack techniques to defeat many known detection techniques. Our technique provides the flexibility to be resilient against more powerful attacks by training the DLN with such an attack for high perturbation attacks or using NAC for low perturbation attacks.
- Lastly, two concurrent unpublished drafts (available online) have proposed an attack @cite similar to ALN and a defense @cite apparently similar to DLN. The difference for the attack work is in using the class label vs classifier output in @math term for the attack. For the defense work, we differ as we show how DLN technique extends to multiple attacks and can be repeatedly used in an attack-defense competition. Moreover, unlike these drafts, we provide another defense technique NAC that works against CW, define robustness and show that our defense techniques approximately aims to achieve our definition of robustness. Further, our formal reasoning reveals the underlying nature of attacks and defenses.
- We follow a data-driven approach to dialog systems. , , and optimize the dialog policy using Reinforcement Learning or the Partially Observable Markov Decision Process framework. In addition, propose to use a predefined ontology as a logical representation for the information exchanged in the conversation. The dialog system can be divided into different modules, such as Natural Language Understanding @cite @cite , Dialog State Tracking @cite @cite , and Natural Language Generation @cite . Furthermore, and propose end-to-end trainable goal-oriented dialog systems.
- Recently, short text conversation has been popular. The system receives a short dialog context and generates a response using statistical machine translation or sequence-to-sequence networks @cite @cite @cite @cite @cite @cite . In contrast to response generation, the retrieval-based approach uses a ranking model to select the highest scoring response from candidates @cite @cite @cite @cite . However, these models are single-turn responding machines and thus still are limited to short contexts with only two speakers. As for larger context, propose the Next Utterance Classification (NUC) task for multi-turn two-party dialogs. extend NUC to multi-party conversations by integrating the addressee detection problem. Since the data is text based, they use only textual information to predict addressees as opposed to relying on acoustic signals or gaze information in multimodal dialog systems @cite @cite .
- Furthermore, several other papers are recently presented focusing on modeling role-specific information given the dialogue contexts @cite @cite @cite . For example, combine content and temporal information to predict the utterance speaker. By contrast, our SIRNN explicitly utilizes the speaker interaction to maintain speaker embeddings and predicts the addressee and response by joint selection.
- To effectively represent a collection of shapes with the same connectivity, a naive solution is to take the vertex positions as the representation. Such representations however are not translation or rotation invariant. Among various representations, the RIMD representation @cite is translation and rotation invariant, and suitable for data-driven shape analysis. Therefore, we use it to represent shapes in our framework.
- A natural application for shape representations is to interpolate or blend shapes. Existing methods can be largely categorized into geometry-based (e.g. @cite ) and data-driven (e.g. @cite ) methods. The latter exploits latent knowledge of example shapes, and so can produce more convincing interpolation results when such examples are available. Our method is a generic framework and as we will show later, it produces comparable or even better results than state-of-the-art data-driven methods. @cite and @cite propose map-based methods to describe distortions between models, which can be used with PCA (Principal Component Analysis) to produce linear embedding, while our embedding application is non-linear.
- @cite propose to learn CNN models using geometry images @cite , which is then extended to synthesize 3D models @cite . Geometry images allow surface details to be well preserved. However, the parameterization used for generating geometry images involves unavoidable distortions and is not unique. This is more challenging for shapes with complex topology (e.g. high-genus models). @cite develop a learning framework to abstract complex shapes and assemble objects using 3D volumetric primitives. @cite propose a neural network architecture for encoding and synthesizing 3D shapes based on their structures. @cite use a variational encoder to synthesize 3D objects segmented into parts. Both @cite and @cite are designed to synthesize man-made objects and require part-based segmentation as input, and cannot cope with unsegmented or deformed shapes addressed in this paper. Such methods can produce shapes with complex structures, but the level of details is restricted to the components and primitives used. We propose a general framework that uses a rotation invariant mesh representation as features, along with a variational encoder to analyze shape collections and synthesize new models. Our method can generate novel plausible shapes with rich details, as well as a variety of other applications.
- Another way to interpret network is @cite @cite @cite @cite by analyzing how the network works. The influence functions @cite are proposed to trace the model prediction. Lakkaraju @cite explored the knowledge blind spots of neural networks in a weakly-supervised manner. Recently, a unified framework has been proposed to interpret network prediction @cite . To better understand the classification process, a decision tree has been used in @cite . Moreover, Alain @cite proposed to add a linear classifier to each layer and then explore the information of the features. However, it only considers the difference between features in different layers, while the intra-layer difference is not involved in consideration. Wang @cite interpreted the network by analyzing the routing path of different inputs. It, however, does not train the network to improve the network interpretability, such that the network still has many uninterpretable parameters. Different from all these methods, we interpret the working principle of the network by disentangling the network architecture, upon which we further decouple each filter in the intra-layer to explore the interpretable semantic concepts across nodes on the calculation path.
- Differing from defining an expert network to decide the conditional computation, Figurnov . @cite dynamically adjusted the number of executed ResNet layers for specific image regions by halting scores. Similar to the previous work, Wang . @cite accelerated the network by skipping the convolutional block based on the activations of previous block. Liu . @cite transformed a network into directed acyclic graph of differentiable modules and achieved dynamically selected executed neurons by using control module. Recently, a feature boosting and suppression method @cite was used to skip unimportant convolutional output channels at runtime . However, it skipped the same number of filters for each layer, which does not consider the inter-layer difference. Different from the above works, we employ a novel architecture controlling module to decouple network architecture by training them fit to data distribution. The network after training by this module becomes interpretable, facilitating us to visualize the intrinsic network structure, to accelerate the network inference, and to conduct adversarial attacking detection.
- Besides the direct channel, can also release location information. An important side channel in wireless networks is the transmission activity, which can be monitored by a wireless eavesdropper to track the user. To defend against wireless eavesdropping, mechanisms are proposed to protect senders receivers using anonymous routing protocols, frequently changing pseudonyms, silent periods, and reduced transmission power @cite @cite . The above mechanisms are in that they modify the user's behavior. In contrast, we study another side channel arising in MECs due to correlated user mobility and service mobility, and propose a defense mechanism using chaffs.
- The idea of using chaffs to protect user security privacy has been explored in other contexts. In communication networks, @cite uses dummy packets as chaffs to hide the true traffic rates, and @cite furthers the idea to hide the transmission patterns of multi-hop flows. In cloud computing, @cite proposes to use decoy data to protect the real data during a data theft attack. Similarly, @cite proposes to use decoy applications running on fake inputs to confuse an insider attacker. However, we are the first to study the use of chaff services to protect user location privacy. Besides the novel application context, our problem also requires new methodology. Specifically, as a real service needs to migrate dynamically to follow a mobile user, its mobility pattern (in addition to its content) can be used to identify the service. To effectively protect the user, the chaff services have to resemble the real service in both content and mobility. =-1
- Another line of related work is service migrations in MECs. Service migrations in MECs are primarily driven by the need to keep a service close to its user, where the decision on whether to migrate a service depends on both the migration cost (if migrating the service to follow the user) and the communication cost (if serving the user from the original location as the user moves away). Modeling the user's mobility as a , several solutions based on have been proposed to minimize the total cost under 1-D @cite @cite or 2-D mobility models @cite @cite . Here we consider the worst case (in terms of location privacy) that the real service follows the user, and focus on protecting the user's location privacy using chaffs. We leave the study of privacy-aware service migration to future work.
- Empirical studies @cite @cite in face recognition proved that a minimum face resolution between @math and @math is required for most stand-alone recognition algorithms, whose performance would be much degraded when applied with even lower resolutions @cite @cite . In the emotion recognition literature, most existing methods assumed the availability of HR frontal faces. @cite first investigated the effects of different image resolutions for facial expression analysis. The author concluded that while the performance difference was negligible when the head region resolution was @math or higher, the recognition turned growingly unreliable when head region resolution was lower than @math . It is thus desirable to obtain more robust features for LR images and low-intensity expressions @cite
- When dealing with LR subjects, the traditional two-stage pipeline tried to first apply SR algorithms before perform recognition tasks. Recently, the SR performance has been noticeably improved, with the aid of deep network models @cite . However, the recovered HR images inevitably over-smoothened details. More importantly, such a straightforward approach yields the sub-optimal performance: the artifacts introduced by the reconstruction process will undermine the final recognition. @cite presented a close-the-loop approach of image restoration and recognition, based on the assumption that the degraded image, if correctly restored, will also have a good identifiability. @cite advanced the methodology using a deep network trained from end to end, and observed the possibility of robust object recognition even when the region of interests (ROI) was smaller than @math pixels. However, it remains to be an open issue how much the performance degradation can be remedied in the same way for emotion recognition.
- One common approach to the links selection problem is to perform A B testing @cite , which splits the traffic for different sets of links on two different web pages, and then evaluate their rewards. However, A B testing does not have any loss regret guarantee as it splits equal amounts of traffic to the links regardless of the links' rewards. That said, A B testing is still widely used in commercial web systems. Our algorithm can be viewed as a complementary approach to A B testing, e.g., our algorithm can select the set of links with the @math st level reward above a given threshold and facilitate a more efficient A B testing for the links selection problem.
- Our bandit formulation is related to the bandit models with multiple plays, where multiple arms are selected in each round. @cite presented the bandit algorithm that extends the single-played algorithm @cite to multiple-played cases using exponential weights. @cite proposed an algorithm that selects multiple arms with the highest upper confidence bound (UCB) indices. @cite presented the multiple-play Thompson Sampling algorithm (MP-TS) for arms with binary rewards. @cite proposed a bandit-based ranking algorithm for ranking search queries. Our bandit model differs from these bandit models as we further consider the constraint on the total @math st level rewards in selecting the multiple arms.
- A basic assumption in decomposition-based EMO is that the diversity of the weight vectors will result in the diversity of the Pareto optimal solutions. This motivates several studies on how to generate a set of uniformly distributed weight vectors @cite , such as the simplex-lattice design @cite , two-layer simplex lattice design @cite , multi-layer simplex lattice design @cite , uniform design @cite , and a combination of the simplex-lattice design and uniform design @cite . A weakness of such systematic weight generators is that the number of generated weight vectors is not flexible, especially in a high-dimensional space. This contrasts with the uniform random sampling method @cite which can generate an arbitrary number of weight vectors for any dimension. In addition, some work has shown that if the geometry of the problem is known then the optimal distribution of the weight vectors for a specific scalarizing function can be readily identified @cite @cite .
- An intuitive solution to this problem is to adaptively update the weight vectors during the optimisation process. Several interesting attempts have been made along this line. In early studies @cite @cite @cite , researchers considered randomly-generated weight vectors (search directions) at each generation. This can make more computational cost allocated on the area around the nondominated solutions found so far @cite . Recently, @cite presented that this strategy could be helpful in dealing with MOPs having an irregular Pareto front. They introduced an external population, which is used to store promising solutions by the @math dominance relation @cite , to help generating the random weights.
- Adapting the weight vectors affects the convergence of the algorithm. Varying the weight vectors essentially changes the subproblems. After a change of the subproblems, their associated solutions need to readjust the search directions. This could lead to the solutions to in the objective space during the optimisation process @cite .
- There is a surprisingly small amount of discussions of abstraction and its relationships with complexity and specificity (or generality) in the literature on software design. One exception is Kramer's contribution in @cite . He also states that there are two types of abstractions, a simplifying and generalising one. However, the article focuses on the abstraction skills of computer sciences students and, hence, does not discuss the consequences of this definition for software design.
- Because we consider also the elements of a (domain-spe -ci -fic) language as artefacts, the general literature on the design of programming language is also relevant @cite @cite @cite @cite . It defines several properties or characteristics of languages. The is a desired property of a language. This is related to abstractness and complexity. A language that is simple and uses abstract concepts is more readable than a language with many elements and technical details. The complexity of a language is also recognised as an important principle in language design by requiring . However, a definition of simplicity and its tradeoffs is not given in the literature. Related to our work is also the principle of or of programming languages. It describes the easiness to express complex processes and structures. We can describe this issue by the abstractness and the specificity. Very abstract language elements (using simplifying abstraction) allow to express those complex processes and structures concisely. The genericity of such an element then allows to alter certain parts only. Hence, this also contributes to programming efficiency.
- Our work is also related to the work on domain-specific languages, e.g., @cite . It is explained there that the design of a domain-specific language must pay off in terms of more efficient development and maintenance. However, the essential tradeoff between specificity and complexity is not explicitly stated. It is also noted that a domain-specific language can be an application library or simply embedded into a so-called general purpose language by abstract data types. This supports our view that all kinds of software design are similar to this respect.
- Prenninger and Pretschner discuss abstractions for model-based testing in @cite . Although this is not general software design the essential ideas apply. They also see abstraction as losing information that can either be automatically inserted or not. The main goal for abstraction is given as simplification because generalising abstraction is not discussed in that paper. However, it is also stressed that abstraction (especially the automatically resolvable one) tends to be highly domain-specific. We are able to show why this is the case using our abstraction types.
- Finally, the paper of Bernholdt, Nieplocha, and Sadayappan @cite is an example that neglects the tradeoffs discussed in this paper. They classify languages for high-performance computing in the dimensions abstraction'' and generality''. Based on this it is stated that the ultimate goal of further language developments should be to rank high in both dimensions, i.e., that new languages should be more abstract and more general at the same time. We show that this is only comes at the cost of higher dynamic complexity although their first goal was complexity reduction.
- The goal of is to restore the original image from its downscaled version. Many end-to-end CNN-based solutions exist now @cite @cite @cite @cite @cite . Initial methods used pixel-wise mean-squared-error (MSE) loss functions, which often generated blurry results. Losses based on the activations of (a number of) VGG-layers @cite and GANs @cite are more capable of recovering photorealistic results, including high-frequency components, hence produce state of the art results. In our work, we incorporate both the GAN architectures and VGG-based loss functions.
- @cite @cite @cite , which attempts to regress the 3 RGB channels from images that were reduced to single-channel grayscale, strongly benefits from the GAN architecture too @cite . Image , and @cite @cite @cite @cite @cite , photographic @cite and @cite , as well as @cite are another improvements and adjustments that are included in our learned model. As opposed to mentioned related work, there is no need to manually model these effects in our case.
- Zhu al @cite loosen this constraint by expressing the loss in the space of input rather than output images, taking advantage of a backward mapping CNN that transforms the output back into the space of input images. We apply a similar idea in this work. However, our CNN architecture and loss functions are based on different ideas: fully convolutional networks and elaborated losses allow us to achieve photorealistic results, while eliminating typical artifacts (like blur and checkerboard) and limitations of encoder-decoder networks.
- Finally, Ignatov al @cite propose an end-to-end enhancer achieving photorealistic results for arbitrary-sized images due to a composition of content, texture and color losses. However, it is trained with a strong supervision requirement for which a dataset of aligned ground truth image pairs taken by different cameras was assembled ( , the DPED dataset). We build upon their loss functions to achieve photorealism as well, while adapting them to the new architecture suitable for our weakly supervised learning setting. While we do not need a ground truth aligned dataset, we use DPED to report the performance on. Additionally, we provide the results on public datasets (KITTI, Cityscapes) and several newly collected datasets for smartphone cameras.
- Decoy routing, a new alternative, inserts steganography information into the header of SSL packets which is recognized by decoy routers'' in the backbone, who re-route packets to their intended destination @cite . One drawback is that nodes currently have to attempt connections at random until they find the decoy routing node.
- where the arrow notation means ordered event pairs, i.e. event @math occurs before event @math . CP consists of two terms: the first is pair-wise mutual information (PMI) and the second is relative ordering of bigrams. PMI measures how often events occur as a pair (without considering their order); whereas relative ordering accounts for the order of the event pairs because temporal order is one of the strongest cues to causality @cite @cite @cite . This work explicitly links their definitions to research using the Penn Discourse Treebank (PDTB) definition of contingency .
- Previous work on belief change has primarily focused on single, experimentally crafted, persuasive messages, rather than exploring whether user-generated dialogic arguments can be repurposed to persuade. Recently however several papers have begun to investigate two challenges in argument mining: (1) understanding the structure of an argument and extracting argument components @cite @cite @cite @cite @cite ; and (2) understanding what predicts the persuasiveness of web-sourced argumentative content @cite @cite @cite @cite @cite .
- Previous work also tests the hypothesis that dialogic exchanges might be more engaging, in the context of expository or car sales dialog @cite @cite @cite @cite . Work comparing monologic vs. dialogic modes of providing information suggest that dialogs: (1) are more memorable and engaging, (2) stimulate the audience to formulate their own questions, and (3) allow audiences to be more successful at following communication @cite @cite @cite @cite @cite @cite @cite .
- Other work @cite explores how user-interface factors (e.g., number and order of argument presentation, whether and how arguments are rated) affect how readers process arguments. Several factors increased the number of passages read, including explicitly presenting contrasting viewpoints simultaneously. This exercise caused people with strong beliefs (about the healthiness of milk) to moderate their views after 20-30 minutes of concentrated study. We do not concentrate on interface factors, instead exploring how persuasiveness relates to audience factors and argumentative style. Also our experiments are run online with hundreds of users, rather than as a controlled study in the lab.
- Deep learning is being used with increasing frequency to solve problems in the domain of computational sustainability and urban planning. At a broader level, CNNs have been extensively used in computer vision applications in recent years, and have achieved state of the art results in image classification and object recognition @cite @cite @cite . New types of network layers, such as batch normalization and dropout, have also been developed to improve the accuracy of CNNs @cite @cite . Convolutional neural networks have been used to predict the spatial distribution of poverty in developing countries by using nighttime lights as a data rich target for a transfer learning task @cite @cite . Pre-trained CNNs have recently been shown to be effective at the problem of remote sensing image scenes classification through the tuning a small number of layers @cite @cite . Similarly, deep learning has been shown to be effective in the task of classifying land cover type, with recent work that has achieved high classification accuracy on new large land cover datasets using mixed CNN based approaches @cite @cite .
- The most similar work to ours also uses CNNs to estimate population from satellite imagery @cite . The motivation of this paper is similar to ours, as we both attempt to create high-resolution gridded population counts for use in planning applications. This paper estimates population in Kenya at a 8km @math resolution with a CNN trained on data from Tanzania at a @math satellite pixel resolution. The author's propose a way to use their CNN's output as a weighted surface for population disaggregation, and compare this method to other methods for disaggregating population counts in Kenya. Our work differs in several important ways. First, we focus on validating our model's predictions as raw population projections and do not consider using our model's prediction as a weighted surface for distributing population counts. If the population (or projected population) of an area is known a priori, then any population method can degrade into a weighting scheme. Secondly, we focus on interpreting the results of our model as a way of validating its ability to generalize. Thirdly, we apply our method to the entire US using census block derived training and testing data.
- On average, county population can be reliably extrapolated over short time horizons with simple linear models, however if some counties experience disproportionally higher or lower growth rates, more complicated models are needed @cite . The US Census has led research into population and demographic projections, and uses a variety of different population and demographic projection methods to create sub-national projections broken down by age, sex, and race @cite @cite . Census postcensal projections, projections done in between census years, are created with a method known as the ratio-correlation method @cite @cite @cite . This method uses the current year's estimated population, number of live births, registered vehicles, public school enrollment, registered voters, deaths, and other information to determine the estimated population change at the next census date. More recently, the American Community Survey has been used as annual supplemental surveys to update the demographics profiles of a variety of sub-national areas in between census years @cite @cite .
- Regarding geometric approaches, one possible strategy to align histology and clinical imaging is to simplify the images into their contours, so as to come down to a monomodal registration problem and use the shape information provided by the external boundaries. In @cite , contours from both histology and slices from a rat brain atlas were extracted via thresholding and represented using B-splines. Then, they were described by means of sets of affine invariants constructed from the sequence of area patches bounded by the contour and the line connecting two consecutive inflections. In @cite , Curvature Scale Space @cite was used for the registration of whole-slide images of histological sections in order to represent shape (the tissue boundary) at various scales. In @cite , curvature maps at different scales were used to match boundaries of full brain MRI extracted via an active contour algorithm. The main weaknesses of active contours are the number of parameters and the sensitivity to initialisation.
- At present, most existing RE-ID models are in a supervised manner. They are mainly based on learning distance metrics or subspace @cite @cite @cite @cite @cite @cite @cite , learning view-invariant and discriminative features @cite @cite @cite , and deep learning frameworks @cite @cite @cite @cite . However, all these models rely on substantial labeled training data, which is typically required to be pair-wise for each pair of camera views. Their performance depends highly on the quality and quantity of labeled training data. In contrast, our model does not require any labeled data and thus is free from prohibitively high cost of manually labeling and the risk of incorrect labeling.
- Apart from our model, there have been some clustering-based metric learning models @cite @cite . However, to our best knowledge, there is no such attempt in RE-ID community before. This is potentially because clustering is more susceptible to view-specific interference and thus data points from the same view are more inclined to be clustered together, instead of those of a specific person across views. Fortunately, by formulating asymmetric learning and further limiting the discrepancy between view-specific transforms , this problem can be alleviated in our model. Therefore, our model is essentially different from these models not only in formulation but also in that our model is able to better deal with cross-view matching problem by treating each view asymmetrically. We will discuss the differences between our model and the existing ones in detail in Sec. .
- Fleuret al @cite have already presented a dataset with abstract classes, using very simple black and white line drawings. This dataset is somewhat reminiscent of the Bongard problems'', presented by Bongard in 1970 @cite as a set of problems that, according to Bongard, neural networks would never be able to solve (though he did not have simple classification in mind, but a textual description of what separates the two classes). In previous work @cite @cite , we have tested different convolutional neural network architectures on the dataset by Fleuret al and came to the conclusion that current CNN architectures have shortcomings when shape comparison is needed to distinguish two classes. As Dodge al @cite argue, the Fleuret dataset is too simplistic and too far from natural images to draw any practical conclusions from it. Our goal is to present a more realistic dataset, with abstract classes, that is equally hard to classify for CNNs.
- In a lot of applications, the process of pose normalisation and object recognition are disjoint. For example, in the breakthrough deep learning face recognition paper DeepFace, Taigman al @cite use a 3D mean face as preprocessing, before feeding the pose-normalised image to a CNN.
- The original STN @cite aimed to combine these two processes into a single network that is trainable end to end. The localiser network estimated a 2D affine transformation that was applied to the regular output grid meaning the network could only learn a fairly restricted space of transformations. Jaderberg al @cite also proposed the concept of a 3D transformer, which takes 3D voxel data as input, applies 3D rotation and translation, and outputs a 2D projection of the transformed data. Working with 3D (volumetric data) removes the need to model occlusion or camera projection parameters. In contrast, we work with regular 2D input and output images but transform them via a 3D model.
- Our localiser learns to fit a 3DMM to a single image. This task has traditionally been posed as a problem of analysis-by-synthesis and solved by optimisation. The original method @cite used stochastic gradient descent to minimise an appearance error, regularised by statistical priors. Subsequent work used a more complex feature-based objective function @cite and the state-of-the-art method uses Markov Chain Monte Carlo for probabilistic image interpretation @cite .
- The DenseReg @cite approach uses fully convolutional networks to directly compute dense correspondence between a 3D model and a 2D image. The network does not explicitly estimate or model 3D pose or shape (though these are implied by the correspondence) and is trained by using manually annotated 2D landmarks to warp a 3D template onto the training images (providing the supervision). Sela al @cite also use a fully convolutional network to predict correspondence and also depth. They then merge the model-based and data-driven geometries for improved quality.
- Richardson al @cite take a step towards removing the need for labels by presenting a semi-supervised approach. They still rely on supervised training for learning 3DMM parameter regression but then refine the coarse 3DMM geometry using a second network that is trained in an unsupervised manner. Very recently, Tewari al @cite presented MoFA, a completely unsupervised approach for training a CNN to regress 3DMM parameters, pose and illumination using an autoencoder architecture. The regression is done by the encoder CNN. The decoder then uses a hand-crafted differentiable renderer to synthesise an image. The unsupervised loss is the error between the rendered image and the input, with convergence aided by losses for priors and landmarks. Note that the decoder is exactly equivalent to the differentiable cost function used in classical analysis-by-synthesis approaches. Presumably, the issues caused by the non-convexity of this cost function are reduced in a CNN setting since the gradient is averaged over many images.
- While the ability of @cite to learn from unlabelled data is impressive, there are a number of limitations. The complexity required to enable the hand-crafted decoder to produce photorealistic images of any face under arbitrary real world illumination, captured by a camera with arbitrary geometric and photometric properties, is huge. Arguably, this has not yet been achieved in computer graphics. Moreover, the 3DMM texture should only capture intrinsic appearance parameters such as diffuse and specular albedo (or even spectral quantities to ensure independence from the camera and lighting). Such a model is not currently available.
- Challenges and group evaluations have a long history in the field of computer vision and biometrics in particular. The goal of these events is to benchmark existing work in a certain problem domain, provide a snapshot of the current state-of-technology and point to open issues that need to be addressed in the future. As a result of these challenges, significant advancements have been made over the years that pushed the capabilities of computer-vision technology. Examples of recent challenges that had a profound impact on the field of computer vision are the ImageNet Large Scale Visual Recognition Challenges (ILSVRC) @cite @cite , which focus on image classification and object localization problems, the Visual Object Tracking (VOT) @cite @cite @cite challenges that aim at evaluating various solutions to object tracking in videos, and the ChaLearn Looking at People @cite @cite @cite series of challenges, where human-centric vision problems are at the center of attention.
- Significant work has been done in generating interior structure of a model to meet various geometric and or physical properties. @cite hollows a 3D-printed object while maintaining its structural strength by adding some internal struts. The interior is optimized by a reduced-order parameterization of offset surfaces in @cite . Various internal structures, such as the skin-frame structure @cite , the honeycomb-like structure @cite , and the medial axis tree structure @cite , were developed for cost-effective purposes while preserving the structural strength of printed objects. Both static balance and dynamic balance have been studied by designing the interior infills as well as changing the model shapes @cite @cite @cite . Instead of designing hollowing structure explicitly, a lot of efforts have been put on topology optimization to obtain distributions of material according to certain performance criteria during the last three decades @cite @cite . However, these works have not handled the problem of avoiding large overhangs. We study this problem by developing a carving operator via support-free elliptic voids.
- * Audio-only speech enhancement and separation Previous methods for single-channel, or monaural, speech enhancement and separation mostly use audio only input. The common approach generates masking matrices containing time-frequency (TF) components dominated by each speaker @cite @cite . Huang @cite are among the first to use a deep learning-based approach for speaker dependent speech separation.
- * Audio-visual speech processing Recent research in audio-visual speech processing makes extensive use of neural networks. The work of Ngiam @cite is a seminal work in this area. Neural networks with visual input have been used for lipreading @cite , sound prediction @cite and for learning unsupervised sound representations @cite .
- Aiming at encoding both local static appearance and motion information, as in the HOG3D, but avoiding high dimensionality and a relatively expensive quantization cost, @cite proposed the Gradient Boundary Histograms (GBH). Instead of using image gradients, the authors use time-derivatives of image gradients to emphasize moving edge boundaries. For each frame, they compute image gradients and apply temporal filtering over two consecutive gradient images. Then, they compute the magnitude and orientation for each pixel which are used to build a histogram of orientation as in HOG.
- Aiming at capturing richer information from the optical flow, @cite proposed the Optical Flow Co-occurrence Matrices (OFCM). The descriptor is based on the extraction of a set of statistical measures from co-occurrence matrices computed using the magnitude and orientation from optical flow information. Their hypothesis for designing the OFCM is based on the assumption that the motion information on a video sequence can be described by the spatial relationship contained on local neighborhoods of the flow field.
- By employing the aforementioned two-stream network, @cite conducted experiments showing the impact on results when changing the network architecture. In addition, they also introduced some data augmentation techniques to improve the network training. To that end, the authors used three distinct architectures (ClarifaiNet @cite , GoogLeNet @cite and VGG-16 @cite ) showing that the best results are achieved by VGG-16 deeper architecture. Afterwards, the authors improved it to the Temporal Segment Networks (TSN) @cite by studying different types of input modalities to two-stream and by employing the Inception with batch normalization network architecture @cite .
- To make a spatial network learn to relate which parts of the image are moving, @cite proposed a feature amplification technique by using magnitude information of the optical flow on the spatial network. To that end, they extract features maps of the last convolutional layer of the spatial network, compute optical flow magnitudes and resize it to be the same size of the previously extracted feature maps. Finally, they perform element-wise product to amplify the activations. Our work differs from them in that we use the magnitude information right on the beginning of the network, letting it learn how the velocity information contributes on the activity recognition process.
- As it can be inferred from the reviewed methods, most of them use either convolution operations over raw pixels or optical flow to model temporal information. The former do not decouple spatial and temporal information, letting appearance information prevail @cite , while the latter approaches rely on horizontal and vertical components of the optical flow. Despite the optical flow-based methods produce promising results, they focus only on displacement information. In view of that, aiming at capturing more information from the optical flow, our method captures not only the displacement, by using orientation, but also captures the magnitude providing information regarding the velocity of the movement.
- The principle states that it has to be difficult to receive the visual information of interest @cite . For example, DAS @cite is a simple graphical password that allows the user to create a free-form drawing on a touchscreen and to use it as her password. Decoy Strokes @cite is a shoulder-surfing resistant variation of DAS that draws strokes alongside the user's password to confuse a malicious observer. The problem with such schemes is that the user is exposed to the same distracting information and may end up confused as well, leading to slower authentication and more frequent input errors. Also, if the attacker is able to observe multiple times during the authentication process or to record it, he may be able to steal the credentials of the user.
- The principle states that it has to be difficult to process the acquired visual information @cite . For example, in one of the cognitive trapdoor games @cite , the user is required to enter her PIN in the following way. The digits on the provided keypad are separated into two sets based on their color; half of them are black, and half of them are white. The user selects the set that the current digit of her PIN belongs to, and then the digits are reassigned to the two color sets. This procedure is repeated until the scheme is able to uniquely determine the correct digit by intersecting the selected sets. Then, the user proceeds to the next digit of her PIN. For an observer, it is extremely difficult, if not impossible, to successfully perform sequential intersections of sets to extract the correct PIN. However, such schemes are usually complex for the users too, with all the inevitable consequences for usability. In addition, recorded material or even repeated observation may reveal the authentication credentials, since all the useful information is observable.
- The principle states that the required input has to change in every authentication attempt @cite @cite . For example, Deja Vu @cite presents to the user a number @math of images, and asks her to specify which of them belong to a predefined set of images, called the user's portfolio. In each authentication attempt, different images from the portfolio are assigned to the set of the @math images. As a consequence, an observer cannot learn the portfolio of the user in a limited number of observations. However, multiple observations may reveal the whole portfolio. In general, with such schemes is difficult for the user to get familiar with a standard input. For example, with Deja Vu the user has to identify different pictures in every authentication attempt. This requires additional cognitive effort and may affect the authentication time and the error rate.
- The principle states that at least part of the information of interest has to be transmitted through channels that are not observable. This way, an observer is always missing a piece of information and shoulder-surfing is mitigated even in cases that multiple observations or recordings are possible. However, the performance of such schemes in usability and deployability varies. For example, schemes which use audio and haptic information @cite @cite suffer from high authentication time and their requirements for additional hardware heavily affect their deployability. However, the emergence of touchscreen devices which are pressure-sensitive may favor schemes which use this kind of haptic information @cite @cite @cite @cite @cite @cite , if they are combined with satisfying usability. Gaze-based authentication @cite corresponds to high authentication time, and authentication based on brainwaves @cite or bone conduction @cite requires special equipment. Fingerprint authentication @cite is a scheme that gains popularity nowadays by offering excellent usability. A usual problem with biometrics like fingerprints is that they cannot be revoked. In addition, biometrics can be used to uniquely identify a person and they raise privacy concerns. However, the concept of cancelable biometrics @cite could alleviate both problems.
- The CT sinogram represents the attenuation line integrals from the radial views and is the raw projection data in the CT scan. Since the sinogram is also a 2-D signal, traditional image processing techniques have been applied for noise reduction, such as bilateral filtering @cite , structural adaptive filtering @cite , etc. The filtered data can then be reconstructed to a CT image with methods like filtered back projection (FBP). Although the statistical property of the noise can be well characterized, these methods require the availability of the raw data which is not always accessible. In addition, by application of edge preservation smoothing operations (bilateral filtering), small edges would inevitably be filtered out and lead to loss of structure and spatial resolution in the reconstructed CT image.
- Note that the above method only performs a single back projection to reconstruct the original image. Another stream of works performs an additional forward projection, mapping the reconstructed image to the sinogram domain by modelling the acquisition process. Corrections can be made by iterating the forward and backward process. This methodology is referred as model-based iterative reconstruction (MBIR). Usually, MBIR methods model scanner geometry and physical properties of the imaging processing, e.g. the photon counting statistics and the polychromatic nature of the source x-ray @cite . Some works add prior object information to the model to regulate the reconstructed image, such as total variation minimization @cite @cite , Markov random fields based roughness or a sparsity penalty @cite . Due to its iterative nature, MBIR models tend to consume excessive computation time for the reconstruction. There are works that are trying to accelerate the convergence behaviour of the optimization process, for example, by variable splitting of the data fidelity term @cite or by combining Nesterov's momentum with ordered subsets method @cite .
- Convolutional neural network (CNN) based methods have recently achieved great success in image related tasks. Although its origins can be traced back to the 1980s, the resurgence of CNN can be greatly attributed to increased computational power and recently introduced techniques for efficient training of deep networks, such as BatchNorm @cite , Rectifier linear units @cite and residual connection @cite . @cite first used CNN to denoise CT images by learning a patch-based neural net and later on refined it with a encoder and decoder structure for end-to-end training @cite . @cite devised a 24 convolution layer net with by-pass connection and contracting path for denoising but instead of mapping in the image domain, it performed end-to-end training in the wavelet domain. @cite adopted perceptual loss into the training, which measures the difference of the processed image and the ground truth in a high level feature space projected by a pre-trained CNN. @cite proposed to use a massive-training artificial neural network (MTANN) for CT denoising. The network accepts local patches of the LDCT and regressed to the center value of the corresponding patch of the convCT.
- To reduce the annotation effort, many well-known studies propose segmentation methods employing simple forms of user annotations to obtain voxel-wise segmentations . While adjusting hyperparameters can be considered an interaction, in this study we concentrate on simplified forms of pictorial input , called weak annotations (WA). Such annotations have been extensively used in the literature, particularly in the context of medical object segmentation problems. @cite used user-provided scribbles (SC) or brush strokes as input and hard constraints to an interactive graphical segmentation problem. Similarly, @cite , @cite and @cite expressed this problem in a spatially continuous setting by using prior region ordering constraints and exploiting parallelism via GPU computing. The GrabCut algorithm employs rectangular regions (RR) as bounding boxes to both compute a colour appearance model and spatially constrain the search for an object. These spatial constraints further allow to reduce the computational effort . The segmentation platform ITK-SNAP combines RR with SC and employs a pre-segmentation (PS) to initialise an active contour model.
- Current methods for attacking these discrete search problems work by lifting' the search from discrete space to continuous space, via an @cite . Specifically, an autoencoder jointly learns two mappings: 1) a mapping from discrete space to continuous space called an ; and 2) a reverse mapping from continuous space back to discrete space called a . These mappings are learned so that if we map a discrete object to a continuous one via the encoder, then map it back via the decoder, we reconstruct the original object. The hope is that, once the autoencoder is fully trained, the continuous space (often called the latent' space) acts as proxy for the discrete space. If this holds, we can use the geometry of the continuous space to improve search using (Euclidean) distance measures and gradients, among many other things. showed that is possible to use this technique to search for promising drug molecules.
- There are several works trying to uncover visual relationship between pair of compatible fashion items. @cite use parameterized distance metric (i.e. Mahalanobis distance) to learn relationships between co-purchased item pairs. @cite extends @cite to allow multiple notions of relatedness' with ensemble of @math Mahalanobis transforms. Both of these approaches only use image features extracted from the network that is trained for image classification of general images. So, the item features they use are not fashion-centric nor context-sensitive.
- The closest work to ours is the work by @cite . They use Siamese convolutional neural network to learn dyadic item co-occurrences. Since they train network to minimize the @math difference between co-purchased pair of item features, it is impossible for them to learn features from set of arbitrary number of items. In addition, they use co-purchase dataset from Amazon https: www.amazon.com to learn latent features of fashion items. However, as mentioned before, we claim that co-purchase data is not suitable for identifying specific style relationships among well-matched style set.
- Meanwhile, recently, noteworthy fashion recommender systems are being proposed, which try to recommend items that go well with other items. @cite use heterogeneous graph to link fashion items those make up a stylish outfit, as well as to link items to their attributes. @cite propose a tensor factorization approach to recommend a set of fashion items. They do not learn item features based on sets, but use discrete item attributes or low level image features.
- Our method combines prior knowledge about objectness'' from pretrained visual models with an attentional mechanism for learning to detect specific task relevant objects. A number of previous works have sought to combine general objectness priors with more specific object detection in the context of robotics and other visual tasks. used region proposals and SIFT features for quickly detecting objects in the context of SLAM @cite . Prior work used an active search approach where the camera could zoom in certain parts of the receptive field to search at higher resolutions @cite . In manipulation, SIFT features have also been used for 3D pose estimation and object localization, using object-specific training data gathered individually for the task @cite @cite . Similarly to these prior works, our method constrains the observations using an object-centric prior. However, we do not require object level supervision for each task, instead using visual features from a pretrained visual model to index into the proposals from the object-centric prior. This approach drastically reduces the engineering burden for each new task, picking out task-relevant objects from a few demonstrations, and provides good generalization over object appearance, lighting, and scale, as demonstrated in our experiments.
- An alternative to building perception systems for task-specific objects is to learn the entire perception system end-to-end together with the control policy. A number of recent works have explored such end-to-end approaches in the context of skill learning, either for direct policy search @cite @cite @cite , unsupervised learning of representations for control @cite @cite , or learning predictive models @cite @cite . A major challenge with such methods is that their ability to generalize to varied scenes and objects depends entirely on the variety of objects and scenes seen during policy training. Some methods have sought to address this by collecting large amounts of data with many objects @cite @cite . In this work, we instead study how we can incorporate prior knowledge about objects from pretrained visual models, while still being able to train rich neural network control policies. In this way, we can obtain good generalization to appearance, lighting, and other nuisance variables, while only training the final policy on a single object instance and in a single scene.
- In the field of graph mining (reviewed in @cite ), conventional ideas of mining maximal subgraph patterns have been realized as maximal frequent subgraph (MFS) extraction @cite @cite @cite @cite and the mining of maximal cliques @cite @cite . MFS extraction approaches @cite @cite @cite usually require the graphs to contain distinct node edge labels or use local consistency to determine a set of node correspondence candidates between different graphs. Moreover, the graphs must have distinguishing structures. Thus, these methods define MFSs using graph isomorphisms. They mainly enumerate the nodes from different graphs to search the subgraphs with isomorphic (or similar) structures and labels. The distinct labels are used to prune the search range and avoid the NP-hard computation in the worst case. Similarly, the mining of maximal cliques @cite @cite @cite @cite mainly extracts dense cliques that maintain geometric consistency. Subgraph patterns for ARGs have been defined @cite @cite , and the softness of clique patterns has been formulated @cite @cite .
- Given a graph template and a number of ARGs, methods for learning graph matching have been proposed to train parameters or refine the graph template for better matching performance. Most techniques @cite @cite @cite @cite @cite take a supervised approach, they require the manual labeling of node correspondences between different ARGs. Leordeanu @cite proposed the first unsupervised method of learning graph matching, and Zhang @cite further refined the template structure in an unsupervised fashion. Cho @cite proposed a similar idea that matched two ARGs and simultaneously extracted the most reliable edges between the two matched subgraphs. Essentially, these methods are not comparable with graph mining. They mainly train parameters or delete bad'' nodes from the graph template, rather than discovering new pattern nodes and recovering the prototype graphical patterns.
- From the perspective of applications, there are numerous ways of mining objects from unlabeled big visual data, such as object discovery @cite , co-segmentation @cite , edge model extraction @cite , the learning of structural patterns @cite @cite , and a number of techniques @cite @cite @cite @cite @cite @cite related to maximal clique mining. However, these studies were mainly designed with some specific techniques oriented to their own applications. They may model the structural knowledge by ignoring texture variations, or model the textural knowledge by ignoring structure deformation.
- Music plays an important role in many peoples' lives. Thus it is not surprising that several works focus on the complicated problem of music synthesis. Several attempts have been made at generating musical compositions. One of the earliest generative models, CONCERT'', was architected to compose simple melodies @cite . However, the limitations of CONCERT were that it could not capture the global structure of music. The generated music was said to lack global coherence''. This is problematic as music has long-range dependencies. Based on the CONCERT model, tackled this problem by building a model that could learn longer-range dependencies.
- Depth sensors are often used together with video cameras to capture RGB-D images. Therefore the idea of hybrid camera is not new for 3D imaging. In fact, consumer-level depth sensors like Kinect are essentially hybrid cameras. It is well known that the depth data produced by low-cost depth sensors is often noisy and low resolution. A common approach to enhance depth maps in the spatial domain is to couple a low-resolution depth map with a high-resolution RGB image. Various solutions based on optimization (e.g., @cite @cite ), joint edge-preserving upsampling filters (e.g., @cite @cite @cite ), spatialtemporal filtering @cite @cite , or shading cues @cite @cite have been explored to increase the spatial resolution of depth maps. All the methods above assume the same frame rates of color and depth maps. The exceptional case is the work by @cite . However, their technique has mainly focuses on the interpolation of depth maps given sparse depth data with respect to camera frames, and has been applied to moving rigid objects only. Instead, our focus is on the estimation of new depth frames involving more complex motions. See the comparison between these two approaches in Sec. .
- While consumer-level depth sensors are able to capture the depth at only a limited frame rate, high-speed depth cameras that already reach hundreds or even thousands of frames per second, have been explored in the fields of computer vision and optical engineering in recent years. Among various solutions, structured light illumination (e.g., @cite @cite @cite @cite @cite @cite ) is the most popular technique, which requires a DLP video projector and a synchronized video camera to acquire structured patterns (e.g., fringe images) projected by special illuminator. Compared with those approaches, our solution can be seen as a post-processing technique, and thus applicable to different types of depth sensors. St "u @cite proposed to modify a typical Time-of-Flight (ToF) camera (e.g., Kinect v2) for model-based tracking at high frame rate (300Hz). However, their solution is limited to tracking of objects with rigid motion. Our work shares close resemblance to that by Kim and Kim @cite , which uses multi-view hybrid cameras (consisting of eight high-speed RGB cameras and six ToF cameras) for motion capture. However, their technique is highly dependent on skeleton tracking, and thus is only suitable for articulated motion.
- Our joint optimization to fuse the color and depth information and estimate the motion field yields a novel scene flow estimation method. Scene flow estimation for depth cameras is an active research topic recently. For example, @cite extended the Horn-Schunck method @cite to the depth cameras with the depth data term for estimating the scene flow from a consumer depth camera. @cite proposed a total variation regularization term for RGB-D flow estimation at real time. Piecewise rigid motion priors have been added to the scene flow estimation in @cite . @cite estimate the scene flow with the joint optimization of motion and segmentation. Their method segments the scene with rigid motions. @cite order each depth frame into layers and assume the motion field in a single layer to be in the small range around the mean rigid rotation. When the objects in the same depth layer have large and different motions, this method would introduce artifacts (see Sec. ).
- As shown in @cite @cite , the piecewise rigid regularization term of the motion field enhances the precision compared with the methods like @cite @cite . Our work employs not only a local rigid prior but also an isometric deformation prior, which corresponds to very common deformations in real scenarios. We follow the as-rigid-as-possible energy to model isometric deformations, which have been demonstrated for various graphics applications, such as shape interpolation @cite , shape deformation @cite @cite and 3D shape tracking @cite . In our work, the as-rigid-as-possible energy is employed for the first time for the scene flow estimation with the assumption of nearly isometric deformations.
- However, most of those models use pixel-level supervision, which can be unavailable in some settings, or costly to acquire in any case. Some works tackle this problem by using fewer labeled images or weaker overall supervision. One common strategy is to use image-level annotations to train a classifier from which class saliency maps can be obtained. Those saliency maps can then be exploited with other means to produce segmentation maps. For instance, WILDCAT @cite uses a Conditional Random Field (CRF) for spatial prediction in order to post-process class saliency maps for semantic segmentation. PRM @cite , instead, finds pixels that provoke peaks in saliency maps and uses these as a reference to choose the best regions out of a large set of proposals previously obtained using MCG @cite , an unsupervised region proposal algorithm. Both pipelines use a combination of a deep classifier and a method that take advantage of spatial and visual handcrafted image priors.
- Fully unsupervised approaches have traditionally been more focused on designing handcrafted features or energy functions to define the desired property of . Impressive results have been obtained when making full use of depth maps in addition to usual RGB images @cite @cite but it is much harder to specify good energy functions for purely RGB images. W-NET @cite instead uses an auto-encoder to build a latent representation that can then be used with a more classic CRF algorithm. Unlike ours, none of these approaches are learned entirely from data.
- Another related line of work relies on the idea of inferring scene decomposition directly from data. Stemming from DRAW @cite , many of those approaches @cite @cite use an attention network to read a region of an image and a Variational Auto-encoder (VAE) to partially reconstruct the image in an iterative process in order to flesh out a meaningful decomposition. The very recent IODINE @cite proposes a VAE adapted for multi-objects representation. LR-GAN @cite is able to generate simple scenes recursively, building object after object, and 2018 @cite decompose an image into single-colored strokes for vector graphics. While iterative processes have the advantage of being able to handle an arbitrary number of objects, they are also more unstable and difficult to train. Most of those can either only be used in generation @cite , or only handle very simple objects @cite @cite @cite . As a proof of concept, we decided to first ignore this additional difficulty by only handling a set number of objects but our model can naturally be extended with an iterative composition process.
- There is a growing body of research on explainable AI @cite @cite @cite @cite , but it is not connected to work on learning with human rationales, which we review below.
- As mentioned above, there has also been some recent criticism of using attention as explanation @cite , due to a lack of correlation between the attention weights and gradient based methods which are more faithful" to the model's reasoning. However, attention weights offer some insight into at least one point of internal representation in the model, and they also impact the training of the later features. Our contribution is to measure how useful these attention based explanations are to humans in understanding a model's decision as compared to a different model architecture that explicitly learns to predict which sentences make good explanations.
- Image segmentation at pixel-level under weak supervision has been studied in @cite @cite @cite . These approaches propose to work under the Multiple Instance Learning framework (MIL). The images are seen as bag of pixels or patches. The elements of the bags have an underlying label that is not available. However a label at the bag level is available and relate to the label of its elements. A negative (i.e. @math ) bag contains only negative elements, while a positive (i.e. @math ) bag contains at least one positive element. A convolutional neural network (CNN) is used to produce pixel (patch) level features that are aggregated through a pooling function to form the bag level class prediction. However these approaches need the image to fit into memory and do not scale on WSIs that contain giga pixels.
- In @cite the authors propose the use of a deep recurrent attention model that classifies a WSI using information provided from a limited number of patches (or glimpses) using visual attention method. Disease localisation is inferred from the glimpses location.
- A human body could be represented by a 3D mesh ( Skinned Multi-Person Linear Model, SMPL @cite ) and a texture image @cite @cite as illustrated in Figure . Each position on the 3D body surface has a semantic identity (identified by a 2D coordinate (u,v) in the canonical UV space) and a texture representation ( RGB pixel value) @cite @cite . Texture image on the UV coordinate system ( , surface-based coordinate system) holds the texture of the 3D surface of the person. Note that the texture images across different persons are densely semantically aligned (see Figure ). In @cite , a dataset with labeled dense semantics ( DensePose) is established and a CNN-based system is designed to estimate DensePose from person images. Neverova al @cite and Wang al @cite leverage the aligned texture image to synthesize person image of another pose or view. Yao al @cite propose to regress the 3D human body ((x,y,z) coordinates in 3D space) in the semantics aligned UV space, with the RGB person image as the input to the CNN.
- There is a considerable volume of work addressing regularization in deep networks, too vast to review here. Most of the efforts are towards analyzing the geometry and topology of the loss landscape at convergence. Work relating the local curvature of the loss around the point of convergence to regularization ( flat minima'' @cite @cite @cite @cite ) has been especially influential @cite @cite . Other work addresses the topological characteristics of the point of convergence (minima vs. saddles @cite ). discuss the effects of the learning rate and batch size on stochastic gradient descent (SGD) dynamics and generalization. At the other end of the spectrum, there is complementary work addressing initialization of deep networks, . There is limited work addressing the timing of regularization, other than for the scheduling of learning rates .
- Also related to our work, there have been attempts to interpret the mechanisms of action of certain regularization methods, such as weight decay @cite @cite @cite @cite @cite @cite , data augmentation @cite , dropout @cite . It has been pointed out in that the Gauss-Newton norm correlates with generalization, and with the Fisher Information Matrix , a measure of the flatness of the minimum, to conclude that the Fisher Information at convergence correlates with generalization. However, there is no causal link proven. In fact, we suggest this correlation may be an epi-phenomenon: Weight decay causes an increase in Fisher information during the transient, which is responsible for generalization ( fig:fisher ), whereas the asymptotic value of the Fisher norm ( , sharpness of the minimum) is not causative. In particular, we show that increasing Fisher Information can actually improve generalization.
- Query routing is a rigorously researched topic in various fields including, networked databases, meta-searching, and search aggregation @cite @cite . However, archive profiling and Memento lookup routing is a niche field that is not explored by many researchers beyond a small community.
- implemented a different approach for Memento routing by building binary classifiers from LANL's Time Travel aggregator cache data @cite . They analyzed responses from various archives in the aggregator's cache over a period of time to learn about the holdings of different archives. They reported a 77 approaches can be categorized as usage-based profiling in which access logs or caches are used to observe what people were looking for in archives and which of those lookups had a hit or miss in the past. While usage-based profiling can be useful for Memento lookup routing, it may not give the real picture of archives' holdings, producing both false negatives and false positives https: groups.google.com forum #!topic memento-dev YE4rt6L5ICg .
- In previous work @cite @cite , we explored the middle ground where archive profiles are neither as minimal as storing just the (which results in many false positives) nor as detailed as collecting every URI-R present in every archive (which goes stale very quickly and is difficult to maintain). We first defined various profiling policies, summarized files according to those policies, evaluated associated costs and benefits, and prepared gold standard datasets @cite @cite . In our experiments, we correctly identified about 78 on the archive profiling framework we established, we further investigated the possibility of content-based profiling by issuing fulltext search queries (when available) and observing returned results @cite if access to the data is not possible. We were able to make routing decisions of 80 is a continuation of this effort to make it more flexible and portable by eliminating the need for rigid profiling policies we defined earlier @cite @cite (which are still good for baseline evaluation purposes) and replacing them with an adaptive approach in which the level of detail is dynamically controlled with a number of parameters. =-1
- As one of the most popular methods for accelerating network inference, pruning has been widely studied recently. In the earlier work, Optimal Brain Damage @cite prunes unimportant weights to reduce the number of parameters and prevent over-fitting. Recently, Han @cite prune the weights which are below a threshold results in a very sparse model with no loss of accuracy. But such a non-structured sparse model has limitations of accelerating inference without specific hardware @cite . Latter, researchers focus on filter level pruning which can not only reduce the memory footprint dramatically but also speedup network inference by any off-the-shelf library. Li @cite prune the less useful filters based on sum of absolute weights directly.
- But the filter of small magnitude does not mean it is not important. Thus, the methods based on the information of activations are studied. Hu @cite calculate the sparsity of activations after the ReLU function and prune the corresponding filters if the sparsity is high. Molchanov @cite adopt a first-order Taylor expansion to approximate the change to loss function induced by pruning each filter. He @cite prune channels by a LASSO regression based channel selection and least square reconstruction. Yu @cite prune the entire network jointly to minimize the reconstruction error of important responses in the final response layer" and propagate the importance scores of final responses to every neuron.
- Besides the above methods which prune filters on a pre-trained network, several methods add regularization or other modifications during training. For example, Liu @cite enforce channel sparsity by imposing L1 regularization on the scaling factors in batch normalization. McDanel @cite utilize incomplete dot products to dynamically adjust the number of input channels used in each layer. Zhou @cite introduce a scaling factor to each filter to weaken the weights step by step and prune the filters after training.
- Meanwhile, other methods are well explored to compress networks. Knowledge distillation @cite first trains a big network ( teacher network) and then trains a shallow one ( student network) to mimic the output distributions of the teacher. Network quantization reduces the number of bits for representing each parameter and some low-bit networks are proposed @cite @cite @cite . Low-rank factorization decompose weights into several pieces @cite @cite . Note that our pruning method can be integrated with the above techniques to achieve a more compact and efficient model.
- As mentioned in the last chapter, image compression has gone through several generations. From traditional compression techniques like JPEG, JPEG2000, to recent adaptive image compression approach @cite proposed by Waveone, image compression evolves many research fields progress. Here are some related works:
- JPEG is a widely used image format using 2D Fourier Discrete Cosine (DCT) Transform. It is also the foundation of most popular H.264 @cite video compression format. However, JPEG 2000 uses the wavelet transform to beat up JPEG for its higher quality in the same level of Bit Per Pixel (BPP). However, the lack of its application and slow encoding and decoding speed hinder its popularity. Google presented WebP in 2010 in order to substitute JPEG or PNG on the internet. The Predicting Module in the MacroBlocking of WebP sends the predicted output to the DCT transform, thus compressing the image in a more effective way. People tend to find a better way of image compression from video compression techniques. WebP can be seen as the key-frame compression of WebM @cite . BPG is derived from HEVC @cite (a.k.a the second part of H.265). It gives a higher dynamic range and a better compression ratio. H.266, i.e., Versatile Video Coding (VVC) @cite proposed by JVET group, is beyond H.265 and desire to have a preferable performance than any other traditional image video compression methods.
- In 2015, @cite proposed a creative architecture composed of convolutional and deconvolutional LSTM recurrent network. In order to get arbitrary size of input images, @cite improved their framework into an RNN based end-to-end network. With the popularity and practicability of autoencoder, @cite proposed compressive autoencoder with an upper-bound the discrete entropy rate loss for continuous relaxation. @cite presented a fully convolutional end-to-end compression framework to work with residual and get the reconstructed image as similar to the groudtruth. @cite proposed BlockCNN for artifact removal and achieved results with better quality on the enhanced images.
- Only a few works are GAN based. proposed a real-time adaptive image compression method @cite but the key is that target and reconstruction are no longer treated separately. They fine-tuned their GAN to decide when they propagated confusion signal, and when to train the discriminator. @cite presented a GAN based network with SSIM loss to get a better artifact removal result. @cite proposed Compression-GAN to re-generate human faces in rather low szies while with high MOS score.
- There is a wide range of classical work on (probabilistic) Byzantine consensus protocols @cite @cite @cite @cite @cite @cite . The disadvantage of the approach of these papers is, however, that they typically require that the nodes exchange @math messages in each round (which means @math messages for each node). In the situation where the communicational complexity matters, this can be a major barrier.
- A good deal of work focuses on failures within a network infrastructure, rather than on malicious agents. The work of Liu @cite defines FastBFT, and which is a fast and scalable BFT (Byzantine fault tolerance) protocol. Within this, the work integrates trusted execution environments (TEEs) with lightweight secret sharing, and results in a low latency infrastructure. @cite define Democratic Byzantine Fault Tolerance (DBFT) and which is a leaderless Byzantine consensus. This provides a robust infrastructure where there is a failure in the leader of the consensus network. The core contribution is that nodes will process message whenever they receive them, instead of waiting for a co-ordinate to confirm messages. Another Byzantine Fault Tolerant method which does not require a leader node is Honey Badger @cite . This method is asynchronous in its scope and can cope with corrupted nodes. Unfortunately, it does not actually make any commitments around the timing of the delivery of a message, and where even if Eve controls the scheduling of messages, there will be no impact on the overall consensus.
- There has also been much research on the probabilistic models where, in each round, a node only contacts a small number of other nodes in order to learn their opinions, and possibly change its own. This type of models is usually called , and which were introduced in the 70s by Holley and Liggett @cite and Clifford and Sudbury @cite . A very important observation is that, in most cases, voter models have only two external invariant measures: one concentrated on the all- @math '' configuration, and the other one concentrated on the all- @math '' -- we can naturally call these two configurations consensus states''. Since then, there has been a range of work on voter models; in particular, let us cite @cite @cite @cite @cite @cite @cite which are specifically aimed at reaching consensus and have low communicational complexity (typically, @math ). However, in these works, the presence of adversarial nodes is usually either not allowed, or is supposed to be very minimal.
- Generally speaking, in most of the related works, the thermal couplings among the neighbouring or adjacent zones are ignored or not explicitly considered, which may lead to the degradation of performance in practice. On one hand, the actual energy cost of HVAC may rise due to the heat transfer between the neighbouring zones ignored in the optimization. On the other hand, the realistic thermal condition of each individual zone may deviate from the comfortable range. An attempt was reported in @cite , which studied the distributed MPC strategy for HVAC system in a multi-zone building, where the thermal couplings between the neighbouring zones are explicitly discussed. To cope with the difficulties due to the nonlinearity and nonconvexity, a distributed ADMM method was applied based on some convexity approximation.
- Exact geometric characterization of @cite . @math which lie on the geodesic between @math and @math , shown as the red curve. Figure (b) shows a case where the metric defined by @cite results in essentially perfect precision and recall. Arguably, in this case the precision and recall should both be low. We will now show that the two most prominent definitions of precision and recall are special cases of the presented framework.
- The limiting case of @math We will now show that @cite corresponds to the case where @math . In particular, write both @math and @math as mixtures with a shared component that should capture the space to which both assign high likelihood, and which can be used to formalize the notions of precision and recall for distributions. The union of @math and all realizable pairs @math will be denoted by @math .
- Finally, we note that the idea of precision and recall for generative models based on distances to the corresponding manifolds first appeared in , but was only applicable for synthetic data. -- if the quantization is not carefully performed, it seems that truncation improves both precision and recall. On the other hand, panel (b) demonstrates that the estimator from @cite (notwithstanding the fundamental issues outlined in ) performs well in practice.
- Fine-tuning @cite lets the pre-trained features and augmented parameters learn the target task together. Fine-tuning usually performs better than feature extraction and training from scratch with random initialization @cite . However, the pre-trained features are substantially contaminated due to noise flowing from random layers to the loss and from there, back-propagated toward the features. This paper tackles this problem by eliminating this noise within the augmented parameters.
- Building scalable and computationally efficient algorithms for kernel methods is an active research area. Some methods use low rank approximations to approximate the kernel directly, such as Nystr " o m approximation or random Fourier features. This approach is very popular and has many variants where the main idea is to reduce the computational cost by low rank approximation while retaining reasonable model quality. Recent work on these approaches can be found in @cite @cite @cite @cite . A different approach uses fast matrix vector multiplications, e.g Fast Gauss Transform or Fast Multipole methods to solve the regression using Krylov iterations @cite @cite @cite . These methods reduce the computational cost of each Krylov iteration significantly, from @math to @math or even @math . However, when the kernel matrix is ill-conditioned, the number of iterations required might be huge. Another approach is to use a preconditioner to reduce the condition number of the kernel matrix and in so doing reduce the required number of Krylov iterations. These approaches are discussed in @cite @cite @cite .
- The proposed sampling scheme involves interacting particles that undergo Langevin diffusion and birth-death process. Other sampling schemes via interacting particles have been proposed recently, including the Stein variational gradient descent (SVGD) flow @cite @cite (see also its continuous limit studied in @cite ). Unlike the samplers based on Stein discrepancy, which replaces the random noise in Langevin dynamics by repulsion of particles, the sampling scheme proposed in this work employs birth-death process to enhance the mixing of existing sampling schemes. In fact, it can also be potentially combined with SVGD to improve its convergence.
- We extend three popular KGE models, TransE, TransH @cite and TransD @cite , using bi-vector. Therefore, we firstly introduce these.
- . In addition to TransE(H,D) that we have already mentioned, translation based methods cover the following models. @cite build entity and relation embedding independent spaces, in which, entities @math , and relation @math . A projection matrix @math has been set, and the score funcion is defined as @math @cite set two separate relation sparse matrices @math and @math to deal with the issue of sparse data. The score function is defined as @math . reduces the cost of calculation of relation projection by modeling subspaces of projection matrices, and the score function is defined as @math , where @math , @math , @math and @math are the corresponding coefficients of @math and @math .
- . @cite adopts a relation-specific diagonal matrix @math to represents the characteristics of a relation. The score function @math is a bilinear function, which score of positive triples should be higher than negative triples. @cite employs circular correlations by holographic to create compositional representations, and has advantages of computation efficiency and representing scalability. @cite adopt tensor factorization to estimate relation axis. @cite embed the entities and relation to complex space, then computes loss vaule. . @cite defines two relation-specific matrices for @math , i.e. @math , and defines the score function as @math . There are many other KGE models try to try to use various embedding methods, such as @cite , @cite , , , , etc.
- gSpan @cite is an efficient frequent subgraph mining algorithm designed for mining multiple input graphs. However, gSpan is designed for multiple graphs of mining problems. If we have a single input graph, we have to find multiple instances in the same graph, therefore it complexes the problem. Michihiro @cite first proposed algorithms to mine patterns from a single graph. They use an expensive anti-monotonic definition of support based on the maximal independent set to find edge-disjoint embeddings. GraMi @cite proposes an effective method in the single large graph and presents an extended version with supporting structural constraints and an approximate version. Pr z ulj @cite introduces the motif counting problem. Ribeiro @cite presents G-Tries which is an effective approach for storing and finding the frequency of motifs. Apar ' cio @cite designs and implements a parallel version of G-Tries. Maximal clique is well studied problem.
- The most common practical approach for the graph isomorphism problems is canonical labeling, a process in which a graph is relabeled in such a way that isomorphic graphs are identical after relabeling. The mainly strategy of the canonical labeling is building search tree for the input graph. Nauty @cite is the first program that could handle large automorphism groups; it uses automorphism to prune the search in testing automorphism. Nauty generates the search tree in depth-first order, while Trace @cite introduces a breadth-first search in generating the search tree. Saucy @cite is an implementation of the Nauty system by utilizing the sparsity and particular construction of colored graphs. However, these libraries focus on the checking of the automorphism, which only suits for the unlabeled graphs. Bliss @cite supports the isomorphism checking of labeled graphs. However, building the search tree brings frequently memory allocating and deallocating which slow down the processing and consume a huge amount of memory. In addition, bliss is designed for the large graph isomorphic checking, while the eigenvalue checking strategy is sufficient in the mining scenes.
- In this work, core-set selection @cite was chosen as the baseline beside random sampling. This method uses an upper bound of the core-set loss, which is the gap between the training loss on the whole set and the core-set. By minimizing this upper bound, the authors show that the problem is equivalent to a K-center problem which can be solved by using a greedy approximation method or a mixed integer program (MIP) solver. Core-set selection, is on of the current state of the art methods in the field of image classification for CNNs. However, both of them are very time consuming and could take several days to find the optimal solution, as for core-set selection is to solve an NP-Hard problem that grows exponentially with the size of the data set.
- Conventionally, imitation learning is used in autonomous agents to learn tasks from demonstrated state-action trajectories. The algorithms developed for this task can be divided into two general categories, (1) behavioral cloning @cite @cite @cite in which the agents learn a direct mapping from the demonstrated states to the actions, and (2) inverse reinforcement learning (IRL) @cite @cite @cite in which the agents first learn a reward function based on the demonstrations and then learn to perform the task using a reinforcement learning ( ) @cite algorithm.
- Another set of model-free algorithms follow a more end-to-end approach to learning policies directly from observations. An algorithm of this type is generative adversarial imitation from observation ( ) @cite , which uses a -like architecture to bring the state transition distribution of the imitator closer to that of the demonstrator. Another approach of this type is the work of merel2017learning , which is concerned instead with single state distributions. stadie2017third also propose an algorithm in this space that combines adversarial domain confusion methods @cite with adversarial imitation learning algorithms in an attempt to overcome changes in viewpoint. The method we propose in this paper also belongs to the category of end-to-end model-free imitation from observation algorithms. However, it is different from the algorithms discussed above in that we explicitly incorporate the imitator's proprioceptive information in the learning process in order to study the improvement such information can make with respect to the performance and speed of the learning process.
- In computer vision, linear diffusion has been proposed for seeded segmentation in the seminal work of Grady @cite . In semantic segmentation and image-valued regression tasks, @cite have shown how to learn context-dependent potentials for GMRFs. These authors did not explore seeded segmentation, and the number of parameters learned was in the hundreds, not hundreds of thousands as in our case.
- @cite have pioneered the use of flexible nonparametric classifiers to determine the potentials in a Conditional GMRF. However, they opted for ensembles of decision trees and had to estimate the impact of each possible split on the loss function, making a number of approximations necessary.
- In recent work, Vernaza and Chandraker @cite have proposed a first-order approximation of the derivative for backpropagation. We show in section that this approximation is not well-behaved for the kind of sparse seeds considered here.
- Our algorithm can also be seen from the viewpoint of coupling a deep network with an undirected probabilistic graphical model. In contrast to @cite @cite we use a GMRF that makes inference easier. End-to-end learning of a GMRF was also proposed in @cite , though these authors do not solve the inference problem exactly as we do; instead, they unroll a few steps of gradient descent into extra layers of their neural network. In a similar setting, Chandra and Kokkinos @cite learned unary and pairwise terms of a Gaussian CRF, this time, computing the backpropagation derivative exactly. Our inference problem can be seen as convex quadratic program. Recent work proposes quadratic programming as a generic neural network layer @cite . However, the paper and implementation do not account for the kind of sparsity that characterizes our formulation, and scales only to a few hundred random variables, where we use tens of thousands. Indeed, the solution of huge sparse systems of the kind discussed here is possible in almost linear time @cite .
- Other important seeded segmentation strategies are based on discrete Markov Random Fields @cite that do however not admit the differentiability we crucially need; or on shortest paths similarities that can be computed even more efficiently than the diffusions we use @cite . However, the use of (single) shortest paths to measure the dissimilarity from each pixel to every seed is not as robust as the averaging over many paths that diffusion processes implicitly accomplish @cite .
- Such fragility is especially pronounced for watershed-type methods that base dissimilarities on a minimax criterion @cite . Even so, such methods have been used successfully in biomedical @cite and other applications. Recent work has sought to mitigate this limitation by learning the edge weights for watershed seeded segmentation in a structured fashion, just as we do, and moreover making the algorithm adaptive @cite , the cost for the latter being a somewhat involved recurrent neural net formulation. The experiments in section show that we can supersede the learned watershed in spite of our simpler architecture.
- Remarkably, all of the above inference schemes for seeded segmentation -- graph cuts, random walker linear diffusion and geodesic distances -- emerge as special cases of a unifying optimization problem @cite .
- DBLP:journals corr abs-1808-00590 design MLCapsule, a guarded offline deployment of MLaaS using Intel SGX. This allows providers of machine learning services to serve offline models with the same security guarantees that are possible for a custom server-side deployment, while having the additional benefit that the user does not have to trust the service provider with their input data. They demonstrate an implementation of PRADA @cite within MLCapsule as a defense against model extraction attacks.
- Recent works on estimating the political ideology of social media users include @cite and @cite which infer users' ideology based on the text in their social media posts using an recurrent neural network framework and natural language processing techniques respectively. In @cite , political ideology is inferred by jointly estimating the ideology of Twitter users and the politicians that they follow using a Bayes ideal-point estimation model.
- More recent papers have also focused on jointly estimating user ideology and news source ideology using matrix factorization. In @cite , the authors developed a shared matrix factorization model that trained on a dataset with article text labeled as factual or fake news in order to jointly estimate the latent features of social media users, article text, and news sources. The model is used to detect which news sources are more likely to produce fake news and estimate users' individual susceptibility to spreading fake news. Using an approach most similar to that detailed in this paper for ideology estimation, @cite used a matrix factorization model to jointly estimate user and news source ideology for Twitter users, with an additional penalty enforcing smoothness over the retweet network. IdeoTrace is an extension of this work that, in addition to jointly estimating website and user ideology using unlabeled data, also considers user ideology at each point on time.
- There is also a large body of work around opinion dynamics, which aims to trace the evolution of users' opinions over time. Note that while this paper is primarily concerned with the estimation of users' political ideology, the term is a generalization of the concept of political ideology, and is defined as an individual's cognitive orientation towards an object", such as an event, topic, or another individual, and can be represented using a real-valued scalar or vector @cite .
- Opinion dynamics models typically either ignore influences on ideology external to the social network between users, or treat these influences as known inputs. The early, well-established work on this problem centered around the development of theoretical models that, given a social network of users with an initial distribution of opinions, use rule-based updates often inspired by physical or biological networks to estimate user opinion over multiple time steps. The models are then shown in simulation to converge to particular distributions based on the network and update rules. For example, in @cite it was shown that modelling confirmation bias by moving users with similar opinions closer together each simulation step and breaking the network link between them otherwise, often results in the formation of a bimodal opinion distribution. For a review of popular opinion dynamics models, we refer to @cite .
- Maximum entropy was used in RL by as an additional term in the loss function to encourage exploration and avoid local minimums @cite @cite @cite @cite . A similar idea has also been utilized in the deep learning community, where entropy loss was used as a regularization technique to penalize over-confident output distributions @cite . In RL, the entropy loss adds more cost to actions that dominate quickly. A higher entropy loss favors more exploration @cite . gave a unified view on entropy-regularized Markov Decision Processes (MDP) and discussed the convergence properties of entropy-regularized RL, including TRPO @cite and A3C @cite .
- More recently, and proposed deep energy-based policies with state conditioned entropy-based regularization, which is known as Soft-Q Learning. They showed that maximum entropy policies emerge as the solution when optimal control is cast as probabilistic inference. Concurrently, showed the connection and the equivalence between Soft-Q Learning and policy gradients. Maximum entropy policies are shown to be robust and lead to better initializations for RL agents @cite @cite . Based on maximum entropy polices, developed an information theoretic objective, which enables the agent to automatically discover different sets of skills.
- We implemented the maximum entropy regularization via prioritized sampling based on achieved goal-states. We believe that the most similar framework is prioritized experience replay @cite . Prioritized experience replay was introduced by as an improvement to the experience replay in DQN @cite . It prioritizes the transitions with higher TD-error in the replay buffer to speed up training. The prioritized experience replay is motivated by TD-errors. However, the motivation of our method comes from information theory--maximum entropy. Compared to prioritized experience replay, our method performs superior empirically and consumes much less computational time.
- The intuition behind our method is to assign priority to those under-represented goals, which are relatively more valuable to learn from (see Appendix). Essentially, our method samples goals from an entropy-regularized distribution, rather than from a true replay buffer distribution, which is biased towards the behavior policies. Similar to recent work on goal sampling methods @cite @cite @cite @cite @cite @cite , our aim is to model a goal-conditioned MDP. In the future, we want to further explore the role of goal entropy in multi-goal RL.
- Other work has considered open domain tasks, such as @cite . Recent approaches have typically relied on a separate entity linking model, such as S-MART @cite , to provide a single disambiguated set of entities to consider. In principle, a learned entity linker could also serve as an entity candidate generator within our framework, although we do not explore such tasks in this work.
- Many formulations of Graph Neural Networks (GNNs) that propagate information over local neighborhoods have recently been proposed @cite @cite @cite @cite . Recent work has often focused on large graphs @cite and effectively propagating information over multiple graph steps @cite . The graphs we consider are relatively small and are fully-connected, avoiding some of the challenges posed by learning representations for large, sparsely connected graphs.
- knowledge distillation is one of the most popular techniques used in model compression @cite @cite . A large quantity of approaches have been proposed to reinforce the efficiency of student models' learning capability. Romero firstly put forward FitNet in which the concept of hint learning was proposed, aiming at reducing the distance between feature maps of students and teachers @cite . Agoruyko @cite considered this issue from the perspective of attention mechanism, attempting to align the features of attention regions. Furthermore, some researchers extended knowledge distillation to generative adversarial problem @cite @cite .
- In the other domains, knowledge distillation also shows its potential. Furlanello interactively absorbed the distillated student models into the teacher model group, through which the better generalization ability on test data is obtained @cite . Bagherinezhad applied knowledge distillation to data argumentation, increasing the numerical value of labels to a higher entropy @cite . Papernot regarded knowledge distillation as a tool to defend adversarial attack @cite , and Gupta , using the same methods, transferred the knowledge among data in different modals @cite .
- . Huang proposed random layer-wise dropout in training @cite . Some researchers extended this idea to inference. Wang and Wu further extended the layer-wise dropout from training to inference by introducing additional controller modules or gating functions based on the current input @cite @cite . Another extension of the layer-wise dropout solutions is to design early-exiting prediction branches to reduce the average execution depth in inference @cite @cite @cite @cite .
- . Inspired by the intuition that neural networks should focus on critical details of input data @cite , reinforcement learning and deep learning algorithms are utilized to identify the importance of pixels in the input images before they are feed into convolutional neural networks @cite @cite .
- The replacement paths problem. The replacement paths problem is motivated by several different applications and has been extensively studied in the last few decades (see e.g. @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite ). It is well motivated by its own right from the fault-tolerance perspective. In many applications it is desired to find algorithms and data-structures that are resilient to failures. Since links in a network can fail, it is important to find backup shortest paths between important vertices of the graph.
- The replacement paths problem has been studied extensively, and by now near optimal algorithms are known for many cases of the problem. For instance, the case of undirected graphs admits deterministic near linear solutions (see @cite @cite @cite @cite ). In fact, Lee and Lu present linear @math -time algorithms for the replacement-paths problem in on the following classes of @math -node @math -edge graphs: (1) undirected graphs in the word-RAM model of computation, (2) undirected planar graphs, (3) undirected minor-closed graphs, and (4) directed acyclic graphs.
- Bernstein presented in @cite a @math -approximate deterministic replacement paths algorithm which is near optimal (whose runtime is @math , where @math is the largest edge weight in the graph and @math is the smallest edge weight).
- In the deterministic regime no algorithm for the directed case is known that is asymptotically better (up to ploylog) than invoking APSP algorithm. Interestingly, in the fault-tolerant and the dynamic settings many of the existing algorithms are randomized, and for many of the problems there is a polynomial gap between the best randomized and deterministic algorithms (see e.g. sensitive distance oracles @cite , dynamic shortest paths @cite @cite , dynamic strongly connected components @cite @cite @cite , dynamic matching @cite @cite , and many more). Randomization is a powerful tool in the classic setting of graph algorithms with full knowledge and is often used to simplify the algorithm and to speed-up its running time. However, physical computers are deterministic machines, and obtaining true randomness can be a hard task to achieve. A central line of research is focused on the derandomization of algorithms that relies on randomness.
- The best known randomized algorithm for the @math simple shortest paths problem in directed unweighted graphs takes @math time ( @cite ), leaving a significant gap compared to the best known deterministic algorithm which takes @math time ( e.g. , @cite , @cite ). We close this gap by proving the existence of a deterministic algorithm for computing @math simple shortest paths in unweighted directed graphs whose runtime is @math .
- One option is to attempt to label the unlabelled data during training. The well known co-training algorithm @cite iteratively increases the pool of labelled data by labelling unlabelled examples using a pair of classifiers trained on different views of the data in the previous pool. Another method @cite that, like co-training, pre-dates the deep learning era, iteratively adds unlabelled data to the labelled set by scoring detections based on a detector independent metric.
- @cite demonstrates how sharing weights between an auto-encoder-like ladder network on unlabelled data with the feature extractor used for labelled data can leverage the unlabelled data during training to increase performance.
- An alternative approach that has been gaining popularity is the use of synthetic training data. The effectiveness of this approach is demonstrated in @cite where the use of synthetic data achieves good results, however it also shows the value of a small quantity of real data for fine-tuning.
- If the data already has labels but those labels are noisy there are a number of approaches that have been proposed to reduce the influence of the noise on the training process. Bootstrapping @cite , for example, incorporates the current state of the model into the loss term resulting in down-weighting of the influence of the label for examples for which the model is confident.
- @cite introduces two methods for estimating the noise transition matrix in classification problems. The S-model @cite , also attempts to learn the noise transition matrix.
- Countering the view that noise needs to be carefully modelled, @cite shows that a large quantity of noisy data can be effectively used to increase performance on a fine-grained classification task. From a similar perspective, @cite attempts to find the limits of weakly supervised data by pre-training with 3.5 billion images for classification and object detection tasks.
- The proposed model is closely related to prior works on recommendation based models in which the task is to learn to recommend items to a user based on their past preferences, usage patterns @cite @cite . The models can be broadly divided into collaborative filtering, content based recommender system, and hybrid models @cite @cite @cite @cite @cite . Collaborative filtering exploits prior information about user-item interactions to learn a vector representations for users and items. These representations capture the attributes of content and users, and are used for recommendation @cite @cite . While, content based recommendation methods directly use the features of items or users to recommend similar items @cite . Our work broadly falls into the category of hybrid recommendation systems @cite , that combine both content representation and use of prior knowledge about user-item interaction (collaborative filtering).
- Fully supervised semantic segmentation has been studied in many works, e.g., @cite , @cite , @cite , @cite , @cite . More recently, weakly-supervised semantic segmentation has come to the fore. Early work such as @cite relied on hand-crafted features, such as color, texture, and histogram information to build a graphical model. However, with the advent of convolutional neural networks (CNN), this conventional approach has been gradually replaced because of its lower performance on challenging benchmarks @cite .
- A natural step to less supervision are more coarse spatial cues like bounding boxes, key points and scribbles. In @cite , use the expectation-maximization algorithm to perform weakly-supervised semantic segmentation based on annotated bounding boxes and image-level labels. Another more sophisticated approach based on bounding boxes was proposed in @cite . The authors use region proposal generated by Multiscale Combinatorial Grouping (MCG) @cite and Grabcut @cite to localize the objects more precisely within the bounding box. More recently, @cite used bounding boxes for object classes and image level supervision for stuff classes. In @cite , made use of a region-based graphical model, with scribbles providing ground-truth annotations to train the segmentation network. Scribbles also served as supervision for the works of @cite @cite , which investigate loss regularizations. Human annotated keypoints were used by @cite for weakly supervised class segmentation and by @cite for weakly supervised affordance segmentation.
- While the works mentioned above require some type of explicit spatial hints, others only rely on the list of present classes in the image. @cite used proposals generated by (MCG) @cite to localize semantically meaningful objects. Recently, @cite leveraged saliency to obtain object proposals and link objects of the same class across images with a graph partitioning approach. @cite addressed the weakly-supervised semantic segmentation problem by introducing a series of constraints.
- The task of weakly supervised visual grounding @cite @cite @cite @cite @cite @cite is related but a different task. Instead of training a network for semantic image segmentation, the goal is to localize a given phrase in an image and the challenge is to handle phrases that are not part of the training data. This means that they require a phrase for inference, while in our case the captions are only given for training but not for inference on the test dataset.
- For disassortative graphs, i.e. when edges carry only dissimilarity information, the goal is to identify clusters such that the amount of edges between clusters is larger than the one inside clusters. Spectral clustering is extended to this setting by considering the signless Laplacian matrix and its normalized version (see e.g. @cite @cite ), defined as:
- Both Laplacians are positive semi-definite, and the smallest eigenvalue is zero if and only if the graph has a bipartite component @cite .
- where @math is the average node degree @math . The Bethe Hessian @math need not be positive definite. In fact, eigenvectors with negative eigenvalues bring information of clustering structure @cite .
- Let @math and @math be the Laplacian and signless Laplacian of @math and @math , respectively. As noted in @cite , @math i.e. it coincides with twice the arithmetic mean of @math and @math . Note that the same holds for @math when the average degree @math is equal to one, i.e. @math when @math . In @cite , the arithmetic mean and geometric mean of the normalized Laplacian and its signless version are used to define new Laplacians for signed graphs:
- where @math is the geometric mean of @math and @math , @math and @math . While the computation of @math is more challenging, in @cite it is shown that the clustering assignment obtained with the geometric mean Laplacian @math outperforms all other signed Laplacians.
- The scalar power mean of two non-negative scalars @math is a one-parameter family of means defined for @math as @math . Particular cases are the arithmetic, geometric and harmonic means, as shown in Table . Moreover, the scalar power mean is monotone in the parameter @math , i.e. @math when @math (see @cite , Ch. 3, Thm. 1), which yields the well known arithmetic-geometric-harmonic mean inequality @math . As matrices do not commute, several matrix extensions of the scalar power mean have been introduced, which typically agree if the matrices commute, see e.g. Chapter 4 in @cite . We consider the following matrix extension of the scalar power mean: where @math is the unique positive definite solution of the matrix equation @math . Please note that this definition can be extended to positive semidefinite matrices @cite for @math , as @math exists, whereas for @math a diagonal shift is necessary to ensure that the matrices @math are positive definite. 5pt
- Since the First International Chinese Word Segmentation Bakeoff in 2003 @cite , a lot of effort has been made on Chinese word segmentation.
- With the rise of statistical machine learning methods, the task of CWS is formalized as a tagging task, i.e., assigning a BEMS label to each character of a string that indicates whether the character is the start of a word(Begin), the end of a word(End), inside a word (Middel) or a single word(Single). Traditional sequence labeling models such as HMM, MEMM and CRF are widely used @cite @cite @cite @cite . .
- Neural CWS Models such as RNNs, LSTMs @cite and CNNs @cite @cite not only provide a more flexible way to incorporate context semantics into tagging models but also relieve researchers from the massive work of feature engineering. Neural models for the CWS task have become very popular these years @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . Neural representations can be used either as a set of CRF features or as input to the decision layer.
- @cite , authors have presented an algorithm to optimally schedule the users in PDCCH and thus, increase the control channel capacity. In @cite , authors have proposed a novel method of allocation for cell radio network temporary identifiers and increase the control channel capacity. In @cite , authors propose power allocation techniques to improve the control channel capacity. However, none of the above papers discussed the beamforming and exploited the large antennae structure for increasing the control channel capacity. Further, we have implemented the optimal LTE-PDCCH scheduling algorithm presented in @cite , and compared its performance against the proposed BF-PDCCH design in .
- The proposed content transfer task is clearly related to a long series of papers in summarization, including recent work with neural techniques @cite @cite . In particular, one recent paper casts the the task of generating an entire Wikipedia article as a multi-document summarization problem @cite . Their best-performing configuration was a two-stage extractive-abstractive framework; a multi-stage approach helped circumvent the difficulties of purely abstractive methods given quite large input token sequences.
- Another closely related task is update summarization @cite , where systems attempt to provide a brief summary of the novel information in a new article assuming the user has read a known set of prior documents. Our focus on curating an authoritative resource is a substantial difference. Also our datasets are substantially larger, enabling generative models to be used in this space, where prior update summarization techniques have been primarily extractive @cite @cite .
- Another form of content transfer bridges across modalities: text generation given schematized or semi-structured information. Recent research has addressed neural natural language generation techniques given a range of structured sources: selecting relevant database records and generating natural language descriptions of them @cite , selecting and describing slot-value pairs for task-specific dialog response generation @cite , and even generating Wikipedia biography abstracts given Infobox information @cite . Our task, while grounded in external content, is different in that it leverages grounding as well as prior text context when generating text. This challenging setting enables a huge range of grounded generation tasks: there are vast amounts of unstructured textual data.
- Different LfD architectures have been explored that employ various machine learning techniques, such as Gaussian Mixture Models, Hidden Markov Models, Neural Networks, Dynamic Motion Primitives, and Reinforcement Learning @cite , @cite , @cite . Methods involving inverse optimal control or Inverse Reinforcement Learning @cite , @cite are also used in cases where RL policy rewards are inferred from the demonstrations. Learning from Demonstration paradigms have been used to learn various kinematic tasks which mostly involve pick and place operations. Contact intensive tasks @cite cannot be easily inferred through noisy visual demonstrations, which adds additional layer of complexity. Solutions to these limitations require providing prior knowledge of environment, i.e. states and skills @cite . A generalized approach is proposed in @cite to tackling these limitations. A gross policy is learnt from demonstration based on physical interactions among agent and objects, and the corresponding forces are self learnt by the robot using reinforcement learning.
- Most existing approaches of group activity recognition are based on motion trajectories of group participants. Ni @cite applied motion trajectory segments as inputs and used frequency responses of digital filters to represent the motion information. Zhu @cite considered motion trajectory as a dynamic system and used the Markov stationary distribution to acquire local appearance features as a descriptor of group action. Chu @cite designed an algorithm to model the trajectories as series of heat sources to create a heat map for representing group actions. Cho @cite addressed the problem by using group interaction zones to detect meaningful groups to handle noisy information. Cheng @cite proposed a layered model of human group action and represented activity patterns with both motion and appearance information. Their performance on NUS-HGA achieved an accuracy of 96.20 @cite used a combination of Deep VGG network @cite and stacked LSTMs. Their model complexity is high and has a large chance of overfitting. In this case, their model is trained on augmented data thus cannot be fairly compared with other methods.
- The field of HRI has been the focus of many research efforts during the last few decades. A key observation is that, successful human-robot collaborations requires that the human has to understand the robot and, simultaneously, that the robot is also able to understand the human actions @cite . This has propelled fields such as robotics, computer vision, human cognition, psychology, and neuroscience, to combine resources in the aim of proposing techniques that tackle those intertwined issues.
- Rakovi ' c et al @cite models the gaze behavior interdependencies for two agents using Hidden Markov Models (HMM) where the states are the gaze fixations of one agent, and the observations are the gaze fixations of the other. HMM then estimates the valid gaze behavior of the agent observing the action, and the correct action performed by the other agent. This is used to build a controller which permits the robot (observer follower) to read the non-verbal communication cues from the gaze of the human participant (leader of the action) and infer the intended action. On another note, @cite state that to improve social interactions, robots and humans should mutually influence each other. As such, @cite worked on action alignment between humans and robots. Where the robot is the leader, and in order to correctly adapt to the intricacies of collaborating with a human, a Discrete Time Markov Model is used to give information whether the human is understanding the action of the robot.
- The work discussed above has focused on either intrapersonal coupling, synchronization, and action understanding from the gaze behavior of two agents. Notwithstanding, understanding of the arm movements in humans and robots is also a focus of extensive work. @cite have created a framework for evaluating predictable and legible arm movements for humans and robots. Where legibility is am exaggerated motion towards the end-goal, and the predictable is a natural human movement. Nonetheless, there is no guarantee of generalization for all types of scenarios. yet no understanding whether this is present in HHI, and in retrospect, HRI scenarios. Although, to improve social interactions, robots and humans should mutually influence each other @cite .
- One approach proposed for an efficient human-robot collaboration is to predict the human movement during the experiment and optimize the path for the robot to avoid colliding with the human @cite @cite . Although, this may work for some scenarios, it does not solve the core problem of HRI, the mutual understanding of each one's action. For these cases, only the robot is understanding the action of the human. One solution found is to delay the handover task of the robot, since that has proven to improve the human understanding of the robot's intention @cite .
- Regarding the stability analysis of subspace methods, there have been works on bounding the error in terms of the minimum singular value of Vandermonde matrices. Such inequalities can be found in @cite @cite for MUSIC and in @cite @cite for ESPRIT, as well as in @cite for the matrix pencil method. One major roadblock for this approach is that accurate bounds for the smallest singular value in the @math regime were not readily available. This difficulty was addressed in @cite , which provided the first accurate analysis of MUSIC in the @math regime. As for ESPRIT, the bounds in @cite @cite do not capture the exact dependence of the error on the minimum singular value, and consequently, are inaccurate when @math (see and and the discussion there).
- The minimum singular value of a Vandermonde matrix highly depends on the configuration of its nodes. The best available bound for the case @math was provided in @cite , which relied on the Beurling-Selberg machinery, see @cite . Recently there are several independent works which provide estimates for @math by incorporating additional geometric information about the support set, see @cite @cite @cite . Accurate lower bounds under a clumps model can be found in @cite .
- Prior works @cite @cite addressed super-resolution from an information theoretic view. They considered the situation where the atoms are located on a grid on @math with spacing @math and the given information consists of noisy continuous Fourier measurements. Both papers derived lower and upper bounds for a min-max error. These results showed that @math minimization is optimal for this discrete model, but it is not computationally feasible. ESPRIT is a polynomial-time algorithm that can be used for the more general off-the-grid-model". The investigation of the optimality of ESPRIT is an interesting future research direction.
- In addition to methods mentioned above, deep learning methods, especially deep convolutional neural networks (CNNs), have also shown great performance for natural image denoising problem. For example, @cite proposed a deep CNN for removing Gaussian noise, @cite used CNN for demosaicking and denoising. @cite introduced a U-Net variant autoencoder to perform natural image restoration. Although these networks have obtained good performance in denoising problem, if we naively concatenate the auxiliary feature buffers with noisy MC rendering images and feed them into these image denoising networks, they cannot generate satisfactory results comparing to other MC denoising models. This is because the auxiliary feature buffers have different natures with RGB images, and without specifically designed structure for them the image denoising networks can not deal well with the auxiliary feature buffers.
- The heuristic-driven approach is usually applied to reduce the computational cost of finding the optimal service, whose weakness resides in simply providing a sub-optimal selection closer to the optimum one @cite . Addressing the issue of service selection in mobile edge computing environment, @cite put forward a selection scheme minimizing the response time, whose arithmetic design integrated genetic and simulated annealing algorithm. @cite conducted research on service selection in the case study of cloud resource configurations, structuring the set of feasible configuration from which diverse Pareto-optimal configurations were selected.
- Furthermore, the machine learning (ML) methodology is also taken into use, intelligentizing the procedure of service selection @cite . On the basis of the reputation-driven matrix factorization approach, @cite conducted accurate QoS prediction on unknown web services, supporting the context of service selection. @cite designed a ML-based service selection approach based on learning-to-rank algorithm, taking advantage of historical service-selection decisions outcomes and eventually delivering personalized service selection of each user.
- Although significant attention has been received in service selection, few selection algorithms have been proposed when multiple service requests are in the service-oriented system for concurrent execution, no wonder from the point of fairness. Bi-criteria @cite analyzed concurrent sequential workflows from two aspects which are makespan and execution cost, and brought forward a fairness-aware service selection policy, preventing the running service of workflow from prolonged idleness and starvation. Specifically, the less a running service is affected by others fighting for simultaneous execution with shared candidates, the higher priority it attains to be selected to be performed in advance. In this way, all of concurrent workflows are well-done to be executed without over penalized. Services with highest execution priority make up the Pareto Set, solved by heuristic algorithms.
- Regarding the fairness metrics, max-min fairness has been thoroughly studied in the issue of resource allocation across the fields of distributed systems, data center network and queueing systems @cite . @cite generalized the max-min fairness to Dominant Resource Fairness on account of resource allocation with placement constraints, where several properties like sharing incentive and strategy-proofness got theoretically proved. @cite investigated the practical factors in Mesos agents which prevented itself from the desired DRF allocation and proposed specified configuration suggestions, filling the gap between target DRF resource allocation and actual resource allocation in the multi-user Apache Mesos cluster.
- Dense descriptor extraction and matching. An alternative to the detect-then-describe approach is to forego the detection stage and perform the description stage densely across the whole image @cite @cite @cite @cite . In practice, this approach has shown to lead to better matching results than sparse feature matching @cite @cite @cite , particularly under strong variations in illumination @cite . This identifies the detection stage as a significant weakness in detect-then-describe methods, which has motivated our approach.
- Image retrieval. The task of image retrieval @cite @cite @cite @cite @cite @cite also deals with finding correspondences between images in challenging situations with strong illumination or viewpoint changes. Several of these methods start by dense descriptor extraction @cite @cite @cite @cite and later aggregate these descriptors into a compact image-level descriptor for retrieval. Works most related to our approach are @cite @cite : @cite develops an approach similar to ours, where an attention module is added on top of the dense description stage to perform keypoint selection. However, their method is designed to produce only a few reliable keypoints as to reduce the false positive matching rate during retrieval. Our experiments demonstrate that our approach performs significantly better for matching and camera localization; @cite implicitly detects a set of keypoints as the global maxima of all feature maps, before pooling this information into a global image descriptor. @cite has inspired us to detect features as local maxima of feature maps.
- Object detection. The proposed describe-and-detect approach is also conceptually similar to modern approaches used in object detection @cite @cite @cite . These methods also start by a dense feature extraction step, which is followed by the scoring of a set of region proposals. A non-maximal-suppression stage is then performed to select only the most locally-salient proposals with respect to a classification score. Although these methods share conceptual similarities, they target a very different task and cannot be applied directly to obtain pixel-wise image correspondences.
- In the previous work for the multi-person pose estimation, large receptive filed is achieved by a sequential architecture in the Convolutional Pose Machines @cite @cite to implicitly capture the long-range spatial relations among multi-parts, producing the increasingly refined estimations. However, low-level information is ignored along the way. Stacked Hourglass Networks @cite @cite processes the feature maps across all scales to capture various spatial relationships of different resolutions, and adopt the skip layers to preserve spatial information at each resolution. Moreover, the Feature Pyramid Network architecture @cite is integrated in the GlobalNet of the Cascaded Pyramid Network @cite , to maintain both the high-level and low-level information from the feature maps of different scales.
- Visual attention has achieved great success in various tasks, such as the network architecture design @cite , image caption @cite @cite and pose estimation @cite . SE-Net @cite proposed a Squeeze-and-Excitation'' (SE) block to adaptively highlight the channel-wise feature maps by modeling the channel-wise statistics. However, SE block only considers the channel-wise relationship and ignores the importance of the spatial attention in the feature maps. SCA-CNN @cite proposed Spatial and Channel-wise Attentions in a CNN for image caption. Spatial and channel-wise attention not only encodes where (i.e., spatial attention) but also introduces what (i.e., channel-wise attention) the important visual attention is in the feature maps. However, spatial and channel-wise attention has little been used in the multi-person pose estimation yet. Chu @cite proposed the effective multi-context attention model for the human pose estimation. However, our proposed spatial and channel-wise attention residual bottleneck for the multi-person pose estimation has not been mentioned in @cite yet.
- . Attention mechanisms have been used to boost the network's ability to select the relevant features from the corresponding parts of the input. In video captioning, people do not describe everything in a video, instead they tend to talk more about semantically important regions and objects. @cite utilizes a spatial attention-based mechanism to learn where to focus in the image. This work is followed by @cite which introduces a temporal attention-based mechanism module to exploit temporal structure for video captioning. Recently, a new use of multimodal fusion attention is proposed to fuse information across different modalities in @cite .
- Perhaps the most common approaches for generating arbitrary shapes are @cite @cite @cite acting on 3D voxels. In @cite , the authors propose to jointly learn to autoencode 3D voxel occupancy grids and regress to them from images. @cite introduced a voxel denoising autoencoder. @cite proposed a voxel-GAN to generate novel shapes. Among the key drawbacks of volumetric methods are their inherent high computational complexity and coarse representation that can alter the topological structure of the shapes. are a simple and lightweight alternative to volumetric representation recently gaining popularity. Several methods have been proposed for representation learning of fixed-size point clouds @cite using the Pointnet @cite architecture. In @cite , point clouds of arbitrary size were addressed. In @cite and @cite , point cloud reconstruction from single images was performed. Despite their compactness, point clouds are not popular for realistic and high-quality 3D geometry generation due to the absence of neighborhood connectivity and lack of an underlying smooth structure.
- a set of recent methods trying to generalize neural network architectures to non-Euclidean domains such as graphs and manifolds @cite . Such methods have achieved very promising results in geometry processing and computer graphics @cite @cite @cite , computational chemistry and drug design @cite @cite , and network science @cite @cite . Multiple approaches have been proposed to construct convolution-like operations on graphs and meshes, including spectral methods @cite @cite @cite @cite , local charting based @cite @cite @cite @cite @cite @cite @cite and graph attention @cite . Finally, in order to aggregate local information, graph or mesh coarsening techniques @cite @cite have been proposed, equivalent to image pooling.
- The particular artificial neural networks we use for causality detection in our primary method fall into the general class of Siamese neural networks, which are composed using two subnetworks with shared weights, each responsible for processing one of the inputs. While originally proposed for signature verification (a type of similarity detection) in @cite @cite , they have recently attracted renewed interest @cite @cite @cite @cite @cite @cite @cite @cite , with many applications ranging from image recognition to audio search. The key abstract feature of Siamese networks that is important for us is their capability to directly encode structural properties of a logical relation (in our case causality) and hence to enable efficient learning tailored to the domain. Although we use relatively low-complexity 1D convolutional Siamese networks suitable for time series processing, we had to resort to curriculum training to obtain a stable learning algorithm. While conventional Siamese networks are suitable for similarity relations that exhibit a structural symmetry, we also use a variation of Siamese networks to detect causality which directly encode the structural antisymmetry of this relation (when viewed as a function) in terms of the network structure.
- Although there are not many studies focusing directly on automated quantitative cellularity assessment, it has been shown that this task can be solved by first segmenting malignant cells and then computing the tumor's area @cite . Many efforts have been devoted to developing supervised and unsupervised methods for automated cell and nuclear segmentation and detection @cite @cite . Supervised segmentation models have superior performance but require hand-labeled nuclear mask annotations @cite . In these approaches, segmented nuclear bodies are used to extract features that are typically inspired by visual markers recognized by pathologists. Commonly used features describe morphology, texture, and spatial relationships among cell nuclei in tissue @cite @cite .
- The conventional approach most relevant to our work is by Peikari @cite who proposed an automated cellularity assesement protocol. First, they used smaller patches, or regions of interest (RoI), extracted from whole slide images to segment all present cell nuclei. Then they extracted a number of predefined features from segmented nuclei and used support vector machines to distinguish lymphocytes and normal epithelial nuclei from malignant ones. Cellularity estimation was done using distinguished malignant epithelial figures for every RoI.
- Alternatively, segmentation-free methods that directly estimate cellularity from histopathology imaging data and nuclei locations annotated by human observers were also shown promising. In particular, Veta @cite proposed a deep learning-based method that leverages an information from a tumor's cells nuclei locations (centroids) and predicts the areas of individual nuclei and mean nuclear area without the intermediate step of nuclei segmentation. In particular, this approach was based on a 10-layer deep neural network predicting nuclear areas quantized into 20 histogram bins. The results showed that predicted measurements had substantial agreement with manual measurements, which suggests that it is possible to compute the areas directly from imaging data, without the intermediate step of nuclei segmentation. This is in spirit similar to one of our approaches, but we do not directly compare our methods to Veta since we use different datasets and performance metrics.
- Recent works by Akbar @cite @cite have compared the conventional approach based on segmentation and feature extraction and direct applications of deep CNNs to image patches in both regression and classification settings. Overall, they showed that the DL-based approach outperformed hand-crafted features in both accuracy and intra-class correlation (ICC) with expert pathologist annotations. Specifically, their best result was achieved by using a pre-trained Inception @cite model that reached ICC of @math and @math with two expert pathologists. In this study we evaluate even wider range of DL-based approaches, including segmentation-based and segmentation-free, in both regression and classification settings. We provide appropriate performance comparisons with previously reported results. All the methods developed in this study are fully automatic and do not require any involvement of the human annotators at the test time.
- Visual representation learning without supervision is an old and active area of research. It has two common modeling approaches: generative and discriminative. A generative approach tries to model the data distribution directly. This can be modeled as maximizing the probability of reconstructing the input @cite @cite @cite and optionally estimating latent variables @cite @cite or using adversarial training @cite @cite . Our work focuses on discriminative learning.
- In this work, we choose to scale image-based self-supervised methods because of their ease of implementation. Many tasks have been designed for images that exploit their spatial structure @cite @cite @cite @cite , color information @cite @cite @cite @cite , illumination @cite , rotation @cite . These tasks model different properties of images and have been shown to contain complementary information @cite . Given the abundance of such approaches to use, in our work, we focus on two popular approaches that are simple to implement, intuitive, and diverse: from @cite and from @cite . A concurrent work @cite also explores multiple self-supervised tasks but their focus is on the architectural details which is complementary to ours.
- Event detection has been widely studied in various domains, including public health, urban computing, and social network analysis. The works @cite @cite @cite and other recent works on event detection @cite @cite use already observed counts. An event is defined as a region with significantly higher counts, such as disease reports or number of taxi drops. Social media posts and geo-tagged tweets have been used as well to detect and forecast events such as social unrests and protests @cite @cite @cite @cite . Regions and time windows where the frequency of certain keywords exhibit abnormal changes are identified as events. These works do not use mobility data. The of the events such as gathering or dispersing .
- has been studied closely in recent years, due to access to public taxi datasets @cite . To the best of our knowledge none of the proposed methods directly address the prediction problem in case of anomaly. State-of-the-art methods for predicting taxi demand use historical data and time series analysis. @cite propose a deep learning framework which captures the spatial and temporal dependencies to predict taxi demand. @cite formulate an LSTM Network to learn the regular pattern of taxi demand. @cite show the regular taxi demand is highly predictable and test different algorithms to approach the maximum accuracy. @cite used spatial clustering to predict demand hotspots. They predict areas with high density of demand using DBSCAN. Such areas, despite having high demand, are part of the regular pattern. @cite used streams of taxi data as time series to predict taxi demand in the next 30-minute period. @cite used time series analysis to solve the demand prediction problem, giving recommendations to drivers. @cite used a simple multi-output ANN to predict demand, using features created from recent demand, time and weather information.
- The DISTRAL algorithm as well as the present paper can be seen as taking an intermediate position: unlike in the class of RL-as-inference algorithms the default policy is not fixed but learned, but unlike in the classical EM policy search the final result of the optimization remains regularized since the default policy is constrained relative to the policy. As explained above this can be seen as analogous to the relative roles of learned model and observation specific posterior in fitting a generative model. Similar to DISTRAL, Divide and Conquer learns an ensemble of policies, each specializing to a particular context, which are regularized towards one another via a symmetric KL penalty, with the behavior of the ensemble distilled to a single fixed policy. In concurrent work @cite propose an information bottleneck architecture for policies with latent variables that leads to a KL-regularized formulation similar to the one described in Appendix . The information bottleneck is implemented in latent space and the default policy is obtained by marginalization with a goal-agnostic prior.
- There are several different types of the visualization techniques for generating heatmaps for a deep network. We classify them into one-step backpropagation-based approaches @cite @cite @cite @cite @cite @cite @cite @cite , and perturbation-based approaches, e.g., @cite @cite @cite @cite .
- Another seemingly related but different domain is the saliency map from human fixation @cite . Fixation Prediction @cite @cite aims to identify the fixation points that human viewers would focus on at first glance of a given image. When predicting eye fixation, the algorithm is guessing the regions humans are looking at, while our goal is to explain what the deep models focus on for a given image to make decisions. Deep models may use completely different mechanisms to classify than humans, hence human fixations should not be used to train or evaluate heatmap models.
- Our result is most closely related to a recent proposal by , who studied any linear functional whose Riesz representer admits an (approximate) linear representation. In another paper, considers theoretical results for estimators based on learning conditional mean function and the propensity score. In both papers, the key condition is that the product of @math -loss for learning the two nuisance parameters is @math , a condition referred to as rate double robustness; see Definition 2 in @cite . Sufficient conditions for rate double robustness have been provided in these works in terms of sparsity levels. For example, Remark 5.2 of shows that rate double robustness is guaranteed when the product of two sparsity levels is @math , while Remark 7 of points out that under the assumption of bounded @math -norm of both parameters, rate double robustness holds whenever one of the sparsity levels is @math .
- We also note two recent papers that consider estimators that resemble ours. consider an estimator that, in the spirit of , first fit a penalized covariate-balancing propensity model, and then re-fit without penalty those coefficients that correspond to features that are relevant to outcome modeling. Meanwhile, augments a penalized covariate-balancing propensity model in an outcome regression; it turns out that his covariate-balancing mechanism designed to address the issue of misspecification is also helpful for relaxing sparsity requirements. Neither paper, however, achieves sparsity double robustness as discussed here; rather, they require both the outcome parameter vector @math and the propensity parameter vector @math to be ultra-sparse---or, if there is misspecification they require the population minimizers of both the outcome and propensity loss functions to be ultra-sparse. Under the framework of @cite @cite , are classified as examples of model double robustness, which means that one of the models (either conditional mean or propensity score) is misspecified. Rate double robustness requires that the product of the @math -norms of the estimation errors in two models is of the order @math .
- The most relevant works are by van Leeuwen and Schoone @cite , and Oda and Watanabe @cite for Hamiltonian cycles, and by Bonnet and Miltzow @cite for matchings. They proved, with elegant arguments, the following results.
- The @math upper bound of Theorem carries over to perfect matchings. As for lower bounds, Bonnet and Miltzow @cite presented two matchings @math and @math such that @math and @math . The bound @math holds even if @math is a bichromatic matching, while the proof of @math does not generalize for the bichromatic setting.
- Driving in the Matrix @cite focuses on 2D object detection using GTA V. The 2D bounding boxes from GTA V are produced from 3D bounding boxes transformed to the 2D perspective and are thus loose-fitting. The authors describe a method to capture depth and stencil (class-wise segmentation) buffers from the graphics pipeline. They use the stencil buffers to tighten 2D bounding boxes around classes and use the depth information to separate instances from the same stencil class. With this method they are able to obtain tight 2D bounding boxes for over 200,000 images. They show how they have obtained a larger coverage of data than Cityscapes @cite . When only training on their synthetic data, they receive better results on KITTI than when only training on Cityscapes. @cite also use 2D images from GTA V to detect distance to objects. They generate over 480,000 instances to train a CNN with 2D images for various tasks such as vehicle distance and lane markings. To summarize, 2D data from GTA V has provided notable performance improvements for training detection networks.
- @cite and SqueezeSeg @cite have leveraged 3D data from GTA V by creating an in-game LiDAR using ray casting. Ray casting can be done in-game through a GTA V native function. The benefit of using GTA V for point cloud segmentation is that each point has an accurate instance level segmentation annotation for any object class (i.e. vehicles, pedestrians, and cyclists). This is a difficult and time-consuming task for a human annotator to achieve as each 3D point needs to be labelled (there can be over a million points per LiDAR scan). In these works, for the KITTI data, individual points are annotated with instance level object information using 3D bounding boxes since the true annotations are unavailable. Augmenting the KITTI data (without reflectance) with GTA V point clouds increased accuracy by about 9 percent IoU. These experiments were only conducted on cars.
- Ray casting may seem like a great solution for creating point clouds in GTA V but there are major downsides with the underlying function mechanisms. SqueezeSeg @cite notes that GTA-V uses very simple physical models for pedestrians, often reducing people to cylinders." Furthermore, some vehicles are simply not hit by ray casting, which could be a major problem for image and LiDAR fusion methods. Our work addresses both of these limitations.
- @cite use deep learning to remove object points from real-world point clouds then insert simulated points from 3D object models. Using this process they are able to augment real-world data with new training samples containing different objects in the same environment without having to collect or annotate more data. They increase accuracy by adding these augmentations to their training dataset. This method does not create images that match, which is an important data element for sensor fusion methods.
- Playing for Data @cite uses GTA V to generate a semantic segmentation dataset comprised of 25,000 video frames. The experiments show that synthetic data significantly increases accuracy when supplementing smaller real-world datasets. Furthermore, it significantly reduces the amount of time and resources for obtaining larger amounts of data. Playing for Benchmarks @cite generates 250,000 video frames of semantic segmentation information and includes annotations for optical flow, instance-level segmentation, 2D object detection, and tracking. However, these methods use a technique called detouring, which avoids interacting with GTA V code and only allows coarse semantic segmentation labelling.
- The URSA dataset @cite increases the quality of semantic segmentation labels generated from GTA V by labelling each texture which is encountered in the game. This allows unlimited data generation without any extra annotation time. They also increase the size of the released dataset to contain over 1,000,000 images. They show further increases compared to Playing for Benchmarks and demonstrate how more synthetic data can increase network performance.
- Quantum measurement has a fundamental role in quantum mechanics with several different theoretical interpretations @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . The measurement of a quantum system collapses of the quantum system into an eigenstate of the operator corresponding to the measurement. The measurement of a quantum system produces a measurement result, the expected values of measurement are associated with a particular probability distribution.
- The von Neumann measurements are a special case of a more general measurement, the POVM measurement @cite @cite @cite @cite @cite @cite . Without loss of generality, the POVM is a generalized measurement that can be interpreted as a von Neumann measurement that utilizes an additional quantum system (called ancilla). The POVM measurement is mathematically described by a set of positive operators such that their sum is the identity operator @cite @cite @cite . The POVM measurements therefore can be expressed in terms of projective measurements (see also Neumark's dilation theorem @cite @cite @cite ).
- Another subject connected to quantum measurement theory is quantum-state discrimination @cite @cite @cite @cite @cite that covers the distinguishability of quantum states, and the problem of differentiation between non-orthogonal quantum states.
- The theoretical background of the gate-model quantum computer environment utilized in our manuscript can be found in @cite and @cite .
- An optimization algorithm related to gate-model quantum computer architectures is defined in @cite . The optimization algorithm is called Quantum Approximate Optimization Algorithm (QAOA). The aim of the algorithm is to output approximate solutions for combinatorial optimization problems fed into the quantum computer. The algorithm is implementable via gate-model quantum computers such that the depth of the quantum circuit grows linearly with a particular control parameter. The work also proposed the performance of the algorithm at the utilization of different gate parameter values for the unitaries of the gate-model computer environment.
- In @cite , the authors studied some attributes of the QAOA algorithm. The authors showed that the output distribution provided by QAOA cannot be efficiently simulated on any classical device. A comparison with the Quantum Adiabatic Algorithm (QADI) @cite @cite is also proposed in the work. The work concluded that the QAOA can be implemented on near-term gate-model quantum computers for optimization problems.
- An application of the QAOA algorithm to a bounded occurrence constraint problem Max E3LIN2 can be found in @cite . In the analyzed problem, the input is a set of linear equations each of which has three boolean variables, and each equation outputs whether the sum of the variables is 0 or is 1 in a mod 2 representation. The work is aimed to demonstrate the capabilities of the QAOA algorithm in a gate-model quantum computer environment.
- In @cite , the authors studied the objective function value distributions of the QAOA algorithm. The work concluded, at some particular setting and conditions the objective function values could become concentrated. A conclusion of the work, the number of running sequences of the quantum computer can be reduced.
- In @cite , the authors analyzed the experimental implementation of the QAOA algorithm on near-term gate-model quantum devices. The work also defined an optimization method for the QAOA, and studied the performance of QAOA. As the authors found, the QAOA can learn via optimization to utilize non-adiabatic mechanisms.
- In @cite , the authors studied the implementation of QAOA with parallelizable gates. The work introduced a scheme to parallelize the QAOA for arbitrary all-to-all connected problem graphs in a layout of qubits. The proposed method was defined by single qubit operations and the interactions were set by pair-wise CNOT gates among nearest neighbors. As the work concluded, this structure allows for a parallelizable implementation in quantum devices with a square lattice geometry.
- In @cite , the authors defined a gate-model quantum neural network. The gate-model quantum neural network describes a quantum neural network implemented on gate-model quantum computer. The work focuses on the architectural attributes of a gate-model quantum neural network, and studies the training methods. A particular problem studied in the work is the classification of classical data sets which consist of bitstrings with binary labels. In the architectural model of a gate-model quantum neural network, the weights are represented by the gate parameters of the unitaries of the network, and the training method acts these gate parameters. As the authors stated, the gate-model quantum neural networks represent a practically implementable solution for the realization of quantum neural networks on near-term gate-model quantum computer architectures.
- In @cite , the authors defined a quantum algorithm that is realized via a quantum Markov process. The analyzed process of the work was a quantum version of a classical probabilistic algorithm for @math -SAT defined in @cite . The work also studied the performance of the proposed quantum algorithm and compared it with the classical algorithm.
- For a review on the noisy intermediate-scale quantum (NISQ) era and its technological effects and impacts on quantum computing, see @cite .
- The subject of quantum computational supremacy (tasks and problems that quantum computers can solve but are beyond the capability of any classical computer) and its practical implications are studied in @cite . For a work on the complexity-theoretic foundations of quantum supremacy, see @cite .
- A comprehensive survey on quantum channels can be found in @cite , while for a survey on quantum computing technology, see @cite .
- In traditional information processing, compressed sensing @cite is a technique to reduce the sampling rate to recover a signal from fewer samples than it is stated by the Shannon-Nyquist sampling theorem (that states that the sampling rate of a continuous-time signal must be twice its highest frequency for the reconstruction) @cite @cite @cite @cite . In the framework of compressed sensing, the signal reconstruction process exploits the sparsity of signals (in the context of compressed sensing, a signal is called sparse if most of its components are zero) @cite @cite @cite @cite @cite @cite . Along with the sparsity, the restricted isometry property @cite @cite @cite is also an important concept of compressed sensing, since, without loss of generality, this property makes it possible to yield unique outputs from the measurements of the sparse inputs. The restricted isometry property is also a well-studied problem in the field of compressed sensing @cite @cite @cite @cite @cite @cite .
- A special technique within compressed sensing is the so-called 1-bit compressed sensing @cite @cite @cite , where 1-bit measurements are applied that preserve only the sign information of the measurements.
- The application of compressed sensing covers the fields of traditional signal processing, image processing and several different fields of computational mathematics @cite @cite @cite @cite @cite @cite @cite @cite .
- From a technical standpoint, our approach is based on the conditional GAN technique. GANs @cite and conditional GANs @cite are popular formulations for learning generative models. For computer vision, GANs are originally introduced to generate images @cite @cite . In addition, conditional GANs are employed to learning a mapping from one manifold to another one, such as image style transfer @cite , image inpainting @cite and image captioning @cite . Moreover, Walker @cite exploit GAN to predict the future frames of the video in pixel space, which does not take the semantic action label into consideration. In recent work @cite , the generative adversarial imitation learning of Ho and Ermon @cite is utilized to forecast the intermediate representation of the future frames, and a linear-SVM is implemented to classify the action based on the anticipated representation. Despite its success in action prediction, it is a two-stage framework and not end-to-end trainable. Compared with this work, we extend the classical GAN framework and make it end-to-end trainable for action prediction task.
- For obtaining robust and superior results, integration of multiple clues or factors is usually adopted in computer vision and machine learning community. Because of close similarity between anomaly map and saliency map, we review some work about multi-saliency fusion here. The straightforward and most intuitive scheme is linear fusion. Evangelopoulos @cite apply this framework to fuse aural, visual and textual saliency. For more elaborate fusion, a Support Vector Machine is trained and used to predict the quality of each saliency map in @cite , and then saliency maps are fused linearly using the quality measure of each map. Besides, Xie @cite merge low and mid level visual saliency within the Bayesian framework, which generates more discriminative saliency map. Furthermore, the Bayesian integration method is also employed in @cite and performs better than the conventional integration strategy.
- In recent years, there has been considerable interest in examining the rich clinical information stored in electronic health records. However, manually annotating a large dataset to fulfill the data-hungry deep learning models is time-consuming and expensive. For example, a radiologist usually read at a speed of 10 mins per example for CT scan studies @cite . Instead, researchers may benefit from using text-mining to generate annotations even if those annotations are of modest accuracy @cite . To reduce manual annotation burden, some researchers leveraged the rich information contained in associated radiology reports. Disease-related labels have been mined from reports for classification and weakly-supervised localization on X-ray and CT images @cite @cite @cite @cite .
- Owing to the rapid growth of available EHR, deep learning methods for text mining become more appealing recently because of its competitive performance versus traditional methods and its ability to relieve the feature sparsity and engineering issue @cite . For example, both multi-channel dependency-based CNNs @cite and shortest path-based CNNs @cite are well suited for sentence-based relation extraction. It is also generally faster to train a CNN model than other deep learning networks.
- More recent attempts have been made to utilize the attention mechanism to help the network focus on salient features @cite . It has been firstly used in recurrent neural networks architectures in the NLP applications like machine translations. In those cases, the attention mechanisms allow the model to attend to'' (correlate) different parts of the sentence at each step, thus the output depends on a weighted combination of all the input states. extended this idea by proposing the Transformer, a model that relies on a self-attention mechanism to draw global dependencies between input and output @cite . They also introduced multi-head'' to attend different portions of the representation subspaces in parallel. further demonstrated that the self-attention mechanism can be used in the CNN-based approaches, achieving both fast and accurate performance in the text classification task @cite .
- Bayesian networks structure learning has attracted great attention in recent years. In a nutshell, structure learning can be defined to find a DAG that fits the data. There are two distinct approaches to structuring learning as score-based and constraint-based. In a score-based approach, each DAG gets a score based on how well it fits the data and finding the DAG that maximises its score @cite @cite . On the other hand, in the constraint-based approach, also known as conditional independence test-based approaches, is based on testing conditional independencies between variables and finding the best DAG that represents these relations @cite .
- In literature, there exist two main methods for online tracking of some feature in a structure, which are temporal difference learning (TDL) @cite and Bayesian change-point detection (CPD) @cite . Nevertheless, both method has not been applied to detect changes in a casual structure, so they need some modifications to do this.
- The standard TDL algorithm provides a dynamic estimate of a univariate random variable using a simple update rule. In this update, the error in the current estimate is updated with a learning rate coefficient. Therefore, the static learning rate plays an important role and controls how quickly or slowly a model learns a problem. If it is chosen too small, the TDL algorithm converges slowly. If it is chosen too large, the algorithm will be so sensitive even when the environment is stable. TDL algorithm can detect slow change but not high stability or dramatic changes. That feature is essential for causal structure learning as causal structures often have non-deterministic connections. In contrast, CPD algorithms are useful for dramatic changes that indicate breaks between periods of stability @cite . CPD algorithms must store large parts of input data. These algorithms assume that the model only has a dramatically changing environment separated by periods of stability. Both algorithm types have not been applied to tracking causal structure. To do so, they need the necessary modifications.
- Talih and Hengartner @cite do other related work. In their work, the data sets are taken sequentially as input and divided into a fixed number of data intervals, each with an associated undirected graph that differentiates one edge from its neighbours. In contrast to our work, in their work, they focus on a particular type of graphical structure change (a single edge added or removed), only work in batch mode and use undirected graphics instead of directed acyclic graphical models. Next, Siracusa @cite uses a Bayesian approach to find posterior uncertainty on possible directed edges at different points in a time series. Our work differs from their work because we use frequentist methods instead of Bayesian methods, and we can work in real time in an incoming data stream.
- Unsupervised approaches select key subshots according to manually designed criteria @cite @cite @cite @cite , such as representative to the video content and diverse with each other, etc. Clustering is one of the most popular unsupervised summarization approaches @cite @cite @cite . Practically, with hand-crafted features, similar frames are grouped into the same cluster, and the cluster centers are taken as the most representative elements and selected into the summary. Earlier works apply clustering algorithms to video summarization directly @cite @cite . Later, more works integrate the domain knowledge of video data into clustering algorithms @cite @cite . As in @cite , the frames are initially clustered in sequential order, as consecutive frames are similar and more probably to be allocated to the same cluster. Other works construct more comprehensive models based on the idea of clustering @cite @cite . As in @cite , the video is transformed into an undirected graph, and the summary is generated by partitioning this graph into clusters. More recently, a co-clustering approach is proposed to simultaneously summarize several videos with the same topic by their co-occurrence, i.e., similar subshots shared by these videos are selected into the summary @cite .
- Dictionary learning is another important unsupervised summarization approach @cite @cite @cite @cite . This kind of approaches seek to select a few key subshots to compose a compact dictionary so as to represent the video content. @cite supposes that the original video can be reconstructed by its summary sparsely. Based on this point, the summary is generated by sparse coding. Furthermore, the Locality-constrained Linear Coding (LLC) is introduced to @cite to preserve the local similarity of video subshots when reconstructing the original video. Besides, to improve the efficiency, @cite propose a quasi real-time dictionary learning approach to summarize the video, which updates the video summary on-the-fly by adding those elements that cannot be well reconstructed by current video summary.
- Supervised approaches learn the hidden summarization patterns from human generated summaries, which have been drawing increasing attention and getting more promising results than unsupervised ones @cite @cite . In supervised approaches, property models are usually taken as the decision criteria to select key subshots @cite @cite @cite @cite . For example, @cite and @cite build importance model and interesting model to score the subshots, and those subshots with higher scores are selected into the summary. @cite designs a stroyness model to constrain that the selected subshots have a smooth story line. Moreover, @cite employs three property models, i.e., interestingness, representativeness, uniformity, to build a comprehensive score function. Some other works even utilize auxiliary information to summarize videos, such as web image priors @cite , video titles @cite , and video category labels @cite , etc.
- GAN @cite has achieved remarkable results. In the training stage of GAN, the discriminator tries to distinguish the outputs of generator and the real distribution. On the contrary, the generator tries to fool the discriminator. After training, the generator can produce outputs which are similar to the real samples. In our model, GAN is used to align the labels for inputs, constrain the content code to common space, and make the style code and content code be separable.
- Several recent works leveraged GANs for semi-supervised learning of classification models. The works of @cite @cite train a discriminator to classify input into different classes. The work of @cite introduces a separate discriminator and classifier models. Other approaches incorporate inference models to predict missing labeled features @cite or harness the joint distribution of labels and data matching @cite . Recently, the setting of training a classifier from a few labels is introducing to generative model. The work of @cite learns a generative model from a few labels by clustering deep features. Unlike the above works, we focus on exploiting semi-supervised learning for image-to-image translation task.
- Image-to-image translation tasks have attracted increasing focus. For instance, Pix2pix @cite achieves translation based on paired image. To use unpaired images, CycleGAN @cite , DiscoGAN @cite , DualGAN @cite and UNIT @cite are proposed. For multi-domain translation, ComboGAN @cite learns multiple generators and StarGAN @cite reduces them to a single one by inputting the target labels and images together. GANimation @cite describes the labels in a continuous manifold. CerfGAN @cite adopts a multi-class discriminator to enable the generator to translate images with high domain shifts. For multi-modal translation, BicycleGAN @cite extends pix2pix by learning a stochastic mapping from source to target. MUNIT @cite and DRIT @cite decompose the image representation into style code and content code, and then decode back to translated images. Augmented CycleGAN @cite learns many-to-many translation by using the stochastic mappings. Recently, SMIT @cite is proposed to solve these basic tasks jointly, in which the images with labels and additional style noises are input to the generator. Unlike existing methods, AGUIT encodes the attributes into the style code to conduct high-level translation tasks.
- Representation disentanglement aims at disentangling and explaining the learned representation. The methods can be divided into two categories, , supervised disentanglement and unsupervised disentanglement. Supervised disentanglement methods @cite @cite @cite @cite @cite @cite make use of labeled data while unsupervised disentanglement methods @cite @cite @cite @cite @cite learn the properties from unlabeled data. The proposed AGUIT can conduct both supervised and unsupervised disentanglement by the proposed ingenious style code.
- More recently in @cite , points corresponding to the edges of the checkerboard were used to fit lines. The corresponding edge lines were obtained from the camera and constraints were formed using plane-line correspondence. As explained in the previous section, these features are more prone to errors and hence we avoid using them.
- Existing unsupervised deep learning methods generally fall into four different categories: (1) Clustering analysis @cite @cite @cite , (2) Sample specificity learning @cite @cite , (3) Self-supervised learning @cite @cite @cite @cite @cite , and (4) Generative models @cite @cite .
- Clustering analysis is a long-standing approach to unsupervised machine learning @cite . With the surge of deep learning techniques, recent studies have attempted to optimise clustering analysis and representation learning jointly for maximising their complementary benefits @cite @cite @cite @cite . Regardless, the key remains the discovery of multiple class consistent clusters (or groups) on the entire training data. This is a difficult task with the complexity and solution space exponentially proportional to both the data and cluster size. It is particularly so for clustering the data in complex structures and distributions such as images and videos. In contrast, the proposed AND model replaces the clustering operation with local neighbourhood identification in a divide-and-conquer principle. This enables the control and mitigation of the clustering errors and their negative propagation, potentially yielding more accurate inference of latent class decision boundaries.
- Sample specificity learning goes to the other extreme by considering every single sample as an independent class @cite @cite . The key idea is that supervised deep learning of neural networks automatically reveals the visual similarity correlation between different classes from end-to-end optimisation. However, this sort of supervision does not explicitly model the class decision boundaries as clustering analysis and the AND model. It is therefore likely to yield more ambiguous class structures and less discriminative feature representations.
- Self-supervised learning has recently gained increasing research efforts @cite @cite @cite @cite @cite . Existing methods vary essentially in the design of unsupervised auxiliary supervision. Typically, such auxiliary supervision is hand-crafted to exploit some information intrinsically available in the unlabelled training data, such as spatial context @cite @cite , spatio-temporal continuity @cite @cite , and colour patterns @cite @cite . Due to the weak correlation with the underlying class targets, such learning methods mostly yield less discriminative models than clustering analysis and our AND method. How to design more target related auxiliary supervision remains an open problem.
- Generative model is a principled way of learning the true data distribution of the training set in an unsupervised manner. The most commonly used and efficient generative models include Restricted Boltzmann Machines @cite @cite @cite , Autoencoders @cite @cite , and Generative Adversarial Networks @cite @cite . The proposed AND model does not belong to this family, but potentially generates complementary feature representations due to a distinct modelling strategy.
- Broadly, AND relates to constrained clustering @cite @cite @cite @cite if considering our neighbourhood constraint as a form of pairwise supervision including must-link and cannot-link. However, our method is totally unsupervised without the need for pairwise links therefore more scalable.
- have originally been proposed to provide high-bandwidth communication between nodes in computing clusters while minimizing the cost of switching hardware @cite . Fat-trees derive their name from the increasing bandwidth requirements on edges closer to the root because they relay traffic for all children in the underlying sub-tree. Fat-trees were later also adopted explicitly or implicitly in , in which nodes connected using Internet protocols are organized in logical networks for efficient communication, to provide, for example, multicast communication @cite @cite .
- Extensive work on tree overlays for multicast applications has been done since the 90s @cite , in which the same data is disseminated from a single source to tens and up to millions of participants. Typical applications of volunteer computing have different data transfer patterns because each participant receives a different sub-set of data. BOINC submits the same computation to a small number of participants (at least three) until a majority agrees @cite , while the current version of Pando @cite does not use redundancy because the code is executed on trusted devices. In addition, in both cases, each participant will return different results to the root.
- To the best of our knowledge, we are the first to propose a fat-tree overlay for scaling volunteer computing applications that supports an infinite number of inputs and provides a scheme for allocating nodes in the tree. ATLAS @cite 's tree of managers organized around work-stealing is perhaps the oldest documented scheme that relies on a tree for scalability but little details about the implementation were provided and the actual implementation was tested with only 8 machines. Javeline++ @cite relies on a tree structure to implement a scheduler but the scheme relied on tasks being finite and the position of a new node in the tree is computed from the root. Bayanihan @cite conceived a tree of servers that maps to the underlying network topology when the bandwidth on the link to a single server is insufficient, but to the best of our knowledge the scheme was never implemented. Connection decisions in our scheme do not require global information about the tree, yet they ensure probabilistic balancing and guarantee the routing of multiple connection messages to the same leaf node.
- BOINC @cite currently supports hundreds of thousands of participants but relies on a dedicated server with sufficient resources and an interaction pattern that is tailored to long running computations. Volunteers obtain the task to perform and transmit the results in two different remote procedure calls. Participant failures are detected with a soft limit on the expected time to completion, which therefore requires an estimate that is application dependent. Our design is tailored to shorter running tasks and instead relies on the heartbeat mechanism provided by WebRTC to detect the failure of a participant. Moreover, by relying on WebRTC to scale up the number of concurrent connections, we can support at least a thousand participants with no investment in dedicated hardware nor renting of hosted resources.
- Compared to other published volunteer computing tools, we are the first to have successfully tested with a thousand participants and the first to use WebRTC to connect participants in a fat-tree overlay. Most published volunteer computing tools @cite @cite @cite @cite @cite @cite @cite @cite @cite were tested with less than a hundred of participants. Some of the most recent have been tested with more than a hundred participants @cite @cite @cite and even up to 400 concurrent participants @cite . But the largest internet deployments of custom tools @cite @cite @cite have so far reached a hundred concurrent participants @cite .
- We propose to benefit of the above methods while avoiding their mentioned limitations. To this aim, we do construct a probabilistic background model, but it is applied to a new feature image that we call the . This residual is obtained by computing the difference between a self-similar version of the target image and the target itself. Being not self-similar, this background is akin to a colored noise. Hence a hypothesis test can be applied, and more precisely multiple hypothesis testing (also called method), as proposed in @cite . In that way, we present a general and simple method that is universal and detects anomalies by a rigorous threshold. It does not require learning, and it is easily made multiscale.
- Recent efforts to deal with label noise address two scenarios @cite : closed-set and open-set label noise. In the closed set scenario, the set of possible labels @math is known and fixed. All samples, including noisy ones, have their true label in this set. In the open set scenario, the true label of a noisy sample @math may be outside @math ; i.e. @math may be an out-of-distribution sample @cite . The remainder of this section briefly reviews related work in the closed-set scenario considered in @cite , upon which we base our approach.
- Several types of noise can be studied in the closed-set scenario, namely or random label noise. The former is also known as symmetric label noise and implies ground-truth labels flipped to a different class with uniform random probability. Non-uniform or class-conditional label noise, on the other hand, has different flipping probabilities for each class @cite . Previous research @cite suggests that uniform label noise is more challenging than non-uniform.
- Other approaches seek to relabel the noisy samples by modeling their noise through directed graphical models @cite , Conditional Random Fields @cite , or CNNs @cite . Unfortunately, to predict the true label, these approaches rely on the assumption that a small set of clean samples is always available, which limits their applicability. @cite have, however, recently demonstrated that it is possible to do unsupervised sample relabeling using the network predictions to predict hard or soft labels.
- In contrast to the aforementioned literature, we propose to deal with noisy labels using exclusively the training loss of each sample without consulting any clean set. Specifically, we fit a two-component beta mixture model to the training loss of each sample to model clean and noisy samples. We use this unsupervised model to implement a loss correction approach that benefits both from bootstrapping @cite and mixup data augmentation @cite to deal with the closed-set label noise scenario.
- Regarding the detection of personalities, emotion and cultural aspects in pedestrian from crowds, @cite proposed a method to identify groups and characterize them to assess the aspects of cultural differences through the mapping of the Hofstede's dimensions @cite . A similar idea, however using computer simulation and not focused on computer vision, is proposed by @cite . They use Hofstede's dimensions to create a simulated crowd from a cultural perspective. Gorbova and collaborators @cite present a system of automatic personality screening from video presentations in order to make a decision whether a person has to be invited to a job interview based on visual, audio and lexical cues. The work proposed by @cite , presents a model to detect personality aspects based on the Big-five personality model using individuals behaviors automatically detected in video sequences.
- Several models have been developed to explain and quantify basic emotions in humans. One of the most cited is proposed by Paul Ekman @cite which considers the existence of 6 universal emotions based on cross-cultural facial expressions (anger, disgust, fear, happiness, sadness and surprise). @cite , the authors proposed a way to detect pedestrian emotions in videos, based on OCC emotion model. To detect the emotions of each pedestrian, the authors used OCEAN as inputs, as proposed by Saifi @cite . In our approach, we proceed with an analysis in order to verify if subjects can perceive geometric features as distances speeds as well as emotions and personalities in video sequences when pedestrians are represented by virtual humans. Next section present how we performed the analysis.
- Deep Neural Decision Forests @cite demonstrates how neural decision tree forests can replace a fully connected layer using stochastic and differential decision trees which assume the node split structure fixed and the node split is learned.
- The same principles have been applied to RF imaging, via a concept known in this spectrum as single-radar imaging @cite . In the RF case, the imaging process refers to the reconstruction of the wavefront that reaches a single user antenna. As in the 1-pixel camera, an actuation device similar to the mirror matrix is required, to generate the random multiple-mode modulators required for performing compressed sensing. Programmable metasurfaces have been shown to efficiently fit this role @cite @cite @cite . RF imaging with metasurfaces has been successfully used for approximating the shape of planar met allic objects in space, by emitting waves that impinge upon these objects and then sensing and reconstructing the resulting wavefront @cite .
- In the present work, we extend the related studies by combining wave manipulation and wavefront sensing at the same time, using the capabilities of HSFs. Thus, the same HSF can sense the direction and attributes of an EM emission from one user device (smartphone, laptop, IoT device), and adaptively steer it toward an access point at the same time @cite @cite . This task is accomplished without adding field sensing hardware to the setup @cite .
- The discrete Kalman filter is a mathematical framework to estimate an unobservable state of a linear stochastic discrete time controlled process through noisy measurements @cite . In this section a very brief and intuitive overview of the Kalman filter to estimate a static one dimensional state will be given.
- Here @math , the measurement, can be an external source or sensor (voltage or altitude sensor), @math is the related measurement variance indicating the uncertainty of the estimate. The @math is the , which optimally combines the estimate and the measurement. A complete and detailed explanation of Kalman filters can be found in @cite @cite .
- Kalman filter-based heuristic ensemble (KFHE) @cite is a multi-class ensemble algorithm proposed by Pakrashi and Mac Namee, which takes a different approach by viewing the ideal hypothesis for a specific classification problem as the static state to be estimated in the @cite .
- Here, @math indicates the class assignment of the prediction and @math indicates the class assigned to the datapoint @math . There are two variants discussed in @cite where @math can be either the exponential function or the linear function in Eq. ). The use of exponential function is similar to AdaBoost, but the final combination in @math is done using the filter in the measurement update step using equation similar to Eq. )
- Deep learning methods generally make use of recurrent networks. One such example is the Connectionist Temporal Classification for speech recognition @cite @cite . More recently, attention-based models @cite such as Listen, Attend, and Spell @cite proposed a system using a sequence-to-sequence model with attention.
- Most similar to our work is Tacotron @cite . In this work, the authors move even closer to a fully differentiable system. The input to Tacotron is a sequence of character embeddings and the output is a linear-scale spectrogram. After applying Griffin-Lim phase reconstruction @cite , the waveform is generated.
- A slightly different, but closely related model is the game analyzed by @cite . Here each edge in a series-parallel network represents a producer that competes by selecting a markup that then determines a price function. The main difference to our work is that producers are identified by an edge and thus do not have the possibility to compete on other edges.
- Leader-follower models as the previous ones are also known as Stackelberg games @cite . Recently, lots of attention has been placed on Stackelberg pricing games. Starting with Labb ' e et al @cite , who considered the problem in which a leader sets prices on a subset of edges and the follower chooses a shortest path, several authors generalized to different combinatorial problems for the follower. For example, @cite studied the Stackelberg minimum spanning tree problem, and Bil o el al. @cite and later Cabello @cite investigated the Stackelberg shortest path tree problem. @cite and @cite considered the power of single-price strategies. B " o @cite extended the analysis of this simple algorithm beyond the combinatorial setting.
- Several approaches have attempted to leverage a pretext task as an alternative form of supervision in some applications where it is difficult to construct massive ground truth data. Doersch @cite proposed to train the deep network by predicting a relative position between two patches randomly extracted from unlabeled images. They utilized the resultant feature representation as a proxy task for object detection and visual data mining. Noroozi and Favaro @cite proposed to solve Jigsaw puzzles for representing object parts and their spatial arrangements, and then applied it to object classification and detection tasks. Pathak @cite proposed an unsupervised image inpainting approach that generates contents of an arbitrary image region conditioned on its surroundings. The learned encoder features are applied to object classification detection and semantic segmentation tasks. Larsson @cite investigated an image colorization as the proxy task in replacement of ImageNet pre-training. The features learned with these works have been successfully transferred to high-level tasks such as classification, detection and segmentation. In our work, we demonstrate that the network pre-trained for monocular depth prediction is a powerful proxy task for learning feature representations in scene understanding tasks such as semantic segmentation and road detection.
- To model the observation of objects in an image, @cite densely searches bounding boxes in the image, and defines the specific objectness score based on the edge map of this image.
- @cite aims to find the edge revealling the depth discontinuity between obstacle and background. It takes the edges between adjacent regions of over-segmented image as inputs, and classifies all the edges into two subsets: occlusion edges and trivial edges. Compared to other edge cues @cite @cite , the occlusion edge has stronger response to obstacle contour, especially for tiny obstacles. The reason is that the surface cue is additionally taken into account. Hence, the occlusion edge is more proper for obstacle discovery.
- A recent manifesto on cloud computing offers an of contemporary cloud capabilities, and future potential and challenges @cite . Our cloud resource characteristics are not as detailed, but focus on specific features of relevance to application scheduling. @cite offers a more specialized bibliometric survey on , along with the journals where the publications appear, country of origin of authors, and year of publication. A taxonomy of methods used to leverage as well as QoS metrics is also provided. While some of our resource characteristics, application characteristics and QoS topics overlap with this, we do not exclusively focus on elasticity but rather examine other dimensions of clouds such as pricing and resilience as well, in addition to application scheduling.
- Similarly, there have been several papers that conceptualize the idea of fog computing and Cloudlets @cite @cite @cite .
- Others examine specific applications that benefit from the fog layer, to complement edge and cloud computing, with smart cities and IoT being key drivers @cite @cite @cite . Some also prescribe different types of interactions between the edge, fog and cloud layers @cite . These are similar to our resource taxonomy, but fail to compare common and contrasting features across edge, fog and cloud, and how application models and schedulers leverage them.
- There are articles which discuss the use of edge, fog and cloud layers in a hierarchical architecture. @cite consider mobility to be a characteristic feature of both the edge and fog layers, with challenges to resource discovery, service scheduling, QoS guarantee and security. A medical emergency use case is used to illustrate the relative benefits of using a cloud-only application deployment design, with one that uses all three. Latency is seen as a QoS goal, but they omit concerns on monetary cost and energy usage. Similarly @cite highlight the security and privacy issues in including edge resources for storage and computation of IoT applications, besides the cloud. A fog layer is not explicitly considered, and resource pricing is mentioned in passing.
- Scheduling for different aspects cloud computing have been examined in the past. @cite present a taxonomy of : application, virtualization and infrastructure. Algorithms are classified based on the specific objectives that should be met at each layer. Scheduling at the virtualization layer has the goal of mapping VMs on to physical machines, which is of particular interest to service providers. This been addressed by other specialized surveys such as @cite @cite . The goal of scheduling at the infrastructure layer is to place the resources and services at different locations in various data centers. @cite drill-down into this layer and offer a taxonomy for inter-cloud architectures, with application brokering in these systems. At the application layer, the problem is to schedule the user's application on VMs, and three goals are discussed by @cite : user QoS, provider efficiency, and negotiation.
- Similarly, @cite also discuss -- service, task and VM -- for public and private clouds. We limit our focus to that are exceedingly popular and where issues of elasticity and costing offer clear challenges and opportunities to end users, and complement this with emerging edge and fog resource layers. Our work also emphasizes the , a subset of which is the task level considered by @cite , with the objective of meeting QoS requirements and or budget constraints for the application. We also offer greater detail on various aspects of the unit of scheduling, which is lacking in these surveys.
- Likewise, @cite limits itself to resource allocation in clouds in the presence of , with various ACO and GA based dynamic scheduling meta-heuristics to handle faults being explored. We go beyond system failure and also examine literature that handle failures due to pricing (out of bid) for pre-emptible and opportunistic resources.
- @cite presents several on clouds, including static and dynamic scheduling, to meet constraints such as budget, deadline and robustness.
- Similarly, @cite offers a brief summary of workflows and their objective criteria for scheduling on the cloud, and review literature on scheduling algorithms to achieve the same. Others have surveyed for scheduling workflows on the cloud @cite . These techniques include hill climbing, simulated annealing, tabu search, GA, PSO and ACO, and these have been used to generate a schedule for workflows on clouds. While these surveys consider multiple classes of scheduling algorithm and system characteristics that lie at the intersection of several dimensions we introduce, they do not offer a taxonomy of the dimensions themselves which is essential for a rigorous analysis of this space. Edge and fog resources which have gained prominence off-late are omitted as well.
- Some early research investigates platform and application models for edge and fog computing @cite . @cite propose a 3-level strictly hierarchical model where the computation is rooted in the cloud, resources are elastically acquired in the cloud and fog layers, and communication is possible between cloud and fog, or fog and edge. But their example applications do not use the cloud, and this degenerates to a client-server model between the edges and their fog parent. The role of virtualization in enabling cloud computing is discussed in @cite , and they see a similar role for the fog as well. They conceive of a VM encapsulating all dependencies for an edge application or user to be hosted on a Cloudlet within 1 hop of the edge, with this VM migrating to remain at 1-hop distance from the edge user.
- Tangentially related is a body of work that provides a QoS-aware block device interface to tenants in the cloud. @cite proposes a block storage system design called FAST to minimize interference among tenants; read operations are redirected when different types of workloads (i.e., random and sequential reads) are co-located. @cite proposes an IO classification architecture with a slightly-modified block interface in order to provide differentiated storage services. Blizzard @cite extends the simple block interface for high IO performance, mapping each virtual driver to multiple backing physical disks, and providing high-level software abstractions such as replication and failure recovery.
- With recent progresses in deep learning, learning-based approaches find their applications in autonomous driving, both in real world and in simulation. NVIDIA used a deep convolutional neural network to learn a lane following policy end-to-end from the font view image @cite @cite . Waymo also used imitation learning to learn a urban driving policy from a huge amount of human driver data @cite . Based on CARLA, an open-source simulator for autonomous driving research, @cite @cite applied deep imitation learning to learn a policy to navigate through a complex virtual urban environment.
- Researchers have also tried reinforcement learning approaches for autonomous driving. @cite used a Deep Q Network to learn to steer a autonomous car in simulation. The action space is discrete and only allows coarse steering angles. @cite developed a continuous control deep reinforcement learning algorithm which is able to learn a deep neural policy to drive the car on a simulated racing track. @cite proposed a deep RL framework for autonomous driving and applied it to the lane following task in simulation. @cite proposed a hierarchical deep RL approach which is able to solve some complex temporal delayed problems such as traffic light passing. Nonetheless, the above approaches were developed either for simple scenarios without complex road geometry and multi-agent interaction, or used manually designed feature representations or policy models.
- Bird-view representation for autonomous driving decision making is proposed recently to reduce the complexity of visual features @cite @cite @cite . They generally compress information of the map and objects to a rasterized image. Direct RGB images are very high dimensional, one can use variational auto-encoder (VAE) @cite @cite for dimension reduction, which learns a low dimensional latent representation of the image via an unsupervised style.
- Model-free deep reinforcement learning is a rich research area and is evolving rapidly. Related algorithms range from the Q learning based approaches such as DQN @cite @cite and double DQN @cite , the actor-critic approaches such as A3C @cite , DDPG @cite and TD3 @cite , the policy optimization approaches such as TRPO @cite and PPO @cite , and the maximum entropy approaches such as soft actor-critic @cite @cite .
- For short-run MCMC, contrastive divergence (CD) @cite is the most prominent framework for theoretical underpinning. The difference between CD and our study is that in our study, the short-run MCMC is initialized from noise, while CD initializes from observed images. CD has been generalized to persistent CD @cite , and has more recently been generalized to modified CD @cite and adversarial CD @cite @cite @cite . However, in all those CD-based framework, the goal is still to learn the EBM, whereas in our framework, we discard the learned EBM, and only keep the learned short-run MCMC.
- Our theoretical understanding of non-convergent MCMC is based on generalized moment matching estimator. It is related to moment matching GAN @cite , however, we do not learn a separate generator model adversarially.
- One of the earliest efforts in hate speech detection can be attributed to @cite who had presented a decision tree based text classifier for web pages with a 88.2 @cite used CNN based classifiers to classify hateful tweets as racist and sexist. @cite introduced a combination of CharCNN and WordCNN architectures for abusive text classification. @cite explored four CNN models trained on character n-grams, word vectors based on semantic information built using word2vec, randomly generated word vectors, and word vectors combined with character n-grams to develop a hate-speech text classification system. @cite used an ensemble of RNNs in order to identify hateful content in social media.
- Liu @cite proposed CSDL-SRC to reduce the high residual error and instability of SRC. The authors consider the weight of each sample feature when generating the dictionary. Assume that is the training sample matrix, where @math represents the dimensions of the sample features, and are the number of training samples and the class number of training samples, respectively. The class of training sample matrix is denoted as , where and is the class of (). Liu build a weight coefficient matrix for , where is the dictionary size of CSDL-SRC and is the class of (). The objective function of CSDL-SRC is as follows: where is the sparse codes of , the -norm regularization term is utilized to enforce the sparsity, is the regularization parameter to control the tradeoff between fitting goodness and sparseness. The denote the column vector of matrix .
- There has been a long history of using saccades in object detection to speed up inference. For example, a special case of saccades is a cascade that repeatedly selects a subset of regions for further processing, as exemplified by the Viola-Jones face detector @cite . The idea of saccades has taken diverse forms in various approaches, but can be roughly categorized by how each crop is processed, in particular, what kind of output is produced after processing each crop.
- Saccades in R-CNN @cite , Fast R-CNN @cite , and Faster R-CNN @cite take the form of crops representing potential objects. After processing, each crop is either rejected or converted to a single labeled box through classification and regression. Cascade R-CNN @cite extends Faster R-CNN by using a cascade of classifiers and regressors to iteratively reject or refine each proposal. The saccades in all these R-CNN variants are thus , in that there is a single type of processing of crops, and each crop produces at most a single object.
- The efficiency of ConvNets is important to many mobile and embedded applications. Much attention @cite @cite @cite @cite @cite @cite @cite has been given to the design of efficient network architectures.
- SqueezeNet @cite proposes a fire module to reduce the number of parameters in AlexNet @cite by 50x, while achieving similar performance. MobileNets @cite are a class of lightweight networks that use depth-wise separable convolutions @cite , and proposes strategies to achieve a good trade-off between accuracy and latency. PeleeNet @cite , in contrast, demonstrates the effectiveness of standard convolutions by introducing an efficient variant of DenseNet @cite consisting of two-way dense layers and a stem block.
- Other networks were designed specifically for real-time detection. YOLOv2 @cite incorporates ideas from NIN @cite to design a new variant of VGG @cite . YOLOv3 @cite further improves DarkNet-19 by making the network deeper and introducing skip connections. RFBNet @cite proposes a new module which mimics the receptive field of human vision systems to efficiently gather information across different scales.
- Feasibility studies which use platforms more socially acceptable, in a physical Human-Robot Interaction (pHRI) context, have inspired this work. Specifically, @cite underlies the key factors that influence a walking trainer robot for elderly individuals, by analyzing the influence of the pHRI with respect to the user's speed. Additionally, @cite presents a participatory design approach which enables a PR2 to guide blind people. More recently, @cite presents a walking-assistant robot that uses gait estimation which is tested with four elderly patients using a semi-humanoid robot.
- However, very few approaches develop physically assistive navigation by the usage of humanoid robots while exploiting its social component. Therefore, further studies need to be conducted considering the needs of chronically ill patients and elderly individuals, by proposing and validating a suitable robotic platform. In this way, investigating the proxemics preferences in this category will allow to provide an effective motivational behaviour, safe model and a system able to capture gait parameters @cite .
- There have been prior investigations on FSK-PNC detection. Specifically, @cite studied coherent detection and @cite @cite @cite @cite studied noncoherent detection. A coherent detector has access to the magnitude and the phase of the received signals, whereas a noncoherent detector only has access to the magnitude of the received signals. Since a coherent detector has all the signal information that a noncoherent detector has, but not vice versa, the coherent detector potentially has better performance. In this paper, we focus on coherent FSK-PNC detection.
- The authors of @cite put forth a coherent FSK-PNC detector for power-balanced channels, assuming that the channels are perfectly known at the relay. That is, the detector does not need to do channel estimation and only has to do detection. Although channel estimation can be performed using a preamble in the packet, a preamble would have occupied a large portion of a short packet, causing large overhead. To avoid the overhead, we consider short packets without preambles. In addition, we do not assume power-balanced channel and our detector can work with arbitrary channel distributions.
- For simple cases like flat terrains, optimization techniques are able to correctly solve, simultaneously, the motion of the main body of the robot and its footstep placements @cite @cite . Using more complex models and solvers, the problem can be reformulated to solve motion on other terrains @cite @cite . However, this algorithm is doomed to fall into local minima, e.g. ignoring intermediate steps on stairs or trying to jump over impassable obstacles. Such behaviors can be reduced by first relaxing the complementary constraint of contacts then slowly converging back to the initial problem @cite @cite @cite . Alternatively, one can decompose the non-smooth terrain into different convex and even patches, and rely on Mixed-Integer Programming to find the best patches for each footstep @cite @cite . However the computation time of those approaches make them difficult to use on a real robot. Overall, optimization techniques are not well suited for collisions and all of the presented approaches ignore this problem.
- Another common approach is to rely on Graph Search @cite @cite @cite . The space of possible footsteps is discrete and actions are selected using graph search algorithms, such as A*. However, such approaches quickly become too computationally expensive when solving for a large number of footsteps and or considering the movement of the main body.
- A final set of methods are based on machine learning. Approaches based on supervised learning @cite @cite take the foosteps generated by a planner, or from motion capture as an input, so the resulting plan will be efficient feasible only if the initial planner or the motions captured correspond to the capabilities of the robot. Reinforcement learning has shown more and more impressive results during the last few years @cite @cite but, as of yet, the results are limited to either flat ground or to behaviors that are unsuitable for real robot hardware on other terrains.
- In this paper, we apply the work of Steve @cite on our robotic platform, ANYmal @cite , and explain some of the adjustments that need to be done to successfully compute feasible contact plans.
- By combining point-to-point ICP and point-to-plane ICP, Segal al @cite introduce a plane-to-plane strategy called Generalized ICP (GICP). The covariance matrices of the local surfaces is used to match corresponding point clouds. Their results show better accuracy over original ICP. The Normal Iterative Closest Point (NICP) @cite takes into account the normal and curvature of each local surface point. Experiments show that NICP can offer better robustness and performance than the GICP.
- Grant al @cite derive a novel Hough-based voting scheme for finding planes in laser point clouds. A plane-based registration method @cite , that aligns detected planes, is adopted to compute the final transformation. For the non-uniformly sampled point clouds from laser sensors, Serafin al @cite present a fast method to extract robust feature points. They remove ground points by a flat-region-removal method, then extract key-points of lines and planes, and use them to estimate the transformation. They show comparative results against the NARF key-point detector @cite and highlight that the feature extraction algorithm is applicable to the Simultaneous Localization and Mapping (SLAM) problem @cite @cite . Similarly, Douillard al @cite remove the ground points by voxelization, cluster the remaining points into the segments, then match these segments through a modified ICP algorithm.
- To overcome the sparsity of lidar point clouds, Collar Line Segments (CLS) @cite groups the points into polar bins, and generates line segments within each bin. Then, ICP can be used to register the corresponding lines and estimate the transformation between two scans. Although it produces better results than GICP @cite , CLS is not real-time because the line segments computation is very slow.
- black Although there are few studies on optimizing the gaits for snake-like robots, studies on optimizing the locomotion gaits of other kinds of robots have been reported for the purpose of energy saving. Initially, researchers adapted techniques for multidimensional function optimization tasks to design efficient gaits, such as evolutionary algorithms @cite or policy gradient search algorithms @cite @cite . However, these algorithms are usually plagued by local optima, which makes the process slow, inefficient, and manually intensive. Afterwards, gait optimization methods based on prior knowledges are further investigated. first optimized the speed and smoothness of the gait with Gaussian process regression on a quadruped robot @cite . used Bayesian optimization approach to regulate those open-loop gait parameters for bipedal robots, which made the robot move faster and robuster @cite . Even though these optimized gaits outperform the hand-coded gaits, they are still very inefficient forms of locomotion compared with the natural movement achieved by animals.
- Some multi-frame denoising methods have also been proposed and can usually achieve better results than single-image denoising @cite . The fusion of multi-frame images can effectively improve SNR, hence enhancing the image quality. Among the traditional multi-frame denoising methods, V-BM4D finds the motion trajectory of the target block between the frames, regards the series of in-motion trajectories as the target volume, and then finds similar volumes to form a four-dimensional set. Finally, it performs filtering in the four-dimensional set, which can achieve effective video denoising @cite . There have been several attempts to deal with multi-frame image denoising through deep neural networks. Godard, et. al., uses a Recurrent Neural Network (RNN) @cite . The use of RNN can efficiently aggregate the information of the frames before and after, as well as increasing the effective depth of the network to enlarge the receptive field @cite . It is worth noting that @cite does not use skip connections. We also use RNN, but with skip connections to combine multi-frame image information for denoising and enhancement.
- In the case of extremely dark environments, Chen, et. al., proposed a new pipeline to replace the traditional one, which includes several procedures including balance, demosaicing, denoising, sharpening, color space conversion, gamma correction, and more. These processes are specifically tuned in the camera module to suit the hardware. However, because of these non-linear processes on the raw data, some information is lost. Starting from raw data can help to improve the image quality. Previous research has proven that using raw data instead of sRGB can effectively enhance the quality of denoising. Raw images are generally 10bit, 12bit or 14bit, and often contain more bits of information than 8-bit sRGB images @cite . Especially in the case of extremely dark environments, raw image can be used to obtain more low-brightness information. Therefore, we will use an end to end system starting from raw data and directly generating an output of a denoised and enhanced sRGB image. We handed over all the processes that were originally handled by the ISP to the neural network. On the one hand, the neural network can fully use the information of these raw images, and on the other hand, it will simplify the pipeline.
- Support vector machine (SVM) @cite is one of the most widely used classification methods with solid theoretical foundation and excellent practical applications. By minimizing a hinge loss with either an L-2 or L-1 penalty, it constructs a maximum-margin hyperplane between two classes in the instance space and extends to a nonlinear decision boundary in the feature space using kernels. Many variants have been proposed, including primal solver or dual solver, and extension to multiclass case @cite @cite .
- The rolling shutter camera was addressed in the Perspective-n-Point (PnP) problem to estimate the camera pose with given 2D projections corresponding to 3D point clouds. Ait-Aider al @cite proposed to estimate the pose and the speed of fast-moving objects in a single image with a given 2D-3D matching, and proposed nonlinear and linear models for non-planar and planar objects. Magerand al @cite extended this study by suggesting a polynomial uniform rolling shutter geometry model and solved the problem through the constrained global optimization. Albl al @cite proposed a new method to estimate the camera pose with 6 points based on the double linearized rolling shutter camera model.
- Besides, SfM and SLAM based on a monocular rolling shutter camera have been intensively studied taking into account the rolling shutter effects. Klein and Murray @cite used the constant velocity model in the SLAM framework to predict and correct the rolling shutter distortion occurring in the next frame. Hedborg al @cite @cite applied the rolling shutter camera projection model to the non-linear optimization step of SfM, bundle adjustment. Their key idea is to exploit temporal continuity of the camera motion on the video input to deal with the rolling shutter distortion. Saurer al @cite also considered the rolling shutter distortion in dense 3D reconstruction and they also proposed a minimal solution for absolute pose estimation problem of rolling shutter cameras @cite . Albl al @cite analyzed the degeneracy of the rolling shutter SfM and suggested how to avoid the degeneracy when shooting videos. Recently, Ito and Okatani @cite derived the degeneracy of rolling shutter SfM as the general expression through a self-calibration-based approach. Zhuang al @cite proposed a constant acceleration model for relative pose estimation and image rectification in two consecutive images.
- On the other hand, the methods to utilize an inertial measurement unit (IMU) to deal with the rolling shutter distortion in visual odometry (VO) and SLAM have been also studied. Jia and Evans @cite proposed a method to estimate the camera orientation using gyroscope measurements and to correct the rolling shutter distortion of the image. Guo al @cite applied a rolling shutter camera projection model to the visual-inertial odometry framework that uses IMUs and cameras to estimate egomotion. They estimated the readout time of the rolling shutter camera as well as the time delay between the IMU and the camera. Albl al @cite proposed a method to improve the speed and accuracy of the method in @cite using the gravity measurements obtained from the IMU. In addition, IMUs are also used to solve conventional relative or absolute pose estimation problems for global shutter cameras. It has been studied to estimate relative pose with the partially known orientation angle between two cameras @cite @cite , or with known vertical direction @cite @cite .
- Singing Synthesis and Conversion Classical singing synthesis methods are mostly concatenative (unit selection) methods @cite or HMM based @cite @cite . Blaaauw and Bonda have demonstrated very convincing singing synthesis using a WaveNet Decoder @cite . Their system receives, as input, both notes and lyrics and produces a stream of vocoder features. The method was extended @cite to adopt between singers, using the same type of note and lyrics supervision, in a data efficient manner, based on a few minutes of clean audio per target singer.
- In the field of singing voice conversion, i.e., transforming an audio of a song to a target voice, almost all literature methods have used parallel data @cite @cite @cite , i.e., different singers that are required to perform the same song. None of these existing methods provide code or benchmarks that can be used for a direct comparison of their results (even if supervised) to ours.
- Very recently, a method that does not require parallel data was presented, in which the acoustic features of the target singer are extracted from their speech (not from a song) @cite . Vocoder features are used for synthesizing the audio. The results are demonstrated on four source singers, one target voice, and as can be heard in their sample page https: sites.google.com site singingvoiceconversion2018 are still partly convincing.
- Backtranslation The technique of back-translation has emerged in Natural Language Processing, where it was presented as a technique for employing monolingual corpora in the supervised training of an automatic machine translation (AMT) system @cite . A sample @math in language @math , which does not have a matching translation in the target language @math , is automatically translated by the current AMT system to a sample @math in that language. One then considers the training pair ( @math , @math ) for translating from the language @math back to language @math .
- Deep neural networks have shown their success in a wide variety of applications. One of the key factors contributes to this success is the creation of powerful training techniques. The representative power of the network becomes stronger as the architecture gets deeper @cite . However, millions of parameters make deep neural networks easily over-fit. Regularization @cite @cite is an effective way to obtain a model that generalizes well. There exist many approaches to regularize the training of deep neural networks, like weight decay @cite , early stopping @cite , etc. Shakeout belongs to the family of regularized training techniques.
- Researchers have studied the possibility of automatically deriving and predicting users activity @cite @cite . @cite proposed Apriori algorithm to detect activity patterns in smart home environment to automate users activities. Similarly @cite used Bayesian network to learn and predict users activities which can be used to generate personalised activities. These works focused predicting and automating users activities which can be represented as workflow, however, it didn't tackle how to build it and which devices will be used. Therefore, our goal is to a) select all devices that fulfil a workflow requirements and b) Choosing devices from the selected list that maximise user preference. For example, a user may prefer to use a brand @math devices for temperature sensing while prefer brand @math for thermostat function. In association with selected devices, policies will be generated automatically to ensure least privilege access, and segregate devices used by different users to protect their privacy.
- The main objective of MUD is to introduce the least privilege access required for IoT devices. However, as MUD policy is generated prior to the deployment it cannot define all access endpoints with fine granularity, especially local ones @cite . In contrast to the MUD policy, we propose to use a user defined activity workflow to drive automatic policy establishment. @cite proposed Vigilia, which uses capability-based RMI access control to derive access control policy. However, it requires predefined capability-base access to specify the access between different type of devices prior to the depolyment. This requirement limits the interaction of a device to what is pre-defined in its driver. @cite developed a firewall that automatically generates profiles (i.e. policies) for IoT devices by monitoring their traffic during learning phase. Then enforce the learned policies during the depolyment. This approach work well, but it requires the devices to generate all possible traffic during the learning phase. However, we target the activity automation workflow so that policies can be automatically generated to ensure least privilege access to devices that are part of the workflow
- . A detailed review of tensor methods falls outside the scope of this section. Herein, we focus on methods which have been used to re-parametrize existing individual convolutional layers. This is done mainly to speed up computation or to reduce the number of parameters. The authors in @cite propose to decompose each of the 4D tensors representing the convolutional layers of a pretrained network into a sum of rank-- (1 ) tensors using CP decomposition. @cite propose a similar approach but use Tucker decomposition instead of CP. @cite also used CP decomposition, but optimize this using the tensor power method. The method of @cite proposed a method to share parameters within a ResNext-like block @cite by applying a Generalized Block Decomposition to a 4-th order tensor. As we show a straightforward extension of existing multi-domain adaptation methods (e.g. @cite ) using tensors results in an adaptation model with a large number of parameters. To improve this, we propose to model groups of identically structured blocks within a CNN with a single high-order tensor.
- Using the three field notation of @cite , the @math problem admits a pseudopolynomial time algorithm, see Lawler @cite , and a FPTAS due to a technique by Pruhs and Woeginger @cite . We are not aware of any simplifications when assuming @math . For preemption without migration, these ideas were further extended to admit a @math approximation @cite . A reduction from preemptive to non-preemptive due to Kalyanasundaram and Pruhs @cite yields a @math approximation. Recently, @cite analyzed the gap between schedules with a bounded number of @math preemptions and an unbounded number of preemptions. They show that the utilization ratio is at most @math , which is asymptotically optimal.
- Baruah and Haritsa @cite were among the first to present results for problems with slack when they addressed the single machine problem with preemption and commitment. For this problem, they gave an algorithm with a competitive ratio @math and a lower bound of @math . For the corresponding non-preemptive problem on a single machine, Goldwasser's algorithm @cite @cite guarantees a tight deterministic competitive ratio of @math . A similar competitive ratio is also given by @cite .
- For parallel identical machines, DasGupta and Palis @cite consider preemption without migration and suggest a simple greedy algorithm. They claim the same competitive factor as for the single machine problem with preemption. They also give a lower bound of @math . For the non-preemptive case, Kim and Chwa @cite apply Goldwasser's algorithm @cite @cite to parallel identical machines. Lee @cite considers a model without commitment and presents a non-preemptive algorithm that adds many newly submitted jobs to a queue and decides later whether to schedule or to reject them. He claims a competitive ratio of @math for small slack values. This competitive ratio is slightly worse than our competitive ratio for the same problem with commitment.
- The field of NLP has witnessed steady development of attention mechanisms in recent years @cite @cite @cite @cite @cite @cite . Starting from the introduction of an attention module in neural machine translation @cite , various attention factors and weight assignment functions based on these factors have been utilized. @cite , the inner product of vectors encoding query and key contents is recommended for computing attention weights, and absolute spatial positions are incorporated as an attention factor. @cite , the weight assignment additionally accounts for the inner product of spatial positions encoded in high-dimensional vectors. The landmark work of Transformer @cite set a new standard, and its latest variants use relative positions instead of absolute positions for better generalization ability @cite @cite . In this paper, we conduct the empirical study on the latest instantiation of Transformer attention @cite from this family of works.
- Aside from Transformer attention, there are variants of convolution, such as deformable convolution @cite @cite and dynamic convolution @cite , that also can be viewed as types of attention mechanisms which operate on a subset of the attention factors using different attention weight functions. They also are included in the study for examination.
- It is worth mentioning a dual form of spatial attention, called channel-wise feature attention @cite @cite @cite @cite . As different feature channels encode different semantic concepts, these works seek to capture the correlations among these concepts through activation deactivation of certain channels. Meanwhile, in the spatial domain, relationships among elements at different spatial positions are modeled, with the same attention weights on feature channels assigned to related spatial positions. The development of channel-wise feature attention has been focused on certain image recognition tasks, like semantic segmentation and image classification. In this paper, our empirical study specifically examines spatial attention mechanisms designed for broad application.
- There exists relatively little analysis of spatial attention mechanisms despite their prevalence in deep networks. This research has largely been conducted by visualizing or analyzing the learned attention weights of a whole attention module on only NLP tasks @cite @cite @cite @cite . Many works @cite @cite @cite suggest that attention weight assignment in encoder-decoder attention plays a role similar to word alignment in traditional approaches @cite @cite @cite @cite . The implicit underlying assumption in these works is that the input elements accorded high attention weights are responsible for the model outputs. However, recent research casts doubt on this assumption @cite , finding that attention weights do not correlate well with feature importance measures, and that counterfactual attention weight configurations do not yield corresponding changes in prediction.
- Safe Reinforcement Learning tries to ensure reasonable system performance and or respect safety constraints during the learning and or deployment processes @cite . Roughly, there are two ways of doing safe RL: some methods adapt the optimality criterion, while others adapt the exploration mechanism. Since classical approaches to exploration like @math -greedy or Boltzmann exploration do not guarantee safety @cite @cite , the research area of deals with the question -- how can we build agents that respect the safety constraints not only during normal operation, but also during the initial learning period? @cite .
- One model-based method proposed to learn the accuracy of the current policy and use another safe policy only in unsafe situations @cite . @cite proposed learning a risk-assessment model, called model, which predicts risky events to shape the learning process. @cite studied if human intervention could prevent catastrophic events; while the approach was successful in some Atari games, the authors argue it would not scale in more complex environments due to the amount of human labour needed.
- Approaches such as DAgger @cite or its extended version @cite formulate imitation learning as a supervised problem where the aim is to match the performance of the demonstrator. However, the performance of agents using these methods is upper-bounded by the demonstrator performance.
- @cite proposed a classification-based RL method using Monte-Carlo rollouts for each action to construct a training dataset to improve the policy iteratively. Recent works such as Expert Iteration @cite extend imitation learning to the RL setting where the demonstrator is also continuously improved during training. There has been a growing body of work on imitation learning where human or simulated demonstrators' data is used to speed up policy learning in RL @cite @cite @cite @cite @cite .
- @cite used demonstrator data by combining the supervised learning loss with the Q-learning loss within the DQN algorithm to pre-train and showed that their method achieves good results on Atari games by using a few minutes of game-play data. @cite employed human demonstrators to pre-train their neural network in a supervised learning fashion to improve feature learning so that the RL method with the pre-trained network can focus more on policy learning, which resulted in reducing training times for Atari games. @cite proposed a learning from demonstration approach where limited demonstrator data is used to impose constraints on the policy iteration phase and they theoretically prove bounds on the Bellman error.
- In some domains (e.g., robotics) the tasks can be too difficult or time consuming for humans to provide full demonstrations. Instead, humans can provide @cite @cite on alternative agent trajectories that RL can use to speed up learning. Along this direction, @cite proposed a method that constructs a reward function based on data containing human feedback with agent trajectories and showed that a small amount of non-expert human feedback suffices to learn complex agent behaviours.
- Contextual information has been considered in various computer science applications. Context-aware applications dynamically adapt to changes in the environment in which they are running: location, time, user profile, history. provide a thorough survey of context-aware models in @cite . Truong and Dustdar survey context-aware Web-Service systems in @cite . Context-awareness has become increasingly popular with the wide adoption of mobile devices. While the types of context these systems consider overlap with ours, the overall approach is different from ours, for instance a contextually-aware Information Retrieval system will use the current context (e.g., user location and time of day) to adjust search results @cite . In contrast, we consider context as information that can be queried and used to guide the search.
- Blanz and Vetter @cite introduced the 3D morphable model to represent textured 3D faces using linear combinations of a set of shape and texture bases, which is derived from collections of real 3D face scans. The model is later extended to include facial expressions by FaceWarehouse @cite . In this paper, we focus on recovering the underlying 3D shapes of human faces, hence we are only interested in regressing 3DMM parameters for shapes and expressions. We argue that more realistic textures for 3D meshes can be obtained with more advanced texture synthesis techniques @cite instead of the 3DMM texture representations.
- Conventional methods for single-view 3DMM fitting are mostly based on analysis-by-synthesis optimization @cite @cite @cite @cite @cite @cite , by constraining the data similarities like pixel colors, facial landmarks, edges, , between observed images and the synthetic images induced by 3DMM. The optimization is usually sensitive to initial conditions and parameters, and hence brittle in practice. This leads to the recent interests in regression-based approaches with deep neural networks.
- Zhu @cite proposed a cascaded CNN to regress and progressively refine 3DMM parameters, trained with supervision data generated by fitting 3DMM parameters using conventional approaches and then augmented by their proposed face profiling technique. Later, Tran al @cite presented that more discriminative results could be obtained with deeper networks and 3DMM pooling over face identities. However, both methods require supervision obtained through optimization-based 3DMM fitting techniques. Dou al @cite proposed to train the regression network using real 3D scans together with synthetic rendered face images with a 3D vertex distance loss. Richardson al @cite showed that a 3DMM regression network can be trained using only synthetic rendered face images and later Kim al @cite proposed a bootstrapping algorithm to adapt the synthetic training data distribution to match real data. Recently, Tewari al @cite and Genova al @cite demonstrated impressive results by training 3DMM regression networks using only unlabeled images with a self-supervised photometric loss and a face recognition loss, respectively.
- To model detailed facial geometries beyond the representation power of 3DMM, some recent studies proposed to supplement additional geometric representations such as displacement maps @cite @cite or parametric correctives @cite besides 3DMM representations. Some other work used volumetric representations @cite or non-regular meshes @cite instead of parametric representations. These types of representations are out of the scope of this paper.
- In the multi-view setting, a straightforward solution @cite for 3DMM-based reconstruction is to first perform traditional multi-view 3D reconstruction @cite and then fit a 3DMM using the reconstructed 3D model as constraints. However, the separated two steps are error-prone: the SfM MVS step cannot utilize the strong human facial prior from 3DMM and hence its results are usually rather noisy, which further leads to erroneous 3DMM fitting. Dou al @cite recently proposed to address the problem using deep convolutional neural networks (CNNs) together with recurrent neural networks (RNNs). They used RNNs to fuse identity-related features from CNNs to produce more discriminative reconstructions, but multi-view geometric constraints are not exploited in their approach. Notice that there are some other 3DMM-based methods in multi-image settings @cite , but in these work each input image is dealt individually, which is not the same as our multi-view setting.
- The problem of navigating self-driving car by utilizing the perception acquired from sensory data has been studied in the literature with and without using end-to-end approaches @cite . For example the works from @cite , @cite use multiple components for recognizing objects of safe-driving concerns, such as lanes, vehicles, traffic signs and pedestrians. The recognition results are then combined to give a reliable world representation, which are used with an Artificial Intelligence (AI) system to make decisions and control the car.
- Recent works focus on using end-to-end approaches @cite . The Autonomous Land Vehicle in a Neural Network (ALVINN) system was one of the earlier systems utilizing multilayer perceptron (MLP) @cite in 1989. Recently, CNNs were commonly used as in the DAVE-2 Project @cite . In @cite , the authors proposed an end-to-end trainable C-LSTM network that uses a LSTM network at the end of the CNN network. Similar approach was taken by the authors in @cite , who designed a 3D CNN model with residual connections and LSTM layers. Other researchers have implemented different variants of convolutional architecture for end-to-end models in @cite , @cite , @cite . Another widely used approach for controlling vehicle steering angle in autonomous systems is via sensor fusion where combining image data with other sensor data such as LiDAR, RADAR, GPS improves the accuracy in autonomous operation @cite , @cite . As an instance, in @cite , the authors designed a fusion network using both image features and LiDAR features based on VGGNet.
- These approaches learn to generate artificial examples to compensate the lack of training data. The Neural Statistician approach learns to produce statistics of a dataset, such as mean or variance, which are used to specify a Gaussian distribution for generating data @cite . Other methods introduce generative adversarial networks to learn sharper decision boundaries (MetaGAN) @cite or model the latent distribution of novel classes @cite . These meta-learners generate fake examples to assist few-shot learning tasks. However, these examples could be non-informative when the few training examples are not representative. Conversely, our method learns to aggregate useful information and suppress noisy information, which can be more stable.
- Dropout is a simple way to prevent neural networks from overfitting @cite . However, it is seldom applied to convolutional layers in CNNs, because the shared-filter architecture dramatically reduces the number of model parameters which reduce the model's capacity to overfit @cite . Still, the experimental results in @cite show performing dropout in convolutional layers can prevent overfitting and further improve the performance on image recognition tasks. Different from applying dropout on common machine learning tasks, we perform dropout in the meta-level to tackle meta-level overfitting problems, in which the dropped model is used for both training and testing examples during meta-training.
- Within the field of deep learning there has been an active effort to address the issue of class-imbalance. The most common approach is to weight the loss function with the inverse frequency of the labels occurrence. This method was proposed by @cite to address the class-imbalance in object detection CNN's where in many cases the majority of classifications are easy to detect background. This is achieved by re-shaping the cross-entropy by adding a modulating factor, ensuring negative frequent classes do not overwhelm the loss function. This was shown to improve the performance for single-class object detectors @cite , where class-imbalance is likely to be high. The ability to pass weights into cross-entropy loss function is now a standard feature for many leading deep learning software libraries. @cite proposed a method by which the softmax loss function is scaled by a scaling parameter determined as a function of the labels frequency. In essence, this is a reactive approach for dealing with scenarios where class im-balance is assumed. @cite propose using a generalised to take advantage of inter-class relationships and multi-scale information. The improved loss function favours semantically meaningful predictions, which can help balance mis-classification due to class im-balance.
- More recently, @cite have proposed the use of Generative Adverserial Networks (GAN) to generate data from the distribution of the data. By supplementing the data manifold with an approximation from the distribution, classification rates were shown to improve by 5
- In addition, this paper is related to the literature of mixtures of experts. Since mixtures of experts were introduced more than two decades ago @cite @cite , it has been applied to different types of experts including SVMs, Gaussian Processes and deep neural networks. Recently, @cite has proposed sparsely-gated mixture-of-experts layers, consisting of up to thousands of feed-forward sub-networks. A trainable gating network determines a sparse combination of these experts to use for each example. There are major differences between @cite and our work. 1) We do not use gating to infer the importance of experts; instead, we use linear weights to represent the importance of experts. 2) The experts in @cite are feed-forward networks with identical architectures, while our work considers different network architectures. 3) The gating in @cite is part of the model and is used at inference time to determine which combination of the experts are used. Our work follows the deep network architecture search literature and uses linear weights to guide the search for an efficient network architecture. At the end of the search, a single expert network is picked and the weights are no longer used in inference.
- A large number of real-world applications deal with non-Euclidean data. To overcome the shortcomings of CNNs, GCNs provide solutions for non-Euclidean data processing. These solutions have found an increasing interest in a variety of applications. In social networks @cite , graphs represent connections between individuals based on mutual interests relations. These connections are non-Euclidean and highly irregular. GCNs help better estimate edge strengths between the nodes of social network graphs, thus leading to more accurate connections between individuals. Graphs are also used to model chemical molecule structures @cite @cite . Understanding the bio-activities of these molecules can have substantial impact on drug discovery. Another popular use of graphs is in recommendation engines @cite @cite , where accurate modelling of user interactions leads to improved product recommendations. Graphs are also popular modes of representation in language processing @cite @cite , to represent complex relations between words, sentences, and larger text units.
- GCNs are a perfect candidate for point cloud processing. Point cloud data poses a representational challenge, given its unstructured nature. Several attempts in creating structure from 3D data exist by either representing the 3D data with multiple 2D views @cite @cite @cite @cite , or by voxelization @cite @cite @cite @cite . More recent work focuses on directly processing unordered point cloud representations @cite @cite @cite @cite @cite . The recent method by Wang al @cite applies GCNs to point clouds. In particular, they propose a dynamic edge convolution algorithm for semantic segmentation of point clouds. The algorithm dynamically computes node adjacency at each graph layer using the distance between point features. This work demonstrates the potential of GCNs for point cloud related applications and beats the state-of-the-art in the task of point cloud segmentation. Unlike most other works, does not rely on RNNs or complex point aggregation methods.
- A lot of the difficulties that face GCNs nowadays ( vanishing gradients, limited receptive field, ) were also present in the early days of CNNs @cite @cite . We bridge this gap and show that the majority of these drawbacks can be remedied by borrowing several orthogonal tricks from CNNs. Deep CNNs achieved a huge boost in performance with the introduction of ResNet @cite . By adding residual connections between inputs and outputs of layers, ResNet alleviates the vanishing gradient problem. DenseNet @cite takes this idea a step further and adds connections across layers as well. Dilated Convolutions @cite are a more recent approach that has lead to significant performance gains, specifically in image-to-image translation tasks such as semantic segmentation @cite , by increasing the receptive field without loss of resolution. We show how we can benefit from these concepts introduced for CNNs, mainly residual dense connections and dilated convolutions, to train very deep GCNs. We support our claim by extending the work of Wang al @cite to a much deeper GCN, and therefore significantly increasing its performance. Extensive experiments on the task of point cloud semantic segmentation validate that these ideas could be used in any general graph scenario.
- Datacenter networks have become a critical infrastructure and especially the popularity of online services @cite (e.g., web search, social networks, storage, financial services, multimedia, etc.) has led to a fast increase of datacenter traffic @cite @cite . Many applications (such as scatter-gather and batch computing applications) generate much datacenter traffic, and consequently, the traffic staying inside the datacenter is often much larger than the traffic entering or leaving the datacenter @cite . It is hence not surprising that the design of effective and efficient (also in terms of cost and cabling) datacenter networks has received much interest over the last years @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . The situation has recently been compared to the early 1980s, when many new interconnection network designs were proposed @cite , not for datacenters, but for parallel computers.
- The advent of technologies for reconfigurable (a.k.a. malleable @cite ) networks introduces an additional degree of freedom to the datacenter network design problem @cite @cite @cite @cite @cite @cite @cite @cite @cite . By relying on movable antennas @cite , mirrors @cite @cite , and disco-balls'' @cite , novel technologies in the context of optical circuit switches @cite @cite @cite @cite , 60 GHz wireless communication @cite @cite , free-space optics @cite @cite , provide unprecedented topological flexibilities, allowing to adapt the topology to traffic demands.
- Indeed, the physical topology is the next frontier in an ongoing effort to render communication networks more flexible and reconfigurable. Over the last years, reconfigurable technologies (typically software) already enabled various innovations in important domains such as traffic engineering @cite @cite , load-balancing @cite @cite , and switching @cite @cite .
- While the discussions on the benefits and limitations of such technologies are still ongoing @cite , the community has identified a number of benefits of more flexible networks. While full bisection bandwidth allows for a flexible placement and scale-out of applications across clusters @cite @cite @cite @cite , the cost of (and energy consumed in) traditional networks is high, and today many datacenters are over-subscribed (e.g., fat-trees or folded Clos networks use a subset of possible roots only). Some empirical evaluations show that for certain workloads, a demand-aware network can achieve a performance similar to a demand-oblivious full-bisection bandwidth network at 25-40 studies also confirm that communication patterns are often and of , which can be exploited in demand-aware networks: in @cite , it is shown that a high percentage of rack pairs does not exchange any traffic at all, while less than 1 80 general, most bytes are delivered by large flows @cite @cite @cite . Furthermore, the need for reconfigurability is motivated by empirical studies showing the difficulty of estimating traffic matrices and predicting the future @cite @cite @cite .
- We also note that the study of reconfigurable networks is not limited to datacenter networks. Interesting use cases also arise in the context of wide-area networks @cite @cite and, more traditionally, in the context of overlays @cite @cite @cite .
- In terms of formal guarantees, an upper bound on what can be achieved in terms of statically optimized demand-aware networks is due to @cite , who build upon initial insights in @cite @cite . We in this paper leverage the degree reduction technique of @cite , however, to derive a very different result. The fixed demand-aware network designs by @cite have recently also been extended to optimize for load, in addition to route lengths @cite .
- We also note that our approach of reconfiguring network topologies to reduce communication costs, is orthogonal to optimization approaches changing the traffic matrix itself (e.g., @cite ) or migrating communication endpoints on a fixed topology @cite .
- The role of public randomness has been given considerable attention in information theory literature starting from Wyner @cite who characterized the minimum rate of public randomness required for two processors to produce (approximatley) independent copies of random variables @math . Public randomness was used for encoding and decoding in arbitrary varying channels by Ahlswede @cite , and Csisz 'ar and Narayan @cite . Generation of public randomness between two users which should be hidden from an eavesdropper was studied in secret key agreement by Maurer @cite , and Ahlswede and Csisz 'ar @cite . Secret key agreement between multiple users was studied by Csisz 'ar and Narayan @cite , and the minimum communication rate required to generate secret key between two users was studied by Tyagi @cite .
- A related work deserving special attention is @cite , which studied the also on hypergraphs. Specifically, it showed that if the @math -uniform hypergraph is (a notion introduced in @cite ), then there is a strategy achieving the optimal communication rate @math and outputting each hyperedge . Our work differs from @cite in the following aspects. First, instead of outputting each hyperedge exactly once, we may output a subset of hyperedges or some hyperedges with different repetitions (cf. Section ). Consequently, our lower bound in Theorem is stronger. Second, the achievability scheme in @cite requires large blocklengths to achieve some small probability of error, while our achievability scheme is more combinatorial and achieves zero error. Third, their type @math condition is an information-theoretic condition, with an unclear relationship to combinatorial hypergraph theory. It is also unknown whether it can be verified in polynomial time that some subset of (possibly repeated) hyperedges in a given hypergraph forms a hypergraph of type @math . In contrast, our conditions in Theorems and are more combinatorial in nature, and the topological connectivity can be efficiently checked using matrix ranks. Hence, our work presents an alternative approach which sheds more lights on the combinatorial perspective.
- We also review some literature on the communication complexity. First introduced in @cite , the blackboard communication protocol serves as an elegant mathematical framework for the study of communication complexity. A series of research are devoted to the lower bounds in communication complexity, where the is the prominent tool for all the deterministic @cite @cite , nondeterministic @cite and randomized communication complexities @cite @cite @cite . We refer to @cite for a survey of these methods. Another closely-related problem is distributed inference under communication constraints @cite , where distributed simulation of private public randomness is useful for distributed learning and property testing @cite @cite . To establish lower bounds on the communication complexity in distributed inference, the copy-paste property of the blackboard communication model typically plays an important role @cite @cite . However, our technique to establish the lower bound is different, where only the sequential nature of the blackboard communication protocol is used in the proof of Theorem (cf. Lemma ), which may be of independent interest.
- There are a few works that involve realistic degradation modeling for single image SR. For instance, Timofte al introduced more degradation operators into the bicubic-downsampled LR images, including motion blur and Poisson noise @cite . Bulat al defined the LR face images with the low-quality assumptions (e.g., noise, blur, and compression artifacts) and trained a GAN @cite to learn the degradation process @cite . On the other hand, as a self-similarity based method, Michaeli and Irani adaptively estimated the degradation model relying on the inherent recurrence of the input image @cite . Shocher al further optimized an image-specific CNN with examples solely extracted from the input image @cite .
- Different from the above approaches, our proposed CameraSR models the R-V degradation from the perspective of camera lenses. The estimation of R-V degradation neither relies on the low-quality assumptions nor the inherent recurrence of LR images. Instead, it is characterized by the samples captured with realistic imaging systems. Such a degradation modeling is inspired by the prior work for realistic image denoising @cite , where a subject captured at a high ISO value is defined noisy and the same one captured at a low ISO value is defined clean. We extend this definition to the SR scenario, which addresses the key challenge of obtaining realistic LR-HR image pairs. Note that the focus of this paper is not the network design. For the comparison purpose, we adopt VDSR @cite and SRGAN @cite as two representative embodiments to demonstrate the effectiveness and generalizability of CameraSR, which can be replaced with any CNN-based methods.
- One successful approach to moving object detection attempts to decompose a matrix @math representing an image sequence into a low-rank matrix @math and sparse matrix @math , so as to recover the background and the foreground @cite . The problem is initially solved by the robust principal component analysis (RPCA). Since the foreground objects are described by the sparse matrix @math , we can categorize existing methods by the types of constraints on @math . The first group of these methods use @math -norm to constrain @math @cite @cite @cite and solve the following convex optimization. where @math denotes the nuclear norm of matrix @math , and @math is the @math -norm of @math .
- The second group of methods used the additional prior knowledge on the spatial continuity of objects to constrain sparse matrix @math and improve the detection accuracy @cite @cite . Using spatial continuity (e.g., @math -norm in @cite ) to enforce the block-sparsity of the foreground, results become more stable than conventional RPCA in the presence of illumination changes. However, it remains a challenge to handle moving shadows or significant changes in illumination. Furthermore, the position of an object in a time-lapse image sequence is discontinuous from one image to another so that the continuity assumption is invalid as a way to separate moving objects and changes in illumination.
- The third group of methods also imposed the connectivity constraint on @math @cite @cite @cite @cite @cite @cite using other formulations than the second group. For example, Liu et al @cite attempted to use a structured sparsity norm @cite and a motion saliency map, to improve the accuracy of moving object segmentation under sudden illumination changes. However, this method still cannot handle shadows and severe illumination changes, especially in time-lapse sequences with independent object locations among the images in the sequence that change similarly to shadow and illumination. In general, although the low-rank framework is well-known to be robust against moderate illumination changes in frame-rate sequences, the existing methods are still not able to handle discontinuous change in illumination and shadow, especially in time-lapse sequences.
- To effectively separate discontinuous changes due to moving objects and those due to illumination, Shakeri et al @cite proposed a method called LISD. This method relies on an illumination regularization term combined with the standard low-rank framework to explicitly separate the sparse outliers into sparse foreground objects and illumination changes. Although this regularization term can significantly improve the performance of object detection under significant illumination changes, LISD assumes a) the invariant representation @cite of all images in a sequence are modeled by only one invariant direction and b) all illumination variations are removed in the invariant representation of images, which are not strictly valid in practice.
- We are motivated by the issue of robustness in the parsing community. This field previously focused on improving parsing accuracy on Penn Treebank @cite . However, robustness was largely improved by evaluation using multiple corpora including Ontonotes @cite and Google Web Treebank @cite . A situation similar to this might also occur in GEC. In other words, evaluation in GEC has relied heavily on the CoNLL-2014 benchmark, which implies that the field is overdeveloping on this dataset.
- Other corpora are used for evaluation, such as KJ @cite and JFLEG @cite @cite @cite @cite @cite . However, these corpora still depend on one or at most two corpora.
- One-stage instance segmentation methods generate position sensitive maps that are assembled into final masks with position-sensitive pooling @cite @cite or combine semantic segmentation logits and direction prediction logits @cite . Though conceptually faster than two-stage methods, they still require repooling or other non-trivial computations (e.g., mask voting). This severely limits their speed, placing them far from real-time. In contrast, our assembly step is much more lightweight (only a linear combination) and can be implemented as one GPU-accelerated matrix-matrix multiplication, making our approach very fast.
- Finally, some methods first perform semantic segmentation followed by boundary detection @cite , pixel clustering @cite @cite , or learn an embedding to form instance masks @cite @cite @cite @cite . Again, these methods have multiple stages and or involve expensive clustering procedures, which limits their viability for real-time applications.
- While real-time object detection @cite @cite @cite @cite , and semantic segmentation @cite @cite @cite @cite @cite methods exist, few works have focused on real-time instance segmentation. Straight to Shapes @cite can perform instance segmentation with learned encodings of shapes at 30 fps, but its accuracy is far from that of modern baselines. Box2Pix @cite relies on an extremely light-weight backbone detector (GoogLeNet v1 @cite and SSD @cite ) combined with a hand-engineered algorithm to obtain 10.9 fps on Cityscapes @cite and 35 fps on KITTI @cite . However, they report no results on the more challenging and semantically-rich COCO dataset, which has 80 classes compared to the 8 of KITTI and Cityscapes. Furthermore, they observe a large drop in relative performance going from a semantically simple dataset (KITTI) to a more complex one (Cityscapes), so an even more difficult dataset (COCO) would pose a challenge. In fact, Mask R-CNN @cite remains one of the fastest instance segmentation methods on semantically challenging datasets (13.5 fps on @math px images; see Table ).
- Learning prototypes (aka vocabulary or codebook) has been extensively explored in computer vision. Classical representations include textons @cite and visual words @cite , with advances made via sparsity and locality priors @cite @cite @cite . Others have designed prototypes for object detection @cite @cite @cite . Though related, these works use prototypes to represent features, whereas we use them to assemble masks for instance segmentation. Moreover, we learn prototypes that are specific to each image, rather than global prototypes shared across the entire dataset.
- In recent work, Seq2Seq models are widely used for sequence transduction tasks such as machine translation @cite @cite , conversation agents @cite , summarization @cite , etc. Initial Seq2Seq models consisted of a Recurrent Neural Network (RNN) that encodes the source sentence @math to a hidden vector of a fixed dimension, followed by another RNN that uses this hidden representation to generate the target sentence @math . The two RNNs are then trained jointly to maximize the conditional probability of the target sentence given the source sentence, i.e. @math . Other works have since extended this framework to include attention mechanisms @cite and transformer networks @cite . For a detailed description of Seq2Seq models, please see @cite . was the first major application of Seq2Seq models to text simplification, applying a standard encoder-decoder approach with attention and beam search. extended this framework to incorporate memory augmentation, which simultaneously performs lexical and syntactic simplification, allowing them to outperform standard Seq2Seq models.
- There are two main Seq2Seq models we will compare to in this work, along with the statistical model from . proposed DRESS (Deep REinforcement Sentence Simplification), a Seq2Seq model that uses a reinforcement learning framework at training time to reward the model for producing sentences that score high on fluency, adequacy, and simplicity. This work showed state-of-the-art results on human evaluation. However, the sentences generated by this model are in general longer than the reference simplifications. proposed DMASS (Deep Memory Augmented Sentence Simplification), a multi-layer, multi-head attention transformer architecture which also integrates simplification rules. This work has been shown to get state-of-the-art results in an automatic evaluation, training on the WikiLarge dataset introduced by . , however, does not perform a human evaluation, and restricting evaluation to automatic metrics is generally insufficient for comparing simplification models. Our model, in comparison, is able to generate shorter and simpler sentences according to Flesch-Kincaid grade level @cite and human judgments, and provide a comprehensive analysis using human evaluation and a qualitative error analysis.
- Shapley value was proposed in a classic paper in game theory @cite and has been widely influential in economics @cite . It has been applied to analyze and model diverse problems including voting, resource allocation and bargaining @cite @cite . To the best of our knowledge, Shapley value has not been used to quantify data value in a machine learning context like ours. Shapley value has been recently proposed as a feature importance score for the purpose of interpreting black-box predictive models @cite @cite @cite @cite @cite @cite . Their goal is to quantify, for a given prediction, which features are the most influential for the model output. Our goal is very different in that we aim to quantify the value of individual data points (not features). There is also a literature in estimating Shapley value using Monte Carlo methods, network approximations, as well as analytically solving Shapley value in specialized settings @cite @cite @cite @cite @cite
- We had to re-implement red-black trees due to the difference of stored contents. Above formalizations are intended to represent sets, and maintain the ordering invariant. Our trees represent vectors, and maintain both that the contents (as concatenation of the leaves) are unchanged, and that meta-data in inner nodes is correct (see Sect. ). Still, we found many hints in related work. For example, in Sect. about insertion, the balancing functions use Okasaki's well-known purely functional balance algorithm @cite , and we formulate our invariants and propositions similarly to above formalizations.
- There are now many proofs of programs that use , but we could not find much discussion trying to synthesize the new techniques put at work. used for teaching @cite @cite , observing benefits for clarity and maintainability, but also giving examples of custom tactics needed to prove programs. @cite have shown how, in some cases, one can avoid relying on ad hoc tactics through an advanced technique involving overloading of lemmas. The techniques we describe in Sect. , while more rudimentary, are simple and efficient, yet we have not seen them described elsewhere.
- Eigen and Fergus @cite addressed the tasks of depth prediction, surface normal estimation, and semantic labeling using a multiscale convolutional network architecture that progressively refines predictions from a sequence of scales. Uhrig al @cite presented a method that leverages a FCN network to predict semantic labels, depth, and an instance-based encoding using each pixel's direction towards its corresponding instance center and consequently applying low-level computer vision techniques. Kokkinos @cite went one step further from the previous approaches, and introduced a CNN, namely UberNet, that jointly handles low-, mid-, and high-level vision tasks in a unified architecture. His universal network tackles boundary detection, normal estimation, saliency estimation, semantic segmentation, human part segmentation, semantic boundary detection, region proposal generation, and object detection. Despite obtaining competitive performance while jointly addressing many different tasks, all these approaches suffer from poor inference times making them unsuitable for real-time autonomous driving applications with high frame-rate demands.
- Recently, Teichmann al @cite argued that improving the computational times is more important than improving performance, especially for the case of self-driving vehicles. They presented an approach to joint classification, detection, and semantic segmentation via a unified architecture where the encoder is shared amongst the three tasks, marginally reaching a computational time of 10 fps on the KITTI dataset. Our approach also focuses on further improving the computational times but addresses different tasks, in particular semantic scene segmentation, instance segmentation, and monocular depth estimation, and achieves a computational time of 21 fps on the Cityscapes dataset. To our knowledge this is the first system to estimate depth, semantic and instance segmentation at these frame-rates.
- Several works target embedded object detection or tracking on similar embedded hardware. @cite compare YOLOV2 and tiny YOLOV2 among a few other classic object detection techniques both in terms of speed and accuracy on a Jetson TX2 platform for a UAV warning system. The same hardware is used by Blanco- @cite . They propose an embedded real-time multi-object tracker based on a foreground-background detector and the GOTURN tracker. The predecessors of the TX2, the TX1 and TK1, are evaluated in @cite where they combine a tiny version of Faster-RCNN with a KCF tracker for single-object tracking on a drone. As stated before, we go further in optimizing our network for the target platform. In addition, we make efforts to compress our trained network for faster inference.
- Other groups have explored grounding single objects referred to by natural-language expressions @cite @cite @cite @cite and grounding all nouns mentioned in a natural language phrase @cite @cite @cite @cite .
- Visual grounding is different from, though related to, tasks such as visual relationship detection @cite , in which the task is not to ground a particular phrase in an image, but to detect all known relationships. The VRD dataset we described above is commonly used in visual relationship detection tasks, and to our knowledge there are no prior studies of bias and other problems in this dataset.
- It should be noted that visual grounding also differs from automated caption generation @cite and automated scene graph generation @cite , which input an image and output a natural language phrase or a scene graph, respectively.
- To exploit datasets with general objects for tracking, numerous Siamese based trackers @cite @cite @cite @cite @cite cast tracking as a matching problem and learn a similarity measurement network. Tracking is carried out by comparing the features of the initial target template and search regions in the current frame. A number of trackers @cite @cite @cite have since been developed by introducing attention mechanisms for better matching between templates and search regions. Although these Siamese frameworks are pre-trained on large video datasets, the pair-wise training sample only tells whether the two samples belong to the same target or not without category information. That is, the Siamese trackers do not fully exploit semantic and objectness information pertaining to specific target objects. In this work, we select the most discriminative and scale-sensitive convolutional filters from a pre-trained CNN to generate target-aware deep features. The proposed features enhance the discriminative representation strength of the targets regarding semantics and objectness, which facilitate the Siamese tracking framework to perform well against the state-of-the-art methods in terms of robustness and accuracy.
- Several gradient-based models @cite @cite are developed to determine the importance of each channel of CNN features in describing a specific object class. The GCAM model @cite generates a class-active map by computing a weighted sum along the feature channels based on the observation that the gradient at each input pixel indicates the corresponding importance belonging to given class labeling. The weight of a feature channel is computed by globally average pooling of all the gradients in this channel. Unlike these gradient-based models using classification losses, we specifically design a regression loss and a ranking loss for the tracking task to identify which convolutional filters are active to describe targets and sensitive to scale changes.
- Related SBIR approaches can be roughly categorized into two types: methods based on hand-crafted features and those based on deep-learned features. In the first category, @cite proposed the gradient field HOG descriptor, which is an adaptive form of HOG descriptor suitable for sketches. Histogram of Edge Local Orientations (HELO) @cite utilized soft computation of local orientations and took spatial information into account for SBIR. @cite proposed Learned KeyShapes (LKS) based on mid-level pattern detection. In the second category, the convolutional neural network was first applied for sketch recognition in Sketch-a-Net @cite . @cite used siamese network to pull features closer for corresponding sketch-image pairs and push them away if they belonged to different categories. Furthermore, @cite exploited triplet loss to take full advantage of the relationships between similar and dissimilar samples for fine-grained instance-level SBIR. Deep Sketch Hashing (DSH) @cite proposed a semi-heterogeneous deep architecture with the help of auxiliary sketch-tokens. However, existing methods are not specifically elaborated for ZS-SBIR, meaning that the visual knowledge transfer from seen to unseen classes under a zero-shot setting is neglected.
- Due to the difficulty associated with collecting and annotating samples for training supervised methods, zero-shot learning has attracted ever-increasing attention among the research community. Existing zero-shot approaches can be divided into two categories: embedding-based and generative-based methods. In the first category, Attribute Label Embedding (ALE) @cite measured the bilinear compatibility between image and label embedding, while Embarrassingly Simple ZSL (ESZSL) @cite and Semantic AutoEncoder (SAE) @cite explicitly regularized the projection between the image embedding space and the class embedding space. Furthermore, Latent Embeddings Model (LATEM) @cite and Cross Modal Transfer (CMT) @cite utilized a non-linear component to exploit more complex correspondence between two embedding spaces. In terms of the generative-based methods, @cite proposed a conditional Generative Moment Matching Network (GMMN) to generate features of unseen classes. @cite proposed Semantics-Preserving Adversarial Embedding Network (SP-AEN) to preserve semantic information during image feature synthesizing. Although our work adopts conditional variational autoencoders with semantic preservation similar to @cite , we further elaborate a novel coarse-to-fine conditional decoder with multi-layer feature fusion and discriminability-preserving schemes to address ZS-SBIR.
- Unsupervised Learning on Free-Form Videos: Though there are a lot methods for unsupervised optical flow learning @cite @cite on videos (either synthetic @cite @cite or structured @cite @cite ), there is very few work about unsupervised learning on free-form videos: @cite uses an offline tracker to provide signal to guide feature learning; @cite @cite @cite learn to verify whether frames come with the correct order, and transfer the feature to action classification; @cite learns for region segmentation on image by considering the moving pattern of rigid objects; @cite learns for video colorization and shows that the learned features capture object or parts which are useful for object tracking; @cite learns correspondence at patch level on videos with reconstruction between frames.
- Filter Flow @cite is a powerful framework which models a wide range of low-level vision problems as estimating a spatially varying linear filter. This includes tasks such as optical flow @cite @cite @cite , deconvolution @cite @cite @cite , non-rigid morphing @cite , stereo @cite @cite defocus @cite , affine alignment @cite , blur removal @cite , etc. However, as it requires an optimization-based solver, it is very computationally expensive, requiring several hours to compute filters for a pair of medium-size images @cite @cite . Kong and Fowlkes propose Predictive Filter Flow, which learns to predict per-pixel filters with a CNN conditioned on a single input image to solve various low-level image reconstruction tasks @cite . There are other methods embracing the idea of predicting per-pixel filters, e.g., @cite and @cite do so for solving burst denoising and video frame interpolation, respectively.
- To date, there are six popular ME databases established for the researchers especially from computer vision field for algorithm development and analysis. The databases included USF-HD @cite , SMIC @cite @cite , CASME @cite , CASME II @cite , CAS(ME) @math @cite and SAMM @cite . The brief information of these six databases are shown in Table . Among the six databases, CASME II has the most abundant samples that contains 247 videos and it is commonly served as the baseline database for algorithm evaluation. It is observed that the database are having small data size. This is because some emotions are more likely to trigger, resulting in uneven distribution of samples. On the other hand, the ethnic and geographical distribution of the participants in each database are different, hindering the development of feature extractors methods and it is also difficult for data generalization. 5pt
- As discussed earlier, 3D understanding is an extensively researched topic. Earlier approaches ( @cite @cite @cite @cite @cite ) employ hand-crafted representations and achieve satisfactory results in the presence of rich and detailed 3D information. Some of the techniques ( @cite @cite @cite @cite @cite ) represent 3D point cloud data using voxel occupancy grid representation, followed by the use of 3D convolutions to compute the 3D bounding boxes. Due to the high computational and memory cost, several approaches based on BEV representation were developed ( @cite @cite @cite ). The BEV-based methods assume that point cloud data is sparse in one dimension, which is usually not the case in many scenarios. Different from these approaches, image-based methods ( @cite @cite @cite @cite @cite @cite @cite ) were developed to infer 3D bounding boxes from 2D images. However, they usually suffer from low accuracy in terms of depth localization. Recently, VoxelNet @cite proposed an end-to-end learning architecture that consumes point cloud data in its raw format.
- Multimodal fusion by combining LiDAR and RGB data has been less explored as compared to single modality-based approaches. Recently, Chen al @cite proposed a multi-view 3D object detection network (MV3D), which takes multimodal data as input and produces 3D bounding boxes by incorporating region-based feature fusion. Although this method demonstrated encouraging results by using multimodal data, it has the following disadvantages: (i) the method converts point clouds into BEV representation, which loses detailed 3D shape information, and (ii) the fusion is performed at a much later stage as compared to the proposed fusion techniques ( , after the 3D proposal generation stage), which limits the ability of the neural network to capture the interaction between the two modalities at earlier stages and hence, the integration is not necessarily seamless. Similar to @cite , Ku al @cite proposed multimodal fusion by incorporating region-based features. They achieved better performance than @cite especially in the small object category by designing a more advanced RPN that employs high-resolution feature maps. This method also uses hand-crafted BEV representation and performs late fusion.
- In a different approach, Qi al proposed Frustum PointNets @cite for 3D detection using LiDAR and RGB data. First, they use a 2D object detector on RGB data to generate 2D proposals, which are then converted to frustum proposals in the 3D space, followed by a point-wise instance segmentation using the PointNet architecture @cite . This method is an image-first approach and hence lacks the capability of utilizing both modalities simultaneously. Most recently, Liang al @cite proposed to aggregate the discrete BEV space with image features by projecting the LiDAR points to image space. This approach interpolates each BEV pixel location with RGB features based on K nearest neighbor search, which may not satisfy real-time requirements as the density and coverage of LiDAR point clouds increase. In contrast to existing approaches that either use a complicated pipeline to process different modalities or perform late-fusion, our simple yet effective fusion strategies can learn interaction between modalities at early stages.
- Kang al @cite propose to first decompose the rain image into high- low-frequency layers and remove rain streaks in the high frequency layer via dictionary learning. Kim al @cite propose to use non-local mean filters to filter out rain streaks. Luo al @cite propose a sparse coding based method to separate rain streaks from the background. Li al @cite propose to use Gaussian mixture models to model rain streaks and background separately for rain removal. Chang al @cite propose to first affine transform the rain image into a space where rain streaks have vertical appearances and then utilize the low-rank property to remove rain streaks. Zhu al @cite exploit rain streak directions to first determine the rain-dominant regions, which are used to guide the process of separating rain streaks from background details based on rain-dominant patch statistics.
- @cite @cite , deep learning is applied to single image deraining and achieves a significant performance boost. They model rain streaks as residuals" between the input output of the networks in an end-to-end manner. Yang al @cite propose to decompose the rain layer into a series of sub-layers representing rain streaks of different directions and shapes, and jointly detect and remove rain streaks using a recurrent network. @cite , Zhang al propose to remove rain streaks and recover the background via the Conditional GAN. Recently, Zhang and Patel @cite propose to classify rain density to guide the rain removal step. Li al @cite propose a recurrent network with a squeeze-and-excitation block @cite to remove rain streaks in multiple stages. However, the performances of CNN-based derainers on real rain images are largely limited by being trained only on synthetic datasets. These derainers also lack the ability to attend to rain spatial distributions. In this paper, we propose to leverage real training data as well as a spatial attentive mechanism to address the single image deraining problem.
- Multi-image rain removal. Unlike single-image deraining, rich temporal information can be derived from a sequence of images to provide additional constraints for rain removal. Pioneering works @cite @cite propose to apply photometric properties to detect rain streaks and estimate the corresponding background intensities by averaging the irradiance of temporal or spatial neighboring pixels. Subsequently, more intrinsic properties of rain streaks, such as chromatic property, are explored by @cite @cite @cite . Recent works @cite @cite @cite @cite @cite @cite @cite @cite @cite focus on removing the rain streaks from the background with moving objects. Chen al @cite further propose a spatial-temporal content alignment algorithm to handle fast camera motion and dynamic scene contents, and a CNN to reconstruct high frequency background details.
- However, these methods cannot be applied for our purpose of generating high-quality rain-free images. This is because if their assumptions ( , low-rank @cite @cite @cite ) are violated, over- under-deraining can happen to the entire sequence and further bury the true background radiance, , the clean background pixels may not exist in this sequence. Hence, in this paper, we propose to use the original sequence of rain images to generate a clean image, and rely on human judgements on the qualities of generated rain-free images.
- Generating the ground truth from real noisy images. One typical strategy @cite @cite to obtain a noise noise-free image pair is to photograph the scene with a high ISO value and a short exposure time for the noise image, and a low ISO value and a long exposure time for the noise-free image. However, this strategy cannot be used here to capture rain-free images. As rain drops fall at a high speed, increasing the exposure time will enlarge the rain streaks, not removing them. Another approach to obtain a ground truth noise-free image is multi-frame fusion @cite @cite @cite , which performs weighted averaging of a pre-aligned sequence of images taken from a static scene with a fixed camera setting. However, as rain streaks have brighter appearances and larger shapes than random noise, this approach is not able to accurately remove rain from the rain pixels. In contrast, we propose to refine the rain pixels based on the observation that the intensity values of the pixels covered by rain fluctuate above their true background intensities.
- The mentioned visual-aided odometry and localization algorithms are highly specific for the rail application and do not provide the desired level of accuracy. Also, imposing such constraints to the problem might reduce the level of redundancy. not directly suitable for continuous motion estimation as they only provide information near switches @cite or require manual data association @cite . Furthermore, by not considering specific constraints, the visual odometry can later be fused with this information to get even more reliable and accurate pose estimation, to detect when a method is failing or to detect changes in the expected environment @cite . The goal of this paper is to study the performance of generic visual-aided ego-motion estimation for the rail application.
- In automotive applications, many of the challenges such as high velocities and constraint motion are similar to the rail domain. There, odometry is often solved by using wheel encoders @cite @cite as they do not suffer from high slip as in rail applications. Furthermore, stereo vision @cite and monocular vo with learned depth @cite have also been used successfully for ego motion estimation. A multitude of state-of-the - art stereo-visual odometry frameworks are tested for automotive applications in the visual odometry part of the KITTI Vision Benchmark Suite . One popular and well performing open-source pipeline is . Unfortunately, the KITTI dataset does not include synchronized imu measurements and therefore does not allow in-depth insights into the benefits inertial data could provide. Finally, the scale of the motion can also be retrieved by exploiting non-holonomic constraints @cite which, however, relies on frequent turns.
- Many anomaly detection techniques used in time series data have been well developed, such as Long Short Term Memory networks in @cite Recurrent neural networks in @cite , Convolution neural networks in @cite , Autoencoders in @cite . Recently, proposed a novel GAN-based Anomaly Detection (GANAD) method combining GAN with LSTM-RNN to detect anomalies on multivariate time series in @cite . first put forward a data augmentation technique focused on improving performance in unsupervised anomaly detection based on GAN @cite .
- As can be seen from above, more recent attention in the literature has been focused on the provision of adversarial training, especially on GAN. GAN, viewed as an unsupervised machine learning algorithm, since initially introduced by in 2014, has achieved outstanding application effects in the field of image recognition. Based on GAN, there has been emerged various kinds of adversarial algorithm. For further details, we refer the interested reader to a website which gives a very comprehensive summary of GAN and its variants [website: https: github.com hindupuravinash the-gan-zoo]. Last year, based on GAN, a generic anomaly detection architecture called GANomaly put forward by in @cite shows superiority and efficacy compared with previous state-of-the-art approaches over several benchmark image datasets, which gives us an inspiration for fault diagnosis in industrial area. To explain our approach thoroughly in next part, we will briefly introduce GANomaly.
- More radical approaches tackling this challenge involve (partially or completely) bypassing the kernel in the data-plane by either i) offloading packet processing to specialized hardware, such as GPU based processing @cite @cite @cite @cite or NetFPGAs @cite , or by ii) shifting packet processing to user-land stacks @cite @cite @cite @cite . The latter represents an active line of research that achieved drastic performance increases and lower CPU footprints by avoiding kernel based packet processing overheads @cite . These advances have proved to be useful for accelerating software switches @cite , HTTP @cite @cite , and DNS servers @cite . However, while bypassing the kernel largely optimizes the achievable packet processing performance, it usually comes at the cost of abandoning a well-maintained and kernel network stack that offers central administration, although @cite reuse the kernel network processing. Likewise, new OS designs propose to generally remove the kernel from the data-plane @cite @cite .
- With impressive learning and characterization capabilities, auto-encoder neural networks have achieved great success in various areas, especially in the scenario of unsupervised learning @cite @cite @cite @cite @cite , such as natural language processing @cite , image processing @cite , object detection @cite , biometric recognition @cite , and data analysis @cite . As state-of-the-art unsupervised techniques, auto-encoder and auto-encoder based neural networks also make outstanding contributions in the field of remote sensing. In this subsection, we briefly introduce the auto-encoder network.
- For the structure shown in Fig. , the hidden layers of the basic automatic coding network have three different structures: a compressed structure, a sparse structure, and an equivalent-dimensional structure. When the number of input layer neurons is greater than the number of hidden layer neurons, it is called a compressed structure @cite . Conversely, when the number of input layer neurons is smaller than the number of hidden layer neurons, it is called a sparse structure. If the input layer and the hidden layer have the same number of neurons, it is named the equivalent-dimensional structure.
- As one key component of unsupervised learning, the autoencoder (AE) is a widely used and promising method aims to learn a latent feature representation of data @cite . It is flexible to implementation and can be stacked easily to form a deep structure, which endows a model the ability to perform complex information extraction and representation @cite . The AE is with an encoder-decoder structure. Raw data example @math is first projected to a latent feature space by the encoder, which is more compressed, sparser, or any specific nature as we need: where @math is the weight of encoder, @math is a bias and @math is a nonlinear activation function used to achieve nonlinearity so that the network can better model complex problem. Next, the latent feature representation @math is projected back to the original feature space by the decoder, in which a reconstructed data example @math is obtained: where @math is the weight of decoder, @math is a bias and @math is a nonlinear activation function. The loss of @math and @math is calculated as: The AE learns to minimze Eq. (3) and forces @math to retain the most powerful expression of data.
- Knowledge Base Population: There is a long line of prior work on learning to extract relational information from text using minimal supervision. Early work on semantic bootstrapping @cite @cite @cite @cite @cite @cite , applied an iterative procedure to extract lexical patterns and relation instances. These systems tend to suffer from the problem of semantic drift, which motivated work on distant supervision @cite @cite @cite @cite , that explicitly minimizes standard loss functions, against observed facts in a knowledge base. The TAC KBP Knowledge Base Population task was a prominent shared evaluation of relation extraction systems . Recent work has explored a variety of new neural network architectures for relation extraction , experimenting with alternative sentence representations in our framework is an interesting direction for future work. Recent work has also shown improved performance by incorporating supervised training data on the sentence level , in contrast our approach does not make use of any sentence-level labels during learning and therefore relies on less human supervision. Finally, prior work has explored a variety of methods to address the issue of noise introduced during distant supervision @cite @cite @cite .
- Structured Learning with Neural Representations: Prior work has investigated the combination of structured learning with learned representations for a number of NLP tasks, including parsing @cite @cite @cite , named entity recognition @cite @cite @cite and stance detection . We are not aware of any previous work that has explored this direction on the task of minimally supervised relation extraction; we believe structured learning is particularly crucial when learning from minimal supervision to help address the issues of missing data and overlapping relations.
- From @math it is possible to obtain a prediction @math for a instance @math . When constructing @math one expects that the obtained values of @math are as close as possible to the real measured target values @math . Regarding algorithmic solutions, accordingly to @cite MTR problems have been solved using two main strategies for the solutions: global methods and local methods. Global methods correspond to solutions that use a single model to predict all target variables at once. Those methods implicitly model the inter-target dependencies, and also offer more compact and less computationally costly solutions for MTR tasks, being more viable to online scenarios. The local methods, on the other hand, employ traditional STR solutions and often manipulate or modify the input space to insert the inter-target dependency information within the problem modelling. In this sense, local methods use multiple ST regressors to solve a MTR task, frequently more than one regressor for each target variable , which in turn make them more costly solutions than the global ones. The simplest local solution for MTR tasks, as previously mentioned, consists in creating a ST regressor for each target and simply ignoring the underlying inter-target dependencies.
- One of the first efforts to tackle regression problems on a online setting was made by @cite . The authors proposed an online and incremental method to build regression trees and dealing with concept drift. Their proposal, called FIMT-DD (Fast Incremental Model Tree with Drift Detection), employs the Hoeffding's bound theorem to decide whether a split decision must be made very much alike the VFDT (Very Fast Decision Tree) . The authors proposed using perceptrons without activation function at the tree's leaves to provide the responses. Being one of first works to tackle regression problems on data streams, the authors mostly evaluated their approach against traditional batch regression algorithms, but their research pioneered the research on Hoeffding (Trees and Decision Rules) algorithms for STR and MTR. The FIMT-DD was only designed to deal with numerical attributes, limiting its application to nominal data, unless some data transformation technique, e.g., one hot encoding, would be applied.
- Deviating from the tree-based solutions, @cite proposes the Adaptive Model Rules (AMRules) algorithm for online STR tasks, which is later on expanded by @cite . The authors propose using decision rules for STR problems. Similarly to the FIMT-DD algorithm, perceptron models are employed to generate the responses. AMRules uses a built-in mechanism for dealing with concept drift based on the Page-Hinkley (PH) test , which simply drops outdated decision rules. In addition, the decision rule algorithm has also a routine to detect anomalous samples, e.g., noisy data, which are not employed to update the decision models.
- @cite also expanded the AMRules framework for dealing with MTR tasks. The authors also employed the idea that the created partitions must reduce the variance in the output space. But different from the previous solution, AMRules does not lie in the global local categorization. Instead, the MTR version of AMRules is capable of specializing in subsets of targets by performing as follows: when executing a rule expansion test, if the variance in the target space is reduced only for some targets, a new decision rule encompassing the targets benefited by the split is created; a complementary rule without the expansion is also created for the remaining targets. Therefore, AMRules creates decision rules which can encompass all the targets, some of them, or even a single target, hence being a hybrid of a local and global method. AMRules was also adapted to deal with multi-label classification tasks .
- Following the trend of using tree-based methods for streaming scenarios, proposed an extension for the FIMT-MT algorithm called iSOUP-Tree (incremental Structured Output Prediction Tree). Their proposal builds upon the research of @cite by enabling the support to categorical features and employing an adaptive prediction model at the leaves. Instead of only using perceptrons at the leaves, iSOUP-Tree also maintains a mean predictor for each target and selects the best current model by monitoring a faded error metric for each of them. The authors adapted the iSOUP-Tree algorithm for multiple settings, including ensembles (Bagging and Random Forest) and Option Trees , and employed all the variations of the MTR method to tackle multi-label classification tasks .
- Despite the efforts made to build methods tailored to split the decision space considering all the targets, neither of the presented solutions effectively take advantage of the inter-target dependencies at the moment of generating predictions. In all of the presented cases, individual models are created for each target, simply ignoring how their values relate to each other. Inspired by the work of @cite , we propose employing the strategy of Stacked Single-target (SST) at the leaves' models for further improving the prediction performance of the MTR decision tree models. Also, considering the mutable characteristics of streaming tasks, our proposal, called Stacked Single-target Hoeffding Tree (SST-HT) can be setup to dynamically selecting for each target when to use its SST model, the standard perceptron, or the most straightforward mean predictor.
- In the past, researchers have developed and applied various neural architectures for NLP, including convolutional neural networks @cite @cite , recurrent neural networks @cite @cite @cite , and recursive neural networks @cite @cite . These generic architectures can be applied to tasks like sentence classification @cite @cite and sentence matching @cite @cite , but the model is trained only on data of a particular task.
- Recently, introduce Embeddings from Language Models (ELMo), an approach for learning high-quality, deep contextualized representations using bidirectional language models. With ELMo, they achieve large improvements on six different NLP tasks. propose Bidirectional Encoder Representations from Transformers (BERT), a new language representation model that obtains state-of-the-art results on eleven natural language processing tasks. Trained with massive corpora for language modeling, BERT has strong syntactic ability @cite and captures generic language features. A typical downstream use of BERT is to fine-tune it for the NLP task at hand. This improves training efficiency, but for inference efficiency, these models are still considerably slower than traditional neural networks.
- Model compression A prominent line of work is devoted to compressing large neural networks to accelerate inference. Early pioneering works include , who propose a local error-based method for pruning unimportant weights. Recently, propose a simple compression pipeline, achieving 40 times reduction in model size without hurting accuracy. Unfortunately, these techniques induce irregular weight sparsity, which precludes highly optimized computation routines. Thus, others explore pruning entire filters @cite @cite , with some even targeting device-centric metrics, such as floating-point operations @cite and latency @cite . Still other studies examine quantizing neural networks @cite ; in the extreme, propose binarized networks with both binary weights and binary activations.
- We can also explore the specific techniques that already exist for segmentation and feature extraction in field-based datasets. One popular set of techniques involve region growing. @cite used an interactive region growing method coupled with various morphological operations in order to extract multi-scale features from volume data. Additionally, Pra @cite utilized a uncertainty based region growing technique to minimize extraction errors that may falsely mask structures of interest or include unwanted background fluctuations. Furthermore, Ram 'i @cite took the GrabCut algorithm @cite , an image segmentation technique that treats pixels as a flow network, and extended it to 3D volumes.
- An alternate method by @cite used the underlying topology to construct hierarchical merger tree of contours formed at varying data values. This is then used as a highly compact representation of features within the data. Another hierarchical approach was done by @cite who used an intensity gradient histogram to continually subdivide a volume into a hierarchal representation of spatial structure that users can traverse and explore. When it comes to time-varying data and feature tracking, @cite used an approach that constructs isosurfaces in higher dimensions (4D and 5D) to better compute and represent the temporal evolution of 3D volume features. Similarly, @cite constructed a 4D Reeb graph to track volume surfaces features over time. Lastly, @cite used temporal compositing and time-varying transfer functions to create a 4D multi-volume raycaster which can depict the evolution of volume features over time.
- In terms of more visual based analyses, @cite devised a method of placing and selecting streamlines based on both the properties of a flow and the user viewing angle. These two selection criteria helped to improve the readability of normally cluttered streamline visualizations. In addition, @cite developed an interactive interface to enable user-driven clustering of large trajectory datasets based on visual trends and @cite used self-organizing maps to enable users to supervise and control aspects of trajectory clustering.
- Since then, @cite have gone on to extend the SLIC algorithm to operate on 3D volumes and use supervoxels'' in conjunction with uncertainty-based refinement to extract volume features from large-scale datasets. Such a technique has a great deal of potential, especially if it can be applied in 4D and extended to operate on both field and point-based data types. As a result, we use the ideas originally established in the SLIC algorithm as a starting point in developing our multifaceted 4D feature segmentation and extraction scheme.
- Domain adaptation, a special scenario of transfer learning @cite , bridges domains of different distributions to mitigate the burden of annotating target data for machine learning @cite @cite @cite @cite , computer vision @cite @cite @cite and natural language processing @cite . The main technical difficulty of domain adaptation is to formally reduce the distribution discrepancy across different domains. Deep networks can learn representations that suppress explanatory factors of variations behind data @cite and manifest invariant factors across different populations. These invariant factors enable knowledge transfer across relevant domains @cite . Deep networks have been extensively explored for domain adaptation @cite @cite , yielding significant performance gains against shallow domain adaptation methods.
- While deep representations can disentangle complex data distributions, recent advances show that they can only reduce, but not remove, the cross-domain discrepancy @cite . Thus deep learning alone cannot bound the generalization risk for the target task @cite @cite . Recent works bridge deep learning and domain adaptation @cite @cite @cite @cite @cite . They extend deep networks to domain adaptation by adding adaptation layers through which high-order statistics of distributions are explicitly matched @cite @cite @cite , or by adding a domain discriminator to distinguish features of the source and target domains, while the features are learned adversarially to deceive the discriminator in a minimax game @cite @cite .
- While the standard domain adaptation advances rapidly, it still needs the vanilla assumption that the source and target domains share the same label space. This assumption does not hold in partial domain adaptation (PDA), which transfers models from many-class domains to few-class domains. There are three valuable efforts towards the PDA problem. Selective Adversarial Network (SAN) @cite adopts multiple adversarial networks with a weighting mechanism to select out source examples in the outlier classes. Partial Adversarial Domain Adaptation @cite improves SAN by employing only one adversarial network and further adding the class-level weight to the source classifier. Importance Weighted Adversarial Nets (IWAN) @cite uses the Sigmoid output of an auxiliary domain classifier (not involved in domain-adversarial training) to derive the probability of a source example belonging to the target domain, which is used to weigh source examples in the domain-adversarial network. These pioneering approaches achieve dramatical performance gains over standard methods in partial domain adaptation tasks.
- These efforts mitigate negative transfer caused by outlier source classes and promote positive transfer among shared classes. However, as outlier classes are only selected out for the domain discriminators, the source classifier is still trained with all classes @cite , whose performance for shared classes may be distracted by outlier classes. Further, the domain discriminator of IWAN @cite for obtaining the importance weights distinguishes the source and target domains only based on the feature representations, without exploiting the discriminative information in the source domain. This will result in non-discriminative importance weights to distinguish shared classes from outlier classes. This paper proposes an Example Transfer Network (ETN) that down-weights the irrelevant examples of outlier classes further on the source classifier and adopts a discriminative domain discriminator to quantify the example transferability.
- On par with domain adaptation, research has been dedicated to open set recognition, with the goal to reject outliers while correctly recognizing inliers during testing. Open Set SVM @cite trains a probabilistic SVM and rejects unknown samples by a threshold. Open Set Neural Network @cite generalizes deep neural networks to open set recognition by introducing an OpenMax layer, which estimates the probability of an input from an unknown class and rejects the unknown point by a threshold. Open Set Domain Adaptation (OSDA) @cite @cite tackles the setting when the training and testing data are from different distributions and label spaces. OSDA methods often assume which classes are shared by the source and target domains are known at training. Unlike OSDA, in our scenario, target classes are entirely unknown at training. It is interesting to extend our work to the open set scenario under the generic assumption that all target classes are unknown.
- Non-asymptotic convergence rate Langevin dynamics based algorithms for approximate sampling log-concave distributions are intensively studied in recent years. For example, overdamped Langevin dynamics are discussed in @cite , @cite , @cite , @cite , @cite and others. Recently, @cite treats the case of non-i.i.d. data streams with a certain mixing property. Underdamped Langevin dynamics are examined in @cite , @cite , @cite , etc. Further analysis on HMC are discussed on @cite , @cite . Subsampling methods are applied to speed up HMC for large datasets, see @cite , @cite .
- The use of momentum to accelerate optimization methods are discussed intensively in literature, for example @cite . In particular, performance of SGHMC is experimentally proved better than SGLD in many applications, see @cite , @cite . An important advantage of the underdamped SDE is that convergence to its stationary distribution is faster than that of the overdamped SDE in the @math -Wasserstein distance, as shown in @cite .
- Finding an approximate minimizer is similar to sampling distributions concentrate around the true minimizer. This well known connection give rise to the study of simulated annealing algorithms, see @cite , @cite , @cite , @cite , @cite , @cite , @cite . Recently, there are many studies further investigate this connection by means of non asymptotic convergence of Langevin based algorithms and in stochastic non-convex optimization and large-scale data analysis, @cite , @cite .
- Relaxing convexity is a more challenging issue. In @cite , the problem of sampling from a target distribution @math where @math is L-smooth everywhere and @math -strongly convex outside a ball of finite radius. They provide upper bounds for the number of steps to be within a given precision level @math of the 1-Wasserstein distance between the HMC algorithm and the equilibrium distribution.
- A previous characterization of morphisms with polynomial growth is due to @cite , who introduce the notion of the rank'' of letters under a morphism. They show that a string @math has rank @math under a morphism @math iff @math is the minimal degree of a polynomial @math such that for every @math , @math . We make use of this result in proving the equivalence between morphic words of polynomial growth and zigzag words.
- The class of multilinear words appears in @cite as the infinite words determined by one-way stack automata, and also in @cite (as the reducts of the prime'' stream @math ). In @cite , prediction of periodic words and multilinear words is studied in an automata-theoretic setting.
- Another line of work has focused on combinatorial settings related to coded trace reconstruction @cite @cite @cite @cite . The reported works study the number of traces required for exact reconstruction when each trace is subjected to a bounded number of adversarial edit errors, and the string may be assumed to belong to a code from some general class. We note that Levenshtein @cite also studies a version of probabilistic trace reconstruction where the string is sent through a memoryless channel, but the deletion channel is not included in this family of channels. Other closely related combinatorial reconstruction problems consist in recovering a string from a subset of its substrings @cite @cite @cite .
- Coded trace reconstruction is also related to the multi-use deletion channel. In fact, the decoding problem for this channel can be interpreted as coded trace reconstruction with a fixed number of traces. Some results are known about the capacity of this channel for small deletion probability @cite , and about maximum likelihood decoding @cite .
- Finally, although we focus on portable DNA-based storage systems with nanopore sequencers @cite @cite , there has also been significant activity on practical aspects of other types of DNA-based storage, e.g., @cite @cite @cite . General overviews of the field can be found in @cite @cite .
- The camera pose problem has been studied for more than a century, and there is a large body of literature on the absolute pose estimation problem @cite . Here, we first review the PnP algorithm without mismatches. When the observations include no outliers, the PnP problem has a closed solution ( @math ). To reduce the sensitivity to noise and consider a larger point set, the Efficient PnP (EPnP) @cite , Optimal PnP (OPnP) @cite , and Unified PnP (UPnP) @cite methods have been developed to produce accurate results with a linear complexity. These algorithms are applied in many related areas and can be regarded as state-of-the-art outlier-free PnP techniques.
- Another topic that is closely related to the absolute pose estimation problem is point set registration @cite @cite . The only difference is that in the point set registration, there is no existing point correspondence. Similarly, the search for globally optimal solutions is a hot topic in the field of point set registration, and the branch-and-bound algorithm has also been broadly applied in recent related studies. One of the most successful algorithms for this purpose may be the algorithm proposed in @cite and its subsequent versions @cite @cite @cite . These works are all based on rotation search theory, and for @math optimization in particular, a more systematic scheme called the nested branch-and-bound is applied. Moreover, the decoupling methods presented in @cite improve efficiency, which inspires us to decouple the rotation and translation subproblems by means of pairwise constraints.
- The recommendation algorithms can be roughly divided into three categories: traditional recommendation algorithms, deep learning based and reinforcement learning based recommendation algorithms. Firstly, traditional recommendation algorithms consists of collaborative filtering @cite , content-based filtering @cite ,and hybrid methods @cite . Secondly, deep learning based recommendation algorithms have become the current mainstream recommendation methods. Deep learning methods can help to learn item embedding from sequences, image or graph information @cite . It can also extract users' potential tastes @cite , or improve the traditional methods directly @cite .
- Deep hierarchical reinforcement learning is dedicated to expanding and combining existing reinforcement learning methods to solve more complex and difficult problems @cite @cite . There is no doubt that the recommendation problem is such a problem. Recently, a goal-based hierarchical reinforcement learning framework @cite @cite has emerged, with high-level and low-level communicating through goals. However, as far as we know, there is no existing hierarchical reinforcement learning method for recommendation system.
- Similar to sentence cropping, define transformation rules to simplify sentences (e.g., I was not given a chance to eat - I ate) and shows that enrichening training set with simplified sentences improves the results of semantic role labeling. One of the first studies in text augmentation @cite , replaces a randomly chosen word with its randomly chosen synonym extracted from a thesaurus. They report improved test scores when a large neural model is trained with the augmented set. induce grammar from semantic parsing training data and generate new data points by sampling to feed a sequence to sequence RNN model. chooses low-frequency words instead of a random word, and generate synthetic sentence pairs that contain those rare words.
- For detection methods, an improved support vector machine (SVM) algorithm is @cite proposed as the general approach, More recently, then a boost method called XGBoost @cite is applied to improve both the efficiency and accuracy of detection. Inspired by the trend towards deep learning, the energy-based deep learning model @cite showed up, and GAN-based model @cite also dabbled in the scope of anomaly detection.
- Besides the CNN model used in this work, we also investigated using graph convolutional network (GCN) @cite to train the client's behavior in the web traffic. GCN is different with CNN by accepting structured graphs instead of images as input. However, it is not suitable for our scenario as it can only perform node classification inside one graph instead of classifying multiple graphs.
- As mentioned, there has been prior work on generating planner independent certificates or proofs for establishing the unsolvability of a given planning problem. Another related direction, has been the effort to generate excuses" for unsolvability of a planning problem @cite @cite . They do not provide an intuitive explanation as to why a problem is unsolvable, but rather identifies initial state values (or some domain model conditions) whose update could make the problem solvable.
- Part of our explanations also try to reveal to the user information about the current task that was previously unknown to them. Thus our methods could also be understood as an example of explanation as model-reconciliation @cite . Since our methods use abstractions, our approach doesn't make too many demands on the inferential capabilities of the user and hence can be applied to much larger and more complex domains.
- Another closely related direction has been the work done on explaining unsynthesizability of hybrid controllers for a given set of high-level task specifications @cite . The work tries to identify the subformulas of the given specification that lead to the unsynthesizability. This particular approach is specific to the planning framework detailed in @cite and the objective of the work parallels the goals of work like @cite @cite .
- Since the pioneering work by Horn and Schunck @cite , variational methods have dominated optical flow estimation. Brox al address illumination changes by combining the brightness and gradient constancy assumptions @cite . Brox al integrate rich descriptors into a variational formulation @cite . In DeepFlow @cite , Weinzaepfel al propose to correlate multi-scale patches and incorporate this as the matching term in a functional. In PatchMatch Filter @cite , Lu al establish dense correspondence using the superpixel-based PatchMatch @cite . Revaud al propose a method EpicFlow that uses externally matched flows as the initialization and then performs interpolation @cite . Zimmer al design the complementary regularization that exploits directional information from the constraints imposed in data term @cite . Our network that infers optical flow and performs flow regularization is inspired by the use of data fidelity and regularization in variational methods.
- Black al propose to represent complex image motion as a linear combination of the learned basis vectors @cite . Roth al formulates the prior probability of flow field as Field-of-Experts model @cite that captures higher order spatial statistics @cite . Sun al study the probabilistic model of brightness inconstancy in a high-order random field framework @cite . Nir al represent image motion using the over-parameterization model @cite . Rosenbaum al model the local statistics of optical flow using Gaussian mixtures @cite . Given a set of sparse matches, Wulff al propose to regress them to a dense flow field using a set of basis flow fields (PCA-Flow) @cite . It can be shown that the parameterized model @cite @cite @cite can be efficiently implemented in a CNN.
- An alternative approach for establishing point correspondence is to match image patches. Zagoruyko al first introduce to CNN-feature matching @cite . G "uney al find feature representation and formulate optical flow estimation in MRF @cite . Bailer al @cite use multi-scale features and then perform feature matching as Flow Fields @cite . Although pixel-wise matching can establish accurate point correspondence, the computational demand is too high for practical use (it takes several seconds even a GPU is used). As a tradeoff, Dosovitskiy al @cite and Ilg al @cite perform feature matching only at a reduced spatial resolution. We reduce the computational burden of feature matching by using a short-ranged matching of warped CNN features and a sub-pixel refinement at every pyramid level. We further reduce the computation cost by constructing a sparse cost volume at high-resolution pyramid levels.
- We are inspired by the feature transformation used in Spatial Transformer @cite . Our network uses the proposed f-warp layer to displace each channel We can also use f-warp layer to displace each channel when multiple flow fields are supplied. The usage, however, is beyond the scope of this work. of the given vector-valued feature according to the provided flow field. Unlike Spatial Transformer, f-warp layer is not fully constrained and is a relaxed version of it as the flow field is not parameterized. While transformation in FlowNet2 and SPyNet is limited to images, LiteFlowNet is a more generic warping network that warps high-level CNN features.
- @cite developed the covariate-adjusted precision matrix estimation or CAPME procedure taking a two-stage approach and using a multivariate extension of the Dantzig selector of . Let @math , @math , @math and @math . The estimate of @math in CAPME solves the optimization problem
- Bayesian approaches seek to implement regularization through the choice of prior, with the ultimate goal being probabilistic uncertainty quantification using the full posterior. @cite put spike-and-slab lasso priors on the elements of @math . That is, @math is drawn from either a spike' Laplace distribution with a sharp peak around zero, or a slab' Laplace distribution that is relatively flatter. A binary variable indicates whether a coefficient is drawn from the spike or the slab distribution. Such an element-wise prior on @math is
- where @math and @math are the parameters for the spike and slab Laplace distributions, and the binary indicator @math follows a priori a Bernoulli distribution with parameter @math , with a beta hyperprior distribution on @math with parameters @math and @math . Similarly, spike-and-slab lasso priors are put on elements @math in @math as well. An Expectation Conditional Maximization (ECM) algorithm is derived for this model to obtain the posterior mode. The hyper-parameters @math for @math , and the corresponding four hyper-parameters for @math , need to be specified in order to apply the ECM algorithm. In @cite , the Laplace distribution hyper-parameters are chosen by the trajectories of individual parameter estimates given a path of hyper-parameters, and the beta hyper-parameters are set at predefined levels. The method does not provide samples from the full posterior.
- In their comprehensive survey @cite , classify distance learning methods into three groups, unsupervised distance learning, supervised global distance learning, and supervised local distance learning. The idea of unsupervised methods is to learn a lower-dimensional embedding that preserves the pairwise distances between data points. Besides classical techniques like PCA and its non-linear generalisations (KPCA @cite , LLE @cite , etc.), unsupervised distance learning can also be implemented with neural networks, e.g., auto-encoders @cite .
- A Siamese network is a neural network that has multiple inputs of the same size, which are processed by identical branches with shared weights before combining them to generate the desired output. first introduced the notion of Siamese networks, in the context of signature verification @cite . They proposed to learn a pairwise distance measure between feature vectors (in their case derived from time-series of @math -coordinates on a tablet) that represent two different objects (in their case signatures).
- After the advent of modern convolutional networks, the same idea was applied to raw images, e.g., @cite @cite @cite @cite @cite . Siamese convolutional branches independently transform two (or more) images @math and @math into high-level representations that are then merged and transformed further into a learned measure @math of similarity. Note, @math is normally not a metric in the formal sense. @cite were perhaps the first to use Siamese networks for one-shot learning, using the learned image-to-image similarity in conjunction with an exemplar for each of the target classes to perform nearest-neighbour classification. Similar ideas are also elaborated in @cite @cite . The approach naturally covers also few-shot learning, by using consensus voting over the similarities to @math exemplars per class. However, the number @math must remain small, otherwise the method quickly becomes inefficient, because the similarity computation for each individual exemplar amounts to a complete forward pass of the network (e.g., with our architecture based on VGG16, @math seconds on an Nvidia Titan Xp GPU).
- The detection of GAN images is a new area in image forensics and there are very few papers in this area @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . Related fields also include detection of computer generated (CG) images @cite @cite @cite @cite . The most relevant work is a recent paper @cite on detecting GAN based image-to-image translation generated using cycleGAN @cite . Here the authors compare various existing methods to identify cycleGAN images from normal ones. The top results they obtained using a combination of residual features @cite @cite and deep learning @cite . Similar to @cite , the authors in @cite compute the residuals of high pass filtered images and then extract co-occurrence matrices on these residuals, which are then concatenated to form a feature vector that can distinguish real from fake GAN images. In contrast to these approaches, our approach does not need any image residuals to be computed. Rather, our method directly computes co-occurrence matrices on the three color channels which are then passed through a deep convolutional neural network (CNN) to learn a model that can detect fake GAN generated images.
- Structured semantic segmentations of the type we propose have been studied by several authors. Sohn al @cite proposed a CVAE to learn the distribution of object segmentation labels using Gaussian latent variables. Due to the learned distribution, the resulting object segmentation was more robust to noisy and partially observed data compared to discriminative CNN models. Pix2Pix from Isola al @cite used a conditional Generative Adversarial Network (GAN) to achieve image to image translation tasks in which the conditional distribution of semantic labels is implicitly learned. However, when used for semantic prediction from colour images, the GAN training process induces hallucinated objects. In addition, the distributions learned by GANs are not directly accessible and optimisable in the form we need for multi-view fusion.
- The communication models have been widely studied in the RL community, e.g., MTDP-COM @cite and DEC-POMDP-COM @cite . However, traditional methods usually either predefine the communication message @cite or optimize the communication message for a predefined control policy @cite , which are inapplicable to the real-world multi-agent systems. Previous studies also try to address the limited-bandwidth restriction by pruning the messages @cite @cite @cite . A general method is the Value of Communication (VoC) @cite . VoC measures the difference between the expected values of communicating and remaining silent. Communication is a better choice when VoC is larger than zero. Nevertheless, calculating VoC requires the knowledge of the environment, which is not easy to get since multi-agent systems are usually too complex. In contrast, we focus on model-free learning.
- black Liao @cite introduced the first CNN for video SR. They performed motion compensation to generate an ensemble of SR-drafts, and then employed a CNN to reconstruct HR frames from the ensemble. Caballero @cite proposed an end-to-end video SR framework by incorporating a motion compensation module with an SR module. Tao @cite black integrated an encoder-decoder network with LSTM to fully use temporal correspondence. This architecture further facilitates the extraction of temporal context. Since correspondence between adjacent frames mainly exists within a local region, video SR methods focus on the black exploitation of local dependency. Therefore, they cannot be directly applied to stereo image SR due to the non-local and long-range dependency in stereo images.
- Light-filed imaging can capture additional angular information of light at the cost of spatial resolution. To enhance spatial resolution, Yoon @cite introduced black the first light-field convolutional neural network (LFCNN). Yuan @cite proposed a CNN framework with a single image SR module and an epipolar plane image enhancement module. To model the correspondence between images of adjacent sub-apertures, Wang @cite developed a bidirectional recurrent CNN. Their network uses an implicit multi-scale feature fusion scheme to accumulate contextual information for SR. Note that, these methods are specifically proposed for light-field imaging with short baselines. Since stereo imaging usually has a much larger baseline than light-field imaging, these methods are unsuitable for stereo image SR.
- Bhavsar @cite argued that image SR and HR depth estimation are intertwined black under stereo setting . Therefore, they proposed an integrated approach to jointly estimate the SR image and HR disparity from LR stereo images. Recently, Jeon @cite proposed a StereoSR to employ parallax prior. Given a stereo image pair, the right image is shifted with different intervals and concatenated with the left image to generate a stereo tensor. The tensor is then fed to a plain CNN to generate the SR result by detecting similar patches within the disparity channel. However, StereoSR cannot handle different stereo images with large disparity variations since the number of shifted right images is fixed.
- Cost Volume Cost volume is widely applied in stereo matching @cite @cite @cite and optical flow estimation @cite @cite . black For stereo matching , several methods @cite @cite use naive concatenation to construct 4D cost volumes. These methods concatenate left feature maps with their corresponding right feature maps across all disparities to obtain a 4D cost volume (, height @math width @math disparity @math channel ). Then, 3D CNNs are usually used for matching cost learning. However, learning matching costs from 4D cost volumes suffers from a high computational and memory burden. To achieve higher efficiency, dot product is used to reduce feature dimension @cite @cite , resulting in 3D cost volumes (, height @math width @math disparity ). However, due to the fixed maximum disparity in 3D cost volumes, these methods are unable to handle different black stereo image pairs with large disparity variations.
- Self-attention Mechanisms Attention mechanisms have been widely used to capture long-range dependency @cite @cite . For self-attention mechanisms @cite @cite @cite , a weighted sum of all positions in spatial and or temporal domain is calculated as the response at a position. Through matrix multiplication, self-attention mechanisms can capture the interaction between any two positions. Consequently, long-range dependency can be modeled with a small increase in computational and memory cost. Self-attention mechanisms have been successfully applied in image modeling @cite and semantic segmentation @cite . Recent non-local networks @cite @cite share a similar idea and can be considered as a generalization of self-attention mechanisms. Note that, since self-attention mechanisms model dependency across the whole image, directly applying these mechanisms to stereo image SR involves unnecessary calculations.
- Very recently, Laina al @cite proposed using the Huber loss instead of the @math loss to deal with the long tail effect of the depth distribution. Cao al @cite demonstrated that formulating depth estimation as a classification task could achieve better results than regression with @math loss, while insufficient analysis is given for the success. In addition, different with our method, they used hard-max inference in the testing phase. Xu al @cite proposed a Multi-Scale Continuous CRFs to better extract the hierarchical information and improve the smoothness of the final results. Our hierarchical information fusion strategy is much simpler than @cite , while we also achieve comparable results.
- Besides the above methods using ground truth depth maps to supervise the network learning, there is another group of methods that using novel view synthesis to supervised the network learning by exploiting the availability of stereo images and image sequences @cite @cite @cite @cite cite Unsupervised-Depth-Motion . Garg al @cite proposed to train a network for monocular depth estimation using an image reconstruction loss, where a Taylor approximation is performed to linearize the loss. Godard al @cite replaced the use of explicit depth data during training with easier-to-obtain binocular stereo footage, which enforces consistency between the disparities produced relative to both the left and right images, leading to improved performance and robustness compared to existing approaches. Along this pipeline, Zhou al @cite presented an unsupervised learning framework for the task of monocular depth and camera motion estimation from unstructured video sequences based on image warping to evaluate the image error. Kuznietsov al @cite learned depth in a semi-supervised way, where sparse ground-truth depth and photoconsistency are jointly used. Ummenhofer al @cite trained a convolutional network end-to-end to compute depth and camera motion from successive, unconstrained image pairs, where the architecture is composed of multiple stacked encoder-decoder networks.
- Our work is also related to the works on FCN (fully convolutional network) based dense labeling. Long al @cite proposed the fully convolution neural network for semantic segmentation, which is widely used in other dense labeling problems. Hariharan al @cite presented that low-level CNN feature is better to the boundary preserving and object location. Recently, Yu al @cite demonstrated that dilated convolution could expand the receptive field of the corresponding neuron while keeping the resolution of the feature map. Chen @cite successfully utilized the dilated convolution on the semantic problem and show how to build them on the pre-trained CNN.
- @math @cite assesses the caching capability of a path and caches contents probabilistically in order to leave caching space for other data chunks, and acceptably distribute contents in caches along the path from the to the . By approximating the distance, that content travels from to . Measuring the content capability of a path, NDN router caches the incoming content. Both uniform and non-uniform cache sizes have been adjusted to fit in both ecosystems.
- Authors in @cite design a cost-aware cache decision policy", that jointly consider content popularity and price of links through which the contents have to be retrieved. With the help of cost-aware modular decision policy designed, any classic decision policy can be employed. Cost-aware policies allow caching just the contents that are costly for the operator because they reduce the load on expensive and cheap links, while increasing the load on free links. According to their evaluations, by tuning the trade-off between popularity and price, the cost-aware performance is classified into mostly popularity-driven, balanced, and mostly cost-driven categories. A larger gain is achieved in balanced mode, since designing a better composition of hit ratio and cost fraction.
- Cache size allocation schemes have been the subject of other studies, which are closer to our contribution in this paper such as @cite @cite @cite @cite . The centrality metrics of routers have been considered in @cite and been found that the achievable gain with the non-uniform cache size allocation method is not significantly better in comparison with uniform cache size allocation scheme. It has been noticed that the Degree-Centrality metrics improve cache hit for all topologies except Abilene topology where the degree metric does not vary significantly across the nodes.
- Authors in @cite have introduced a novel cache size optimization scheme and classify cache routers based on their roles in content delivery. The information about users' behavior and network traffic distribution are collected by the number of received requests, requests served and content replacements. Their scheme has showed better performance than the graph-related schemes as the important routers have been allocated 80 In @cite , a scheme has been presented to give higher cache resource to applications with high performance requirement. In order to assign priority to applications, a manifold learning method has been introduced to perform data mining. The presented evaluations show that the hit ratio of applications such as HTTP web and file sharing is more than Web media and User Generated Contents (such as blogs, chats, tweets). As claimed, the differences are from popularity and average content size of the applications.
- A new metric has been presented in @cite , named Request Influence Degree (RID), to reflect the importance of every router along the content delivery path by considering incoming Interest and outgoing Data messages. The number of all requests received by a router from others has been calculated and multiplied by a weight. The weight is computed from miss rates of caches along the path to show the number of requests that should be sent by a to reach an in-network router. An Interest message arrived at a special router has this information of past misses. However, as the current router needs to have knowledge about past routers on the path, the message-passing among routers is increased significantly. The short-term and the long-term parameters presented in this article, are inspired from @cite , and the work presented by Rossi and Rossini @cite .
- Estimating disparity by finding dense correspondences is challenging due to the presence of smooth regions, repeating textures, specular highlights, and half-occlusions. Stereo algorithms proceed by first computing an initial score of match quality between each pixel in the reference pixels and all candidate matches. These candidates are indexed by a finite discrete set of candidate disparity values common to all pixels---typically integer pixel disparities ranging from zero to some maximum value---and correspondingly, the matching scores are organized in a cost volume'' along the spatial and disparity dimensions. Traditionally, stereo algorithms used hand-crafted similarity metrics for matching that take into account local neighborhoods around pixels for robustness, while also being efficient to compute @cite .
- However, these matching scores are still ambiguous and thus local reasoning alone is insufficient for accurate disparity estimation. This is why stereo algorithms have a second globalization'' stage, where the local match information in the cost-volumes is aggregated while promoting properties such as smoothness, piece-wise planarity, etc. in the estimated disparity maps. This aggregation was traditionally as optimization of an energy function @cite @cite @cite , again with an emphasis on computational efficiency.
- Z bontar and LeCun @cite @cite demonstrated that using deep neural networks for stereo estimation could deliver significant improvements in accuracy over traditional stereo pipelines. Their work only replaced the local matching stage---they proposed learning networks that took a pair of @math patches in the left and right image as input to produce a matching score. Once this network was trained, it was applied on all candidate match pairs to populate the cost volume, which was then smoothed using traditional aggregation techniques @cite . Surprisingly, by just replacing the matching cost with a learned metric, this method was able to achieve significant gains in accuracy. However, these gains came with a reduction in speed, taking more than a minute to process a single stereo pair. Z bontar and LeCun @cite also considered faster architectures, as did Luo @cite , but these were less accurate and still took 0.7 @cite and 0.8 @cite seconds to process a stereo pair.
- These methods were driven by the presence of moderate-sized real stereo datasets @cite @cite with ground-truth data captured using a LIDAR. Noting the benefits of learned methods for stereo, Mayer @cite introduced a much larger, synthetically rendered, dataset to enable training of more complex networks---with layers that carry out both matching and globalization computations (the latter replacing traditional aggregation) and are trained end-to-end. GC-Net @cite uses shared 2D convolutions to extract features from each image in the stereo pair, concatenates them to form a 4D tensor (indexed by spatial dimensions, disparity, and features) and then uses 3D convolution layers to process this cost volume. Since then, a number of methods have used a similar approach to using 3D convolutions with innovations in network architecture for cost-volume construction and processing @cite @cite @cite @cite , yielding improvements in accuracy and run-times. The fastest among these is the smaller DES-net architecture in @cite , which primarily allocates layers for accurate cost computation and achieves a run-time of 0.05 seconds per stereo pair.
- Random forest @cite is a distinguished ensemble learning algorithm inspired by the random subspace method @cite and random split selection @cite . In the original method, decision trees are built upon bootstrap datasets from the training set using the CART methodology @cite . Its various variants, such as quantile regression forests @cite and deep forests @cite , have been proposed and used in a wide range of applications @cite @cite @cite @cite for their effective training process and great performance.
- One important property, consistency, has yet been established for random forests. The consistency of a prediction method ensures that its performance converges to the optimum as the sample size goes to infinity. This property was firstly discussed in Breiman's mathematical heuristics report @cite . After that, the consistency of two directly simplified random forests was verified by Biau @cite .
- Several variants of random forests have been proposed and shown to be consistent; for example, quantile regression forests @cite , random survival forests @cite , and an online version of random forests @cite . Recently, Denil @cite developed a version in which a subspace of candidate features is selected according to a Poisson distribution. Wang @cite introduced Bernoulli random forests (BRF), in which two Bernoulli distributions are used in the tree construction process. Among all these variants, the performance of the BRF is the closest to that of the original random forest.
- A rich body of approaches to unsupervised domain adaptation aim to reduce the gap between the source and target domains by learning domain-invariant feature representations, through various statistical moment matching techniques. Some methods utilize maximum mean discrepancy (MMD) @cite @cite to match the hidden representations of certain layers in a deep neural network. Other approaches uses Central moment discrepancy (CMD) method @cite to explicitly match each order and each hidden coordinate of higher order moments. Adaptive batch normalization (AdaBN) @cite has also been proposed to modulate the statistics in all batch normalization layers across the network between domains.
- Another family of strategies tackles the domain adaptation problem by leveraging the adversarial learning behavior of GANs @cite . Such technique was first used at where a domain discriminator is trained to correctly classify the domain of each input feature and the feature generator is trained to deceive the domain discriminator so that the resulting feature distribution is made domain invariant @cite @cite @cite . Later the technique was applied at to perform distribution alignment in raw input space, translating source domain to the style'' of target domain and obtaining models trained on transformed source data @cite @cite @cite @cite @cite @cite @cite . Recently the technique was used at by assuming that the output space contains similar spatial structure for some specific tasks such as semantic segmentation. The method in @cite thereby aligns pixel-level ground truth through adversarial learning in the output space. Other hybrid approaches have also been proposed in @cite @cite .
- In contrast, Saito al in @cite proposed to align distributions by explicitly utilizing task-specific classifiers as a discriminator. The framework maximizes the discrepancy between two classifiers' output to detect target samples that are outside the support of the source and then minimizes the discrepancy to generate feature representations that are inside the support of the source with respect to the decision boundary. Instead of aligning manifold in feature, input, or output space by heuristic assumptions, this approach focuses on directly reshaping the target data regions that indeed need to be reshaped.
- Wasserstein metric, the natural geometry for probability measures induced by the optimal transport theory, has been investigated in several fields such as image retrieval @cite , color-based style transfer @cite , and image warping @cite . The Wasserstein distance has also recently raised interest in stabilizing generative modeling @cite @cite @cite , learning introspective neural networks @cite , and obtaining Gaussian mixture models @cite thanks to its geometrically meaningful distance measure even when the supports of the distributions do not overlap.
- As for domain adaptation, Courty al in @cite first learn a transportation plan matching source and target samples with class regularity. JDOT method @cite learns to map input space from source to target by jointly considering the class regularity and feature distribution. DeepJDOT method @cite further improves upon JDOT by jointly matching feature and label space distributions with more discriminative feature representations in a deep neural network layer. However, the fact that these approaches explicitly enforce an one-to-one mapping between source samples and target samples in label space could largely restrict the practical usages when balanced source-target pairs are unavailable. It is also unclear how to extend these approaches to more generic tasks when one data sample has structured output space such as pixel-wise semantic segmentation.
- In this work, we propose a principled framework to marry the two powerful concepts: distribution alignment by task-specific decision boundary @cite and the Wasserstein distance @cite . The Wasserstein metric serves as a reliable discrepancy measure between the task-specific classifiers, which directly measures the support of target samples from source samples instead of producing explicit one-to-one mapping in label space. A variational version of the Wasserstein discrepancy further provides straightforward and geomatrically meaningful gradients to jointly train the feature generator and classifiers in the framework efficiently.
- Optimization-Based Approaches. Optimization-based approaches usually suffer from high computational cost and is sensitive to the initial estimate. To reduce the computational cost, many works have been proposed to improve the efficiency in hardware-level @cite @cite @cite or software-level @cite @cite @cite . Although these works have successfully reduced the DRR generation time to a reasonable range, the overall registration time is still non-negligible @cite @cite and the registration accuracy might be compromised for faster speed @cite @cite . For better initial pose estimation, many attempts have been made by either sampling better initial position @cite @cite , using multistart strategies @cite @cite , or a carefully designed objective function that is less sensitive to the initial position selection @cite . However, these methods usually achieve a more robust registration at the cost of longer running time as more locations, and the corresponding DRRs need to be sampled and generated, respectively, to avoid being trapped in the local extrema.
- In the earlier stage of source code mining, most of the existing works focused on the NLP-based techniques, which process source codes as tokens @cite @cite @cite or APIs @cite @cite sequences. The related tasks are to predict sequences composition, translation between natural language and code, code completion, and etc. @cite believed that source code has certain statistical properties, similar to natural languages. Thus, they used n-gram, a popular statistical model in NLP, to model source codes and illustrated that their model can enhance the Eclipse's ability to complete Java code. @cite used the Pointer Network model for end-to-end training of source code to achieve IDE code completion task. @cite adopted Encoder-Decoder based RNN to obtain API usage sequences from API-related natural language queries.
- The popularity of Graph Neural Networks (GNN) @cite @cite @cite inspired researchers to applying GNN to source code mining @cite @cite . The core idea is to represent the program as a simple graph and then feed the graph as an input into GGNN to obtain the distributed node representation for each node in the graph. Finally, the distributed representation is used to detect error variables or predict variable names.
- The most related work to our work is the work of Allamanis @cite , which applied GGNN to the program graph integrating AST and data-flow edges, in order to predict variable names. However, this work did not take into accounts the function-call relations embedded in the programs, the dynamic distributed vector representations for edges in the program graph, the influence of different adjacent nodes through attention mechanism, and the aggregation of global information through attetion mechanism. To the best of our knowledge, our work is the first work that applies the GNN-based model to program classification task.
- Another class of models combines model-free and model-based algorithms, known as hybrid models. These have been applied to learning reward functions from demonstration @cite , and to obtaining approximate models of the dynamics used in differential dynamic programming and policy gradient evaluations, which required only a small number of real-life trials @cite . More recently @cite , designed an algorithm which learnt stationary nonlinear policies for solutions to unknown dynamical systems using priors and an improved iLQG algorithm. As compared to hybrid models, RLOC offers an alternative framework which learns linear and nonlinear plant dynamics on-line and does not rely on approximate models. We show using two low-dimensional standard benchmark problems that RLOC outperforms matches the benchmark algorithms: naive switching, LQR, state-of-the-art iLQR and state-of-the-art PILCO, in terms of accuracy and computational requirements, thus forming a promising framework for addressing this important class on nonlinear control problems with unknown dynamics.
- The task of fine-grained entity typing was first thoroughly investigated in @cite , which utilized Freebase-guided distant supervision (DS) @cite for entity typing and created one of the early large-scale datasets. Although DS provides an efficient way to annotate training data, later work @cite pointed out that entity type labels induced by DS ignore entities' local context and may have limited usage in context-aware applications. Most of the following research has since focused on testing in context-dependent scenarios. While early methods @cite @cite on this task rely on well-designed loss functions and a suite of hand-craft features that represent both context and entities, proposed the first attentive neural model which outperformed feature-based methods with a simple cross-entropy loss.
- Closely related to the problem of LF baseline extension, LF view interpolation aims at synthesizing the in-between LF views based on a sparse set of reference input views. Methods for LF view interpolation can be divided into two categories: The first category requires explicit estimation of the scene disparity to guide the view synthesis @cite @cite @cite . Notably, @cite proposed to use the disparity estimated by a convolutional neural network (CNN) as guide to warp all reference views to the target angle, these warped views are subsequently fed into a second CNN for color refinement until a final prediction is reached. Bicubic interpolation was adopted during warping which links the gradients of errors from the synthesized views with those from the disparity estimation, which enables end-to-end training. Methods under this category is either limited by the precision of disparity estimation, or by the warping operators which only samples over a limited local area and fails to back-propagate synthesis errors over larger ranges.
- Methods in the second category directly synthesize the target view via exploration of the geometrical features embedded within the EPIs @cite , across stacked sub-aperture views along different directions @cite , or via alternating filtering between the spatial-angular domains @cite . Although methods under this category generally produce more realistic renderings, their performance is still limited for scenarios with large camera baselines.
- Scene content at different depth show complex occlusion relationships when viewed from distant viewing angles. It is a well-adopted strategy to segment the pixels into separate layers based on their motion @cite and disparity range @cite @cite for independent processing. Notably, @cite proposed an end-to-end training framework that first leans the Multi-Plane Images (MPI) with increasing depth ranges. The inferred MPIs are then used to synthesize a range of novel views via a subsequent module. Such a framework preserves the scene geometry in an computationally efficient way, however it produces noticeable distortions when the parallax shift is large; and it is beyond its capability to deal with large ambiguous regions caused by occlusion.
- The challenge of predicting ambiguous image content caused by occlusion is directly related to the problem of image synthesis and inpainting. The Generative-Adversarial Network (GAN) @cite show wonderful performance for these applications. Semantic labels have been used to provide content consistent outcomes for image synthesis @cite @cite . Global and local context features have been adopted to provide a more stable and consistent inpainting @cite @cite . These works provide valuable inspirations to our work, in which we aim at inpainting the occluded regions with surface consistent constraints.
- Recently, the performance of the SR has been greatly improved with powerful capabilities of the deep learning-based methods. As a pioneer work, @cite propose a deep learning-based model SRCNN that works much better than the traditional algorithms. However, SRCNN requires large computation resources compared to its depth, since the model takes upsampled images as an input. On the other hand, FSRCNN @cite and ESPCN @cite get LR images and upsample the output at the end of the network. This strategy reduces the computation substantially compared to the scheme. However, the performance could be degraded since most of the recovering processes are done in the LR space. Another issue is that it is tricky to apply the multi-scale training @cite because of the resolution-mismatch problem between the input images across the different scale factors.
- However, the aforementioned methods have only a few convolutional layers because of the instability and difficulty of training. To tackle this issue, VDSR @cite introduces global residual learning and shows significant improvement over the previous methods by using 20 convolutional layers. The global residual learning maps the LR images @math to their residual images @math . Then, it produces the SR images @math by adding the residual back into the original, , @math . This paradigm facilitates training a deep model with fast and stable convergence. Another approach is progressive upsampling @cite @cite @cite , which upsamples the intermediary features periodically to restore the image details gradually. By doing so, those methods effectively perform SR on extremely low-resolution cases compared to the one-stage upsampling manner.
- One possible issue of applying a deep learning-based method is the efficiency of the model. To address this concern, most of the previous studies aim to build a lightweight model in terms of parameters. For example, DRCN @cite and DRRN @cite use a recursive network to reduce the number of parameters so that the modle training is much easier even with the limited data. Similarly, MemNet @cite has recursive units in the memory block to boost the performance with only a small number of parameters. This idea is applied to the progressive model as well. The MSLapSRN @cite improves LapSRN @cite by tying the parameters of each feature-embedding blocks and results in the superior performance to the LapSRN.
- Generally, deep learning-based SR models are trained using distortion-based (or pixel-based) loss functions ( MSE or L1 loss). The network with these objectives can be optimized easily, but it tends to create blurry artifacts and fails to recover the details such as object edges. This characteristic can be problematic since a human can judge the absence of high-frequency information effortlessly @cite . Hence, to overcome the inherent issue of using distortion-based losses, a generative adversarial network @cite has been adopted to the SR field @cite recently. By doing so, GAN-based methods show promising results in preserving human-perceptive quality. However, since using only an adversarial loss makes the training process unstable, most of the GAN-based models are trained with the addition of pixel losses, despite its downsides. To overcome the inherent problems of pixel losses, @cite introduces the perceptual loss that calculates the distance between the embedded features of two images.
- Besides the SRGAN, many recent works @cite @cite @cite try to improve the perceptual quality. The EnhanceNet @cite and TSRN @cite adopt texture-matching loss @cite in combination with an adversarial training and perceptual losses. Texture-matching loss was originally used in the texture synthesis problem. Given a target texture image, it generates the output image by iteratively matching the statistics extracted from a pretrained network to the target texture. By using texture information in the SR problem, the model can produce more realistic textures and reduce artifacts. On the other hand, ESRGAN @cite improves the SRGAN in a different direction. Internally, ESRGAN replaces standard residual units with the residual-in-residual dense block (RRDB) inspired by the SRDenseNet @cite and RDN @cite . Then it adds the relative discriminator loss @cite to increase the visual quality. However, perceptually-oriented models are not suitable for the real world applications despite the fascinating performances. On the contrary, our proposed models create photo-realistic images with a reasonable amount of computation in order to suit the real-world demands.
- For the photo-realistic SR task, how to measure the quality is a major argument. There are many studies that propose distortion-based metrics for the image quality assessment @cite @cite @cite . But these works do not always reflect the visual quality, and some metrics often contradict human judgment @cite . @cite reveal the trade-off between the average distortion and perceptual quality. Considering the trade-off, we mainly use NIMA @cite and LPIPS @cite perceptual quality metrics as our benchmark test. The NIMA is a newly proposed metric that predicts the distribution of human opinion scores using a deep learning model. It makes the assessment in a non-reference, where all the evaluation is done without ground-truth images. The LPIPS criticizes the perceptual quality by using the distance between two deep features generated by the network pretrained on ImageNet @cite . Upon the pretrained network, they add an extra linear layer and fine-tune it to the human perceptual dataset. In our experiments, we use a fine-tuned AlexNet (linear version 0.1) @cite .
- There has been rising interest in building a small and efficient network @cite @cite @cite @cite . These approaches can be categorized into three groups: Compressing pretrained networks using pruning or quantizing techniques, transferring knowledge of a deep model to shallow one, and designing small but efficient models. In this section, we summarize the latter category, which aims to build a lean neural network in terms of design engineering.
- @cite introduces SqueezeNet to build a parameter-efficient architecture based on the AlexNet @cite . By doing so, they achieve comparable performance level with 50 @math fewer parameters than the baseline model. Unlike the SqueezeNet, MobileNet @cite aims to decrease the number of operations in order to reduce the inference runtime. This model decomposes the standard convolution to the 1 @math 1 and depthwise separable convolution used in the previous studies @cite @cite . While the MobileNet cuts down the computational cost effectively, 1 @math 1 convolution becomes the new bottleneck and thus can be the limitation to pushing down the overall cost. To mitigate this issue, ShuffleNet variants @cite @cite use the channel shuffle unit following the 1 @math 1 group convolution. Referring to the recent literature, we apply a depthwise separable convolution technique in the residual block with a generalized form to achieve a fast and lightweight SR model.
- In the early stage, hand-crafted representations have been well explored for video action recognition. Many feature descriptors for 2D images are generalized to 3D spatiotemporal domain, Space-Time Interest Points (STIP) @cite , SIFT-3D @cite , Spatiotemporal SIFT @cite and 3D Histogram of Gradient @cite . The most successful hand-crafted representations are dense trajectories @cite and its improved version @cite , which extract local features along trajectories guided by optical flow.
- Encouraged by the great success of deep learning, especially the CNN model for image understanding, there are a number of attempts to develop deep learning methods for action classification @cite . The two-stream architecture @cite utilizes visual frames and optical flows between adjacent frames as two separate inputs of the network, and fuses their output classification scores as the final prediction. Many works follow and extend this architecture @cite @cite @cite . The LSTM networks have also been employed to capture temporal dynamics and long range dependences in videos. @cite @cite CNN is used to learn spatial feature for each frame, while LSTM is used to model temporal evolutions.
- More recently, with the increasing computing capability of modern GPUs and the availability of large-scale video datasets, 3D ConvNet (C3D) has drawn more and more attention. @cite a 11-layer C3D model is designed to jointly learn spatiotemporal features on the Sports-1M dataset @cite . However, the huge computational cost and the dense parameters of C3D make it infeasible to train a very deep model. Qiu al @cite proposed Pseudo-3D (P3D) which decomposes a 3D convolution of @math into a 2D convolution of @math followed by a 1D convolution of @math . In another work @cite , similar architecture is explored and referred to as (2+1)D. @cite proposed the Inflated 3D ConvNet (I3D), which is exactly C3D whose parameters are initialized by inflating the parameters of pre-trained C2D model.
- The most closely related work to ours is Slicing CNN @cite , which also learns features from multiple views for crowd video understanding. However, there are substantial differences between Slicing CNN and the proposed CoST. Slicing CNN learns independent features of the three views via three different network branches, which are merged at the top of the network. Aggregation of spatial and temporal features is conducted only once at the network level. On the contrary, we learn spatiotemporal features collaboratively using a novel CoST operation. Spatiotemporal feature aggregation is conducted layer-wise.
- Algorithms for learning to learn , or meta learning, aim to acquire a procedure that can more efficiently and effectively learn to solve new tasks. We consider meta learning in the context of reinforcement learning, i.e. meta reinforcement learning @cite @cite . Prior model-free meta reinforcement learning algorithms can generally be categorized as being recurrence-based , gradient-based , or a hybrid of the two @cite @cite . We build a gradient-based meta-RL method that extends the MAML algorithm . Unlike these prior model-free meta-RL works, we focus on the problem of learning to adapt to different dynamics, rather than adapting to different rewards.
- Prior model-based RL approaches have considered the problem of learning to adapt to different dynamics through meta-RL or through learned priors @cite . Using a learned model is suitable when sample efficiency is a concern, but achieves lower asymptotic performance than model-free meta-RL . Recent work by used MAML to adapt to different learned dynamics models within an ensemble, to improve model-based RL. We also consider the problem of adapting to different dynamics, but in the context of adapting to different environments, rather than different estimated models of the same environment. Further, our method improves upon MAML by not requiring a reward function for adaptation.
- For 3D human pose estimation, Du al @cite trained an overcomplete dictionary of body joint positions as well as joint velocities. They use a Levenberg-Marquardt optimizer to find the dictionary basis coefficients that minimize the 2D backprojection error on the RGB input frame. This way, joint velocities are used to regularize the joint position estimates. In the experiments section we show that our approach yields superior results on the Human3.6M dataset.
- Haarnoja et al. @cite focus on the integration of a one-shot estimation as measurement into a Kalman framework, but require the estimator to provide a prediction of the noise covariance together with the measurement. The authors demonstrate a superior performance of their Kalman model by comparing to simple one-shot estimation and to a recurrent model that disregards measurement noise covariance. In contrast, our model is designed to regard the estimator that provides measurement updates as a black-box system and automatically estimates the measurement noise covariance based on past observations, which enables us to combine it with existing one-shot estimators.
- This paper sheds new light into the close duality connection between generalizations of two popular algorithmic schemes for problems of the form , namely the conditional gradient (also known as Frank-Wolfe) algorithm @cite @cite @cite and the mirror descent algorithm @cite @cite @cite . The conditional gradient and the mirror descent algorithms share the feature of not requiring any orthogonal projections. This feature makes them attractive in a variety of applications where orthogonal projections are too costly or impractical but where subgradient oracles and Bregman projections are viable. Both the conditional gradient and mirror descent algorithms as well as numerous variants of them have been subjects of active research for several years. Some of the many articles in this rapidly evolving literature include @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite as well as the many references therein.
- A plethora of works @cite have compared and studied deep learning performance under different hardware and software configurations. In particular, researchers have investigated the scaling potential of using CPU servers @cite , single GPU servers, and multi-GPU servers @cite . As the computational needs of deep learning grows so does the support for distributed training over a cluster of GPU servers @cite @cite @cite . Prior work has considered the impact of network communication @cite @cite @cite ; how to tune hyperparamters, e.g., learning rate and batch size @cite @cite @cite @cite @cite ; and how to mitigate the communication bottlenecks and the impact of stale model parameters @cite @cite @cite @cite . However, most works on distributed training performance @cite @cite @cite make the implicit assumptions of and cluster configurations. Our study aims to understand the training performance of cheap transient servers that have dynamic availability, revocation patterns, and unit costs. In addition, these previous studies often focus on measuring training speed using the average time to process one mini-batch @cite @cite @cite . While in this work, we consider multiple important performance metrics---including training time, cost, and accuracy---that could be impacted by training on transient servers.
- Since transient servers are cheaper than their on-demand counterparts, many researchers have studied how to effectively run applications on cloud transient servers with as few modifications as possible @cite @cite . Some researchers have proposed transient-aware resource managers @cite @cite to optimize job schedulers by taking into account the revocation rates of transient servers. Other researchers have proposed system-level fault-tolerance techniques such as dynamic checkpointing to optimize the execution time of various applications, including web services @cite @cite , big data applications @cite @cite @cite @cite and other memory-intensive applications @cite . DeepSpotCloud @cite looked at how to effectively train deep learning models by migrating from one GPU server to a cheaper transient server. Our efforts differs from prior work in two major ways. First, we focus on understanding how distributed training can benefit from cheap transient servers. Unlike the commonly studied batch jobs, big data applications, or even web services, training deep learning models poses a unique trade-off of converging accuracy and training speed. Second, we explored the feasibility and quantified the benefits of performing distributed training on transient servers and identify important transient-aware design changes in distributed training frameworks in order to more effectively utilize transient resources.
- Malinowski @cite made an early attempt at solving the VQA task. Since then, solving the VQA task has received increasing attention from the computer vision and natural language processing communities. VQA approaches can be classified into the following methodological categories: the coarse joint-embedding models @cite @cite @cite @cite , the fine-grained joint-embedding models with attention @cite @cite @cite @cite @cite @cite @cite and the external knowledge based models @cite @cite @cite .
- The coarse joint-embedding models are the most straightforward VQA solutions. Image and question are first represented as global features and then integrated to predict the answer. Zhou proposed a baseline approach for the VQA task by using the concatenation of the image CNN features and the question BoW (bag-of-words) features, with a linear classifier learned to predict the answer @cite . Some approaches introduce more complex deep models, e.g., LSTM networks @cite or residual networks @cite , to tackle the VQA task in an end-to-end fashion.
- One limitation of coarse joint-embedding models is that their global features may contain noisy information, making it hard to correctly answer fine-grained problems (e.g., what color are the cat's eyes?'') . Therefore, recent VQA approaches introduce the mechanism @cite into the VQA task by adaptively learning the local fine-grained image features for a given question. Chen proposed a question-guided attention map'' that projects the question embeddings to the visual space and formulates a configurable convolutional kernel to search the image attention region @cite . Yang proposed a stacked attention network to learn the attention iteratively @cite . Some approaches introduce off-the-shelf object detectors @cite or object proposals @cite as the attention region candidates and then use the question to identify related ones. Fukui proposed multi-modal compact bilinear pooling to integrate image features from spatial grids with textual features from the questions to predict the attention @cite . In addition, some approaches apply attention learning to both the images and questions. Lu proposed a co-attention learning framework to alternately learn the image attention and the question attention @cite . Nam proposed a multi-stage co-attention learning framework to refine the attentions based on memory of previous attentions @cite .
- Despite joint embedding models for VQA delivering impressive performance, they are not good enough for answering problems that require complex reasoning or common sense knowledge. Therefore, introducing external knowledge is beneficial for VQA. However, existing approaches have either only been applied to specific datasets @cite @cite , or have been ineffective on benchmark datasets @cite . There is room for further exploration and development.
- Fukui first introduced the bilinear model to solve the problem of multi-modal feature fusion in VQA. In contrast to the aforementioned approaches, they proposed the Multi-modal Compact Bilinear pooling (MCB), which uses the outer product of two feature vectors to produce a very high-dimensional feature for quadratic expansion @cite . To reduce the computational cost, they used a sampling-based approximation approach that exploits the property that the projection of two vectors can be represented as their convolution. The MCB model outperformed the simple fusion approaches and demonstrated superior performance on the VQA dataset @cite . Nevertheless, MCB usually needs high-dimensional features (e.g., 16,000-D) to guarantee robust performance, which may seriously limit its applicability due to limitations in GPU memory.
- To overcome this problem, Kim proposed the Multi-modal Low-rank Bilinear Pooling (MLB) approach based on the Hadamard product of two feature vectors (i.e., the image feature @math and the question feature @math ) in the common space with two low-rank projection matrices: @cite : where @math and @math are the projection matrices, @math is the dimensionality of the output feature, and @math denotes the Hadamard product or the element-wise multiplication of two vectors. To further increase model capacity, nonlinear activation like @math is added after @math . Since the MLB approach can generate feature vectors with low dimensions and deep models with fewer parameters, it has achieved comparable performance to MCB. In @cite , the experimental results indicated that MLB may lead to a slow convergence rate (the MLB with attention model takes 250k iterations, which is about 140 epochs, to converge @cite ).
- CoVe @cite trained a machine translation model, and used the source language encoder as a contextual representation model for other downstream tasks. As large in-domain parallel corpus is hard to obtain, the potential of CoVe is limited. In contrast, a few recent approaches learn contextual encoder on unannotated corpus with language model objectives. ELMo @cite concatenated a forward and a backward LSTM-based language model while GPT-1 @cite and GPT-2 @cite used unidirectional transformer-based language models. BERT @cite introduced masked language models and provided deep bidirectional representation.
- Most studies for language modeling focus on directly reducing the complexity of the softmax layer. Following , we roughly group them into two categories: sampling-based approximations and structural approximations. Sampling-based approximations include the sampled softmax @cite and NCE @cite . The sampled softmax approximates the normalization term of softmax by sampling a subset of negative targets, while NCE replaces the softmax with a binary classifier. On the other hand, structural approximations such as the hierarchical softmax @cite and the adaptive softmax @cite , form a structural hierarchy to avoid expensive normalization. The adaptive softmax, in particular, group words in the vocabulary into either a short-list or clusters of rare words. For frequent words, a softmax over the short-list would suffice, which reduces computation and memory usage significantly. The adaptive softmax has been shown to achieve results close to that of the full softmax whilst maintaining high GPU efficiency @cite .
- Regarding contextual representation models, ELMo used the sampled softmax while GPT and BERT resorted to subword methods. Specifically, they used WordPiece @cite or BPE @cite to split the words into subwords and the language models were trained to consume and predict these subwords. This method is efficient and scalable, as the subword vocabulary can be kept small. Language models trained in this way are neither strictly nor . Hence we categorize them as language models. One potential drawback, however, is that these models produce representations for fragments of words, and it takes extra efforts to generate word-level representations from them. In this paper, we focus on language models and we will discuss language models in Section .
- For self-driving vehicles many approaches have taken advantage of LIDAR reflectivity to achieve precise localization. These methods are commonly used in commercial approaches, for example, the approach of @cite uses a prior map of reflectivity and exploits road marks to reliably localize a vehicle in an urban environment. The authors formulate the world as a mixture of Gaussians (GMM) over 2D grid cells. The GMM represents the heights of points in each cell and the reflectance in a vigorous way that allows the approach to be robust to weather alteration and road degradation.
- @cite a query point cloud is downsampled and matched against a collection of previously collected submaps globally. All clouds are described on a GPU using a combination of networks trained with a lazy quadruplet loss to produce a 256-dimensional feature vector. In contrast, we firstly segment the query and map point clouds to produce a small set of segments, local keypoints. The segments are then described in real time on a CPU. Thus, our approach uses a combination of local features for each segment and matches those in the map. In this way we are robust to disturbances from occlusions and changing environments.
- Authors in @cite , consider an OFDMA network in the uplink transmissions case. They employ a one-to-many matching game theory algorithm for user association and a one-to-one matching game for channel allocation problems. Moreover, the transmission process is assumed in the non-cooperative mode where the CSI is perfectly known. In this paper, we propose a new algorithm for hybrid cooperative node association via many-to-many matching game and sub-channel allocation in PD-NOMA-based MISO system via many-to-one matching game based on the general framework of @cite . Unlike the methods in @cite , there are some extra-interferences due to the NOMA and CoMP in our considered network. Moreover, we assume externalities in our matching game to insure stability of our proposed method.
- Recently, an increasing number of research works @cite @cite @cite realize the importance of considering the workload distribution among deployed controllers. @cite redefined the CPP as a capacitated controller placement problem which considered the controller capacity as a constraint. With the aim of minimizing the load diversity among controllers, @cite applied GA to divide the network into a given number of domains so as to balance the workload in each domain. Similar research can also been found in @cite @cite @cite . Despite their promising performance, these methods tend to oversimplify the workload distribution. For instance, @cite considered that the workload of a controller equaled the number of connected switches. In other words, all switches are assumed to receive the same amount of network traffic. @cite set the controller workload as a fixed value corresponding to the maximum requests it can receive from the switches. However, these assumptions are unlikely to remain valid since the network traffic is highly dynamic and unpredictable. Therefore, how to model the impact of workload distribution in the CPP remains unsolved.
- Apart from that, existing works on the CPP are based on static-binding-based controller architectures which can easily lead to controller overloading when associated switches generate high volume of requests within a short time frame @cite . Although dynamic-binding-based controller architectures @cite have been proposed to alleviate this issue by migrating switches from overloaded controllers to underloaded ones, the requests from one switch can still only be processed by one controller. Thus, the granularity of workload distribution is still limited to switch level. In this case, the destination controller is prone to be overloaded since its workload will increase dramatically if the switch migrated to it has accumulated a large number of pending requests.
- For decades, people have been doing researches on robotic grasps @cite @cite @cite @cite @cite . Most early works @cite @cite used human-designed features to represent grasps in images or required the full 3-D model of objects to generate grasps @cite @cite @cite . These methods was popular at that time, but they all faced challenges for non-robust features or lacking of the full 3-D models when used in real world applications.
- @cite first proposed to use a rectangle representation method to estimate the gripper configuration and the rectangle metric to evaluate a grasp. They carefully designed the grasp features from images and tried to search robotic grasps and then rank them to choose the best one. Deep learning methods were first applied by @cite to grasp detection. While the process of generating and selecting robotic grasps is similar to the one used in @cite , used sparse auto-encoder to directly extract features from images and achieved the accuracy of 75.6
- @cite first proposed using neural network to directly regress the grasp rectangle parameters from images. By removing the steps of searching potential grasps, the regression method is efficient when compared to the classification method. Their accuracy on the Cornell Grasp Dataset was about 88.0 @cite spent more time on combing rgb features and depth features for accurate grasp detection. They proposed the multi-modal fusion method to regress robotic grasp configurations from RGB-D images and achieved the accuracy of 88.90
- DeepWalk @cite first generates a corpus on graphs by random walk and then trains a skip-gram model on the corpus. LINE @cite learns node presentations by preserving both first-order and second-order proximities. NetMF @cite is a unified matrix factorization framework for theoretically understanding and improving DeepWalk and LINE. Node2Vec @cite adds two parameters to control the random walk process while SDNE @cite proposes a structure-preserving embedding method. GCN @cite incorporates neighbors' feature representations using convolutional operations. GraphSAGE @cite provides an inductive approach to combine structural information with node features.
- For graph with multiple types of vertices and or edges, PMNE @cite proposes three methods to project a multiplex network into a continuous vector space. MVE @cite embeds networks with multiple views in a single collaborated embedding using the attention mechanism. MNE @cite uses one common embedding and several additional embeddings of each edge-type for each node, which are jointly learned by a unified network embedding model. Mvn2Vec @cite explores the embedding results by simultaneously modeling preservation and collaboration. HNE @cite jointly considers the contents and topological structures to be unified vector representations. PTE @cite constructs large-scale heterogeneous text network from labeled information, which is then embedded into a low-dimensional space. Metapath2Vec @cite and HERec @cite formalize meta-path based random-walks to construct the heterogeneous neighborhood of a node and then leverage skip-gram models to perform node embeddings.
- The use of laser scanners is also popular for pose estimation in robotic systems. The most classic method consists in matching two point clouds to estimate the transformation between them by minimizing the matching error, this solution is known as Iterative Closest Point (ICP) @cite . More complex and robust methods to achieve the same goal have been proposed since then, like LOAM @cite , which runs two different algorithms in parallel to achieve real time processing. One algorithm can run faster to obtain a low fidelity odometry, and another one slower for a more precise matching. One of the main challenges for this type of localization methods is the error caused by moving obstacles or lack of information because of occlusions. For this reason, some localization solutions @cite @cite are based on detecting what is static in the environment to only use this information for the scan matching process.
- In our work we chose to use as the sensor only a 2D laser scanner, which could reduce considerably the price of future intelligent vehicles. At this moment, @cite were the only authors to use this kind of sensor to estimate odometry by the use of a deep learning approach. They propose a network to perform scan matching and loop closure using CNNs. Although the loop closure networks shows good accuracy, the scan matching results are still very inaccurate compared to classic methods.
- Based on the idea presented in @cite , we propose a new solution for 2D laser-based odometry estimation using deep learning networks. We explore the use of a RNN along with CNNs to learn temporal features in order to improve the odometry results. We also propose a new configuration for the CNNs where we were able to achieve better results. Finally, we explore this solution in outdoor environments, training and testing it with the KITTI @cite dataset, which contains sequences different of type of scenarios.
- There has been a growing interest on machine learning from natural language instructions. Much work has been done in the setting where an autonomous agent learns to complete a task in an environment, for example, learning to play games by utilizing text manuals @cite @cite @cite and guiding policy learning using high-level human advice @cite @cite @cite . Recently, natural language explanations have been used to augment labeled examples for concept learning @cite and to help induce programs that solve algebraic word problems @cite . Our work is similar in that natural language is used as additional supervision during learning, however, our natural language annotations consist of user feedback on system predictions instead of explanations of the training data.
- Sketching algorithms are specifically designed for the streaming model; that is, for data that is presented as a sequence of updates. The sketching paradigm is attributed to @cite @cite ; see the survey @cite for an introduction and overview of early work.
- A related yet different problem is examined in @cite . In a production distributed environment, a stream of data points may be split across multiple nodes, each holding part of the values of a data point. These parts will eventually need to be aggregated on a core node for outlier detection, but this incurs significant communication cost. The solution proposed is based on compressing local data into a sketch. Another related problem is that of supporting multiple outlier detection queries, i.e., combinations of @math and @math values. Examples include @cite and @cite . The latter presents multi-query extensions to MCOD and its approach is compatible with the our parallel pMCOD solution; here, we have examined single-query solutions only.
- Finally, apart from the platforms discussed, there are additional alternatives. For example, ChronoStream @cite is a prototype system for elastic big stream processing in a distributed environment, providing low latency. However, we have decided to adopt Flink because it combines strong positive features, as discussed in Section , with mature engineering and a broad user community, while we did not consider elasticity issues in this work.
- Yoon Kim @cite adapts CNNs for sentence classification on a variety of tasks (i.e. sentiment analysis, question type). This work suggests that a single layer CNN is comparable to or performs marginally better than state-of-the-art models for each of the tasks considered. Given the success of the models described here, we propose using variants of GRU and LSTM (e.g. single cell, bi-directional, multi-layer) models in addition to CNN-based architectures to assess the ability of both direct learning and transfer learning to predict agricultural sentiment that would help to accurately predict crop yield.
- ---Previous work by @cite demonstrates that transfer learning can be effective, even for a simple SVM classifier. We hypothesize that by consistently using GloVe word embeddings that capture the semantic similarities among words, we could enable effective cross-domain transfer learning on sentiment prediction tasks.
- Instance segmentation goes beyond object detection and requires predicting the exact mask of each object. Multi-Task Network Cascades (MNC) @cite build a cascade of prediction and mask refinement. Fully convolutional instance-aware semantic segmentation (FCIS) @cite is a fully convolutional model that computes a position sensitive score map shared by every region of interest. @cite , which is also a fully convolutional approach, learns pixel embedding. Mask R-CNN @cite extends the FPN model with a branch for predicting masks and introduces new differential cropping operation for both object detection and instance segmentation.
- Detecting small objects may be addressed by increasing the input image resolution @cite @cite or by fusing high-resolution features with high-dimensional features from the low-resolution image @cite @cite @cite @cite . This approach of using the higher resolution, however, increases computational overhead and does not address the imbalance between small and large objects. @cite instead uses a Generative Adversarial Network (GAN) to build features in a convolutional network that are indistinguishable between small and large objects in the context of a traffic sign and pedestrian detection. @cite uses different anchor scales based on different resolution layers in a region proposal network. @cite shifts image features by the correct fraction of the anchor size to cover gaps between them. @cite @cite @cite @cite add the context when cropping a small object proposal.
- . To estimate the motion flow of 2D LiDAR, Chen al @cite proposed Bayesian occupancy filter. Gindele al @cite further extended this method by incorporating prior knowledge. Choi al @cite proposed a recurrent flow network. Note that, the proposed network is different from the conception in the deep learning community. The proposed network mainly consists of a context layer, which is to encode the velocity of each cell in the 2D LiDAR scan map. The velocity also can be regarded as the motion flow. All these methods model the variation of each cell and its local neighbors and then estimate the motion flow. However, the presentation ability of these shallow models usually is too weak to encode the complex problem, which leads to unsatisfying performance. Moreover, in these methods, there are many parameters to be empirically adjusted, which limits their application.
- Apart from 3DCNN, long short-term memory (LSTM) structure has also been shown to be suitable for modeling long term dynamics of human motions and has been applied to human motion classification and prediction (e.g., @cite @cite ). ConvLSTM has been proposed to specifically solve spatial-temporal sequence prediction problems @cite . In a ConvLSTM, the convolutional modules can capture the vocal tract features of each ultrasound frame at fine granularity, while LSTM cells exploit the auto-correlated properties of tongue movements to infer future frames. The present article describes the first use of ConvLSTMs to predict tongue motions in unlabeled ultrasound data with good performance.
- The contributions of the current work can be summarized as follows: 1) We applied ConvLSTM to predict tongue motions in ultrasound image sequences based on past motions, demonstrating that in most cases ConvLSTM network outperforms the 3DCNN adopted in @cite ; 2) @cite only investigated predictions of tongue motions in the immediately following frames. However, we also tested the performance of our method in predicting tongue motions in more distant ultrasound frames and ConvLSTM consistently outperforms 3DCNN in multiple quantitative comparisons.
- Grasp Planning. Robotic grasping is one of the most widely studied topics in the area of object manipulation. In grasping task, we aim to achieve a desired object constraint in front of external disturbance @cite . The existing grasping techniques can be divided into analytic and empirical methods. Analytic methods try to evaluate grasp configuration according to some metric @cite @cite @cite @cite on the assumption of simplified contact model, Coulomb friction and rigid-object modeling @cite . While these methods often required known object model and location @cite . Contrary to analytic approaches, empirical methods try to obtain object representations from data, which can be used to evaluate grasp configuration with heuristics @cite .
- Trojan attack is an emerging attack on DNN models, which is one insidious variant of poisoning attacks considering the requirement of manipulating training data. It allows the attacker to easily craft adversarial examples in the physical world to perform targeted attacks. Previous work on poisoning attacks usually aim to degrade a classifier's accuracy to clean inputs @cite @cite . In contrast, trojan attack maintains a prediction accuracy for clean inputs as high as a benign model, while misdirecting the input to a targeted class whenever the input is trojaned with an attacker-chosen trigger.
- Bagdasaryan @cite show that federated learning is fundamentally vulnerable to trojan attacks. Firstly, participants are enormous, e.g., millions, it is impossible to guarantee that none of them are malicious. Secondly, federated learning is designed to have no access to the participant's local data and training process to ensure the privacy of the sensitive training data; therefore, participants can use trojaned data for training. The authors demonstrate that with controlling no more than 1
- Though there are general defenses against poisoning attacks @cite , they cannot be immediately mounted to against trojan attacks. In practice, the user has no knowledge of the trojan trigger and no access to trojaned training sample, which make combating trojan attacks more challenging.
- The countermeasures in @cite , @cite suggest approaches to remove the trojan behavior without frist check whether the model has been potentially trojaned. Fine-tuning is used to remove potential trojans by pruning dedicatedly chosen parameters of the DNN model @cite . However, this method substantially degrades the model accuracy @cite . It is also cumbersome to perform removal operation to any DNN model as most of them maybe benign. Approaches presented in @cite incur high complexity and computation costs.
- Chen @cite propose an activation clustering (AC) method to detect whether the training data has been trojaned or not prior to the deployment. The intuition behind this method is that the reasons why the trojaned and the benign samples receive the same predicted label by the trojaned DNN model are different. By observing neuron activation of benign samples and trojaned samples that produce the same label in hidden layers, one can potentially distinguish trojaned samples from legitimate samples via the activation difference. However, this method requires that the user has access to the trojaned training data in hand, which appears to be unrealistic. It is very unlikely that the attacker will ship his her trojaned data samples to the user given that the attacker has performed the trojaned attacks.
- Wang @cite in 2019 S @math P propose a Neural Cleanse method to detect whether a DNN model has been trojaned or not prior to deployment. Neural Cleanse is based on the intuition that, given a backdoored model, it requires much smaller modifications to all samples to make them being misclassified into the attacker targeted (infected) label than any other uninfected labels. Therefore, their method iterates through all labels of the model and determine if any label requires a substantially smaller amount of modification to achieve misclassification. One advantage of this method is that the trigger can be reversed and thus discovered during the trojaned model detection process. However, this method has two noticeable limitations. Firstly, it could incur high computation costs proportional to the number of labels. The computation cost of detection process can take up to several days for certain DNN models even when the optimization is adopted. Secondly, similar to SentiNet @cite , it becomes ineffective for large size triggers.
- These kind of approaches typically first estimate the depth of a scene @cite @cite @cite @cite , such as structure tensor-based local direction estimation in the EPI domain @cite , and phase-based depth estimation in the image domain @cite . Then the input views are warped to novel viewpoints and blended in different manners, e.g., soft blending @cite and learning-based synthesis @cite . In recent years, some studies for maximizing the quality of synthetic views have been presented that are based on CNNs. Flynn al @cite proposed a deep learning method to synthesize novel views using a sequence of images with wide baselines. Kalantari al @cite used two sequential convolutional neural networks to model depth and color estimation simultaneously by minimizing the error between synthetic views and ground truth images. Zhou al @cite trained a network that infers alpha and multiplane images. The novel views are synthesized using homography and alpha composition. However, all these approaches are based on the Lambertian assumption without explicitly addressing the non-Lambertian challenge.
- Main obstacle for DSLF reconstruction without exploiting depth information is the contradiction between aliasing effect and over-blurring @cite . Some researchers considered the reconstruction as a spectrum recovery in the Fourier domain. Shi al @cite performed DSLF reconstruction as an optimization for sparsity in the continuous Fourier domain. Vagharshakyan al @cite utilized an adapted discrete shearlet transform to remove the high-frequency spectrums that introduce aliasing effects in the Fourier domain.
- Recently, some learning-based approaches were also proposed for depth-free reconstruction @cite . Yeung al @cite applied a coarse-to-fine model using an encoder like view refinement network for larger receptive field. However, the network appears aliasing effects when reconstructing light field with large disparities. For explicitly addressing the aliasing effects, Wu . @cite took advantage of the clear texture structure of the EPI and proposed a blur-restoration-deblur'' framework. However, when applying a large blur kernel for large disparities, the approach tends to fail at recovering the high-frequency details, and thus leading to the blur effect.
- Our approach toward safe reinforcement learning differs from existing approaches that do not include a formal verification component (e.g., as surveyed by Garc ' i a and Fern ' a ndez @cite and the SMT-based constrained learning approach of @cite ) because we focused on safe learning; i.e., instead of relying on oracles or conjectures, constraints are derived in a provably correct way from formally verified safety proofs. The difference between verifiably safe learning and safe learning is significant, and is equivalent to the difference between verified and unverified software. Unlike most existing approaches our safety guarantees apply to both the learning process and the final learned policy.
- Several recent papers focus on providing safety guarantees for model-free reinforcement learning. Trust Region Policy Optimization @cite defines safety as monotonic policy improvement, a much weaker notion of safety than the constraints guaranteed by our approach. Constrained Policy Optimization @cite extends TRPO with guarantees that an agent nearly satisfies safety constraints during learning. Br ' a @cite give probabilistic guarantees by performing a heuristic-driven exploration of the model. Our approach is model-based instead of model-free, and instead of focusing on learning safely without a model we focus on identifying accurate models from data obtained both at design time and at runtime. Learning concise dynamical systems representations has one substantial advantage over model-free methods: safety guarantees are stated with respect to an explainable model that captures the safety-critical assumptions about the system's dynamics. Synthesizing explainable models is important because safety guarantees are always stated with respect to a model; therefore, engineers must be able to understand inductively synthesized models in order to understand what safety properties their systems do (and do not) ensure.
- propose an approach, based on deep reinforcement learning, for efficiently discovering defects in models of cyber-physical systems with specifications stated in signal temporal logic @cite . Model falsification is an important component of our approach; however, unlike , we also propose an approach toward obtaining more robust models and explain how runtime falsification can be used to obtain safety guarantees for off-model learning.
- Our approach includes a model synthesis phase that is closely related to program synthesis and program repair algorithms @cite @cite @cite . Relative to work on program synthesis and repair, VPMUs are unique in several ways. We are the first to explore program repair. Our approach combines program verification with mutation. We treat programs as in which one part of the model is varied according to interactions with the environment and another part of the model is systematically derived (together with a correctness proof) from these changes. This separation of the dynamics into inductively synthesized models and deductively synthesized controllers enables our approach toward using programs as representations of dynamic safety constraints during reinforcement learning.
- Although we are the first to explore hybrid program repair, several researchers have explored the problem of synthesizing hybrid systems from data @cite @cite . This work is closely related to our update. Sadraddini and Belta provide formal guarantees for data-driven model identification and controller synthesis @cite . Relative to this work, our update is continuous-time, synthesizes a computer-checked correctness proof (as opposed to a least violation criterion implied by the synthesis algorithm, both not independently checked by a theorem prover), and does not require dynamics to be compact, bounded, and locally connected. Unlike @cite , our full set of model updates is sometimes capable of synthesizing nonlinear dynamical systems from data (e.g., the static @math circular update) and produces computer-checked correctness proofs for permissive controllers.
- Consensus algorithms are used in load balancing. In fact, it was one of the early methods to balance loads @cite Load balancing has shown to reduce the hot spots in the sensor networks @cite . It also helps in extending the lifespan, sensor's energy might not be sufficient to support long communication and may require hops to forward the information @cite . The load balancing is also used frequently for routing in networks @cite @cite .
- It is important to combine the data from different sensors for the purpose of event detection or other applications such as consensus @cite . Some common data fusion functions include mean distributed averaging @cite , Kalman filter @cite , Maximum likelihood estimator @cite and linear least-squares estimator @cite . Nodes often collaborate for a better performance @cite . There are many applications in event detection that require data fusion @cite @cite @cite .
- Darwin suggested that human and animal facial emotions are evolutionary @cite . Motivated by Darwin's work, @cite @cite found that the seven expressions, namely happiness, anger, fear, surprise, disgust, sadness and contempt remain the same across different cultures. Facial action coding system (FACS) is proposed in @cite to investigate the facial expressions and the corresponding emotions described by the activity of the atomic action units (cluster of facial muscles). Facial expression can be analyzed by mapping facial action units for each part of the face (eyes, nose, mouth corners) into codes.
- @cite demonstrate the capability of the CNN network trained on various FER datasets by visualizing the feature maps of the trained model, and their corresponding FACS action unit. Motivated by Xception architecture proposed in @cite , @cite proposed mini-Xception. @cite proposed two different deep network models for facial expression recognition. The first network extracts temporal appearance features, whereas the second extracts temporal geometric features and these networks are combined and fine tuned in the best possible way to obtain better accuracy from the model. Motivated by these two techniques, we have trained and obtained multiple models, the details of which are explained in section .
- The segmentation and classification of time series has many applications in different fields such as predicting failures in an oil process plant, reconstructing trajectories in air traffic control, the identification of interaction scenarios in robotic environments, real-time brain computer interfaces, probabilistic forecasting of volcano eruptions or forecasting time series with multiple seasonal patterns . For a recent review of change point detection methods we refer to . These models have been especially popular in computer vision for human motion modeling . Regarding the modeling aspect, Gaussian processes have been used to model temporal structure within segments @cite . Alternative methods, which are often based on HMMs, have recently been combined with recurrent and structured inference procedures . In the following, we provide more details about the models we are building on top of: HSMMs and BOCPD, and we point out the differences concerning inference and comment on a setting which leads to a similar generative model.
- The BOCPD @cite generative model assumes that the segments composing a sequence of observations @math are non-overlapping product partitions. This means that every partition @math (i.e., segments for HSMMs) is generated i.i.d. given a fixed observation model @math . The parameters @math are also drawn i.i.d. across partitions from some fixed distribution parameterized by @math . Formally, the UPM represents the distribution @math which generates observations based on the current run length @math and observed values @math . The original formulation of BOCPD did not provide a learning procedure since the model hyperparameters @math were assumed known.
- The joint probability in is not only insightful for detecting change points, but can also be used to perform predictions about future observations. The predictive distribution for the next observation (shown in ) is derived by marginalizing over the current run length posterior, which is obtained from . Note that an HSMM (Section ) with a single hidden state @math leads to almost the same generative model of BOCPD. One of the key differences is that the HSMMs are traditionally parameterized in terms of the total segment duration, whereas the BOCPD parameterization is based on the run length through the hazard function instead of the duration model. As hinted earlier, the equivalence between the hazard function and the duration model is well-understood---see @cite for details. However, the chosen parameterization has relevant implications for the UPMs, as discussed in Section .
- The key difference between HSMMs and BOCPD is related with how inference is performed and its ultimate goal. HSMMs have been mostly used for batch processing of time series and retrospective inference of hidden states through the celebrated Forward-Backward and Viterbi algorithms @cite . On the other hand, BOCPD focuses on the filtering setting (i.e., online updating) and on events which are relevant for the most recent observation. It does so by explicitly accounting for potentially incomplete segment realizations.
- In recent years, many studies have been conducted to obtain a more flexible prior. Stick-breaking VAE @cite has a stochastic latent dimensionality by transforming the standard normal distribution which is commonly used as a prior, into a stick breaking process @cite . VAE with VampPrior @cite uses the aggregated posterior of the trainable pseudo input as the prior to obtain a flexible prior from the data during learning. Learning prior for AAE @cite adopts a code generator in an AAE to obtain the prior distribution which expresses the data well.
- Generative Moment Matching Networks (GMMN) @cite learns mapping from a uniform prior distribution to data distribution. Similar to our work, GMMN can be used in an autoencoder to implicitly estimate the latent distribution . In this case, GMMN learns mapping from a uniform prior to latent distribution.
- ( wachinger2018deepnat ), named DeepNat, is a whole brain segmentation method that achieves a segmentation of all structures of the brain with around 90 Although not a hippocampus segmentation work, @cite inspired our consensus strategy that involves the use of multiple FCNNs performing segmentation over different MRI orientations, merged into a single final volume. While his method uses another network to produce the final consensus, in our post processing we simply add the activation heatmap of each FCNN, apply a pre-defined threshold, and perform 3D labeling to eliminate all but the two bigger connected components, which is shown to improve performance significantly.
- We discuss and compare semantic image segmentation frameworks with a particular focus on real-time execution with low energy and memory requirements @cite @cite @cite @cite @cite @cite @cite @cite .
- State-of-the-art semantic segmentation DCNNs combine two separate modules: the encoder and the decoder. The encoder module uses a combination of convolution and pooling operations to extract DCNN features. The decoder module recovers the spatial details from the sub-resolution features, and predicts the object labels ( the semantic segmentation) @cite @cite . Most commonly, the encoder is adapted from a simple classification DCNN method, such as VGG @cite or ResNet @cite . In semantic segmentation, the fully connected layers are removed.
- The seminal fully convolution network (FCN) @cite laid the foundation for most modern segmentation architectures. Specifically, FCN employs VGG @cite as encoder, and bilinear upsampling in combination with skip-connection from lower layers to recover spatial detail. U-Net @cite further exploited the spatial details using dense skip connections.
- Later, inspired by global image-level context prior to DCNNs @cite @cite , the pyramid pooling module of PSPNet @cite and atrous spatial pyramid pooling (ASPP) of DeepLab @cite are employed to encode and utilize global context.
- Other competitive fundamental segmentation architectures use conditional random fields (CRF) @cite @cite or recurrent neural networks @cite @cite . However, none of them run in real-time.
- Similar to the object detection @cite @cite @cite , speed became one important factor in image segmentation system design @cite @cite @cite @cite @cite @cite . Building on FCN, SegNet @cite introduced a joint encoder-decoder model and became one of the earliest efficient segmentation models. Following SegNet, ENet @cite also design an encoder-decoder with few layers to reduce the computational cost.
- More recently, two-branch and multi-branch systems were introduced. ICNet @cite , ContextNet @cite , BiSeNet @cite and GUN @cite learned global context with reduced-resolution input in a deep branch, while boundaries are learned in a shallow branch at full resolution.
- MobileNet @cite decomposes a standard convolution into a depthwise convolution and a @math pointwise convolution, together known as depthwise separable convolution. Such a factorization reduces the floating point operations and convolutional parameters, hence the computational cost and memory requirement of the model is reduced.
- Chollet @cite designed the Xception network using efficient depthwise separable convolution. MobleNet-V2 proposed inverted bottleneck residual blocks @cite to build an efficient DCNN for the classification task. ContextNet @cite used inverted bottleneck residual blocks to design a two-branch network for efficient real-time semantic segmentation. Similarly, @cite @cite @cite propose multi-branch segmentation networks to achieve real-time performance. Since floating point multiplications are costly compared to integer or binary operations, runtime can be further reduced using quantization techniques for DCNN filters and activation values @cite @cite @cite . Pruning is applied to reduce the size of a pre-trained network, resulting in faster runtime, a smaller parameter set, and smaller memory footprint @cite @cite @cite .
- Fast-SCNN relies heavily on depthwise separable convolutions and residual bottleneck blocks @cite . Furthermore we introduce a two-branch model that incorporates our learning to downsample module, allowing for shared feature extraction at multiple resolution levels (Figure ). Note, even though the initial layers of the multiple branches extract similar features @cite @cite , common two-branch approaches do not leverage this. Network quantization and network compression can be applied orthogonally, and is left to future work.
- It is a common belief that pre-training on auxiliary tasks boosts system accuracy. Earlier works on object detection @cite and semantic segmentation @cite @cite have shown this with pre-training on ImageNet @cite . Following this trend, other real-time efficient semantic segmentation methods are also pre-trained on ImageNet @cite @cite @cite . However, it is not known whether pre-training is necessary on low-capacity networks. Fast-SCNN is specifically designed with low capacity. In our experiments we show that small networks do not get significant benefit from pre-training. Instead, aggressive data augmentation and more number of epochs provide similar results.
- The power of multiple choices @cite is a classic paradigm for load balancing which is used in many practical implementations of request routing algorithms. Simplicity and achieving much better results in comparison to a simple randomized method are the main advantages of this paradigm. In @cite the authors have implemented an extension to this paradigm, called . In this method, each server can receive the clients' requests and forward them to the most appropriate server. The best server is chosen between a random server and it's next neighbor. The next-neighbor load sharing method leads to better response time as the authors claimed. In addition, Gardner and Balter have used the power of two choices concept to propose a new theoretical dispatching model @cite . In this algorithm, each request is copied in multiple servers' queues. This way, the requests would wait in multiple queues at the same time, resulting in the minimum waiting time across queues. The authors showed that copying each request in only two servers will reduce average response time noticeably.
- Some other published algorithms like Joint Shortest Queue (JSQ) consider crowdedness of servers as the most critical decision-making parameter of redirecting. In a later work, Control-Law Load Balancing (CLB) algorithm @cite improves JSQ's results. However, it still neglects the communication cost and assumes replica sets are close together.
- Considering the communication cost along with the maximum load of servers (that translates to response time) as the two critical decision-making parameters affecting request dispatching is the main goal of @cite and @cite . These papers show that there is a trade-off between communication cost and response time in a distributed network. To handle the above-mentioned trade-off, @cite introduces three basic algorithms each of which leads to a different trade-off curve. Furthermore, it assumes that each server has a limited cache size, and therefore, it can respond to a specific group of requests. This limitation increases the complexity of the redirecting algorithms.
- In @cite , authors improved the CLB algorithm by adding a new parameter @math to control communication cost. Although the existing trade-off is not mentioned in this paper, the simulation results somehow admit the existence of such trade-off. Its results show that more constraints on communication cost lead to worse response time. Despite @cite 's assumption about limited cache size, the authors of @cite consider every server is capable of responding to every request which is a simplified model of real CDNs.
- Bees algorithm is a population-based search scheme, which is categorized in the class of meta heuristic algorithms. The process is a mimicry of foraging behavior of honeybees to look for the best solution to an optimization problem @cite .
- Early work on image matching and retrieval started in the late 1970's and it has become a fundamental aspect of many problems in computer vision. General-purpose color-based CBIR systems very often employ histograms in different color spaces, color layout and region-based search, or a combination thereof @cite . Typical techniques for identifying shapes involve edge histograms or image moments, like for instance centroid distances @cite @cite .
- More recent developments in image retrieval gave rise to advanced techniques like SIFT @cite , SURF @cite for local descriptors and VLAD @cite or Fisher Vectors @cite for aggregation. Once local feature descriptors have been obtained by means of SIFT, SURF or a similar approach, it is also possible to apply a Bag of Words (BoW) model to create a global, aggregated feature vector @cite @cite @cite .
- Content-based Music Retrieval (CBMR) or just Music Information Retrieval (MIR) is a multidisciplinary field that straddles different domains ranging from computer science to psychology. There is a large community surrounding MIR organized in the http: www.ismir.net , which holds the annual MIREX evaluation campaign for MIR algorithms @cite .
- Generally, MIR tasks can be characterized by either their specificity and their granularity. Based on these two dimensions, @cite classifies existing Query-by-Example techniques for music into four larger categories: , , and .
- In @cite @cite , a linear combination of is used to obtain a function that approximates the 3D model's surface. The weight coefficients of this function serve as components in a feature vector. On the other hand, @cite proposes for 3D models, which are based on projections of the 3D model onto the faces of a circumscribing dodecahedron. Subsequently, classical CBIR techniques can be applied on the resulting images to extract feature descriptors, namely calculation of and for the resulting shape.
- Except for the analysis of such complex systems, there are some important results on the design of local rules that give rise to a desired global behaviour. @cite designed local rules that allow a network to reach a desired steady-state degree distribution, while proposed a global-to-local design methodology to compose heterogeneous swarms for self-organized task allocation @cite .
- The platform we used for implementing the scoring method is MISP. In @cite , the MISP platform is described from a technical perspective and its main modules introduced. A major advantage is that MISP is a collaborative open-source project that continuously evolves by community-driven effort. Each member can consume or produce threat intelligence data.
- A case study on information sharing is presented in @cite , where a survey was performed on different aspects as hurdles, problems and legal aspects in information sharing. The major outcome was that information sharing remains a group or community activity. Another interesting outcome of this survey was the need for accurate information sharing practices and low false positive rates.
- Information sharing is related to a lot of challenges as presented in @cite , where requirements and needs for successful threat intelligence platforms, such as the added value of shared data and privacy, are discussed. Other requirements are for example a new quality control approaches.
- In @cite , the facility to share information, automate information sharing and the ability to generate, refine and control data is discussed by redefining these problems into a concept of knowledge management for the area of cyber security by adding user needs. This leads to the fact of the veracity of information and also to false positives. In @cite , an assessment approach for malware threat levels is introduced, based on scores and weighting factors. Article @cite refers to a data mining approach using similarity metrics to identify statistical relations in shared information. In @cite is described a data-driven approach to evaluate and visualize mixed content from news and social media content based on emotive facets to give value to content.
- Foreground Background Video Segmentation Several works have focused on the binary version of the video segmentation task, separating all the moving objects object from the background. Early approaches @cite @cite @cite @cite relied on heuristics in the optical flow field, such as closed motion boundaries in @cite to identified moving objects. These initial estimates were then refined with appearance, utilizing external cues, such as saliency maps @cite , or object shape estimates @cite . Another line of work focused on building probabilistic models of moving objects using on optical flow orientations @cite @cite . None of these methods are based on a robust learning framework and thus they do not generalize well to unseen videos. The recent introduction of a standard dataset, DAVIS 2016 @cite , for this task has led to a renewed interest. More recent approaches propose deep models for directly estimating motion masks, as in @cite @cite @cite . These approaches are similar to ours in that they also use a two-stream architecture to separately process motion and appearance, but they are unable to individually segment object instances. Our method separately segments and tracks each individual moving object in a video.
- Object Detection The task of segmenting object instances from still images has seen immense success in recent years, bolstered by large, standard datasets such as COCO @cite . However, this standard task focuses on segmenting every instance of objects belonging to a fixed list of categories, leading to methods that are designed to be blind to objects that fall outside the categories in the training set.
- Less is known for the contact process on general Galton-Watson trees. Recently, Huang and Durrett @cite proved that on Galton-Watson trees, @math if the offspring distribution @math is subexponential. Along with Theorem , we now have the complete characterization of the existence of extinction in the contact process on Galton-Watson trees.
- Moreover in @cite , a cutoff phenomenon" of the fraction of infected vertices was established. In @cite , the same result as above is proven for @math with bounded @math . For unbounded degree distribution @math , the only known result was due to Chatterjee and Durrett @cite , where they showed that if @math obeys a power law, then the contact process always displays long survival for any @math , though their survival time was slightly weaker than exponential ( @math for any @math ). Our Theorems and generalize the aforementioned results to any general @math and to an exponentially long survival.
- In recent years, many algorithms have been proposed to deal with multi-label learning tasks. In terms of the @math being considered, these approaches can be roughly categorized into three strategies @cite @cite .
- There is a vast literature tackling the problem of undersampled MRI reconstruction. State-of-the-art solutions include both signal processing techniques (e.g. Compressed Sensing (CS)) as well as machine learning ones. On one hand, CS-based MRI reconstruction has been widely studied in the literature @cite @cite @cite @cite @cite . These approaches usually result in over-smoothed reconstructions, which involve a time consuming optimization process, limiting their practical scalability. On other hand, deep learning based approaches have been introduced as a promising alternative to MRI reconstruction @cite @cite @cite @cite @cite . In @cite , a cascaded CNN with a consistency layer is presented to ensure measurement fidelity in dynamic cardiac MRI reconstruction. In @cite , a Unet architecture @cite is used to reconstruct brain images, while @cite proposes a recurrent inference machine for image reconstruction. Moreover, following recent trends, architectures involving image refinement mechanisms seem to be gaining increasing attention @cite @cite @cite . Although all previously-mentioned approaches are able to improve the reconstruction error, the human perception of the results is still not compelling. Therefore, recent works have also focused on exploring different training objectives such as adversarial losses @cite @cite @cite to enhance the perceptual reconstruction quality @cite @cite .
- Significant effort has been devoted in the computer vision literature to provide uncertainty estimates @cite of predictions. There are two possible sources of uncertainty @cite : 1) model uncertainty due to an imperfect model (epistemic uncertainty) and 2) data uncertainty due to imperfect measurements (aleatoric uncertainty). While model uncertainty can be decreased with better models, data uncertainty vanishes only with the observation of all variables with infinite precision. In medical imaging, uncertainty is often used to display probable errors @cite and has been mainly studied in the context of image segmentation @cite @cite . Segmentation errors (i.e. wrong label predictions) are often easier to detect by domain experts than reconstruction errors (i.e. shift of pixel values), which could potentially mislead diagnosis. Therefore, the study of uncertainty is crucial in the context of MRI reconstruction. In this paper, we focus on , which is caused by the partially observed -space. This uncertainty can be captured by proper model parametrization, e.g. in regression tasks a Gaussian observation model is often assumed @cite @cite ; this assumption can be relaxed to allow the use of arbitrary observation models as explained in @cite .
- Previous research on optimizing -space measurement trajectories from the MRI community include CS-based techniques @cite @cite @cite @cite , SVD basis techniques @cite @cite @cite , and region-of-interest techniques @cite . It is important to note that all these approaches work with fixed trajectories at inference time. By contrast, @cite proposed an on-the-fly eigenvalue based approach that adapts to encoding physics specific to the object. However, contrary to our approach, it requires solving an optimization problem at inference time. Moreover, since we train all the components of our pipeline jointly, our adaptive acquisition incorporates information on the image physics, the object being imaged, and the reconstruction process to select the next measurement.
- A special case, where only a subset of nodes needs a unique code, can be modeled with a bipartite graph, and this version of Identifying Codes is called Discriminating Codes'' and was studied in @cite . This special case is relevant for our study as, our problem formulation of individual to organization network requires us to find the unique signatures of all nodes of one side of bi-partition of a bipartite graph, by selecting only a subset of the nodes in the other side of bi-partition. This formulation corresponds directly to Discriminating Codes''.
- Our results contribute to the understanding of the limitations of recommender systems @cite @cite @cite @cite @cite @cite , with direct applications to the design of fair, transparent and efficient ranking systems @cite @cite @cite , as well as methods to reduce the spread of misinformation @cite @cite or uncivil behavior @cite @cite .
- An extensive literature on modeling (click) user behavior is weakly related to our work @cite @cite @cite @cite . Closer to our work, several papers have proposed models of the dynamics of interactions between individual searches and ranking algorithms, e.g., for understanding the feedback loop between ranking system and user queries @cite , explaining the observed mitigation of search engines' popularity bias @cite , or the competition of memes using limited attention @cite . The paper closest to ours is @cite , which also obtains a few-get-richer effect in a model where individuals get multiple signals and where (news) items are ranked via a probabilistic popularity-based ranking. Besides being simpler, our model works with a discrete and deterministic ranking of the websites rather than a continuous and probabilistic one. Among other things, this allows for a tighter connection with the experiment of .
- @cite "Google may favor popularity over richness; it provides advertising that competes directly with "editorial" content; it so overwhelmingly dominates the industry that users seldom get a second opinion, and this is unlikely to change. "
- We provide here a brief background on the formal modeling and verification framework used in this paper. We begin by noting that the lack of formal policy modeling in current network systems contribute to frequent misconfigurations @cite @cite @cite . We use the concept of a metagraph , which is a generalized graph-theoretic structure that offers rigorous formal foundations for modeling and analyzing communication-network policies in general. A metagraph is a directed graph between a collection of sets of atomic'' elements @cite . Each set is a node in the graph and each directed edge represents the relationship between two sets. Fig. shows an example where a set of users ( @math ) are related to sets of network resources ( @math , @math , @math ) by the edges @math and @math describing which user @math is allowed to access resource @math .
- Past works have employed machine learning to classify IoT devices for asset management @cite @cite . Method in @cite employs over 300 attributes (packet-level and flow-level), though the most influential ones are minimum, median, and average of packet volume, Time-To-Live (TTL), the ratio of total bytes transmitted and received, and the total number of packets with RST flag reset. Work in @cite proposes to use features with less computation cost at runtime. Existing Machine learning based proposals need to re-train their model when a new device type is added -- this limits the usability in terms of not being able to transfer the models across deployments.
- We partly adopt the notation from @cite to formulate our problem statement. Consider a set @math of objects @math . Each object @math belongs to a class @math . The finite, ordered set of classes is denoted by @math and is of size @math . In addition, each object is attributed with a continuous variable @math of interest. We introduce the @math of dimension @math given by @math , where @math denotes the indicator function. The @math , where @math is an @math -vector of ones, counts the number of occurrences of the classes @math in the population. That is, @math equals the number of @math for which @math . The @math of length @math is given by @math for @math .
- Assume that the classes @math are not known, but instead predicted to be @math . The predicted class matrix based on the predictions @math is denoted by @math . Similarly, @math denotes the predicted counts vector. Assume that @math is known for all @math . The goal is to estimate the sum of @math over each of the classes @math . In other words, the main problem statement is how to find, using the predicted @math and the known @math , an accurate estimator of the In the sentiment analysis application from before, the set @math is the set of messages and @math is binary (a positive or negative message). Each @math equals 1 and hence @math is a 2-vector with @math the number of positive messages and @math the number of negative messages. In the land cover mapping application obtained from @cite , each @math is a pixel and @math may attain @math different values. The value @math is the real land area corresponding to pixel @math . Hence, the 15-vector @math contains the total land area of each of the 15 land cover classes.
- Now, a first estimator of @math might be the @math -vector However, we know that @math is a biased estimator of @math , recall equation . To estimate (and then correct) this statistical bias, we assume a , following the methodology in @cite ; the value of @math , given the value of @math , is assumed to be a stochastic variable following a . The stochastic variable depends on @math , but draws for different @math are assumed to be independent. The unknown event probabilities are denoted by @math and stored in @math of dimension @math . The suggested estimator @math is shown to have expectation @math , where @math is the @math -th column of @math . If @math is invertible, we denote its inverse by @math . An unbiased estimator of @math is then given by @math . We refer to this bias correction method as .
- To the best of our knowledge, @cite is the first work describing the classification error model and studying classification-based aggregates. We mention front runners from two fields using similar bias correction methods.
- The first field is , which studies the distribution of health and disease. There, it is well-known how a low base rate can lead to high bias even if sensitivity ( @math ) and specificity ( @math ) are high @cite (cf. The Base Rate Example). As mostly binary classifiers are considered (sick or not), bias correction is straightforward [pp. 87-89] lash_applying_2009 . In addition, a standard Bayesian method of bias correction, predominantly using a uniform or Jeffreys prior without parameter constraints, is applied in epidemiology @cite @cite . We will generalize these Bayesian methods to the entire family of conjugate prior distributions and to the setting of multi-class classification. Moreover, we will improve empirical performance by imposing well-chosen parameter constraints.
- The second field is , which analyzes land use based on large volumes of remote sensing data, for example using SVM @cite . As can be expected, multi-class classification is not uncommon in this subject area (see @cite for a case study with @math classes). Since accurately estimating the total area of types of vegetation is highly relevant in monitoring ecological systems @cite , there have been many efforts in the field to make better use of accuracy data @cite . To the best of our knowledge, our bias correction method is a novel contribution to these efforts.
- In machine learning, the accuracy of classification-based aggregates is relatively understudied. The literature on machine learning is mainly concerned with minimizing loss for future predictions and therefore focuses on a different kind of bias. Much work deals with model selection bias and overfitting @cite and sample selection bias @cite . However, reducing these types of bias (by, e.g., using @math -fold cross-validation) does not necessarily reduce the bias of classification-based aggregates. This is a pitfall especially if the base rate is low, i.e., when dealing with . Of course, correctly dealing with class-imbalanced data is well-studied @cite . However, as long as the classifier is not error-free, it will still result in biased aggregate predictions.
- An alternative is an that measures the bias on the aggregate level instead of on the individual level @cite . Other alternatives include averaging multiple (biased) estimators into a single, more accurate estimator @cite . Such alternatives will reduce the bias of classification-based aggregates, but our proposed method will completely remove it for sufficiently large test sets.
- An extensive review on Bayesian inference for categorical data analysis can be found in @cite . It specifically comments on the use of prior distributions and the lack of consensus about what noninformative means'' @cite . We will avert this discussion by analytically deriving the posterior distribution for the entire family of conjugate priors. We empirically evaluate two common prior choices; the uniform (flat) prior and the Jeffreys prior. In real-world applications, our bias correction method can be implemented for non-conjugate prior as well, by utilizing Markov chain Monte Carlo (MCMC) methods.
- . Most systems proposed so far for identifying (and tracking) moving objects in videos rely on 2D object detection in each frame, which is computationally expensive and does not jointly consider object(s') motion information. Some systems (e.g., @cite @cite ) have used explicit motion information ( optical flow) as a linking feature to associate detection regions or to smooth detection scores as a post-processing step. Those motion information is derived separately outside of the network and is not integrated organically with the network training. Inspired by the correlation and regression based trackers such as @cite @cite @cite @cite , @cite proposed a D &T framework which relies on a resNet-101 as frame-level feature extractor and two parallel region proposal networks (RPN) to generate 2D box proposals. The detected boxes are associated using a proposed ROI tracking module by computing the correlation map between the feature maps. Another work in @cite first uses SSD @cite to detect objects and extracts corresponding spatial features through ROI pooling to create per-frame feature. Then an association LSTM is proposed to regress and associate object locations given the frame-level feature tensor for past @math consecutive frames as input.
- In @cite , Sola, Turner and Viklund showed the existence of a phase transition in the ALE( @math ) model at @math by focussing on the regime @math . They showed, in this case, that if particles are taken to be slits, and the regularisation parameter @math is sufficiently small, then the clusters converge to macroscopic growing slits. Almost all other previous work relates to HL(0). In @cite , Norris and Turner showed that the HL(0) cluster converges to a disk with internal branching structure given by the Brownian web. More recently, Silvestri @cite analysed the fluctuations in HL(0) and showed that these converge to a log-correlated fractional Gaussian field. Several other papers consider modifications of the HL(0) model @cite @cite @cite .
- In this paper, we approach the question of a phase transition in ALE( @math ) at @math from the opposite side to @cite by focussing on the regime @math and showing the existence of a phase transition at the level of fluctuations. Our results apply in a more universal setting as we can consider a wide class of particle shapes, not just slits.
- Only few works have addressed the video domain, aimed at reproducing the visual explanations achieved in image-based models. Karpathy al @cite visualized the Long Short Term Memory (LSTM) cells. Bargal al @cite have studied class activations for action recognition in systems composed of 2D CNN classifiers combined with LSTMs for monitoring the temporal variations of the CNN outputs. Their approach was based on Excitation Backpropagation @cite . Their main focus was based on the decision made by Recurrent Neural Networks (RNNs) in action recognition and video captioning, with the convolutional blocks only being used as per-frame feature extractors. In 3D action recognition, Chattopadhay al @cite have proposed a generalized version of the class activation maps generalized for object recognition.
- The adversarial attacks and defenses have received significant attention from the machine learning community in the last couple of years @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . A wide range of defenses have been proposed to harden neural networks against adversarial attacks. However, a pattern has emerged in which the majority of adversarial defenses are broken by new attacks. For example, @cite , @cite , @cite , @cite broke several of these proposed defenses.
- The current state-of-the-art white-box attack methods are the iterative fast gradient sign method (I-FGSM) @cite @cite , DeepFool @cite , Carlini and Wagners attack (CW attack) @cite , elastic-net attacks to deep neural networks @cite , robust physical perturbations attack @cite , EOT attack @cite . In the white-box attacks, the network parameters are assumed to be visible to the attacker. Black-box adversarial attacks are also possible by computing universal adversarial perturbations @cite , using ensemble approaches @cite , using substitute models @cite @cite , employing zero-order optimization-based attacks @cite @cite .
- A number of certifiable'' defense mechanisms have been developed for certain classifiers. @cite harden a two-layer classifier using semidefinite programming, and @cite proposes a convex duality-based approach to adversarial training that works on sufficiently small adversarial perturbations with a quadratic adversarial loss, while @cite considers training a robust classifier using the convex outer adversarial polytope. @cite shows how applying interval bound propagation during training, combined with MILP-based exact verification, can lead to provably robust networks. These provable defenses, although very insightful, are either restrictive or are computationally expensive.
- In 2007, Heydt- @cite found the vulnerabilities in the first-generation RFID-enabled credit cards. Garcia @cite showed that MIFARE Classic, a widely used smart card, was vulnerable to reverse engineering and man-in-the-middle attack in 2009. D " u zenli @cite proposed a novel approach that applies neural network forecasting to security for closed-loop prepaid cards based on low-cost technologies such as RFID and 1-Wire in 2015. In 2017, @cite proposed a security solution of an RFID card through cryptography. @cite proposed a side-channel attack on a protected RFID card in 2018.
- Designing collaborative mechanisms in addition to the adversarial training has garnered growing interest for deep generative models. LeCun @cite promoted to replace the discriminator by a collaborator to provide encouraging feedback. A conceptually unadversarial framework is jokingly introduced in @cite . Several recent works present more concrete realizations of the collaborative paradigm. Chen @cite proposed a method for the generator and discriminator to collaborate on representation learning while competing on the generative task. A similar work @cite trained the generator and the discriminator collaboratively to learn relevant features for super-resolution. In contrast to these methods, our work is focused on collaboration mechanisms during sampling process.
- GAN-based sample modifications have been recently explored in several tasks, such as image editing @cite , image inpainting @cite , super-resolution @cite , adversarial defenses @cite . Most of these works update the prior of the generator in the latent space to seek an output closest to their target. Another recent work proposed to activate or deactivate a part of the middle layer of the generator with the involvement of human intervention @cite . Our work provides a more generic framework guided by the discriminator, which allows the sample refinement to be performed any layer of the generator and enables the refined samples to go beyond the data manifold learned by the generator.
- Our proposed discriminator shaping method can be viewed as a form of adversarial training, which is widely used in classification tasks to improve robustness @cite @cite @cite . Concurrent to our work, Zhou @cite proposed to train the discriminator adversarially in a restricted region around the generated samples in order to stabilize the GAN training. In our work, we shape the discriminator loss landscape in a wider range spanning from the generator distribution to the real distribution in order to more effectively guide the sample refinement process. @PARASPLIT
- In the context of indoor and outdoor robot navigation, most development has been based on the use of LIDAR or depth-sensing cameras for localization and avoidance @cite @cite . These approaches create a pointcloud from data captured by the LIDAR or depth camera. This pointcloud is then used to build a costmap @cite , which stores locations of obstacles and free space. Pathfinding algorithms can be used on this costmap to find a safe path for a robot to travel.
- There are two main approaches to pixel-wise segmentation: encoder-decoder CNNs @cite and atrous convolution with bilinear upsampling @cite . The former uses convolutions and pooling layers to downsample an input image, then uses learned deconvolutions @cite and or unpooling layers, along with skip connections to create an output image with the same resolution as the input and each pixel labeled with a classification. The latter uses atrous (dilated) convolutions to increase the effective receptive field of the convolutions while downsampling less than the encoder-decoder architecture, and uses simple bilinear upsampling on the output classifications to resize the output image to the same size as the input. Due to the atrous convolution's larger receptive field and ability to parse features at different scales, it has been much more successful at general-purpose semantic segmentation tasks than encoder-decoder architectures.
- Widely used trie-based indexes @cite @cite usually work the best with near uniform data distribution. To adapt them to less ideal data distributions, use dynamic fanout to optimize trie height @cite , remove unnecessary nodes @cite , and aggregate nodes to form a more balanced structure @cite .
- Many system optimizations can be approached by machine learning models trained from historical data. In the area of database, examples include cardinality estimation @cite @cite @cite @cite , join order planning @cite @cite @cite and configuration tuning @cite . Besides database, works have been done to improve buffer management systems @cite , sorting algorithms @cite , memory page prefetching @cite @cite and memory controller @cite and scheduling @cite . Many of these scenarios face similar challenges of dealing with shifting data distribution, which could be other applications of our model caching mechanism.
- Sparsifying parameters in learned neural network layers is an intuitive approach for compression. One general way for sparsification is pruning, which iteratively removes network parameters that have the least impact to the final prediction accuracy. For example, @cite proposed using a hard threshold formed by the product of a scalar and the standard deviation of the weights. Following this work, filter-level pruning methods @cite have been introduced which enables a more structured pruning strategy. Most recent, He al @cite proposed using AutoML for model compression, which gave larger acceleration rate than others. The problem with pruning based compression is that it cannot guarantee a good initialization for retraining and thus requires tedious iterative processing.
- Matrix quantization trades network representation precision of weights and activations for computational throughput. Xue al @cite used low-rank approximation to reduce parameter dimensions which saves storage and improves training and testing times. Denton al @cite explored matrix factorization methods for speeding up CNN inference time. These works showed it can be exploited to speed up CNN testing time by as much as 200
- knowledge distillation (KD) transfers knowledge from one or several large pre-trained teacher networks to a small student network which can achieve comparable capability of the teacher networks during inference. Hinton al @cite trained a small student network to match the soft targets of a cumbersome teacher network by setting a proper temperature parameter. Chen al @cite introduced cross sample similarities and brought the learning to rank technique to model compression and acceleration. Our approach is compatible with KD to achieve higher compression rate.
- Quantization methods group weights with similar values or reduce the number of required bits to reduce the number of parameters. Gong al @cite applied vector quantization to the parameter values. Han al @cite pruned the unimportant connections and retrained the sparsely connected networks, then quantized the link weights using weight sharing and Huffman coding to further reduce the model size. Binary-weight neural networks such as BinaryNet @cite and XNor-Net @cite use one binary bit to represent each weight parameter while maintaining the model accuracy. Our approach operates in a more principled way for compressing neural networks and this guarantees a faster converge speed during the re-training stage.
- The recognition of grocery products shares commonalities with the task addressed in @cite @cite @cite , which consists in recognizing a real-world example of a garment item based on the catalog of an online shop. Like our solution, these works rely on matching and retrieval using deep features extracted from CNN architectures. However, they leverage on labeled paired couple of samples depicting the same item in the and domain to learn a cross domain embedding at training time while our proposal leverages only on labeled images from one domain, thereby vastly relaxing the applicability constraint. Moreover, computer vision has been successfully applied in the retail environments for costumer profiling ( @cite ), automatic shelf surveying ( @cite ), visual market basket analysis ( @cite ) and automatic localization inside the store ( @cite ).
- Using CNNs to obtain rich image representations is nowadays an established approach to pursue image retrieval, both as a strong off-the-shelf baseline ( @cite ) and as a key component within more complex pipelines ( @cite ). @cite train a CNN using triplets of samples to create an embedding for face recognition and clustering. Since then, this approach has been used extensively to learn representations for a variety of different tasks, with more recent works advocating smart sampling strategies ( @cite ) or suitable regularizations ( @cite ) to ameliorate performance. Similarly to our proposal, @cite extend the idea of triplets by a novel formulation amenable to embed label structure and semantics at training time based on tuplets. Unlike @cite @cite , in this paper we propose to embed label structure within the learning process using only standard triplets; moreover our method uses only one exemplar image per class and augment the training set by a GAN trained jointly together with the embedding network.
- Few shot learning has been addressed successfully by @cite through classifiers trained on top of a fixed feature representation by artificially augmenting a small training set with transformations in the feature space. Yet, in the grocery product recognition scenario the items to be recognized at test time change quite frequently, which would mandate frequent retraining of new classifiers. Besides, as product packages exhibit very low intra-class variability, the generalization ability of a classifier may not be needed. Thus, we prefer to learn a strong image embedding and rely on K-NN similarly to perform recognition. In this respect our approach shares commonalities with @cite and @cite , where the authors address few shot learning by learning suitable embedding spaces and matching functions.
- Starting from the pioneering works of @cite and @cite , GANs have received ever increasing attention in the computer vision community as they enable to synthesize realistic images with few supervision. Recently GAN frameworks have been successfully deployed to accomplish image-to-image translation, with ( @cite ) and without ( @cite @cite ) direct supervision, as well as to tackle domain shift issues by forcing a classifier to learn invariant features @cite . We draw inspiration from these works and deploy a GAN at training time to pursue domain adaptation as well as to improve the effectiveness of the learned embedding. A related idea is proposed in @cite though, unlike @cite , (a) we explicitly deploy the GAN while learning the embedding to attain domain adaptation, (b) use only one sample per class and (c) train the GAN to produce realistic though hard to embed training samples, the of our GAN not only plays an adversarial game against the but also against the network that learns the embedding..
- Much work has been done in the area of automating speechreading by computers @cite @cite @cite . There are two main approaches to this task. The first, and the one most widely attempted in the past, consists of modeling speechreading as a visual-to-textual mapping. In this approach, the input video is manually segmented into short clips which contain either whole words from a predefined dictionary, or parts of words comprising phonemes, visemes @cite or characters. Then, visual features are extracted from the frames and fed to a classifier. Assael al @cite , Chung al @cite and others @cite @cite @cite have all recently showed state-of-the-art word and sentence-level classification results using neural network-based models.
- The second approach, and the one used in this work, is to model speechreading as a visual-to-acoustic mapping problem in which the label" of each short video segment is a corresponding feature vector representing the audio signal. Kello and Plaut @cite and Hueber and Bailly @cite attempted this approach using various sensors to record mouth movements, whereas Cornu and Milner @cite used active appearance model (AAM) visual features as input to a recurrent neural network.
- Our approach is closely related to recent speaker-dependent video-to-speech work by Ephrat and Peleg @cite , in which a convolutional neural network (CNN) is trained to map raw pixels of a speaker's face directly to audio features, which are subsequently converted into intelligible waveform. The differences between our approach and the one taken by @cite can by broken down into two parts: improvement of the encoder and redesign of the decoder.
- The goal of our encoder modification is to improve analysis of facial movements, and consists of a preprocessing step which registers the face to a canonical pose, the addition of an optical flow branch, and swapping the VGG-based architecture with a ResNet based one. Our decoder is designed to remedy a major flaw in @cite , namely the unnatural sound of the reconstructed speech it produces. To this end, we use the sound representation and post-processing network of @cite , which introduces longer-range dependency into the final speech reconstruction, resulting in smoother, higher quality speech. Section expounds on the above differences, and Section contains a comparison of the results of @cite to ours.
- Our work also builds upon recent work in neural sound synthesis using predicted spectrogram magnitude, including Tacotron (Wang al) @cite for speech synthesis, and the baseline model of NSynth (Engel al) for music synthesis. While Tacotron focuses on building a single-speaker text-to-speech (TTS) system, our paper focuses on building a single-speaker video-to-speech system.
- This work complements and improves upon previous efforts in a number of ways: Firstly, we explore how to better analyze the visual input, i.e. silent video frames, in order to produce an encoding which can be subsequently decoded into speech features. Secondly, while prior work has predicted only the output corresponding to a single video frame, we jointly generate audio features for a sequence of frames, as depicted in Figure , which improves the smoothness of the resulting audio. Thirdly, @cite focused on maximizing intelligibility at the expense of natural sounding speech on a relatively limited-vocabulary dataset. We aim to overcome the challenges of the more complex TCD-TIMIT @cite dataset, while optimizing for both intelligibility and natural-sounding speech.
- Data selection with the aim of model size and training time reduction has the objective to use the minimum amount of data while still maintaining high vocabulary coverage @cite @cite @cite . In a comparative study, find that similarity-objected methods perform best if the test domain and general corpus are very different, while a coverage-objected method is superior if test and general corpus are relatively similar. A comprehensive survey on data selection for SMT is provided by . While in this work we have used a similarity objective to rank our bitext, one could also apply dynamic data selection using a coverage objective.
- Some other previous work has addressed training efficiency for NMT, for example by parallelizing models or data @cite , modifying the NMT network structure @cite , decreasing the number of parameters through knowledge distillation @cite @cite , or by boosting parts of the data that are challenging' to the NMT system @cite . The latter is most related to our work since training data is also adjusted during training, however we reduce the training data size much more aggressively and study different techniques of data selection.
- In recent years, recognizing and understanding surgical procedures at different levels of granularity has been a focus of research @cite @cite . Surgical procedures can be generally broken down to four main levels, from higher to lower: phases, steps, tasks and motions @cite . At the higher level of surgical process modeling, statistical models have been proposed using recorded force and motion data @cite @cite , surgical tool usage @cite and video data @cite to classify surgery phases. Most existing work has addressed the recognition of activities using different techniques such as neural networks and Hidden Markov Models (HMM) @cite @cite . At the lower level, effort has been applied to detect surgical motion @cite @cite or model surgical gestures and classify them using different methods such as HMM and Linear Discriminant Analysis (LDA) @cite . A common drawback in these methods is that they are time consuming and require significant human interaction and pre-processing.
- The problem of finite-time blowup for wave maps attracted a lot of interest in the recent past. The bulk of the literature focuses on the two-dimensional case which is energy-critical. The existence of finite-time blowup for energy-critical wave maps into the two-sphere has first been observed numerically in the work of Bizo 'n-Chmaj-Tabor @cite . Rigorously, the existence of blowup solutions was proved by Krieger-Schlag-Tataru @cite , Rodnianski-Sterbenz @cite , and Rapha "el-Rodnianski @cite , see also @cite @cite . We remark that the blowup in the energy-critical case is of type II and proceeds by dynamical rescaling of a soliton, cf. @cite . In fact, there are by now powerful nonperturbative techniques for energy-critical equations which allow one to prove versions of the celebrated , see the work by C ^ote @cite , C ^ote-Kenig-Lawrie-Schlag @cite @cite , and, very recently, Jia-Kenig @cite , Duyckaerts-Jia-Kenig-Merle @cite , see also @cite @cite . Large-data global well-posedness and scattering is addressed in the fundamental work by Tataru-Sterbenz @cite @cite and Krieger-Schlag @cite .
- Facial expression analysis usually employs a three-stage training consisting of feature learning, feature selection, and classifier construction. These features can be either hand-designed or learned from training images. Then, a subset of the extracted features, which is the most effective to distinguish one expression from the others, is selected to facilitate an efficient classification and enhance the generalization capability. Finally, a classifier is constructed given the extracted feature set for each target facial expression. Recently, it has been demonstrated that expression recognition can benefit from performing these three stages together with certain multi-layer (i.e., deep) neural network architectures. As a result, the majority of deep learning techniques applied for facial expression recognition in-the-wild revolve around learning static discriminative templates via deep convolutional neural networks (DCNN) and using score aggregation for video classification to universal expressions @cite .
- @cite proposes a novel Boosted Deep Belief Network (BDBN) for performing the three training stages iteratively in a unified loopy framework. More recent studies adopt CNN architectures that permit feature extraction and recognition in an end-to-end framework. For instance, @cite employed an ensemble of multiple deep CNNs. @cite used three inception structures proposed by @cite in convolution for facial expression recognition. The Peak- Piloted Deep Network (PPDN) @cite is introduced to implicitly learn the evolution from non-peak to peak expressions.
- @cite proposed a hybrid approach, which combines top-down and bottom-up components, where top-down information is extracted using scene descriptors and bottom-up information is extracted by analyzing the face of the members of a group. Later, @cite came up with a multi-modal method combining face, upperbody and context information, where face upperbody is viewed as the bottom-up component and scene as top-down component. For representing an image, information aggregation was proposed to encode multiple peoples information for group-level image.
- In the recent GReco Sub-challenge of EmotiW 2016, the baseline feature used is the CENsus TRansform hISTogram (CENTRIST) descriptor @cite . @cite proposed a framework based on ensemble of features in LSTM and ordinal regression, which won the sub-challenge. The second place is the method from @cite , which is based on geometric features extracted from faces in an image. Partial least square regression is used to infer the group-level happiness intensity. @cite proposed a LSTM based approach and fined tuned the AlexNet model @cite by training on the Static Facial Expressions in the Wild (SFEW) dataset @cite and the HAPPEI dataset @cite .
- The dataset used in the experiment is the Happy People Images (HAPPEI) dataset @cite , which is a fully annotated dataset (by human labellers) for the task of group happiness level prediction. Every image in the data set is given a label between [0, 5]. The training and validation sets contain 2,836 images and 10,400 faces in total as shown in Table 1. However, the HAPPEI dataset is a highly unbalanced dataset, with most of the faces groups labelled as 3, but very few of them labelled as 0 or 5. Training directly with such skewed data may compromise the generalization ability of the whole system. * Table 1: HAPPEI dataset
- This paper is a continuation of our previous works @cite @cite . The differences between this paper and @cite @cite are as follows. First, the problem considered in this paper is machine learning with perceptron. In contrast, the problem dealt with in @cite @cite is association rule mining: @cite is for semi-honest agents on vertically partitioned databases, and @cite for dishonest agents on a centralized database. Second, the approaches are different. For mining association rules, quantum circuits are deployed to encode private information for semi-honest agents @cite and quantum tests are employed to detect cheat of dishonest agents @cite . But when learning a perceptron, classical training vectors have to be used to update the current classifier. This will cause privacy leakage. So the method in @cite @cite cannot be directly employed to preserve privacy in perceptron learning. Therefore, in this paper, a new quantum protocol is presented.
- Structure-based localization methods utilize a 3D scene representation obtained from SfM and find correspondences between 3D points and local features extracted from a query image establishing a set of 2D-3D matches. Finally, the camera pose is established by applying RANSAC loop in combination with a Perspective-n-Point algorithm @cite . However, descriptor matching is expensive and time-consuming procedure making camera relocalization complicated problem for large scale scenes such as urban environment. In order to accelerate this stage, @cite @cite eliminate correspondence search as soon as enough matches have been found, and @cite @cite propose to perform matching with the 3D points of top-retrieved database images. Sattler al @cite demonstrate that combining visual place recognition approaches with local SfM leads to better localization performance compared with 3D-based methods. However, the localization process itself is still very time-consuming.
- It has also been shown that machine learning methods have potential for providing efficient solutions to the pose estimation problem. Similar to structure-based localization approaches, Shotton al @cite utilize a regression forest to predict a 3D point location for each pixel of an input RGB-D image. Thus, the method establishes 2D-3D matches which are then used to recover 6-DoF camera pose by applying RANSAC. Rather then finding point correspondences, Valentin al @cite propose to exploit the uncertainty of the predicted 3D point locations during pose estimation. Brachmann al @cite propose a differentiable RANSAC method for camera localisation from an RGB image. However, it still requires dense depth maps in the training stage.
- The proposed approach is related to all previously discussed CNN-based methods, but it is the first one solving image-based localization problem via relative camera pose. Inspired by @cite @cite @cite , we apply Siamese neural network to predict relative orientation and relative translation between two views. These relative translation estimates are then triangulated to recover the absolute camera location. Compared with @cite @cite @cite @cite @cite , our study provides more general and scalable solution to image-based localization task. That is, the proposed approach is able to estimate 6-DoF camera pose for scenes registered to different coordinates frames, unlike existing CNN-based methods. Finally, compared with traditional machine learning approaches, our method does not require depth maps for training, thus it is applicable for outdoor scenes as well. Further details of our approach will be given in the following sections.
- Multi-dimensional RNNs @cite have been employed to model sequences in both temporal and spatial dimensions. Recent approaches @cite @cite model spatial sequences in an image to accomplish dense output for semantic segmentation tasks. Byeon al @cite utilized four 2D-LSTMs running in each direction, whereas Visin al. @cite employed two bi-directional RNNs as two layers for up-down and left-right spatial modelling. The key difference between these two and our approach is that we try to model spatial context by aggregating multiple patches as a 2D-grid of patches instead of modelling spatial context within a single patch. Both @cite and @cite model spatial context for natural images, whereas RANs can model much larger context in multi-gigapixel images.
- K-Nearest Neighbors (K-NN) based classifiers have been widely used to recognize gestures from the RSS data. In @cite and @cite , the authors used window-based statistical features (e.g. mean, variance, maximum, number of peaks, etc.) applied to a K-NN classifier for recognizing hand gestures on a smartphone. They achieved accuracies of @math 90 We depart from the literature works in four different ways: (1) we do not make any modifications to the existing hardware or software applications of the phone; (2) we introduce a new traffic induction approach to enable high-frequency RSS measurements; (3) we use custom but simple signal processing techniques and an efficient LSTM-RNN machine learning method to classify over-the-air hand gestures; (4) we share the experiment datasets and the partial source codes of our solution for the community to build this research further.
- To the best of our knowledge, very few works uses deep learning or neural network based methods to radio signal based activity gesture classification. For example, the authors in @cite used a Convolutional Neural Network (CNN) for classifying user driving behaviors based on narrow-band radio signals and achieved $88
- To get a prediction at the same resolution than the input image, Long, Shelhamer al proposed recently Fully Convolutional Networks (FCN) @cite by adding a deconvolution part after a classical convolutional neural network. The idea is that, after decreasing in the convolutional network, a deconvolution part, using upsampling operator and deconvolution (or fractionally-strided convolution) increases the feature maps size back to the input resolution. Noh al @cite extended this idea by using maximum unpooling upsampling operators in the deconvolution part. The deconvolution network is the symmetric of the convolution one and each maximum pooling operation in the convolution is linked to a maximum unpooling one in the deconvolution by sharing the pooling positions. Ronneberger al @cite are going even further with their U-Net by concatenating the feature maps obtained in the convolution part with feature maps of the deconvolution part to allow a better reconstruction of the segmented image. Finally, Lin al @cite used the same idea of U-Net but instead of concatenating the feature maps directly, they used a refineNet unit, containing residuals units, multi-resolutions fusions and chained residual pooling, allowing the network to learn a better semantic transformation.
- All of these networks are based on the idea that subsampling is important to increase the receptive field and try to override the side effect of resolution loss with deconvolutionnal technics. In our , composed of multiple streams working at different feature map sizes, we use the subsampling and upsampling operators as connectors between streams allowing the network to take decisions at any resolution. The upsampling operators are not used to correct this side effect but to allow multi-scale decisions in the network. In a recent work, Newell al @cite stacked many U-Net showing that successive steps of subsampling and upsampling are important to improve the performance of the network. This idea is improved in our with the strong connections between streams.
- With their Full Resolution Residual Network (FRRN) @cite , Pohlen al combine a conv-deconv network with a residual one. They also use different streams but only two of them: one for the residual network linked with upsampling and subsampling operations, and one for the conv-deconv network which does not have any residual connections. subsumes FRNN and can be seen as a generalisation of this network.
- The idea of networks with multiple paths is not new @cite @cite @cite . Zhou al studied a face parsing task with interlinked convolutional neural networks @cite . An input image is used at different resolutions by multiple CNN whose feature maps are interconnected. Huand al @cite use the same architecture but make it dynamically adaptable to computational resource limits at test time. Recently, Saxena al have presented Convolutional Neural Fabrics @cite which structure forming a grid is very similar to ours and which also use the full multi-scale grid to make predictions. However, to better scale to full resolution images and large size datasets, we make use of residual units and we introduce a new dropout technique to better train our grids. Besides, we constrain our network, similarly to conv-deconv ones, to have down-sampling layers, followed by upsampling blocks, where @cite use up and down sampling across all network layers.
- Our work is also related to which refers to training a smaller student'' network to perform better by learning from a larger teacher'' network. We adopt this terminology and refer to pre-trained word embeddings as the teacher and sub-lexical embeddings as the student. This problem has mostly been considered for classification and framed as matching the probabilities of the student to the probabilities of the teacher @cite @cite @cite . In contrast, we work directly with representations in Euclidean space.
- E-advice technology offers a form of electronic word-of-mouth'', with new potential for gathering valid suggestions that guides the consumer's choice. Since some years, extensive and nationally representative surveys have been carried out, to evaluate the specific aspects of ratings information that affect people attitudes toward e-commerce''. It is the case, e.g., of work in @cite , which highlights how people, while taking into accounts the average of ratings for a product, still do not take care of the number of reviews leading to that average. The high impact of reviews on consumers is also testified by the fact that a positive (or negative) review about a product can be as effective as a recommendation by a friend see, e.g., https: www.brightlocal.com learn local-consumer-review-survey (All URLs accessed on June 7, 2017) . Further, positive comments convey a series of strong benefits, like, e.g., an improvement in search engines' ranking, a stronger perception of trust, and increased sales @cite @cite @cite .
- Polarity detection techniques fall under the wide umbrella of sentiment analysis @cite @cite . Several approaches have been proposed in the literature for polarity detection. A significant branch rely on lexicon-based features, due to the availability of lexical resources for sentiment analysis, such as, e.g., the lexicons SenticNet, SentiWordNet and a Twitter opinion lexicon, proposed in @cite @cite @cite @cite , respectively. Usually, lexicon-based approaches involve the extraction of term polarities from the sentiment lexicons and the aggregation of the single polarities to predict the overall sentiment of a piece of text.
- Concerning subjectivity in texts, i.e., those expressions representing opinions and speculations, work in @cite is one of the first studies to perform subjectivity analysis, to identify subjective sentences and their features. In the specific field of polarity detection applied to product reviews, work in @cite assigns a numerical score to a textual description exploiting the SentiWordNet lexicon: the task is especially useful when a reviews platform only allows to leave a text as a review, without an associated numerical score. A more recent work in @cite considers analogous topics. Work in @cite proposes an unsupervised approach that involves the extraction of terms and slangs polarities from three sentiment lexicons and the aggregation of such scores to predict the overall sentiment of a tweet . @cite @cite @cite , the authors consider the contextual polarity of a word, i.e., the polarity acquired by the word contextually to the sentence in which it appears. For a survey of sentiment analysis algorithms and applications, the interested reader can refer to @cite . For the specific scenario of polarity evaluation and sentiment analysis in specific social networks, the interested reader can refer to the series of work in @cite @cite , inherent to Twitter.
- This brief overview of the literature shows heterogeneous techniques and applications for polarity detection, both supervised and unsupervised, in different contexts and for different goals. In this work, thanks to the availability of a labeled dataset, we exploit a supervised approach, which automatically learns a model from the annotated data. In order to choose the most effective algorithm, we test different supervised algorithms and we finally select a linear Support Vector Machine (SVM) @cite , due to its efficiency in dealing with the task at hand.
- Popularity prediction of online content has gained a lot of attention within the research community due to the ubiquity of Internet and a stunning increase in the number of its users @cite @cite @cite . None of the above mentioned works, however, considers using visual information for popularity prediction, while the method proposed in this paper relies solely on the visual cues.
- The first attempt to use visual cues in the context of popularity prediction is the work of @cite , where they address the problem of popularity prediction for online images available on Flickr. Using a dataset of over 2 million images, the authors train a set of Support Vector Machines using visual features such as image color histogram or deep neural network outputs and apply them in the context of popularity prediction. Following this methodology, we use recently proposed neural network architectures for the purpose of video popularity prediction based on visual cues only.
- Person search can be regarded as the combination of person re-identification and person detection. Most of existing works of person re-identification focus on designing hand-crafted discriminative features @cite @cite @cite , learning deep learning based high-level features @cite @cite @cite @cite @cite @cite and learning distance metrics @cite @cite @cite @cite @cite . Recent deep learning based person re-identification methods @cite @cite @cite @cite re-design the structure of the deep network to improve performance. For instance, @cite designed two novel layers to capture relationships between two views of a person pair. Among distance metric learning methods, @cite proposed KISSME (KISS MEtric) to learn a distance metric from equivalence constraints. Additionally, @cite proposed to solve the person re-id problem by learning a discriminative null space of the training samples while @cite proposed a method learning a shared subspace across different scales to address the low resolution person re-identification problem.
- Recently, LSTM based attention methods have shown good performance in image description @cite , action recognition @cite @cite and person re-identification @cite . In @cite , Xu showed how the learned attention can be exploited to give more interpretability into the model generation process, while @cite @cite adopted attention methods to recognize important elements in video frames based on the action that is being performed. Moreover, @cite formulated an attention model as a triplet recurrent neural network which dynamically generates comparative attention location maps for person re-identification. Analogously, our proposed NPSM also has such a locally emphasizing property, but NPSM is a query-aware model while the above attention-based methods all adopt a blind attention mechanism.
- The first optimal-time and linear-space solution to document listing is due to Muthukrishnan @cite , who solves the problem in @math time using an index of @math bits of space. Later solutions @cite @cite improved the space to essentially the statistical entropy of @math , at the price of multiplying the times by low-order polylogs of @math (e.g., @math time with @math bits on top of the entropy @cite @cite ). However, statistical entropy does not capture repetitiveness well @cite , and thus these solutions are not satisfactory in repetitive collections.
- One precedent is Claude and Munro's index based on grammar compression @cite . It builds on a grammar-based pattern-matching index @cite and adds an inverted index that explicitly indicates the documents where each nonterminal appears; this inverted index is also grammar-compressed. To obtain the answer, an unbounded number of those lists of documents must be merged. No relevant worst-case time or space guarantees are offered.
- Another precedent is ILCP @cite , where it is shown that an array formed by interleaving the longest common prefix arrays of the documents in the order of the global suffix array, ILCP, has long increasing runs on repetitive collections. Then an index of size bounded by the runs in the suffix array @cite and in the ILCP array performs document listing in time @math , where @math and @math are the search and lookup time, respectively, of a run-length compressed suffix array @cite @cite . Yet, there are only average-case bounds for the size of the structure in terms of @math : If the base document is generated at random and the edits are spread at random, then the structure uses @math bits on average.
- The last previous work is PDL @cite , which stores inverted lists at sampled nodes in the suffix tree of @math , and then grammar-compresses the set of inverted lists. For a sampling step @math , it requires @math bits plus the (unbounded) space of the inverted lists. Searches that lead to the sampled nodes have their answers precomputed, whereas the others cover a suffix array range of size @math and are solved by brute force in time @math .
- To be fair, those indexes perform well in many practical situations @cite . However, in this article we are interested in whether providing worst-case guarantees in time and space.
- Computers are increasingly being used to perform music-related tasks (automated music analysis, music recommendation, composition, etc). To perform such tasks reliably, there is a need for computers to grasp concepts that are relevant to our perception and understanding of music @cite . Empirical findings from music psychology are valuable in this respect, since they shed light on the process of human music perception and cognition.
- To model these types of findings, computational models of tonal perception typically aim to provide methods that, given a musical context, compute a response that can be judged to be more or less appropriate for the implicit tonality of that context. Given the predominance of the probe-tone paradigm for studies of human tonal perception, a common practice is to elicit a -goodness-of-fit response from the model for a probe-tone given a musical stimulus, such that the responses can be compared to human probe-tone ratings (e.g. @cite @cite @cite @cite ). Another way to judge the responses is to define a metric over the responses and compare the resulting topology to geometric constructs from music theory, such as the Tonnetz @cite , a toroidal representation of key distance @cite , or the circle of fifths @cite .
- The computational models proposed in the literature tend to emphasize one of various different factors that are thought to play a role in tonal perception. Whereas some works seek to explain empirical results mainly by a computational account of the lower levels of the auditory system @cite @cite , others focus more strongly on the role of long-term memory in tonal perception @cite @cite @cite .
- Another limitation of most long-term memory models for tonal learning is that they work with stimuli that are reduced in one or more ways. For example, the input may consist of discrete representations of tones such as MIDI note numbers @cite , pitch classes @cite , artificial harmonic representations @cite , or of artificial harmonic sounds such as Shepard tones @cite . Furthermore, the musical material that a model is exposed to may be limited to monophonic melodic lines @cite , sets of chords or harmonic cadences @cite , or even a set of probe-tone profiles @cite . A notable exception to this is @cite , which uses an audio recording of Bach's Well Tempered Clavier (WTC), performed on a harpsichord, to train a SOM by converting the acoustic signal to . The work of @cite also takes an ecological approach by using real audio and plausible psychological representations, with multiple representations along the sensory-cognitive spectrum, to better account for human tonal expectation.
- @cite defines 3D VLEs as computer-generated, three-dimensional simulated environments that are coupled with well-defined learning objectives.'' Students enter these environments as avatars via a networked computer and interact with each other with text chat, voice chat, and the manipulation of shared objects. Adding VR to VLEs can offer efficient generalization of skills from a virtual environment to the real world, since the environment mimics real world imagery and context as noted by @cite . @cite presents a review and investigation into VRLE's and the reported benefits these pedagogic systems provide to the students, as well as how they impact students' learning journeys within the program of study. The authors conclude that a correctly implemented VLE improves student performance and is correlated with improved engagement.
- @cite presents a comprehensive review of virtual reality-based instruction research and concludes that VR environments are effective for teaching K-12 and higher education. These results are used by instructional designers to design VRLEs.
- There is a variety of conditional planners, despite the hardness of computing conditional plans: even for polynomially bounded plans with limited number of sensing actions, its complexity is PSPACE-complete @cite . Some of these planners are online, such as CLG @cite , K-Planner @cite , SDR @cite , HCP @cite and CPOR @cite . Since sensing is done online, these solvers do not end up handling many contingencies and thus sometimes do not reach a goal state. Some of the conditional planners are offline, such as Contingent-FF @cite , POND @cite , PKS @cite , @cite , DNFct @cite , PO-PRP @cite , HCplan @cite . These planners construct whole conditional plans with decision points for sensing outcomes, where each execution via a branch of the plan leads to a goal state. Our planner is an offline planner.
- Offline conditional planners can be further classified into two groups: search-based approaches and compilation-based approaches. The former group views conditional planning as a nondeterministic search problem in belief space (e.g., Contingent-FF, PKS, POND), whereas most of the planners in the latter group compiles a conditional planning problem into many conformant planning problems by deciding for the order of sensing actions and then computing the branches of the conditional plan by means of conformant planning (e.g., CLG, DNFct, PO-PRP). Recall that conformant planning considers incomplete initial state and no observability, and its aim is to find an action sequence that reaches the goal for every possible initial state @cite . Our planner can be characterized as a compilation-based planner since we compile a conditional planning problem into many sequential planning problems.
- has similarities with HCplan: they both compute hybrid conditional plans, utilize parallel computation of branches, use nonmonotonic default constructs to represent nonoccurrences of sensing actions, and outcomes of sensing actions are determined nondeterministically. They do not have to decide for the order of sensing actions in advance. On the other hand, the input language for HCplan is @cite , while it is answer set programming (ASP) @cite for . In HCplan, outcomes of sensing actions are nondeterministically decided by external computations, whereas in they are nondeterministically decided by ASP. Moreover, keeps track of visited belief states and avoids recomputing subtrees of a hybrid conditional plan.
- Although not compilation-based, the offline conditional planner is also related to : It is not hybrid, not parallel, and its input language is the action language @math ; but it uses ASP to compute conditional plans. Based on 0-approximations @cite , the idea is to approximate a belief state by a consistent set of fluent literals that are known to be true, and to define transitions considering what definitely holds and what may change at the next state. With such an approximation, the computational complexity of conditional planning becomes NP-complete. Given upper bounds for the height and the number of branches, generates a conditional plan with one call of an ASP solver based on an intelligent formulation of possible transitions. does not consider 0-approximations of belief states, but requires that the initial values of fluents that cannot be identified by any sensing action are known in advance, decided nondeterministically (e.g., by a disjunction over the possible values), or specified with respect to some assumptions (e.g., by defaults). Since is compilation-based, it calls an ASP solver multiple times to build a conditional plan.
- There is a variety of recent work on hybrid planners that combine motion planning with classical planners. Some of them are based on modifications introductions of search algorithms for motion task planning @cite @cite @cite @cite @cite @cite , some of them are based on formal methods @cite @cite , and some of them are based on modification of representation of domains @cite @cite @cite @cite . is similar to the latter group, since feasibility checks are embedded in action descriptions via external atoms (in the spirit of semantic attachments in theorem proving @cite ) without having to modify the classical planners or motion planners. We refer the reader to the recent studies @cite @cite surveying and empirically analyzing some of these approaches.
- The result of @cite has motivated a long series of works on the maximization of non-monotone submodular functions subject to a matroid constraint @cite @cite @cite @cite @cite . The currently best algorithm of this kind achieves an approximation ratio of @math @cite for general matroid constraints, and no better approximation guarantee is known for uniform matroid constraints. On the inapproximability side, it is known that no polynomial time algorithm can achieve approximation ratios better than @math and @math for the maximization of non-monotone submodular functions subject to uniform and general matroid constraints, respectively. Hence, for non-monotone submodular functions there is still a large gap between the best known approximation and inapproximability results. This gap is somewhat bridged by a result of @cite giving a @math -approximation algorithm for the maximization of a special class of non-monotone submodular functions, known as symmetric submodular functions, subject to a general matroid constraint.
- Our work is inspired by Yin, Porikli and Collins @cite who proposed a single target tracker for aerial imagery and @cite who used different set of visual features to extract feature likelihood maps and adaptively fuse them to obtain a single fusion map. While hyperspectral imagery has shown to have potential for vehicle tracking @cite @cite @cite @cite @cite @cite @cite , the sensors are rare and expensive and hence most of the aerial tracking work has been done in infrared, single-band or RGB. In this section, we review the efforts done in the above sensor domains to solve vehicle detection and tracking and how hyperspectral imagery could possibly solve some of the problems.
- Infrared imagery is helpful for object detection and tracking since it can pick up heat signatures emitted by the objects that conventional cameras are not capable of seeing. It has the ability to penetrate through smoke, fog and is generally insensitive to changes in light condition. The COCOALIGHT system @cite proposed by consists of three phases - motion compensation, object detection and lastly, object tracking within and beyond the visible spectrum. The authors used feature-based (e.g. KLT @cite ) image registration to compensate for motion of the aerial imaging platform and then adopted cumulative frame differencing to detect and track foreground objects. @cite proposed ATR-Seg, a shape-aware manifold-learning based algorithm for joint tracking-recognition-segmentation in infrared imagery. @cite present two new frameworks that use local clustering segmentation and kernel-based tracking theory to improve accuracy in target detection and tracking. However, since thermal imagery does not give unique fingerprints for different objects of the same category and is very contrast dependent, the above methods suffer during cluttered scenarios.
- Detection, tracking and most recently, the counting of targets from satellite or aerial platform are useful for both commercial and government purposes. @cite uses a disparity maps based approach with prior information of the road structure, orientation during image capture, and a global digital elevation model (DEM) to narrow down areas that can have vehicles. They then use an offline-trained histogram of oriented gradients (HoG) classifier for vehicle detection. @cite proposed an online discriminative feature selection method that couples the classifier score with the importance of samples, leading to a more robust and efficient tracker. Yousefhussien, Browning and Kanan @cite propose the Smooth Pursuit Tracking (SPT) algorithm which uses three kinds of saliency maps: appearance, location, and motion to track objects under all conditions, including long-term occlusion. Elliethy and Sharma @cite proposed an innovative approach to register captured WAMI frames with vector road map data(Open Street Map @cite ) and track vehicles within those registered frames simultaneously, leading to efficient results.
- Perhaps the most famous multiple-access communication protocol is the (slotted) ALOHA protocol @cite @cite . Follow-up papers study the efficiency of multiple-access protocols for packets that are generated by some stochastic process (see e.g. @cite @cite @cite ), or worst-case scenarios of bursty inputs @cite .
- There are various game theoretic models of slotted ALOHA that have been studied in the literature, apart from the ones mentioned in the introduction; see for example @cite @cite @cite . However, in most of these models only transmission protocols that always transmit with the same probability are considered. There has been also research on pricing schemes @cite as well as on cases in which the channel quality changes dynamically with time and players must choose their transmission levels accordingly @cite @cite @cite . An interesting game-theoretic model that lies between the contention and congestion model was studied in @cite ; where decisions of when to submit is part of the action space of the players.
- IoT Middleware is an active research area as well. The @cite has proposed an SOA architecture for mobile IoT, with a focus on the functional scalability. A novel probablistic registry allows low-latency approximate queries for registered sensing and actuation services. It does static scheduling of streaming service dataflows using the Dioptase middleware @cite , and interfacing across heterogeneous IoT protocols using an Enterprise Service Bus (ESB). in contrast supports hybrid data models -- a higher level abstraction than protocols, offers richer composition semantics including delegating to external engines, and uses point-to-point communication between tasks (push and pull) rather than a central ESB. Our Hypercat registry is simpler but based on BSI standards, and can be replaced or federated for scaling. Advanced scheduling algorithms @cite or device mobility is not a focus in this paper, but future work.
- @cite has proposed a programming model for composing IoT applications across mobile, Fog and Cloud layers. They consider a multi-way 3-level dataflow model with computation starting in the Cloud, elastic resources acquired in the Cloud and Fog, and communication possible between all 3 layers. Each edge has one Fog parent based on spatial proximity, that may be reassigned. While a useful abstraction, their strictly hierarchical resource and dataflow model are much more restrictive that our use of any network topology and a directed graph as dataflow. Theirs effectively degenerates to a client-server model.
- are precursors to IoT where mobile phones off-load applications to Cloud resources. @cite , mobile data stream applications are dynamically partitioned for computation across mobile devices and Cloud. They propose a genetic algorithm for the partitioning to maximize throughput and adapt to changing devices load. They are limited to mobile data stream applications rather than dataflow or hybrid data models We also support Fog resources, native runtime engines and dynamic migration of tasks among the resources. The Hybrid Mobile Edge Computing (HMEC) architecture @cite uses edge devices for mobile applications. They use a peer-to-peer (P2P) approach of both proximate and distant edge devices, and perform to improve performance and reduce energy usage. Similarly, @cite offloads tasks to the Cloud using RPC with static analysis and dynamic profiling of mobile applications. It maintains a complete device clone in the Cloud, which can be costly. These are designed for monolithic existing mobile applications rather than dataflow composition, and neither consider a service paradigm or Fog servers.
- like Seti@home @cite have targeted the use of idle compute capacity in desktops. However, some of the inherent P2P characteristics are missing in an IoT scenario. Device churn is a major factor in P2P but less so for infrastructure IoT, or even mobile devices that are typically within cell communication. This, coupled with the growth of global Cloud data centers, make it feasible for centralized services for coordination. Dataflow composition is also a non-goal for such P2P systems that typically use a task-queue model for opportunistic computing.
- Different from most traditional metric learning methods that focus on learning a Mahalanobis distance in Euclidean space @cite @cite or high dimensional kernel space @cite , deep metric learning usually transforms the raw features via DNNs, and then compare the samples in Euclidean space directly.
- In @cite , Bucila al first proposed to approximate an ensemble of classifiers with a single neural network. Recently, Hinton al revived this idea under the name knowledge distill @cite . The insight comes from that the softened probabilities output by classifiers encode more accurate embedding of each sample in the label space than one-hot labels. Consequently, in addition to the original training targets, they proposed to use soft targets from teacher networks to guide the training of student networks. Through this process, KD transfers more precise supervision signal to student networks, and therefore improves their generalization ability. Subsequent works FitNets @cite , Attention Transfer @cite and Neuron Selectivity Transfer @cite tried to exploit other knowledges in intermediate feature maps of CNNs to improve the performance. Instead of using forward input-output pairs, Czarnecki al tried to utilize the gradients with respect to input of teacher network for knowledge transfer with Sobolev training @cite . In this paper, we exploit a unique type of knowledge inside deep metric learning model -- cross sample similarities to train a better student network.
- Learning to rank refers to the problem that given a query, rank a list of samples according to their similarities. Most learning to rank methods can be divided into three types: pointwise, pairwise and listwise, according to the way of assembling samples. Pointwise approaches @cite @cite directly optimize the relevance label or similarity score between the query and each candidate; while pairwise approaches compare the relative relevance or similarity of two candidates. Representative works of pairwise ranking include Ranking SVM @cite and Lambda Rank @cite . Listwise methods either directly optimize the ranking evaluation metric or maximize the likelihood of the ground-truth rank. SVM MAP @cite , ListNet @cite and ListMLE @cite fall in this category. In this paper, we introduce listwise ranking loss into deep metric learning, and utilize it to transfer the soft similarities between candidates and the query into student models.
- Introspective classification @cite @cite has been proposed in the context of classification systems employed in safety-critical applications where errors have severe consequences. In contrast to the commonly used precision-recall metrics, these works introduce introspective capacity as a metric for choosing classification algorithms. In particular, they argue that when presented with an unusual test datum, classification algorithms must respond with high uncertainty. Similarly, recent works have focused on estimating the difficulty of a given input image or location in order to make informed decisions. @cite propose the ALERT framework which assesses the difficulty of a given input image irrespective of the specific algorithm used for a vision task. More recently, Daftry et. al. @cite apply a similar idea by using spatio-temporal features from a deep network. In a similar fashion, Gur a u et. al. @cite build a place specific performance record of a vision system which facilitates self-evaluation. However, these methods cannot be directly used to flag missed objects. In contrast, our system would benefit from reliable confidence estimates as this would reduce false positives of object detectors.
- In a similar spirit to our proposed approach, differences in output from pairs of mirror images @cite or from multiple models processing the same image @cite have been used to evaluate vision systems. In @cite , mirrorability is proposed as a measure for evaluating performance of vision systems. However, a typical ensemble detector could consider mirroring as one of the test time data augmentation techniques amongst others such as cropping and shifting. In @cite , multiple deep neural networks addressing a common task are cross-referenced and the inconsistencies are used to flag errors of individual networks. This is in principle similar to using an ensemble of single frame object detectors where the ensemble is expected to be a better object detector than individual components. Such ensemble detectors, either with test time data augmentation or multiple separate models trained for the same task, could easily replace the individual detectors in our proposed system and such work compliments the proposed approach. As the ensemble detector is still single image-based it is not using the spatially or temporal information exploited here.
- Sensor fusion poses a related problem by using multiple modalities to understand the world and in some cases identify errors or provide self-supervised labeling @cite . However, in this work we focus on identifying errors within camera imagery alone. This enables single frame object detectors to be independently validated and improved without the confounds of other modalities.
- The dimension reduction consists in transforming data represented in a large space into a representation in a space of smaller dimension @cite . In many fields, the dimension reduction is considered as a very important step because it facilitates the classification, visualization or compression of large data. The main goal here is to limit the effect of issues caused by the high dimensionality data @cite .
- Software risk management constitutes means to efficiently assess and control risk factors affecting the overall software development activities @cite (planned ones and deviations) and is often associated with project management for adults'' as baptised by @cite . The importance of risk management for software engineering in general and requirements engineering in particular has been addressed in several risk management approaches tailored to software engineering processes @cite . Already the spiral model of Boehm @cite explicitly includes risk management within software development. The Riskit method @cite provides a sound theoretical foundation of risk management with a focus on the qualitative understanding of risks before their possible quantification. Karolak @cite proposes a risk management process for software engineering that contains the activities risk identification, risk strategy, risk assessment, risk mitigation, and risk prediction. With ISO 31000 @cite , which was released in 2009, there is even a family of standards for risk management available that can also be instantiated in software engineering and its subareas like testing, where ISO 31000 has already been applied in the context of risk-based testing @cite , or requirements engineering.
- A recent study on industrial development practices @cite shows that practitioners see the need to explicitly include traditional risk management approaches into RE that tends to be done rather agile. This further strengthens our confidence in the need to tailor risk management approaches to the particularities of RE actively taking into account the volatility therein (as discussed in our introduction).
- In fact, most work on risk management in the context of requirements engineering focuses on identifying risks in a bottom-up approach and analysing risks during the requirements engineering process. For instance, @cite provide a goal-driven approach for risk assessment in requirements engineering, and @cite provide an estimation approach and respective tool support @cite . For risk management within requirements engineering itself, @cite provide a list of top risks in requirements engineering, which includes overlooking a crucial requirement, inadequate customer representation, modelling only functional requirements, not inspecting requirements, attempting to perfect requirements before beginning construction as well as representing requirements in the form of designs. However, evidence-based approaches to risk management in requirements engineering as proposed in this paper are so far not available. For such an approach, we need a proper empirical basis on requirements engineering which has been recently established by the NaPiRE (Naming the Pain in Requirements Engineering) initiative.
- Prior work has focused quite extensively on the analysis and the detection of sybil and or fake accounts in online social networks by relying on tightly-knit community structures @cite @cite @cite @cite @cite @cite . By contrast, we work to detect accounts that are employed by like farms to boost the number of Facebook page likes, whether they are operated by a bot or a human. We highlight several characteristics about the social structure and activity of fake profiles attracted by the honeypot pages, e.g., their interconnected nature or the activity bursts. In fact, our analysis does not only confirm a few insights used by sybil detection algorithms but also reveals new patterns that could complement them. Fraud and fake activities are not restricted to social network, but widespread also on other platforms, such as online gaming. In this context, @cite rely on self-similarity to effectively measure the frequency of repeated activities per player over time, and use it to identify bots. Also, @cite analyze the characteristics of the ecosystem of multiplayer online role-playing games (MMORPGs), and devise a method for detecting gold farming groups (GFGs), based on graph techniques.
- Thomas al @cite analyzed trafficking of fake accounts in Twitter. They bought fake profiles from 27 merchants and developed a classifier to detect these fake accounts. In a similar study, Stringhini al @cite @cite analyzed the market of Twitter followers , which, akin to Facebook like farms, provide Twitter followers for sale. Note that Twitter follower markets differ from Facebook like farms as Twitter entails a follower-followee relationship among users, while Facebook friendships imply a bidirectional relationships. Also, there is no equivalent of liking a Facebook page in the Twitter ecosystem.
- Remarks on New Material''. Compared to our preliminary results (published in @cite and reported in ), this article clearly introduces significant additional new material. Specifically: (i) we introduce an empirical evaluation demonstrating that temporal and social graph analysis can only be used to detect naive farms (), and (ii) we present a novel timeline-based classifier geared to detect accounts from stealthy like farms with a remarkably high degree of accuracy (Sections and ).
- In our opinion, existing tools do not adequately consider developers' needs. In a recent study, Petre observed that software developers will not adopt tools and ideologies at odds with their considered practice'' @cite . note that a tool integrating sketches into the software development workflow must support a broad range of work styles @cite , which most of the above mentioned tools do not achieve.
- More recently, @cite proposed concept of dynamic images. The dynamic image is based on the rank pooling concept @cite and is obtained through the parameters of a ranking machine that encodes the temporal evolution of the frames of the video. Dynamic images are obtained by directly applying rank pooling on the raw image pixels of a video producing a single RGB image per video. And finally, by feeding the dynamic image to any CNN architecture for image analysis, it can be used to classify actions.
- Deep recurrent CNN architectures are also explored to model dependencies across the frames. In @cite , convolutional features are fed into an LSTM network to model temporal dependencies. @cite considered four networks to address action recognition in videos. The first network is similar to spatial network in the two stream architecture. The second network is a CNN with one recurrent layer, it expects a single optical flow image and in recurrent layer, optical flows over a range of frames are combined. In the third network, they feed a stack of consecutive frames, the network is also equipped with a recurrent layer to capture the long term dependencies. Similarly, the fourth network expects a stack of optical flow fields as input. However, the network is equipped with a fully connected recurrent layer. Finally, boosting is used to combine the output of all four networks.
- Already in the seventies, Garey and Johnson @cite proved that Connected Vertex Cover is -complete even for planar graphs of maximum degree 4. Later, Priyadarsini and Hemalatha @cite strengthened this result to 2-connected planar graphs of maximum degree 4, and Fernau and Manlove @cite strengthened it to planar bipartite graphs of maximum degree 4. Wanatabe, Kajita, and Onaga @cite proved -completeness of Connected Vertex Cover for 3-connected graphs. More recently, Munaro @cite established -completeness of Connected Vertex Cover for line graphs of planar cubic bipartite graphs and for planar bipartite graphs of arbitrarily large girth. Ueno, Kajitani, and Gotoh @cite proved that Connected Vertex Cover can be solved in polynomial time for graphs of maximum degree at most 3 and for trees. Escoffier, Gourv es, and Monnot @cite improved the latter result by showing that Connected Vertex Cover is polynomial-time solvable for chordal graphs. It is known that Vertex Cover is polynomial-time solvable for chordal graphs as well. The same authors proposed to study the complexity of Connected Vertex Cover for other graph classes for which Vertex Cover is polynomial-time solvable.
- Grigoriev and Sitters @cite proved that Connected Feedback Vertex Set is -complete for planar graphs with maximum degree 9. Besides this result not much is known on the computational complexity of this problem except that it is fixed parameter tractable when parameterized by @math @cite . The Connected Odd Cycle Transversal has been mainly studied from parameterized point of view @cite @cite . It is implicit in these works that Connected Odd Cycle Transversal is -complete, though we were not able to find any proof of this (simple) result. For the sake of completeness, we note now that it is implied by the stronger results we present in Theorems and .
- Differently from confidence estimation, which has been widely studied over the last two decades @cite @cite , ASR QE is a rather new task. The problem, at its core, shows a strong parallelism with QE in the machine translation (MT) field, where the goal of bypassing the need of manually-created reference translations has motivated a large body of research. The motivations (cost effective quality prediction at run-time) and the methods (supervised learning, either as regression or multiclass classification) are indeed the same in both fields. For a complete overview of the current approaches to MT QE, we refer the reader to the comprehensive overviews published within the yearly Workshops on Statistical Machine Translation @cite @cite @cite @cite and to the works dealing with quality prediction at word level @cite @cite , sentence level @cite @cite @cite @cite and document level @cite .
- In the ASR field, confidence estimation strongly relies on knowledge about the inner workings of the decoder that produces the transcriptions. Such knowledge includes acoustic and language model scores and word timing information associated to arcs of word lattices. QE, instead, approaches the problem as a system-independent task, in which this information is not necessarily accessible. In the first work along this direction @cite , we explored ASR QE as a supervised regression problem in which the WER of an utterance transcription has to be automatically predicted. Given a set of (, , ) triplets as training instances, different algorithms were evaluated on test data consisting of unseen (, ) pairs. All models used an effective set of signal textual hybrid features, named black-box features in contrast with the glass-box ones that model the internal behavior of the decoder. Results showed that ASR QE predictions can closely approximate the true'' WER scores calculated over reference transcripts, outperforming a strong baseline in a variety of test conditions of increasing complexity. Interestingly, when combined with the glass-box features ( confidence scores), the predictions are better than those obtained by using confidence scores alone.
- In @cite , ASR QE was explored by focusing on the problem of domain mismatches between training and test data. Indeed, as pointed out by @cite , simple supervised learning methods are very sensitive to large variations in the distribution of the instances in the two sets (both at the level of labels and at the level of features). The proposed solution relies on multitask learning to train robust models that exploit similarities and differences between possibly related tasks, transferring knowledge across them. Results show that the approach is able to take advantage of data coming from such heterogeneous domains and to significantly improve over single-task learning baselines both in regression and in classification. These findings indicate the reliability of ASR QE in particularly challenging test conditions that are out of the scope of this work. Casting ASR QE for system combination as a multitask learning problem, however, is certainly an aspect that we will explore in the future.
- In addition to DS, MVDR and other signal-based combination approaches @cite , the automatic selection of the best channel can be used to perform signal enhancement. The simplest method to do this consists in measuring the signal-to-noise ratio (SNR) of the recorded signals @cite , assuming that the channel with the highest SNR will be the easiest to transcribe. Choosing the channel for which the ASR engine has generated the highest confidence score can also represent a viable solution. For instance, in a previous work with data from the aforementioned CHiME-3 challenge, we successfully applied channel selection based on sentence confidence estimation @cite to select the best microphone.
- An alternative to signal-level combination and best microphone selection is represented by hypothesis-level combination. Along this direction, @cite proposes to use CNC for lecture transcriptions and @cite extends it with a hybrid approach that leverages beamforming and signal-level diversity to transcribe meetings. In @cite , inter-microphone agreement is used to build a confusion network from multiple lattices and improve ASR in a domotic application.
- Many researchers have revealed that the deep model is suffering from over-parameterization heavily. For example, Denil al @cite demonstrated that a network can be efficiently reconstructed with only a small subset of its original parameters. What should be emphasized is that, this redundancy seems necessary during the model training stage, since the highly non-convex optimization problem is hard to be solved with current technique @cite @cite . Therefore, most compression strategies aim to convert a pre-trained model into a small scale model. In this section, we will give a brief introduction of these popular methods from several different aspects.
- In most deep models, the parameters of each layer form a large and dense matrix, which leads to both storage and computational difficulties. Mathematically, if we can approximate this dense matrix with several low-rank small scale matrices, the matrix-vector multiplication can be quickly finished with some special technique like the fast Fourier transform (FFT). Thus, both memory footprint and computational complexity can be reduced dramatically. Inspired by this idea, many methods have been proposed to approximate the original weights. Sindhwani al @cite utilize a linear combination of several structured matrices to model the original parameter matrix. These structured matrices can be transformed into very low rank matrices via some mathematical operations. Denton al @cite adopt matrix factorization to explore the linear structure of neural networks. They use singular value decomposition (SVD) to construct the approximation.
- Network pruning is a classic topic in model compression, which has been widely studied for a long time. In early 90s of the 20th century, pruning has already been adopted to reduce the number of connections and prevent over-fitting @cite @cite . Recently, Han al @cite proposed a pruning method to remove the redundancy of deep models. Small-weight connections below a threshold would be discarded, leading to a sparse architecture. But their method did not reduce the size of activation tensor, which would dominate the memory footprint when batch size is large. Thus some researchers focus their attention on filter pruning to reduce channel number of activation tensor. Hu al @cite proposed a data-driven neuron pruning approach to remove unimportant neurons. Li al @cite pruned neurons via a similar strategy. They use the absolute weight sum to measure the importance of each filter, and less useful filters are dropped. Our approach is similar to these methods, but use a completely different strategy both in channel selection and model fine-tuning.
- Parameter quantization is another classic compression method that has been well studied. One of the widely used methods is product quantization @cite , which decomposes the space into a Cartesian product of low-dimensional subspaces and quantizes each subspace separately. Gong al @cite compared product quantization with several different quantization methods, and found that even with a simple k-means based method, their approach could achieve impressive results. Chen al @cite introduced the hashing trick into model compression, and proposed HashedNets. All connections mapped into the same hash bucket shared the same parameter value. Similar strategy is also adopted in the Deep Compression method @cite . However, @cite @cite required the parameter matrices to be reconstructed during test phase, which limited its further practical application. In order to reduce the run-time memory consumption, Wu al @cite proposed a novel quantization method, which achieved simultaneous acceleration and compression for both convolutional and fully-connected layers.
- Knowledge distillation can be regarded as a kind of transfer learning, which aims to transform the knowledge learned by a cumbersome model to a simple model. The original cumbersome model plays the role of a teacher, and a small model is trained to mimic the performance of the teacher. With the guidance from the teacher model, the student model can even outperform this teacher @cite . Ba and Caruana @cite proposed to use the logits to train the mimic model. However, in order to achieve a comparable accuracy, the mimic model must have the same number of parameters. Hinton al @cite used a temperature parameter to control the soft level of probability distribution, and received better results. Luo al @cite revealed that the top hidden layer preserves much more knowledge than logits when transfer to some domain-specific tasks like face recognition.
- In most deep learning models, we use a 32-bit floating point number to represent a parameter. If it can be binarized, model size can be reduced @math , and vector multiplication can be quickly finished with dedicated hardware like the XNOR gate. Therefore, binary network has become an important research field in model compression. Formerly, a binary network is very hard to converge even on small scale datasets such as MNIST. Recently, Courbariaux al @cite proposed BinaryConnect and achieved comparable accuracy to normal networks on these datasets. However, BinaryConnect only binarized weights, the activation of each layer is still a real-valued tensor. Thus Courbariaux al further binarized both weights and activation in @cite . At the same time, Rastegari al @cite started the exploration of binary network on large scale dataset like ImageNet, and achieved AlexNet level accuracy.
- Ganin al @cite propose a bifurcated classifier that splits into label classification and domain classification branches after common feature extraction layers. A gradient reversal layer is placed between the common feature extraction layers and the domain classification branch; while the domain classification layers attempt to determine which domain a sample came from the gradient reversal operation encourages the feature extraction layers to confuse the domain classifier by extracting domain invariant features. An alternative and simpler implementation described in their appendix minimises the label cross-entropy loss in the feature and label classification layers, minimises the domain cross-entropy in the domain classification layers but it in the feature layers. The model of Tzeng al @cite runs along similar lines but uses separate feature extraction sub-networks for source and domain samples and train the model in two distinct stages.
- Saito al @cite use tri-training @cite ; feature extraction layers are used to drive three classifier sub-networks. The first two are trained on samples from the source domain, while a weight similarity penalty encourages them to learn different weights. Pseudo-labels generated for target domain samples by these source domain classifiers are used to train the final classifier to operate on the target domain.
- Russo al @cite present a bi-directional GAN composed of two generators that transform samples from the source to the target domain and vice versa. They transform labelled source samples to the target domain using one generator and back to the source domain with the other and encourage the network to learn label class consistency. This work bears similarities to CycleGAN @cite .
- A number of domain adaptation models maximise domain confusion by minimising the difference between the distributions of features extracted from source and target domains. Deep CORAL @cite minimises the difference between the feature covariance matrices for a mini-batch of samples from the source and target domains. Tzeng al @cite and Long al @cite minimise the Maximum Mean Discrepancy metric @cite . Li al @cite described , a variant of batch normalization @cite that learns separate batch normalization statistics for the source and target domains in a two-pass process, establishing new state-of-the-art results. In the first pass standard supervised learning is used to train a classifier for samples from the source domain. In the second pass, normalization statistics for target domain samples are computed for each batch normalization layer in the network, leaving the network weights as they are.
- Most work on dueling bandits focuses on strong regret. shows that the worst-case expected cumulative strong regret up to time T for any algorithm is @math . Algorithms have been proposed that reach this lower bound under the Condorcet winner assumption in the finite-horizon setting: Interleaved Filter (IF) @cite and Beat the Mean (BTM) @cite . Relative Upper Confidence Bound (RUCB) @cite also reaches this lower bound in the horizonless setting. Relative Minimum Empirical Divergence (RMED) @cite is the first algorithm to have a regret bound that matches this lower bound. proposed two algorithms, Copeland Confidence Bound (CCB) and Scalable Copeland Bandits (SCB), which achieve an optimal regret bound without assuming existence of a Condorcet winner.
- Active learning using pairwise comparisons is also closely related to our work. considers an active learning problem that is similar to our problem in that the primary goal is to sort arms based on the user's preferences, using adaptive pairwise comparisons. It proposes a novel algorithm, the Query Selection Algorithm (QSA), that uses an expected number of operations of @math to sort @math arms, where @math is the dimension of the space in which the arms are embedded, rather than @math . and consider top-k element selection using adaptive pairwise comparisons. They propose a generalized racing algorithm focusing on minimizing sample complexity. @cite studies adaptive preference learning across arms using pairwise preferences. They show that a greedy algorithm is Bayes-optimal for an entropy objective. While similar in that they use pairwise comparisons, these algorithms are different in focus from the current work because they do not consider cumulative regret.
- The problem of recommending links to the users of a social network has been widely studied, we refer to @cite and @cite for surveys on the link recommendation and link prediction problems, respectively. The problem of recommending links by taking into account the information spreading capability, instead, has received little attention in the literature. In the following we focus on such problem and on the problems of modifying a graph in order to maximize or minimize the spread of information through a network under LTM and ICM models.
- Under ICM, @cite consider graph modifications other than edge addition, edge deletion and source selection, such as increasing the probability that a node infects its neighbours. They proved that optimizing the selection of such modifications with a limited budget is @math -hard and is neither submodular nor supermodular. @cite study the problem of node addition to maximize the spread of information, and provide a counterexample showing that the objective function is not submodular. @cite propose methods for efficiently finding good approximate solutions on the basis of a greedy strategy for the edge deletion problem under the ICM, but do not provide any approximation guarantees.
- Much work has been proposed to summarize a video using supervised learning. Representative methods use category-specific classifiers for importance scoring @cite @cite or learn how to select informative and diverse video subsets from human-created summaries @cite @cite @cite @cite or learn important facets, like faces, hands, objects @cite @cite @cite . Although these methods have shown impressive results, their performance largely depends on huge amount of labeled examples which are difficult to collect for unconstrained web videos. Our CVS approach, on the other hand, exploits visual context from topic-related videos without requiring any labeled examples, and thus can be easily applied to summarize large scale web videos with diverse content.
- Without supervision, summarization methods must rely on low-level visual indices to determine the relevance of parts of a video. Various strategies have been studied, including clustering @cite @cite @cite @cite , interest prediction @cite @cite , and energy minimization @cite @cite . Leveraging crawled web images is also another recent trend for video summarization @cite @cite @cite . However, all of these methods summarize videos independently by neglecting relationships that possibly reside across them. The use of neighboring topic-related videos to improve summarization still remains as a novel and largely under-addressed problem.
- Our focus on sparse coding as the building block of CVS is largely inspired by its appealing property in modeling sparsity and representativeness in data summarization. In contrast to prior works @cite @cite @cite @cite @cite , we develop a novel collaborative sparse optimization that finds shots which are informative about the given video, as well as, the set of topic-related videos.
- Abstractive text summarization has achieved successful performance thanks to the sequence-to-sequence model @cite and attention mechanism @cite . first used an attention-based encoder to compress texts and a neural network language decoder to generate summaries. Following this work, recurrent encoder was introduced to text summarization, and gained better performance @cite @cite . Towards Chinese texts, built a large corpus of Chinese short text summarization. To deal with unknown word problem, proposed a generator-pointer model so that the decoder is able to generate words in source texts. also solved this issue by incorporating copying mechanism. Besides, proposes a minimum risk training method which optimizes the parameters with the target of rouge scores.
- Our work is also related to neural attention model. Neural attention model is first proposed by . There are many other methods to improve neural attention model @cite @cite and accelerate the training process @cite .
- Representing hidden states with deeper operations was introduced just a few years ago @cite . In these works, @cite use additional nonlinear layers for representing the transition from input to hidden layers, hidden to hidden layers, and hidden to output layers. They also improved the RNN architecture by a adding shortcut connection in the deep transition by skipping the intermediate layers. Another work from @cite proposed a new RNN design for a stacked RNN model called Gated Feedback RNN (GFRNN), which adds more connections from all the previous time-step stacked hidden layers into the current hidden layer computations. Despite adding additional transition layers and connection weight from previous hidden layers, all of these models still represent the input and hidden layer relationships by using linear projection, addition and nonlinearity transformation.
- On the tensor-based models, @cite proposed a simple RNN with a tensor product between the input and hidden layers. Such architecture resembles RecNTN, given a parse tree with a completely unbalanced tree on one side. Another work from @cite also use tensor products for representing hidden layers on DNN. By splitting the weight matrix into two parallel weight matrices, they calculated two parallel hidden layers and combined the pair of hidden layers using a tensor product. However, since not all of those models use a gating mechanism, the tensor parameters and tensor product operation can not be fully utilized because of the vanishing (or exploding) gradient problem.
- On the recurrent neural network-based model, @cite proposed multiplicative RNN (mRNN) for character-level language modeling using tensor as the weight parameters. They proposed two different models. The first selected a slice of tensor weight based on the current character input, and the second improved the first model with factorization for constructing a hidden-to-hidden layer weight. However, those models fail to fully utilize the tensor weight with the tensor product. After they selected the weight matrix based on the current input information, they continue to use linear projection, addition, and nonlinearity for interacting between the input and hidden layers.
- The most relevant works related to this paper focus on the relation between the quantile penalty and the asymmetric Laplace distribution(ALD) . @cite jointly estimate the model and the shape parameters for quantile penalty, and @cite infer the joint posterior distribution of these parameters.
- @cite proposed the following framework for parallel BO: given a set of current observations @math and pending experiments @math , an additional set of fantasies @math can be assumed to be the result of those pending experiments. A step of Bayesian optimization can then be performed using the augmented dataset @math and the acquisition function @math . Two different values are proposed for the fantasies: the , where @math for some constant @math and all @math , and the , where @math is given by the GP predictive mean at @math .
- compute a Monte Carlo approximation of the expected acquisition function over potential fantasies sampled from the model's predictive distribution. Recent methods have been proposed to modify the parallel EI procedure to recommend points jointly @cite @cite @cite .
- In the field of texture recognition, local binary patterns (LBP) @cite is one of the most commonly used texture description approaches. Besides texture recognition, LBP based texture description has been applied to other vision tasks, including face recognition @cite , gender recognition @cite and person detection @cite . The LBP descriptor works by thresholding intensity values of a pixel around its neighborhood. The threshold is computed from the intensity of each neighborhood's center pixel. A circular symmetric neighborhood is employed by interpolating the locations not exactly at the center of a pixel. A variety of LBP variants have been proposed in literature, including Local Ternary Patterns @cite , Local Binary Pattern Variance @cite , Noise Tolerant Local Binary Patterns @cite , Completed Local Binary Patterns @cite , Extended Local Binary Patterns @cite and Rotation Invariant Local Phase Quantization @cite . In addition to the introduction of different LBP variants, the fusion of LBP descriptor with color features have also been investigated in previous studies @cite @cite .
- In recent years, Convolutional Neural Networks (CNNs) @cite have shown to provide excellent performance for many computer vision tasks. CNNs are generally trained using large amount of labeled training samples and take fixed sized RGB images as input to a series of convolution, normalization and pooling operations (termed as layers). The network typically ends with several fully-connected (FC) layers, used to extract features for recognition. Several attempts have been made to improve deep network architectures, including increasing the depth of the network by introducing additional convolutional layers @cite @cite . In addition to RGB based appearance networks, other modalities such as motion and depth have also been used to construct multi-cue deep networks for action recognition @cite and RGB-D object recognition @cite .
- In recent years, deep learning methods have made a breakthrough for satellite image analysis, with several works published in the major remote sensing journals. The most notable applications of deep neural networks (DNNs) in remote sensing include land cover classification with optical images @cite @cite @cite , hyperspectral image analysis @cite @cite @cite or Synthetic Aperture Radar (SAR) image analysis @cite .
- A large majority of published works use DNNs trained on patches extracted from satellite images. DNNs are usually not trained on databases of full sized satellite images (1 to several GB per image) due to memory limitations, even on powerful GPU servers. CNNs are the most commonly used deep learning architectures for the classification of optical @cite and SAR @cite satellite images. Because large datasets of satellite images with high quality labels are not easily available, most of the earlier works utilized pre-trained DNNs that were trained on computer vision benchmark datasets (ImageNet), not on satellite images @cite .
- A variety of texture recognition approaches have been proposed in literature @cite @cite . The work of @cite proposes a statistical approach to model textures based on the joint probability distribution of filter responses. The work of @cite proposes an approach based on Weber's law which consists of two components: differential excitation and orientation. An image is represented by the concatenation of these two components in a single representation. The work of @cite introduces an approach that uses lookup-table based vector quantization for texture description. A set of low and mid-level perceptually inspired visual features are introduced by @cite for texture recognition. A multi-resolution framework based on LBP is proposed by @cite for rotation invariant texture recognition. As discussed earlier, LBP is one of the most successful approaches for texture recognition with several variants existing in literature @cite @cite @cite .
- Other than LBP and its variants, bag-of-words based representations employing SIFT features and Fisher Vector encoding scheme have shown promising results for texture recognition @cite . Recently, deep features have also been investigated for texture recognition. Bruna and Mallat @cite introduce the wavelet convolutional scattering network (ScatNet), where no learning is required and convolutional filters are defined as wavelets. The work of @cite proposes a deep network based on multistage principal component analysis (PCANet). The work of @cite proposes to use the convolutional layers of the deep networks as dense local descriptors encoded with Fisher Vector to obtain the final image representation.
- From the ML perspective, we formulate the PPI task as a binary classification problem where discriminative classifiers are trained with a set of positive and negative relation instances. In the last decade, ML-based methods for the PPI tasks have been dominated by two main types: the feature-based vs. kernel based method. The common characteristic of these methods is to transform relation instances into a set of features or rich structural representations like trees or graphs, by leveraging linguistic analysis and knowledge resources. Then a discriminative classifier is used, such as support vector machines @cite or conditional random fields @cite .
- Steerable frames are a concept established early for signal processing. Initially introduced by @cite , the concept was extended to the Steerable Pyramid by @cite and to a Lie-group formulation by @cite @cite . Further, steerability has recently been related to tight frames, presenting Simoncelli's Steerable Pyramid and multiple other Wavelets arising as a special case of the non-orthogonal Riesz transform @cite . Steerable pyramids have been applied to CNNs as a pre-processing step @cite , but have not yet been learnable. We incorporate steerable frames in CNNs to increase their de facto expressiveness and to allow them to learn their configurations, rather than picking them a priori.
- Another way to impose structure onto CNN representations and subsequently increase their data-efficiency is by pre-defining the possible transformations, as done in Transforming Autoencoders @cite , which map their inputs from the image to pose space through a neural network. The Spatial Transformer Networks @cite learn global, and deformable convolutional networks @cite local transformation parameters in a similar way while applying them to a nonlinear co-registration of the feature stack to some estimated pose. Dynamic Filter Networks @cite move one step further and estimate filters for each location, conditioned on their input. These approaches are all dynamic in a sense that they condition their parameters on the input appearance. Our proposed dynamic residual block can be interpreted as a middle ground that combines the idea of Dynamic Filter Networks with explicit pose prediction into blocks that can locally estimate filter poses from a continuous input space. As such, we overcome the difficulty of estimating local filter pose, while being able to separate pose and feature learning globally without the need for differentiable samplers or locally connected layers.
- Research in handwriting recognition has a long history dating back to the 1960s @cite . Hidden Markov Model (HMM) based handwriting recognition @cite @cite @cite is one of the most widely used approaches while neural networks are gaining in popularity @cite . Some approaches also leverage additional constraints for recognizing handwriting in specific domains such as postal addresses @cite @cite and banking checks @cite @cite . These handwriting recognition systems are developed to take advantage of the English language @cite , which is intrinsically different from source code. For instance, variable names are often created from concatenated words (e.g. camelCase or underscore naming), which poses a problem for the traditional handwriting recognition system as it expects spaces to appear between words contained within its dictionary. We do not aim to contribute to the extensive literature in handwriting recognition, but rather, we intend to examine how we can leverage this existing work for application to handwritten source code recognition interfaces.
- There are three tracking scenarios that are important to consider: short-term tracking, long-term tracking, and tracking in a crowded scene. If objects are visible over the whole course of the sequences, short-term model-free tracking algorithms are sufficient to track a single object without applying a pre-trained model of target appearance. There are many short-term tracking algorithms developed in the literature @cite @cite such as online density estimation @cite , context-learning @cite , scale estimation @cite , and using features from multiple CNN layers @cite @cite . However, these short-term tracking algorithms can not re-initialize the trackers once they fail due to long-term occlusions and confusions from background clutter.
- In opposition to HWRA, handwriting recognition on surface using Deep Learning models like Bidirectional LSTMs @cite , Connectionist Temporal Classifiers @cite , and Multidimentional RNNs @cite have outperformed other baseline models (, @cite ). Similar Deep Learning models, such as Convolutional Neural Networks @cite and LSTMs @cite , have also shown improved results in the domain of gesture recognition.
- Topological data analysis has been used for various tasks such as 3D shapes classification @cite or protein structure analysis @cite . However, such techniques have not been used in NLP, primarily because the theory is inaccessible and suitable applications are scarce. offers an introduction to using persistent homology in NLP, by creating representations of nursery-rhymes and novels, as well as highlights structural differences between child and adolescent writing. However, these techniques have not been applied to core NLP tasks.
- As opposed to the large body of works that has been devoted to the study of various social networks based applications @cite @cite @cite @cite @cite @cite @cite @cite @cite and in particular recommendation systems in social networks @cite @cite @cite @cite @cite @cite , very few works have addressed the problem of hashtag recommendation for easy categorization and retrieval of tweets.
- In an different attempt, @cite proposes a method that recommends hashtags based on similar users and tweets by calculating the preference weight of a user towards a certain hashtag based on the TF-IDF and then selecting the top users with high cosine similarities with other users. Also, they use the same approach for calculating the top similar tweets. Since many tweets do not contain hashtags, the recommended hashtags may be from similar tweets instead of similar users. They claim that their method is able to recommend more personalized hashtags compared to the other methods and their method suits both user preferences and the tweet content.
- In the realm of social network analysis @cite @cite @cite @cite @cite @cite @cite @cite @cite , diffusion of information has been an active research area in the social network analysis @cite @cite @cite @cite @cite , however much less is known about spread of ideas in the form of hashtags. Specifically, prediction of adopting hashtags in a given time frame has been virtually an untrodden area yet.
- Most traditional works have focused on the structure and topology of the social graph. They try to maximize the spread of information and thus marketing profits by detecting the influential nodes and then leveraging viral marketing and social recommendation networks @cite @cite @cite @cite @cite @cite . Meanwhile some woks have modeled the temporal dynamics of information spread @cite @cite @cite @cite . In particular, @cite studies the ways network structure reacts to users posting and sharing contents. Few have used collaborative filtering to predict the probability of a tweet to get retweeted @cite . Some tasks such as profiling users based on substance, style, status and social tendency has been addressed by using tweet content @cite .
- A more recent work @cite proposed a hybrid linear regression based approach for predicting the spread of ideas in a given time period. In particular, they used both topology of the social graph and contents of ideas to model the propagation of ideas in Twitter by viewing hashtags as ideas and training a regression model to predict the hashtag spread in a time frame.
- Our work fits into the line of research on rumor spreading, population protocols, and related interaction models. Our work also touches on the issue of how distributed systems may spontaneously achieve some form of coordination with minimum agent capabilities. The basic work in this direction, starting with the seminal paper @cite , focuses on synchronizing timers through asynchronous interprocess communication to allow processes to construct a total ordering of events. A separate interesting question concerns local clocks which, on their own, have some drift, and which need to synchronize in a network environment (cf. e.g. @cite @cite , or @cite for a survey of open problems).
- For protocols with convergence to a single point in the configuration space in the limit of large population size, a discussion of the limit behavior is provided in @cite , who provide examples of protocols converging to limit points at coordinates corresponding to any algebraic numbers.
- We also remark that local interaction dynamics on arbitrary graphs (as opposed to the complete interaction graph) exhibit a much more complex structure of their limit behavior, even if the graph has periodic structure, e.g., that of a grid. Oscillatory behavior may be overlaid with spatial effects @cite , or the system may have an attractor at a critical point, leading to simple dynamic processes displaying self-organized criticality (SOC, @cite ).
- The AONT-RS technique @cite is similar to SSMS, as it combines symmetric encryption with data dispersal. It applies an all-or-nothing transform(AONT) @cite to create @math fragments: encrypted data are divided into @math fragments and an additional fragment is generated by xor -ing hashes of these data fragments with the key used for encryption. Additional @math fragments are produced using a systematic Reed-Solomon error correction code. Data integrity is ensured by the use of a canary that is dispersed within the fragments.
- Multicommunity studies Our investigation of user engagement in multicommunity settings follows prior literature which has examined differences in user and community dynamics across various online groups, such as email listservs. Such studies have primarily related variations in user behaviour to structural features such as group size and volume of content @cite @cite @cite @cite . In focusing on the linguistic content of communities, we extend this research by providing a content-based framework through which user engagement can be examined.
- Reddit has been a particularly useful setting for studying multiple communities in prior work. Such studies have mostly focused on characterizing how individual users engage across a multi-community platform @cite @cite , or on specific user engagement patterns such as loyalty to particular communities @cite . We complement these studies by seeking to understand how features of communities can mediate a broad array of user engagement patterns within them.
- The capacity of the deletion channel has been studied in the probabilistic model. In the model where the deletions are iid and occur with a fixed probability @math , an immediate upper bound on the channel capacity is given by the capacity of the erasure channel @math . Mitzenmacher and Drinea showed in @cite that the capacity of the deletion channel in the iid model is at least @math . Extensive work in the literature has focused on determining lower and upper bounds on the capacity @cite @cite @cite @cite @cite @cite . We refer interested readers to the comprehensive survey by Mitzenmacher @cite . Ma @cite also studied the capacity in the bursty model of the deletion channel, where the deletion process is modeled by a Markov chain.
- : Though most analysis of celebrities on social media to date have focused on how the celebrities place themselves, there are a few studies on the role fans' in interacting with celebs. @cite study fans of NBA teams on Facebook and find four key motives for fans to engage with their favorite teams: passion, hope, esteem and camaraderie'. @cite study the role of celebrity self disclosure on Twitter and the reaction of fans. They report that increased celebrity self-disclosure leads to enhanced fans' feeling of social presence, thereby positively affecting para-social interaction with celebrities'.
- : @cite introduce the idea of a para-social breakup, the end of a PSR. They find that a para-social breakup is like a regular romantic relationship breakup but not as intense. Breakups on social media have been studied in the past @cite . We define a para-social breakup on Twitter as an act of unfollowing the celebrity and study the impact of fan behavior on a breakup. Work has been done on reasons for unfollowing others on Twitter @cite . conclude that most users unfollow those who left many tweets within a short time, created tweets about uninteresting topics, or tweeted about the mundane details of their lives.'. In our case of a para-social breakup, we find that most unfollowing happens as an act of defiance against the celeb and despair of not receiving the personal attention that fans perceive they deserve.
- Previous works have also attempted to achieve Imitation Learning that outperforms the original expert. @cite use Imitation Learning followed by Reinforcement Learning. Kai-Wei, et al @cite use Monte Carlo estimates to calculate @math , and train an apprentice @math to maximise @math . At each iteration after the first, the rollout policy is changed to a mixture of the most recent apprentice and the original expert. This too can be seen as blending an RL algorithm with Imitation Learning: it combines Policy Iteration and Imitation Learning.
- AlphaGo Zero (AG0) @cite , presents an independently developed version of ExIt, Our original version, with only policy networks, was published before AG0 was published, but after its submission. Our value networks were developed before AG0 was published, and published after @cite and showed that it achieves state-of-the-art performance in Go. We include a detailed comparison of these closely related works in the appendix.
- * Shape analysis. In the context of scene understanding, a large body of work focuses on estimating attributes for indoor scenes by computing high-level object features and analyzing inter-object relations (see @cite for a survey). More recently, with renewed interest in convolutional neural networks, researchers have explored data-driven approaches for various shape and scene analysis tasks (cf., @cite ). While there are too many efforts to list, representative examples include normal estimation @cite @cite , object detection @cite , semantic segmentation @cite @cite , localization @cite , pose estimation @cite @cite , and scene recognition using combined depth and image features from RGBD input @cite , etc.
- In the context of non-semantic segmentation (i.e., object-level region extraction without assigning semantic labels), one of the most widely used interactive segmentation approaches is GrabCut @cite , which builds GMM-based foreground and background color models. The state-of-the-art in non-semantic segmentation is arguably the method of Arbel ' a @cite , which operates at the level of contours and yields a hierarchy of segments. Classical segmentation methods that target standard color images have also been extended to make use of additional information. For example, @cite propose a version of GrabCut that handles binocular stereo video, @cite compute depth-ordered segmentations using optical flow from video sequences, and @cite leverage scanned depth information to decompose images into layers. In this vein, leverages additional channels of information (disparity and normals) that can be directly estimated from input mono or stereo images. By doing so, our method performs well even in ambiguous regions, such as textured or shaded areas, or where foreground-background colors are very similar. In , we present various comparisons with state-of-the-art methods and their variants.
- Fine-tuning @cite @cite is a basic example of multi-task learning, where we can leverage different learning tasks by considering them as a pre-training step. Other models alternate learning between each training task, for example in natural language processing @cite . Multi-task learning can also be used in a data streaming setting @cite , or to prevent forgetting previously learned tasks in reinforcement learning @cite . It can also be used to learn unsupervised features from various data sources with an auto-encoder @cite .
- The aim of auto-scaling approaches is to acquire and release resources dynamically while maintaining an acceptable QoS @cite . The auto-scaling process is usually represented and implemented by a MAPE-K (Monitor, Analyze, Plan and Execute phases over a Knowledge base) control loop @cite .
- Threshold-based rules are the most popular approach offered by many platforms such as Amazon EC2 http: aws.amazon.com ec2 , Microsoft Azure http: azure.microsoft.com or OpenStack https: www.openstack.org . Conditions and rules in threshold-based approaches can be defined based on one or more performance metrics, such as CPU load, average response time or request rate. @cite investigate horizontal auto-scaling using threshold-based and reinforcement learning techniques. In @cite , the authors describe a lightweight approach that operates fine-grained scaling at resource level in addition to the VM-level scaling in order to improve resource utilization while reducing cloud provider costs. @cite extend the typical two threshold bound values and add two levels of threshold parameters in making scaling decisions. @cite propose a simple strategy for dynamic scalability of PaaS and SaaS web applications based on the number of active sessions and scaling the VMs numbers if all instances have active sessions exceed particular thresholds. The main advantage of threshold-based auto-scaling approaches is their simplicity which make them easy to use in cloud providers and also easy to set-up by clients. However, the performance depends on the quality of the thresholds.
- RL @cite is learning process of an agent to act in order to maximize its rewards. The standard RL architecture is given in Figure . The agent is defined as an auto-scaler, the action is scaling up down, the object is the target application and the reward is the performance improvement after applying the action. The goal of RL is how to choose an action in response to a current state to maximize the reward. There are several ways to implement the learning process. Generally, RL approaches learn estimates of the Initialized Q-values @math , which maps all system states @math to their best action @math . We initialise all @math and during learning, choose an action @math for state @math based on @math -greedy policy and apply it in the target platform. Then, we observe the new state @math and reward @math and update the Q-value of the last state-action pair @math with respect to the observed outcome state ( @math ) and reward ( @math ).
- Two well-known RL approaches are SARSA and Q-learning @cite . @cite use an appropriate initialization of the Q-values to obtain a good policy from the start as well as convergence speedups to quicken the learning process for short convergence times. @cite propose a hybrid learning system by combining queuing network model and SARSA learning approach to make resource allocation decisions based on application workload and response time.
- The design of robust networks especially has been of prime interest to researchers for many decades. In the field of network design, survivable networks are explored that remain functional when links are severed or nodes fail, that is, network services can be restored in the event of catastrophic failures @cite @cite . Recently, researchers have proposed an interesting generic and declarative approach to network design @cite .
- To the best of our knowledge, there has been no prior work focusing on demographic verification within the adult domain, although various works have focused on related components of Porn 2.0, such as pornographic practices, communities and subcultures @cite ; interest recommendations @cite ; dating services @cite @cite ; user commenting @cite ; content popularity @cite and illegal content dissemination @cite . The closest to our work is a recent study of the PornHub social network @cite , although it did not touch upon age verification.
- A number of studies have also looked at online gender swapping more generally @cite . For example, @cite @cite found that gender swapping in online gaming is commonplace. The reasons for such activities have also been explored via surveys @cite . Reasons given include: curiosity and the desire to experiment; the perception that the opposite gender is treated better; and the belief that playing the opposite gender will allow new forms of behaviour, or gain advantages. This complements our own work, although we do not focus on finding the reasons for users to catfish. Other general work on identifying deception in online communication include linking authors across OSNs @cite , detecting online personas @cite , and identifying divergent political inclinations @cite .
- Classical graph algorithms in the context of MPC were first studied by Brickell and Shmatikov in 2006 @cite . The authors proposed secure two party protocols for computing single source and all pair shortest paths problem. Since then, many graph algorithms have been explored in the MPC setting, including BFS, DFS, Dijkstra, minimum spanning tree and classical flow algorithms @cite @cite @cite . Securely constructing an anonymized version of a network distributedly held by a set of parties has been previously studied for various security models @cite @cite @cite .
- There exist works that have aimed at designing secure MPC algorithms for a few network measures. These works have generally been motivated by some particular application scenario. To employ SNA for criminal investigation, Kerschbaum and Schaad @cite propose secure multiparty protocols for closeness and betweenness centrality measures. They assume a different definition of betweenness centrality compared to the classical one specified by Brandes @cite ). Furthermore, they employ the adjacency matrix graph representation and hence their protocols are not very efficient for sparse graphs. @cite propose secure multiparty protocols for degree distribution, closeness centrality, Pagerank and K-shell decomposition for the adjacency matrix representation. Employing the ORAM primitive and the edgelist graph representation, the Pagerank and K-shell decomposition protocols designed in the current paper asymptotically outperforms the algorithms in the above work. @cite study the a set of centrality measures over multilayer networks distributedly held by a set of individuals organizations. They propose information theoretic secure MPC protocols for distance based centrality measures.
- Transfer learning has been studied for a long time. There is no standard definition of transfer learning in the literature @cite . We follow the definition from @cite : transfer learning aims at performing a task on a target dataset using some knowledge learned from a source dataset. The idea has been applied to many fields such as speech recognition @cite and finance @cite .
- The successes of ANNs for many applications over the last few years have escalated the interest in studying transfer learning for ANNs. In particular, much work has been done for computer vision @cite @cite @cite . In these studies, some of the parameters learned on the source dataset are used to initialize the corresponding parameters of the ANNs for the target dataset.
- CQELS-Cloud @cite is the first RSP system which mainly focuses on the engine elasticity and scalability. The whole system is based on Apache Storm. Firstly, CQELS-Cloud compresses the incoming RDF streams by dictionary encoding in order to reduce the data size and the communication in the computing cluster. The query logical plan is mapped to a Storm topology, and the evaluation is done through a series of SPARQL operators located on the vertex of the topology. Then, to overcome the performance bottlenecks on join tasks, the authors propose a based on probing sequence. From the aspect of implementation, CQELS-Cloud is designed as the streaming service for high concurrent requests. The capability of CQELS-Cloud to cope with massive incoming RDF data streams is still missing. Furthermore, to the best of our knowledge, CQELS-Cloud is not open source, customized queries and data feeding are not feasible. Katts is another RSP engine based on Storm. The implementation of Katts @cite is relatively primitive, it is more or less a platform for algorithm testing but not an RSP engine. The main goal of Katts is designed to verify the efficiency of graph partitioning algorithm for cluster communication reduction.
- Although the SPARQL query optimization techniques have been well developed recently, CQELS is still the only system which considers query optimization to process RDF data stream. However, the greedy-like left-deep plan leads to sequential query evaluation, which makes CQELS benefit from few additional computing resources. The conventional SPARQL optimization for static data processing can be hardly applied in a streaming context. Recent efforts @cite @cite @cite @cite possess long data preprocessing stage before launching the query execution. The proposed solutions do not meet real-time or near real-time use cases. The heuristic-based query optimization in @cite totally ignores data statistics and thus does not promise the optimal execution plan for @math running streaming service.
- In his thesis, Mazel-Gee developed the theory of model @math -categories and used it to generalize Goerss-Hopkins theory to the setting of an arbitrary presentable, stable @math -category, see @cite . The current note concerns a simpler problem and is not directly related to Mazel-Gee's work, but the latter's emphasis on homotopy-invariant language and the prominence of product-preserving preshaves provided the author with much of the motivation to work out the case of @math -algebras in detail.
- Visual attributes have been used quite extensively for a variety of tasks such as image classification @cite @cite @cite , deeper image understanding @cite , image search @cite @cite , segmentation @cite and semi-supervised learning @cite . They are mid-level semantic ( nameable) visual properties such as furry, wooden, young, . They have been shown to be an effective modality for humans and machines to communicate with each other @cite @cite @cite @cite . We build upon these recent developments and propose attributes as a mode of communication among machines that may otherwise be unable to communicate due to differences in sources of data and sensors. The fact that attributes are semantic ensures that humans annotating the first few labeled images in both domains are annotating the same semantic concept. This provides a common ground between both domains, necessary for communication.
- Among the various levels of supervision that have been explored in the community, semi-supervised learning approaches @cite try to achieve a good balance between maximizing accuracy while minimizing human input. A commonly used semi-supervised technique is the bootstrap or self-learning approach @cite where an agent initially learns from a small amount of labeled data. It then retrieves images from a large unlabeled pool whose labels it is most confident of, and transfers these to the labeled set to re-learn its model. Such approaches often suffer from semantic drift @cite . Recently, visual attributes have been successfully used to resist this drift @cite . This approach however relies on a human annotating all attribute-category relationships offline. In our approach, the machines communicate these attribute-category relationships to each other, alleviating the need for human involvement. Moreover, the attribute-category relationships estimated by machines are soft and more robust. Human annotated attribute-category relationships tend to be hard @cite @cite , ignoring instance-level variations within categories.
- Visual attributes have been used as a medium for humans to communicate their domain knowledge to machines during an active learning loop for image classification @cite @cite or semi-supervised clustering @cite . Another variant of active learning called coactive learning exists where the requirement that humans provides optimal labels to examples is loosened @cite . Again, our approach eliminates the need for human involvement in each iteration by having machines communicate their current understanding with each other.
- Existing works on aspect rating prediction use reviews to analyze latent aspect ratings @cite @cite and ignore the aspect ratings provided by users. While the widely used CF approaches for rating prediction view ratings as values and do not encode aspect dependencies @cite @cite @cite @cite @cite @cite @cite @cite @cite .
- In contrast, most statistical approaches handle ordinal data using an ordinal probit model @cite @cite @cite . Although they allow a Bayesian inference but it necessitates using truncated Gaussian distributions and forced ordering of cut-off points. This leads to complicated and even sub-optimal inference.
- The authors of @cite used stick-breaking formulation to parameterize the underlying continuous rating. However, since the non-conjugacy made an MCMC sampling non-trivial, they performed an approximate variational Bayesian inference. For correlated topic models @cite , P ' o lya-Gamma auxiliary variable augmentation is used with logistic-normal transformation, whereas the work in @cite used stick-breaking likelihood for categorical data. However, none of these works use stick-breaking likelihood with a P ' o lya-Gamma variable augmentation to exploit conjugacy to facilitate Gibbs sampling.
- One common approach to tackle the varience in pose is to find latent spaces where the correlation between two views are maximized. Canonical correlation analysis(CCA) @cite projects the data from two views into two low dimensional subspace which are highly correlated. @cite have extended CCA method so that it exploits the labels of training data to find a more discriminative projection direction. Both of the mentioned methods can exploit kernels to model non-linearity. Although methods based on latent spaces have proven to be powerful tools for both multi-view image classification and multi-modal data classification, they require learning a projection direction for every viewpoint and their ability to generalize to unseen viewpoints is limited.
- Another set of solutions try to fit the given 2D images to predefined 3D shapes of objects (e.g. a face) from a single view image @cite @cite . In @cite authors propose a 3D pose normalization for face recognition in order to make it robust to variation in pose. @cite exploits 3D CAD models to detect and find the pose of objects such as bikes and cars. The use of these methods are restricted to objects with available predetermined 3D models. A rather interesting solution was proposed by @cite that does not require 3D reconstruction of the face, instead they use the cost of stereo matching as their error function. However, they make the assumption that epipolar lines are horizontal which does not hold true for the object recognition in the general case.
- In this paper, we take a geometric approach to the problem of viewpoint variation. Our work is inspired by @cite , who used homographgy constraints to recognize body pose transitions between two successive frames of two video cameras, observing human actions. Although, we are not dealing with video frames in this work, we show that the concept can be extended also to a pair of still images of a rigid object (i.e. instead of dealing with moving points in space viewed by two pairs of frames (4 images), we can extend the idea to recognizing a rigid object from two images. The key to achieve this extension is to consider quadruple of points in each camera image, instead of triplets of points in two frames of each camera The result is a rigid object recognition method that can handle unknown viewpoints and internal camera parameters.
- We refer to @cite which makes use of DFT to analyze an AMP-type iterative detection algorithm for code-division--multiple-access systems with the classical iid assumption about the entries of the observation matrix. From the technical point of view, this paper can be viewed as an extension of @cite where DFT is used for solving the TAP equations of the Ising model of spin glass theory. Due to space limitations, we do not present the proofs of our results; they are obtained by following arguments similar to those used in @cite , see also the paragraph below .
- Independent agent Kalman filters are a common starting point for crowd prediction. However, this approach leads to an uncertainty explosion that makes efficient navigation impossible @cite . Some research has thus focused on uncertainty. For instance @cite @cite @cite develop high fidelity independent models, in the hope that controlled uncertainty will improve navigation performance. @cite predictive uncertainty is bounded; intractability is avoided with receding horizon control @cite ; collision checking algorithms developed in @cite , with roots in @cite @cite , keeps navigation safe. The insight is that since replanning is used, predictive covariance can be held constant at measurement noise. @cite @cite more sophisticated individual models are developed: GP mixtures @cite with a Dirichlet Process (DP) prior over mixture weights @cite . DP priors allow the number of motion patterns to be data driven while the GP enables inter-motion variability; RRT @cite finds feasible paths; collision checking @cite guarantees safety. In the work above, independent agent models are the core contribution; we show that this is insufficient for crowd navigation.
- Proxemics @cite studies proximity relationships, providing insight about social robot design: in @cite @cite @cite proxemics informs navigation protocol. Similarly, @cite studies pedestrian crossing behaviors using proxemics. @cite RRTs are combined with a proxemic potential field @cite . Instead of using proxemic rules, @cite adopts the criteria of @cite . Personal space rules guide the robot's behavior by extending the Risk-RRT algorithm developed in @cite (Risk-RRT extends RRT to include the probability of collision along candidate trajectories). The work in @cite is more agnostic about cultural considerations; a probabilistic collision cost'' is based on @cite . The work in @cite argues that robot and environment dynamics and a sufficient look-ahead guarantees collision avoidance. Although these approaches model human-robot interaction, they do not model human-robot : respecting a proper distance between human and robot (similar to @cite ) is emphasized.
- Some approaches learn navigation strategies by observing example trajectories. @cite , learned motion prototypes guide navigation. @cite , maximum entropy inverse reinforcement learning (max-Ent IRL) learns taxi cab driver navigation strategies from data; this method is extended to an office navigation robot in @cite . @cite , max-Ent IRL is extended to dynamic environments. Their planner is trained using trajectories, and the method recovers a planner which duplicates the behavior of the simulator. @cite , agents how to act in multi-agent settings using game theory and the principle of maximum entropy.
- @cite coupled human-robot models generated autonomous behaviors that were efficient and communicative (between a single human and a single robot); it remains unclear if coupled dynamical systems using reinforcement learning scales to multiple agents. @cite , human state information was gathered by coupling robot action to human prediction. Using deep learning for the crowd problem @cite raises an important question (since @cite collects millions of training examples): can be learned? The combinatorics of social navigation () makes naive approaches (e.g., brute force learning without exploiting sparsity) seem infeasible.
- @cite pure sampling is observed to be ineffective for coupled models; some mechanism is required to guide sampling ( @cite @cite makes a similar observation, but RVO HRVO was shown brittle to noise and motion ambiguity in @cite ). To guide sampling, Voronoi techniques applied to static obstacles parses the action space into convex regions. However, static Voronoi techniques (or any static convex region identifier) leads to suboptimal strategies: by ignoring motion, a human-robot decoupling assumption is made. In reactive environments, free space is dynamically generated by the probabilistic interplay of human and robot trajectory distributions (see Figure ). Sparse IGP ( @math ) achieves this high level interaction, establishing sparsity and optimality guarantees in the process.
- There is a large body of work that tried to improve word embeddings using external resources. extended the CBOW model @cite by adding an extra term in the training objective for generating words conditioned on similar words according to a lexicon. extended the skipgram model @cite by representing word senses as latent variables in the generation process, and used a structured prior based on the ontology. used belief propagation to update pre-trained word embeddings on a graph that encodes lexical relationships in the ontology. Similarly, improved word embeddings by representing each sense of the word in a way that reflects the topology of the semantic network they belong to, and then representing the words as convex combinations of their senses. In contrast to previous work that was aimed at improving word representations, we propose an approach for obtaining embeddings at the , while jointly optimizing the model parameters for the NLP task of interest.
- OpenCL targeting the TILT overlay @cite was suggested as an alternative to the Altera OpenCL tool (which generates a heavily pipelined, spatial design which maximizes throughput at the cost of significant resource usage) when a lower throughput is adequate. TILT uses a weaker form of application customization by varying the mix of pre-configured standard FUs and optionally generating application-dependent custom units. However, as each application requires that the TILT overlay be recompiled, a hardware context switch (referred to as a kernel update in the paper) takes on average 38 seconds. Additionally, the Altera OpenCL HLS implementation of the benchmark application was 2 @math more efficient in terms of throughput per unit area.
- Additionally, in the context of software hardware systems on hybrid FPGAs, the reconfiguration time required to load a hardware accelerator onto the FPGA, and not just the compile time, is also a significant factor in exploiting the acceleration benefits and the virtualization feature of the FPGA hardware @cite @cite . Hence, the fast configuration afforded by overlays is important in such systems where software and hardware execution is tightly coupled. Furthermore, overlays offer the benefit of portability, whereby the same application can be loaded onto identical overlays on different physical devices without re-compilation. That is, overlays offer the potential to support FPGA usage models where programmability, abstraction, resource sharing, fast compilation, and design productivity are critical issues.
- Social media geolocation has been undertaken on various platforms such as Facebook @cite , Flickr @cite , and Wikipedia @cite . Especially, Twitter geolocation is the predominant field among all of them because of its availability @cite .
- * -2mm With the goal to transfer the modality information from the source domain to the target domain, recent developments in CMS, such as texture synthesis @cite @cite @cite , face photo-sketch synthesis @cite @cite , and multi-modal retrieval @cite @cite , have shown promising results. In this paper, we focus on the problems of image super-resolution and cross-modality synthesis, so only review related methods on these two aspects.
- By always considering a scenario with multiple images, sophisticated methods for patch comparison using deep learning have been proposed @cite @cite . However, the corresponding models were trained for highly specific keypoint matching scenarios. Techniques like these do not capture the variations present in realistic forgeries, and thus cannot be used as-is in a real-world forensic scenario.
- Recently, several pseudo label-based methods have been extended to multi-view setting @cite @cite @cite via cluster consensus learning. In these approaches, pseudo-labels derived from certain clustering algorithms are required to be the same across different views in order to incorporate multi-view information. For example, adaptive Unsupervised Multi-view Feature Selection (AUMFS) @cite rely on spectral clustering on the combined similarity graphs obtained from different views. Multi-View Feature Selection (MVFS) @cite and MVUFS @cite can be seen as extention of NDFS @cite and RUFS @cite to multi-view feature selection by enforcing consensus on the cluster indicators from different views, respectively. However, they rely on the cluster labels to guide feature selection, and the noisy cluster labels may lead to suboptimal feature selection results. Also, they evaluate features based on linear regression and hence cannot select high-quality features if they are non-linearly correlated with the class labels.
- If searchers are unable to move, then the Meeting problem becomes equivalent to asking what is the maximum number of points that can be placed in a polygon in such a way that no two of them see each other. This is called the , and has been studied in @cite , where tight bounds are given in terms of the number of vertices of the polygon. The situation with mobile searchers is of course radically different.
- The Gathering problem has been extensively studied in several contexts @cite @cite . The literature can be divided into works considering robots in a geometric space, and works considering agents on a graph @cite @cite @cite @cite .
- Here we focus on Gathering in geometric spaces, since it is related to our setting. In the Look-Compute-Move model, asynchronous oblivious robots with unlimited visibility can solve the Gathering problem without additional assumptions @cite .
- The case of limited visibility has been studied, as well. Any given pattern can be formed by asynchronous robots with agreement on a coordinate system @cite . Without such agreement, it is still possible to converge to the same point, perhaps without ever reaching it @cite .
- Fault-tolerant Gathering has been investigated in @cite @cite for oblivious robots with unlimited visibility. Rendezvous has been investigated also when the robots have a constant amount of visible memory'' @cite .
- @cite , they investigate upper and lower bounds on the movements of two robots solving the Rendezvous problem in a polygon. The authors give a wealth of results under different assumptions, but a common handedness (i.e., a common notion of clockwise direction) and unlimited persistent memory are always assumed.
- Another problem for robots in polygons is the search for an intruder, e.g., @cite @cite , where one robot tries to escape while others have to locate it. This setting is clearly quite different, as in the Meeting problem we consider robots that cooperate to achieve a common goal.
- The model in which robots can obstruct each other's view has also been studied. Here, the goal is typically to make all robots see each other by making sure that no three of them are collinear @cite @cite @cite . As with the literature on Gathering, none of these works considers robots in a polygon.
- Recall that a sub-routine of our Meeting algorithms consists in drawing a map of the polygon. A related problem is that of constructing the visibility graph of a polygon by mobile robots @cite @cite @cite . Here the effort has been focused on finding minimal assumptions on the robots' power that allow them to solve the problem.
- To the best of our knowledge, only two papers deal with this problem. @cite , a definition of computation is not explicitly given, but it is said that a certain point is not computable, in the sense that its coordinates cannot be computed with basic arithmetic operations and extractions of roots of any degree. @cite , computation is explicitly defined in terms of algebraic functions, although all that is actually needed is the ability to construct regular polygons, as well as all points that can be geometrically obtained with compass and straightedge.
- Interestingly, solutions to geometric problems have been proposed that involve functions whose computability (in an intuitive sense) is unclear. An example is in @cite , where the Rendezvous point computed by one of the algorithms is either the central vertex or the midpoint of the central segment of the medial axis of a polygon. Since the central segment of the medial axis could be any parabolic arc, its midpoint is a transcendental function of the polygon's vertices. As such, it is not constructible with compass and straight edge. It turns out that this is not a real problem for that particular algorithm, because the robots can easily agree on an endpoint of the aforementioned parabolic arc or on the parabola's vertex, instead of its midpoint. Still, it is interesting to observe that the notion of computability emerging from @cite is more comprehensive'' than the one of @cite .
- The basic idea of dynamic betweenness algorithms is to keep track of the old betweenness scores (and additional data structures) and efficiently update the information after some modification in the graph. Based on the type of updates they can handle, dynamic algorithms are classified as (only edge insertions and weight decreases), (only edge deletions and weight increases) or (all kinds of edge updates). However, one commonality of all these approaches is that they build on the techniques used by BA @cite , which we therefore describe in in more detail.
- Nasre al @cite compare the distances between each node pair before and after the update and then recompute the dependencies from scratch as in BA (see ). Although this algorithm is faster than recomputation on some graph classes (i.e. when only edge insertions are allowed and the graph is sparse and weighted), it was shown in @cite that its practical performance is much worse than that of the algorithm proposed by Green al @cite . This is quite intuitive, since recomputing all dependencies requires @math time independently of the number of nodes that are actually affected by the insertion.
- Recently, the problem of designing privacy mechanisms for hypothesis testing has gained interest. @cite show that the optimal locally differential privacy (L-DP) mechanism has a form and can be obtained as a solution of a linear program. @cite deal with a privacy-guaranteed hypothesis testing by using chi-square goodness of fit as the utility measure and adding Gaussian or Laplace noise to dataset to guarantee DP-based privacy protection.
- Our problem differs from these efforts in using MI as the privacy metrics. In @cite , the L-DP formulation, focused on the high privacy regime, requires the mechanism to limit distinction between any two letters of the source alphabet for a given output. The requirement also gathers all privacy mechanisms satisfying a desired privacy protection measured by L-DP within a hypercube. Therefore, the authors simplify the trade-off problem to a linear program by exploring the sub-linearity of the relative entropy function. In contrast, all privacy mechanisms giving a desired MI-based privacy form a convex set which is not a polytope. However, taking advantage of E-IT, we propose good approximations for the MI-based privacy utility trade-offs in high privacy regime. In fact, we present closed-form privacy mechanisms for both binary hypothesis testing with arbitrary alphabets as well as @math -ary hypothesis testing with binary alphabets. Furthermore, for @math -ary hypothesis testing with arbitrary sources, the privacy mechanism can be attained effectively by solving an SDP.
- The connection between hypothesis testing and privacy has been studied in the context of location anonymization and smart meter privacy. In location privacy, the problem of determining if a sequence of anonymized data points (e.g. location positions without an accompanying user ID) belongs to a target user can be formulated as a hypothesis test. More specifically, if the distribution of the user's data is known and unique among other users, any observed sequence can be tested against the hypothesis that it was drawn from this distribution, thus revealing if it belongs to the target user. Within this context, Montazeri et al @cite @cite studied the problem of anonymizing sequences of location data, and characterized the probability of correctly guessing a target user's data within a larger dataset. In related work on smart meter privacy, Li and Oechtering @cite considered the problem of private information leakage in a smart grid. Here, an adversary challenges a consumer's privacy by performing an unauthorized binary hypothesis test on the consumer's behavior based on smart meter readings. Li and Oechtering @cite propose a solution for mitigating the incurred privacy risk with the assist of an alternative energy source.
- The theoretical analysis done by Montazeri et al @cite @cite and Li and Oechtering @cite are related to the one presented here in that they also make use of large deviation (information-theoretic) results in hypothesis testing. However, we apply these powerful theoretical tools to a different setting, in which data is purposefully randomized before disclosure in order to provide privacy, while guaranteeing utility in terms of a successful hypothesis test. Whereas they consider a hypothesis testing adversary, here we consider a precise hypothesis test as part of the utility metric.
- MI has been amply used as a measure for quantifying information leakage within the information-theoretic privacy literature (cf. @cite @cite @cite @cite @cite @cite @cite and the references therein). The connection between MI-based metrics and other privacy metrics has been studied, for example, by Makhdoumi and Fawaz @cite . In the present paper, we approximate MI by the chi-squared divergence which, in turn, also posses interesting estimation-theoretic properties @cite . An exploration of the role of chi-squared related metrics in privacy has appeared in the work of @cite @cite .
- More recently, @cite , have studied the multi-armed bandit problem with a graph based feedback structure similar to @cite , and @cite . However, they assume that the graph structure is never fully revealed. In contrast, in many cases such as the problem of routing in communication networks and the problem of influence maximization in social networks, the graph structure is revealed or learnt apriori and is known. When the graph structure is known, the authors in @cite propose algorithms for the stochastic setting whose regret performance is bounded by the domination number of the graph. In contrast, the algorithms proposed in @cite assume that the graph is unknown and achieve a regret that is upper bounded by the independence number of the graph. (Note that the independence number of a graph is larger than or equal to the domination number). Our current work proposes a general feedback structure of which @cite and @cite can be viewed as a special case. Moreover, we present algorithms that benefit significantly from the knowledge of the graph feedback structure.
- In the past few years, sensing human activity has become ubiquitous and traffic has been no exception. In this vein, several studies have focused on collecting traffic data to monitor road conditions @cite @cite , roadway operation @cite @cite , and assessing driving experience @cite @cite @cite . However, the large majority of these works @cite @cite @cite @cite @cite target motorized vehicles as these are still dominant in today's traffic volume. Nevertheless, bicycle usage has recently grown mainly in urban areas @cite ; and perhaps the marginal decrease of cyclists' fatality in comparison to all other road groups @cite is a by-product of this trend. These facts have raised awareness to cyclists' safety, and the research community is starting to give more and more attention to this issue (see, for example, the February 2017 Safety Science special issue on Cycling Safety).
- Concerning cyclists' safety, @cite and @cite study the relation between the number of cyclists going through a given lane or intersection and the risk of crash with other cyclist and motorist, respectively. More recently, @cite use a large sample of GPS cyclists' trip data acquired via a smartphone application in order to validate deceleration rate as a surrogate safety measure. Particularly, the authors explore the correlation of deceleration with accidents at intersections as a potential proactive measure to prevent cyclist injuries.
- Yet, in terms of the sensors used, there has been practically no distinction between assessing drivers' or cyclists' experience, as previous methods usually depend on inertial sensor data (such as accelerometer or gyroscope). @cite we introduced a new approach to detect and identify driving events primarily based on processing images from an action camera. We are able to overcome the issues of using a camera mounted on the bicycle as an acquisition sensor, since the natural shake of the cyclists movement is filtered at the computation of the optical flow. In its previous version, the SMARTcycling tool captured and processed data from the cyclist's smartphone, an action camera, and a cardio acquisition belt. Applying image processing techniques based on optical flow descriptors to the action camera videos, the SMARTcycling tool showed good accuracy on driving events classification and road condition identification. This tool was also able to evaluate cyclists' stress using the ECG data collected from a bio-metric belt.
- Due to its amenable properties, in terms of set-up and data acquisition, we claimed that SMARTcycling @cite paves the way to large scale assessment, as cities often provide public bicycle sharing programs, where it can be easily deployed.
- In this work we delve into more involved computer vision and image processing techniques to be able to automatically identify and contextualize dangerous events from external factors, sparing both the action camera and the bio-metric belt, which imply a more complex set-up. The descriptor used in @cite was context independent (splitting the image into fixed zones), we now use a different and richer approach that encodes the context surrounding the cyclist when performing event detection. Moreover, contrary to our previous work, we analyze the whole image, incorporating motion, temporal dependence and image semantics.
- Here we follow a different direction, taking advantage of the good properties yielded by using images as primary source of data. Indeed, computer vision techniques have been applied, for quite some time, to traditional cyclist monitoring tasks as volume counts @cite and average speed, due to their reliability and efficiency when compared to manual methods @cite . However, so far no work has addressed identification of dangerous situations using an on-board smartphone camera.
- We apply state-of-the-art classification methods (convolutional Neural Networks, specifically the Faster R-CNN from @cite ) to obtain the localization and presence probability of objects in the image. The semantics provided by object detection and classification allows to interpret and understand the detected dangerous situations, providing much more insight than other types of measurements.
- The first work related with native language identification is that of @cite , in which they tried profiling anonymous authors with their native languages. Totally five different groups of English authors (whose native languages are Russian, Bulgarian, French, and Spanish) were picked from the first version of International Corpus of Learner English (ICLE) in their experiments. By applying a combined feature sets, including function words, character n-grams, part-of-speech bi-grams and spelling mistakes, they gained an accuracy of 65 Similar work was done by tsur2007using . They focused on the relationships between choice of words in second language writing and the frequency of native language syllables, also known as the phonology of native languages. estival2007author studied a wide range of lexical and document structure features in their native languages classification task. And zheng2003authorship , though they did not directly conduct related experiments on nationality detection, they provide some features of style markes that could be used in the task of judging one's native languages. Besides, gamon2004linguistic analysized the power of some general features under different frequency cutoffs. But none of these measured the usefulness of syntactic features under a general condition for the task of native language detection.
- Geometric computation on uncertain data has received considerable attentions in recent years. A general introduction can be found in @cite . Many fundamental geometric problems have been studied under uncertainty, e.g., nearest-neighbor search @cite @cite , minimum spanning trees @cite , closest pair @cite @cite @cite , range search @cite @cite , linear separability @cite @cite , dominance relation @cite , etc. These problems were studied either under existential uncertainty (which is used in this paper) or under locational uncertainty (where the locations of the data points are uncertain).
- Specifically, the expected diameter of a SCH was investigated in some recent works. Huang and Li @cite provided an FPRAS for computing the expected farthest-pair distance of a stochastic dataset in a metric space. This directly implies an FPRAS for computing the expected diameter of a SCH, since in Euclidean space the farthest-pair distance of a set of points is just the diameter of their convex hull. However, an FPRAS can only obtain the desired approximation with high probability, and there seems no way to verify whether an answer obtained by the FPRAS is truly a good approximation. @cite gave a deterministic @math -approximation algorithm in @math , which is based on (exactly) computing the expected diameter of the stochastic smallest enclosing ball. Although @cite only considered the case in @math , the algorithm can be naturally extended to compute a @math -approximation of the expected diameter of a SCH in @math . Nevertheless, the runtime of this algorithm grows exponentially as @math increases, since computing the expected diameter of the stochastic smallest enclosing ball requires @math time @cite . The width and combinatorial complexity of a SCH have not yet been investigated previously, to our best knowledge.
- For shift-invariant systems, i.e., generalized shift-invariant systems with a single, fixed translation lattice @math , the Bessel and frame properties can be in terms of (bi-infinite) matrix-valued functions, so-called , as introduced by Ron and Shen @cite , see also Janssen @cite . As a consequence, the aforementioned necessary and sufficient conditions in , and are easily derived For example, the necessary condition in for shift-invariant systems follows from the norm estimate @math by noticing that the @math th column the dual Gramian matrix at @math is @math . from simple norm estimates of bi-infinite Hermitian matrices @math on @math , see [Section 1.6] MR1350650 . In particular, the estimates and are known for separable Gabor systems @cite @cite ; these estimates are the best known improvement of Daubechies' Gabor frame bound estimates @cite . Furthermore, the dual Gramian characterization has, in a fiberization formulation @cite , been extended to the setting of locally compact abelian groups @cite @cite @cite . Hence, for shift-invariant systems, or more generally, trans -la -tion-in -vari -ant systems on such groups, the conditions , and follow from these characterizations and should not be considered new.
- An alternative route for deriving necessary and sufficient conditions for wavelet frames with integer, expansive dilations goes through quasi-affine systems @cite @cite @cite . This link is known to generalize to rational, expansive dilations @cite , although one has to consider a family of quasi-affine systems to capture the frame property of the given wavelet system. Since quasi-affine systems are shift-invariant, sufficient and necessary conditions for rational wavelet systems are readily available. We stress that such estimates differ slightly from the ones presented in this paper. The estimates presented in @cite for wavelet systems with integer, expansive dilations utilize the quasi-affine route, but they ignore the phase of the wavelet generator and are therefore not optimal for tight frames.
- Recent projects like COBRA @cite , Strata @cite and FOCUS @cite support visual light communications by streaming a sequence of 2D barcodes from a display to the camera of the receiving smartphone. However, the scope of their work is different from ours. They focus on designing new 2D (color or monochrome) barcode systems that are robust for message streaming (via video sequences) between relatively large smartphone screens (or other displays) and the capturing camera. In contrast, our work focuses on tackling the critical challenges such as CMI and CCI to support fast and robust decoding when dense color QR codes are printed on paper substrates with maximal data-capacity-per-unit-area ratio.
- H. Bagherinia and R. Manduchi @cite propose to model color variation under various illuminations using a low-dimensional subspace, e.g., principal component analysis, without requiring reference color patches. T. Shimizu et. al. @cite propose a 64-color 2D barcode and augment the RGB color space using seed colors which functions as references to facilitate color classification. Their method uses 15-dim or 27-dim feature both in training and testing which is prohibitively time-consuming for mobile devices in real-world applications. To decode color barcodes from blurry images, H. Bagherinia and R. Manduchi @cite propose an iterative method to address the blur-induced color mixing from neighboring color patches. However, their method takes more than 7 seconds on a desktop with Intel i5-2520M CPU @ 2.50GHz to process a single image, which is completely unacceptable for mobile applications.
- The idea of having one part of a language model be low-rank and another part to be an additive correction to the low-rank model has been investigated in other work @cite @cite . In both of these cases, the correction term is encouraged to be sparse by including an L1 penalty. Our implementation did not promote sparsity in the hash adaptation features but this idea is worth further consideration. The hybrid LSTM and count based language model is an alternative way of correcting for a low-rank approximation @cite .
- studies how to incorporate side information into an RNN language model. For their data, they claim a bigger win by adapting at the output layer rather than the hidden layer. (This matches our own observations on the Reddit and SCOTUS data.) Their work did not address adapting at both the hidden and output layers simultaneously. Most work on adaptation does not consider combining multiple context factors but there are some exceptions @cite @cite @cite .
- Research in automated object detection using GPR is a recent development. Prior to this research, there were two primary methods for detecting objects in GPR scan images: manual detection using the human eye, and using commercial software @cite . These two methods have respective issues. Manually detecting hundreds of rebar in an image is time consuming and requires training to correctly identify rebar, and using commercial software requires purchasing an expensive software license. The goal of research in automated rebar detection is to offer a third option that is less expensive and requires less time from the user.
- As we already mentioned most related work for context and memory in Sec. , in this section we mainly review ideas that use sequential prediction for object detection. A large portion of the literature @cite @cite @cite focuses on sequential approaches for region proposals ( , foreground background classification). The motivation is to relieve the burden for region classifiers by replacing an exhaustive sliding-window search @cite with a smarter and faster search process. In the era of ConvNet-based detectors, such methods usually struggle to keep a delicate balance between efficiency and accuracy, since a convolution based @math @math classifier ( region proposal network @cite ) already achieves an impressive performance when maintaining a reasonable speed. Sequential search has also been used for localizing small landmarks @cite , but the per-class model assumes the existence of such objects in an image and lacks the ability to use other categories as context.
- Another commonly used trick especially beneficial for reducing localization error is iterative bounding box refinement @cite @cite @cite @cite , which leverages local image context to predict a better bounding box iteratively. This line of research is complementary to our SMN, since its goal is to locate the original instance better, whereas our focus is on how to better detect objects given the current detections.
- An interesting recent direction focuses on using deep reinforcement learning (DRL) to optimize the sequence selection problem in detection @cite @cite @cite @cite . However, due to the lack of full supervision signal in a problem with high-dimensional action space Jointly reason about all bounding boxes and all classes. , DRL has so far only been used for bounding box refinements or knoledge-assisted detecton, where the action space is greatly reduced. Nevertheless, SMN can naturally serve as an encoder of the state in a DRL system to directly optimize average precision @cite .
- Note that the idea of using higher-dimensional memory in vision is not entirely new. It has resemblance to spatial attention, which has been explored in many high-level tasks @cite @cite @cite @cite . To bypass NMS, LSTM @cite cells arranged in 2D order @cite and intersection-over-union (IoU) maps @cite have been used for single-class object detection. We also notice a recent trend in using 2D memory as a map for planning and navigation @cite @cite . Our work extends such efforts into generic, multi-class object detection, performing joint reasoning on both space and semantics.
- While both our approach and use Auxiliary Context mechanism for incorporating cross-sentence context, there are two main differences: 1) we have separate parameters to better control the effects of the cross- and intra-sentence contexts, while they only have one parameter matrix to manage the single representation that encodes both contexts; 2) based on the intuition that not every target word generation requires equivalent cross-sentence context, we introduce a context gate @cite to control the amount of information from it, while they don't.
- Our work is also related to multi-source @cite and multi-target NMT @cite , which incorporate additional source or target languages. They investigate one-to-many or many-to-one languages translation tasks by integrating additional encoders or decoders into encoder-decoder framework, and their experiments show promising results.
- @cite use a set of boosted classifiers to map audio features onto tags collected from the Web. Due to the nature of their classifier, it uses the objective approach and, therefore, need the actual audio files, which we are not using. @cite survey various music-similarity measures and concludes that measures derived from co-occurrence in personal music collections are the most useful ground truth metrics from those evaluated. @cite define a song similarity measure based on the analysis of songs' timbres, and also evaluate their metric. @cite proposes a matrix factorization method that works well for data with implicit feedback, such as song listening patterns.
- * Generational Collectors Segregating objects by age has been studied for a long time @cite as a way to take advantage of the weak generational hypothesis @cite . By promoting objects that survive a number of collections into older generations, the collector can concentrate on collecting younger generations more often (since these are more likely to contain more dead objects) @cite . The use of multiple generations (compared to using a single generation) has been shown to reduce application pauses @cite @cite @cite @cite .
- Opposed to previous works such as the Beltway framework @cite and the Mature Object Space collector @cite , does not promote copy objects gradually through older generations since this only generates more object copying (which we are trying to avoid). Instead, objects are pretenured based on the object lifetime profile.
- * Pretenuring and Object Demographics Pretenuring is also a well studied technique @cite @cite @cite @cite @cite @cite . It consists on allocating objects (that are known to live for a long time) directly in older generations. By doing this, the overhead associated to object promotion is avoided. The key problem to pretenuring lies on how to estimate the lifetime profile of an object (analyzed next).
- Pretenuring is tightly coupled with object lifetime profiling, which is used to extract object lifetime estimations, used to guide pretenuring. Extracting objects demographic information can either be performed dynamically @cite @cite @cite or statically @cite @cite @cite . Profiling information can come from stack analysis @cite @cite , connectivity graphs @cite and can also include other program's traces @cite .
- Our proposed profiler, (presented in Section ) builds upon previous works @cite @cite @cite by resorting to stack analyzes. However, opposed to previous works, it accurately estimates in which generation an object should be allocated in. In other words, our profiler answers the question of how long will the object probably live while previous profilers only tell us if the object will probably live enough to be considered old (this information is not sufficient to take advantage of an N-Generational heap).
- * Region-based Garbage Collection The hypothesis that many objects, allocated in the same scope, share the same faith, i.e., have similar lifetimes has also been leveraged by many region-based memory management algorithms @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . In such algorithms, objects with similar lifetime profiles are allocated in scope-based regions, which are deallocated as a whole when objects inside these regions are no longer reachable. However, existing region-based algorithms either require sophisticated static analysis @cite @cite @cite @cite @cite @cite (which does not scale to large systems), heavily rely on manual code refactoring @cite @cite (to guarantee that objects in the same region die approximately at the same time), or support only simple programming models @cite @cite @cite (such as parallel bag of tasks).
- Other region-based collectors @cite use thread-local allocation regions to allocate objects. This approach also does not support more complex models where most large data structures can be maintained by multiple threads (for example, Cassandra's in-memory tables).
- * Off-heap based Solutions There are some solutions based on off-heap memory @cite @cite @cite (i.e., allocating memory for the application outside the GC-managed heap). While this is an effective approach to allocate and keep data out of the range of the GC (and therefore, reducing object copying), it has several important drawbacks: i) off-heap data needs to be serialized to be saved in off-heap memory, and de-serialized before being used by the application (this obviously has performance overheads); ii) off-heap memory must be explicitly collected by the application developer (which is error prone @cite @cite and completely ignores the advantages of running inside a memory managed environment); iii) the application must always have objects identifying the data stored in off-heap (these so called header objects are stored in the managed heap therefore stressing the GC). Furthermore, as shown in Section , 's approach outperforms off-heap memory.
- Although 's observable benefit is the reduction of application pause times, our contribution is orthogonal to other techniques used to implement low pause time collections such as incremental (for example Immix @cite and G1 @cite ) or concurrent compaction (for example C4 @cite ), and to real-time collectors (for example the work in Fiji VM @cite and the Metronome @cite collector).
- Many constructions of erasure codes have been proposed to reduce the repair traffic. Regenerating codes @cite are a special family of erasure codes that minimize the repair traffic and provably achieve the optimal trade-off between storage redundancy and repair traffic. Constructions of regenerating codes have been proposed, such as Interference Alignment codes @cite @cite @cite , Product-Matrix codes @cite , Zigzag codes @cite , FMSR codes @cite @cite , PM-RBT codes @cite , and Butterfly codes @cite . As stated in , recent studies @cite @cite @cite @cite also propose MSR code constructions for general parameters.
- Some erasure codes aim to minimize I O (i.e., the amount of data read from storage) during repair. For example, Rotated RS codes @cite and Hitchhiker @cite propose new parity constructions that send fewer blocks in a single-node failure repair.
- Some erasure codes trade storage efficiency for repair performance. Simple Regenerating Codes (SRC) @cite retrieve data from a small number of non-failed nodes to repair a failed node, thereby limiting the I O overhead of accessing non-failed nodes during repair. Locally repairable codes trade storage efficiency for repair performance by associating local parity blocks with different subsets of nodes. Thus, they can retrieve data from a smaller number of nodes during repair, and limit both repair traffic and I O. Two representative constructions are Azure's LRC @cite and Facebook's LRC @cite .
- The above erasure codes mainly adopt flat block placement in hierarchical data centers to tolerate rack failures (as mentioned in @cite @cite @cite @cite @cite @cite ). Our work complements the above studies by specifically minimizing the critical cross-rack repair traffic via hierarchical block placement.
- Erasure-coded repair in hierarchical data centers has been studied, but in a limited context. Some studies focus on a data center with two racks @cite @cite , or propose locally repairable codes for multiple racks @cite . R-STAIR codes @cite place an extra parity block in each rack to allow rack-local repair without cross-rack traffic. However, R-STAIR codes require sophisticated configurations of parameters of full-rack and partial-rack fault tolerance. CAR @cite is specifically designed for RS codes, and exploits intra-rack encoding to reduce the cross-rack repair traffic. However, CAR does not provide any theoretical guarantee of minimizing the cross-rack repair traffic as in DRC @cite . We extend the DRC framework @cite from the applied side: we provide practical DRC constructions and evaluate their prototype implementation.
- Some studies propose efficient repair approaches for existing erasure codes. For example, lazy repair @cite @cite triggers repair only when the number of failures reaches a threshold to avoid repairing temporary failures. CORE @cite extends existing regenerating codes to support the optimal recovery of multi-node failures, and presents a prototype implementation on HDFS. HACFS @cite dynamically switches encoded blocks between two erasure codes to balance storage overhead and repair performance. PPR @cite divides a repair into partial operations executed by multiple servers in parallel to reduce the overall repair time. Repair pipelining @cite further reduces the repair time to almost the same as the normal read time by slicing the repair along a linear chain. Our work differs from them by proposing new regenerating code constructions for hierarchical data centers.
- In recent years, vision data is being interpreted as sequences leading to the successful application of RNNs (and their variants, e.g. Long-Short Term Memories (LSTMs), Gated Recurrent Units (GRUs), etc.,) to vision problems. For instance, @cite and @cite applied 1-D RNNs and multi-dimensional RNNs to model contextual dependencies in object recognition image classification and offline handwriting recognition respectively. 2-D LSTMs instead were applied to scene parsing @cite , Lately, scene labeling @cite , object segmentation @cite , have been reformulated as sequence learning, thereby allowing RNNs to be applied directly. Scene labeling, in particular, has seen the use of RNNs coupled with Directed Acyclic Graphs (DAGs) to model an image as a sequence @cite @cite @cite . There have also been a few studies that utilize RNNs to compute visual representations @cite @cite . Clearly, RNN-based approaches are .
- (1) The ConvNet-based approaches model makes use of convolutional filters which allow them to learn the short-range context of surrounding neighbors designed by these filers. Therefore they are limited to generalize to long-range contexts dependencies. (2) The RNN-based approaches usually utilize a feature extractor that is independent of the sequence modeling framework, in many cases being trained component wise and not end-to-end @cite . (3) Purely RNN based approaches fail to extract robust visual features during sequence learning itself. This is due to simple linear models being used as the recurrent internal models.
- Reinforcement learning approaches solve tasks through repeated interactions with the environment guided by a reward signal that indicates the success or failure of a trial. A wide variety of techniques have been developed that exploit this idea @cite , with a broad distinction often made between value-based and policy search methods. While the former estimate and improve a value function, policy search methods directly optimize the parameters of a policy to maximize cumulative reward. The latter have been routinely applied in robotics, in part because they straightforwardly handle continuous and high-dimensional action spaces @cite and applications include manipulation @cite @cite @cite @cite @cite @cite @cite @cite , locomotion e.g. @cite @cite , and a range of other challenges such as helicopter flight @cite .
- The use of rich and flexible function approximators such as neural networks in RL dates back many years, e.g. @cite @cite @cite @cite . In the last few years there has been a resurgence of interest in end-to-end training of neural networks for challenging control problems, and several algorithms, both value and policy focused have been developed and applied to challenging problems including continuous control, e.g. @cite @cite @cite @cite @cite @cite @cite @cite @cite . These methods work well with large neural networks and can learn directly from raw visual input streams. With few exceptions, e.g. @cite @cite @cite @cite , they have been considered too data-inefficient for robotics applications.
- One exception are guided policy search methods (GPS) @cite @cite . These have recently been applied to several manipulation problems and employ a teacher algorithm to locally optimize trajectories which are then summarized by a neural network policy. GPS algorithms gain data-efficiency by employing aggressive local policy updates and by performing extensive training of their neural network policy before collecting more real-world data. The teacher can use model-based @cite or model-free @cite trajectory optimization. The former can struggle in situations with strong discontinuities in the dynamics, and both rely on access to a well defined and fully observed state space.
- This work thus made use of a complementary solution to the need for large amounts of interaction data: the use of experimental rigs that allow large scale data collection, e.g. @cite , including the use of several robots from which experience are gathered in parallel @cite @cite @cite . This can be combined with single machine or distributed training depending on whether the bottleneck is primarily one of data collection or also one of network training @cite .
- Finally, the use of demonstration data has played an important role in robot learning, both as a means to obtain suitable cost functions @cite @cite @cite @cite but also to bootstrap and thus speed up learning. For the latter, kinesthetic teaching is widely used @cite @cite @cite @cite . It integrates naturally with trajectory-based movement representations but the need for a human operator to be able to guide the robot through the full movement can be limiting. Furthermore, when the policy representation is not trajectory based (e.g. direct torque control with neural networks) the use of human demonstration trajectories may be less straightforward (e.g. since the associated controls are not available).
- The first CNN-based watermarking technique is proposed by @cite . Unlike the proposed technique, @cite uses CNN as an auto-encoder to present a new domain. However, @cite is a non-blind method. Therefore, although the robustness of these techniques is high, they are less useful in real scenarios. The proposed technique, on the other hand, is a blind watermarking.
- More recently, social media text normalization was tackled by using contextual graph random walks. @cite proposed a method that uses random walks on a contextual similarity bipartite graph constructed from n-gram sequences on large unlabeled text corpus to build a normalization lexicon. They obtained a precision of 92.43 Our technique is most similar to @cite , since we implement an adaptation of the method presented in the mentioned work. The method proposed by @cite aims to learn distributed representations of words to capture the notion of contextual similarity and subsequently learn normalization lexicons from these representations in a completely unsupervised manner. The lexicons are represented as finite-state machines (FSMs) and the process of normalization is performed by transducing the noisy words from the FSMs. Our work makes use of different distributed representation of words, different scoring function for candidate generation and hash structures (dictionaries) instead of FSMs. We also introduce a method for automatically expanding the learned lexicons.
- Since a large part of misspellings found in UGC is phonetically-motivated, @cite proposed a phonetic-based speller for correcting such errors. The speller combines edit distance and several specific phonetic rules for Portuguese in order to generate correction candidates. The correction of internet slang and proper name and acronyms capitalization is based on a set of lexicons. Each lexicon contains many pairs of wrong--correct form of words. The correction is performed by looking up the noisy word in the lexicon and substituting it by the correct version. Despite this technique achieving good results in the product review domain, it is not scalable and is too restricted, since there is no form of automatic lexicon-learning. Therefore, it is not suitable for a generic, domain-free normalizer. The results obtained by @cite will be further discussed, as they are the main source of comparison for our work.
- Action recognition is a well studied problem with standard datasets @cite @cite @cite @cite @cite @cite focused on tasks such as classification @cite @cite @cite @cite @cite and temporal or spatio-temporal localization @cite @cite @cite . Action recognition is, however, hard due to the large intra-class variability of different actions and difficulty of annotating large-scale training datasets. As a result, the performance of automatic recognition methods is still far below the capability of human vision. In this paper we focus on the problem of action classification, i.e., classifying a given video clip into one of @math given actions classes. We review the main approaches to this problem below followed by a brief review of feature aggregation.
- Dense trajectories: Up until recently, the dominating video representation for action recognition has been based on extracting appearance (such as histograms of image gradients @cite ) and motion features (such as histogram of flow @cite ) along densely sampled point trajectories in video. The descriptors are then aggregated into a bag-of-visual-words like representation resulting in a fixed-length descriptor vector for each video @cite @cite . The representation can be further improved by compensating for unwanted camera motions @cite . This type of representation, though shallow, is still relevant today and is in fact part of the existing state-of-the-art systems @cite @cite @cite . We build on this work by performing a video-level aggregation of descriptors where both the descriptors and the parameters for aggregation are jointly learned in a discriminative fashion.
- Convolutional neural networks: Recent work has shown several promising directions in learning video representations directly from data using convolutional neural networks. For example, Karpathy alWspace @cite showed the first large-scale experiment on training deep convolutional neural networks from a large video dataset, Sports-1M. Simonyan and Zisserman @cite proposed the two-stream architecture, thereby decomposing a video into appearance and motion information. Wang alWspace @cite further improved the two-stream architecture by enforcing consensus over predictions in individual frames. Another line of work has investigated video representations based on spatio-temporal convolutions @cite @cite , but these methods have been so far hard to scale to long videos (maximum of 120 frames in @cite ), limiting their ability to learn over the entire video.
- Feature aggregation: Our work is also related to feature aggregation such as vectors of locally aggregated descriptors (VLAD) @cite and Fisher vectors (FV) @cite @cite . Traditionally, these aggregation techniques have been applied to keypoint descriptors as a post processing step, and only recently have been extended to end-to-end training within a convolutional neural network for representing still images @cite . We build on this work and extend it to an end-to-end trainable video representation for action classification by feature aggregation over space and time.
- With the seminal work of SRCNN @cite , a majority of recent SR methods employ deep neural networks @cite @cite @cite @cite @cite @cite . Most of them resize input frames before sending them to the network @cite @cite @cite @cite , and use very deep @cite , recursive @cite or other networks to predict HR results. Shi al @cite proposed a subpixel network, which directly takes low-resolution images as input, and produces a high-res one with subpixel location. Ledig al @cite used a trainable deconvolution layer instead.
- For deep video SR, Liao al @cite adopted a separate step to construct high-resolution SR-drafts, which are obtained under different flow parameters. Kappeler al @cite estimated optical flow and selected corresponding patches across frames to train a CNN. In both methods, motion estimation is separated from training. Recently, Caballero al @cite proposed the first end-to-end video SR framework, which incorporates motion compensation as a submodule.
- Progress was made in spatial transformer networks @cite where a differentiable layer warps images according to predicted affine transformation parameters. Based on it, WarpNet @cite used a similar scheme to extract sparse correspondence. Yu al @cite warped output based on predicted optical flow as a photometric loss for unsupervised optical flow learning. Different from these strategies, we introduce a Sub-pixel Motion Compensation (SPMC) layer, which is suitable for the video SR task.
- With the advent of modern VR systems, it become very natural to extend 2D painting to 3D space; beginning with @cite 's work, VR painting systems such as Tilt Brush http: www.tiltbrush.com and Quill http: storystudio.oculus.com lead a new trend for 3D painting and modeling. All these systems render quad strips as a result of painting strokes, thus have a difficulty in representing volumetric objects. Furthermore, voxel-based modeling or painting such as Oculus Medium http: www.oculus.com medium or High Fidelity http: highfidelity.io were introduced. However, due to the space and time complexity of uniform grid, they support only limited canvas spaces and also limited detail.
- Existing ray-casting algorithms for adaptive grids often utilize acceleration techniques, such as empty space skipping @cite , by using multiple rendering passes. In order to reduce neighbor finding cost, octree neighbor linking were proposed @cite @cite . Also the ROPE algorithm @cite was developed to accelerate k-d tree neighbor finding. These precomputed neighbors have been continuously used. @cite suggests stackless ray-casting after updating all six neighbors from the CPU to the GPU in view-dependent manner. @cite @cite used similarly precomputed neighbors to accelerate neighbor finding for samples stored at octree corners. In this paper, we make a small step forward by using only three neighbors, computed in the GPUs from the primal octree represented by only two indices: parent and the first child.
- To our best knowledge, octrees as deep as 24 were not used for ray casting. The deepest trees we found was @math used in @cite , which is equivalent to the octree depth of 12 (our canvas is equivalent to @math ), and the ray angle drift error has not been identified as a challenge. We propose a solution and thoroughly analyze the numerical precision associated with ray drift during ray traversal using a very deep tree.
- Previous work on incorporating network structure into trust models has focused on authentication protocols, showing that independent paths can reduce an adversary's ability to impersonate a target @cite . Other work has shown that identifying independent paths in arbitrary networks is NP-hard and provided approximation algorithms @cite . Our work complements these by introducing the partial trust assumption extending the focus beyond authentication. When network topology can be controlled, we sidestep the NP-hard problem of finding independent paths on arbitrary networks by using the mathematical structure of the butterfly topology to construct provably independent paths.
- Our proposed routing algorithm makes use of a structured network , in which link structure is predetermined. Structured networks have been a popular tool in parallel processing architectures @cite . More recently, peer-to-peer systems based on distributed hash tables have used structured overlay networks to map table keys to local TCP IP routes @cite @cite . Such networks can be designed to have favorable structural and routing properties, which can be used to to improve attack-tolerance.
- Face alignment has advanced in the last five years after the reemergence of deep neural networks. Following @cite , we classify previous works on face alignment into two basic categories.
- model approaches are statistical methods which perform keypoint detection by maximizing the confidence of part locations in a given input image. Zhu and Ramanan @cite used a part-based model for face detection, pose estimation and landmark localization assuming the face shape to be a tree structure. @cite by proposed learning a dictionary of probability response maps followed by linear regression in a Constrained Local Model (CLM) framework. Other works in this category include Active Shape Models @cite and Constrained Local Models @cite . To enforce the message passing protocol between different keypoints the proposed method assumes a tree structure of the keypoints; however does not assume that all the keypoints are visible and contributing equally to each other. In the proposed tree structure the messages between neighboring keypoints are passed via learnt transform kernels which are further conditioned on the 3D head pose of the face image.
- In a framework, image appearances are directly mapped to output space producing keypoint coordinates. Recently, a multitude of cascade regression-based methods @cite , @cite significantly boost the keypoint detection performance, compared to statistical methods described above. However, these methods along with methods from @cite , @cite , @cite and @cite were mostly evaluated on face images where all the facial keypoints are visible. To handle occlusion, @cite proposed an occlusion-robust cascaded regressor. Supervised Descent Method (SDM) @cite learns a cascade of regression models based on SIFT features. To mitigate the conflicting gradient directions in SDM, @cite suggested domain dependent descent maps. Inspired by @cite , Cascade Compositional Learning (CCL) @cite developed a head pose based method by partitioning the optimization domain. Different from all these methods, our approach is a non-iterative single shot method, which along with keypoint locations also provides the estimated 3D head pose and individual visibility of each fiducial point.
- Researchers also proposed using 3D morphable models to estimate the landmark points. Pose Invariant Face Alignment (PIFA) @cite by suggested a 3D model-based approach that employed cascaded regressors to predict the coefficients of 3D to 2D projection matrix. @cite also by formulated the face alignment problem as a dense 3D model fitting problem, where the camera projection matrix and 3D shape parameters were estimated by a cascade of CNN-based regressors. However, @cite suggests that optimizing the base shape coefficients and projection is indirect and sub-optimal since smaller parameter errors are not necessarily equivalent to smaller alignment errors. 3DDFA @cite modeled depth data in Z-Buffer and fitted a dense 3D face model to the image via CNNs. In contrast to these methods the proposed method does not rely on 3D morphable models, but still provides with accurate 3D pose estimates.
- The question of whether @math -sampling is possible in low memory in turnstile streams was first asked in @cite @cite . The work @cite applied @math -sampling as a subroutine in approximating the cost of the Euclidean minimum spanning tree of a subset @math of a discrete geometric space subject to insertions and deletions. The algorithm given there used space @math bits to achieve failure probability @math (though it is likely that the space could be improved to @math with a worse failure probability, by replacing a subroutine used there with a more recent @math -estimation algorithm of @cite ). As mentioned, the currently best known upper bound solves @math -sampling @math using @math bits @cite , which Theorem shows is tight.
- Recently, there has been a growing interest for demographic aspects of social media data. Mislove @cite provided one of the first studies in this space, by looking at the gender and racial demographics of Twitter users, and analyzing how the demographics vary across different US states. Pew research @cite conducted user surveys to understand the demographics of users in different social media platforms. There have also been past attempts to understand the use of social media among particular demographic groups. For example, Madden @cite explored how teenagers use different social media sites. Gilbert @cite analyzed social media use among rural users. In another research direction, many efforts attempt to quantify inequalities in social media systems, including Wikipedia @cite , Pinterest @cite , and Twitter @cite . However, to the best of our knowledge, we are not aware of any effort that approached the demographics behind crowdsourced recommendations deployed in social media sites. Thus, our endeavor is complementary to the above mentioned approaches.
- Many prior works have focused on Twitter Trending Topics. For example, Zubiaga presented an approach to automatically categorize trending topics @cite . Lee characterized how spammers can exploit trending topics @cite . Benhardus @cite proposed alternative algorithms for detecting trending topics. Our prior work @cite identified temporal coverage biases in the selection of trending stories.
- Our present effort contributes to make the demographic biases of Twitter trend recommendations transparent, and we hope that the methodology to compute demographic distribution of users can be leveraged to make other crowdsourced systems (e.g., social search @cite ) transparent as well. More importantly, for algorithms that operate on large-scale crowdsourced data, we show that along with making the outputs of the algorithms (and the algorithm itself) transparent, it is also important to understand the non-uniformities in the inputs to the algorithms.
- MIL is a classical weakly supervised learning problem and was first proposed in @cite for drug activity prediction. After that, many solutions have been proposed for MIL @cite @cite @cite . In MIL, a set of bags are given, and each bag is associated with a collection of instances. MIL has two constraints: 1) If a bag is positive, at least one instance in the bag is positive; 2) If a bag is negative, all instances in the bag are negative. It is natural to treat WSOD as a MIL problem. Then the problem turns into finding an instance classifier only given bag labels. Our method also follows the MIL line, and the classifier refinement is inspired by the classifier updating procedure in mi-SVM @cite to some extent. The differences are that, in mi-SVM, it uses an alternative strategy to relabel instances and retrain a classifier, while we adopt an online refinement algorithm; the mi-SVM relabel instances according to the instance score predicted by the classifier, while we select instances according to the spatial relation.
- In statistical machine translation (SMT), synthetic bilingual data have been primarily proposed as a means to exploit monolingual corpora. By applying a self-training scheme, the pseudo parallel data were obtained by automatically translating the source-side monolingual corpora @cite @cite . In a similar but reverse way, the target-side monolingual corpora were also employed to build the synthetic parallel data @cite @cite . The primary goal of these works was to adapt trained SMT models to other domains using relatively abundant in-domain monolingual data.
- Inspired by the successful application in SMT, there have been efforts to exploit synthetic parallel data in improving NMT systems. Source-side @cite , target-side @cite and both sides @cite of the monolingual data have been used to build synthetic parallel corpora. In their work, the pseudo parallel data combined with a real training corpus significantly enhanced the translation quality of NMT. In , , domain adaptation of NMT was achieved by fine-tuning trained NMT models using a synthetic parallel corpus. attempted to build NMT systems without any direct source-to-target parallel corpus. In their work, the pseudo parallel corpus was employed in fine-tuning the target-specific attention mechanism of trained multi-way multilingual NMT @cite models, which enabled zero-resource NMT between the source and target languages. Lastly, synthetic sentence pairs have been utilized to enrich the training examples having rare or unknown translation lexicons @cite .
- One of the most important challenges in large Scrum teams is inter-team coordination @cite . Coordination requires communication both between teams and within a team. @cite shows in a case study that handling with knowledge over a longer period of time can only be managed with extensive communication. But important events that occur during development could be relevant to others and are often not sufficiently documented or communicated @cite . Software developers do not update relevant documents, do not see their benefits and wish more automatic generation of documented content @cite @cite . An example of such a generation is an automatic summarizer for daily scrum meetings proposed by @cite . Nevertheless there is a gap in the current state of the art on how developers of agile teams stay informed when being absent. Our proposed tool tries to fill this gap.
- In multi-object tracking, data association based methods fall into a sub-domain known as the technique, which has shown impressive tracking performance in unconstrained environments. A thorough review can be found in @cite . As evidenced in Section sec:introduction , the local association method has aroused considerable research interests. Especially with the success of recurrent neural networks (RNNs) in computer vision community @cite , RNNs-based methods have witnessed significant advances on MOT problems. Based on the pioneer work introduced by Ondruska and Posner @cite , RNNs-based method quickly sparked significant interest to model the local association, and inspired a number of extensions including @cite @cite @cite . Nevertheless, the RNNs usually comes with high computational and memory demands both during the model training and inference. We here introduce to explicitly enforce locality into the global data association formulation, and introduce a hybrid data association framework that is able to integrate the advantages of both local and global association methods.
- Maintaining locality for global data association is critical for multi-object tracking performance, since global optimization might scale poorly for the complex scenario and long batches without local constraints. Many global association methods enforce locality by iteratively optimizing trajectories @cite @cite @cite @cite , or using tracklets ( , short-term trajectory fragments) instead of individual detections @cite @cite @cite . However, these strategies are hardly applied to online video streams. Alternatively, one can divide an online video stream into consecutive batches with temporal sliding windows, and apply global data association to each video batch @cite @cite @cite . In order to produce consistent trajectories, the connection between optimized trajectories from adjacent batches need to be considered. However, most existing methods adopt heuristic strategies to connect adjacent batches and can not ensure the optimality of the trajectories.
- Recently, multi-commodity flow has been introduced into multi-object tracking in @cite @cite . Ben Shitrit al @cite employed the multi-commodity network to account for different appearance groups which are fixed beforehand. Each appearance group ( , a basketball team) is supposed to be a specific commodity in the network, and solving multi-commodity flow problems is able to distinguish different appearance groups during the optimization process. Dehghan al @cite have focused on integrating object detector learning and multi-object tracking, where the multi-commodity network is used to track a fixed number of objects in a short video batch. Our approach is different from these methods in that we use a multi-commodity network to formulate a hybrid data association strategy to handle online data. Furthermore, a high-quality near-optimal solution to the min-cost multi-commodity flow problem can be achieved by an efficient algorithm, especially when the number of objects (commodities) is relatively large. Thus we do not need to heuristically prune the graph @cite or iteratively relax the hard constraints @cite .
- goes into similar directions with our analysis. @cite analyses the collaborator structure of Wikipedia. They further classify the collaborators into five different classes based on the number of revisions. Furthermore, they measure the population growth of the collaborators falling into the five different classes. In their paper the authors conclude an interesting observation of the shift of how content is mostly provided by collaborators with lower number of edits, due to the increased fraction of such users in the Wikipedia community structure. This, however, does not correlate with any decline of the content provided by collaborators with high number of edits, hence, is accounted to the higher fraction of low edit users. In contrast to the work from , we have a different focus in our analysis, namely that of entity and event lag in Wikipedia, without any distinction of the Wikipedia community structure. In @cite the authors analyze several aspects of Wikipedia's editors. They conclude that the number of edits is decreasing. Another slightly related work @cite analyzes the number of research papers about Wikipedia, here too they conclude that the number has been decreasing, however, papers that use Wikpedia's data has seen an increase.
- Closely related work is done by @cite @cite @cite . Their work, similarly to ours, focuses on the dynamics of Wikipedia's coverage of real world entities. @cite , the authors consider emerging events like the T = o hoku catastrophe http: en.wikipedia.org wiki 2011_T tries to detect links between entities withing a knowledge base. The work done by @cite uses social network theory measures, such as Katz index to find links between entities. This is related to our work since we analyze the co-referencing of entities within Wikipedia, their collaborator structure and interlinking with events in the Wikipedia's event portal. Such attributes of entities are used to analyze their implications on the entity lag in Wikipedia against news corpora.
- typically deals with event onset identification from a stream of text. @cite , , analyze twitter data for first story detection. Wikipedia in this case is used through its entity event page views to filter tweets that do not represent events. The two sources of information are considered as streams which later on are mapped, by simply checking the spikes of page views for a certain entity event in a tweet. In our case, the focus is at modeling between two sources of information, Wikipedia and NYT corpus, rather than its usage for story detection.
- Reinforcement learning @cite is a technique that is used to solve a Markov decision process (MDP). A MDP is a tuple @math where @math is the set of possible world states, @math is the set of possible actions, @math is a transition function @math , @math is the reward function @math , and @math is a discount factor @math .
- Our proposed solution is to adapt techniques originally designed to adversarially attack machine learning systems. In adversarial attacks against machine learning systems, particularly neural network based machine vision systems, an adversarial system learns how to generate sensory stimuli that produce the wrong classification while being indistinguishable by humans from real stimuli @cite @cite @cite @cite . Instead of causing our reinforcement learning agent to make an incorrect assessment of its sensory inputs, we will cause our agent to incorrectly believe it is accruing reward and manipulating the environment even though it has been interrupted.
- has recently gained a lot of attention in NLP and deep learning. represent words in a low-dimensional, continuous space where each dimension corresponds to semantic or syntactic latent features. Similar to distributional word embeddings, are usually based upon co-occurrence statistics, but they are more compact, less sensitive to data sparsity, and able to represent an exponential number of word clusters @cite @cite @cite .
- Memory caching has a long history and has been widely employed in storage systems @cite , databases @cite , file systems @cite , web servers @cite , operating systems @cite , and processors @cite . Over the years, a vast amount of caching algorithms have been proposed. These algorithms run on a single machine and can be broadly divided into two categories:
- LRU @cite and LFU @cite are the two widely used caching algorithms. As shown in Section , neither algorithm adapts well to the data access pattern in data analytic clusters even though they are simple to implement. Many cache policies evict prefetch data blocks through hints from applications @cite @cite , which are provided by the programmers to indicate what data will be referenced again and when. Nevertheless, inserting such hints can be difficult to the programmer, who has to carefully examine the underlying data access pattern.
- Despite the significant performance impact of memory caches, cache management remains a relatively unchartered territory in data parallel systems. Prevalent parallel frameworks (e.g., Spark @cite , Tez @cite , and Tachyon @cite ) simply employ LRU to manage cached data on cluster machines, which results in a significant performance loss @cite @cite .
- To our knowledge, the recently proposed MemTune @cite is the only caching system that leverages the application semantics. MemTune dynamically adjusts the memory share for task computation and data caching in Spark and evicts prefetches data as needed. As opposed to our proposal that accounts for the entire DAG and its dependencies, MemTune only considers locally dependent blocks of currently runnable tasks. Moreover, when it comes to a multi-tenant environment, the complexity of MemTune is also multiplied, as MemTune keeps track of all the submitted DAGs and traverses them at runtime to search for the downstream tasks whenever a task completes. In comparison, LRC parses the job DAG upon job submission and only sends the reference count profile to the corresponding worker node. The reference count updating message is light and simple compared to the DAG structure itself, and can be easily applied to any DAG-based system and even heterogeneous environments running different systems, like Tachyon @cite .
- Our work is closely related to that on similarity matching using Siamese Networks @cite , which was extended for convolutional networks for face verification in @cite . Koch et. al @cite show results using convolutional variants of these networks on the few shot classification tasks for Omniglot. Various formulations for the similarity objective such as contrastive loss @cite and triplet loss @cite and efficient methods to do the same @cite are also closely related.
- Other related approaches include those based on Deep Metric Learning such as deep embeddings for triplet loss @cite and better metrics for fine-grained similarity @cite , and popular methods like Neighbourhood Component Analysis @cite .
- Incremental SfM methods @cite @cite @cite register a new image to a partial model already constructed, using 3D-2D correspondences. A number of Perspective-n-Points (PnP) algorithms @cite @cite have been proposed for solving this resection problem . , Whatever the method, at least 3 correspondences common to 3 images are required: three 3D points visible in the same 3 images, and up to minimum of 6 @cite .
- Hierarchical SfM methods, that additionally merge partial models, also have similar constraints. In @cite , the two models to merge are overlapping in the sense that they share one or several images, and pairs of 3D points projecting to the same 2D features in both models are used to relate the models, which implies tracks of length 3 or more. In @cite , models are merged via feature matches between images separately associated to each model, requiring that the same four 3D points are reconstructed in both models, which implies in turn tracks of length 4 (connections between 2 tracks of length at least 2). Even with relaxed requirements where merging uses 4 points that are seen but not necessarily reconstructed in the other model @cite , feature tracks of length 3 are required.
- As for global SfM methods, their main objective is merging relative motions between two cameras into a consistent graph of all cameras. Besides robustness concerns, to get rid of outlier edges, and various approaches to average rotations, one of the main issues is that the relative translations are only given up to an unknown scale factor; only their directions are known. Most methods to infer global translations rely on information redundancy assuming a densely-connected graph @cite @cite , or on additional information from trifocal tensors @cite @cite (hence requiring 4 tracked points across 3 views). A number of other methods @cite @cite @cite compute the global translations, possibly along with the 3D points, by solving equations relating points visible in two images; however, they implicitly assume that enough points are visible in at least 3 images to cancel the degrees of freedom of the relative scale factors. Besides, they do not all address point match outliers.
- More generally, when associating both points and lines in a Perspective-n-Features'' framework, a minimum of 3 features visible in 3 views is still required @cite @cite .
- Our approach for relating the scale of two bifocal calibrations is based on coplanar line pairs. To our knowledge, coplanar lines have been used for pose estimation, but only in a two-view context and with a Manhattan-world assumption, to identify planar structures @cite . A related topic is plane-based SfM, but it has mostly been studied assuming prior knowledge (user-given) about the scene planes @cite @cite or in tracking scenarios with videos @cite . In @cite @cite , a reference plane is used to estimate both the global translations and 3D points, but it must be visible in all images.
- A related work regarding the estimation of scale factors and the identification of planar structures concerns direct structure estimation (DES) via homography estimations, with the computation of coplanar point clusters, but it does not estimate poses and it also relies on trifocal points @cite .
- Line triangulation and line bundle adjustment has been well studied given an initial global pose estimation @cite , but not associated to coplanarity issues.
- Stacked auto-encoders @cite @cite @cite @cite @cite @cite have been studied in the past years for unsupervised deep feature extraction and nonlinear dimension reduction. Their extensions for dealing with images are convolutional stacked auto-encoders @cite @cite . Most of these methods contain a two-stage training procedure @cite : one is layer-wise pre-training and the other is overall fine-tuning. One of the significant drawbacks of this learning procedure is that the layer-wise pre-training is time-consuming and tedious, especially when the base layer is a Restricted Boltzmann Machine (RBM) rather than a traditional auto-encoder or when the overall network is very deep.
- Recently, there is an attempt to discard the layer-wise pre-training procedure and train a deep auto-encoder type network in an end-to-end way. In @cite , a deep deconvolution network is learned for image segmentation. The input of the architecture is an image and the output is a segmentation mask. The network achieves the state-of-the-art performance compared with analogous methods thanks to three factors: 1) introducing a deconvolution layer and a unpooling layer @cite @cite @cite to recover the original image size of the segmentation mask, 2) applying the batch normalization @cite to each convolution layer and each deconvolution layer to reduce the internal covariate shifts, which not only makes an end-to-end training procedure possible but also speeds up the process, and 3) adopting a pre-trained encoder on large-scale datasets such as VGG-16 model @cite . The success of the architecture motivates us that it is possible to design an end-to-end training procedure for fully convolutional auto-encoders.
- Clustering has also been studied in the past years based on independent features extracted from auto-encoders (see, e.g. @cite @cite @cite @cite ). Recently, there are attempts to combine the auto-encoders and clustering in a unified framework @cite @cite . In @cite , the authors proposed Deep Embedded Clustering (DEC) that learns deep representations and cluster assignments jointly. DEC uses a deep stacked auto-encoder to initialize the feature extraction model and a Kullback-Leibler divergence loss to fine-tune the unified model. In @cite , the authors proposed Deep Clustering Network (DCN), a joint dimensional reduction and @math -means clustering framework. The dimensional reduction model is based on deep neural networks. Although these methods have achieved some success, they are not suitable for dealing with high-dimensional images due to the use of stacked auto-encoders rather than convolutional ones. This motivates us to design a unified clustering framework based on convolutional auto-encoders.
- However, the classification approach taken here is closer related to previous work done by Schwenk @cite , with feed-forward neural networks that take in a fixed input sentence and output the probability of a sequence of words occurring next, as well as the work of , who only predicted the next word @cite . The difference in our approach is that the GRU used here can take in a variable-length sentence, and is much more flexible than the models by Schwenk and , which were restricted to only accepting input of a pre-specified fixed length.
- Data-driven system identification is a popular approach that is at the core of learning for control techniques. Examples of these techniques include model-based reinforcement learning for instance @cite . We focus here on works related to learning mechanical models of unknown objects. Several cognitive models that combine Bayesian inference with approximate knowledge of Newtonian physics have been proposed recently @cite @cite @cite . These methods learn probabilistic models from noisy physical simulations. Nevertheless, these models are built to explain the learning of Newtonian physics in humans, rather than to be used for robotic manipulation, which typically requires a higher precision as well as faster learning and inference times.
- Another alternative, which is becoming increasingly popular, addresses these challenges through end-to-end learning @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . This involves the demonstration of many successful examples of physical interaction and learning the controls for solving a problem as a function of the sensing input. These approaches, however, usually require many physical experiments to effectively learn. The proposed method aims to be more data-efficient, and can quickly adapt online to minor changes in object dynamics. Furthermore, it is not clear for existing methods how uncertainty, a consequence of learning from a small number of data points, could be handled in a principled way. Note that there is a significant body of work on learning sliding models of objects using white-box optimization @cite @cite @cite . It is not clear, at the moment, if these methods would perform better than the proposed approach. A drawback of white-box methods is that they are often used only in simple setups, such as pushing planar objects @cite .
- Previous work directed toward automatic lymph node detection is limited @cite @cite . A special filter was used in @cite to detect lymph nodes. This minimum directional difference (Min-DD) filter is constructed with the assumption that lymph nodes have uniform intensity, which is not always the case. The Min-DD filter method @cite was improved in @cite by adding a Hessian-based blobness measure for reducing false positives. Several automatic algorithms have been proposed for liver lesion detection and segmentation, including combinations of adaptive multi-thresholding and morphological operators @cite or k-means clustering on mean shift filtered images @cite . However, these histogram-based methods require a good contrast between lesions and parenchyma. Other techniques, such as AdaBoost, have been used in both semi-automatic approaches @cite and in automatic settings to classify image textures @cite . In another approach, Shimizu @cite trained two AdaBoost classifiers with a set of statistical and gradient features, as well as with features based on a convergence index filter that enhances blob-like structures. Our work is innovative due to the following:
- Our method uses language information to guide the visual recognition. This corresponds to the recent trend in utilizing language information for benefiting visual recognition. For example, language information has also been incorporated in phrase grounding @cite @cite @cite tasks. @cite @cite , attention model is employed to extract linguistic cues from phrases. Language guided attention has also been widely used in visual question answering @cite @cite @cite @cite and has recently been applied to one-shot learning @cite .
- @math @cite casts graphs into a format suitable for learning convolutional neural networks ( @math s): 1) graphs are decomposed into a fixed number of neighborhood subgraphs; 2) which are then casted to a fixed-size receptive field. Both 1) and 2) involve either padding or truncation in order to meet the fixed-size requirements. The truncation operation can be detrimental for the statistical performance of the downstream @math since it throws away part of the input graph. On the other hand @math is able to handle structured inputs of variable sizes without throwing away part of the them. And this is one of the reasons because @math has better statistical performance than @math (See ).
- The creation of large scale cloze datasets such the DailyMail CNN dataset @cite or the Children's Book Corpus @cite paved the way for the construction of end-to-end neural architectures for reading comprehension. A thorough analysis by , however, revealed that the DailyMail CNN was too easy and still quite noisy. New datasets were constructed to eliminate these problems including SQuAD @cite , NewsQA @cite and MsMARCO @cite .
- However, most of these approaches do not consider the semantic similarities between the combinations of different words. Differing from these previous approaches, we exploit item descriptions by making use of distributed representations of words @cite to assist with recommendation when rating and labeling data are sparse.
- The most closely related work is @cite where the authors use temporal variation in search volumes to forecast fertility rates for U.S. states. Our approach is different as we focus on variation, on the meaning and context of births, and make use of Google Correlate. Another relevant study using Google Correlate is @cite which shows that the search terms Google Correlate lists for birth rates" and infant death rates" in different states are statistically meaningful.
- Also related to fertility, Reis and Brownstein @cite show that the volume of Internet searches for abortion is proportional to local abortion rates and directly proportional to local restrictions on abortion.
- A large body of work benefits from implicit or explicit geometric reasoning to address the novel view synthesis problem. When multiple images are available, multi-view stereo algorithms @cite are applicable to explicitly reconstruct the 3D scene which can then be utilized to synthesize novel views. An alternative approach recently proposed by @cite uses deep networks to learn to directly interpolate between neighboring views. @cite propose to rectify the two view images first with estimated homography by deep networks, and then synthesize middle view images with another deep networks. In case of single input view, @cite propose to first predict a depth map and then synthesize the novel view by transforming each reconstructed 3D point in the depth map. However, all these approaches only utilize the information available in the input views and thus fail in case of disocclusion. Our method, on the other hand, not only takes advantage of implicit geometry estimation but also infers the parts of disocclusion.
- Another line of geometry-based methods utilize large internet collections of 3D models which are shown to cover wide variety for certain real world object categories @cite @cite . Given an input image, these methods first identify the most similar 3D model in a database and fit to the image either by 3D pose estimation @cite or manual interactive annotation @cite . The 3D information is then utilized to synthesize novel views. While such methods generate high quality results when sufficiently similar 3D models exist, they are often limited by the variation of 3D models found in the database. In contrast, our approach utilizes 3D models only for training generation networks that directly synthesize novel views from an image.
- One of the first convolutional networks capable of generating realistic images of objects is proposed in @cite , but the network requires explicitly factored representations of object type, viewpoint and color, and thus is not able to generalize to unseen objects. The problem of generating novel views of an object from a single image is addressed in @cite @cite @cite using deep convolutional encoder-decoder networks. Due to the challenges of disentangling the factors from single-view and the use of globally smooth pixel-wise similarity measures (e.g. @math or @math norm), the generation results tend to be blurry and low in resolution.
- Recently, a number of image generation methods introduce the idea of using pre-trained deep networks as loss function, referred as perceptual loss, to measure the feature similarities from multiple semantic levels @cite @cite @cite @cite . The generation results from these works well preserve the object structure but are often accompanied with artifacts such as aliasing. At the same time, generative adversarial networks @cite @cite , introduce a discriminator network, which is adversarially trained with the generator network to tell apart the generated images from the real ones. The discriminator encapsulates natural image statistics of all orders in a real fake label, but its min-max training often leads to local minimum, and thus local distortions or painting-stroke effects are commonly observed in their generated images. Our work uses a combined loss function that takes advantages of both the structure-preserving property of perceptual loss and the rich textures of adversarial loss (See Fig. ).
- The traditional shortest path query problem has been studied extensively, which is to compute a shortest path to move from @math to reach'' a query point. Each shortest path query can be answered in @math time by using the shortest path map of @math , denoted by @math , which is of @math size. To build @math , Mitchell @cite gave an algorithm of @math time for any @math and @math space, and later Hershberger and Suri @cite presented an algorithm of @math time and space. If @math is a simple polygon (i.e., @math ), @math can be built in @math time, e.g., see @cite .
- For the quickest visibility queries, @cite also built a quickest visibility map'' of @math size in @math time, which can answer each query in @math time. In addition, @cite gave a conditional lower bound on the problem by showing that the 3SUM problem on @math numbers can be solved in @math time, where @math is the preprocessing time and @math is the query time. Therefore, a data structure of @math preprocessing time and @math query time would lead to an @math time algorithm for 3SUM.
- In the simple polygon case (i.e., @math ), better results are possible for both the quickest visibility queries and the segment queries. For the quickest visibility queries, Khosravi and Ghodsi @cite first proposed a data structure of @math size that can answer each query in @math time. @cite gave an improved result and they built a data structure of @math size in @math time, with @math query time. For the segment queries, @cite built a data structure of @math size in @math time, with @math query time. Chiang and Tamassia @cite achieved the same result for the segment queries and they also gave some more general results (e.g., when the query is a convex polygon).
- Similar in spirit to the point-to-segment'' shortest path problem, Cheung and Daescu @cite considered a point-to-face'' shortest path problem in 3D and approximation algorithms were given for the problem.
- The pioneering work @cite coined or learning as a problem, where each sample @math (to be denoted in the following) consists of a set of instances @math , i.e., @math Each instance @math can be attributed a label @math but these instance-level labels are not assumed to be known even in the training set. The sample @math was deemed positive, if at least one of its instances had a positive label, i.e., label of a sample @math is @math For this scenario the prevalent approach is the so-called , i.e., to train a classifier on the level of individual instances @math and then infer the label of the bag @math as @math
- However, merging collusion-resistant fingerprinting schemes and secure embedding is a difficult task. Early secure watermark embedding schemes assumed that the use of anti-collusion codes make the schemes resistant against collusion attacks without giving any proof-of-concept. Recently, two asymmetric fingerprinting schemes based on @math -secure codes were proposed. @cite proposed a solution that allows a buyer to pick up fingerprint bits from a list controlled by the merchant, in such a way that he she does not know the chosen elements. However, the proposed scheme requires heavy computation due to use of an oblivious transfer protocol. Also, the number of communication rounds between a buyer and a seller is impracticable as it has a linear relation with the length of the code. @cite proposed an asymmetric fingerprinting scheme based on the B-S code with constant communication round but at a cost of a longer codeword.
- Unlike the P2P content distribution systems described in the above paragraph, the following P2P distribution systems fail to provide privacy to the end users. A fingerprint generation and embedding method was proposed by @cite for complex P2P file sharing networks for copyright protection. In this system, wavelet transforms and principal component analysis (PCA) techniques are used for the fingerprint generation. The proposed framework provides a novel solution of legal content distribution, but it does not include collusion resistance and user privacy. Similarly, @cite proposed a P2P system which provides secure distribution of copyright-protected music contents. In this framework, the RSA public-key cryptosystem is used to generate a unique digital fingerprint for every end user within the network. Then, the generated fingerprint is embedded into the music file such that the music provider can establish the identification of any end user performing an unauthorized re-distribution of the file. The proposed system provides a secure mean for distributing large-scale music contents over P2P networks, but it fails to offer privacy to the end users.
- Stochastic encryptions were originally proposed in @cite for physical-layer security in the context of fixed-sample-size estimation problems with quantized data. For the fixed-sample-size hypothesis testing in sensor networks, the joint design of the stochastic encryption and the LFC decision rule that minimizes the LFC detection error probabilities subject to a constraint on the EFC error probabilities is studied in @cite . Nonetheless, the design approach in @cite is ad hoc and results in a suboptimal stochastic encryption. This design approach is made more rigorous in @cite , where the optimal stochastic encryption is obtained with respect to the J-divergence which is adopted as the performance metric for both LFC and EFC. However, the approach proposed in @cite cannot be applied to sequential detection.
- Early works mainly focus on simple actions in well-controlled environments and can be found in recent surveys @cite @cite @cite . Recently, researchers have started investigating untrimmed videos in the wild and have designed various features and techniques. We briefly review the following that are also useful in temporal action localization: frame-level Convolutional Neural Networks (CNN) trained on ImageNet @cite such as AlexNet @cite , VGG @cite , ResNet @cite , ; 3D CNN architecture called C3D @cite trained on a large-scale sports video dataset @cite ; improved Dense Trajectory Feature (iDTF) @cite @cite consisting of HOG, HOF, MBH features extracted along dense trajectories with camera motion influences eliminated; key frame selection @cite ; ConvNets adapted for using motion flow as input @cite @cite @cite ; feature encoding with Fisher Vector (FV) @cite @cite and VLAD @cite @cite .
- There are also studies on spatio-temporal action detection, which aim to detect action regions with bounding boxes over consecutive frames. Various methods have been developed, from the perspective of supervoxel merging @cite @cite @cite , tracking @cite @cite @cite @cite , object detection and linking @cite @cite @cite @cite @cite , spatio-temporal segmentation @cite @cite , and leveraging still images @cite @cite @cite .
- Gaidon al @cite @cite introduced the problem of temporally localizing actions in untrimmed videos, focusing on limited actions such as drinking and smoking'' @cite and open door and sit down'' @cite . Later, researchers worked on building large-scale datasets consisting of complex action categories, such as THUMOS @cite @cite and MEXaction2 @cite @cite @cite , and datasets focusing on fine-grained actions @cite @cite @cite or activities of high-level semantics @cite . The typical approach used in most systems @cite @cite @cite @cite @cite is extracting a pool of features, which are fed to train SVM classifiers, and then applying these classifiers on sliding windows or segment proposals for prediction. In order to design a model specific to temporal localization, Richard and Gall @cite proposed using statistical length and language modeling to represent temporal and contextual structures. Heilbron al @cite introduced a sparse learning framework for generating segment proposals of high recall.
- Zeiler al @cite originally proposed de-convolutional networks for image decomposition, and later Zeiler and Fergus @cite re-purposed de-convolutional filter to map CNN activations back to the input to visualize where the activations come from. Long al @cite @cite showed that deep learning based approaches can significantly boost performance in image semantic segmentation. They proposed Fully Convolutional Networks (FCN) to output feature maps of reduced dimensions, and then employed de-convolution for upsampling to make dense, pixel-level predictions. The fully convolutional architecture and learnable upsampling method are efficient and effective, and thus inspired many extensions @cite @cite @cite @cite @cite @cite @cite @cite @cite .
- Recently, Tran al @cite extended de-convolution from 2D to 3D and achieved competitive results on various voxel-level prediction tasks such as video semantic segmentation. This shows that de-convolution is also effective in the video domain and has the potential to be adapted for making dense predictions in time for our temporal action localization task. However, unlike the problem of semantic segmentation, we need to upsample in time but maintain downsampling in space. Instead of stacking a convolutional layer and a de-convolutional layer to conduct upsampling and downsampling separately, our proposed CDC filter learns a joint model to perform these two operations simultaneously, and proves to be more powerful and easier to train.
- The problem of optimal storage operation or storage allocation for facilitating the integration of intermittent renewable energy generators in electricity networks has been studied in @cite @cite @cite @cite @cite @cite @cite @cite , with total cost minimization objective functions, and in @cite @cite @cite @cite @cite , with profit maximization goals. However, the price volatility management problem using optimal storage allocation has not been investigated in the literature.
- The operation of a storage system is optimized, by minimizing the total operation costs in the network, to facilitate the integration of intermittent renewable resources in power systems in @cite . Minimum (operational installation) cost storage allocation problem for renewable integrated power systems is studied in @cite @cite @cite under deterministic wind models, and in @cite under a stochastic wind model. The minimum-cost storage allocation problem is studied in a bi-level problem in @cite @cite , with the upper and lower levels optimizing the allocation and the operation, respectively. The paper @cite investigates the optimal sizing, siting, and operation strategies for a storage system to be installed in a distribution company controlled area. We note that these works only study the minimum cost storage allocation or operation problems, and the interplay between the storage firms and other participants in the market has not been investigated in these works.
- The paper @cite studies the optimal operation of a storage unit, with a given capacity, which aims to maximize its profit in the market from energy arbitrage and provision of regulation and frequency response services. The paper @cite computes the optimal supply and demand bids of a storage unit so as to maximize the storage's profit from energy arbitrage in the day-ahead and the next 24 hour-ahead markets. The paper @cite investigates the profit maximization problem for a group of independently-operated investor-owned storage units which offer both energy and reserve in both day-ahead and hour-ahead markets. In these works, the storage firm receives the market price as an exogenous input, i.e. the storage is modeled as a price taker firm due to its small capacity.
- The operation of a price maker storage device is optimized using a bi-level stochastic optimization model, with the lower level clearing the market and the upper level maximizing the storage profit by bidding on price and charge discharge in @cite . The storage size in addition to its operation is optimized in the upper level problem in @cite when the lower level problem clears the market. Note that the energy and price bids of market participants other than the storage firm are treated exogenously in these models.
- In @cite @cite , the storage firms are modeled as strategic players in Cournot-based electricity markets. However, they do not study storage sizing problem and the effect of intermittent renewables on the market. Therefore, to the best of our knowledge, the problem of finding optimal storage capacity subject to a price volatility management target in electricity markets has not been addressed before.
- First, let us mention that Rubel's universal differential equation has been extended in several papers. In particular, Duffin proved in @cite that implicit universal differential equations with simpler expressions exists, such as @math for any @math The idea of @cite is basically to replace the @math function @math of Rub81 by some piecewise polynomial of fixed degree, that is to say by splines. Duffin also proves that considering trigonometric polynomials for function @math leads to the universal differential equation @math This is done at the price of approximating function @math respectively by splines or trigonometric splines solutions which are @math (and @math can be taken arbitrary big) but not @math as in @cite . Article @cite proposes another universal differential equation whose construction is based on Jacobian elliptic functions. Notice that @cite is also correcting some statements of @cite .
- All the results mentioned so far are concerned with approximations of continuous functions over the whole real line. Approximating functions over a compact domain seems to be a different (and somewhat easier for our concerns) problem, since basically by compactness, one just needs to approximate the function locally on a finite number of intervals. A 1986 reference survey discussing both approximation over the real line and over compacts is @cite . Recently, over compact domains, the existence of universal ordinary differential equation @math of order @math has been established in @cite : it is shown that for any @math , there exists a third order @math differential equation @math whose solutions are dense in @math . Notice that this is not obtained by explicitly stating such an order @math universal ordinary differential, and that this is a weaker notion of universality as solutions are only assumed to be arbitrary close over a compact domain and not all the real line. Order @math is argued to be a lower bound for Lipschitzian universal ODEs @cite .
- Zeng al first posited the notion of SDN controllers instructing switches to send test packets in the context of their Automated Test Packet Generation system in @cite . We extend this idea to the variety of active measurements in use today, and implement such a system in real SDN hardware and software.
- The notion of using network devices to perform active measurements was first standardized in @cite and @cite , protocols for performing one and two-way active measurements of delay and loss. Our work is broader, generalizes these protocols, and provides a means for such schemes to be implemented in network switches without explicit vendor support. Most closely related to our own work is SDN traceroute @cite , a technique to reveal the sequence of switches and ports that actual data-plane packets traverse in an SDN network. SDN traceroute relies on injecting measurement probes via messages, and installing rules to match and retrieve tagged probe traffic via messages. While SDN traceroute and rely on the same primitives, is designed to provide a platform for existing measurements, IP-level ping and traceroute, using SDNs.
- Performing computations over noisy channels is the heart of , initiated by Schulman @cite @cite . A long line of work considers the 2-party case and obtains various coding schemes, as well as bounds on their capabilities in various settings and noise models @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . See @cite for a survey on interactive coding.
- Previous work in distributed settings that allow edge failures are typically different from our setting in various aspects. Most notable are synchrony assumptions, complete communication graphs, addressing specific distributed tasks and assuming a bound on the number of links that may exhibit failures. This is in contrast to our work, which addresses an asynchronous setting with an arbitrary topology, and considers the simulation of any distributed task. In particular, all links may send corrupted messages, with the bound being the number of corruptions rather than the number of faulty links. For instance, Singh @cite and Sayeed, Abu-Amara and Abu-Amara @cite consider the specific task of leader election and agreement for complete networks. Gong, Lincoln, and Rushby @cite , Siu, Chin and Yang @cite and Dasgupta @cite consider agreement in complete synchronous networks with both faulty nodes and faulty links.
- Pelc @cite shows that if the number of Byzantine-corrupted links is bounded by @math , reliable communication is achievable only over graphs whose connectivity is more than @math . The same work also considers the case where each link is faulty with some probability. In a more recent work, Feinerman, Haeupler and Korman @cite also address complete synchronous networks, and study the specific problems of broadcast and majority consensus under random errors.
- Synchronizers for unreliable settings have been studied by @cite , which address a dynamic setting, and by Harrington and Somani @cite , which assume faulty nodes.
- Vaccination programs are an efficient and cost effective method to improve public health. With sufficiently many people vaccinated the population gains herd immunity, meaning the disease cannot spread. Timely actions to avoid drops in vaccination coverage are therefore of great importance. Many countries have no registries of timely vaccination uptake information, but rely for example on yearly surveys. In such countries estimations of near real-time vaccination uptake based solely on web data are valuable. We extend prior work in this area @cite , which showed that vaccination uptake can be estimated sufficiently accurately from web search data. Our extension consists of a new estimation method that adapts dynamically to temporal fluctuations in the signal (web search queries in our case) instead of assuming temporal stationarity as in @cite . This contribution is novel within vaccination uptake estimation.
- Linear models have been used previously to estimate health events, for instance by combining data from multiple sources with an ensemble of decision trees @cite , or, closer to our work, by using query frequencies for influenza like illness @cite or vaccination uptake estimation @cite . These approaches are designed for stationary time-series analysis, i.e. they assume data is generated by a stationary stochastic process. Our motivation is that vaccination uptake often does not follow stationary seasonal patterns. External events such as disease outbreaks, suspicion of adverse effects, or temporary vaccine shortages can alter uptake patterns for shorter or longer periods of time. Hence, while historical data is a good estimator in stable periods, as shown in @cite , we reason that adapting the estimation to any unstability can reduce estimation error. We experimentally confirm this on all official children vaccines data used in Denmark between 2011 - 2016.
- Human Pose Estimation Articulated human poses were usually modeled by combination of unary term and graph models, , mixture of body parts @cite @cite @cite or pictorial structures @cite . Recently, significant progresses have been achieved by introducing ConvNets for learning better feature representation @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . For example, Chen and Yuille @cite introduced the ConvNet to learn both the unary and the pairwise term of a tree-structured graphical model. Tompson al @cite used multiple branches of ConvNets to fuse the features from an image pyramid, and used a Markov Random Field (MRF) for post-processing. Convolutional Pose Machine @cite incorporated the inference of the spatial correlations among body parts within the ConvNets. State-of-the-art performance is achieved by the stacked hourglass network @cite and its variant @cite , which use repeated pooling down and upsampling process to learn the spatial distribution. Our approach is complementary to previous approaches by incorporating diverse image dependent multi-context representation to guide the human pose estimation.
- Multiple Contextual Information The contextual information is generally referred to as regions surrounding the target locations @cite @cite @cite , object-scene relationships @cite @cite @cite , and object-object interactions @cite . It has been proved efficient in vision tasks as object classification @cite and detection @cite @cite @cite . Recent works modeled contextual information by concatenating multi-scale features @cite @cite , or by gated functions to control the mutual influence of different contexts @cite . The contextual regions, however, are manually defined as rectangles without considering the objects appearance. In this work, we adopt visual attention mechanism to focus on regions which are image dependent and adaptiving for multi-context modeling. Our approach increases the diversity of contexts.
- Visual Attention Mechanism Since the visual attention model is computationally efficient and is effective in understanding images, it has achieved great success in various tasks such as machine translation @cite , object recognition @cite @cite @cite @cite , image captioning @cite @cite , image question answering @cite , and saliency detection @cite . Existing approaches usually adopt recurrent neural networks to generate the attention map for an image region at each step, and combine information from different steps overtime to make the final decision @cite @cite @cite . To the best of our knowledge, our work is the first to investigate the use of attention models for human pose estimation. In addition, our design of the holistic attention map and the part attention map in learning attention in hierarchical order and the modeling of attention from different context and resolution are not investigated in these works.
- Training a control network with solely RGB images as input has already been demonstrated in 1990, by @cite . In that work, an FNN was trained online from a set of shifted and rotated images. This important work showed that networks are capable of performing a restrictive task like following a road. Also, it showed the need for recovery data in the training set. The network contained only 5 hidden units and 30 discrete output units. The computational power of today allows us to work with more complex networks and continuous control.
- It is much more difficult to pilot an aerial vehicle than it is to keep a car on the road, given the same amount of congestion. @cite trained a deep CNN to follow forest trails. A big dataset of trails recorded from 3 cameras was created. One camera facing forward was annotated with the control of going straight. Two cameras pointing sideways had annotated control to compensate for the different orientations. The deep network was able to classify the images with high accuracy.
- Another difficulty encountered by @cite was that once the obstacle is out of the field of view of the drone, the navigation control stops avoiding this obstacle while it might still be in flying range. This was often the reason for a crash. In this work we train both FNNs as well as RNNs. RNNs have a memory which can be especially useful in these situations.
- @cite show how a CNN can learn to behave in a wide variety of situations. They train a very deep network of 9 layers for which a large amount of real world data, 72 hours of driving, was obtained. With the aid of a simulator the data was further augmented. The simulator interprets the real data and creates a model of the perceived environment. By shifting and interpolating between sideways looking cameras, a variety of orientations and driving behavior is obtained and annotated with corresponding control labels. This driving behavior differs from the expert's behavior, providing examples which are unlike the perfect expert's behavior.
- Neural networks need a lot of annotated data. @cite have made a virtual environment from which a very big annotated dataset was obtained. They train a control network to drive a car autonomously on a dataset containing less than 8 percent of real data. In the same spirit, we see it fit to explore the training behavior of different control networks first in a simulated environment. Once the control behaves properly the step to the real world only needs a relatively small amount of extra training data from the real world.
- Finally, limited work has been done by @cite in training an LSTM for controlling an agent in a reinforcement learning toy example. This work focuses on online learning, i.e. with the training data provided sequentially. This makes the training procedure very slow for big networks.
- Video recordings are already used in Software Engineering (SE); e.g., in ethnographic studies of development organizations and in user experience research (see @cite for review). To the best of our knowledge, automatic classification of sequences in videos is not utilized.
- There also exist tools for automatic user interface testing that search for a specific image on the screen @cite . However, these tools can only match user provided images to areas on the screen. While they do provide a threshold-error to allow near matches, they cannot account for unpredictable variation on the screen. While both approaches can automatically profile user action, only ours remains invariant to scale, color change, and version style change; thus, reducing the amount of manual labour and saving analysts' time.
- It was suggested @cite that DCNNs may be used in source code analysis for ... viz. code suggestion, code summarization, traceability link recovery, and feature location''. To the best of our knowledge, no-one in SE has considered extracting UI-based actions from videos using DCNN.
- is a conventional paradigm for recommendation. This methodology firstly applies a factorization form on the rating matrix @math to get the factorization matrices @math , @math and then multiplies @math to estimate the missing ratings, where @math is the estimated rating matrix and @math @math is the user item-related latent factor matrix respectively. Since this branch addresses different assumptions on @math and @math , these methods fall into four primary subcategories according to the applied assumptions. generally emphasizes latent factors as being non-negative, such as NMF @cite , SVP @cite , MMMF @cite , PMF @cite . , such as CISMF @cite . explores the rank features and generalization ability to enhance the factorization, such as LLORMA @cite @cite , R1MP @cite , SoftImpute @cite , @cite , @cite , @cite . treats the output of the rating matrix as discrete values to avoid noise and obtain more interpretations, such as ODMC @cite .
- is formulated as a regression problem, such as regression for graph GRALS @cite , blind regression @cite , Riemannian manifold based regression @cite , and others @cite .
- leverages social information to enhance recommendation such as relationship between users, personalized profiles or movies' attributes. There list some latest researches: SR @cite , PRMF @cite , geo-specific personalization @cite , social network based methods @cite and other social context integration methods @cite .
- The classical example of the modified spreading process incorporates the effect of a @cite . never spread the information even if they were exposed to it multiple times. Nevertheless, can actively convert other spreaders or susceptible nodes into . That complicated logic may lead to the elimination of the epidemic threshold Epidemic threshold determines whether the global epidemic occurs or the disease simply dies out. and has been actively developed @cite .
- Iribarren moro have developed the similar model, where an integro-differential equations have been introduced. That equations describe the cascade sizes when the number of messages send by a node is described by the Harris discrete distribution.However, the general solution to their equations is not known and merely solutions for nontrivial cases (e.g., superexponential processes @cite ) has been considered. Our discoveries provide much simpler method and lucidly explain, that the underlying social graph is far more complex than just the graph of followers.
- Results similar to ours were also obtained in the study on the bias of traceroute sampling. Achlioptas traceroute1 characterize the degree distribution of a BFS tree for a random graph with a given degree distribution. Their explanation why the degree distribution under traceroute sampling exhibits power-law motivated researchers to study bias in P2P systems @cite and network discovery @cite . Their research also resulted in the development of the new tools in the social networks sampling @cite .
- One of the greatest issue in analyzing the cascade size distribution is the lack of a good theoretical background for this process. Recently, researchers @cite has presented new models, designed to fit real distribution of cascade sizes. Moreover, goodness of fit test against CGM model has been conducted. It turns out, that CGM works well for small cascades, however it is unreliable for the large ones. The introduction of time dependent parameters significantly improves predictions for the large cascades @cite .
- In future, theoretical studies on cascades size distribution could explain phenomena in Gossip-based routing @cite , rumor virality and influence maximization @cite , recurrence of the cascades @cite or assist in forecasting rumors @cite . Currently the research in this area is purely empirical and we need more theoretical models to understand the process of information propagation @cite .
- @cite introduce global log-bilinear regression models as an alternative to shallow neural-networks to produce word embeddings. They show their model is able to produce a word vector space with meaningful substructure. The algebraic rule they use to solve the is the same as that originally introduced in @cite . Although they applied the distance measure previously presented by Levy and Goldberg @cite , they report that this distance did not produce better results than the original one. The work presented in @cite is relevant for our research since it confirms that the usability of the word vector algebraic rule extends over vector representations obtained using a variety of model types and algorithms.
- @cite propose associating different levels of meaning for words with different types of representations. For example, verbs or other relational words would be represented by matrices while nouns as vectors. Algebraic operations involving matrices and vectors are used to produce sentence vectors. In principle, GP approaches could cater for joint use of vector and matrix representation by means of strongly typed GP @cite or other GP variants that guarantee type constraint enforcement. However, it makes more sense to exhaust the potential of homogeneous word representations before recurring to GP based on more complex word representations.
- In the algebraic setting the smallest formula for the determinant has size @math , which can be deduced from e.g. @cite . The best known lower bound on the formula size of @math is @math by @cite . That paper also gives a quadratic lower bound for an explicit polynomial (note that the lower bound for the determinant is not quadratic in the number of variables).
- Toda @cite proved that several definitions for the class @math are equivalent, see also @cite . In particular @math is the class of polynomials that can be written as determinants of matrices of polynomially bounded size whose entries are affine linear forms. Due to its pure mathematical formulation, lower bounds for this attracted the attention of geometers @cite @cite @cite . Moreover, Mulmuley and Sohoni's geometric complexity approach @cite @cite is also mainly focused on lower bounds for the determinantal complexity and the symmetries of the determinant polynomial play a key role in their work. Recently @cite showed that it is not possible to prove superpolynomial lower bounds on the determinantal complexity using only information about the occurrences non-occurrences of irreducible representations in the coordinate rings of the orbit closures of the determinant and the (padded) permanent. This disproves a major conjecture in geometric complexity theory. The proof in @cite is fairly general and also holds for lower bounds on the formula size. Only very recently the formula size analog to determinantal complexity, the was studied from a geometric perspective @cite .
- Ben-Or and Cleve @cite proved that a family of polynomials has polynomially bounded formula size if and only if it is computable by width-3 ABPs of polynomial size. An excellent exposition on the history of small-width computation can be found in @cite , along with an explicit polynomial that cannot be computed by width-2 ABPs: @math . Saha, Saptharishi and Saxena [Cor. 14] SSS:09 showed that @math cannot be computed by width-2 ABPs that correspond to the iterated matrix multiplication of upper triangular matrices.
- @cite used SVM and Na "i ve Bayes classifiers to compare the three schemes described above. @cite also studied the problem of extracting focus, techniques and the domain of research papers to identify the influence of research communities over each other.
- General domain discourse parsing is a well-studied problem. While there are many discourse theories (see @cite , chapter 2 for an overview), Rhetorical Structure Theory (RST) by @cite , received a lot of attention. It is generally accepted that relations between non-overlapping chunks of text need to be considered to account for the overall meaning . Accordingly, rhetorical relations are central in RST for marking the structure. In contrast, the taxonomy we use applies to the clauses themselves, instead of the relations between them. This is made possible by the specificity of our domain: In the general case, it may not be possible to identify the type of a clause in isolation. However, it has to be noted that the information conveyed by our clause-centric formalism may also be expressed using a relation-centric discourse formalism like RST. Figure shows one possible RST tree for the text shown in Figure .
- Our work is closely related to @cite , in which the authors have studied the universal spatial sensor locations for discrete bandlimited space; in some sense, finding universal spatiotemporal sampling sets for convolution operators with eigenvalues subject to the same largest geometric multiplicity in our problem, is analogous to finding universal spatial sampling sets for discrete bandlimited space @math that is subject to the same cardinality of @math in @cite . However, we do not make sparsity assumptions on the signal space. Instead, we seek sub-Nyquist spatial sampling rate, but want to compensate the insufficient spatial sampling rate by oversampling in time.
- Understanding human mobility patterns at a global scale using digital data has been of great interest to demographers and social science researchers. In particular, geo-tagged Twitter data has been used extensively in the past to study global human mobility patterns @cite @cite . Tensors are higher dimensional extensions of matrices, which can be used to represent multi modal data. A comprehensive survey on tensors and applications of tensors can be found in @cite . Tensor factorization provides a principled way to analyse large scale multi-modal datasets. Recent progress on scalable implementations of tensor factorization @cite have lead to the application of tensors in a wide range of fields, including Criminology, Neuroscience, Socialscience, etc. See @cite for a detailed survey. Our paper complements existing work on using Twitter data by showing the applicability of a new tool (tensor factorization) to better understand large scale migration behavior.
- Mapping and SLAM (Simultaneous Localization and Mapping) are own subareas of robotics that overlap with the topic of this paper. SLAM algorithms try to build maps of the robot's physical environment while simultaneously localizing the robot inside the map @cite . A history and formal definition of the SLAM problem are given by Durrant-Whyte and Bailey @cite . We refer the interested reader to the article by @cite who give a good overview of the evolution of SLAM from the past to the future.
- In contrast to mapping physical environments, virtual borders have to be mapped in a different way. @cite propose a stroke-based interface on a tablet PC to control the workspace of a vacuum cleaning robot. This technique needs several top-down view cameras in the robot's environment to cover the whole working area. Commercial solutions to the problem of defining the robot's workspace comprise magnetic strips that are placed on the ground to indicate the borders @cite or beacon devices to generate infrared signals @cite . These conic beams will not be crossed by the robot and can be used to block doors or corridors. The disadvantages are intrusiveness, low flexibility and energy consumption. Other explicit methods to incorporate additional information into maps encompass situated dialogues @cite or activity recognition based on wearables @cite . These works integrate semantics into maps, but they do not allow the flexible definition of certain areas, e.g. arbitrary polygons inside a room. The flexibility of our approach distinguishes our work from previous works.
- In contrast to these direct approaches to incorporate additional information into maps, several works focused on the implicit integration of further information, e.g. social information @cite . O' @cite learn human motion patterns and update the robot's navigational map according to humans' trajectories. In addition, @cite use a map learned from robots' sensors and human trajectory observations to navigate to any goal in the environment. Human motion maps are proposed by @cite to represent the motion distribution in particular areas. Furthermore, @cite learn about human behaviors associated with areas in the environment. Another approach based on implicit observations is proposed by @cite who learn the locations of doors and staircases by observing humans' interactions around them. Virtual border teaching, as we use this term throughout the paper, is a certain form of semantic mapping @cite in that we give semantics, and space, to certain areas.
- To allow non-experts the teaching of virtual borders, a user-friendly approach has to be chosen. Along classical robot programming, LfD (Learning from Demonstration) is a technique that deals with teaching robots new skills by (human) demonstrations. @cite present a comprehensive survey of robot LfD and explain the foundations of the technique. The authors categorize different approaches according to the mapping between the teacher and the learner. Teleoperation is used to learn the grasping of objects @cite or to perform tasks demonstrated by kinesthetic teaching @cite . If the learner uses its own sensors, but the teacher does not directly control the learner's platform, it is referred to as shadowing. @cite program a robot's movement by demonstration through system identification, and autonomous navigation in complex unstructured terrain is learned in the work of @cite . Other recent applications range from learning handwriting by demonstration @cite to controlling a robot arm manipulator @cite . The last two applications also use visual markers in their teaching process.
- In the study of assortment optimization problem, an important class of assortment is called the revenue-ordered assortment, in which the assortment is selected according to the revenue order of each product. Revenue-ordered assortment has been shown to be optimal in many assortment optimization problems or has a worst-case performance guarantee (see, e.g., Talluri2004 , rusmevichiengtong_robust , rusmevichiengtong_mmnl , wang_wang ). Indeed, as shown in @cite , the optimal revenue-ordered assortment achieves a worst-case ratio of the optimal revenue under a class of regular discrete choice models''. In this paper, we show that the revenue-ordered assortment is optimal under a special case of the MCST model, and has a worst-case performance guarantee under the general case. Furthermore, in our numerical experiments, we show that the revenue-ordered assortment generally achieves good performance.
- Over the last years, several learning-based approaches to single image depth reconstruction have been proposed that are trained in a supervised way. Often, measured depth from RGB-D cameras or 3D laser scanners is used as ground-truth for training. Saxena al @cite proposed one of the first supervised learning-based approaches to single image depth map prediction. They model depth prediction in a Markov random field and use multi-scale texture features that have been hand-crafted. The method also combines monocular cues with stereo correspondences within the MRF.
- Many recent approaches learn image features using deep learning techniques. Eigen al @cite propose a CNN architecture that integrates coarse-scale depth prediction with fine-scale prediction. The approach of Li al @cite combines deep learning features on image patches with hierarchical CRFs defined on a superpixel segmentation of the image. They use pretrained AlexNet @cite features of image patches to predict depth at the center of the superpixels. A hierarchical CRF refines the depth across individual pixels. Liu al @cite also propose a deep structured learning approach that avoids hand-crafted features. Their deep convolutional neural fields allow for training CNN features of unary and pairwise potentials end-to-end, exploiting continuous depth and Gaussian assumptions on the pairwise potentials. Very recently, Laina al @cite proposed to use a ResNet-based encoder-decoder architecture to produce dense depth maps. They demonstrate the approach to predict depth maps in indoor scenes using RGB-D images for training. Further lines of research in supervised training of depth map prediction use the idea of depth transfer from example images @cite @cite @cite , or integrate depth map prediction with semantic segmentation @cite @cite @cite @cite @cite .
- In this section, different representations used by trackers to model the appearance of objects are presented. Generally, object representations can be classified into two broad categories, i.e. generative and discriminative. In generative representations, the object is modeled using features extracted from the object and then the object is matched by finding the most similar region compared to the model as in template matching trackers @cite , @cite . For example, Mean-Shift tracker @cite uses color features to find the object of interest, and Frag-Track @cite models the object using histograms of local patches. Trackers like IVT @cite use subspace models to incrementally learn the object representation. Sparse representation trackers like @cite , @cite , consider a set of linear combination of templates to represent the object. The generative representations are usually less complex, but they are often unable to tackle the cluttered background scenes due to lack of background information included in the model, and might easily fail in such scenarios.
- In contrast, discriminative representations consider tracking as a binary classification task. CSK @cite uses color features and employs an online binary classifier for tracking. OAB @cite updates discriminative features via online boosting methods. Struck @cite uses an SVM classifier to generate and learn the labels online for tracking and KCF @cite samples the region around the target. The cyclic shifts simulates translations of the target object. TLD @cite uses two types of experts to train the detector online while tracking. The discriminative methods can tackle cluttered background scenes. However they are sensitive to noise because not a lot of information is available to train the classifier in the initial frame and therefore commonly suffer from tracking drift.
- Part-based trackers @cite , @cite divide the object in smaller regions or patches, while @cite uses superpixels as discriminative features and use learning to distinguish the object from background. The work of @cite , proposes to decompose the object into superpixels and then use graph matching to find the association among frames. Due to their robust appearance representation using multiple parts, they provide useful cues during partial occlusion. On the other hand, they may not be able to handle object deformation due to the abrupt variation in translation of multiple parts.
- Some trackers like CAT @cite and SemiT @cite , use contextual information or supporting regions to deal with occlusion. But they might suffer from ambiguities due to the presence of several region of interest with their context. Some authors combine multiple features @cite , or multiple trackers @cite , to maintain multiple appearance models. An extensive summary on various appearance model representations and visual object trackers can be found in @cite and @cite respectively.
- For automated machine learning in general, approaches have mainly focused on optimizing subsets of a machine learning pipeline @cite , which is otherwise known as hyperparameter optimization. One readily accessible approach is grid search, which applies brute force search within a search space of all possible model parameters to find the best model configuration. Relatively recently, randomized search @cite and Bayesian optimization @cite techniques have entered into the foray and have offered more intelligently derived solutions---by adaptively choosing new configurations to train---to the hyperparameter optimization task. Much more recently, a novel bandit-based approach to hyperparameter optimization have outperformed state-of-the-art Bayesian optimization algorithms by 5x to more than an order of magnitude for various deep learning and kernel-based learning problems @cite . Although TPOT-MDR is an automated machine learning approach, it is more specialized on bioinformatics problems rather than general machine learning.
- Narrowing the focus to automated machine learning in bioinformatics, the literature is far more sparse. One such example is @cite , in which they analyze metabolomics data using a modified Bayesian optimization algorithm integrated with the classification algorithms provided in WEKA, a suite of machine learning software written in Java. The Bayesian optimization provided feature subset selection, which filtered irrelevant and redundant features from the datasets to achieve dimensionality reduction. These techniques lead to an improvement of classification accuracy.
- Genetic programming and evolutionary computation methods have also been successfully applied to bioinformatics studies, such as @cite @cite , but they do not focus on designing and tuning a series of standard data analysis operations for a specific dataset. As such, although they are related techniques, they do not fall into the automated machine learning domain.
- @cite demonstrate that shallow feed-forward nets can learn the complex functions previously learned by deep nets with knowledge distillation. On the TIMIT phoneme recognition and CIFAR-10 image recognition tasks, shallow nets can be trained that perform similarly to deeper convolutional models.
- @cite present knowledge distillation for image classification (MNIST) and acoustic modelling. They show that nearly all of the improvement that is achieved by training an ensemble of deep neural nets can be distilled into a single neural net of the same size.
- Another branch of approaches for model compression is weight binarization @cite @cite @cite or quantization @cite @cite @cite . @cite used a fixed-point representation to quantize weights of pre-trained neural networks, so as to speed up testing on CPUs. @cite explored several alternative quantization methods to decrease model size, showing that procedures such as vector quantization, with @math -means, enable 4 @math 8 times compression with minimal accuracy loss. @cite proposed a method for fixed-point quantization based on the identification of optimal bit-width allocations across network layers. @cite @cite have shown that ternary weight quantization into levels @math can achieve @math or @math model compression with slight accuracy loss, even on large-scale classification tasks. Finally, @cite has shown that filter weights can be quantized to @math without noticeable loss of classification accuracy on datasets such as CIFAR-10 @cite .
- Saliency detection has been extensively studied in computer vision, and saliency models in general can be categorized into visual attention prediction or salient object detection. The former methods @cite @cite @cite @cite try to predict scene locations where a human observer may fixate. Salient object detection @cite @cite @cite aims at uniformly highlighting the salient regions, which has been shown benefit to a wide range of computer vision applications. More detailed reviews of the saliency models can be found in @cite @cite . Saliency models can be further divided into static and dynamic ones according to their input. In this work, we aim at detecting saliency object regions in videos.
- Image saliency detection has been extensively studied for decades and most of the methods are driven by the well-known strategy. Early bottom-up models @cite @cite are mainly based on detecting , assuming salient regions in the visual field would first pop out from their surroundings and computing feature-based contrast followed by various mathematical principles. Meanwhile, some other mechanisms @cite @cite @cite have been proposed to adopt some prior knowledge, such as , or global information, to detect salient objects in still images. More recently, deep learning techniques have been introduced to image saliency detection. These methods @cite @cite typically use CNNs to examine a large number of region proposals, from which the salient objects are selected. Currently, more and more methods @cite @cite @cite @cite tend to learn in an end-to-end manner and directly generate pixel-wise saliency maps via fully convolutional networks (FCNs) @cite .
- Compared with saliency detection in still images, detecting saliency in videos is a much more challenging problem due to the complication in the detection and utilization of temporal and motion information. So far, only a limited number of algorithms have been proposed for spatiotemporal saliency detection. Early models @cite @cite @cite can be viewed as simple extensions of exiting static saliency models with extra temporal dimension. Some more recent and notable approaches @cite @cite @cite @cite @cite @cite to this task have been proposed, showing inspired performance and good potentials in many computer vision applications @cite @cite @cite @cite @cite . However, the applicability of these approaches is severely limited by their high-computational costs. The main computational bottleneck comes from optical flow estimation, which contributes much to the promising results.
- In recent years, the border of saliency detection has been extend to capturing common saliency among related images videos @cite @cite @cite @cite @cite , inferring the salient event with video sequences @cite or scene understanding @cite @cite @cite . However, there are significant differences between above methods and traditional saliency detection, especially considering their goals and core difficulties.
- In this section, we mainly focus on famous, deep learning models for computer vision applications in dynamic scenes, including action recognition @cite @cite , object segmentation @cite @cite , object tracking @cite @cite @cite @cite @cite , attention prediction @cite and semantic segmentation @cite , and explore their architectures and training schemes. This will help to clarify how our approach differs from previous efforts and will help to highlight the important benefits in terms of effectiveness and efficiency.
- Many approaches @cite @cite @cite directly feed single video frames into neural networks trained on image data and adopt various techniques for post-processing the results with temporal or motion information. Unfortunately, these neural networks give up learning the temporal information which is often very important in video processing applications.
- A famous architecture for training CNNs for action recognition in videos is proposed in @cite , which incorporates two-stream convolutional networks for learning complementary information on appearance and motion. Other works @cite @cite adopt this architecture for dynamic attention prediction and video object segmentation . However, these methods train their models on multi-frame dense optical flow, which causes heavy computational burden.
- In the areas of human pose estimation and video object processing, online learning strategy is introduced for improving performance @cite @cite @cite @cite @cite . Before processing an input video, these approaches generate various training samples for fine-tuning the neural networks learned from image data, thus enabling the models to be optimized towards the object of interest in the test video sequence. Obviously, these models are quite time-consuming and the fine-tuned models are only specialized for specific classes of objects.
- Spam is a serious problem on almost all online media and spam detection has been studied for many years. Spammers may use different techniques on different platforms so spam detection technique developed for one platform may not be directly applicable on other platforms. Thomas al @cite reported that spam targeting email is significantly different from spam targeting Twitter. In Twitter, there are different types of spamming activities such as link farming @cite , spamming trending topics @cite , phishing @cite , aggressive posting using social bot @cite , to name a few. These different activities pollute timeline of users as well as Twitter search results.
- Many social spam detection studies focus on the identification of spam accounts. Lee al @cite analyzed and used features derived from user demographics, follower following social graph, tweet content and temporal aspect of user behavior to identify content polluters. Hu al exploited social graph and tweets of a user to detect spam detection on Twitter. They formulated spammer detection task as an optimization problem @cite . Online learning has been utilized to tackle the fast evolving nature of spammer @cite . They have utilized both content and network information and incrementally update their spam detection model for effective social spam detection. Tan al @cite proposed an unsupervised spam detection system that exploits legitimate users in the social network. Their analysis shows the volatility of spamming patterns in social network. They have utilized non-spam patterns of legitimate users based on social graph and user-link graph to detect spam pattern. Gao al @cite identified social spam by clustering posts based on text and URL similarities and detecting large clusters with bursty posting patterns.
- Removing spam users cannot filter every spam message as spammer may create another account and restart spamming activity. This calls for tweet-level spam detection. Inspired from content-based techniques for emails, @cite utilized standard classifiers to detect spam tweets. Language modeling approach has been used to compute the divergence of trending topic, suspicious message, and title of the page linked in the tweet @cite . Similarly, Castillo al @cite analyzed the credibility of tweets on trending topics based on users' tweeting and retweeting behaviors, tweet content and link present in the tweets. As spammers keep on evolving over time, hence semi-supervised approach is suitable for tracking such changing spamming activities. Semi-supervised spam detection approach has been utilized to identify spam on voice-over-IP call @cite . Similarly, a semi-supervised approach is reported to have better performance than supervised approach for malware detection task @cite . However, to the best of our knowledge semi-supervised approach has not be utilized to detect spam tweets. Our proposed method is capable of continuously updating itself by using semi-supervised approach.
- Information retrieval (IR) models and algorithms have been extremely successful during the last 20 years in providing everybody with easy access to the vast amount of information available on and through the web. Web and IR conferences connect a large number of researchers working on problems related to web search, the Social Web and data mining, and related topics. Surprisingly little work has been spent, however, on issues related to temporal retrieval, or exploration and analysis of large temporal collections like web archives @cite , even though the need for focusing research on these issues has been recognized @cite @cite .
- All of the above tools and interfaces help support the exploration and search of web archives for individual users and researchers. In addition, ArchiveWeb aims at supporting the collaborative exploration of web archives. Previous research on helping users keep track of their resources includes tools that provide better search and organizational facilities based on metadata time @cite or tagging @cite . Our system provides similar organizational functionalities refined through close collaboration with several learning communities and previous work on the LearnWeb platform @cite , thus gaining advantage from several years of development and user feedback in that context. ArchiveWeb builds on the LearnWeb experience which already supports collaborative sensemaking by allowing users to share and collaboratively work on resources retrieved from various web sources.
- In classical paper by @cite , only deterministic matrices were considered. @cite established similar bounds for singular value decomposition, which is similar to spectral decomposition but applies to non-Hermitian matrices as well. Recently, @cite studied @math perturbation bounds for structured low-rank matrices that have incoherent eigenvectors, but their result is suboptimal when perturbation matrix is random.
- @cite and @cite studied the top singular vectors (containing eigenvectors as a special case) of a perturbed (rectangular) matrix. To compare our result with theirs, suppose the matrices under consideration are symmetric. When @math is of low-rank, and @math , and the perturbation is random, they proved upper bounds that improve over Davis-Kahan theorem (or Wedin's theorem ). Roughly speaking, they showed that when @math , with large probability, See Theorem 9 in @cite . They also proved a more general theorem that requires a weaker assumption on the random model. Their bound is the same as what we obtained (see Example ), except for a log term. Yet it is not clear why their result requires @math , and how @math helps. Our nonasymptotic RS perturbation theory offers a natural explanation and generalizes their results (with the price of a stronger independence assumption).
- When performing experiments is not an option, to identify the causal relations between the variables we need additional assumptions on the data generating process. The most widely employed assumption is the additive noise assumption, which asserts that the unobserved variables affect the observable variables additively. Under this assumption, authors in @cite showed that, except for a measure zero parameter set, one can identify the true causal direction between two variables, as long as the relation is non-linear. A similar result is known when the noise is non-Gaussian, irrespective of the relation between the variables @cite . These approaches inherently assume continuous variables and additive noise. Other works consider discrete variables with the additive noise @cite , or continuous variables without the additive noise assumption @cite .
- Another approach is to exploit the postulate that the cause and mechanism are in general independently assigned by nature. The notion of here is captured by assigning maps, or conditional distributions to random variables to argue about independence of cause and mechanism. In this direction an information-geometry based approach is suggested @cite . Independence of cause and mechanism is captured by treating the log-slope of the function as a random variable, and assuming that it is independent from the cause. In the case of a deterministic relation @math , there are theoretical guarantees on identifiability. However, this assumption is restrictive for real data.
- In @cite , four different MD simulation configurations using the LAMMPS code and AIREBO force field were used to obtain consistent friction estimates for a rotating nested nanotube bearing. Other dissipation estimates for rotating nested nanotube bearings have been determined using intralayer interactions based on the Brenner potential @cite with interlayer interactions based on the Kolmogorov-Crespi registry-dependent potential @cite , and with the COMPASS force field @cite . Dissipation estimates for sliding nested nanotube bearings have been determined using a custom force field and custom numerical simulator in @cite @cite , and a custom force field based on @cite and the r-RESPA integrator in @cite .
- A MD model of experimentally realized graphene-on-graphene sliding is presented in @cite . The self-retracting motion of sheared graphene sheets is studied with an in-house MD integrator in @cite . Energy dissipation during high speed sliding of graphene sheets is investigated with GROMACS @cite and the DREIDING force field @cite in @cite .
- In addition to the relatively simple cases of nested nanotubes and graphene-graphene sliding, MD studies have evaluated more complex nanomachines. One example is a study of meshing gears made from functionalized carbon nanotubes @cite using the Brenner potential @cite . In @cite a complex molecular planetary gear mechanism is analyzed using the UFF forecefield and Gasteiger partial charges. In @cite a custom integrator is used with a custom force field based on CHARMM and AMBER force fields, with partial charges determined by calculations, to model the behavior of the experimentally realized nanocars" @cite rolling on a gold surface.
- An experimental and MD study measured the energy required to force rotation of a sterically-congested molecular bond in a surface-bound molecule @cite . The work to rotate one bond was @math . The energy barrier involved in rotating the bond in this molecule is much larger than that of the rotary joint presented in this paper. All of this energy is not necessarily dissipated since much could be recovered by a properly designed mechanism. Nevertheless, this study provides an upper bound on the energy dissipated, and also indicates the work that would need to be supplied and, ideally, recovered for a single rotation.
- Anonymization techniques can protect location privacy by generalization and suppression techniques, i.e., with an expression of lower granularity. In @cite the @math -anonymity concept was introduced so that a user's location cannot be distinguished from other @math users' locations. Instead of reporting the exact location, a user sends a region containing the locations of other @math people. In @cite , the CliqueCloak algorithm was proposed to provide different @math -anonymous requirements for each user. A privacy-preserving architecture for LBS with different anonymization techniques was suggested in @cite . A node density-based location privacy scheme was proposed to improve anonymity without degrading quality of service (QoS) @cite . The authors of @cite proposed a cluster based anonymization scheme to replace the real node identities with random identities generated by the cluster heads. In @cite , a clustering anonymization for vehicular networks was developed to hide the road and traffic information. However, all the above anonymization techniques cannot protect location data shared by a group of users since an individual's location is concealed.
- During the last decades, either the subspace-based techniques @cite @cite or the affinity-based methods @cite @cite have been receiving an increasing interest on segmentation of different types of motions from a real video.
- @cite uses the distances of each pair of feature trajectories as the measurement to build the affinity matrix based on a translational motion model. This method can segment motions with unlimited number of missing or incomplete trajectories, which means they are robust to the video with occlusions or moving camera problems. Another approach which is based on the affinity is called Multi-scale Clustering for Motion Segmentation (MSMC) @cite . Based on the conventional image classification technique split and merge, they use the correspondences of each two features between two frames to segment the different motions with many missing data. One of the general problems of affinity-based method is highly time-consuming. They have to be implemented with an optimized platform in order to save the computation times.
- An extensive amount of research has been performed on inferring the type and the strength of social ties in a network. Sintos and Tsaparas @cite examine the problem of labeling connections in a network depending on whether they are strong or weak using only the graph structure of the network. Similarly, Xiang and colleagues @cite estimate relationships' strength by considering interaction activity and user similarity based on the Homophily theory in order to develop an unsupervised model that exhibits the latent properties of the network. @cite utilize the statistical properties of communication patterns among actors to deduce the type and strength of links in a network. Backstrom and Kleinberg @cite use the underlying network structure to identify the most influential person in an actor's social network neighborhood. @cite focus on the problem of labeling the edges in a social network as positive or negative based on the user behavior of decision-making.
- More recently, @cite argue that not only the sign and strength are important when it comes to obtaining a better understanding of social network structure, but also the direction of the ties between the actors. Liebowitz @cite integrates the usage of an analytical hierarchical process with social network analysis on the organizational level to create a knowledge map. Moreover, @cite adopt the principle of social stratification in their approach, which refers to the categorization of people in society into ranked groups based on their status, power, wealth or knowledge. By applying stratification on humans, they assumed that people who are higher up in the hierarchy tend to have a higher status (ranking) in comparison to people who are lower in the hierarchy. Further, people at the top levels of the hierarchy are less likely to connect to people at low levels of the hierarchy. An algorithm has been suggested to find the best hierarchy in a directed network. However, they have not considered the time dimension in the problem. Moreover, they study the problem at a network-level and not at an actor-level as in this study.
- Finally, link prediction plays an essential role in discovering interactions within social networks. Along this line, it draws an immense interest in the field of data mining and networks communication. The link prediction problem is one of the underlying problems in social network evaluation. It has been used in different contexts, for instance, in companies to discover the interactions within social networks @cite , even in security sector by monitoring terrorist networks @cite or to be used in prediction of missing links in a community.
- There are several approaches to the problem of sampling graphs (or directed graphs) with a given degree sequence, though none is known to be efficient for all degree sequences. First we consider undirected graphs. The configuration model of Bollob ' a s @cite gives expected polynomial time uniform sampling if @math . McKay and Wormald @cite adapted the configuration model to give an algorithm which performs uniform sampling from @math in expected polynomial time when @math .
- Jerrum and Sinclair @cite used a construction of Tutte's @cite to reduce the problem of approximately sampling from @math to the problem of approximately sampling perfect matchings from an auxilliary graph. The resulting Markov chain algorithm is rapidly mixing if the degree sequence @math is : see @cite . Stable sequences are those in which small local changes to the degree sequences do not greatly affect the size of @math . Specifically, a graphical degree sequence @math is stable if [ ( - + 1)^2 4 ( n - + 1 ). ] Many degree sequences which satisfy the conditions of Theorem are stable; however, not all stable sequences satisfy the conditions of Theorem . (For example, if @math and @math then @math is stable @cite but @math , which is not large enough for Theorem .)
- We note that Barvinok and Hartigan @cite showed that the adjacency matrix of a random element of @math is close'' to a certain maximum entropy matrix'', when the degree sequence is . The definition of tame depends on the maximum entropy matrix, but a sufficient condition is that @math and @math for some constants @math . Some degree sequences satisfying this latter condition are stable sequences, and many of these degree sequences also satisfy the condition of Theorem . It would be interesting to explore further the connections between stable degree sequences, tame degree sequences and the mixing rate of the switch Markov chain.
- Steger and Wormald @cite gave an easily-implementable algorithm for sampling regular graphs, and proved that their algorithm performs asymptotically uniform sampling in polynomial time when @math (where @math denotes the degree). Kim and Vu @cite gave a sharper analysis and established that @math suffices for efficient asymptotically uniform sampling. Bayati, Kim and Saberi @cite extended Steger and Wormald's algorithm to irregular degree sequences, giving polynomial-time asymptotically uniform sampling when @math . From this they constructed a sequential importance sampling algorithm for @math . A similar approach to that of @cite was described and analysed by Zhao @cite in a general combinatorial setting. Zhao showed that for sampling from @math , when @math , his algorithm performs asymptotically uniform sampling in time @math .
- For any directed degree sequence @math , it follows from @cite that the state space @math is connected if the set of transitions of the directed switch chain is expanded to also allow the reversal of directed 3-cycles. In the bipartite setting, this corresponds to an adaptation of the chain (for undirected graphs) which sometimes replaces 3 edges per step, rather than 2. Erd o @cite proved that this chain is rapidly mixing for half-regular bipartite graphs with a forbidden matching, where a bipartite graph is half-regular if one vertex bipartition is regular. This gives an alternative Markov chain for sampling regular directed graphs, for any degree, including dense regular directed graphs.
- We conclude this section with two recent papers. Erd o s, Mikl ' o s and Toroczkai @cite showed how to build on the results of @cite @cite @cite using several ingredients including a Markov chain factorisation theorem by the same authors @cite and a certain canonical decomposition of degree sequences due to Tyshkevich @cite @cite . Their approach works by taking degree sequences for which rapid mixing of the switch chain is known, and combining them in order to construct new degree sequences for which they prove that the switch chain is also rapidly mixing. Erd o also considered the directed setting, where Theorem now provides a wider range of directed degree sequences for which rapid mixing is known, extending the foundation of the method used in @cite . This should further enlarge the set of more directed degree sequences for which the directed switch chain can be shown to be rapidly mixing.
- Gao and Wormald @cite have recently described an extremely efficient expected polynomial time algorithm for exactly uniform sampling @math -regular undirected graphs, where @math . The expected running time of their algorithm is @math . They also describe a variant of their algorithm with expected running time @math such that the total variation distance of the output distribution from uniform is @math , again when @math .
- There are several challenges in crowd-sourcing mobile phone data in urban spaces related with incomplete samples, unknown contexts, user privacy, trustworthiness of data and conservation of energy @cite . Moreover, most of these applications comes with use of sensor devices, those though low cost comes with extra added expense in terms of money and maintenance.
- Environment pollution is an area where participatory sensing can play a lead role in data collection and management @cite . Effective waste management in urban area is handled in @cite @cite . The air quality control or monitoring applications often comes with low cost sensors interfaced with mobile phones or devices. Such examples are Gas-Mobile @cite , Common Sense @cite and P-sense @cite . An air temperature monitoring system was proposed in @cite that collects air temperature from the temperature of the mobile phone batteries. A successful application of crowd-sourcing and participatory sensing in water monitoring in World Water monitoring challenge http: www.monitorwater.org . It includes 1458517 participants from 142 countries in the world reporting 70,927 water bodies @cite . Creek Watch @cite uses iPhone application to employ users to aid water management. Iphone camera is also used in reporting water quality in @cite .
- The problem of user activity detection in large-scale networks has been already addressed in the context of compressive sensing in @cite @cite where the authors show the existence of a non-adaptive optimal activity detection strategy. The approach in @cite @cite relies heavily on methods from high dimensional probability and on methods for estimating certain partial sums affected by noise.
- Computation over multiple-access channels, to the best of our knowledge, was first proposed for a special case in @cite and later formalized for a more general setting in @cite . A special case that has received much attention is the computation of the modulo sum of sources over multiple-access channels because it yields a more efficient approach to network coding which is called Physical Layer Network Coding @cite . For the binary case, this can be viewed as computation coding for the logical Exclusive Or function. We, in contrast, need a computational channel code which computes logical disjunctions (or equivalently conjunctions).
- A result similar to our Corollary appeared in an independent work @cite using a slightly different scheme; we do point out, however, that our result is more general in the sense that we do not restrict the growth of the number of active devices depending on the number of total devices, while the result in @cite , on the other hand, offers a slightly better constant ( @math where we have @math ).
- Additive cyclic codes over Galois rings were investigated in @cite . In this paper, we investigate the same problem but over a more general ring family, finite commutative chain rings. When we focus on non-Galois finite commutative chain rings, we observe two different kinds of additivity.
- The first one, so-called Galois-additivity, is a natural generalization of the study in @cite , anyway our way of construction in this generalization is slightly different from the one in that paper. The authors in @cite were using some linear codes over the base ring and their generator matrices, but we just make use of ideals and do not get involved generator matrices. Our main result with this approach is Theorem . Also we have some further results related to the code size relations (Corollary ) and self-duality (Corollary ) as well as we illustrate these results in a concrete example (Example ).
- Notice that the idea in @cite has been generalized recently also in @cite . However, the generalization in @cite is from Galois rings to free @math -algebras, where @math is a finite commutative chain ring. Recall that we consider also non-free algebras (a finite commutative chain ring does not have to be a free module over a subring of it). On the other hand, our paper does not cover @cite since every free @math -algebra does not have to be a chain ring.
- Similar tools to T2API that also do not provide content assist include the Bing Developer Assistant @cite , @cite , and @cite .
- Attributes have been used to describe objects (both fixed @cite @cite @cite @cite and relative @cite ), faces @cite , scenes @cite and actions @cite @cite . These attribute detectors are then run on new images for high level recognition @cite @cite . Researchers have explored creating a set (or bank) of detectors pretrained on objects such as Object Banks @cite , an ontology of abstract concepts such as Classemes @cite or scene attributes @cite @cite .
- Another line of work proposes learning visual concepts from the Web with minimal human supervision ( webly supervised approaches'). NEIL @cite uses image search engine results in a semi-supervised setting to learn and train visual concept detectors. LEVAN @cite uses Google NGram corpus to extract all possible words related to a given concept, extracts images from image search engine and learn visual concepts related to any given keyword. The authors of @cite use a multiple instance learning approach to learn concepts from image search results. Some approaches learn concepts from images and their labels @cite , from image descriptions @cite or by using a deep network @cite using principles from curriculum learning. Our proposed work is inspired by web supervision but for a different domain. The key difference between our approach and other webly supervised concept learning approaches is that our methods are designed to obtain concepts. We explain further in .
- @math is a matrix at time interval @math to @math . The given constraint states that a link can not send data more than its capacity @cite . It is difficult and unmanageable to allocate a suitable utility function and optimal rate to distinct users in complex networks. Hence, Kelly has divided this problem into two simpler problems named as user's optimal problem and network's optimal problem @cite . Let user @math decide an amount @math to pay per unit time, while @math is demanded a price @math for unit flow. Hence, user will acquire a flow, @math at time @math . User's optimal price for sending @math at time @math can be obtained using the optimization problem as On the other hand, network wants to maximize weighted log function of @math . Therefore, network optimization problem can be formulated as
- In recent years, many deep learning approaches have been explored for resolving the sequence labeling tasks. @cite proposed an effective window-based approach, in which they used a feed-forward neural network to classify each word and conditional random fields (CRF) to capture the sequential information. CNNs are also widely used for extracting effective classification features @cite @cite .
- RNNs are a straightforward and better suited choice for these tasks as they model sequential information. @cite presented a BiLSTM-CRF model, and achieved state-of-the-art performance on several tasks, like named entity recognition and text chunking with the help of handcrafted features. @cite used a BiLSTM for labeling and a CNN to capture character-level information, like @cite and additionally used handcrafted features to gain good performance. Many works have then been investigated to combine the advantages of the above two works and achieved state-of-the-art performance without handcrafted features. These works usually use a BiLSTM or BiGRU as the major labeling architecture, and a LSTM or GRU or CNN to capture the character-level information, and finally a CRF layer to model the label dependency @cite @cite @cite .
- In addition, many similar works have also been explored for slot filling, like RNN @cite @cite , LSTM @cite @cite , adding external memory @cite , adding encoder @cite , using ranking loss @cite , adding attention @cite and so on.
- Researchers have proposed language models using RNN, which learns the probability of next sequence data at the character or word level @cite @cite . The proposed language models were tested on web corpora (i.e. Wikipedia, news articles) and qualitative examples showed their applicability. @cite proposed a sequence-to-sequence learning algorithm with RNN and long short-term memory (LSTM) architecture @cite , and @cite proposed RNN encoder-decoder architecture. Those studies were applied to the machine translation problem.
- Recently, the RNN machine translation approach was extended to the short message generation problem @cite . Considering the message and response as a translation problem, the Neural Responding Machine achieved 40
- Moreover, many researchers have conducted studies on transfer learning. @cite @cite suggested that a base-trained model with general data could be transferred to another domain. Recently, @cite showed, through experiments, that the lower layers tended to have general features whereas the higher layer tended to have specific features. However, none of this research was applied to an RNN language model.
- The implementation of D2D in the cellular networks is a promising approach to offload cellular traffic and avoid congestion in the core network @cite . In @cite , D2D and cellular mode selection was considered for achieving better link quality. The work of @cite assumed that D2D user has a protection zone such that the uplink cellular-to-D2D interference cannot be larger than a threshold, and showed that the capacity of a D2D link can be enhanced while the capacity loss of cellular users is negligible. In @cite , cooperative transmissions in the D2D overlay underlay cellular networks were studied, and it was verified that the D2D transmission capacity can be enhanced with the assistance of relay. In @cite , a contract-based cooperative spectrum sharing was developed to exploit the transmission opportunities for the D2D links and keep the maximum profit of the cellular links. Nevertheless, the aforementioned literature only considered D2D communications in the traditional cellular networks, and more research efforts are needed to comprehensively understand the D2D communications in the future cellular networks such as 5G with many disruptive technologies @cite .
- Br "udern and Wooley @cite @cite @cite treat diagonal systems in @math variables. This is the best value of @math possible with the classical circle method. In particular they prove the Hasse principle for @math whenever the @math are diagonal, @math is smooth and @math . They also prove an asymptotic formula of the type whenever @math holds, or when @math and @math holds @cite @cite @cite . In the case @math they prove a Hasse principle for certain pairs of diagonal cubics in as few as 11 variables @cite .
- Crowdsourcing is another domain being explored for writing assistance. Crowd-powered projects like Soylent @cite , Legion @cite and Chorus @cite have encouraged researchers to harness the crowd for complex tasks --- and have even been implemented on different interfaces, including smart-watches @cite . Projects like Turkomatic @cite , Ensemble @cite , Crowdforge @cite , and others @cite have succeeded in converting macro tasks into micro chunks that makes it easier for crowd workers to accomplish expert-level tasks such as writing articles and stories.
- In recent years, several different neural network based models have been proposed and successfully applied to dependency parsing. Among these neural models, there are three approaches most similar to our model --- the two graph-based parsers with BLSTM feature representation @cite @cite , and the neural bi-affine attention parser @cite .
- @cite propose a histopathology image segmentation algorithm in which the concept of multiple clustered instance learning (MCIL) is introduced. The MCIL algorithm @cite can simultaneously perform image-level classification, patch-level segmentation and patch-level clustering. However, as mentioned previously, their approach is a patch-based system that is extremely space-demanding (requiring large disk space to store the features) and time-consuming to train. In addition, a boosting algorithm is adopted in @cite with all feature types pre-specified, but features in our approaches are automatically learned.
- Holistically-nested edge detector (HED) is developed in @cite by combining deep supervision with fully convolutional networks to effectively learn edges and object boundaries. Our deep weak supervision formulation is inspired by HED but we instead focus on a weakly-supervised learning setting as opposed to being fully supervised in HED. Our deep weak supervision demonstrates its power under an end-to-end MIL framework.
- We selected METIS and RCM for comparison to the partitioning and ordering aspects of , as they represent the most popular and current state-of-the-art approaches for these problems in terms of both speed of computation and quality produced. There are various other partitioning algorithms and methods, including multi-level partitioners similar to METIS @cite @cite @cite , coordinate and geometry-based partitioners @cite , and hypergraph partitioners @cite . Hypergraph partitioners can often calculate higher quality partitions than graph partitioners for regular matrices, but at a considerably higher cost to compute. Other graph partitioners have utilized label propagation in single or multilevel approaches @cite @cite @cite @cite , demonstrating improved algorithm execution times with these partitions versus methods.
- In addition to RCM ordering, Cuthill-McKee (CM) @cite , nested dissection @cite , and Approximate Minimum Degree (AMD) @cite are a few examples of sparse matrix reordering strategies used in the past. Some techniques, such as space-filling curves @cite or spectral bisection and orderings based on calculated eigenvectors @cite have been utilized for both partitioning and ordering of sparse matrices. Ordering methods on irregular networks such as social and Internet graphs has been studied for the purposes of visualization @cite and compression @cite @cite . Although the authors know of no performance analysis of the effects of applying these ordering techniques to distributed graph computation in literature, promising future work might involve utilizing and optimizes these ordering for such purposes.
- Large scale computing centers today have not reached a common consensus on the best'' processor option for HPC systems, also because system choices are driven not only by application performances, but also by cost of ownership and energy aspects which are becoming increasingly critical parameters @cite . Several computing centers do adopt machines based on GPUs, but other ones prefer to stay on more traditional CPUs, offering a lower peak performance, but better computing efficiency for a wider range of applications.
- As a programming model we have selected OpenACC, as it currently has a wider compiler support, in particular targeting NVIDIA GPUs, which are widely used in HPC clusters and commonly used for scientific computations. OpenACC has been successfully used to port and run other scientific codes, such as Lattice Boltzmann applications @cite @cite @cite in computational fluid-dynamics, showing a good level of code and performance portability on several architectures. The migration of our code to OpenMP4, if needed, as soon as compiler support becomes more mature, is expected to be a simple additional effort.
- Representations for documents are often created by using generative topic models such as Latent Dirichlet Allocation (LDA) @cite . In LDA, documents consist of a mixture of topics, with each topic defining a probability distribution over the words in the vocabulary. Each document is therefore represented by a vector of mixture weights over its associated topics.
- More recently, an undirected topic model based on the restricted Boltzmann machine (RBM) @cite called the Replicated Softmax @cite was proposed. Instead of viewing documents as distributions over topics, it forms a binary distributed representation of each document, and was shown to outperform LDA both as a generative document model and as a means of representing documents for a retrieval task. One problem with the Replicated Softmax model is that it becomes too computationally expensive to use it with larger vocabulary sizes, as the complexity scales linearly with the vocabulary size. This was one of the factors that led to the development of an autoregressive neural topic model called DocNADE @cite , which is based on the NADE model @cite . The DocNADE model outperformed the Replicated Softmax when evaluated using the same document modelling and retrieval benchmark, and had an added advantage in that the probability of an observation could be computed exactly and efficiently. The most recent state-of-the-art for this task is a deeper version of DocNADE @cite .
- A number of studies have examined multi-community platforms in different contexts. Subcommunity survival @cite @cite @cite is sometimes framed in the context of a meta-community. Also, find that different newsgroups exhibit different conversation patterns, though tehy don't examine if the same users behave differently across platforms (as in vasilescu2013stackoverflow ). Finally, adamic2008knowledge examine the quality of user answers across different categories of Yahoo Answers.
- Despite exhibiting some undesirable upvoting patterns @cite , Reddit itself has been used as a data source in various contexts. For instance, the study of altruistic requests @cite , the study of domestic abuse discourse @cite , and work about titles @cite demonstrate that useful information can be learned from Reddit comments and upvotes.
- Spatio-temporal action localization is an active research area and aims to localize an action simultaneously in space and time. There is a sizable body of work on this problem @cite @cite @cite @cite @cite @cite @cite ; however, most of this work focuses on short, trimmed videos. In addition, spatio-temporal localization requires substantial effort to manually generate the object bounding boxes needed for training. This is not feasible for large-scale video analysis such as unconstrained Internet videos.
- Multi-task deep networks have become a popular approach to solve multiple semantically related tasks in computer vision. In general, they can be formulated as a fan-out model, where the earlier layers share a single representation and the later layers diverge. Multi-task learning has shown promising results in various domains, such as action parsing @cite , semantic segmentation @cite , natural language processing @cite , object detection @cite , etc.
- Among these works, the DAP3D-Net in @cite is most closely related to our work. They use a general fan-out model to automatically parse actions in videos and output action types, action attributes, and action locations, simultaneously. However, our work is different from theirs in several key ways: (a) @cite performs spatio-temporal action localization, while our goal is temporal action localization; and (b) they train on well-aligned synthetic action data. In contrast, we utilize widely adopted, untrimmed, realistic video benchmarks. We take inspiration from @cite in designing our multi-task learning framework. But turning a cascaded pipeline into a single network is not easy. In particular, care must be taken to address the data imbalance problem when training a parallel network. We also introduce a training data augmentation technique based on shear transformation, design a multiple intermediate loss fusion strategy, and develop a temporal actionness regression module. These contributions highlight the novelty of our joint learning framework compared to previous work on action detection in large-scale untrimmed videos.
- The line of research introduced in this paper relies on distributed word representations @cite and dictionary learning for sparse coding @cite and also shows close resemblance to @cite .
- Distributed word representations assign some relatively low-dimensional, dense vectors to each word in a corpus such that words with similar context and meaning tend to have similar representations. From an algebraic point of view, the embedding of word @math having index @math in a vocabulary @math can be thought of as the result of a matrix-vector multiplication @math where the @math column of matrix @math contains the @math -dimensional ( @math ) embedding for word @math and vector @math is the one-hot representation of word @math . The one-hot representation of word @math is such a vector, which contains zeros for all of its entries except for index @math where it stores a one. Depending on how the columns of @math (i.e. the word embeddings) get determined, we could distinguish a plethora of approaches @cite @cite @cite @cite @cite @cite .
- According to these findings, in order to avoid any hassles of tuning the hyperparameters of the word embedding model employed, we rather primarily use the publicly available pre-trained polyglot word embeddings of @cite without any task specific modification for our experiments. A key thing to note is that polyglot word embeddings are not tailored for any specific language analysis task such as POS tagging or NER. These word embeddings are instead trained in a manner favoring the word analogy task introduced by . The polyglot project distributes word embeddings for more than 100 languages. also report results on POS tagging, however, word representations they apply for these experiments are different from the task-agnostic representations they made publicly available.
- There have been further previous research conducted on training neural networks for learning distributed word representations for various specific language analysis tasks. propose neural network architectures to four natural language processing tasks, i.e. POS tagging, named entity recognition, semantic role labeling and chunking. train word representations on large amounts of unannotated texts from Wikipedia, then update the pre-trained word representations for the individual tasks. Our approach is different in that we do not update our word representations for the different tasks and most importantly that we use successfully the features derived from sparse coding in a log-linear model instead of a neural network architecture. A final difference to @cite is that we experiment with a much wider range of languages while they report results for English only.
- The general goal of sparse coding is to express signals in the form of linear combination of basis vectors and the task of finding an appropriate set of basis vectors is referred to as the dictionary learning problem @cite . Generally, given a data matrix @math with its @math column @math representing the @math @math -dimensional signal, the task is to find @math and @math , such that @math . This can be formalized into an @math -regularized linear least-squares minimization problem having the form with @math being the convex set of matrices that comprise of column vectors having an @math norm at most one, matrix @math acts as the shared dictionary across the signals, and the columns of the sparse matrix @math contains the coefficients for the linear combinations of each of the @math observed signals.
- Performing sparse coding of word embeddings has recently been proposed by , however, the objective function they optimize differs from ). In , we compare the effects of employing different sparse coding paradigms including the ones in @cite .
- After the resurgence of ConvNets for image classification @cite @cite , they have been successfully adopted for a variety of computer vision tasks such as object detection @cite @cite @cite @cite , semantic segmentation @cite @cite @cite @cite , instance segmentation @cite @cite @cite , pose estimation @cite @cite , depth estimation @cite @cite , edge detection @cite , optical flow predictions @cite @cite . However, by construction, final ConvNet features lack the finer details that are captured by lower convolutional layers. These finer details are considered necessary for a variety of recognition tasks, such as accurate object localization and segmentation.
- To counter this, skip' connections have been widely used with ConvNets. Though the specifics of methods vary widely, the underlying principle is same: using or combining finer features from lower layers and coarse semantic features for higher layers. For example, @cite @cite @cite @cite combine features from multiple layers for the final classifier; while @cite @cite use subsampled features from finer scales, @cite @cite upsample the features to the finest scale and use their combination. Instead of combining features, @cite @cite @cite do independent predictions at multiple layers and average the results. In our proposed framework, such upsampling, subsampling and fusion operations can be easily controlled by the lateral and top-down connections.
- In related research, several works have achieved optical flow based control of a MAV , e.g., @cite @cite @cite . As mentioned in the introduction, the standard optical flow methods are computationally too heavy to run on a quadcopter of less than 50 g. For instance, have flown with a 25 g quadcopter before, while computing optical flow for visual odometry @cite . However, this was done on an external computer. As miniaturization of hardware also poses a limitation on communication bandwidth, this can result in a significant delay in the controls. To obtain full autonomy, it would be wise to uncouple a MAV of any external dependencies.
- To design extremely lightweight MAV s for autonomous flight, some researchers looked into EMD sensors @cite and other 1D signal sensors @cite . @cite proposed the design of a 45 g quadcopter for optical flow based control with 1D flow sensors. They followed up with this research on a heavier 278 g platform containing 8 of these sensors pointing in all directions @cite . With this they could hover the quadcopter in various cluttered environments. The results are impressive, nevertheless they were achieved by using multiple single purpose sensors. As they can only sense motion, it does not leave much room to detect other variables necessary for navigation.
- More similar to our research, implemented an efficient optic flow algorithm on a small lightweight (2 g) omnidirectional camera system on a 30 g helicopter @cite . With a ring of 8 low-resolution image chips (64 x 64 pixels), the MAV could compute optical flow. It did this by computing the edges, compressing the images and calculate the displacement by block matching which resulted in translational optical flow. The vision calculations where done on-board the helicopter with 10 Hz, yet the flight controls where computed off-board. Although the potential of a full on-board implementation is there, the redundancy lies in the ratio of cameras to sensed variables. One camera has the potential of detecting flow in 3 directions; they used 8 to only detect 2 (forward and sideways velocity).
- Optical flow can also be used to detect obstacles @cite , however the MAV needs to be constantly on the move. This is not required if stereo vision is used for depth information. With this, developed a reactive avoidance controller for a quadcopter (30 cm in diameter) @cite . From the obtained stereo disparity map, they accumulated the values along the columns to get a summed disparity factor. Assuming that the obstacles are vertical and long, these can be detected quickly. The stereo map was calculated over the entire image first before accumulation to a vector. This significantly impacts the amount of computation making it less suitable for implementation on a smaller MAV .
- Recently, a cheap'' linear model @cite is proposed to replace the attention mechanism with a low-complexity function. This cheap linear attention mechanism achieves an accuracy in the middle of Global Attention and a non-attention model on a question-answering dataset. This approach can be considered as another interesting way to balance the performance and computational complexity in sequence-generation tasks.
- A significant number of empirical studies have sought to establish relationships between some aspects of expression and particular explanatory factors. These factors can be roughly divided into those that relate to the performer's intention of expressing particular emotions, and those that relate to the musical structure, in the broadest sense of the word. For example, a widely confirmed mapping between emotion and expression is that slow tempo, legato articulation, and softer timbres contribute to the perception of the music as sad or solemn, whereas a faster tempo, staccato articulation and brighter timbres tend to induce a perception of happiness @cite , @cite , @cite . Similarly, various structural aspects of the musical score have been found to influence musical expression @cite . Most notably, musical grouping structure (the division of the music into , and ) is often expressed in arc like shapes in tempo and dynamics @cite . Another type of musical structure that musicians express through expressive variations is the metrical structure @cite .
- Computational models of expressive music performance seek to clarify the relationships between certain properties of the musical score and performance context with the actual performance of the score @cite . These models can serve mainly analytical purposes @cite @cite , by showing the relation between structural properties of the music and its effect in the performance of such music, mainly predictive purposes @cite , i.e. the models are used to render expressive performances, or both @cite , @cite @cite . Computational models of music performance tend to follow two basic paradigms: approaches, where the models are defined through music-theoretically informed rules that intend to map structural aspects of a music score to quantitative parameters that describe the performance of a musical piece, and (or ) approaches, where the models try to infer the rules of performance from analyzing patterns obtained from (large) datasets of observed (expert) performances @cite .
- One of the most well-known rule-based systems for musical music performance was developed at the Royal Institute of Technology in Stockholm (referred to as the KTH model) @cite . This system is top-down approach that describes expressive performances using a set of (music theoretically sound cognitively plausible) performance rules that predict aspects of timing, dynamics and articulation, based on a local musical context.
- Among the machine learning methods for musical expression is the model proposed by @cite . This model uses artificial neural networks (NNs) in a supervised fashion in two different contexts: 1) to learn and predict the rules proposed by the KTH model and 2) to learn the performing style of a professional pianist using an encoding of the KTH rules as inputs. Similarly, the (see Section ) used by @cite and @cite represents a bottom-up approach that uses a lower level encoding of a musical score in order to learn how different aspects of the score contribute to generate an expressive performance of a musical piece.
- The project aims to assist the process of conducting transnational and comparative studies of into the Holocaust by bringing together in an online portal https: portal.ehri-project.eu descriptions of archival material across a geographically dispersed and fragmented archival landscape @cite . In this, Its principle goal is related to data collection, integration, and curation.
- Knowledge-bases have many applications -- see for example, Google's Knowledge Graph @cite which is used to provide users with concise summaries to queries about entities, semantic search engines @cite , question answering systems such as IBM Watson @cite , etc. Our aim is to build a domain-specific knowledge-base (in this case, computer science) which helps in these kinds of applications, but in addition can serve as a resource for learning, including, for example, generating a reading order of concepts in order to learn a new one, automatically generating test or quiz questions about concepts to test student understanding, etc.
- Recently, systems which facilitate knowledge-base construction from heterogeneous sources have been proposed. For example, DeepDive @cite aims to consume a large number of heterogeneous data sources for extraction and combines evidence from different sources to output a probabilistic KB. Similarly, Google's Knowledge Vault @cite also aims to fuse data from multiple resources on the Web to construct a KB. Our effort is similar in that we make use of heterogeneous data sources and customise our extractions. However, since our focus is quite narrow and we use very few sources, we do not perform any inferencing.
- * Entity extraction One of the important aspects of building domain-specific knowledge-bases is that a dictionary of terms that are relevant to the domain should be acquired. It is possible that such dictionaries are already available (for example, lists of people), but for others, we need techniques to build this dictionary. @cite gives an overview of supervised and unsupervised methods to recognize entities from text. We follow a more straightforward approach -- we specifically target technology websites and write wrappers to extract a list of entities related to computer science.
- * Information Extraction Research in information extraction to build knowledge-bases make use of a variety of techniques (see @cite for an overview). In general, information extraction can be done from mostly structured resources such as Wikipedia (see, for example, YAGO @cite ) or from unstructured sources (for example, OpenIE @cite ) where the relations are not known ahead of time. Moreover, there are rule-based systems such as SystemT @cite , using surface patterns and supervised techniques for known relations, distant supervision, etc. (see, for example, Hearst patterns @cite and @cite , @cite ). We use a mix of these approaches -- we formulate different ways to exploit the structured information sources in Wikipedia, and use surface patterns to extract relationships from unstructured sources, such as online books. Some of these techniques provided us with high quality triples, while others failed. We analyze both our successes as well as failures in the paper.
- At the root of H-Nets lies the property of @cite . Filters exhibiting steerability can be constructed at any rotation as a finite, linear combination of base filters. This removes the need to learn multiple filters at different rotations, and has the bonus of constant memory requirements. As such, H-Nets could be thought of as using an infinite bank of rotated filter copies. A work, which combines steerable filters with learning is @cite . They build shallow features from steerable filters, which are fed into a kernel SVM for object detection and rigid pose regression. H-Nets use the same filters with an added rotation offset term, so that filters in different layers can have orientation-selectivity relative to one another.
- @cite introduce equivariance to @math -rotations and dihedral flips in CNNs by copying the transformed filters at different rotation--flip combinations. More recently they generalized this theory to all group-structured transformations in @cite , but they only demonstrated applications on finite groups---an extension to continuous transformations would require a treatment on anti-aliasing and bandlimiting. @cite use a larger number of rotations for texture classification and @cite also use many rotated handcrafted filter copies, opting not to learn the filters. To achieve equivariance to a greater number of rotations, these methods would need an infinite amount of computation. H-Nets achieve equivariance to all rotations, but with finite computation.
- To find a query @math with (nearly) optimal value @math w.r.t. a measure @math given leading diagnoses @math , existing interactive KB debuggers @cite @cite @cite @cite proceed as follows: They (1) use a reasoner @math to compute for different seeds @math a set @math of common entailments of all KBs in @math and (2) if @math , then they use @math to assign all diagnoses in @math to their respective set among @math , @math and @math . (3) If both @math and @math are non-empty (i.e. @math is a query with QP @math ) and @math is sufficiently good, a @math -minimal subset @math of @math is computed using @math such that @math .
- In general, steps 1 and 2 are executed @math times yielding a worst case exponential number of expensive reasoner calls. @cite couples this process with a heuristic to direct the search faster towards optimal queries, but partitions suggested by the heuristic might in fact be no QPs which hinders efficient search space pruning. Also, the strong reasoner dependence persists. As pointed out by @cite , all these techniques suffer from the problem that the (number and types of) entailments output by @math , i.e. the shape of @math , have a significant influence on the quality and the number of different calculated queries. @cite shows that objectively worse queries might be found at the cost of neglecting better ones. Further drawbacks are that duplicate QPs might occur in the search (as different seeds can lead to same QP), effective employment of heuristics and pruning is not possible (see above) and that no guarantees whatsoever can be given w.r.t. properties of the minimized query @math . The method H-QUO proposed in this work solves all these issues.
- Previous work on program analysis applied to Android security has used both static and dynamic analysis. With the former, the program's code is decompiled in order to extract features without actually running the program, usually employing tools such as Dare @cite to obtain Java bytecode. The latter involves real-time execution of the program, typically in an emulated or protected environment.
- Static analysis techniques include work by @cite , who analyze API calls to identify over-privileged apps, while Kirin @cite is a system that examines permissions requested by apps to perform a lightweight certification, using a set of security rules that indicate whether or not the security configuration bundled with the app is safe. RiskRanker @cite aims to identify zero-day Android malware by assessing potential security risks caused by untrusted apps. It sifts through a large number of apps from Android markets and examines them to detect certain behaviors, such as encryption and dynamic code loading, which form malicious patterns and can be used to detect stealthy malware. Other methods, such as CHEX @cite , use data flow analysis to automatically vet Android apps for vulnerabilities. Static analysis has also been applied to the detection of data leaks and malicious data flows from Android apps @cite @cite @cite @cite .
- DroidScope @cite and TaintDroid @cite monitor run-time app behavior in a protected environment to perform dynamic taint analysis. DroidScope performs dynamic taint analysis at the machine code level, while TaintDroid monitors how third-party apps access or manipulate users' personal data, aiming to detect sensitive data leaving the system. However, as it is unrealistic to deploy dynamic analysis techniques directly on users' devices, due to the overhead they introduce, these are typically used offline @cite @cite @cite . ParanoidAndroid @cite employs a virtual clone of the smartphone, running in parallel in the cloud and replaying activities of the device -- however, even if minimal execution traces are actually sent to the cloud, this still takes a non-negligible toll on battery life.
- Scale-invariance: The vast majority of recognition pipelines focus on scale-invariant representations, dating back to SIFT @cite . Current approaches to detection such as Faster RCNN @cite subscribe to this philosophy as well, extracting scale-invariant features through ROI pooling or an image pyramid @cite . We provide an in-depth exploration of scale-variant templates, which have been previously proposed for pedestrian detection @cite , sometimes in the context of improved speed @cite . SSD @cite is a recent technique based on deep features that makes use of scale-variant templates. Our work differs in our exploration of context for tiny object detection.
- Context: Context is key to finding small instances as shown in multiple recognition tasks. In object detection, @cite stacks spatial RNNs (IRNN @cite ) model context outside the region of interest and shows improvements on small object detection. In pedestrian detection, @cite uses ground plane estimation as contextual features and improves detection on small instances. In face detection, @cite simultaneously pool ROI features around faces and bodies for scoring detections, which significantly improve overall performance. Our proposed work makes use of large local context (as opposed to a global contextual descriptor @cite @cite ) in a scale-variant way (as opposed to @cite ). We show that context is mostly useful for finding low-resolution faces. Multi-scale representation: Multi-scale representation has been proven useful for many recognition tasks. @cite @cite @cite show that deep multi-scale descriptors (known as hypercolumns'') are useful for semantic segmentation. @cite @cite demonstrate improvements for such models on object detection. @cite pools multi-scale ROI features. Our model uses hypercolumn'' features, pointing out that fine-scale features are most useful for localizing small objects (Sec. and Fig. ).
- RPN: Our model superficially resembles a region-proposal network (RPN) trained for a specific object class instead of a general objectness'' proposal generator @cite . The important differences are that we use foveal descriptors (implemented through multi-scale features), we select a range of object sizes and aspects through cross-validation, and our models make use of an image pyramid to find extreme scales. In particular, our approach for finding small objects make use of scale-specific detectors tuned for interpolated images. Without these modifications, performance on small-faces dramatically drops by more than 10
- In addition to the patch-based methods, some methods combine hand-crafted low-level features with deep networks as an alternative approach. For example, Tang al @cite first extract the LBIQ features @cite and feed the features into a Restricted Boilzman Machine to predict image quality scores. Hou al @cite pose IQA problem as a classification problem. They slot images into different categories according to the image quality and propose a quality pooling method under the Bayesian framework to predict quality scores.
- Objectness and Saliency in IQA: Although there lacks of literature that organically fuses the attentional mechanism into the NR-IQA, the semantic objectness or saliency has been applied. Objectness and saliency are static property of image regions, whereas attention is an active perception process of an observer. Liu al @cite determine the final score of an image by averaging the predicted patch scores with weights. The patch weights are the saliency values obtained from eye-tracking data. The performance gain of the method justifies the importance of introducing visual attention to IQA. Zhang and Li @cite argue that visual saliency and perceptual quality are highly related, and they utilize the relationship between a reference image saliency map and its distortion image saliency map to predict image quality scores. Zhang al @cite propose an IQA algorithm using object-like regions. They assume that semantic regions contribute to perceptual quality assessment. Hou and Gao @cite propose a saliency-guided framework whose idea is similar to @cite . In summary, these methods exploit image saliency maps in post-processing, , adopt saliency-weighted average score rather than a uniform average score as final prediction. Zhang al @cite study different combinations of different saliency models and IQA methods.
- Recurrent Attentional Models: Recently, deep learning models with attentional mechanism receive a lot of interest. The soft attentional models @cite @cite implement deterministic attention mechanism trained by normal backpropagation. Kuen al @cite realize the attention mechanism through the differentiable spatial transformer @cite and recurrent connections to refine saliency map step by step. Stochastic attention in the hard attentional models @cite @cite @cite are often optimized by the REINFORCE algorithm @cite . Implementing the similar idea, Mnih al @cite propose a well-designed attentional model with RNN for object recognition and Ba al @cite recognize and localize multiple objects by maximizing a variational lower bound. Sorokin al @cite propose a soft attention mechanism designed as element-wise multiplication with importance vectors and a hard attention mechanism optimized by the REINFORCE algorithm.
- To our knowledge, PeerCensus @cite is the first work to envision a committee that approves transactions using PBFT. It makes the observation that Byzantine consensus enables fast transaction confirmation. The description and pseudocode are very high level. Under our best interpretation, it seems that the committee is responsible for reconfiguring itself using Byzantine consensus, but no detail is given on reconfiguration. ByzCoin @cite employs multi-signatures to improve the scalability of PBFT. The reconfiguration algorithm in ByzCoin seems to be left open with multiple options. The description in the paper @cite is quite terse, and can be interpreted in multiple ways. A later blog post @cite suggests adopting hybrid consensus @cite . If ByzCoin goes this path, then our comparison to hybrid consensus applies to ByzCoin as well. Through private communication, we learned that ByzCoin designers also had in mind a PBFT-style reconfiguration protocol, which seems to involve extra steps not described in the paper. We will have to wait to see a detailed published reconfiguration protocol to compare with ByzCoin properly. Pass and Shi proposed hybrid consensus @cite , which we have compared to in detail in .
- Works that discuss the accuracy limitations of differentially private results in different contexts include @cite @cite @cite @cite @cite . Although these accuracy issues have hindered the practical adoption of DP, they have also boosted further research to find fixes. This research has taken two main lines: (i) to come up with novel differentially private mechanisms that improve the accuracy of the results, and (ii) to propose relaxations of DP that require less data distortion and allow for more accurate results.
- An outcome of the first line is the design of general mechanisms that improve accuracy with respect to the basic Laplace noise addition mechanism. @cite , the calibration of Laplace noise to the global sensitivity of the data is replaced by the calibration of suitable noises to the smooth sensitivity. On the other hand, the authors of @cite showed that the Laplace distribution is not optimal to attain DP based on calibration to the global sensitivity. They described and constructed the optimal absolutely continuous distributions: essentially, a distribution is optimal if the probability mass is as concentrated as possible around zero given the DP constraints. Other outcomes of the first line of research are mechanisms that are less sensitive. @cite @cite @cite , several methods based on microaggregation of records are proposed to generate differentially private data sets. @cite , the dependence between attributes is analyzed to reduce the dimensionality in the computation of differentially private histograms. Other works that try to improve the accuracy in histogram publication are @cite @cite . @cite , a differentially private alternative to the ID3 algorithm for learning decision trees is proposed.
- In , the GAN is able to generate interesting local structure but globally incoherent images on various datasets. enlarges GAN's representation capacity by introducing an extra vector to allow the generator to produce samples conditioned on other beneficial information. Motivated from this, several conditional variants of GAN has been applied to a wide range of tasks, including image prediction from a normal map @cite , image synthesis from text @cite and edge map @cite , real-time image manipulation @cite , temporal image generation @cite @cite @cite , texture synthesis, style transfer, and video stylization @cite .
- Researchers also aim at stretching GAN's limit to generate higher-resolution, photo-realistic images. initially apply a Laplacian pyramid framework on GAN to generate images of high resolution. At each level of their LAPGAN, both the generator and the discriminator are convolutional networks. As an alternative to LAPGAN, successfully designs a class of deep convolutional generative adversarial networks which has led to significant improvements on unsupervised image representation learning. Another line of work aimed at improving GANs are through feature learning, including features from the latent space and image space. The motivation is that features from different spaces are complementary for generating perceptual and natural-looking images. With this perspective, some researchers use distances between learned features as losses for training objectives for generative models. combine a variational autoencoder objective with a GAN and utilize the learned features from the discriminator in the GANs for better image similarity metrics. It is shown that the learned distance from the discriminator is of great help for the sample visual fidelity. Recent literature have also shown impressive results on image super-resolution to infer photo-realistic natural images for 4x upscaling factors @cite @cite @cite .
- @cite proposed an image deconvolution CNN to deblur a blurry image in a non-blind setting. They built a network based on the separable kernel property that the (inverse) blur kernel can be decomposed into a small number of significant filters. Additionally, they incorporated the denoising network @cite to reduce visual artifacts such as noise and color saturation by concatenating the module at the end of their proposed network.
- On the other hand, @cite proposed a blind deblurring method with CNN. The proposed network mimics conventional optimization-based deblurring methods and iterates the feature extraction, kernel estimation, and the latent image estimation steps in a coarse-to-fine manner. To obtain pairs of sharp and blurry images for network training, they generated uniform blur kernels using a Gaussian Process and synthesized lots of blurry images by convolving them to the sharp images collected from the ImageNet dataset @cite . However, they reported performance limits for large blurs due to their suboptimal architecture.
- Similarly to the work of Couzinie- @cite , @cite proposed a sequential deblurring approach. First, they generated pairs of blurry and sharp patches with 73 candidate blur kernels. Next, they trained classification CNN to measure the local likelihood of a specific blur kernel of local patch. And then smoothly varying blur kernel is obtained by optimizing an energy model that is composed of the CNN likelihoods and smoothness priors. Final latent image estimation is performed with conventional optimization method @cite . Note that all these methods require an accurate kernel estimation step for restoring the latent sharp image. In contrast, our proposed system is learned to produce the latent image directly without estimating blur kernels.
- In other computer vision tasks, several forms of coarse-to-fine architecture or multi-scale architecture were applied @cite @cite @cite @cite @cite . However, not all multi-scale CNNs are designed to produce optimal results similarly to @cite . In depth estimation, optical flow estimation, etc., networks usually produce outputs having smaller resolution compared to input image resolution @cite @cite @cite . These methods have difficulties in handling long-range dependency even if multi-scale architecture is used. Therefore, we make a multi-scale architecture that preserves fine-grained detail information as well as long-range dependency from coarser scales. Furthermore, we make sure intermediate level networks help the final stage in an explicit way by training network with multi-scale losses.
- Interactive Learning Interactive model learning with human-in-the-loop is attractive for two reasons: (1) It provides a user with tools that can significantly alleviate or even eliminate the need for careful preparation of large-sized training data. (2) It allows to reduce the human labelling effort by exploiting a model's capacity interactively. Human-computer interactive models have been considered in image segmentation @cite @cite , object recognition @cite @cite , semi-supervised clustering @cite and object counting @cite . In addition, relevance feedback @cite @cite @cite and active learning @cite @cite are also related to a similar idea of exploiting human feedback to improve model learning. The former has been exploited for interactive image retrieval where human feedback to search results are used to refine a query. The latter aims to reduce the human labelling effort by active sample selection for model training. In active learning, knowledge cumulation during model deployment is not considered, and some offline pre-labelled data are typically needed for model initialisation.
- Trust-aware recommender systems have been widely studied, given that social trust provides an alternative view of user preferences other than item ratings @cite . @cite found that trust networks are small-world networks where two random users are socially connected in a small distance, indicating the implication of trust in recommender systems. @cite presented a contextual social network model that takes into account both participants' personal characteristics and mutual relations and proposed a new probabilistic approach, SocialTrust, to social context-aware trust inference in social networks.
- @cite proposed a multi-view clustering based on Euclidean distance by combination both similarity view and trust relationships that is including explicit and implicit trusts. @cite proposed a recommendation framework named MR3, which jointly modeled users' rating behaviors, social relationships, and review comments. @cite proposed a novel social recommendation method, namely Probabilistic Relational Matrix Factorization (PRMF), which aims to learn the optimal social dependency between users to improve the recommendation accuracy.
- There are two main recommendation tasks in recommender systems, namely item recommendation and rating prediction, and our work focuses on the rating prediction task. Matrix factorization technique, because of achieving higher accuracy and better alleviate the data sparsity issue, is a widely-used recommendation method in model-based CF @cite . Trust-aware model-based MF approaches assume that users' preferences are similar to or influenced by their trusted users @cite . The intuition behind is that social friends share similar preferences and influence each other by recommending items. It has been shown that such additional side information among users is useful to deal with the concerned issues and thus to improve recommendation performance @cite .
- Specifically, @cite clustered users by multi-views of similarity and trust, in order to resolve the relative low accuracy. @cite proposed a social regularization method (SoRec) by considering the constraint of social relationships. Ma, King, and Lyu @cite proposed a social trust ensemble method (RSTE) to linearly combine a basic matrix factorization model and a trust-based neighborhood model together. They proposed SoReg method that the active user's user-specific vector should be close to the average of her trusted neighbors, and use it as a regularization to form a new matrix factorization model @cite .
- Jamali and Ester @cite built a new model (SocialMF) on top of SoRec by reformulating the contributions of trusted users to the formation of the active user's user-specific vector rather than to the predictions of items. @cite proposed a graph Laplacian regularizer to capture the potentially social relationships among users, and form the social recommendation problem as a low-rank semidefinite problem. proposed a social recommendation method in @cite , which the authors utilize as trust network information in the experimental process. @cite proposed a hybrid method (TrustMF) that combines both a truster model and a trustee model from the perspectives of trusters and trustees. @cite considered both global and local trust as the contextual information in their model. @cite took into consideration both the explicit and implicit interactions among trusters and trustees in a recommendation model.
- @cite and @cite , only used the social context information, such as tagging and did not incorporate the situation of implicit friendship between users. Fang, Bao, and Zhang @cite decomposed trust into four general factors and then integrate them into a matrix factorization model. @cite focused on the leverage of the hidden social relations between users. Accordingly, they have investigated the power of link prediction techniques to extract the implicit friendship and incorporated it with explicit friendship into probabilistic matrix factorization. @cite extended SVD++ with social trust information and proposed TrustSVD, a trust-based matrix factorization technique. However, it is also noted that even the latest work @cite can be inferior to other well-performing ratings-only models. All these works have shown that a matrix factorization model regularized by trust outperforms the one without trust. That is, trust is helpful in improving predictive accuracy. However, there are certain drawbacks among the previous studies.
- In contrast to the incorporation of the explicit friendship relation, there may be implicit correlations between users based on rating matrix. But, the majority of the literature on social recommendation ignores the role of the implicit friendship relation in boosting the accuracy of the recommendations specially whenever explicit trust is not available @cite . In this paper, our method differs from the previous work because we present how social relation can be extracted from users' ratings to items by describing Hellinger distance between users in recommender systems and propose to incorporate the predicted trust scores into social matrix factorization models for improving recommendation performance. In this paper, due to better accuracy in rating prediction compared to the similar works, we take @cite work as a baseline to verify the effectiveness of our method.
- In the following, we review recent advances in scene parsing and semantic segmentation tasks. Driven by powerful deep neural networks @cite @cite @cite @cite , pixel-level prediction tasks like scene parsing and semantic segmentation achieve great progress inspired by replacing the fully-connected layer in classification with the convolution layer @cite . To enlarge the receptive field of neural networks, methods of @cite @cite used dilated convolution. Noh al @cite proposed a coarse-to-fine structure with deconvolution network to learn the segmentation mask. Our baseline network is FCN and dilated network @cite @cite .
- Other work mainly proceeds in two directions. One line @cite @cite @cite @cite @cite is with multi-scale feature ensembling. Since in deep networks, higher-layer feature contains more semantic meaning and less location information. Combining multi-scale features can improve the performance.
- To make good use of global image-level priors for diverse scene understanding, methods of @cite @cite extracted global context information with traditional features not from deep neural networks. Similar improvement was made under object detection frameworks @cite . Liu al @cite proved that global average pooling with FCN can improve semantic segmentation results. However, our experiments show that these global descriptors are not representative enough for the challenging ADE20K data. Therefore, different from global pooling in @cite , we exploit the capability of global context information by different-region-based context aggregation via our pyramid scene parsing network.
- Recent image scene parsing progress is mostly stimulated by various new CNN architectures, including the fully convolutional architecture (FCN) with multi-scale or larger receptive fields @cite @cite @cite and the combination of CNN with graphical models @cite @cite @cite @cite @cite . There are also some recurrent neural networks based models @cite @cite @cite @cite @cite . However, without incorporating the temporal information when directly applying them to every frame of videos, the parsing results commonly lack cross-frame consistency and the quality is not good.
- To utilize temporal consistency across frames, the motion and structure features in 3D data are employed by @cite @cite @cite . In addition, @cite @cite @cite @cite use CRF to model spatiotemporal context. However, those methods suffer from high computation cost as they need to perform expensive inference of CRF. Some other methods employ optical flow to capture the temporal consistency explored in @cite @cite . Different from above works that heavily depend on labeled data for supervised learning, our proposed approach takes advantage of both the labeled and unlabeled video sequences to learn richer temporal context information.
- Generative adversarial networks were firstly introduced in @cite to generate natural images from random noises, and have been widely used in many fields including image synthesis @cite , frame prediction @cite @cite and semantic inpainting @cite . Our approach also uses adversarial loss to learn more robust spatiotemporal features in frame predictions. Our approach is more related to @cite @cite by using adversarial training for frame prediction. However, different from @cite @cite , PEARL tackles the VSP problem by utilizing spatiotemporal features learned in frame prediction.
- There is a large body of research on requirements engineering in general and on specific RE methods in particular. Our study touches on the results of many of them, but we cannot discuss them all here. There exist surprisingly few comprehensive systematic literature reviews. For instance, there is a systematic review on effectiveness of requirements elicitation techniques @cite and mapping studies on creativity @cite , requirements specification improvement methods @cite as well as the empirical work on requirements specifications techniques @cite . In the latter, the authors emphasise that most studies are experiments, and that the practitioners' view is missing.
- We will focus on related work which performed survey research in the area of requirements engineering or at least with a strong RE component. Parts of the following text are based on our related work discussion in @cite as the related work has not changed significantly. In RE survey research, we see two major areas: investigations of techniques and methods and investigations of general practices and contemporary issues in practice. Both areas investigate to some degree problems in RE and their causes.
- Contributions that investigate techniques and methods analyse, for example, selected requirements phases and which techniques are suitable to support typical tasks in those phases. @cite performed a broader investigation of all phases to analyse the perceived value of the RE practices recommended by Sommerville and Sawyer @cite . Studies like those reveal the effects of given techniques when applying them in practical contexts.
- The first run in Germany together with the overall study design was published in @cite . A preliminary version was also published in @cite and the detailed data and descriptive analysis is available as technical report @cite . This first run already covered the spectrum of status quo and problems. It had additional questions on the expectations on a good RE which we removed in the second run because they provided the least interesting insights. The study design was described with the bi-yearly replications and world-wide distribution in detail. Furthermore, a first version of a theory of the status quo and problems in RE was provided in the form of hypotheses. Overall, we were able to get full responses from 58 companies to test the theory. Most of the proposed theory could be supported and changes were discussed based on the data. Finally, a detailed qualitative analysis of the experienced problems and how they manifest themselves was made. The article at hands concentrates on the replication of that part of the survey, with further emphasis on the problems and their causes.
- In @cite , we used the Brazilian data to explore how to analyse problems and causes in RE in detail. In particular, we tested the use of on this data to better understand the relationship of causes and problems. We introduced those diagrams for causal analysis purposes @cite , and have further detailed them subsequently @cite . We decided to employ these diagrams in our subsequent efforts, including this article, in which we use them for analysing causes and effects of problems based on the complete data set.
- Thereafter, in @cite , we concentrated on analysing the similarities and differences in the experienced problems between Brazil and Germany. We also used the probabilistic cause-effect diagrams for problem and cause analysis. Our key insights in this article were that the dominating factors are related to human interactions independent of country, project type or company. Furthermore, we observed a higher inclination to standardised development process models in Brazil and slightly more non-agile, plan-driven RE in Germany.
- The literature contains several works that use machine learning for predicting outcomes of decision processes in power networks based on pre-solved various input conditions; such methods are referred to as supervised learning algorithms. We limit our analysis to the problem of generation (re)scheduling and reserve activation. In @cite , frequency and active power time series are used for determining whether generator reserve activation is satisfactory or not. In this application, manual labeling of the data is required by experts, and there are only four possible classes. Reliability is maximized in @cite by learning a function that assesses the implications of rescheduling. The sought output is a policy, that dynamically maps system states to rescheduling actions. Reference @cite uses supervised learning for finding recourse strategies in generation management, by generating a training set via Monte-Carlo simulation of load and generation disturbances and then learning a near-optimal recourse strategy to handle similar disturbances observed in real-time. Recourse strategy learning is also investigated in @cite , where boosting is used to create binary classifiers for boolean variables of the mixed integer programs resulting from daily generation re-planning problems.
- Row-diagonal parity (RDP) code proposed in @cite and EVENODD code in @cite can tolerate two arbitrary disk erasures. Due to increasing capacities of hard disks and requirement of low bit error rates, the protection offered by double parities will soon be inadequate. The issue of reliability is more pronounced in solid-state drives (SSD), which have significant wear-out rate when the frequencies of disk writes are high. Indeed, triple-parity RAID (Redundant Arrays of Inexpensive Disks) has already been advocated in storage technology @cite . Construction of array codes recovering multiple disk erasures is relatively rare, in compare to array codes recovering double erasures. We name the existing MDS array codes in @cite @cite @cite @cite @cite @cite @cite @cite @cite as , as their constructions are based on Vandermonde matrices.
- Early works on deblurring focused on the blur caused by camera shake in constant depth images. They are roughly categorized into spatially-invariant and spatially-varying configurations. Spatially-invariant deblurring achieved some success in single-image deblurring @cite @cite @cite @cite and video deblurring @cite . However, the spatially-invariant blur model cannot deal with a rotational camera motion, which is a significant and common component in practical scenarios @cite . To overcome this limitation, some researchers parameterized the blur as a possible camera motions in a 3D space, and this approach is applied to single-image @cite @cite @cite @cite and video deblurring @cite @cite . Although these methods solve spatially-varying motion blur in some extent, they are limited to camera shake in a constant depth and cannot handle more general depth variation or object motion problem.
- In the case of blurred images including depth variations, the blur cannot be represented by a simple homography. Some methods solved this problem by casting a blur kernel estimation problem as a scene depth estimation problem @cite @cite @cite @cite . These methods extended the applicability of deblurring methods. However, they are limited to static scenes, and do not take the mixture of pixels at occlusion boundaries into account.
- Recently, several object motion deblurring methods have been developed. Some of the methods divided the image into segments to restore each of them independently. They divided the image using hybrid cameras @cite @cite , based on similar motions @cite @cite @cite , or under the guidance of the matting @cite . There are also some methods without segmentations. Cho al @cite used patch-based synthesis for deblurring by detecting and interpolating proper sharp patches at nearby frame. Kim and Lee approximated the blur kernel as the pixel-wise 2D linear motion and performed deblurring of dynamic scene in a single-image @cite and a video @cite . These object motion deblurring methods perform well, but do not consider the interaction between the object and the background at occlusion boundaries.
- At occlusion boundaries, blurred pixels consist of a mixture of foreground pixels and background pixels and it plays an important role for object motion deblurring. To address this problem, some authors used layered models @cite @cite @cite . However, these methods assumed the background to be static and modeled the foreground motion only. Sellent al @cite used outlier rejections @cite to handle the occlusions. Takeda and Milanfar @cite proposed a method that can deal with occlusions using a spatiotemporal approach, but it requires priorly given blur kernels and depends on time interpolators.
- Third, Conditional Random Fields (CRF) have long been a popular choice to enforce structure consistency to segmentation outputs. More recently, fully connected CRFs @cite have been used to include structural properties of the output of FCNs @cite . Interestingly, in @cite , RNN have been introduced to approximate mean-field iterations of CRF optimization, allowing for an end-to-end training of both the FCN and the RNN.
- The Seq2Seq model is a newly emerging approach. It was initially proposed by @cite @cite @cite for machine translation. Compared with the traditional statistical machine translation approaches (e.g., @cite ), Seq2Seq models require less human efforts. Later, @cite developed the attention mechanism which largely promoted the applications of the Seq2Seq models. In addition to machine translation, Seq2Seq models achieved the state-of-the-art performance in many other tasks such as response generation @cite Some researches (e.g., @cite @cite ) have directly applied the general Seq2Seq model @cite to the paraphrase-oriented task. However, the experiments of @cite demonstrated that the introduction of hand-crafted features significantly improved the performance of the original model. Consequently, the general Seq2Seq model used for machine translation seemed not suitable for the paraphrase task which involves both copying and rewriting.
- Some existing work has tried to modify the output dimension of the decoder to speed up the training process. In training, @cite restricted the decoder to generate the words from the actual target words together with a sampled word set. @cite supplemented the 1-nearest-neighbors of words in the source text, as measured by the similarity in the word embedding space. Notice that, these models still decoded on the full vocabulary during test. In comparison, our restricted generative decoder always produces the words in a small yet highly relevant vocabulary.
- @cite represent an entity as the average of its word embeddings in entity name, allowing the sharing of textual information located in similar entity names.
- @cite jointly embed knowledge and text into the same space by aligning the entity name and its Wikipedia anchor, which brings promising improvements to the accuracy of predicting facts. @cite extend the joint model and aligns knowledge and words in the entity descriptions. However, these two works align the two kinds of embeddings on word level, which can lose some semantic information on phrase or sentence level.
- @cite also represent entities with entity names or the average of word embeddings in descriptions. However, their use of descriptions neglects word orders, and the use of entity names struggles with ambiguity. @cite jointly learn knowledge graph embeddings with entity descriptions. They use continuous bag-of-words and convolutional neural network to encode semantics of entity descriptions. However, they separate the objective functions into two energy functions of structure-based and description-based representations. @cite embeds both entity and relation embeddings by taking KG and text into consideration using CNN. To utilize both representations, they need further estimate an optimum weight coefficients to combine them together in the specific tasks.
- Besides entity representation, there are also a lot of works @cite @cite @cite to map textual relations and knowledge base relations to the same vector space and obtained substantial improvements.
- We first present here static and dynamic community detection methods relevant to this paper. Other classic methods are reviewed in @cite @cite @cite . Then, we review an approach more related to the proposed method: the vertex-centred paradigm.
- The main community detection method family is criterion optimisation. A global or local criterion measuring the quality of a graph partition into communities, such as the well-known modularity @cite , is optimised through several iterations of an algorithm loop until convergence. Many existing criteria yield good quality partition (compared to a ground truth for example), but suffer from different drawbacks such as being subject to local extremum or resolution limit @cite . This kind of method is also known to be time-consuming @cite .
- More recently, label propagation methods @cite @cite @cite offer a decentralised alternative. They rely on propagation of a node identifier (so-called label") from each vertex to every other in the network. However, despite being fast and suitable for detection in a decentralised environment, they have been found not to be stable as well @cite @cite . Moreover, they make massive use of propagation and can overflood the network with unnecessary traffic, especially in a decentralised environment.
- But even more than in the static case, taking into account the nature of the considered networks and the dynamics they are subject to is essential to design efficient methods @cite . In this context, decentralised methods adaptation to process the dynamic case have been found to offer good performance, in terms of partition quality as well as computational efficiency, also offering the advantage to be easily implemented in parallel frameworks, as it is the case for label propagation @cite @cite . It is also very popular for applications in specific environments such as small decentralised mobile networks such as Pocket Switched Networks (PSN), for which community detection helps to improve network discovery and information routing @cite @cite .
- Finally, vertex-centred approaches have gained popularity as a promising new community detection method family. They rely on the principle that some vertices in the network are leaders" or seeds" and the rest are followers @cite . Communities are formed by gathering followers around leaders, like in the approach @cite . Although this method is more related to @math -means clustering (re-allocating the leaders) than to a true leader-follower design, the introduced idea of expanding communities around leaders considering the potential of a follower vertex (resp. a group of follower vertices) to join a leader vertex has been exploited by numerous algorithms. @cite greedily expands communities around seeds and gather communities using ensemble clustering. @cite starts with a careful selection of leaders before computing ranked community membership for each follower, then adjusting preferences and memberships using strategies borrowed from social choice theories until stabilisation. and @cite locally expand around seed via EM or Projected Gradient Descent algorithm, using conductance to delimit communities. @cite consider each vertex as a potential leader and build preference dependencies allowing to form communities. True leaders are the core of the dependencies were the rest can be considered as followers.
- Vertex-centred methods have also attracted attention to develop new dynamic community detection algorithms: for instance , an adaptation of Top-Leaders @cite , , an adaptation of LICOD for multiplex networks enabling use on evolving networks @cite , OLEM OLTM @cite that locally optimises modularity and the original approach of @cite based on weighted-edge graphs, using weight update rules to cope with the dynamicity together with a fitness function to ensure partition quality.
- We can also cite agent-based approaches like consider each vertex as an agent and apply dynamic evolution rules to simulate the community formation, yielding a community structure @cite .
- The major drawback with these algorithms is that they loose one of the initial benefits of the leader-based approach, i.e. lightness and flexibility. Built on top of Top-Leaders, Evo-Leaders @cite adds a costly split-merge of community at each time step. mux-LICOD @cite uses degree centrality and shortest path calculation to compare leaders and followers. Shortest path computation can be costly if used for each vertex to each potential leader. It also relies on an aggregation phase repeated until stabilisation, though experiments do not reveal whether the stabilisation is fast or not. Finally, 's method @cite relies on a fitness function and a set of ad-hoc update rules and pruning over updates. It is hard to know however how efficient this policy is, as the experiments proposed by the authors are limited to a comparison with re-computation of the static counterpart. While faster than static re-computation (which is generally expected for specifically dynamic-addressed algorithms), the proposed @math -score comparison with the set of static re-computed instances is not meaningful, as static re-computation has been proved to give unstable results @cite .
- Many earlier approaches relied on using a Bag of Words (BoW) based pipeline. Such methods typically extracted local spatio-temporal features and encoded them using a dictionary @cite @cite @cite @cite @cite @cite . One of the first works @cite described a video with BoW histograms that encoded Histograms of Gradients (HoG) and Histograms of Flow (HoF) features over 3D interest points. Later works improved upon this pipeline in several ways @cite @cite by using dense sampling for feature extraction @cite , describing trajectories instead of 3D points @cite @cite , and using better pooling and encoding methods @cite @cite @cite . Improving upon these methods Wang al @cite proposed the Improved Dense Trajectories (iDT) approach that showed significant improvement over previous methods by using a combination of motion stabilized dense trajectories, histogram based features and Fisher Vector (FV) encodings with spatio-temporal pyramids. Some recent methods have improved upon this pipeline by either using multi-layer fisher vectors @cite or stacking them at multiple temporal scales @cite . All of these approaches rely on the usage of various local features combined with standard pooling operators.
- Mensink al @cite observed that the classifier has this property. NCM represents each class as a prototype vector that is the average feature vector of all examples observed for the class so far. This vector can be computed incrementally from a data stream, so there is no need to store all training examples. A new example is classified by assigning it the class label that has a prototype most similar to the example's feature vector, with respect to a metric that can also be learned from data. Despite (or because of) its simplicity, NCM has been shown to work well and be more robust than standard parametric classifiers in an incremental learning setting @cite @cite @cite .
- Alternative approaches fulfill the class-incremental learning criteria @math -- @math , that we introduced in , only partially: Kuzborskij al @cite showed that a loss of accuracy can be avoided when adding new classes to an existing linear multi-class classifier, as long as the classifiers can be retrained from at least a small amount of data for all classes. Chen al @cite @cite and Divvala al @cite introduced systems that autonomously retrieve images from web resources and identifies relations between them, but they does not incrementally learn object classifiers. Royer and Lampert @cite adapt classifiers to a time-varying data stream but their method cannot handle newly appearing classes, while Pentina al @cite show that learning multiple tasks sequentially can beneficial, but for choosing the order the data for all tasks has to be available at the same time.
- Li and Wechsler @cite , Scheirer al @cite , as well as Bendale and Boult @cite aimed at the related but distinct problem of in which test examples might come from other classes than the training examples seen so far. Polikar al @cite @cite introduced an ensemble based approach that can handle an increasing number of classes but needs training data for all classes to occur repeatedly. Zero-shot learning, as proposed by Lampert al @cite , can classify examples of previously unseen classes, but it does not include a training step for those.
- The recent success of (deep) neural networks can in large parts be attributed to their ability to learn not only classifiers but also suitable data representations @cite @cite @cite @cite , at least in the standard batch setting. First attempts to learn data representations in an incremental fashion can already be found in the classic neural network literature, @cite @cite @cite @cite . In particular, in the late 1980s McCloskey al @cite described the problem of , the phenomenon that training a neural network with new data causes it to overwrite (and thereby forget) what it has learned on previous data. However, these classical works were mainly in the context of connectionist memory networks, not classifiers, and the networks used were small and shallow by today's standards. Generally, the existing algorithms and architectural changes are unable to prevent catastrophic forgetting, see, for example, Moe-Helgesen al's survey @cite for classical and Goodfellow al's @cite for modern architectures, except in specific settings, such as Kirkpatrick al's @cite .
- For iCaRL, we adopt the principle of : to update the model parameters for learning a representation, we use not only the training data for the currently available classes, but also the exemplars from earlier classes, which are available anyway as they are required for the prototype-based classification rule. Additionally, iCaRL also uses to prevent that information in the network deteriorates too much over time. while Hinton al @cite originally proposed distillation to transfer information between different neural networks, in iCaRL, we use it within a single network between different time points. The same principle was recently proposed by Li and Hoiem @cite under the name of to incrementally train a single network for learning multiple tasks, multiple object recognition datasets. The main difference to the class-incremental multi-class situation lies in the prediction step: a multi-class learner has to pick one classifier that predicts correctly any of the observed classes. A multi-task (multi-dataset) leaner can make use of multiple classifiers, each being evaluated only on the data from its own dataset.
- Scene labeling is one of the most challenging problems in computer vision. Recently, convolutional neural network based methods achieved great success in this task. Farabet @cite made one of the earliest attempts at applying hierarchical features produced by CNNs to scene labeling. Eigen @cite designed a multi-scale convolutional architecture to jointly segment images, predict depth, and estimate normals for an image. Long @cite applied fully convolutional network (FCN) to this task. Noh @cite also used deconvolution layers for image segmentation. They adopted an encoder-decoder architecture, where encoder part consists of convolution and pooling operations and decoder part consists of deconvolution and unpooling operations. Badrinarayanan @cite designed a similar architecture named SegNet. In @cite , Yu and Koltun developed a dilated convolutional module to preserve multi-scale contextual information for image segmentation.
- Although CNN based methods introduced powerful scale-invariant features for scene labeling, they performed poorly in preserving local textures and fine structures in predictions. These problems were addressed by combining CNNs with probabilistic graphical models such as conditional random fields (CRFs). Chen @cite suggested to put a fully connected CRF @cite on top of FCN to capture structural dependencies in images. Zheng @cite showed that CNN and CRF can be jointly trained by passing the inference errors of CRFs back to CNN. Liu @cite improved @cite by introducing a more complex pairwise term for CRF. CRF based methods usually require carefully designed pair-wise potentials and unfortunately their exact inference is usually intractable.
- Liang @cite designed a graph LSTM to handle image data. However, their method is built on superpixels, which is computationally expensive and is not directly applicable to image features. Byeon @cite developed a scene labeling method based on a 2D LSTM network, which first divided an input image into non-overlapping patches and then sequentially fed them into LSTM networks in four different orders. @cite built a RNN segmentation algorithm based on recently proposed ReNet @cite . Their idea was to to alternatively sweep an image in different directions and then sequentially input each row (or column) into a RNN. Shuai @cite @cite designed a quaddirectional 2D RNN architecture for scene labeling, where each pixel was connected to its 8 nearest neighbors.
- @cite , used deep belief network (DBN) based on restricted Boltzmann machine (RBM) for feature reduction with support vector machine (SVM) as a classifier to implement a network intrusion detection system (NIDS) on NSL-KDD @cite intrusion dataset. @cite , used discriminative RBM (DRBM) to develop a semi-supervised learning based network anomaly detection and evaluated its performance in an environment where network traffic for training and test scenarios were different. They used real-world traffic traces and KDD Cup-99 @cite intrusion dataset in their implementation. @cite , used RBM based DBN with a neural network as a classifier to implement an NIDS on KDD-Cup 99 dataset. @cite , proposed an NIDS for the security of in-vehicular networks using DBN and improved detection accuracy compared to previous approaches. @cite , we implemented a deep learning based NIDS using NSL-KDD dataset. We employed self-taught learning @cite that uses sparse autoencoder instead of RBM for feature reduction and evaluated our model separately on training and test datasets. @cite , proposed a system that combines spectral clustering (SC) and sparse autoencoder based deep neural network (DNN). They used KDD-Cup99, NSL-KDD, and a sensor network dataset to evaluate the performance of their model.
- Low-level features are designed to capture signal-based information. Statistical metrics are the most common approaches, which include mean, variance, standard deviation, energy, entropy and correlation coefficients @cite @cite . Fourier Transform (FT), Wavelet Transform (WT) @cite , Discrete Cosine Transform (DCT) @cite as well as auto-regressive (AR) coefficients @cite are also commonly applied in HAR tasks for their promising performances. @cite analyzed electromyography (EMG) signals by extracting conventional auto-regressive coefficients and cepstral coefficients as features. @cite designed the hand-crafted feature based on the Empirical Cumulative Distribution Function (ECDF) to preserve characteristics of inertial signal distribution. Pl "o @cite improved on that work and proposed the ECDF-PCA feature. They implemented the Principal Component Analysis (PCA) method on signals normalized by ECDF and significantly improved performance. In this paper, statistical values, FFT coefficients and ECDF-PCA are calculated as low-level features to demonstrate the generalization ability of the proposed framework.
- Mid-level features are generally extracted from the low-level ones to explore the components and structural information of signals. They are prevalent in HAR tasks for robustness against noise and discrimination in representations @cite @cite . @cite and @cite implemented the bag-of-words (BOW) model to obtain statistical descriptions of motion primitives. Their works showed the effectiveness of the BOW model in sensor-based action recognition tasks. @cite extracted the occurrence statistics feature from low-level actions in a way that is similar to @cite and then implemented the JointBoosting-framework. One characteristic of their method was to adopt a top-down perspective, using a feature selection algorithm to learn the distinctive motion primitives from the labeled high-level action. @cite and @cite both utilized sparse coding and adopted the convolution basis, which could resist shifts in time and thus reduce redundancy of basis. Our work is similar to @cite , in which the mid-level representation is achieved through hard coding and occurrence statistics.
- High-level recognition tasks mainly focus on obtaining intuitive and semantic descriptions of actions. Pattern-mining methods @cite @cite @cite @cite @cite @cite and probabilistic graphical models @cite @cite are the most prevalent approaches. Pattern-mining methods explore the diversity in human actions through learning discriminative action patterns and motion primitives. @cite applied probabilistic topic models which stemmed from the text processing community to automatically extract action patterns from sensor data. They described the recognition of daily routines as a probabilistic combination of learned patterns. @cite presented an algorithm capable of identifying temporal patterns from low-level actions and utilized these temporal patterns to further represent high-level human actions. Various methods have also been proposed based on probabilistic graphical models @cite @cite to capture the temporal, sequential, interleaved or concurrent relationship among motion primitives. However, graphical models are limited to capturing rich temporal relationships in complex actions and also suffer from the exponential increase in calculation when the number of involved actions grows @cite . Instead of modeling temporal relationships, we propose an efficient pattern-mining approach, which takes advantage of being compact in representation, intuitive in understanding and efficient in calculation.
- Multiple-Instance Learning (MIL) methods in HAR have been widely applied to cope with scarcity of annotation. @cite proposed a framework involving MIL as a weakly supervised recognition method to deal with scarcity of labels and proved its robustness to erroneous labels. Similarly, MIL methods with several novel extensions were introduced to handle different annotation strategies in action recognition @cite . Instead of dealing with annotation scarcity, MLPL proposed in this paper implements MIL to explore latent patterns in human actions, by which the high-level feature would be acquired.
- Other representative works in HAR include template matching methods, heuristic methods and deep learning ones. Template matching methods often derived from DTW and LCSS algorithm. @cite proposed Segmented DTW to recognize and bound the gesture in each frame through finding the best match among the object and all templates of different classes. Nonsegmented DTW proposed by @cite was a more efficient variation through reusing the previous computation. Nguyen- @cite @cite improved LCSS algorithm and proposed WarpingLCSS and SegmentedLCSS, which were more efficient than DTW-based methods and robust to noisy annotation. Besides, heuristic methods are often related to specific tasks @cite and depend on domain knowledge. Reyes- @cite designed temporal filters to recognize actions as well as postural transitions. In addition, various studies on deep learning methods have been conducted recently, mainly derived from Convolutional Neural Networks (CNN) @cite @cite @cite and Recurrent Neural Network (RNN) @cite . These methods explored relationships between the temporal and spatial dependency of recordings and sensors. @cite applied CNN with partial weight sharing method. The framework they proposed achieved outstanding performance on three popular datasets.
- Perona-Malik diffusion turns smoothly shaded surfaces into piecewise constant profiles, an effect that is often referred to as a staircasing artifact, and that can be seen in the central Pepper'' image in Figure . To avoid this effect, higher-order diffusion replaces the two first-order spatial derivatives in the heat equation with second-order derivatives. More recently, higher-order PDEs were also generalized to implicit surfaces @cite and image colorization @cite .
- In a one-dimensional setting, discrete variants of higher order data regularization can be traced back to a 1922 article by Whittaker @cite . A first approach for higher order regularization in image processing involving the absolute value of all second order partial derivatives has been proposed by Scherzer @cite . The resulting method has the drawback of not being rotationally invariant. An extension of classical regularization by choosing @math as argument of the penalizer in the smoothness term has been proposed by You and Kaveh @cite . Their method introduces speckle artifacts around edges that require some post-processing. Both problems can be solved by using the squared Frobenius norm of the Hessian matrix @math as argument of the penalizer. This has been proposed by @cite .
- Two very similar higher order methods based on evolution equations without underlying variational formulation have been introduced by Tumblin and Turk @cite and Wei @cite . They use fourth-order evolution equations of the form where @math is the gradient norm @cite or the Frobenius norm of the Hessian @cite .
- We generalize this approach to a novel anisotropic fourth-order diffusion equation that smoothes along the crease, while creating a sharp peak in the orthogonal direction, to clearly indicate its center. We are aware of only one previous formulation of anisotropic fourth order diffusion, proposed by Hajiaboli @cite . However, it has been designed to preserve edges, rather than enhance creases. Consequently, it is not well-suited for our purposes, as we will demonstrate in the results. Moreover, it differs from our approach in that it does not make use of a fourth-order diffusion tensor, and includes no mechanism for scale selection.
- Despite the long history of research in this area, improved filtering and detection of ridges continues to be an active topic in medical image analysis. Our work on improving localization through fourth-order diffusion complements recent advances. For example, the SCIRD ridge detector by @cite , or the vesselness measure by @cite that gives better responses for vessels of varying contrasts, could replace the vessel segmentation by @cite that we use as a prefiltering step. Several recent works @cite @cite @cite @cite have addressed diffusion in crossings and bifurcations, and could be combined with our work to improve the performance of our filter in such cases.
- Shah and Zaman @cite @cite studied the rumor source detection problem, and proposed a new graph centrality called . They proved that the node with the maximum rumor centrality is the maximum likelihood estimator (MLE) of the diffusion source on regular trees under the continuous-time SI model. In addition, it has been proved that the detection probability is greater than zero on regular trees and approaches one for geometric trees. @cite analyzed the detection probability of rumor centrality for general random trees. Later, the performance of rumor centrality has been studied under different models, including multiple sources @cite , incomplete observations @cite , multiple independent diffusion processes from the same source @cite .
- Kai and Ying @cite proposed a sample path based approach for single source detection under the SIR model and introduced the concept of . Assuming the homogeneous SIR model, a complete network snapshot and regular tree networks, they @cite proved that the Jordan infection center is the root of the most likely diffusion sample path, and it is within a constant hop-distance from the actual source with a high probability. Assuming tree networks , the performance of the Jordan infection center has been further studied, including partial observations under the heterogeneous SIR model @cite , multiple sources @cite , the SI model and SIS model @cite .
- The generalization of the Physarum dynamics to linear programming problems that we consider here has been first suggested by Johannson and Zou @cite . Most relevant to the current paper is the work of Straszak and Vishnoi @cite , who initiated the rigorous study of the Physarum dynamics for LP problems of the form . Straszak and Vishnoi proved that a solution to the dynamics exists over the entire time horizon @math , and that a bound on the convergence time of the continuous dynamics can be expressed in terms of the parameter @math , the of the constraint matrix @math , as summarized by their theorem that we quote here for comparison.
- As mentioned in the introduction, several convex optimization methods can be interpreted as discretizations of ordinary differential equation systems: for example, the Interior Point method @cite @cite and the Mirror Descent method [Chapter 3] Nemirovski:1983 . Straszak and Vishnoi @cite proved that the Physarum dynamics with feasible start can be interpreted as natural gradient descent in an appropriate information metric. Amari @cite gives an overview of natural gradient methods in the context of information geometry; see also Raskutti and Mukherjee @cite .
- All algorithms presented above, but the Hsu and Huang @cite , assume nodes have unique identity. The Hsu and Huang's algorithm is the first one working in an anonymous network. This algorithm operates under any sequential daemon (fair or adversarial) in order to achieve symmetry breaking. Indeed, @cite proved that in some anonymous networks there exists no deterministic self-stabilizing solution to the maximal matching problem under a synchronous daemon. This is a general result that holds under either the fair or the adversarial distributed daemon. This also holds whatever the communication and atomicity model (the state model with guarded rule atomicity or the link-register model with read write atomicity). @cite proposed a generalized scheme that can convert any anonymous and deterministic algorithm that stabilizes under an adversarial sequential daemon into a randomized one that stabilizes under a distributed daemon, using only constant extra space and without identity. The expected slowdown is bounded by @math moves. The composition of these two algorithms can compute a maximal matching in @math moves in an anonymous network under a distributed daemon.
- From the viewpoint of graph partitioning, two bipartite spectral graph partition methods were proposed in @cite @cite to co-cluster documents and words by finding minimum cut vertex partitions in a bipartite graph. More recently, authors in @cite put forward the isoperimetric co-clustering algorithm to partition the document-word bipartite graph, which minimizes the ratio of the perimeter of the bipartite graph partition and the partition area under a well-defined graph-theoretic area. For the information theory based co-clustering, samples and features are regarded as the instances of two random variables @cite , , an information bottleneck method was implemented in @cite to cluster documents by using word clusters. Besides, an information-theoretic co-clustering algorithm specially designed for contingency table was introduced in @cite and a more general co-clustering framework based on Bregman divergence was given in @cite .
- However, these graph regularized methods have a common shortcoming that the manifold used for co-clustering might not be the true intrinsic manifold, and it will even deviates far from the desired in an adverse situation. It is a nontrivial task to seek the intrinsic manifold in reality. To alleviate this difficulty, in light of @cite , we assume the intrinsic manifold of the sample or feature distribution lies in a convex hull of some candidate manifolds, and hope to maximally approximate the true intrinsic manifold using the convex combination of them. This way, the local geometrical structure can be better preserved in both the sample and feature spaces for co-clustering.
- Another, related line of work involves learning models of the environment . Although learning environment models as auxiliary tasks could improve RL agents ( @cite @cite ), this has not yet been shown to work in rich visual environments.
- More recently, auxiliary predictions tasks have been studied in 3D reinforcement learning environments. @cite showed that predicting internal features of the emulator, such as the presence of an enemy on the screen, is beneficial. @cite study auxiliary prediction of depth in the context of navigation.
- A number of studies have been conducted to deal with RS distortion in single or multi-view images. Depending on the type of problems, they are classified into absolute pose problem @cite @cite @cite @cite , relative pose problem @cite , multi-view optimization (bundle adjustment) @cite @cite @cite , stereo @cite @cite , and rectification stabilization @cite @cite @cite @cite .
- The most closely related to our study is the study of @cite . Their study is the first to show that there is degeneracy in RS SfM. Specifically, employing the most widely used RS model with linearized rotation and constant velocity translation, they showed the existence of a CMS. The CMS is that all images are captured by cameras with identical y (i.e., RS readout) direction in space'' . They proved that these images can also be explained by RS cameras and a scene all lying in a single plane, meaning degeneracy.
- Our study differs from theirs in that we derive a general representation of CMSs, although we assume an RS model of rotation-only camera motion. We also show that our method can explain the same CMS as @cite but in a different way, and then present a new method to cope with it. Another difference is that our derivation needs approximation beyond the approximation of linearized rotation. Thus, rigorously speaking, our results state that, at least in the range where our approximation is effective, RS SfM will suffer from the same degeneracy. However, considering the nature of the employed approximations, it will probably have at least instability beyond this range. These agree well with our experimental results that will be shown later.
- Recently, increasing number of researches studied sentence ordering using neural models @cite @cite @cite . framed sentence ordering as an isolated task and firstly applied neural methods on sentence ordering. In addition, they designed an interesting task of ordering the coherent sentences from academic abstracts. focused on a very similar ordering task which ranks image-caption pairs, additionally considering the image information. mainly applied neural models to judge if a given text is coherent.
- Graphs can provide a general way of representing the diverse interactions of entities and have been studied extensively @cite . In addition to studies on representation and quantification of relations and similarities @cite , various studies focused on large-scale graph data and use of structural information. Recently, deep learning methods to automatically extract structural characteristics from graphs have been proposed @cite @cite .
- Examples of deep learning applied to non-grid, non-Euclidean space include graph wavelets from applying deep auto-encoders to graphs and using the properties of automatically extracted features @cite , analysis of molecular fingerprints of proteins saved as graphs @cite , and a CNN-based model for handling tree structures in the context of programming language processing @cite .
- Particularly relevant to our approach is the localized SCNN model @cite , which is a deep learning approach that can extract the properties of deformable shapes. The generalized SCNN model @cite @cite , a key component of our framework, borrowed the Fourier transform concept from the signal processing field in order to apply CNNs in a grid domain to a graph-structured domain. In this model, the convolutional operation was re-defined for graphs.
- Over the last few decades, cache-based side-channel attacks have emerged to be a prevalent class of security breaches for many systems. A detailed account on these attacks can be found in the survey @cite . The observer models used in this paper are based on existing cache attacks @cite @cite . However, we believe that CHALICE is generic to incorporate more advanced attack scenarios @cite @cite @cite , as long as the attacks are expressed via the intuition given in .
- To defend against cache-based side-channel attacks, several countermeasures have been proposed over the past few years. Some of these countermeasures require hardware support, such as designing new cache architecture @cite or compiler support, such as devising new instruction-based scheduling @cite . More recently, the approach described in @cite leverages on software diversity at runtime to randomize the cache behavior and hence, reducing the probability for a potential cache side-channel attack. Our proposal is orthogonal to approaches proposing countermeasures. Of course, we believe that our framework can be leveraged as a valuable tool to discover potential flaws in countermeasures proposed to mitigate cache side channels.
- Finally, static cache analysis @cite is still an active research topic. Compared to static cache analysis, our approach has significant flavors of testing and debugging. Moreover, our approach can highlight memory accesses that leak significant information via side-channels. This can be leveraged to drive security-related optimizations.
- Problem in the absence of the travel cost constraint can be efficiently solved using the framework of adaptive submodularity developed by Golovin al @cite @cite as shown by Javdani al @cite @cite and Chen al @cite @cite . Hollinger al @cite @cite propose a heuristic based approach to select a subset of informative nodes and perform minimum cost tours. Singh al @cite replan every step using a non-adaptive information path planning algorithm. Such methods suffer when the adaptivity gap is large @cite . Inspired by adaptive TSP approaches by Gupta al @cite , Lim al @cite @cite propose recursive coverage algorithms to learn policy trees. However such methods cannot scale well to large state and observation spaces. Heng al @cite make a modular approximation of the objective function. Isler al @cite survey a broad number of myopic information gain based heuristics that work well in practice but have no formal guarantees.
- Online POMDP planning also has a large body of work ( @cite @cite @cite . Although there exists fast solvers such as POMCP (Silver and Veness @cite ) and DESPOT (Somani al @cite ), the space of world maps is too large for online planning.
- Pivot language is used to help machine translation when there is no enough resources to train a translation model from source language to target language @cite @cite @cite @cite @cite @cite . For example, , @cite used 22 Indo-European and Asian languages to evaluate how to select a good pivot language for machine translation. They evaluated 45 features falling into eight categories. Besides the language family feature, they used more translation-relevant features such as length of sentence, reordering, overlap of vocabulary, etc. They showed that the final result is mostly affected by the source-pivot and pivot-target translation performance. They also mentioned machine learning based method in the future work, but we are unaware of a follow-up paper that succeeded in doing it. Different from machine translation which needs the sentence level source-pivot and pivot-target translation, in cross-lingual classification, it is sufficient to use word dictionaries, making borrowing a bridging language more scalable to many languages and thus more practically useful. To the best of our knowledge, we have studied largest number of LWLs (49) and SWLs (39) with largest number of linguistic features (196).
- In contrast to the Image-QA task, video-based QA is a much less explored task. @cite have studied joint parsing of videos and corresponding text to answer queries. @cite recently collect a Multimodal QA dataset consisting movie clips, plot, subtitle, script, and Described Video Service (DVS). Similar to most Image-QA datasets, they ask human annotators to generate QA pairs. This approach requires an enormous amount of human efforts since annotators must verify that the context of the answer to the question can be localized in the movie. @cite collect a larger video-based QA dataset with @math questions automatically generated from other manually created video caption datasets. Our proposed method focus on answering rather than a questions. Moreover, our videos and descriptions are harvested from an online video repository without any additional manual effort to generate descriptions. Hence, we believe our proposed method further advances towards a large-scale setting for the video-based QA task.
- Deep generative models attempt to capture the probability distributions over the given data. Restricted Boltzmann Machines (RBMs) are the basis of many other deep generative models, and they have been used to model the distributions of images @cite and documents @cite . Deep Belief Networks (DBNs) @cite and Deep Boltzmann Machines (DBMs) @cite are extended from the RBMs. The most successful application of DBNs is for image classification @cite , where DBNs are used to extract feature representations. However, RBMs, DBNs and DBMs all have the difficulties of intractable partition functions or intractable posterior distributions, which thus use the approximation methods to learn the models. Another important deep generative model is Variational Autoencoders (VAE) @cite , a directed model, which can be trained with gradient-based optimization methods. But VAEs are trained by maximizing the variational lower bound, which may lead to the blurry problem of generated images.
- Recently, Generative Adversarial Networks (GANs) have been proposed by Goodfellow al @cite , who explained the theory of GANs learning based on a game theoretic scenario. Compared with the above models, training GANs does not require any approximation method. Like VAEs, GANs also can be trained through differentiable networks. Showing the powerful capability for unsupervised tasks, GANs have been applied to many specific tasks, like image generation @cite , image super-resolution @cite , text to image synthesis @cite and image to image translation @cite . By combining the traditional content loss and the adversarial loss, super-resolution generative adversarial networks @cite achieve state-of-the-art performance for the task of image super-resolution. Reed al @cite proposed a model to synthesize images given text descriptions based on the conditional GANs @cite . Isola al @cite also used the conditional GANs to transfer images from one representation to another. In addition to unsupervised learning tasks, GANs also show potential for semi-supervised learning tasks. Salimans al @cite proposed a GAN-based framework for semi-supervised learning, in which the discriminator not only outputs the probability that an input image is from real data but also outputs the probabilities of belonging to each class.
- Despite the great successes GANs have achieved, improving the quality of generated images is still a challenge. A lot of works have been proposed to improve the quality of images for GANs. Radford al @cite first introduced convolutional layers to GANs architecture, and proposed a network architecture called deep convolutional generative adversarial networks (DCGANs). Denton al @cite proposed another framework called Laplacian pyramid of generative adversarial networks (LAPGANs). They constructed a Laplacian pyramid to generate high-resolution images starting from low-resolution images. Further, Salimans al @cite proposed a technique called feature matching to get better convergence. The idea is to make the generated samples match the statistics of the real data by minimizing the mean square error on an intermediate layer of the discriminator.
- Rodrigues and Ver 'i ssimo reduced the timestamps cost in message passing systems using the information about the communication topology @cite .
- * Full replication Lazy Replication @cite is a classic framework for providing causal consistency in distributed shared memory, where all clients and replicas both maintain a vector clock of size equal to the number of replicas. Clients can issue updates and queries to any replica, and replicas exchange gossip messages to keep their data up-to-date.
- @cite presented an algorithm that computes the optimal scheduling of measurements for a linear dynamical system. Their formulation does not directly model a target tracking problem. Instead, the goal is to track a linear dynamical system using a set of sensors such that one sensor can be activated at any time instance. The posterior covariance in estimating a linear system in a Kalman filter depends on the prior covariance and sensor variance but not on the actual measurement values (unlike the case in non-linear systems). Thus, one can build a search tree enumerating all possible sensor selections and choosing the one that minimizes the final covariance. The main contribution of was to present a pruning technique to reduce the size of the tree while still preserving optimality.
- @cite extended this result to active target tracking with a single robot. A major contribution was to show that robot trajectories that are nearby in space can be pruned away (under certain conditions), leading to further computational savings. This was based on a linear system assumption. In this paper, we build on these works and make progress towards generalizing the solution for state-dependent observation systems.
- Noisy channel decompositions have been successfully used in a variety of problems, including speech recognition , machine translation , spelling correction , and question answering . The idea of adding language models and monolingual data in machine translation has been explored in earlier work. @cite propose two strategies of combining a language model with a neural sequence to sequence model. In shallow fusion, during decoding the sequence to sequence model (direct model) proposes candidate outputs and these candidates are reranked based on the scores calculated by a weighted sum of the probability of the translation model and that of the language model. In deep fusion, the language model is integrated into the decoder of the sequence to sequence model by concatenating their hidden state at each time step. @cite incorporate target language unpaired training data by doing back-translation to create synthetic parallel training data. While this technique is quite effective, its practicality seems limited to problems where the inputs and outputs contain roughly the same information (such as translation). @cite leverages the abundant monolingual data by doing multitask learning with an autoencoding objective.
- Examples of trackers based on background subtraction include the work of Fuentes and Velastin @cite , Torabi @cite , Jun @cite , Kim @cite , Mendes @cite , and Jodoin @cite .
- Fuentes and Velastin @cite proposed a method that performs simple data association via the overlap of foreground blobs between two frames. In addition to matching blobs based on overlap, Torabi @cite validated the matches by comparing the histograms of the blobs and by verifying the data association over short time windows using a graph-based approach. These approaches track objects in a merge-split manner as objects are tracked as groups during occlusion.
- Jun @cite used background subtraction to estimate the object properties. A watershed segmentation technique is used to over-segment the vehicles. The over-segmented patches are then merged using the common motion information of tracked feature points. This allows to segment vehicles correctly even in the presence of partial occlusion. Kim @cite combines background subtraction and feature tracking approach with a multi-level clustering algorithm based on the Expectation-Maximization (EM) algorithm to handle the various object sizes in the scene. The resulting algorithm tracks various road users such as pedestrians, vehicles and cyclists online and the results can then be manually corrected in a graphical interface. Mendes @cite proposed a method that also combines KLT and background subtraction. This approach increases its accuracy by setting and regions to reconfirm object trajectories and by segmenting regions when occlusions are detected.
- Since this is the first work that rigorously studies the finite-horizon throughput region, the most related prior works are the ones on infinite-horizon throughput region. Specifically, the seminal work in @cite @cite introduced the infinite-horizon throughput region and gave two important results: the infinite-horizon throughput region is the convex hull of one-slot throughput region; and the max-weight algorithm can achieve any given rate-tuple in the throughput region. @cite , the infinite-horizon throughput region was generalized and applied to time-varying wireless networks, and a max-weight algorithm based transmission policy was designed. We recommend the tutorials in @cite @cite @cite to readers who are interested in the infinite-horizon throughput region.
- Some recent studies focused on reducing the delay by shrinking the infinite-horizon throughput region @cite @cite @cite @cite , where the average delay was studied in @cite @cite , and the worst-case delay was analyzed in @cite @cite . It was observed by @cite @cite @cite @cite that choosing a rate-tuple closer to the boundary of the infinite-horizon throughput region causes a larger delay, and hence, shrinking the throughput region removes those rate-tuples near the boundary corresponding to large delays. Furthermore, the effect of finite buffer size was consider in @cite @cite @cite , and it turned out that the required buffer size increases with the rate-tuple. Indeed, by Little's law, the average length of data queue is proportional to the delay. Hence, this line of work also demonstrated the delay caused by the rate-tuples in the infinite-horizon throughput region.
- We stress that in light of the studies on infinite-horizon throughput region, a small number of studies have introduced the concept of finite-horizon throughput region. However, they did not analyze any property of the finite-horizon throughput region: The work in @cite proposed a @math -slot lookahead utility which helped to analyze the short-term performance for the proposed opportunistic scheduling algorithm, but no analysis on finite-horizon throughput region was presented; @cite , the rate-tuple over a finite time horizon was defined, but it was only employed to derive the infinite-horizon throughput region when the number of time slots goes to infinity. A possible reason for the lack of study on the finite-horizon throughput region might be that the finite-horizon throughput region was thought to have similar properties as its infinite-horizon counterpart. As we will discuss in this work, however, the finite-horizon throughput region behaves very differently as compared with the infinite-horizon throughput region.
- Deep neural networks are usually over-parameterized and the redundancy can be removed using low-rank approximation of filter matrix as shown in the work of @cite . Since then, many low-rank based methods have been proposed. Jaderberg @cite proposed to use filter low-rank approximation and data reconstruction to lower the approximation error. @cite presented a novel nonlinear data reconstruction method, which allows asymmetric reconstruction to prevent error accumulation across layers. Their method achieved high speed-up on VGG-16 model with minor increase on top-5 error for ImageNet @cite classification. Low-rank tensor decomposition methods like CP-decomposition @cite , Tucker decomposition @cite and Block Term Decomposition (BTD) @cite are also investigated and showed high speed-up and energy reduction.
- Fixed-point quantization based methods are also investigated by several recent works. developed the Expectation Backpropagation (EBP) @cite method, which is a variational Bayes method to binarize both weights and neurons and achieved good results for fully connected networks on MNIST dataset. In the work of BinaryConnect @cite , the authors proposed to use binary weights for forward and backward computation while keep a full-precision version of weights for gradients accumulation. Good results have been achieved on small datasets like MNIST, CIFAR-10 and SVHN. Binary-Weight-Network (BWN) and XNOR-net were proposed in a more recent work @cite , which was among the first ones to evaluate the performance of binarization on large-scale datasets like ImageNet @cite and yielded good results. These methods train neural networks from scratch and can barely benefit from pretrained networks. @cite found a way by first quantize pretrained weights using a reduced number of bits, followed by retraining. However, their method achieved good results only for longer bits on small datasets and heavily relied on carefully choosing the step size of quantization using exhaustive search. The scalability on large-scale datasets remained unclear.
- Besides low-rank based and fixed-point quantization based methods mentioned above, there have been other approaches. @cite utilized network pruning to remove low-saliency parameters and small-weight connections to reduce parameter size. Product quantization was investigated in the work of @cite to compress and speed-up DNNs at the same time. Teacher-student architectures @cite @cite were also well studied and achieved promising results.
- Our approach also isolates the benefit of higher @math in dealing with overfitting, as the pairwise correlations @math can be measured progressively more accurately as @math increases. In this respect, we follow a line of research using such pairwise correlations to model arbitary higher-order structure among visible units, rooted in early work on (restricted) Boltzmann Machines ( @cite @cite @cite @cite ). More recently, theoretical algorithms have been developed with the perspective of learning from the correlations between units in a network, under various assumptions on the activation function, architecture, and weights, for both deep ( @cite ) and shallow networks (using tensor decompositions, e.g. @cite @cite ). Our use of ensemble aggregation techniques (from @cite @cite ) to study these problems is anticipated in spirit by prior work as well, as discussed at length by @cite in the context of distributed representations.
- Several translations from Simulink have been proposed in the literature, including to Hybrid Automata @cite , BIP @cite , NuSMV @cite , Lustre @cite , Boogie @cite , Timed Interval Calculus @cite , Function Blocks @cite , I O Extended Finite Automata @cite , Hybrid CSP @cite , and SpaceEx @cite . It is unclear to what extent these approaches provide formal guarantees on the determinism of the translation. For example, the order in which blocks in the Simulink diagram are processed might a-priori influence the result. Some works fix this order, e.g., @cite computes the control flow graph and translates the model according to this computed order. In contrast, we prove that the results of our algorithm are equivalent for any order. To the best of our knowledge, the abstract translation proposed hereafter for Simulink is the only one formally defined and mechanically proven correct.
- The focus of several works is to validate the preservation of the semantics of the original diagram by the resulting translation (e.g., see @cite @cite @cite @cite ). In contrast, our goal is to prove equivalence of all possible translations. Given that Simulink semantics is informal ( what the simulator does''), ultimately the only way to gain confidence that the translation conforms to the original Simulink model is by simulation (e.g., as in @cite ).
- Most related work is based on the strong and limiting assumption that @math is a binary random variable (e.g., @math for the source domain, and @math for the target domain). In particular, are all based on the minimization of some form of divergence between the two distributions of @math and @math . For this reason, these works cannot directly be generalized to non-binary or continuous nuisance parameters, both from a practical and theoretical point of view. Notably, @cite enforces fairness through a prejudice regularization term based on empirical estimates of @math . While this approach is in principle sufficient for handling non-binary nuisance parameters @math , it requires accurate empirical estimates of @math for all values @math , which quickly becomes impractical as the cardinality of @math increases. By contrast, our approach models the conditional dependence through an adversarial network, which allows for generalization without necessarily requiring a growing number of training examples.
- @cite aim also at a practical application of product evaluation with specific goals. However, they concentrate more on the actual evaluation process using the ISO 14598 standard which we assume given by the V-Modell XT. They also do not explicitly discuss the aim of using the evaluation results for future optimisations.
- @cite propose a method of optimal and adaptive testing with cost constraints. They discuss that it is effective to adapt testing and to explore the interplay between software and control. However, their model does only consider testing and is not explicitly integrated in a complete process model.
- Ambler uses process patterns in @cite to describe task-specific self-contained pieces of processes and workflows in a reusable way. Such patterns can be applied to solve complex tasks when needed. St "orrle @cite shows how process patterns can be described in great detail using UML. The idea of process patterns is further refined by @cite in form of a modular and extensible software development process based on collections of independent process components. These process patterns essentially are the basis of the extension mechanism of the V-Modell XT.
- DBSCAN @cite is one of the most classic density-based methods. In DBSCAN, the cut-off density of an object @math is defined as the number of objects falling inside a ball of radius @math centred at @math . If the cut-off density of @math is higher than @math , @math is regarded as a key object. When the distance between two key objects is less than @math , they are called density-reachable. Density-reachable key objects form basic clusters. A non-key object is assigned to a basic cluster if it is within @math distance to a key object in the respective cluster; otherwise, the non-key object is treated as noise. In DBSCAN, @math and @math need to be carefully tuned as they would affect the number and formations of the final clusters. Furthermore, DBSCAN cannot handle clusters of heterogeneous densities well as both parameters, even properly selected, are fixed. In contrast, in this work, we introduce the novel concept of RNKD, which in effect homogenizes density measures and thus can handle clusters of different densities.
- Kernel density is a well-known alternative to cut-off density. Kernel density is continuous and is less sensitive to parameter selection. However, traditional kernel density methods face the same difficulty as DBSCAN when dealing with complex datasets that contain clusters with different densities @cite , and tend to confuse low-density clusters with noise. Consequently, KNN kernel density has been applied to handle such situations @cite . The proposed RNKD estimation is inspired by the KNN kernel density with further improvement allowing the inclusion of low-density clusters.
- In order for our approach to work, we must be able to identify UVC graphs and, moreover, find their unique vector colorings. This is the problem we consider in the first part of this work. UVC graphs are interesting in their own right. They were first introduced in @cite to construct tractable instances of the graph realization problem. In the same work UVC graphs were used to construct uniquely colorable graphs. UVC graphs are also closely related to the notion of universal completability (equivalently, the universal rigidity of apex graphs). This in turn is relevant to the low-rank matrix completion problem @cite .
- In Section we focus on 1-walk-regular graphs (see Definition ). The class of 1-walk-regular graphs is central to this work as we know an optimal vector coloring for all such graphs which we call the canonical vector coloring ( see Definition ). Furthermore, there exists a necessary and sufficient condition for showing they are UVC (Theorem ). Both of these results were shown in @cite .
- Standard examples of 1-walk-regular graphs are the @math and their @math -analogs, the @math . These graphs were shown to be UVC in @cite , and for completeness we explicitly give their vector colorings in . We also establish another family of UVC graphs @math (Theorem ). These graphs are constructed from the @math -distance graphs of the @math -cube.
- In we show how to construct UVC graphs whose unique vector colorings are not strict (Theorem ). We note that this is not possible using the sufficient condition from @cite (Theorem ). Using this construction we obtain graphs whose vector chromatic number is smaller than their strict vector chromatic number. Few examples of such graphs are known (e.g., see @cite ).
- For an arbitrary graph @math , the is the vertex minimal subgraph to which @math admits a homomorphism. Every graph has a unique core, and the core of @math is itself a core. Moreover, @math and @math have the same core if and only if they are , i.e., @math and @math . Cores are the unique minimal elements of these homomorphic equivalence classes. In this sense, the core of a graph is the smallest graph retaining all of its homomorphic information. In we show that if @math is UVC, and its unique optimal vector coloring is injective on its vertex neighborhoods, then @math is a core (Theorem ). To illustrate the usefulness of our sufficient condition, we given an easy proof of the fact that for @math , both @math and @math are cores. This is well-known @cite , but our proof avoids invoking the Erd o s-Ko-Rado Theorem to describe the structure of the maximum independent sets of these graphs, and it also avoids using the No Homomorphism Lemma. As a second application, we also show that the graphs @math constructed in are cores.
- Explicit nonlinear random feature maps have been constructed for many types of kernels, such as intersection kernels @cite , generalized RBF kernels @cite , skewed multiplicative histogram kernels @cite , additive kernels @cite , and polynomial kernels @cite @cite . In this paper, we focus on approximating Gaussian kernels following the seminal Random Fourier Features (RFF) framework @cite , which has been extensively studied both theoretically and empirically @cite @cite @cite .
- Key to the RFF technique is Monte-Carlo sampling. It is well known that the convergence of Monte-Carlo can be largely improved by carefully choosing a deterministic sequence instead of random samples @cite . Following this line of reasoning, @cite proposed to use low-displacement rank sequences in RFF. @cite studied optimizing the sequences in a data-dependent fashion to achieve more compact maps. In contrast to the above works, this paper is motivated by an intriguing new discovery that using orthogonal random samples provides much faster convergence. Compared to @cite , the proposed SORF method achieves both lower kernel approximation error and greatly reduced computation and memory costs. Furthermore, unlike @cite , the results in this paper are data independent.
- Structured matrices have been used for speeding up dimensionality reduction @cite , binary embedding @cite , deep neural networks @cite and kernel approximation @cite @cite @cite . For the kernel approximation works, in particular, the structured randomness'' leads to a minor loss of accuracy, but allows faster computation since the structured matrices enable the use of FFT-like algorithms. Furthermore, these matrices provide substantial model compression since they require subquadratic (usually only linear) space. In comparison with the above works, our proposed methods SORF and ORF are more effective than RFF. In particular SORF demonstrates lower approximation error and better efficiency than RFF. Table compares the space and time costs of different techniques.
- We also note that @cite established the asymptotic normality for the averaged implicit SGD procedure, which has the same limiting distribution as ASGD. Therefore, as long as our key Lemma also holds for averaged implicit SGD, our estimators (i.e., plug-in or batch-means) would also provide a consistent covariance estimate for averaged implicit SGD. We leave the verification of Lemma for averaged implicit SGD as a future work direction.
- From the above, the modest goal of the present work should be clear. We seek to project the input feature vectors to a category space a subspace formed by category basis vectors. The multi-class FLD falls short of this goal since the number of projected dimensions is one less than the number of classes. The multi-class (and more recently multi-label) SVM @cite literature is fragmented due to lack of agreement regarding the core issue of multi-class discrimination. The varieties of supervised PCA do not begin by clearly formulating a criterion for category space projection. Variants such as CCA @cite @cite , PLS @cite and structured SVM's @cite while attempting to add structure to the categories do not go as far as the present work in attempting to fit a category subspace. Kernel variants of the above also do not touch the basic issue addressed in the present work. Nonlinear (and manifold learning-based) dimensionality reduction techniques @cite @cite @cite are unsupervised and therefore do not qualify.
- There have been extensive studies on extracting typed entities and relations in text ( , context-dependent extraction). Most existing work follows an diagram---they first perform entity recognition and typing @cite @cite to extract typed entity mentions, and then solve relation extraction @cite @cite to identify relation mentions of target types. Work along both lines can be categorized in terms of the degree of supervision. While supervised entity recognition systems @cite @cite focus on a few common entity types, weakly-supervised methods @cite @cite and distantly-supervised methods @cite @cite @cite use large text corpus and a small set of seeds (or a knowledge base) to induce patterns or to train models, and thus can apply to different domains without additional human annotation labor. For relation extraction, similarly, weak supervision @cite @cite and distant supervision @cite @cite @cite @cite @cite @cite approaches are proposed to address the domain restriction issue in traditional supervised systems @cite @cite @cite . However, such a pipeline" diagram ignores the dependencies between different sub tasks and may suffer from error propagation between the tasks.
- Recent studies try to integrate entity extraction with relation extraction by performing global sequence labeling for both entities and relations @cite @cite @cite , incorporating type constraints between relations and their arguments @cite , or modeling factor graphs @cite . However, these methods require human-annotated corpora (cleaned and general) for model training and rely on existing entity detectors to provide entity mentions. By contrast, the framework runs domain-agnostic segmentation algorithm to mine entity mentions and adopts a label noise-robust objective to train models using distant supervision. In particular, @cite integrates entity classification with relation extraction using distant supervision but it ignores label noise issue in the automatically labeled training corpora.
- combines the best of two worlds---it leverages the noisy distant supervision in a robust way to address domain restriction (vs. existing joint extraction methods @cite @cite ), and models entity-relation interactions jointly with other signals to resolve error propagation (vs. current distant supervision methods @cite @cite ).
- Our proposed framework incorporates embedding techniques used in modeling words and phrases in large text corpora @cite @cite @cite ,and nodes and links in graphs networks @cite @cite . Theses methods assume links are all correct (in unsupervised setting) or labels are all true (in supervised setting). seeks to in the embedding process ( , see our comparisons with LINE @cite , DeepWalk @cite and FCM @cite in Sec. ). Different from embedding structured KB entities and relations @cite @cite , our task focuses on embedding entity and relation mentions in contexts.
- In the context of modeling noisy labels, our work is related to partial-label learning @cite @cite @cite and multi-label multi-instance learning @cite , which deals with the problem where each training instance is associated with a set of noisy candidate labels (where). Unlike these formulations, our extraction problem deals with both and . In Sec , we compare our full-fledged model with its variants and to validate the Hypothesis on entity-relation interactions.
- Bollob 'as . @cite show that the percolation threshold of dense graph is reciprocal of the largest eigenvalue of the adjacency matrix. However, the conclusion requires restrictive conditions for the networks. Especially for sparse networks, this estimation can be only regarded as providing a lower bound for the true percolation threshold.
- Since a lot of realistic networks are sparse, Karrer . @cite and Hamilton . @cite simultaneously propose that the reciprocal of the largest eigenvalue of the non-backtracking matrix is a tighter lower bound for bond and site percolation threshold, respectively, on sparse networks. This prediction is based on a message passing technique and obtained by heuristic equations and approximations on locally treelike structures. Radicchi @cite further presents a mapping between the site and bond percolation to mathematically verify the predicted bond and site percolation thresholds are identical in this method. Although the estimation based on the non-backtracking matrix is more precise than that based on the adjacency matrix, it is still not close enough to the true percolation threshold on many real networks @cite , since it suffers from the limitation of the treelike assumption. Radicchi . @cite also derives an alternative matrix of the non-backtracking matrix based on triangle elimination to improve the estimate for the site percolation threshold. However, this alternative matrix would overshoot on bond percolation. That means this estimate may overestimate the percolation threshold, leading it no longer provides a lower bound for the bond percolation threshold.
- Ying have proposed a GPU implementation for parallel computation of DNA sequence distances @cite which is based on the Euclidean distance maps (EDM), a problem in the @math -simplex class. The authors mention that the problem domain is indeed symmetric and they do realize that only the upper or lower triangular part of the interaction matrix is sufficient. Li @cite have also worked on GPU-based EDMs on large data and have also identified the symmetry involved in the computation. Jung @cite proposed packed data structures for representing triangular and symmetric matrices with applications to LU and Cholesky decomposition @cite . The strategy is based on building a (RB) for accessing and storing a triangular matrix (upper or lower). Data structures become practically half the size with respect to classical methods based on the full matrix. The strategy was originally intended to modify the data space ( the matrix), however one can apply the same concept to the parallel space.
- Ries contributed with a parallel GPU method for the triangular matrix inversion @cite . The authors identified that the parallel space indeed can be improved by using a (REC) of the grid, based on a strategy. The approach takes @math time by doing a balanced partition of the structure, from the orthogonal point to the diagonal.
- Q. Avril proposed a GPU mapping function for collision detection based on the properties of the @cite . The map is a thread-space function @math , where @math is the linear index of a thread @math and the pair @math is a unique two-dimensional coordinate in the upper triangular matrix. Since the map works in thread space, the map is accurate only in the range @math of linear problem size.
- Our main results extend the work by Ya g an @cite who established zero-one laws for the connectivity of inhomogeneous random key graph @math without employing a link-failure model. It is clear that, although a crucial first step in the study of heterogeneous key predistribution schemes, the assumption that all links are operational, i.e., reliable , is not likely to hold in most practical settings. In this regard, our work extends the results by Ya g an @cite to more practical WSN scenarios where the unreliability of links are taken into account. In fact, by setting @math for each @math (i.e., by assuming that all links are reliable ), our results reduce to those given in @cite .
- The reliability of secure WSNs was also studied in @cite , but under the Eschenauer-Gligor scheme @cite where all sensors receive the same number of keys. However, when the network consists of sensors with varying level of resources (e.g., computational, memory, power) and or with varying level of security and connectivity requirements, it may no longer be sensible to assign the same number of keys to all sensors. Our work addresses this issue by generalizing @cite to the cases where nodes can be assigned different number of keys. When @math , i.e., when all nodes belong to the same class and receive the same number of keys, our result recovers the main result in @cite .
- Another notable work that is related to ours is by @cite , who studied the @math -connectivity and @math -robustness in the inhomogeneous random key graph. A graph is said to be @math -connected if it remains connected after removal (i.e., ) of any @math nodes. Thus, the results obtained in @cite ensure the reliability of the network against the failure of any @math nodes, for some integer constant @math . Since @math -vertex-connectivity implies @math -edge-connectivity, the network is ensured to be reliable against the failure of at least @math edges, for some integer constant @math . Our work complements these results by considering the case when edge fails with probability @math , so that the total number of failed links is possibly infinite; e.g., as many as @math links may fail.
- Computational sarcasm primarily focuses on sarcasm detection: classification of a text as sarcastic or non-sarcastic. present a survey of sarcasm detection approaches. They observe three trends in sarcasm detection: semi-supervised extraction of sarcastic patterns, use of hashtag-based supervision, and use of contextual information for sarcasm detection @cite @cite @cite . However, to the best of our knowledge, no past work aims to identify phrases in a sarcastic sentence that indicate the target of ridicule in the sentence.
- Related to sarcasm target identification is sentiment target identification. Sentiment target identification deals with identifying the entity towards which sentiment is expressed in a sentence. present an approach to extract opinion words and targets collectively from a dataset. Aspect identification for sentiment has also been studied. This deals with extracting aspects of an entity (for example, color, weight, battery in case of a cell phone). Probabilistic topic models have been commonly used for the same. present a probabilistic topic model that jointly estimates sentiment and aspect in order to achieve sentiment summarization. perform multi-aspect sentiment analysis using a topic model. Several other topic model-based approaches to aspect extraction have been reported @cite . To the best of our knowledge, ours is the first work that deals with sarcasm target identification.
- SGD is the oldest and simplest method of solving problem . Though SGD is easy to implement and converges to modest accuracy quickly, it requires a long tail of iterations to reach good' solutions and also requires adjusting a step-size parameter. On the other hand, SDCA methods are free of learning-rate parameters and have faster convergence rate around the end @cite @cite . A modified SGD has also been proposed with faster convergence by switching to SDCA after quickly reaching a modest solution @cite . Recently, variance reduced' modifications to the original SGD have also caught attention. These modifications estimate gradients with small variance as they approach to an optimal solution. Mini-batch algorithms are also proposed to update several dual variables (data points) in a batch rather than a single data point per iteration @cite . Mini-batch versions of both SGD and SDCA have slower convergence when the batch size increases @cite @cite . All these sequential algorithms become ineffective when the datasets get bigger.
- In the early single communication scheme @cite @cite @cite , a dataset is decomposed' into smaller parts that can be solved independently. The final solution is reached by accumulating' the partial solutions using a single round of communications. This method has limited utility because most datasets cannot be decomposed in such a way. Using the primal-dual relationship , fully distributed algorithms of DCA are later developed where each processor updates a separate @math which is then used to update @math , and synchronizes @math across all processors ( CoCoA @cite ). To trade off communications vs computations, a processor can solve its subproblem with @math dual updates before synchronizing the primal variable ( CoCoA+ @cite ,DisDCA @cite ). @cite @cite , a more general framework is proposed in which the subproblem can be solved using not only SDCA but also any other sequential solver that can guarantee a @math -approximation of the local solution for some @math . Nevertheless, the synchronized update to the primal variables has the inherent drawback that the overall algorithm runs at a speed of the slowest processor even when there are fast processors @cite .
- Multi-core shared memory systems have also been exploited, where the primal variables are maintained in a shared memory, removing the communication cost. However, updates to shared memory requires synchronization primitives, such as locks, which again slows down computation. Recent methods @cite @cite avoid locks by exploiting (asynchronous) atomic memory updates in modern memory systems. There is even a wild version in @cite that takes arbitrarily one of the simultaneous updates. Though the shared memory algorithms are faster than the distributed versions, they have an inherent drawback of being not scalable, as there can be only a few cores in a processor board.
- Besides distributed DCA methods, there are several recent distributed versions of other algorithms with faster convergence, including distributed Newton-type methods (DISCO @cite , DANE @cite ) and distributed stochastic variance reduced gradient method (DSVRG @cite ). It has been shown that they can achieve the same accurate solution using fewer rounds of communication, however, with additional computational overhead. In particular, DISCO and DANE need to solve a linear system in each round, which could be very expensive for higher dimensions. DSVRG requires each machine to load and store a second subset of the data sampled from the original training data, which also increase its running time.
- The ADMM @cite and quasi-Newton methods such as L-BFGS also have distributed solutions. These methods have low communication cost, however, their inherent drawback of computing the full batch gradient does not give computation vs communications trade-off. In the context of consensus optimization, @cite gives an asynchronous distributed ADMM algorithm but that does not directly apply to solving .
- To the best of our knowledge, this paper is the first to propose, implement and analyze a hybrid approach exploiting modern HPC architecture. Our approach is the amalgamation of three different ideas -- 1) CoCoA+ DisDCA distributed framework, 2) asynchronous multi-core shared-memory solver @cite and 3) asynchronous distributed approach @cite -- taking the best of each of them. In a sense ours is the first algorithm which asynchronously uses updates which themselves have been computed using asynchronous methods.
- Prior to the current work, certain specific cases had already been solved. In the case of the torus, Ng and Schauenburg's Congruence Subgroup Theorem implies the much stronger result that any Reshitikhin-Turaev representation of the mapping class group of the torus has finite image @cite . Another related result is due to Fjelstad and Fuchs @cite . They showed that, given a surface with at most one boundary component, the mapping class group representations corresponding to the untwisted (i.e. @math ) Dijkgraaf-Witten theory have finite image. Their paper uses an algebraic method of Lyubashenko @cite that gives a projective mapping class group representation to any factorizable ribbon Hopf algebra, in their case, the double @math . In our case, we instead consider the mapping class group action on a vector space of @math -colored embedded graphs defined by Kirillov @cite , yielding a simpler, geometric proof of the more general twisted case.
- Bantay also calculated the images of certain representations of mapping class groups on the Hilbert space of an orbifold model associated to @math @cite . These representations appear to coincide with the twisted Dijkgraaf-Witten representations. However, due to lack of proof, the precise connection is unclear.
- EnsembleMatrix @cite enables users to choose any combination of multiple classifiers to build a combination model, after showing users the confusion matrix of all classifiers. Thus it provides an interaction visualization tool emerging user intelligence in ML process. However, both the training and mode structure of each classifier are still invisible to users.
- Expertise. The PI has conducted extensive research on optimizing boolean crowdsourcing @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite , and the design of optimized crowd-powered systems---the database Deco @cite @cite @cite @cite @cite , and search engine DataSift @cite @cite . The PI's work brings together techniques from databases, data mining, and crowdsourcing, and has been published in top-tier venues. In the last four years, the PI has published 20 papers on the topic , and over 50 overall, with an h-index of 20 , garnering four best paper award citations VLDB'10, KDD'12, ICDE'14, ICDE'16 and three best dissertation awards on the topic SIGMOD Jim Gray Award, SIGKDD Dissertation Award Runner-up, Stanford Dissertation Award (one each from databases and data mining). Given his expertise in boolean crowdsourcing, the PI is well-equipped to take on the challenges in open-ended crowdsourcing.
- Area 2: Crowdsourcing Systems and Toolkits. Many groups have been building crowdsourcing systems and toolkits to harness crowdsourcing in a declarative'' manner @cite @cite @cite @cite , as well as several domain-specific toolkits @cite @cite @cite @cite . All these systems and toolkits could benefit from the design of optimized algorithms as building blocks.
- Area 4: Decision Theory: Recent work has leveraged decision theory for improving cost and quality in simple workflows, typically using POMDPs (Partially Observable MDPs), to dynamically choose the best decision to make at any step, e.g.. @cite @cite . While some of this work could be applicable to some open-ended tasks, there are no optimality guarantees associated with any of these techniques.
- already proposed an attention-based approach with multichannel input @cite . In their work an attention mechanism is embeded within a Recurrent Neural Network based acoustic model in order to weigh channel inputs for a number of frames. The more reliable channel input will get higher score. By calculating phase difference between microphone pairs, they also incorporated spatial information in their attention mechanism to accelerate learning of auditory attention. They reported that this model achieve comparable performance to beamforming techniques.
- One advantage of using such model is that no prior knowledge of microphone array geometry is required and real time processing is done much faster than models with front-end preprocessing techniques. However, by using conventional DNN-HMM acoustic model, they did not utilize the full potential of attention mechanism in their proposed model. The feature vectors, weighted by attention mechanism, can help to condition the generation of the next element of the output sequence. In our model, attention mechanism is similar to @cite but it I also connected to the ARGS to produce an end-to-end results.
- The notion that biological motor systems are hierarchical is ancient, dating to the 19th century @cite . In the 20th century, Bernstein promulgated the notion of hierarchical control through motor synergies,'' or stereotyped, multi-joint muscle activation combinations @cite . More recently, the notion of spinal motor primitives has been forwarded by Mussa-Ivaldi and Bizzi @cite and others @cite . Motor primitives have resonated in robotics, especially as Dynamic Movement Primitives @cite , which are low-dimensionality attractor systems that can simplify the learning of robot movements, and hierarchical robot control abstractions date to at least the 1980s @cite . Modern control theorists have also considered abstract hierarchical architectures for manipulation @cite and more bio-mechanical descriptions of locomotion @cite .
- The reinforcement learning literature has also explored a wide variety of temporal abstractions that wrap low-level control into @cite or @cite . These temporal abstractions may be applied to motor control @cite @cite @cite , may be transferred to new tasks @cite @cite @cite , and may also incorporate information hiding principles @cite @cite . However, this prior work typically requires precise subgoals to be specified, and treats options or skills as atomic actions. In contrast, our low-level motor skills emerge organically from pre-training in the context of natural tasks; and our high-level controller modulates these skills in a flexible manner to achieve its ends. Recent work @cite proposes an architecture for discovering temporally extended macro actions from scratch.
- Evaluation of visual attribute meaningfulness is traditionally conducted by manually checking the presence absence of consistent identifiable visual concepts in a set of given images. This task usually requires a large-scale human labelling effort. A system such as the Amazon Mechanical Turk (AMT) www.mturk.com is able to handle this task for small datasets. However, since this process needs to be repeated whenever new attributes are discovered or novel methods are proposed, this manual process is ineffective and expensive. In our case, the AMT Human Intelligence Task (HIT) is to evaluate the meaningfulness of attributes by examining corresponding positive and negative images according to each attribute. The average time of each worker spent on this typical HIT is 2 minutes @cite . Then an AMT worker may require 320 minutes to evaluate 32 attributes discovered by 5 different methods ( @math minutes). The time spent could increase significantly if statistically reliable results are desired by increasing the number of AMT workers.
- Some unsupervised semantic visual representation learning works @cite @cite @cite @cite have indicated that it is possible to discover the meaningful concepts unsupervisedly from data itself with or without side information. Such as Chen al @cite introduce a simple yet powerful unsupervised approach to learn and predict visual attributes directly from data. With the help of deep Convolutional Neural Networks (CNNs), they train to output a set of discriminative, binary attributes often with semantic meanings. Hong al @cite propose a novel algorithm to cluster and annotate a set of input images with semantic concepts jointly. They employ non-negative matrix factorization with sparsity and orthogonality constraints to learn the label-based representations with the side information (a labeled reference image database) obtaining promising results.
- All of these works imply there may be some potential relations between meaningful concepts. Fortunately, the shared structure assumption among meaningful attributes proposed in @cite can serve as the foundation of the automatic measurement. Based on this assumption, Parikh and Grauman al proposed an active learning approach that uses Mixtures of Probabilistic Principal Component Analysers (MPPCA) @cite to predict how likely an attribute is nameable. Nevertheless, their work only focuses on deciding whether an attribute is nameable or not. Their work does not tackle the problem of quantitatively measuring the attribute meaningfulness. In addition, this approach requires human interaction to populate the nameability space. Thus, their method is not suitable for addressing our goal (i.e., to automatically evaluate the meaningfulness of attribute sets).
- In our previous work, the shared structure assumption is utilized @cite . In particular, the work in @cite proposed a selection approach of attribute discovery methods to assist attribute-based keywords generation for video description from surveillance systems. However, the work did not consider quantitative analysis of the meaningfulness of the discovered attributes ( how much meaningful content is contained in a set of automatically discovered attributes). In addition, the characteristics of the meaningfulness of attributes may vary to some extent.
- CE or method @cite is similar to DALI in that it too breaks each structure in the query set into a series of fragments that it then attempts to reassemble into a complete alignment. A series of pairwise combinations of fragments called aligned fragment pairs, or AFPs, are used to define a similarity matrix through which an optimal path is generated to identify the final alignment. Only AFPs that meet given criteria for local similarity are included in the matrix as a means of reducing the necessary search space and thereby increasing efficiency @cite . However, in spite of having good accuracy it is impossible to implement these two methods as a real time web service due to their huge computational cost.
- FATCAT @cite or lexible structure lignmen by haining ligned fragment pairs with wists is a unique approach in a sense that it took the structural rearrangements that the proteins go through into account. This is different as in most methods the structure is treated like a fixed body. In comparison with other concurrent methods like FlexProt @cite and other rigid body approaches it produced good results in most cases.
- Another direction in this field was taken by in @cite where they used the . They took 3 atoms or 3 @math -carbon atom triplets (both can be done) from the protein chains. From n atoms, among @math triplets, they take triplets with some restrictions. At this point they have a set of triplets. Then they prepare a hash table with sides of the triangle as keys to the hash table. Then they selectively align atoms of the two chains using the hash table.
- ProteinDBS @cite is another method that uses some common features of CBIR (Content Based Image Retrieval) to compare @math -carbon distance matrix images. This method is much faster than the previous ones as it compares only some specific image features. Notably however, the feature preprocessing done by ProteinDBS is computationally expensive.
- Classical neural networks were shallow (1-2 layers), where batch gradient descent methods worked well. However, with deeper networks, the algorithms frequently suffered from the vanishing gradient problem @cite ). The standard algorithm for training them (described in section ) fails because the gradients become smaller by several orders of magnitude as the network becomes deeper. This problem was solved in @cite and @cite , who demonstrated that a network can be trained one layer at a time with @cite , and then put it together into a single network for classification @cite . Another solution, that we will use, are rectified linear units, which have become the standard in the field, though our results are valid for other types of neurons as well. These optimizations are available in Caffe and other Deep Learning packages.
- In biology, apoptosis -- the death of neurons -- and its inverse neurogenesis, have been studied @cite , and it has been determined that these processes can aid in learning effectively. Chambers modeled the apoptosis neurogenesis process by re-initializing the weights of randomly selected neurons periodically. Their study concluded that periodic apoptosis can improve the performance of a network. This forms the basis of our paper -- by conducting adaptive apoptosis during the training phase.
- Several researchers have conducted offline neuron apoptosis -- after the completion of the training phase. This is in sharp contrast to our proposed approach, in which apoptosis is conducted during the training phase itself, which reduces the overall training time, while reducing the space requirements. For offline apoptosis, researchers have conducted neuron apoptosis due to a lack of computational resources @cite , regularization to prevent overfitting @cite , which provides algorithms for removing synapses and possibly neurons. Kamruzzman have demonstrated similar results @cite , where the fundamental objective is to remove redundant weights, and possibly neurons. The pruning has been also applied for generating human-usable rules for classification @cite . Other researchers have considered temporary neuron pruning -- typically referred as Dropout , proposed by Hinton @cite . A selective Dropout is proposed by other authors @cite . However, since the Dropout is temporary, they do not affect the final topology of the neural network -- although they help with regularization.
- Recently, Han have proposed methods to remove non-contributing weights after the training phase @cite . However, this incurs significant slowdown (up to 2.5x). Other approaches -- such as HashedNets -- compress the neural networks without removing weights neurons @cite . Murray and Chiang have proposed methods to remove non-contributing synapses (which are significantly different from our approach of removing redundant neurons) @cite @cite . Since we are primarily focused on removing redundant neurons adaptively, we consider our approaches to be complimentary to their approach.
- Although there are proposed methods for sentence segmentation of Portuguese datasets @cite @cite @cite , none of them are used for transcriptions produced in a clinical setting for the elderly with dementia and related syndromes. The study most similar to our scenario is @cite , which proposes a segmentation method for aphasic speech based on lexical, PoS and prosodic features using tools and a generic acoustic model trained for English. Their approach is based on a CRF model, and the best results for this study were obtained for non-spontaneous broadcast news data.
- @cite proposed an approach based on probabilistic graphical models to annotate simultaneously entities, types and relations in tables. To perform this task, they map types in a given ontology to values in a column of a table. The model is based on Factor Graph @cite . The two main design decisions of this model consist on the choice of random variables and its values, and the construction of potential functions, a.k.a factors. There are random variables for the type of each column in the data, for the label of each cell and for each relation between two columns. The chosen factors are a function of the weighted sum of features. A major drawback of this approach is its requirement of having an ontology in the domain, which is not always possible or practical.
- CNNs have been successfully applied to various problems, especially in the field of computer vision @cite . Residual learning @cite allows such networks to have a very super deep structure. Recurrent neural networks (RNNs) have been used successfully for sequence learning tasks @cite . The incorporation of long short-term memory (LSTM) enables RNNs to learn long-term temporal dependency. However, both kinds of neural networks can only capture spatial or temporal dependencies. Recently, researchers combined above networks and proposed a convolutional LSTM network @cite that learns spatial and temporal dependencies simultaneously. Such a network cannot model very long-range temporal dependencies ( , period and trend), and training becomes more difficult as depth increases.
- Most of the navigable representations we have mentioned require complicated construction algorithms, generally defying a parallel implementation. It is not known how to compute a book embedding @cite in parallel, which is necessary to build the representations of Jacobson and of Munro and Raman. There are also no parallel algorithms to build orderly spanning trees @cite , necessary for the representation of Chiang Its predecessor @cite uses instead a triangulation and a canonical ordering; for the latter there is only a CREW construction running in @math time with @math processors @cite . As for the vertex separators @cite required to build the representation of Blandford and of Blelloch and Farzan, Kao @cite designed a linear-work, logarithmic-span algorithm for computing a cycle separator of a planar graph. However, the construction of these representations of planar embeddings decompose the input graph by repeatedly computing separators until each piece is sufficiently small. This increases the total work to @math even if this optimal parallel algorithm is used. The best linear-work parallel algorithms @cite for building the realizers @cite used in the construction of Barbay 's representation have @math span in the expected case but @math deterministic span.
- For touch only input techniques, one of the major issues is the imprecision and occlusion of touch interfaces, also known as the fat finger problem. For example, Offset Cursor displays the cursor at a fixed distance on the screen to avoid the finger occlusion @cite ; similarly, Shift applies a hybrid approach where a normal coarse direct touch selection is followed by a precise position adjustment @cite . Along this line, Roudaut al proposed TapTap and MagStick that further improve the efficiency and accuracy of small target selection @cite . Another relevant area is that concerning one-handed interaction. Karlson al have shown many cases in which users prefer one-handed use for mobile devices @cite . Some gestures for performing one-handed zooming have been widely used on commercial systems, such as double tap on iOS and tap then slide in Google Map mobile app. Other techniques such as rubbing @cite or rolling @cite have also been proposed to facilitate single hand touch manipulations.
- However, touches are not always convenient for a user. Examples include when one is wearing gloves or visually impaired, leading some researchers to leverage the device motions to design novel gestures for mobile phone interaction. Based on a users spatial awareness, Virtual Shelves supports triggering shortcuts mapped to a hemisphere space in front of the body @cite ; however, external sensors for tracking the phone are required. Accelerometer and gyroscope are widely used internal sensors for detecting motion gestures, for instance: auto screen rotation based on a devices orientation @cite , rolling a phone on different axes to scroll or command @cite , tapping the back of a device or jerking it @cite , and whacking a phone with hard contact forces @cite . Also, through coupling the accelerometer data and touch events on an interactive surface, PhoneTouch allows for direct target selection using the phone and a pick &drop style of data transferring @cite .
- Combining touch and motion further extends our interaction vocabulary with mobile phones. Hinckley al presents a summary of such techniques and how the creation of novel gestures is possible with touch and motion sensors @cite . Device orientation inferred from motion sensors is one important input to be used simultaneously with touch. TiltText employs tilting angles to resolve text input ambiguities of the traditional phone keypad @cite . Other usages of tilting and touch include measuring wrist deflection angles @cite and scrolling @cite . ScatterDice Mobile supports the exploration of multi-dimensional data on mobile phones by mapping its orientation to different chart viewing perspectives @cite .
- In addition to orientation, device movement can provide more input modalities. For example, Boom Chameleon demonstrates a display tracked with 6 degrees-of-freedom in space as a window in a virtual 3D world @cite . TouchProjector @cite and Spilling @cite allow the user to manipulate an object by direct touch on the screen augmented by movement of the phone. Another type of motion involves hard force gestures such as shaking, tapping, and striking the device. Examples include using accelerometer to measure typing pressure @cite and the chucking technique @cite . Hinckley al also present a number of techniques in this category, such as hold and shake and hard drag @cite .
- Hand pose estimation is a frequently visited problem in Computer Vision, and is the subject of a plethora of published work. We refer to @cite for an overview of earlier work, and here we will discuss only more recent work, which can be divided into two types of approach.
- The first type of approach is based on discriminative models that aim at directly predicting the joint locations from RGB or RGB-D images. Some recent works include @cite @cite @cite @cite @cite that use different approaches with Random Forests, but restricted to static gestures, showing difficulties with occluded joints, or causing high inaccuracies at finger tips. These problems have been addressed by more recent works of @cite @cite that use Convolutional Neural Networks, nevertheless still lacking in high accuracy levels.
- For the hand model, different hand-crafted models were proposed. The choice of a simple model is important for maintaining real-time capabilities, and representing a trade-off between speed and accuracy as a response to a potentially high degree of model abstraction. Different hand-crafted geometrical approximations for hand models were proposed. For example, @cite uses a hand model consisting of spheres, @cite adds cylinders, ellipsoids, and cones. @cite models the hand from a Sum of Gaussians. More holistic hand representations are used by @cite @cite @cite , with a linear blend skinning model of the hand that is rendered as depth map. @cite increases the matching quality by using depth images that resemble the same noise pattern as the depth sensor. @cite uses a fully shaded and textured triangle mesh controlled by a skeleton.
- Different modalities were proposed for the similarity function, which are coupled to the used model. The modalities include, , depth values @cite @cite @cite , salient points, edges, color @cite , or combinations of these @cite @cite @cite @cite .
- The optimization of the similarity function is a critical part, as the high dimensional pose space is prone to local minima. Thus, Particle Swarm Optimization is often used to handle the high dimensionality of the pose vector @cite @cite @cite @cite . Differently, @cite @cite @cite use gradient-based methods to optimize the pose, while @cite uses dynamics simulation. Due to the high computation time of these optimization methods, which have to be solved for every frame, @cite does not optimize the pose but only evaluates the similarity function for several proposals to select the best fit.
- In order to kick-start optimization, @cite uses a discriminative part-based pose initialization, and @cite uses finger tips only. @cite predicts candidate poses using a Hough Forest. @cite requires predefined hand color and position, and @cite relies on a manual initialization. Furthermore, tracking-based methods use the pose of the last frame @cite @cite @cite , which can be problematic if the difference between frames is too large, because of fast motion or low frame rates.
- Since we learn to generate images of the hand, our approach is also related to generative approaches, in particular @cite . It uses a feedback loop with an updater mechanism akin to ours. It predicts updates for the position from which a patch is cropped from an image, such that the patch fits best to the output of a generative model. However, this step does not predict the full set of parameters. The hidden states of the model are found by a costly sampling process.
- @cite relies on a given black-box image synthesizer to provide synthetic samples on which the regression network can be trained. It then learns a network to substitute the black-box graphics model, which can ultimately be used to update the pose parameters to generate an image that most resembles the input. In contrast, we learn the generator model directly from training data, without the need for a black-box image synthesizer. Moreover, we will show that the optimization is prone to output infeasible poses or get stuck in local minima and therefore introduce a better approach to improve the pose.
- Topic browsing tools have been built based on topic models. Many tools use LDA for topic modeling @cite @cite @cite @cite . They do not show any hierarchy of topics. attempts to build a tool with a topic hierarchy by recursively splitting and remodeling a corpus based on LDA. Unlike HLTA, the topic model does not have a strong statistical basis.
- On the other hand, a different set of methods, named approaches, starts with an initial clusters of pixels, then refines iteratively until convergence in visual consistency. In this line of research, Mean-shift @cite , Turbo-pixel @cite and state-of-the-art SLIC @cite should be mentioned. Note that, SLIC has picked as one of our baselines and compare our method with.
- A number of previous works have explored the problem of model-free tracking of objects in the environment of an autonomous vehicle ( @cite @cite @cite ). Typically, these approaches follow the traditional paradigm of a multi-component pipeline, with separate components to parametrise and detect objects, associate new measurements to existing tracks, and estimate the state of each individually tracked object. The use of multiple stages in the framework is cumbersome and introduces extra unnecessary failure modes for the tracking algorithm.
- Recent work proposes to replace these multiple stages with an end-to-end learning framework known as , by leveraging neural networks to directly learn the mapping from raw laser input to an unoccluded occupancy grid ( @cite @cite ), even with relatively small amounts of data. The approach utilises an RNN architecture using @cite to capture the state and evolution of the world in a sequence of laser scan frames. Another work @cite considers and takes a different angle to predicting occupancy in dynamic environments. They explicitly encode a range of velocities in the hidden layers of a recurrent network, and use Bayesian optimization to learn the network parameters which update velocity estimation and occupancy prediction. However, the model does not explicitly track objects through occlusion.
- In this study, our proposed speech enhancement algorithm can be categorized as modulation subspace based semi-supervised LSD. Modulation domain based source separation technologies are mainly developed according to the knowledge that the spectrogram of speech can be described as a time-varying weighted sum of component modulations @cite . By exploiting intrinsic decomposition through well convex optimization, low-rank and sparse analysis overcomes the high sensitive of the conventional principle component analysis (PCA) when subjecting to large corruptions.
- SD aims at extracting subgroups of individuals for which the distribution on the target variable is statistically different from the whole (or the rest of the) population [ @cite @cite ]. Two similar notions have been formalized independently and then unified by @cite : Contrast set mining and emerging patterns. Close to SD, redescription mining aims to discover redescriptions of the same group of objects in different views @cite . Exceptional model mining (EMM) was first introduced by @cite (see a comprehensive survey by @cite ). EMM generalizes SD dealing with more complex target concepts: There are not necessarily one but several target variables to discriminate. EMM seeks to elicit patterns whose extents induce a model that substantially deviates from the one induced by the whole dataset.
- An intuitive idea to address the problem of resource waste due to the delivery of non-displayed video data is to stream only the part of the video that corresponds to the viewport. This solution however does not enable fast navigation within the @math -degree video: When the client moves the head, the center changes, requiring a new viewport to be immediately displayed. Since the device has no knowledge about other parts of the spherical video, it has to notify the server about the head movement and wait for the reception of the newly adjusted viewport. As seen in other interactive multimedia systems @cite , this solution cannot meet the 10 ms latency requirement in the standard Internet, even with the assistance of CDN . In addition, this solution requires the server to extract a part of the video (thus to spend computing resources) for each client connection.
- To deal with the cases of end-users consuming only a fraction of the video (navigable panorama @cite @cite @cite and large-resolution video @cite ), the most studied delivery solution leverages the concept of . The idea is to spatially cut a video into independent tiles. The server offers multiple video representations of each tile; the client periodically selects a representation for each tile and it has to reconstruct the full video from these tiles before the viewport extraction. In a short paper, have sketched a tile-based streaming system for @math -degree videos. In their proposal, the spherical video is mapped onto an video, which is cut into @math . More recently, proposed a -based tiling of a @math -degree video to take into account projection distortion. They also present an approach to describe the tiles with MPEG DASH SRD formatting principles. also propose the delivery of tiles based on a prediction of the head movements. evaluate the impact of different tiling scheme on the compression efficiency and on the transmission bit-rate saving.
- A @math -degree video provider (Facebook) has recently released detailed the implementation of its delivery platform @cite . The spherical video is projected onto a pyramid layout from up to @math central points to generate a set of video representations. Since the front face of pyramid projection has a better image quality than the other faces, the system is in essence similar to our concept of QER . The end-users periodically select one of the representations based on their center. This implementation corroborates that, from an industrial perspective, the extra-cost of generating and storing multiple QER -based representations of the same video is compensated by bandwidth savings and enhanced system usability. However, as seen in , the pyramid projection is not the best regarding the viewport quality. Moreover, the system uses the same video quality on each face, which is less efficient than our proposal. Finally, the impact of the video encoding on the solution is not given.
- Visual Caption Generation: There is an increasing interest in jointly learning from images videos and natural language sentences. In the last 1-2 years, several works have been developed for automatic image @cite @cite @cite @cite @cite @cite @cite and video @cite @cite captioning. Most of these models use Recurrent Neural Networks (RNN), or LSTMs, in an encoder-decoder architecture (i.e. CNNs or CNN+LSTM encoder, for images or videos respectively, to generate a hidden semantic state and RNN LSTM decoder that decodes resulting hidden state using a trained language model to produce a final sentence description). Automatic captioning is a very challenging task, both from an algorithmic and evaluation point of view. The latter is particularly challenging with difficulties arising from evaluation of specificity, linguistic correctness and relevance of generated captions.
- Visual Question Answering: Because of the aforementioned challenges, image @cite @cite @cite and, more recently, video @cite Visual Question Answering (VQA) has became a preferred alternative to caption generation. VQA datasets @cite @cite @cite @cite consist of large number of structured (e.g. multiple choice or fill-in-the-blank) and unstructured (e.g. free form) image-specific questions with corresponding answers. As a result, evaluation in VQA setting tends to be considerably more objective, requiring algorithms to have, at a minimum, certain level of visual understanding to answer the poised questions.
- Image-Caption Retrieval: Another alternative is image-caption retrieval that has been defined as a standard way to evaluate joint language-visual models @cite @cite @cite @cite . The core idea is to rank a set of images according to their relevance to a caption query ( a.k.a , image retrieval) or ranking captions according to their relevance to the given image query ( a.k.a , caption retrieval). Image-caption retrieval approaches, typically, learn a joint embedding space that minimizes a pair-wise ranking objective function between images and captions. Particularly relevant to our paper, is the work of @cite , where it is acknowledged that different forms of captions form a visual-semantic hierarchy and the order-preserving constraints are used as objective for learning.
- An algorithm using a column-pivoted QR for cluster assignment of general point-cloud data with an assumption of orthogonality amongst the clusters has previously appeared in the literature in the context of spectral relaxation of the k-means objective @cite . Curiously, though we find the basic idea to be powerful, this algorithmic approach seems to have been ignored and we can find no further reference to it. We expand upon this work, taking advantage of algorithmic improvements appearing in the computational chemistry literature for greater efficiency. Further, by addressing the sparse adjacency matrix case instead of the Gram matrix of point-cloud data, we are able to strongly motivate our approach based on proven spectral properties of model graphs and graphs with assumed underlying cluster structure.
- Spectral methods as we discuss here stem from work on the Fiedler vector @cite @cite for the two-block case and spectral embeddings coupled with k-means clustering @cite in the multi-block case. For a more comprehensive overview of initialization procedures for k-means see, , @cite . Recent analysis of these methods applied to the SBM @cite @cite focuses on misclassification of nodes. Another line of work considers matrices besides the adjacency matrix or normalized Laplacian to achieve theoretical detection thresholds @cite @cite . Other recent work @cite demonstrates where spectral methods break down and argues for the use of SDP-based methods.
- Using global descriptors of the local point cloud for loop-closures is also proposed @cite @cite @cite . propose to describe each local point cloud with a 1D histogram of point heights, assuming that the sensor keeps a constant height above the ground. The histograms are then compared using the metric for recognizing places. describe point clouds with rotation invariant features such as volume, nominal range, and range histogram. Distances are computed for scalar features and cross-correlation for histogram features, and an AdaBoost classifier is trained to match places. Finally, ICP is used for computing the relative pose between point clouds. In another approach, split the cloud into overlapping grids and compute shape properties (spherical, linear, and several type of planar) of each cell and combine them into a matrix of surface shape histograms. Similar to other works, these descriptors are compared for finding loop-closures.
- While local keypoint features often lack descriptive power, global descriptors can struggle with invariance. Therefore other works have also proposed to use 3D segments or objects for the place recognition task. , for example, propose to perform place recognition by detecting planes in 3D environments. The planes are accumulated in a graph and an interpretation tree is used to match sub-graphs. A final geometric consistency test is conducted over the planes in the matched sub-graphs. The work is extended in @cite to use the covariance of the plane parameters instead of the number of points in planes for matching. This strategy is only applied to small, indoor environments and assumes a plane model for segments which is no longer valid in unstructured environment. A somewhat analogous, seminal work on object-based loop-closure detection in indoor environments using RGB-D cameras is presented by . Although presenting interesting ideas, their work can only handle a small number of well segmented objects in small scale environments.
- We therefore aim for an approach which does not rely on assumptions about the environment being composed of simplistic geometric primitives such as planes, or a rich library of objects. This allows for a more general, scalable solution. Inspiration is taken from and which proposed different SLAM techniques based on segments. A strategy for aligning Velodyne scans based on segments is proposed in @cite where the is used to compare and match segments as defined in @cite . Analogously, @cite proposed an Extended Kalman Filter solution which uses segments as landmarks, rather than point features.
- Traceability through Wi-Fi technology has been investigated from several end-point perspectives. In @cite , the authors propose a cooperative system called Argos capable of distributively collect Wi-Fi traffic in an urban area. A group of geographically distributed sniffers is instructed to be aware of a desired client unique address, and after the required traffic is collected, all intervening sniffers pass the information to a central server which in turn infers about the mobility patterns.
- Many online learning methods were also proposed in resent years, e.g., @cite proposed the kernel based perceptron with budget, @cite improved the online passive-aggressive algorithm, and @cite extended online gradient descent. However, they require the data to be of the same length or well aligned. In addition, they are more suitable to operate in the feature space rather than on the raw time series. Therefore, we consider these methods as pseudo-online because they need to preprocess (i.e., truncating or aligning) the raw time series.
- The security of lattice coset codes can be quantized either by Eve's correct decision probability, or alternatively by the mutual information of the message and Eve's received signal. For the (AWGN) channel, upper bounds are known for both approaches @cite @cite . More importantly, both are increasing functions of the of the lattice @math , yielding its minimization as a design criterion. Sequences of lattice coset codes achieving security and reliability are also constructed in @cite . For different fading channel models, various alternative design criteria based on probability and information bounds were derived in @cite @cite , and @cite , respectively. Codes achieving security and reliability in the (MIMO) channel, and an information bound, were given in @cite . However, to the best of our knowledge, practical low-dimensional code constructions are an open problem in the fading (SISO) channel.
- To the best of our knowledge, the only DTT approximation archived in literature was proposed in @cite . That approximation was obtained by means of a parameterization of integer functions combined with a normalization of transformation matrix columns. The derived approximation in @cite furnishes good coding capabilities, but it lacks orthogonality or near-orthogonality properties. As a consequence, the forward and inverse transformations are quite distinct and possess unbalanced computational complexities.
- In the field of meeting summarization, while extractive techniques have been widely employed so far @cite @cite , abstractive techniques, including sentence compression, template and graph-based approaches, have been focused on recently.
- Liu and Liu used sentence compression to generate summaries of meetings. However, they reported that the quality of the generated summaries are not so good and there is a potential limit to apply such methods to summarization. Murray mapped conversations to an ontology that was complemented with a Natural language generation (NLG) component used for transforming utterances to summaries. The corresponding full summarization system was later presented in @cite , where a user study was conducted on the abstractive summaries that were generated. However, the full summarization system involved extensive manual labor to set specific speakers, entities, etc. in a template before using an NLG realizer to generate the summaries. Lu and Cardie proposed a method that learns templates from the human written summaries and generates the summaries of decisions and actions of meetings by using the best set of templates for a particular summary ranked using a greedy approach. In contrast, we cannot use templates because we assume that the type of a conversation (action, decision, etc) is not known apriori.
- Our previous work @cite has briefly described the effectiveness of the fusion-based technique, which is also employed in this work. Our preliminary results demonstrated that the fusion based model can combine and convey useful information, generating reasonable abstractive meeting summaries. Hence, we extend this work using topic segments to build an end-to-end framework. We address the issue of readability of the generated summaries by modeling the strength of grammatical relations in the optimization problem. Our approach does not require creation of templates. Instead, our model aims to generate a sentence on each topic by identifying relevant grammatical relations and informative words from a collection of important utterances in a meeting.
- @cite is a distributed information management service. It works locating and collecting the status of a set of servers and reporting summaries of this information. is implemented using a P2P overlay, where every peer run an agent (, in a MAS fashion). However, was developed primarily using simple data models. Besides, its operation is aimed at read-oriented applications.
- (SDIMS) @cite is a service to aggregate information about large-scale network systems. The service is built using ideas from @cite and Distributed Hash Tables (DHT). However, as in most DHT approaches, consistency and replication issues are a known challenge.
- The presented efforts show interesting characteristics for consistency of shared information in distributed systems. However, these efforts have vulnerabilities which make them not appropriate for management CDNs, such as centralization @cite , simple data models @cite , and replication issues @cite .
- On the other hand, work on causal inference for observational network data is still in its infancy. @cite extend these IPW estimators in the presence of general forms of interference on networks. They focus on direct and indirect effects based on Bernoulli allocation strategies similar to those defined by . Although their estimators allows for general forms of interference, their estimator and their asymptotic results are proved under partial interference and clustered data. and then propose a TMLE estimator for similar treatment and spillover effects and prove asymptotic results under IID assumptions. Finally extend this TMLE estimator to allow for dependence due to both contagion and homophily and derive asymptotic results that allow the number of ties per node to increase as the network grows.
- In recent years, deep neural network has achieved big breakthroughs in many fields. In computer vision field, convolutional neural network (CNN) @cite is one of the most efficient tools to extract effective image features from raw image pixels. In speech recognition, deep belief network (DBN) @cite is used and much better performance is obtained comparing with Gaussian mixture models. Comparing with traditional models that have shallow structure, deep learning can model the underlying patterns from massive and complex data. With such learning ability, deep learning can be used as a good feature extractor and applied into many other applications @cite @cite . In CTR prediction field, besides @cite that is mentioned in Section 2.1, DNN has also been used in some public CTR prediction competitions https: www.kaggle.com c avazu-ctr-prediction https: www.kaggle.com c criteo-display-ad-challenge recently. In these two competitions, only basic features are available for participants. An ensemble of four-layer DNNs which use fully-connected layers and different kinds of non-linear activations achieves better or comparable performance than LR with feature conjunction, factorization machines, decision trees, etc. Comparing with this method, our model can extract more powerful features by taking consideration of the visual features in image ads.
- In @cite , Meng simply adopted the detection algorithms from the probabilistic Bayesian framework for the sparse event detection in (WSN). In @cite , the authors applied greedy CS detection algorithms for jointly decoding the multi-user activity and data in (CDMA) systems. It is shown that an application of the CS methods can greatly reduce the length of the spreading sequences, thereby leading to a higher network throughput. However, both of the schemes directly applied the standard CS decoding algorithms for device detection at the central controller, which ignores the correlation in the behavior among the M2M devices and the additional sparsity in the activation pattern, thus leading to non-optimality in the processing especially in terms of efficiency and computational complexity.
- The authors of @cite proposed a sparse signal recovery scheme via decentralized in-network processing for event detection in WDN based on a consensus optimization formulation. However, since a random sleeping strategy is used to enforce the compressive data collection, the detection field for each active time slot is rather uncertain and may lead to incorrect detection decisions with high probability. Moreover, none of the schemes exploits the cluster-like behavior among the devices, which exhibits a certain level of correlation in the device activations.
- As mentioned earlier, the dimension of real world voting systems has been the focus for several studies. The Amendment of the Canadian constitution @cite and the US federal legislative system @cite have dimension @math . The voting systems of the Legislative Council of Hong Kong @cite and the Council of the European Union under its Treaty of Nice rules @cite have a dimension of exactly @math . Kurz and Napel @cite have established that the dimension of the voting system of the Council of the European Union under its Treaty of Lisbon rules is between @math and @math .
- There has been a great deal of work on clustering data using ensemble approaches (see @cite for a review). However, when it comes to clustering vertices in , ensemble approaches have been relatively scarce See the survey @cite for various community detection algorithms. . Dahlin and Svenson @cite were the first to propose an instance-based ensemble CD algorithm for networks which fuses different community structures into a final representation. A few methods addressed the utility of merging several community structures @cite . A new ensemble scheme called CGGC was proposed to maximize modularity @cite . Kanawati proposed YASCA, an ensemble approach to different network partitions derived from ego-centered communities computed for each selected seed @cite . He further emphasized the quality and diversity of outputs obtained from the base algorithms for ensemble selection @cite .
- A consensus clustering'' @cite approach was recently proposed which leverages a consensus matrix to produce a disjoint community structure which outperformed previous approaches. Our work differs from this approach in at least three significant ways: (i) they measure the number of times two vertices are assigned to the same community, thus ignoring the global similarity of vertices; whereas we capture the global similarity by representing the network within a feature space and grouping redundant base communities into meta communities; (ii) they either take multiple algorithms or run a particular algorithm multiple times for generating an ensemble matrix, whereas we consider both options; (iii) we are the first to show how aggregating multiple disjoint base communities can lead to discover both disjoint and overlapping community structures simultaneously. We show experimentally that beats consensus clustering.
- News and social media offer a powerful reflection of public attitudes over time. For example, by analyzing such data, it is possible to predict cultural events such as revolutions @cite @cite , or examine public opinion on same-sex marriage @cite . Here we use such data to discover and validate similar trends in the public perception of artificial intelligence.
- Finally, crowdsourcing is a powerful tool for enabling new kinds of quantitative analyses. For example, it is possible to crowdsource lexicons of words to answer novel research questions @cite , or leverage crowds to bootstrap classifiers that can then be applied to much larger corpora @cite @cite . Here we use crowds to identify themes in articles that would be difficult to analyze under fully automated approaches.
- There is work on transliteration using only monolingual phonetic mappings @cite @cite . While these mappings are less expensive than name pairs, they still require expert knowledge to create.
- We contrast our similarity metric with a measure called Weighted AVerage Entropy (WAVE) @cite . WAVE measures the frequency of alphabet ngrams weighted by the entropy of the ngram mapping. However, to estimate WAVE, one needs to have a set of name pairs in @math .
- It has always been standard practice to learn reordering models from alignments for conventional SMT either at the phrase level or word level. At the phrase level, proposed a lexicalized MSD model for phrasal reordering; proposed a feature-rich model to learn phrase reordering for BTG; and proposed a neural network method to learn a BTG reordering model. At the word level, surveyed many word reordering models learned from alignment models for SMT, and in particular there are some neural network based reordering models, such as @cite . Our work is inspired by these works in spirit, and it can be considered to be a recurrent neural network based word-level reordering model. The main difference is that in our approach the reordering model and translation model are trained jointly rather than separately as theirs.
- A covert channel can be considered as a secret channel used to exfiltrate information from a secured environment in an undetected manner. @cite investigated the existence of different covert channels that can be used to communicate between two malicious applications. They examined the common resources (such as battery) shared between two malicious applications and how they could be exploited for covert communication. Similar studies presented in @cite @cite @cite @cite exploited unknown covert channels in malicious and clean applications to leak out private information.
- As demonstrated by @cite , the adversary is further empowered as smartphones continue to have more computational power and extensive functionalities. The authors empirically showed that speech-like data can be sent over a cellular voice channel. The attack was successfully carried out with the help of a custom-built rootkit installed on Android devices. @cite , demonstrated the feasibility of covertly exfiltrating data via SMS and inaudible audio transmission, without the user's knowledge, to other mobile devices including laptops.
- Keystroke inference is another type of attacks that has been successfully demonstrated on the smartphone platform. Cai and Chen @cite used the information collected by motion sensors to deduce the different areas of vibrations on the keypad. @cite applied similar side channel techniques on smartwatches and showed that they can capture individual keystrokes using wrist movements. Additionally, the authors from @cite proposed a framework that detects and decodes keystrokes by measuring the relative physical position and distance between each vibration. Moreover, eavesdropping the network traffic of an Android device, it is possible to identify the set of apps installed on a victim's mobile device @cite @cite , and even infer the actions the victim is performing with a specific app @cite .
- Multiple projectors are also used for applications in light-field displays @cite @cite . For these applications, in order to create the large number of rays needed for the light field, each ray is projected separately for a specific viewpoint and is not intended to be mixed with other rays. This is in contrast with our proposed method, where the multiple independent images are created at the intended depths and on the intended surfaces exactly by leveraging the mixing properties of rays from the projectors.
- One of the first works on device usage logging in the literature is , targeted at the Windows Mobile platform and presented in @cite . It is a logging tool capable of collecting context data and performing actions in response to triggers configured by the experiment designer. Examples include presenting a survey to the user after a phone call has ended or when she is near a particular location. Even though targeted at an obsolete operating system, it is still relevant for historical reasons, being one of the first mobile device logging applications.
- An approach similar to is implemented by , presented by in @cite . It is an Android tool designed to present the user with interactive surveys and self-monitoring tasks depending on triggers such as time and location. This tool is also capable of automatically collecting sensor data from the device, such as accelerometer, GPS, WiFi, microphone audio recordings and cell towers logs. This data is meant to give context in order to better understand the user's behavior, activities and surroundings, and is uploaded to a central server for analysis. We did not include it in our comparison due to its limited scope when it comes to the number and variety of logged data sources. Ohmage is also a tool designed with user interaction in mind, while ours is a passive tool that does not interfere with the user's activities.
- Finally, in @cite propose a logging framework for the on-line failure analysis of Android smartphones. This consists of a framework that logs data related to app crashes and phone hangs reboots. This data is then sent to a remote server to help in analyzing and troubleshooting bugs. Collected data includes the list of running applications and services, memory usage statistics, battery status and the current network connections. Such logging framework is not included in our main survey because it only logs a few predefined data sources, all related to app debugging.
- The use of convolutional neural network (CNN) has brought huge improvement in action classification @cite . @cite finetunes the CNN pre-trained on ImageNet and shows improvement over traditional methods. @cite designs a multi-task (person-detection, pose-estimation and action classification) model based on R-CNN. @cite develops an end-to-end deep convolutional neural network that utilizes contextual information of actions. HICO @cite introduces a new benchmark for recognizing human-object interactions, which contains a diverse set of interactions with common object categories, such as hold banana" and eat pizza". Ramanathan @cite proposes a neural network framework to jointly extract the relationship between actions and uses them for training better action retrieval models. These methods all rely on fully-labelled data.
- Weakly supervised action concept learning relies on weakly-labelled data, such as video-caption stream data @cite @cite and focuses on automatically discovering and learning action concepts. @cite designs a method to automatically discover the main steps for specific tasks, such as make coffee" and change tire", from narrated instructional videos. Their method solves two clustering problems, one in text and one in video, applied one after each other and linked by joint constraints to obtain a single coherent sequence of steps in both modalities. Ramanathan @cite propose a method to learn action and role recognition models based on natural language descriptions of the training videos. Yu @cite discover Verb-Object (VO) pairs from the captions of the instructional videos and use the associated video clips as training samples. The learned classifiers are evaluated in event classification, compared with well defined action categories in HMDB51 @cite and UCF50 @cite . @cite proposes a general concept discovery method from image-sentence corpora and apply the concepts on image-sentence retrieval tasks.
- ACD @cite solves a similar problem to ours. It automatically discovers action concepts from image-sentence corpora @cite @cite , clusters them and trains classifier for each action concept cluster. However, there are two main drawbacks in this method: (1) no hierarchical clustering: once the action concepts are clustered, the detailed information are lost; (2) no vocabulary expansion: if the target test action categories are missed in the training set, ACD would fail to perform classification.
- Image captioning methods take an input image and generate a text description of the image content. Recently, methods based on convolutional neural networks and recurrent neural networks @cite @cite have shown to be an effective way on this task. VSA @cite is one of the recent successful models. It uses bidirectional recurrent neural networks over sentences, convolutional neural networks over image regions and a structured objective that aligns the two modalities through a multimodal embedding. Besides image captioning, other relevant work includes natural language object retrieval @cite or segmentation @cite , which takes an input image and a query description and outputs a corresponding object bounding box or a segmentation mask.
- Geometric methods. Multiview shape estimation methods such as binocular stereo, structure-from-motion and multiview stereo have been applied quite successfully to the problem of face shape estimation. The key problem in this context is establishing correspondence between views over apparently-featureless regions of the face such as the cheeks and forehead. One solution to this problem is to either paint @cite or project @cite a pattern onto the face that provides matchable features. An alternative passive approach is to use very high resolution images in which fine scale features such as freckles, wrinkles and skin pores are resolved. These provide ideal features for robust matching. Finally, methods such as shape-from-silhouette do not rely on feature matching and can hence be applied to faces @cite even when small features are not visible.
- The state-of-the-art approach in geometric face capture is due to Beeler al @cite . Since their method is reliant on a very accurate geometric calibration, they propose a novel calibration process based on a spherical calibration target. Shape estimation then proceeds in two steps. First a base mesh is obtained using a multiview stereo approach. Next, detail is embossed onto the mesh using a shading-based heuristic. Whilst the resulting meshes contain convincing detail, the fine scale shape detail is not accurate since it is hallucinated from a texture cue rather than satisfying any meaningful geometric or photometric constraint.
- Nehab al @cite proposed an efficient approach for combining estimated surface normals and surfaces (in the form of a depth map or mesh). Their approach is particularly applicable to surface normals estimated using photometric methods which are likely to contain low frequency bias. This low frequency bias is removed by the base mesh which in return is refined by the accurate high frequency detail present in the photometric surface normal. Their approach is based on a linear approximation to the underlying objective of minimising angular error between target normals and those of the final surface.
- Machine learning and inductive synthesis (the subfield closest to our approach) have long histories; there is a tremendous amount of work that we do not cover. We refer the reader to @cite @cite for a general perspective on inductive synthesis.
- Much of our motivation comes from recent papers using ideas from descriptive complexity in inductive synthesis. For example, given a specification in an expressive logic (second-order), @cite synthesized equivalent formulas in less expressive logics which can be evaluated more efficiently. Automatically finding complexity-theoretic reductions between computational problems was first considered by @cite . They focused on quantifier-free reductions, a weak class of reduction defined by tuples of quantifier-free formulas.
- We provide an algorithm that makes general assumptions: No strong physical model is applied, instead, we assume that a skelet al structure can be used to characterise broad appearance and skelet al matching can be used to characterise broad motion. This general idea has been used by @cite for rain cloud tracking with radar image; we differ in the way in which both construct and match skeletons. Noted that the proposed method is much simpler compare to @cite .
- There is a body of literature about building multilingual parallel corpora from movie subtitles @cite @cite . Tiedemann has put a lot of efforts in this direction and has made substantial contributions for aligning movie subtitles @cite @cite @cite . Initially, he gathered bi-texts for 59 languages in his OpenSubtitles2013 corpus, which was obtained from 308,000 subtitles files of around 18,900 movies downloaded from OpenSubtitles.org, one of the free online databases of movie subtitles.
- For alignment, Tiedemann, started with the traditional approach of length-based sentence alignment @cite using sentence boundaries tagged in an earlier stage. This is based on the idea that sentences are linguistically-driven elements and, thus, it would be more appropriate to process them using linguistic features rather than merely aligning subtitles appearing simultaneously on the screen. Nonetheless, there are many untranslated parts of the movie for reasons discussed earlier. The initial results were unsatisfactory.
- It is also worth mentioning the AMARA project https: www.amara.org en @cite , a unique, open, scalable and flexible online collaborative platforms which takes advantage of the power of crowdsourcing and encourages volunteer translation and editing of subtitles of educational videos.
- @cite used the AMARA content to generate a parallel corpus out of the translations in order to improve an SMT system for the educational domain. They aligned the multi-lingual text of the educational videos and assessed its quality with several measures in comparison with the IWSLT training set, which is very close to the AMARA domain. It is important to note that 75
- In the last decade saliency prediction has been widely studied. The seminal works by Koch and Ullman @cite and Itti al @cite introduced a biologically-plausible architecture for saliency detection that extracts multi-scale image features based on color, intensity and orientation. Later, Hou and Zange @cite proposed a method that analyzes the log spectrum of each image and obtains the spectral residual, which allows the estimation of the behavior of pre-attentive visual search. Torralba al @cite show how global contextual information can improve the prediction of observers' eye movements in real-world scenes.
- Similarly, Goferman al @cite present a technique which aims at identifying salient regions that are distinctive with respect to both their local and global surroundings. Similarly to Cerf al @cite , Judd al @cite propose an approach that combines low-level features (color, orientation and intensity) with high-level semantic information (i.e. the location of faces, cars and text) and show that this solution significantly improves the ability to predict eye fixations. In general, these approaches presented hand-tuned features or trained specific higher-level classifiers.
- A first attempt to model saliency with deep convolutional networks (DCNs) has been recently proposed by Vig al @cite . They propose Ensembles of Deep Networks (eDN), a CNN with three layers. Since the amount of data available to learn saliency is generally limited, this architecture cannot scale to outperform the current state-of-the art.
- State-of-the-art results for grasping novel objects and grasping objects in unstructured environments such as in dense clutter generally use machine learning to evaluate and choose the best among possible grasps @cite @cite @cite .
- A recent trend is making the robot learn a grasping task completely autonomously, using active learning. In order to achieve this, the robotic system must generate the feedback data autonomously. Grasps are automatically annotated as success or failure based on sensors in the gripper @cite @cite @cite or visual feedback, e.g., by comparing images of the table before and after dropping a supposedly grasped object @cite .
- Object trajectory estimation is often of interest in cases of anomaly detection; see @cite @cite @cite @cite @cite @cite @cite @cite @cite . An object shows an anomaly if it does not follow learned normal trajectories. This approach usually suffers from many weaknesses, such as disability to efficiently handle occlusions, and being too complex for processing crowded scenes.
- To avoid these two weaknesses, it is proposed to use spatio-temporal low level features such as optical flow or gradients. @cite use a (MRF) to model the normal patterns of a video with respect to a number of features, such as rarity, unexpectedness, and relevance. Boiman and Irani @cite consider an event as being abnormal if its reconstruction is impossible by using previous observations only. @cite use an exponential distribution for modeling the histograms of optical flow in local regions.
- A mixture of dynamic textures (MDT) is proposed by @cite for representing a video. In this method, the represented features fit into a Gaussian mixture model. In @cite , the MDT is extended and explained in more details. Kim and Grauman @cite exploit a mixture of probabilistic PCA (MPPCA) model for representing local optical flow patterns. They also use an MRF for learning the normal patterns.
- @cite introduce social force (SF) as an efficient technique for abnormal motion modeling of crowds. Detection of abnormal behaviors using a method based on spatial-temporal oriented energy filtering in proposed by @cite .
- @cite construct an over-complete normal basis set from normal data. A patch is considered to be abnormal if reconstructing it with this basis set is not possible.
- @cite , an extension of the bag of video words (BOV) approach is used by A context-aware anomaly detection algorithm is proposed in @cite , where the authors represent the video using motions and the context of videos. @cite , a method for modeling both motion and shape with respect to a descriptor (named motion context'') is proposed; they consider anomaly detection as a matching problem. @cite introduce a method for learning the events of a video by using the construction of a hierarchical codebook for dominant events in a video. Ullah et al @cite learn an MLP neural network using trained particles to extract the video behavior. A Gaussian mixture model (GMM) is exploited for learning the behavior of particles using extracted features. In addition, in @cite , an MLP neural network for extracting the corner features from normal training samples is proposed; authors also label the test samples using that MLP.
- Authors of @cite extract corner features and analyze them based on their properties of motion by an enthalpy model, a random forest with corner features for detecting abnormal samples. @cite propose a unified anomaly energy function based on a hierarchical activity-pattern discovery for detecting anomalies.
- Work reported in @cite @cite models normal events based on a set of representative features which are learned on auto-encoders @cite . They use a one-class classifier for detecting anomalies as being outliers compared to the target (normal) class. See also the beginning of where we briefly reviewed work reported in @cite ; this paper proposes a cascaded classifier which takes advantage of two deep neural networks for anomaly detection. Here, challenging patches are identified at first by using a small deep network; then the neighboring patches are passed into another deep network for further classification.
- @cite , the histogram of oriented tracklets (HOT) is used for video representation and anomaly detection. A new strategy for improving HOT is also introduced in this paper. @cite propose an informative structural context descriptor (SCD) to represent a crowd individually. In this work, a (spatial-temporal) SCD variation of a crowd is analyzed to localize the anomaly region.
- A hierarchical framework for local and global anomaly detection is proposed in @cite . Normal interactions are extracted by finding frequent geometric relationships between sparse interest points; authors model the normal interaction template by Gaussian process regression. @cite exploit sparse semi-nonnegative matrix factorization (SSMF) for learning the local pattern of pixels. Their method learns a probability model by using local patterns of pixels for considering both the spatial and temporal context. Their method is totally unsupervised. Anomalies are detected by the learned model.
- @cite propose an unsupervised framework for detecting the anomalies based on learning global activity patterns and local salient behavior patterns via clustering and sparse coding.
- In contrast to the computational models we have presented, dogmatism is usually measured in psychology through survey scales, in which study participants answer questions designed to reveal underlying personality attributes @cite . Over time, these surveys have been updated @cite and improved to meet standards of psychometric validity @cite .
- Other researchers have studied topics similar to dogmatism, such as signals of cognitive style in right-wing political thought @cite , the language used by trolls on social media @cite , or what makes for impartial language on twitter @cite . A similar flavor of work has examined linguistic models that capture politeness @cite , deception @cite , and authority @cite . We took inspiration from these models when constructing the feature sets in our work.
- Finally, while we examine what makes an opinion dogmatic, other work has pushed further into the structure of arguments, for example classifying their justifications @cite , or what makes an argument likely to win @cite . Our model may allow future researchers to probe these questions more deeply.
- Randomized algorithms for matrix approximation date back to research @cite @cite in theoretical computer science (TCS) in the late 1990s. Starting around 2004, this work inspired numerical analysts to develop practical algorithms for matrix approximation and related problems @cite . See the paper [Sec. 2] HMT11:Finding-Structure for a comprehensive historical discussion. The surveys @cite @cite provide more details about the development of these ideas within the TCS literature.
- The two stream architecture @cite proposed by extracts the temporal data by training a CNN alexnet network on the optical flow which computed between consecutive frames. Many works offer to extend the proposed idea of two stream network by different views. @cite tried to improve the result by using deeper networks. @cite proposed an action detection method based on the two stream network. @cite extended the two stream network by implementing different fusion methods in different layers instead of the late fusion in the score layer of the two stream network of @cite . As they claimed in their work, in contrast with most of the works, their results got state of the art without combining with IDT @cite method.
- @cite handled the temporal information by using a long short term memory on the extracted features of frames. Duo to not having an end-to-end network, the results have not improve as much as expected.
- The handcrafted features of improved trajectory proposed by @cite have considerable results as well as having the power of improving convolutional neural network based results in combination with them. Hence most of the proposed works got advantage of this power by concatenating the IDT feature with the proposed feature. @cite use the power of IDT by extracting convolutional neural network based features locally around the trajectory and then encode the local features by fisher vector encoding.
- The core idea behind the aforementioned methods is to utilize kernels that naturally correspond to each single view and integrate kernels either linearly or non-linearly to get a final grouping output @cite @cite . Tzortzis @cite proposed computing separate kernels on each view and then combined with a kernel-based method to improve clustering. A matrix factorization based method is presented to group the clusters obtained from each view @cite , which is termed as subspace learning based methods @cite @cite @cite @cite @cite . Based on the assumption that each input view is generated from a latent subspace, it focuses on achieving this latent subspace shared by multiple views. Recent works @cite @cite @cite show that some useful prior knowledge, such as sparse or low-rank information, can help capture the latent group structure to improve clustering performance.
- The other applicable existing work is protocol reverse engineering that the process of the inferring the specification of an unknown network protocol @cite . These techniques leverage approach for automatically extracting information of unknown target protocol. Even though they contribute to analysis of unknown network environment and helpful to provide major SCADA protocols, it is difficult to cover for analysis of all our CIs in the field. The main difficulty of this problem is closed SCADA information that cannot be provided in public. These techniques based on assumption that we require enough samples from target environment or classification from other distinct types of protocols. Understandably, our CI network information is not public and some major vendors do not public their SCADA protocol specification and information. Even though, these techniques help to analysis several famous SCADA protocol, but this approach is extremely expensive cost to cover all of environment. As SCADA market grows, our CI environment becomes heterogeneous for SCADA device and network protocol. Considering the possibility that customized protocol from standard also can make different result from identified signatures, our method can be applied complementary for analysis of CI that cannot be covered by protocol reversing efforts.
- Automatic description of images was developed by several groups @cite @cite @cite @cite @cite @cite @cite @cite @cite , and was also applied to parts of images @cite @cite .
- Compositional aspects of language and images have been recently explored by @cite , who approached a visual QA task by breaking questions into substructures, and re-using modular networks. @cite combined subjects, objects and relationships in a graph structure for image retrieval. @cite learned spatial relations for generating descriptions based on a template. @cite modelled synthetic scenes generated using CRF. The dataset of @cite has combinations of entities modelled with CRF. @cite developed ways to match sentences and images, through a space of meaning parametrized by subject-verb-object triplets which our structured model is closely related to. Very recently, @cite trained a model that leverages language priors from semantic embeddings to predict subject-relation-object tuples. The performance of their model on the unseen-compositions subset in their test set, exhibits a very large generalization gap. Finally, generalization to new objects has often been achieved by smearing'' to semantically-related entities @cite @cite @cite , but this is outside the scope of this paper.
- In IEEE VLC standard @cite @cite , either a Reed-Solomon (RS) code or a concatenated code composed of RS and convolutional codes (CC) is directly used as the error-correcting code (ECC) for short data frames. No modification of FEC codes is made in the scheme as it uses CSs for dimming function and RLL line codes for flicker mitigation. In recent years, several improved coding schemes have been proposed @cite @cite @cite @cite @cite @cite @cite .
- Two coding schemes based on modified Reed-Muller (RM) codes and minimal use of CSs have been proposed by Kim and Jung for dimming support in VLC systems @cite @cite . Although the RM codes-based schemes can guarantee the DC balance at exact 50 To overcome the limitation of RM codes-based scheme, two kinds of coding schemes using advanced error correction codes were proposed. The first one is a turbo codes-based coding scheme proposed by Lee and Kwon @cite . However, it requires code puncturing, pseudo-noise sequence scrambling and CSs inserting to avoid long runs of 1s or 0s and to match the codewords' weight with the desired dimming ratio, introducing high coding overhead. Besides, for the dimming ratios greater than @math , all the binary symbols of the codewords need to be flipped to obtain those dimming ratios. Otherwise, the puncturing rate will be too high and make the information in a message indistinguishable.
- The other one is an LDPC codes-based scheme proposed by Kim to adaptively adjust dimming values in VLC systems @cite . As shown in Figure , its dimming function is also provided via puncturing methods and CSs, and RLL codes are used to avoid long runs. The significant advantages of the method include the lower number of codes required to support codes with different coding rate and the relatively small performance degradation for dimming control. Nevertheless, with the rate-1 2 LDPC codes, the effect of the puncturing rate and the rate-1 2 Manchester line codes, the overall code rate could be very low. For example, a 3-bit message will be encoded into a 15-bit codeword using the LDPC-based scheme. Furthermore, these two schemes require iterative decoding algorithm, which increases the computational cost and latency.
- The similar problems also exist in the joint FEC-RLL coding mechanism proposed by Lu and Li trying to simultaneously achieve bandwidth efficiency, error correction and run-length control @cite . The concatenated convolutional-Miller coding scheme consists of an OOK modulated Miller code serving as the inner code and a convolutional code serving as the outer code. Unfortunately, Miller codes have disappointing power efficiency and bit error rate (BER) performance, which cannot be ignored in practice.
- A fountain codes-based encoding scheme was also proposed by Feng et. al. @cite to improve transmission efficiency and used the least CSs. However, the coding scheme requires a feedback link, which may not be available in VLC broadcasting scenario and introduces constraint to the whole system. Beside, scrambling is also needed.
- With regard to the extensions of HMM, Xue proposed transition-emitting HMMs (TE-HMMs) and state-emitting HMMs (SE-HMMs) to treat the discontinuous symbol @cite , of which application is an off-line handwriting word recognition. The observation data contain discontinuous and continuous symbols between characters when writing in cursive letters. They specifically examined such discontinuous features and continuous features, and extended HMM to treat both of them. Bengio focused on mapping input sequences to the output sequences @cite . The proposed model supports a recurrent networks processing style and describes an extended architecture under the supervised learning paradigm. Salzenstein dealt with a statistical model based on Fuzzy Markov random chains for image segmentations in the context of stationary and non-stationary data @cite . They specifically examined the observation in a non-stationary context, and proposed a model and a method to estimate model parameters . Yu proposed the explicit-duration hidden Markov model @cite @cite @cite @cite . They emphasized the interval between state transition, and proposed a new forward-backward algorithm to estimate the model parameters.
- Xia @cite proposed a Lagrangian smoothing algorithm for QAP which solves a sequence of @math regularization subproblem X , (A^ X B X^ ) + |X |_ ^2 with dynamically decreasing parameters @math The initial value @math in subproblem was chosen such that the problem is convex and the subproblem with fixed @math was approximately solved by the Frank-Wolfe algorithm up to fixed number of iterations (it was called the truncated Frank-Wolfe method in @cite ). Note that Bazaraa and Sherali @cite first considered regularization problem where @math is chosen such that the problem is strongly concave, but they only solved once. In addition, Huang @cite used the quartic term @math to construct the regularization problem, where @math is the Hadamard product and @math is the matrix of all ones.
- One special case of QAP (and thus problem ) is the graph matching problem: which has wide applications in pattern recognition, computer vision, etc @cite . Note that the objective function in is always convex in @math and can be rewritten as @math , which is no longer convex in @math . By constructing a convex and a concave quadratic optimization problem over doubly stochastic matrices, @cite @cite considered path following algorithms which solve a sequence of convex combinations of the convex and concave optimization problems, wherein the combination parameters were chosen such that the resulting problems change from convex to concave. Moreover, @cite observed that initialization with a convex problem can improve the practical performance of the regularization algorithms.
- Recently, Fogel @cite used QAP to solve the seriation and 2-SUM problems. QAP considered therein takes the following special structure: where @math is binary symmetric, @math , and @math . Here, @math represents a diagonal matrix whose diagonal elements are @math . They used the regularization term @math with @math , where @math denotes the identity matrix of size @math , to construct a convex relaxation problem over @math for . Using a recent result of Goemans @cite , Lim and Wright @cite constructed a new convex relaxation problem over a convex hull of the permutation vectors for . In their problem, the number of variables and constraints reduces to @math in theory and @math in practice. The advantage of this new formulation was illustrated by numerical experiments. However, as pointed in @cite , it is still unclear how to extend their new formulation to the more general QAP .
- The aforementioned regularization or relaxation methods all considered solving optimization over @math . By using the fact that @math , Wen and Yin @cite reformulated QAP as X ^ n n , (A^ (X X) B (X X)^ ) X^ X = I_n, X 0 and proposed an efficient algorithm by applying the augmented Lagrangian method to handle the constraint @math . In their algorithm, a sequence of augmented Lagrangian subproblems with orthogonality constraint is solved inexactly.
- Our system is an evolutionary improvement over the LSTM seq2seq system of and as such, it is most related in terms of architecture to other recent RNN-based approaches to NLG, which are not context-aware: RNN generation with a convolutional reranker by and an improved LSTM-based version @cite , as well as the LSTM encoder-aligner-decoder NLG system of . The recent end-to-end trainable SDS of does have an implicit access to previous context, but the authors do not focus on its influence on the generated responses.
- There have been several attempts at modelling entrainment in dialogue @cite @cite @cite and even successful implementations of entrainment models in NLG systems for SDS, where entrainment caused an increase in perceived naturalness of the system responses @cite or increased naturalness and task success (, lopes_automated_2013 ; lopes_rule-based_2015 ). However, all of the previous approaches are completely or partially rule-based. Most of them attempt to model entrainment explicitly, focus on specific entrainment phenomena only, and or require manually selected lists of variant expressions, while our system learns synonyms and entrainment rules implicitly from the corpus. A direct comparison with previous entrainment-capable NLG systems for SDS is not possible in our stand-alone setting since their rules involve the history of the whole dialogue whereas we focus on the preceding utterance in our experiments.
- Then it is natural to test such features for the NBI image recognition task. Thanks to the publicly available pre-trained CNN models, we use Caffe @cite to extract CNN features Our contribution is to show the comparison of performances between CNN features from different layers from different CNN models, in contrast to existing work @cite @cite @cite that uses only few layer features. Discussions based on obtained results is another contribution of this paper in order to design and propose a better CNN model for this kind of specific task: NBI colorectal polyp image recognition.
- On the theoretical side, when samples consist of pairwise comparisons, @cite first established consistency and asymptotic normality of the maximum likelihood estimate when all teams play against each other. For a broader class of scenarios where we allow for sparse observations, where the number of total comparisons grow linearly in the number of teams, @cite show that Rank Centrality achieves optimal sample complexity by comparing it to a lower bound on the minimax rate. For a more general class of traditional observations, including pairwise comparisons, @cite provide similar optimal guarantee for the maximum likelihood estimator. @cite introduced Spectral MLE that applies Rank Centrality followed by MLE, and showed that the resulting estimate is optimal in @math error as well as the previously analyzed @math error. @cite study a new measure of the error induced by the Laplacian of the comparisons graph and prove a sharper upper and lower bounds that match up to a constant factor.
- However, in modern applications, the computational complexity of the existing approaches blow-up due to the heterogeneity of modern datasets. Although, statistical and computational tradeoffs have been investigated under other popular choice models such as the Mallows models by @cite or stochastically transitive models by @cite , the algorithmic solutions do not apply to random utility models and the analysis techniques do not extend. We provide a novel rank-breaking algorithms and provide finite sample complexity analysis under the PL model. This approach readily generalizes to some RUMs such as the flipped Gumbel distribution. However, it is also known from @cite , that for general RUMs there is no consistent rank-breaking, and the proposed approach does not generalize.
- The cumulative response of readers to social media and online content has been studied using a variety of measurements, including: the volume of comments in response to blog posts @cite and news articles @cite @cite , the number of Twitter shares of news articles @cite , the number of reshares on Facebook @cite and retweets on Twitter @cite @cite @cite @cite , and the difference in the number of reader up and down votes on posts and comments in Reddit discussion forums @cite @cite . An advantage of working with the Reddit data is that both positive and negative reactions are accounted for, so the total (karma in Reddit) is a reasonable proxy for community endorsement.
- For all the different types of measures, a challenge in predicting the cumulative reaction is that the cases of most interest are at the tails of a Zipfian distribution. Various prediction tasks have been proposed with this in mind, including regression on a log score @cite , classification into 3-4 groups (e.g. none, low, high) @cite @cite @cite , a binary decision as to whether the score will double given a current score @cite , and relative ranking of comments @cite @cite . In our work, we take the approach of classification, but use a finer grain quantization with bins automatically determined by the score distribution.
- Other work has been done to map assignments, student ability, lesson gains and pre-requisites all onto the same dimensional embedding space, where dimensions represent latent skills @cite . This mapping enables the model to produce potential lesson and assignment pathways so that students can have an optimal path for learning. The work in this paper differs by attempting to utilize all sources of student actions, such as forum posts or specific video viewings, and as such requires the use of a different approach.
- To date, most existing work on automated analysis focuses on the detection of AU activations @cite @cite @cite @cite . The problem of AU intensity estimation is relatively new in the field. Most of the research in this area focuses on independent modeling of AU intensities @cite @cite @cite @cite @cite @cite @cite @cite @cite . Only recently, joint estimation of the intensity levels has been addressed @cite @cite @cite @cite @cite . This is motivated by the fact that intensity annotations are difficult to obtain (due to the tedious process of manually coding) and that AU levels are highly imbalanced. Thus, by imposing the structure on the output in terms of AU co-occurrences robust intensity estimation is expected.
- Improved dense trajectories @cite dominated the field of video analysis for several years until the two-stream ConvNets architecture introduced by Simonyan and Zisserman @cite achieved competitive results for action recognition in video. In addition, motivated by the great success of applying deep ConvNets in image analysis, researchers have adapted deep architectures to the video domain either for feature representation @cite @cite @cite @cite @cite or end-to-end prediction @cite @cite @cite @cite .
- There is previous work on action recognition in RGB-D data. @cite use depth motion maps (DMM) for real-time human action recognition. Yang and Tian @cite cluster hypersurface normals in depth sequences to form a super normal vector (SNV) representation. Very recently, @cite apply weighted hierarchical DMM and deep ConvNets to achieve state-of-the-art performance on several benchmarks. Our work is different from approaches that use RGB-D data in several key ways:
- (i) : These methods use depth information obtained from depth sensors. Besides limiting their applicability, this results in depth sequences that have much higher fidelity than those which can be estimated from RGB video. Our estimated depth sequences are too noisy for recognition techniques designed for depth-sensor data. Taking the difference between consecutive frames in our depth sequences only amplifies this noise making techniques such as STOP features @cite , SNV representations @cite , and DMM-based framework @cite @cite , for example, ineffective.
- (ii) : RGB-D benchmarks such as MSRAction3D @cite , MSRDailyActivity3D @cite , MSRGesture3D @cite , MSROnlineAction3D @cite and MSRActionPairs3D @cite are much more limited in terms of the diversity of action classes and the number of samples. Further, the videos often come with other meta data like skeleton joint positions. In contrast, our benchmarks such as UCF101 contain large numbers of action classes and the videos are less constrained. Recognition is made more difficult by the large intra-class variation.
- We note that we take inspiration from @cite @cite in designing our modified DMMs. The approaches in these works use RGB-D data and are not appropriate for our problem, though, since they construct multiple depth sequences using different geometric projections, and our videos are too long and our estimated depth sequences too noisy to be characterized by a single DMM.
- Security and the general deployment of AWNs based on wireless-as-a-comm-link have been analyzed in @cite , which discusses the security and trust challenges faced by AWNs. Beside this, a crucial component of the security of aircraft devices is the trusted boot process discussed in @cite . The security, trust and assurance issues related to bringing a user device into an aircraft network are evaluated in @cite .
- Though applied in many studies @cite @cite @cite @cite @cite , lemmatization has not been directly explored in the context of topic modeling. An infinite-vocabulary LDA model containing a prior on words similar to an @math -gram model has been developed @cite ; this prior could be viewed as loosely encoding beliefs of a concatenative morphology, but its effect was not analyzed in isolation.
- While not satisfactorily explored in the topic modeling community, morphology has been actively investigated in the context of word-embeddings. The latent topic vectors that topic models discover have many parallels to continuous embeddings---both are real-valued representations that stand proxy for (some notion of) lexical semantic information. Most notably, BianGL14 learned embeddings for individual morphemes jointly within the standard word2vec model @cite and SoricutO15 used the embeddings themselves to induce morphological analyzers. Character-level embedding approaches have also been explored with the express aim of capturing morphology @cite @cite .
- provides concurrent execution of multiple applications on a single device. @cite , the authors propose and make the case for a GPU multitasking technique called . The experimental results show that the proposed spatial multitasking can obtain a higher performance over cooperative multitasking. @cite , investigate the concurrent kernel execution mechanism that enables multiple small kernels to run concurrently on the Kepler GPUs. Also, the authors evaluate the Xeon Phi offload models with multi-threaded and multi-process host applications with concurrent coprocessor offloading @cite . Both multitasking and multiple streams share the idea of spatial resource sharing. Different from multi-tasking, using multiple streams needs to partition the workload of a single application (rather than multiple applications) into many tasks.
- There is a large body of workload partitioning techniques, which intelligently partition the workload between a CPU and a coprocessor at the level of algorithm @cite @cite or during program execution @cite @cite . Partitioning workloads aims to use unique architectural strength of processing units and improve resource utilization @cite . In this work, we focus on how to efficiently utilize the coprocessing device with multiple streams. Ultimately, we need to leverage both workload partitioning and multiple streams to minimize the end-to-end execution time.
- @cite , Gomez- present performance models for asynchronous data transfers on different GPU architectures. The models permit programmers to estimate the optimal number of streams in which the computation on the GPU should be broken up. @cite , present an analytical performance model to indicate when to apply which overlapping method on GPUs. The evaluation results show that the performance model are capable of correctly classifying the relative performance of the different implementations. @cite , carry out a systematic investigation into task partitioning to achieve maximum performance gain for AMD and NVIDIA GPUs. Unlike these works, we aim to evaluate the necessity of using multiple streams and investigate how to use streams systematically. Using a model on Phi to determine the number of streams will be investigated as our future work.
- have developed a framework for predicting the performance of applications executing on accelerators @cite . Using automatically extracted application signatures and a machine profile based on benchmarks, they aim to predict the application running time before the application is ported. Evaluating offloading necessity is a former step of applying multiple streams. In this work, we evaluate the necessity of using multiple streams with a statistical approach.
- Existing algorithms can be classified into deblocking oriented and restoration oriented methods. The deblocking oriented methods focus on removing blocking and ringing artifacts. In the spatial domain, different kinds of filters @cite @cite @cite have been proposed to adaptively deal with blocking artifacts in specific regions ( edge, texture, and smooth regions). In the frequency domain, Liew al @cite utilize wavelet transform and derive thresholds at different wavelet scales for denoising. The most successful deblocking oriented method is perhaps the Pointwise Shape-Adaptive DCT (SA-DCT) @cite , which is widely acknowledged as the state-of-the-art approach @cite @cite . However, as most deblocking oriented methods, SA-DCT could not reproduce sharp edges, and tend to overly smooth texture regions.
- The restoration oriented methods regard the compression operation as distortion and aim to reduce such distortion. These methods include projection on convex sets based method (POCS) @cite , solving an MAP problem (FoE) @cite , sparse-coding-based method @cite , semi-local Gassian process model @cite , the Regression Tree Fields based method (RTF) @cite and adjusted anchored neighborhood regression (A+) @cite . The RTF takes the results of SA-DCT @cite as bases and produces globally consistent image reconstructions with a regression tree field model. It could also be optimized for any differentiable loss functions ( SSIM), but often at the cost of performing sub-optimally on other evaluation metrics. As a recent method for image super-resolution @cite , A+ @cite has also been successfully applied for compression artifacts reduction. In their method, the input image is decomposed into overlapping patches and sparsely represented by a dictionary of anchoring points. Then the uncompressed patches are predicted by multiplying with the corresponding linear regressors. They obtain impressive results on JPEG 2000 image, but have not tested on other compression schemes.
- Super-Resolution Convolutional Neural Network (SRCNN) @cite is closely related to our work. In the study, independent steps in the sparse-coding-based method are formulated as different convolutional layers and optimized in a unified network. It shows the potential of deep model in low-level vision problems like super-resolution. However, the problem of compression is different from super-resolution in that the former consists of different kinds of artifacts. Designing a deep model for compression restoration requires a deep understanding into the different artifacts. We show that directly applying the SRCNN architecture for compression restoration will result in undesired noisy patterns in the reconstructed image.
- Transfer learning in deep neural networks becomes popular since the success of deep learning in image classification @cite . The features learned from the ImageNet show good generalization ability @cite and become a powerful tool for several high-level vision problems, such as Pascal VOC image classification @cite and object detection @cite @cite . Yosinski al @cite have also tried to quantify the degree to which a particular layer is general or specific. Overall, transfer learning has been systematically investigated in high-level vision problems, but not in low-level vision tasks. In this study, we explore several transfer settings on compression artifacts reduction and show the effectiveness of transfer learning in low-level vision problems.
- A fundamental technique to encode @math -complete problems as disjunctive programs is known as . The technique goes back to the @math -completeness proof for the existence of stable models in the case of disjunctive programs @cite . Although saturation can be applied in a very systematic fashion to some programs of interest, identify the impossibility of having negation as a central limitation of oracles encoded by saturation, rendering the oracle call to a bare minimality check rather than showing that an oracle program has no stable models. This limitation can be partially circumvented using @cite @cite , but these techniques do not necessarily decrease the of disjunctive programming from the user's perspective.
- In this section we formally define the underlying eavesdropper model and the associated secrecy capacity. We then present a brief description two key components of our secure coding schemes: 1) Gabidulin precoding scheme and 2) a code construction from @cite . We conclude the section with a discussion on the prior work in the area of designing secure coding schemes for distributed storage systems.
- We consider a DSS with @math storage nodes where each node stores @math symbols over a finite field @math . We assume that the DSS employs a coding scheme such that content of any @math out of @math nodes in the system is sufficient to construct the content stored in the remaining @math nodes. Furthermore, we assume that the content of every node in the system can be exactly reconstructed by contacting any @math out of @math remaining nodes and downloading @math symbols (over @math ) from each of the contacted nodes. It follows from the Singleton bound that such a system can store a file with at most @math (independent) symbols (over @math ). In fact, an MDS coding scheme does store a file @math of size @math symbols (over @math ). For such coding schemes, it follows from the work of @cite that Here, we focus on the DSS employing those exact-repairable coding schemes that are both storage and repair-bandwidth efficient, i.e., we have that
- We consider the @math -eavesdropper model introduced in @cite . Let @math nodes in the DSS are indexed by the set @math . For @math and @math such that @math , an @math -eavesdropper can directly access the content stored on any @math storage nodes indexed by the set @math . Additionally, the eavesdropper observes the data downloaded during the repair of any @math storage nodes indexed by the set @math . The nodes indexed by the sets @math and @math are referred to as storage-eavesdropped and download-eavesdropped nodes, respectively. Note that a download-eavesdropped node may reveal more information compared to a storage-eavesdropped node as the content stored on a node is a function of the data downloaded during its repair. In this document we focus on coding schemes that are information theoretically secure against an @math -eavesdropper. We formalize this notion in the following definition.
- Using external lexicon is first described in Pi-Chuan (2008) @cite . (2012) @cite find the lexicon features are also very helpful for domain adaptation of WS models, (2009) @cite first propose the simple yet effective guild-feature based method, which is further extended in @cite @cite @cite . (2013) @cite propose a model that performs heterogeneous Chinese word segmentation and POS tagging and produces two sets of results following @math and @math styles respectively. Their model is based on linear perceptron, and uses approximate inference. (2015) @cite first propose the coupled sequence labeling approach. , (2015) @cite make extensive use of the coupled approach in participating the NLPCC 2015 shared task of WS &POS for Webo texts. , (2016) @cite further improves the coupled approach in terms of efficiency via context-aware pruning, and first apply the coupled approach to the joint WS &POS task. In this work, we directly use the coupled model built in for converting the WS &POS annotations in PD into the style of CTB .
- In addition to using document coherence for improving IR, the reverse has also been reported, namely using IR to improve coherence modelling @cite . The idea here is to link entities that have different lexical form but are semantically related (e.g. Gates and Microsoft ), by retrieving mentions of those entities from multiple web sources and mining their relations. This approach gives good performance. Interestingly, when mining such relations between entities from web data, the task of characterising the type of these relations has also been addressed using graph representations and has been modelled as an IR, and specifically learning to rank, problem @cite .
- To our knowledge, the current state of the art in coherence modelling in terms of accuracy is the deep learning approach of @cite , where a recursive neural network learns sentential compositionality and is then used to model document coherence. This approach is supervised and computationally much heavier than graph-based coherence modelling.
- Other existing studies do not consider piece-wise joins, but rely on high density of sensors to guarantee that every motion is within the detection range of multiple sensors @cite , @cite , @cite . These studies also assume implicitly that the source signal strength at unit distance and no shielding is known.
- The work of @cite (see also @cite @cite @cite @cite ) sought to maximize the size of a matching subject to @math -matching constraints; this was motivated by applications to online dating and kidney exchange. More generally, see, e.g. @cite @cite , for pointers to other work on kidney exchange problems. The work of @cite abstracted out the general problem of maximizing a function (in their case, the rank function of the intersection of matroids or knapsacks) subject to probing constraints (again, intersection of matroids and knapsacks). This was improved and generalized by Adamczyk, et al @cite to submodular objectives. All these results use LP relaxations, or non-linear geometric relaxations for the submodular settings.
- The previous work of the authors @cite gave results for the case where @math was the rank function of matroids (or their intersections). That work bounded the adaptivity gap by logarithmic factors, and gave better results for special cases like uniform and partition matroids. This work both improves the quantitative bounds (down to small constants), generalizes it to all submodular functions with the hope of getting to all subadditive functions, and arguably also makes the proof simpler.
- Action recognition has been extensively studied in past few years @cite @cite @cite @cite @cite . Previous works related to ours fall into two categories: (1) convolutional networks for action recognition, (2) temporal structure modeling.
- Several works have been trying to design effective ConvNet architectures for action recognition in videos @cite @cite @cite @cite @cite . Karpathy @cite tested ConvNets with deep structures on a large dataset (Sports-1M). Simonyan @cite designed two-stream ConvNets containing spatial and temporal net by exploiting ImageNet dataset for pre-training and calculating optical flow to explicitly capture motion information. Tran @cite explored 3D ConvNets @cite on the realistic and large-scale video datasets, where they tried to learn both appearance and motion features with 3D convolution operations. Sun @cite proposed a factorized spatio-temporal ConvNets and exploited different ways to decompose 3D convolutional kernels. Recently, several works focused on modeling long-range temporal structure with ConvNets @cite @cite @cite . However, these methods directly operated on a longer continuous video streams. Limited by computational cost these methods usually process sequences of fixed lengths ranging from 64 to 120 frames. It is non-trivial for these methods to learn from entire video due to their limited temporal coverage. Our method differs from these end-to-end deep ConvNets by its novel adoption of a sparse temporal sampling strategy, which enables efficient learning using the entire videos without the limitation of sequence length.
- Many research works have been devoted to modeling the temporal structure for action recognition @cite @cite @cite @cite @cite @cite . Gaidon @cite annotated each atomic action for each video and proposed Actom Sequence Model (ASM) for action detection. Niebles @cite proposed to use latent variables to model the temporal decomposition of complex actions, and resorted to the Latent SVM @cite to learn the model parameters in an iterative approach. Wang @cite and Pirsiavash @cite extended the temporal decomposition of complex action into a hierarchical manner using Latent Hierarchical Model (LHM) and Segmental Grammar Model (SGM), respectively. Wang @cite designed a sequential skeleton model (SSM) to capture the relations among dynamic-poselets, and performed spatio-temporal action detection. Fernando @cite modeled the temporal evolution of BoVW representations for action recognition. These methods, however, remain unable to assemble an end-to-end learning scheme for modeling the temporal structure. The proposed temporal segment network, while also emphasizing this principle, is the first framework for end-to-end temporal structure modeling on the entire videos.
- The connection between urban perception and human activity speaks primarily to two streams of literature. The first one is the stream of literature focused on the environmental factors contributing to crime, which has a long tradition in criminology and urban sociology. While our paper does not focus on crime per se, the connection between the physical appearance of neighborhoods and natural surveillance suggested by Jacobs and Newman makes our results relevant to that stream of literature @cite @cite @cite . The second one is the stream of literature using surveys, and more recently, computer vision methods to quantify peoples perception of urban environments @cite @cite @cite .
- Evidence in favor of the broken windows theory has been presented by Kelling and Coles @cite , who looked at data and stories from New York to Seattle to argue that community policing is an effective way to deter more serious forms of crime. Kelling and Sousa @cite provide additional evidence by using an extensive dataset on crime, demographics, and economic data from New York City. More recently, Corman and Mocan @cite used New York City data on the policing of misdemeanors (as a proxy for broken windows policing), and on robbery, car theft, and grand larceny, to provide evidence in support of broken windows policing.
- The broken windows theory is also supported by a few field experiments, such as those conducted by Keizer in The Netherlands @cite . In six experiments, Keizer intervened environments by spraying graffiti on walls, or leaving supermarket carts unattended and studied the behavior of subjects, in both the presence and absence of disorder, to see when people broke norms (such as littering). Their data showed a significant increase in peoples norm breaking behavior when they were in the presence of disorder.
- Moreover, the BWT has been criticized by work showing that the social and ethnic context of a neighborhood may matter more than urban disorder. In Sampson @cite and Sampson and Raudenbush @cite @cite , community data from Chicago was used to argue that racial and economic context were more predictive of disorderly behavior than physical disorder. To help bridge their results with the literature, Sampson and Raudenbush @cite proposed an alternative interpretation of the theory, where both neighborhood disorder and crime, are manifestations of a lack of informal forms of control within disengaged and distrusting communities. The reframed theory, therefore, interprets the link between disorder and crime as a manifestation of the lack of informal forms of social control and organization.
- More recently, however, this literature started leveraging crowdsourcing @cite and computer vision methods @cite @cite to improve the scale, precision, and resolution of the evaluative maps created.
- On the data collection side of this literature, Salesses @cite created a large crowdsourced visual perception survey to measure people's perception of streetscapes, and to create comparable evaluative maps for New York, Boston, Linz, and Salzburg, which they also used to measure the segregation and inequality of experiences in these cities, and to show that violent crime correlates with the variance of appearence of safety in an area.
- But to scale the study of urban perception to multiple cities, and to high spatial resolutions, researchers begun developing computer vision methods to score millions of images @cite . These computer vision approaches build on research predicting human perception from visual data @cite @cite @cite and on research analyzing visual streetscapes for city understanding @cite @cite @cite @cite @cite @cite . The latter of these two lines of research has been fueled by the widespread availability of geolocated image data, such as Google Street View maps or city snapshots publicly shared on social networks (e.g. Flickr, Instagram) @cite @cite @cite . Using geotagged image data Doersch @cite showed that geographically representative visual elements, like architecture styles, can be automatically discovered from Street View Imagery. In a dynamic study, Naik @cite used computer vision and images from different time periods to measure urban change, and to study the factors that contribute to neighborhood improvement. Computer vision methods have also been used to show that a city's visual attributes work as proxy of social and economic characteristics, such as crime rates and proximity to local businesses @cite @cite , or census characteristics, such as income and inequality @cite .
- This new wave of research has benefited from advances in deep learning, which have been used not only to measure appearance, but also for place recognition. Zhou @cite introduced Places205 Dataset, a large data collection gathering more than 7 million labeled pictures of scenes. They achieve state-of-the-art classification results by training deep Convolutional Neural Networks (CNNs). More recently, Arandjelovi 'c @cite introduce NetVLAD, a modified CNN architecture able to address large scale visual place recognition. In this work, we build on top of recent work in place recognition @cite @cite to fine-tune a deep CNN architecture and show experimentally that under scarce training data, sample augmenting helps achieve state-of-the-art results on safety prediction from streescape images.
- Interestingly, the lower bound on the repair bandwidth of an MDS vector code given in continues to hold even when the contacted nodes contribute different number of symbols during the repair process @cite , i.e., for every @math , we have
- The code construction based on Hadamard matrices from @cite requires @math and @math . In @cite , propose the zigzag code construction for @math and every value of @math . This construction enables exact repair of only @math (systematic) code blocks. @cite generalize the zigzag code construction to enable exact repair of all code symbols. The constructions presented in @cite @cite @cite work with the sub-packetization level @math which is exponential in @math . For @math and all values of @math , @cite construct exact-repairable MDS vector codes that have optimal repair bandwidth and work with the sub-packetization level @math . Note that for @math , this construction provides the codes with the sub-packetization level which is polynomial in @math . The construction with @math and the similar sub-packetization levels that enable exact-repair of only @math systematic nodes are also presented in @cite @cite . The construction from @cite is generalized to work for all possible values of @math with the sub-packetization level @math in @cite .
- While regularized optical flow estimation goes back to Horn and Schunck @cite , randomized patch matching @cite is a relatively new field, first successfully applied in approximate nearest neighbor estimation where the data term is well-defined. The success in optical flow estimation (where the data term is not well-defined) started with publications like @cite @cite . One of the most recent works is Flow Fields @cite , which showed that with proper multi-scale patch matching, top performing optical flow results can be achieved.
- Regarding patch or descriptor matching with learned data terms, there exists a fair amount of literature @cite @cite @cite @cite . These approaches treat matching at an abstract level and do not present a pipeline to solve a problem like optical flow estimation or 3D reconstruction, although many of them use 3D reconstruction datasets for evaluation. Zagoruyko and Komodakis @cite compared different architectures to compare patches. Simo- @cite used the Siamese architecture @cite with @math distance. They argued that it is the most useful one for practical applications. Recently, several successful CNN based approaches for stereo matching appeared @cite @cite @cite . However, so far there are still few approaches that successfully use learning to compute optical flow. Worth mentioning is FlowNet @cite . They tried to solve the optical flow problem as a whole with CNNs, having the images as CNN input and the optical flow as output. While the results are good regarding runtime, they are still not state-of-the-art quality. Also, the network is tailored for a specific image resolution and to our knowledge training for large images of several megapixel is still beyond todays computational capacity.
- A first approach using patch matching with CNN based features is PatchBatch @cite . They managed to obtain state-of-the-art results on the KITTI dataset @cite , due to pixel-wise batch normalization and a loss that includes batch statistics. However, pixel-wise batch normalization is computationally expensive at test time. Furthermore, even with pixel-wise normalization their approach trails heuristic approaches on MPI-Sintel @cite . A recent approach is DeepDiscreteFlow @cite which uses DiscreteFlow @cite as basis instead of patch matching. Despite using recently invented dilated convolutions @cite (we do not use them, yet) they also trail the original DiscreteFlow approach on some datasets.
- Video annotation applications Video understanding is important for many applications ranging from behavior studies @cite to surveillance @cite to autonomous driving @cite . Large-scale annotated computer vision video datasets @cite @cite @cite @cite @cite enable the development of algorithms that are able to automatically process video collections. However, the lack of large-scale multi-label video datasets makes it difficult to study the intricate interactions between objects and actions in the videos rather than focusing on recognition of one or a handful of concepts.
- Efficient video annotation Video annotation is very time-consuming. Determining the absence of a concept in an image takes on the order of seconds; in contrast, determining the absence of a concept in a video takes time proportional to the length of the video. Efforts such as @cite @cite @cite exploit temporal redundancy between frames to present cost-effective video annotation frameworks. The approaches of @cite @cite @cite and others additionally incorporate active learning, where the annotation interfaces learns to query frames that, if annotated, would produce the largest expected change in the estimated object track. However, these methods combine human annotation with automatic computer vision techniques, which causes several problems: (1) these techniques are difficult to apply to challenging tasks such as activity recognition where computer vision models lag far behind human ability; (2) these methods are difficult to apply to scenarios where very short or rare events, such as shoplifting, may be the most crucial, and (3) the resulting hybrid annotations provide unfair testbeds for new algorithms.
- Glance @cite focuses on parallelizing video annotation effort and getting an answer to a single question in real-time. Our work can be effectively combined with theirs: they parallelize annotation in 30-second video chunks, while we explore the most effective ways to obtain multiple labels simultaneously for every 30-second video.
- There are two recent large-scale video annotation efforts that successfully utilize crowdsourcing. The first effort is ActivityNet @cite which uses a proposal verification framework similar to that of ImageNet @cite . They define a target set of actions, query video search engines for proposal videos of those actions and then ask crowd workers to clean up the results. The second effort @cite entirely crowdsources the creation of a video dataset: one worker writes a video script containing a few target objects actions, another one acts out the script and films the video, and others verify the work. In both these efforts, each video comes pre-associated with one or a handful of action labels, and workers are tasked with verifying these labels. In contrast, we're interested in the much more challenging problem of multi-label video annotation beyond the provided labels.
- Multi-label image annotation Increasingly more complex image annotations are provided in recent dataset @cite @cite @cite . Multi-label image annotation has been studied by e.g., @cite @cite @cite @cite @cite . We incorporate insights from these works into our video annotation framework. We use a hierarchy of concepts to accelerate multi-label annotation following @cite @cite . Inspired by @cite , we explore using cheap but error-prone annotation interfaces over thorough but more expensive formulations.
- Motifs were already used to analyse text @cite @cite . @cite analysed the frequency of motifs in word adjacency networks from texts in English, French, Spanish and Japanese. Despite the differences among these four languages, the authors discovered that they present very similar interconnection patterns. One possible explanation for that is that languages possess an intrinsic structure, which divides words into categories . Therefore, words from one category (e.g. prepositions) tend to be adjacent with others from different categories (e.g. nouns or articles) @cite . @cite realized that some motifs, as the ones labelled as 12 and 13 in Figure below, rarely happen in real texts.
- While being less popular, other works such as random walk based methods also enrich the research in this field. Pons and Latapy @cite introduced a measure of similarity between nodes based on random walks. Rosvall and Bergstrom @cite utilized the probability flow of random walks on a network as a proxy for information flows to reveal community structure. Perozzi, AI-Rfou, and Skiena @cite conducted random walks on a network, and treated them as sentences, later they used Natural Language Processing techniques to map the nodes into a Euclidean space for various tasks including clustering analysis.
- LDA topic model proposed by Blei, et al @cite has been applied in many domains such as document modeling @cite , image processing @cite , information retrieval @cite and the list goes on. Zhang, et al @cite developed a method for network community detection based on LDA. In their model, a social interaction profile was treated as a document and LDA was smoothly fitted into the community detection task, where communities were treated as topics.
- Nonparametric Bayesian models are becoming popular these days due to their flexibility @cite , @cite and new posterior inference methods such as Variational Inference @cite , @cite . Teh, Jordan, Beal, and Blei @cite extended the Dirichlet Process to the Hierarchical Dirichlet Process which can also be viewed as an extension of LDA model. Hoffman, Blei, Wang, and Paisley @cite developed the Stochastic Variational Inference for several non-parametric Bayesian models including HDP. In social network research, Morup and Schmidt @cite , @cite formulated a non-parametric Bayesian community generative model for social network analysis. Kim, Gopalan, Blei, and Sudderth @cite proposed the hierarchical Dirichlet process relational model which allowed nodes to have mixed membership in an unbounded set of communities. Guo and Nordman @cite introduced a series of Bayesian nonparametric statistical models for community detection. Blundell and Teh @cite proposed an efficient Bayesian nonparametric model for discovering hierarchical community structure in social networks.
- Unlike vision-based HAR systems, sensor-based HAR technologies commonly deal with time series of state changes and or various parameter values collected from a wide range of sensors such as contact sensors, accelerometers, audio and motion detectors, etc. @cite and @cite present comprehensive reviews of sensor-based activity recognition literature. The most recent work in this domain includes knowledge-based inference @cite @cite , ensemble methods @cite @cite , data-driven approaches @cite @cite , and ontology-based techniques @cite .
- Since the seminal work of Phototourism @cite , incremental SfM has been a popular pipeline for 3D reconstruction of unorganized photo collections. Robust open source solutions such as Bundler @cite and VisualSfM exist @cite using largely similar strategies to grow 3D models by successively adding one image at a time and carefully performing filtering and refinement. These pipelines have limited scalability but display excellent robustness and accuracy. In contrast, global SfM techniques @cite @cite @cite are able to compute all camera poses simultaneously, leading to extremely efficient solvers for large-scale problems; however, these methods lack robustness and are typically less accurate than incremental SfM. Alternatively, hierarchical SfM methods compute 3D reconstructions by first breaking up the input images into clusters that are individually reconstructed then merged together into a common 3D coordinate system @cite . Typically, bundle adjustment is run each time clusters are merged. This optimization ensures good accuracy and robustness but still a more scalable overall pipeline since fewer instances of bundle adjustment are performed compared to incremental SfM.
- To additionally recover scale transformations in multi-camera rigs, Ventura al @cite presented the first minimal solution to the generalized pose-and-scale problem. They use the generalized camera model and employ Gr "obner basis computations to efficiently solve for scale, rotation, and translation using 4 2D-3D correspondences. Sweeney al @cite presented the first nonminimal solver, gDLS, for the generalized pose and scale problem. By extending the Direct Least Squares (DLS) P @math P algorithm of Hesch and Roumeliotis @cite , rotation, translation, and scale can be solved for in a fixed size polynomial system. While this method is very scalable the large elimination template of the gDLS method makes it very inefficient and the Cayley rotation parameterization results in singularities. In this paper, we extend the gDLS solution method to a non-singular representation and show that using the Gr "obner basis technique achieves an 8 @math speedup.
- In a color constancy task, @math is given, while @math under canonical illumination is sought. We consider the case when neither camera sensibility @math nor the illumination @math are available, making this task under-constrained or under-determined @cite .
- Machine learning algorithms have succeeded in learning a function directly mapping low-level image feature to illumination. Support Vector Regression (SVR) is first applied to deal with color constancy task in @cite , but the performance is limited by weak predictive power of the color histogram. Bayesian approaches @cite learn the probability of illumination assuming normal-distributed reflectances. The Exemplar method @cite estimates illumination via finding nearest neighbor surfaces of a test image using an unsupervised clustering of texture and color features,
- CNNs have been applied successfully to the color constancy problem. Barron al @cite formulate the color constancy as a 2D spatial localization task in log chromatic space using a convolutional classifier specifically designed for object localization. In Bianco al @cite , a five-layer ad-hoc CNN is designed that combines feature generating and multi-channel regression to estimate illumination in an end-to-end way, similarly to @cite . Bianco al @cite compare results obtained with their five-layer ad-hoc CNN with reference -- the output of layer fc @math of an AlexNet fed into SVR. The AlexNet with SVR performed worse, but outperform most of the statistic-based methods. The selection of AlexNet and layer fc @math is ad-hoc, no other layers or net architectures were explored.
- introduced Neural Turing Machines (NTM). NTMs augment traditional RNNs with external memory that can be written to and read from. The memory is composed of a predefined number of writable slots. They are addressable via content or position shifts which allows solving simple algorithmic tasks. The capacity is also limited, but the external memory slots can carry information over long ranges more easily than traditional RNNs. NTMs inspired subsequent work on using different kinds of external memory, like queues and stacks for solving transduction tasks @cite or neural theorem provers to perform first-order @cite inference.
- Attention-based architectures store information, typically in form of hidden RNN states, dynamically for each time-step while processing a given context. These states are retrieved through an attention mechanism that softly selects a state that matches a given query state. This can be viewed as keeping the encoded context in memory. Such architectures achieved impressive results on tasks that involve larger contexts such as Reading Comprehension @cite @cite @cite , Machine Translation @cite @cite or recognizing textual entailment @cite @cite .
- Based on ideas of the attention mechanism, End-to-end Memory Networks @cite select explicit memories for query answering. Memories are encoded into two representations: i) the input representation for query matching and ii) the output representation for subsequent utilization. This distinction is important, because the representation that is used to match the original query has a different responsibility than the representation that is used to answer or update the query. Thus, attention-based approaches for answering queries using supporting documents can be considered a special case of Memory Networks where hidden states form both the input- and output representation of the individual memories which are jointly encoded. Variants of Memory Networks have achieved very good results in various NLP tasks, such as language modeling @cite , reading comprehension @cite and question answering @cite @cite @cite .
- One important contribution of Memory Networks is the idea of refining or updating the query @cite or memories @cite for multiple memory retrieval cycles before answering the query. This idea lead to significant improvements for architectures that employ the attention mechanism iteratively for reading comprehension tasks @cite .
- Recently, other forms of explicit memory have been suggested for neural architectures. For instance, associative memory can be used to effectively compress multiple memories into redundant copies of a single memory array. It has shown very promising results, e.g., for language modeling @cite or recognizing textual entailment @cite , and might therefore be suited to compress large amounts of external memories when used in conjunction with our model.
- Reducing tail latency in highly distributed services has attracted much attentions in recent years @cite . Existing techniques based on producing typically fall into three categories. The first category uses additional resources to reduce component latency variance, either by increasing the degree of parallelism @cite or by executing redundant requests @cite . The second category modifies the designs of hardware and OS @cite or software systems @cite . The third category mitigates the latencies of straggling components by enforcing dynamic component-node migrations @cite or reissuing requests on these components @cite @cite @cite . Our work forms a complement to these techniques, and we do not explain them in details here. In this section, we discuss related work based on producing .
- . Based on workload characteristic of past queries, some techniques pre-compute specialized structures (e.g. samples, histograms, or wavelets) of input datasets. Each structure can be used to answer a specific type of query requests with both accuracy and latency bounds @cite . Although these techniques can provide low latency for requests with certain attributes (e.g. the high-frequency terms in search engine), they are impractical to process online services' arbitrary requests, in which the combinations of attributes are unpredictable. Hence these techniques are orthogonal to AccuracyTrader, which uses pre-computed synopses that aggregate the entire information of all attributes to support arbitrary requests.
- In order to mitigate the problems with word representations in high-dimensional spaces, neural language models have been introduced. Unlike classical approaches, these methods learn distributed, low-dimensional representations of words through the use of neural networks @cite @cite @cite . The networks are trained by directly taking into account the word order and their co-occurrence, based on the assumption that words frequently appearing together in the sentences also share more statistical dependence.
- Despite their effectiveness, training a large number of parameters of neural network-based approaches has been a serious obstacle to wider use of the neural models. In particular, a single parameter update requires iteration over the entire vocabulary, which can grow prohibitively large for many practical applications. However, with the recent advances in the NLP domain, and in particular with the development of highly scalable continuous bag-of-words (CBOW) and skip-gram (SG) language models for word representation learning @cite , the models have been shown to obtain state-of-the-art performance on many traditional language tasks after training on large-scale textual data sets.
- Alternative methods aim to address this issue by directly retrieving ads by calculating similarity between bag-of-words representation of queries and bag-of-words representation of ads obtained from ad title and description. Even though it reduces sparsity, this approach does not completely solve the vocabulary mismatch problem @cite and may even introduce additional issues, especially in the case of templated ad creatives where majority of phrases in titles and descriptions have little to do with the advertised product (e.g., free shipping", best prices", buy now").
- Finally, to rank the ads by relevance (e.g., in the case too many ads are retrieved via query rewriting), it is common to apply the learning-to-rank approach @cite that learns weights for query and ad features based on editorially-labeled training set. These methods typically require enormous amounts of accurate editorial grades for (query, ad) matches that are often very expensive to obtain.
- By contrast, we derive general lower bounds on the Bayes risk, which automatically serve as lower bounds on the minimax risk. We use strong data processing inequalities as a unifiying general method for quantifying the contraction of mutual information in decentralized estimation. Our results apply to general priors, sample generating models, and distortion functions. When particularized to the examples in the existing works, our results can lead to sharper lower bounds on both the Bayes and the minimax risk. For example, we improve the lower bound for the mean estimation on the unit cube studied in @cite , as well as the lower bound for the meta-problem of Shamir @cite . Moreover, we consider the situations where the sample sets are conditionally dependent and where the communication channels are noisy. We also provide a general way to quantify the degradation of estimation performance caused by distributing resources to multiple processors, which is only discussed for specific examples in existing works.
- Our proposed method follows the online framework that maintains fixed-sized buffers to store instances from each class label. Further, it exploits the online second-order method @cite to learn a robust bipartite ranking function. This distinguishes the proposed method from @cite @cite employing first-order online learning. Also, the proposed method is different from @cite @cite by learning a confidence-weighted ranker capable of dealing with non-separable data and learning an adaptive large margin. The most similar approaches to our method are @cite @cite . However, these are not designed to directly maximize the AUC. They also use classical first-order and second-order online learning whereas we use the soft variation of confidence-weighted learning that has shown a robust performance in the classification task @cite .
- Time-series prediction models have been shown to be effective for anomaly detection by using the prediction error or a function of prediction error as a measure of the severity of anomaly @cite @cite @cite . Recently, deep LSTMs have been used as prediction models in LSTM-AD @cite @cite @cite where a prediction model learnt over the normal time-series using LSTM networks is used to predict future points, and likelihood of prediction error is used as a measure of anomaly. EncDec-AD learns a representation from the entire sequence which is then used to reconstruct the sequence, and is therefore different from prediction based anomaly detection models. Non-temporal reconstruction models such as denoising autoencoders for anomaly detection @cite and Deep Belief Nets @cite have been proposed. For time-series data, LSTM based encoder-decoder is a natural extension to such models.
- There is a wealth of prior work on crosslingual word embeddings, which all exploit some kind of bilingual resource. This is often in the form of a parallel bilingual text, using word alignments as a bridge between tokens in the source and target languages, such that translations are assigned similar embedding vectors @cite @cite . These approaches are affected by errors from automatic word alignments, motivating other approaches which operate at the sentence level @cite @cite @cite through learning compositional vector representations of sentences, in order that sentences and their translations representations closely match. The word embeddings learned this way capture translational equivalence, despite not using explicit word alignments. Nevertheless, these approaches demand large parallel corpora, which are not available for many language pairs.
- Another distinguishing feature on the above-cited research is the method for training embeddings. and use a cascade style of training where the word embeddings in both source and target language are trained separately and then combined later using the dictionary. Most of the other works train multlingual models jointly, which appears to have better performance over cascade training @cite . For this reason we also use a form of joint training in our work.
- There has also been an increased interest in models which use neural networks for SRL. proposed models which perform many NLP tasks without hand-crafted features. Though they did not achieve the best results on the constituent-based SRL task @cite , their approach inspired , who achieved state-of-the-art results using deep bidirectional LSTMs. Our approach for dependency-based SRL is not directly comparable.
- Some approaches to community detection are purely based on structure @cite @cite @cite . Generative models such as @cite incorporate node features in the generative process for predicting community membership as well. Our method for community detection is based on matrix factorization. The idea is not dissimilar from PCA approaches using the graph Laplacian @cite and so-called signless' Laplacian @cite .
- There are also a number of studies specifically on the role of social networks and risky sexual behavior ( @cite @cite @cite ). However, each of these papers focuses on an individual's interactions with social connections, and how this might affect engaging in risky sexual behavior. In this paper, we examine how entire social communities and the individual's membership or relationship to these communities affects engaging in risky sexual behavior.
- @cite @cite showed that for certain classes of continuous latent variables and variational posteriors, by reparametrizing the parameters or latent variables to be estimated, one can obtain a lower variance Monte Carlo approximation to the gradient of the lower bound. @cite found a more general variance reduction method that is conceptually similar to Gibbs sampling, but tends to perform worse when using the same variational posterior.
- @cite (BBVI) showed how to do stochastic variational inference in the non-conjugate case for both discrete and continuous latent variables with minimal model-specific derivation. This involves taking a Monte-Carlo approximation of the gradient of the expected lower bound, using ADAGRAD to avoid taking derivatives by hand, and using Rao Blackwellization with a mean-field assumption and control variates to reduce the variance of the gradients. @cite (O-BBVI) extended this work by adding importance sampling to further reduce the variance of the gradient estimator. @cite (VBIL) builds on the same naive estimator as BBVI, but uses natural gradient descent instead of ADAGRAD and treats the ABC case. They show that despite a bias being introduced by taking the log of a likelihood estimator, it is equivalent to using the true likelihood, and they apply this to several intractable likelihood problems. See the supplementary material for the similarity between BBVI and VBIL.
- @cite used automatic differentiation to fully automate variational inference, which we refer to as AVI. AVI fits a Gaussian variational distribution in a transformed space, leading to a non-Gaussian approximation in the original space when transformed back. This limits the range of variational distributions. In the transformed space they make a fully-factorized mean-field assumption. AVI requires rewriting the simulator in STAN. With complex simulators, this is unrealistic and negates the main benefit: that it is automatic.
- If the system provided a (the troubadours' jargon for rationale''; see Agamben [p. 79] agamben1999end ), we could debug that, and perhaps involve additional AI systems in the process @cite .
- Stream processing systems allow users to compose applications as a dataflow graph, with task vertices having some user-defined logic, and streaming edges passing messages between the tasks, and run these applications continuously over incoming data streams. Early data stream management systems (DSMS) were motivated by sensor network applications, that have similarities to IoT @cite @cite . They supported continuous query languages with operators such as join, aggregators similar to SQL, but with a temporal dimension using windowed-join operations. These have been extended to distributed implementations @cite and complex event processing (CEP) engines for detecting sequences and patterns. Current distributed stream processing systems (DSPS) like Storm and Spark Streaming @cite @cite @cite leverage Big Data fundamentals, running on commodity clusters and Clouds, offering weak scaling, ensuring robustness, and supporting fast data processing over thousands of events per second. They do not support native query operators and instead allow users to plug in their own logic composed as dataflow graphs executed across a cluster. While developed for web and social network applications, such fast data platforms have found use in financial markets, astronomy, and particle physics. IoT is one of the more recent domains to consider them.
- Work on DSMS spawned the Linear Road Benchmark (LRB) @cite that was proposed as an application benchmark. In the scenario, DSMS had to evaluate toll and traffic queries over event streams from a virtual toll collection and traffic monitoring system. This parallels with current smart transportation scenarios. However, there have been few studies or community efforts on benchmarking DSPS, other than individual evaluation of research prototypes against popular DSPS like Storm or Spark.
- Stream Bench @cite has proposed 7 micro-benchmarks on 4 different synthetic workload suites generated from real-time web logs and network traffic to evaluate DSPS. Metrics including performance, durability and fault tolerance are proposed. The benchmark covers different dataflow composition patterns and common tasks like grep and wordcount. While useful as a generic streaming benchmark, it does not consider aspects unique to IoT applications and streams.
- SparkBench @cite is a framework-specific benchmark for Apache Spark, and includes four categories of applications from domains spanning Graph computation and SQL queries, with one on streaming applications supported by Spark Streaming. The benchmark metrics include CPU, memory, disk and network IO, with the goal of identifying tuning parameters to improve Spark's performance. CEPBen @cite evaluates the performance of CEP systems based of the functional behavior of queries. It shows the degree of complexity of CEP operations like filter, transform and pattern detection. The evaluation metrics consider event processing latency, but ignore network overheads and CPU utilization. Further, CEP applications rely on a declarative query syntax to match event patterns rather than a dataflow composition based on user-logic provided by DSPS.
- There has been a slew of Big Data benchmarks that have been developed recently in the context of processing high volume (i.e., MapReduce-style) and enterprise web data that complement our work. @cite is a workload suite for evaluating Hadoop with popular micro-benchmarks like Sort, WordCount and TeraSort, MapReduce applications like Nutch Indexing and PageRank, and machine learning algorithms like K-means Clustering. @cite analyzes workloads from social network and search engines, and analytics algorithms like Support Vector Machine (SVM) over structured, semi-structured and unstructured web data. Both these benchmarks are general purpose workloads that do not target any specific domain, but MapReduce platforms at large. @cite uses a synthetic data generator to simulate enterprise data found in online retail businesses. It combines structured data generation from the TPC-DS benchmark @cite , semi-structured data on user clicks, and unstructured data from online product reviews. Queries cover data by processing periodic refreshes that feed into the data store, by including free-text user reviews, and by querying over a large web log of clicks. We take a similar approach for benchmarking fast data platforms, targeting the IoT domain specifically and using real public data streams.
- There has been some recent work on benchmarking IoT applications. In particular, the generating large volumes of synthetic sensor data with realistic values is challenging, yet required for benchmarking. @cite provides a scalable synthetic generator of time-series datasets. It uses a Markov chain model for scaling the time series with a limited number of inputs such that important statistical properties of the stream is retained in the generated data. They have demonstrated this for smart meter data. The benchmark also includes six SQL queries to evaluate the performance of different query platforms on the generated dataset. Their emphasis is more on the data characteristics and content, which supplements our focus on the systems aspects of the executing platform. CityBench @cite is a benchmark to evaluate RDF stream processing systems. They include different generation patterns for smart city data, such as traffic vehicles, parking, weather, pollution, cultural and library events, with changing event rates and playback speeds. They propose fixed set of semantic queries over this dataset, with concurrent execution of queries and sensor streams. Here, the target platform is different (RDF database), but in a spirit as our work.
- There are two major paradigms in WSD: supervised and knowledge-based. Supervised algorithms learn, from sense-labeled corpora, a computational model of the words of interest. Then, the obtained model is used to classify new instances of the same words. Knowledge-based algorithms perform the disambiguation task by using an existing lexical knowledge base, which usually is structured as a semantic network. Then, these approaches use graph algorithms to disambiguate the words of interests, based on the relations that these words' senses have in the network @cite .
- A popular supervised WSD system, which has shown good performances in different WSD tasks, is (IMS) @cite . It takes as input a text and for each content word (noun, verb, adjective, or adverb) outputs a list of possible senses ranked according to the likelihood of appearing in a determined context and extracted from a knowledge base. The training data used by this system are derived from SemCor @cite , DSO @cite and collected automatically exploiting parallel corpora @cite . Its default classifier is LIBLINEAR http: liblinear.bwaldvogel.de with a linear kernel and its default parameters.
- Unsupervised and knowledge-based algorithms for WSD are attracting great attention from the research community. This because, supervised systems require training data, which are difficult to obtain. In fact, producing sense tagged data is a time-consuming process, which has to be carried out separately for each language of interest. Furthermore, as investigated by Yarowsky and Florian , the performances of a supervised algorithm degrade substantially with the increasing of sense entropy. Sense entropy refers to the distribution over the possible senses of a word, as seen in training data. Additionally, a supervised system has problems to adapt to different contexts, because it depends on prior knowledge, which makes the algorithm rigid, therefore can not efficiently adapt to domain specific cases, when other optimal solution may be available @cite .
- One of the most common heuristics that allows to exploit sense tagged data such as SemCor @cite is the most frequent sense. It exploits the overall sense distribution for each word to be disambiguated, choosing the sense with the highest probability regardless of any other information. This simple procedure is very powerful in general domains but can not handle senses with a low distribution, which could be found in specific domains.
- With these observations in mind created three domain specific corpora to evaluate WSD systems. They tested whether WSD algorithms are able to adapt to different contexts, comparing their results with the most frequent sense heuristic, computed on general domains corpora. They used an unsupervised approach to obtain the most frequent sense for a specific domain @cite and demonstrated that their approach outperforms the most frequent sense heuristic derived from general domain and labeled data.
- Other unsupervised and semi-supervised approaches, instead of computing the prevalent sense of a word, try to identify the actual sense of a word in a determined phrase, exploiting the information derived from its context. This is the case of traditional algorithms, which exploit the pairwise semantic similarity among a target word and the words in its context @cite @cite @cite . Our work could be considered as a continuation of this tradition, which tries to identify the intended meaning of a word given its context, using a new approach for the computation of the sense combinations.
- A new graph based, semi-supervised approach, introduced to deal with multilingual WSD @cite and entity inking problems, is Babelfy @cite . Multilingual WSD is an important task because traditional WSD algorithms and resources are focused on English language. It exploits the information from large multilingual knowledge, such as BabelNet @cite to perform this task. Entity linking consists in disambiguating the named entities in a text and in finding the appropriate resources in an ontology, which correspond to the specific entities mentioned in a text. Babelfy creates the semantic signature of each word to be disambiguated, that consists in collecting, from a semantic network, all the nodes related to a particular concepts, exploiting the global structure of the network. This process leads to the construction of a graph-based representation of the whole text. Then, it applies Random Walk with Restart @cite to find the most important nodes in the network, solving the WSD problem.
- APNs or similar high level net''-formalisms are an established, expressive modeling language for distributed systems @cite @cite . Moreover, tools for Colored Petri Nets support simulation and (partial) verification @cite @cite . The idea to prove stable properties in Petri nets that use distinguishable tokens has been pursued at least since the early 80s @cite . Ever since, the class of invariants became a substantial part of Petri Net analysis @cite @cite @cite . Other stable properties for Algebraic Petri Nets have been studied in the context of Traps Co-Traps @cite . In elementary Petri Nets (P T-Nets), stable properties such as traps and co-traps have been studied @cite and been shown as useful for verification @cite @cite . Compared to this, the number of publications regarding stable properties in is comparatively small. In the last years, Petri Net variants with distinguishable tokens gained more attention to model data in distributed systems and applying analytic methods such as @cite @cite @cite .
- As discussed in section , one of the challenges in providing deterministic processing is the merging of multiple timestamp sorted input streams. This has been discussed in the context of parallel-distributed SPEs @cite @cite and replica-based fault tolerance protocols for data streaming @cite . Existing approaches for streaming aggregation rely on separated input queues (similar to the @math protocol). As shown in our evaluation, this merging is not efficient and implementations such as can drastically improve the overall operator's performance.
- There is a substantial literature on nonparametric estimation of causal effects of binary treatment or intervention variables . For continuous treatments or intervention variables, recent work also includes @cite , besides the marginal integration approach . Marginal integration itself has been proposed by for smooth function estimation in structured nonparametric regression, mainly for additive models. The theoretical analysis in is the basis for inferring causal effects in the setting of independent data . We are not aware of any other work which considers estimation of causal effects, as defined in , for the setting of stationary time series.
- Online social network has greatly changed the content delivery, e.g. , the distribution of social contents is shifted from a central-edge'' manner to an edge-edge'' manner. Bakshy @cite studied the social influence of people in the online social network, and observed that some users can be very influential in social propagation. Li @cite studied the content sharing in the online social network, and observed the skewed popularity distribution of contents and the activity of users. @cite investigated response time of social contents using collected traces, and confirmed the in nature of social media, which motivated our study. In our previous study @cite , we also observed the correlation between social connection and propagation, and users' preferences of content.
- As online social networks are affecting for all types of online contents, conventional content delivery paradigms need improvement using social information. Pujol @cite designed a social partition and replication middleware where users' friends' data can be co-located in datacenter servers. @cite investigated using social information for content delivery over the edge networks. @cite @cite investigated the possibility to infer social propagation according to users' social profiles and behaviors, and allocate network resource at edge-cloud servers based on propagation predictions. @cite further proposed the cloud mobile media concept to utilize cloud-based resource for mobile media content processing and distribution. @cite proposed a novel peer-assisted paradigm using social relationship for improved social media distribution.
- Understanding mobility of users is a key to design effective delivery strategies for mobile social contents. @cite proposed to use traces of users' associations with Wi-Fi access points to investigate how users move among popular locations. Based on user mobility models, @cite found that it is possible to generate movement patterns which are statistically similar to the real movement. @cite surveyed the usage of spatial, temporal and social properties to capture the mobility behaviors.
- @cite then pointed out that human movements are not random walks, and the patterns of human walks and walks contain some statistical similarity; in particular, features including heavy-tail flight and the super diffusive nature of mobility are observed. In @cite , power-law and exponential decay of inter-contact times between mobile devices are observed. @cite studied the mobility and encountering patterns for users in regions during particular events, e.g. , conferences.
- When jointly studying mobility and social network of users, @cite observed that though human movement and mobility patterns have a high degree of variation, they exhibit structural patterns due to geographic and social constraints. In particular, short-ranged travel is periodic both spatially and temporally, and not affected by the social network structure. Recently, such mobility studies have improved the edge network and content delivery design, @cite studies the mobility characteristics of people to guide wireless network deployment. @cite found the similarity between individuals' mobility patterns and their social proximities.
- As early as Mobile Ad hoc Network (MANET) was proposed, researchers started to envision using device-to-device communications for content delivery. @cite studied opportunistic networking and discussed possible scenarios to use it. In the context of using smart devices with limited energy for content delivery, @cite formulated the optimization problem of opportunistic forwarding, with the constraint of energy consumed by the message delivery for both two-hop and epidemic forwarding. In 's study @cite , they investigated the selection of target-set of users for content deployment to minimize the mobile data traffic over cellular networks.
- In the one dimensional case ( @math ), computing the Wasserstein barycenter of a finite set of probability measures simply amounts to averaging (in the usual way) their quantile functions. In statistics, this approach has been referred to as quantile synchronization @cite . In the presence of phase variability in the data, quantile synchronization is known to be an appropriate alternative to the usual Euclidean mean of densities to compute a structural mean density that is more consistent with the data. Various asymptotic properties of quantile synchronization are studied in @cite in a statistical model and asymptotic setting similar to that of this paper with @math . However, other measures of risk than the one in this paper are considered in @cite , but the optimality of the resulting convergence rates of quantile synchronization is not discussed.
- The results of this paper are very much connected with those in @cite where a new framework is developed for the registration of multiple point processes on the real line for the purpose of separating amplitude and phase variation in such data. In @cite , consistent estimators of the structural mean of multiple point processes are obtained by the use of smoothed Wasserstein barycenters with an appropriate choice of kernel smoothing. Also, rates of convergence of such estimators are derived for the Wasserstein metric. The statistical analysis of multiple point processes is very much connected to the study of repeated observations organized in samples from independent subjects or experimental units. Therefore, some of our results in this paper on smoothed empirical Wasserstein barycenters are built upon the work in @cite . Nevertheless, novel contributions include the derivation of an exact formula to compute the risk of non-smoothed Wasserstein barycenters in the case of samples of equal size, and new upper bounds on the rate of convergence of the Wasserstein risk of non-smoothed and smoothed empirical Wasserstein barycenters, together with a discussion of their optimality from the minimax point of view.
- The construction of consistent estimators of a population Wasserstein barycenter for semi-parametric models of random measures can also be found in @cite and @cite , together with a discussion on their connection to the well known curve registration problem in statistics @cite @cite .
- HMMs have previously been used for weakly-supervised learning from token or type constraints @cite @cite @cite . HMMs are generative models, and in this setting the words in the target sentence form the observed sequence and the morphological tags the hidden sequence. The projected constraints are used as partially observed training data for the hidden sequence.
- To the best of our knowledge, the only other JVM implementing the Java memory model on a non cache coherent architecture is Hera-JVM @cite . Hera-JVM also employs caches which it handles in a similar manner to our implementation, with the difference that it starts a write-back at every write, as we discuss in sec:implementation . Regarding the synchronization mechanisms, Hera-JVM relies on the Cell B.E.'s GETLLAR and PUTLLC instructions to build an atomic compare-and-swap operation. However, such instructions are not available on the architectures at hand @cite @cite . Additionally, Hera-JVM did not aim to formally prove its adherence to the JMM.
- Jagadeesan @cite also describe an operational semantics for the Java Memory Model. Their work, however, does not account for caches or buffers. It abstracts away the hardware details and considers reads and writes to become actions that float into the evaluation context. This approach does not explicitly define when and where writes should be eventually committed to satisfy the JMM @. In our approach, we explicitly define where data get stored after any evaluation step.
- We thus consider our approach to be closer to the implementation. Cenciarelli @cite use a combination of operational, denotational, and axiomatic semantics to define the JMM @. In that work, the authors show that all the generated executions adhere to the JMM, but as in @cite they do not account for the memory hierarchy.
- Many approaches have been proposed with potential applications in smart coating; these can be categorized as active and passive systems. In passive systems, particles move based only on their structural properties and interactions with their environment, or have only limited computational ability but lack control of their motion. Examples include population protocols @cite as well as molecular computing models such as DNA self-assembly systems (see, e.g., the surveys in @cite @cite @cite ) and slime molds @cite @cite .
- Our focus, however, is on active systems, in which computational particles control their actions and motions to complete specific tasks. Coating has been extensively studied in the area of , but not commonly treated as a stand-alone problem; it is instead examined as part of (e.g., @cite ) or (e.g., see respective section of @cite ). Some research focuses on coating objects as an independent task under the name of or . The techniques used in this context include stochastic robot behaviors @cite @cite , rule-based control mechanisms @cite and potential field-based approaches @cite . While the analytic techniques developed in swarm robotics are somewhat relevant to this work, many such systems assume more computational power and movement capabilities than the model studied in this work does. Michail and Spirakis recently proposed a model @cite for network construction inspired by population protocols @cite . The population protocol model is related to self-organizing particle systems, but is different in that agents (corresponding to our particles) can move freely in space and establish connections at any time. It would, however, be possible to adapt their approach to study coating problems under the population protocol model.
- In the context of molecular programming, our model most closely relates to the nubot model by @cite @cite , which seeks to provide a framework for rigorous algorithmic research on self-assembly systems composed of active molecular components, emphasizing the interactions between molecular structure and active dynamics. This model shares many characteristics of our amoebot model (e.g., space is modeled as a triangular grid, nubot monomers have limited computational abilities, and there is no global orientation) but differs in that nubot monomers can replicate or die and can perform coordinated rigid body movements. These additional capabilities prohibit the direct translation of results under the nubot model to our amoebot model; the latter provides a framework for future, large-scale swarm robotic systems of computationally limited particles (each possibly at the nano- or micro-scale) with only local control and coordination mechanisms, where these capabilities would likely not apply.
- Finally, in @cite we presented our Universal Coating algorithm and proved its correctness. We also showed it to be worst-case work-optimal, where work is measured in terms of number of particle movements.
- Common Language Specification The CLS is a set of rules that is important for language implementors and application developers. Some programming languages offer features that are not supported in other programming languages. For example, according to Hamilton, incompatible types are the primary barriers that keep languages from interoperating'' @cite . The CLS ensures that CLS-compliant components can interact with each other, e.g. by defining a set of data types that all CLS-compliant languages support. Language implementors have to support these data types to call their implementation CLS-compliant and their compilers must generate CLS-compliant artifacts that do not use other data types in their public interface.
- When an instance of a CLI class is accessed in Ruby for the first time, Ruby .NET dynamically create[s] a special Ruby.Class object to represent the foreign CLI type.'' @cite . These Ruby classes use CLI reflection instead the Ruby method dictionary during method lookup. Ruby .NET retains a dictionary that maps CLI classes to already generated Ruby classes.
- With STX:LIBJAVA https: swing.fit.cvut.cz projects stx-libjava , implemented the Java programming language on top of the Smalltalk X virtual machine. STX:LIBJAVA @cite uses a modified Smalltalk X virtual machine that can execute both Smalltalk byte code and Java byte code. It distinguishes between Java objects and Smalltalk object, but both live in the same object memory. When the control flow crosses the language boundary, i.e. a Smalltalk method is called from Java or the other way around, STX:LIBJAVA generates a proxy method that performs the method resolution, transforms arguments, and passes the control flow to the real method. In MagLev , bridge methods are a similar concept but they do not lookup methods by themselves or transform objects between programming languages.
- In a later paper, evaluate the concept of behavior objects for Java in Smalltalk X @cite . Just as in MagLev, every Smalltalk X object is a Java object. Instead of environments, use different behavior objects for every programming language. A behavior object contains the methods for an object in one programming language. A Smalltalk behavior object is a Smalltalk class and a Java behavior object is an object similar to a Java class. Therefore, objects can have two classes. In addition, they use mapping functions to transform the object layout for object state, assuming that different programming languages expect different object state layouts. Just as in MagLev, every programming language can define a different superclass. The superclass is stored in the behavior object.
- A comparison of advantages of the different schemes is shown in Tab. : Delta-based approaches are clearly able to yield lowest storage costs for changed contents in principle, but they have important limitations in practice: First, their efficiency depends on a priori knowledge of relations between chunks---a problem tackled by, e.g., the DERD framework @cite . Second, they substantially increase retrieval costs as reconstruction of delta-encoded chunks requires retrieval of corresponding base chunks of which only parts are actually required. Chunking-based schemes achieve savings when storing changed contents (data) irrespective of knowledge about relations (their best values are highlighted in green in the table). The most efficient strategies CDC ASC yield their savings depending on the distribution of contents specific file formats.
- This problem is solved by our proposals ML-SC ML-CDC: Due to a specific, multi-level application of SC CDC, we achieve logarithmic metadata costs, allowing high storage efficiency even for small chunk sizes, independent from content sizes. To the best of our knowledge, our strategy is unique. @cite apply CDC recursively to enable efficient replication of files over a network (assuming the receiver has a similar file to the one being transferred), but they do not target storage systems and fail to achieve logarithmic costs due to a fixed recursion depth. Further, their approach is different to ours: Instead of breaking large chunks recursively into smaller ones, they generate the smallest chunks first and use CDC recursively to break lists of chunk references into smaller parts---requiring multiple passes. Yasa and Nagesh @cite employ hierarchical chunking starting with CDC at the highest level as we do, but they use only two levels (SC at second level), so storage overhead is still linear.
- SiRiUS @cite , Plutus @cite and Tahoe-LAFS @cite are examples of file systems with authenticity guarantees: They apply SC to contents and compute a Merkle tree @cite over the chunks to allow authenticity verification even for parts of contents, but they do not support data deduplication: Tahoe-LAFS creates entirely different ciphertexts for similar file contents, the other systems even for identical ones.
- To allow efficient usage of cloud storage for, e.g., backups, more specialized tools are required. Common backup tools like duplicity @cite rely on backups, i.e., they store differences to previous backups. This can be used in combination with GnuPG to preserve snapshots in a storage-efficient and secure manner, but causes communication overhead when specific versions are read. VCS could be used for delta-based backups to a limited extent, but they are typically inefficient w.r.t. large files and have limited security properties: Git @cite only ensures integrity authenticity of the version history by integrating signatures. SVN does not, but an extension @cite adds storage-efficient file-level encryption. Cumulus @cite is a backup system that supports large files and allows direct access to arbitrary snapshots, but it is less storage-efficient as it only deduplicates identical data between different versions of individual files. Farsite @cite , in contrast, is a distributed file system targeting on chunking-based backups that deduplicates different identical files despite secure encryption. It cannot save space for snapshots of different versions of a file, though, as it relies on WFC. @cite extend Farsite's concept to CDC, but they do not provide any explicit authenticity guarantees.
- Many more works exist in the field of cloud storage security. Most of them, however, have a different focus and are orthogonal to our work. Athos @cite , e.g., is a solution for outsourcing file systems that achieves integrity in a way that file system operations are possible with logarithmic communication costs. The solution is orthogonal to our work in the sense that this requirement is w.r.t. the total number of files directories in the file system and not w.r.t. the size of single file contents. A similar goal is pursued by @cite . Both works are based on authenticated skip lists, an authenticated data structure (ADS) initially proposed by @cite , while our work is based on another ADS---the Merkle tree @cite .
- ADS, in general, is an umbrella term for data structures involving three parties (a trusted who publishes data, an untrusted that stores structured data, and a that requests data) that enable authenticated, efficient queries to the data. @cite In this sense, |sec-cs| can be considered an ADS with additional data deduplication and confidentiality properties: The cloud storage backend can be considered the and the user client plays the roles of and . An overview of existing ADS and methods for constructing ADS in general are provided by @cite and @cite .
- Automated algorithm selection has been a long desired goal in computer science, with possibly the first formulation by Rice @cite . As such, it has been applied to a variety of disciplines in the field, such as scientific computing @cite , game theory @cite , and artificial intelligence problems such as satisfiability @cite . Most efforts, however, focus on methods to analyze and learn how to apply the single best algorithm to solve a problem. For example, Guo @cite uses Bayesian networks to learn and select the best algorithm to solve a problem; neither the problems or algorithms are specific, and can be applied generally to a variety of problems. Our SFI framework is complementary to much of this existing work. SFI can decompose complex models into smaller sub--models that these sophisticated learning algorithms can operate on, potentially providing even better performance than learning on a single model.
- Estimating mutual information from data requires an large number of observations---especially when the dimensionality is high. The proposed variational lower bound can be viewed as a way of estimating mutual information between a high-dimensional continuous variable and a discrete variable. Only a few examples exist in literature @cite under this setting. We hope our method will shed light on new ways to estimate mutual information, similar to estimating divergences in @cite .
- @cite is an automated proof generation tool in Coq. Taking existing Coq theories as input, first produces proof traces, from which it infers an extended finite state machine. Given a conjecture, uses this model to search for its proof. 's search algorithm is based on the breadth-first search (BFS) to return the shortest proof. can also adopt BFS, as BFS is a special case of IDDFS. However, our experience tells that the search tree tends to be very wide and some tactics, such as , need to be followed by other tactics to complete proofs. Therefore, we chose IDDFS for . Both and off-load the construction of proof scripts to search and try to reconstruct efficient proof scripts. Compared to , allows users to specify their own search strategies to utilize the engineer's intuition, which enables to return incomplete proof scripts, as discussed in Section .
- Martin al first discussed a monadic interpretation of tactics for their language, , in an unpublished draft @cite . We independently developed |interp| with the features discussed above, lifting the framework from the tactic level to the strategy level to traverse larger search spaces. The two interpreters for different ITPs turned out to be similar to each other, suggesting our approach is not specific to Isabelle but can be used for other ITPs.
- offers a framework for encoding and applying common patterns of reasoning in Isabelle, following the style of proof planning @cite . addresses the performance issue by memoization technique, on the other hand try strips off backtracked steps while searching for a proof, which Isabelle can check later without try . While works on its own data structure , managed to minimize the deviation from Isabelle's standard execution model using constructor classes.
- Automated image captioning: Automated image captioning systems have grown in prominence owing, in large part, to tremendous performance gains in deep learning in recent times. Existing automated image captioning systems can be categorized as either generative or retrieval-based. Generative systems involve correctly identifying objects, verbs, adjectives, prepositions or visual phrases in an image and generating a caption from these or directly from the representation of the image @cite , @cite , @cite , @cite , @cite , @cite . Retrieval-based approaches @cite @cite , on the other hand, involve retrieving the most suitable caption from a database of captions and assigning it to an image. All the above approaches address image captioning in a general sense, but our contribution is to address image captioning for two specific situations, one where rapid system development time is needed to deploy image captioning (i.e., the @math -NN model), and image captioning for resource-constrained devices where algorithmic efficiency is critical (i.e., the second issue).
- Action (Verb) recognition : Action recognition in videos is a relatively easier problem and has been addresed by using various kinds of spatio-temporal descriptors followed by a classifier @cite , @cite , @cite . However, in static images, unlike videos, there is no temporal information. Also, the problem is further exacerbated by the lack of reliable annotation data. Hence, several existing works in action recognition on static images use supervised learning of visual information integrated with linguistic information mined from a text corpus @cite , @cite , @cite . In this paper, we argue that if we know just the objects in an image, we can succesfully use word embeddings to describe the underlying action, even in static images, with a reasonable degree of accuracy. More recently, Jain et. al. @cite have shown the efficacy of word embeddings for zero-shot action recognition in videos. In contrast, the strength of our approach is that we use word embeddings in a relatively simple manner and yet obtain good results on a challenging set of static images.
- Other related works include ) , which builds dynamic representations of the candidate answers while reading the document, and accumulates the information about an entity by max-pooling; consists of two networks, where one proposes a small set of candidate answers, and the other reranks the proposed candidates conditioned on the query and the context; @cite adopts a multi-stage hierarchical architecture along with a flow-based attention mechanism; showed a 10
- This paper mainly focuses on activation functions and the weight initialization of deep neural networks. Therefore, we review the related work in the two fields. Note that training very deep networks can also be realized by developing new architectures such as introducing skip connection as in @cite @cite , but this is beyond the scope of the paper.
- Past approaches use domain-specific knowledge of music for feature design, often from a single attribute only. For example, chromas @cite @cite @cite @cite and spectral component histograms @cite have been utilized as harmonic features, having in mind that the harmonic content is more likely to change around the downbeat positions. Other features, more generally considered as rhythm markers, are based on the onset detection function (ODF) @cite @cite @cite @cite @cite . It is worth noting here that, even if isolated onsets are not always present at downbeat positions, onset patterns will often be synchronous with the bar changes. ODF are often extracted across frequency bands @cite @cite in order, for instance, to separate the events corresponding to kick drums or bass from those of the snare. A third category concerns timbre inspired features @cite @cite @cite @cite . Alterations of the timbre content occur more likely at the start of a new section and near a downbeat position. This feature extraction step can be done in conjunction with an onset @cite , tatum @cite or beat @cite segmentation.
- The goal of the second step is to map the features into a downbeat detection function. This can be done with heuristics. When harmony features are used, most systems focus on measuring a change in the feature properties @cite @cite @cite @cite . This can be done with the cosine distance, the Kullback-Leibler divergence or a difference of the principal harmonic components. If rhythm features are considered, comparison with pre-stored rhythm patterns @cite @cite @cite , beat enhanced onset detection function @cite and relative spectral balance between high and low energy content at different beat positions @cite have also been proposed.
- Machine learning systems can also be considered. Generic approaches such as Support Vector Machines (SVM) with an auditory spectrogram @cite @cite have been used at the risk of not being tailored for this particular problem. Other systems focus on recognizing rhythm patterns in the data to find the downbeats. It can be done with a Conditional Deep Belief Network @cite (CDBN) or a Gaussian Mixture Model (GMM) coupled to a k-means clustering @cite @cite . These rhythm related methods are more adapted to the problem but they often make strong style-specific assumptions and require music with a very distinctive rhythm style to work well @cite @cite @cite .
- To take into account the slow bar length variation over time, induction methods are often used. The estimated downbeat sequence will be the one that maximizes the downbeat detection function and minimize the bar length variation. It can be done in a greedy way, one downbeat after another, starting at the first downbeat @cite @cite or at the start of the first characteristic rhythm pattern @cite . To avoid being stuck in a local minimum, most algorithms search a more global downbeat sequence path. It can be done with dynamic programming @cite or Hidden Markov Models (HMM) @cite @cite @cite that can sometimes be coupled with a learned language model @cite . A particularly interesting temporal model is the dynamic bar pointer model @cite . It consists of a dynamic bayesian network jointly modeling the downbeat positions, the tempo and the rhythm pattern. This method was refined to improve the observation probabilities @cite and reduce of the computational complexity of the inference @cite . Most of the aforementioned systems don't handle varied time signatures during the downbeat sequence extraction.
- The simplest basis set for representing unknown eigenfunctions is the direct product (DP) of one-dimensional basis functions. If a fast matrix-by-vector operation is given, then Krylov iterative methods are available and the only problem is the exponential storage requirements @math . Alternatively one can prune a direct product basis @cite @cite @cite or use a basis that is a product of functions with more than one variable @cite @cite .
- In this work, we focus on DP basis and further reduce @math storage by approximating unknown eigenvectors in the TT-format. We refer the reader to @cite @cite for detailed surveys on tensor decompositions.
- Canonical tensor decomposition (also called CP decomposition or CANDECOMP PARAFAC model) of the eigenvectors of vibrational problems was considered in work by Leclerc and Carrington @cite . The authors used rank-reduced block power method (RRBPM). Each iteration of this method involves matrix-by-vector product, which can be efficiently implemented in tensor formats. The problem is that this method has poor convergence. Moreover, canonical decomposition is known to have stability issues @cite .
- The hierarchical RRBPM (H-RRBPM) proposed in @cite by Thomas and Carrington is a significant improvement of the RRBPM. This method also utilizes sum-of-products representation, but treats strongly coupled coordinates together. Coupled coordinates are further decomposed hierarchically.
- The Multi Configuration Time Dependent Hartree (MCTDH) approach @cite also uses tensor format, namely the Tucker decomposition. This approach reduces complexity, but suffers from the curse of dimensionality. This problem was solved in the multilayer MCTDH @cite which is similar to the Hierarchical Tucker representation @cite .
- We would also like to discuss tensor algorithms for solving eigenvalue problems developed in mathematical community. There are two natural directions of solving eigenvalue problems in tensor formats. One direction is straightforward generalization of iterative methods to tensor arithmetics with rank reduction on each iteration. For the canonical decomposition power method with shifts was generalized in @cite @cite and used in the RRBPM method. The preconditioned inverse iteration (PINVIT) for tensor formats was considered in @cite @cite @cite . The inverse iteration used in this paper differs from the PINVIT, which is basically preconditioned steepest descent. Tensor version of the inverse iteration based on iterative solution of arising linear systems was considered in @cite .
- The PINVIT iteration can explicitly utilize a preconditioner. The construction of preconditioners in tensor formats for eigenvalue problems was considered in @cite @cite @cite @cite . The approach for a general operator @cite uses Newton-Schulz iteration in order to find a good preconditioner. However, due to high amount of matrix multiplications, this approach becomes time-consuming. In order to construct a preconditioner one can use approximate inverse matrix or approximate solution of linear systems. See @cite @cite @cite @cite for solvers in tensor formats.
- Alternatively to iterative methods, one can pose an optimization problem -- minimization of the Rayleigh quotient with the constraint on rank. This approach was recently considered in the matrix product state (MPS) community @cite and independently in @cite . The only disadvantages is that all eigenfunctions are treated in one basis, which leads to large ranks and the method becomes slow (calculation of the acetonitrile took several days). Nevertheless, this approach becomes very efficient and accurate when small number of eigenpairs are required.
- Graphical energy minimisation techniques are popular methods for regularised image segmentation due to inherent optimality guarantees and computational efficiency @cite @cite . They have been extensively used in the optimisation of interactive @cite @cite @cite @cite and fully automated segmentation methods @cite @cite @cite @cite .
- An iterative optimisation of such energy functionals allows to address more complex problems, such as the pixelwise segmentation from bounding box annotations. The popular GrabCut method @cite iteratively updates parameters of a Gaussian mixture model (GMM) and optimises the resulting energy with a graph cut. Lempitsky @cite extended this method by adding a topological prior to prevent excessive shrinking of the region and @cite improved upon its performance by employing a fast fully connected Conditional-Random Field (CRF) @cite . Similar approaches include the time-implicit propagation of levelsets via continuous max-flow @cite @cite , iterative graph cuts @cite and the use of the expectation-maximisation (EM) algorithm @cite @cite @cite .
- Recently, several multiple instance learning (MIL) techniques were investigated, particularly when images could potentially contain multiple objects. Cinbis @cite proposed a multi-fold MIL method to obtain segmentations from image level tags. Vezhnevets and Buhmann @cite addressed the problem with a Texton Forest @cite framework, extending it to MIL. With the re-emerging of convolutional neural networks @cite @cite , MIL methods have been proposed to exploit such methods @cite @cite . However MIL-based methods, even when including additional modules under weak supervision @cite , have not been able to achieve comparable accuracy to fully supervised methods @cite . However, latest developments using CNN learning with weakly supervised data have shown remarkable improvements in accuracy. @cite parse clinical reports to associate findings and their locations with optical coherence tomography images and further obtain segmentations of the reported pathology. @cite iterative between updating region proposals in bounding boxes and model training. @cite formulate an Expectation-Maximization (EM) algorithm @cite to iteratively update the training targets. Both of the latter methods were able to achieve comparable accuracy to those under full supervision.
- Model selection is about balancing the trade-off between model complexity and fit to the data. Models with more parameters have a natural advantage at fitting data. Simpler models have lower variability, and are less sensitive to noise in the data. A good model choice should avoid both over-fitting and under-fitting, and only include additional parameters when they do capture meaningful structures in the data @cite @cite . The frequentist approach to model selection cast the problem as a hypothesis testing. It focus on estimating the likelihood-ratio between a pair of candidate models under the null hypothesis. Such frequentist method has been used for both model selection and order selection @cite @cite . In this paper, we shall follow the other major school of statistics and use Bayesian techniques for model selection.
- Generative models are also used for shape recognition. Sun and Super @cite propose a Bayesian model, which use the normalized contour fragments as the input features for shape classification. Wang @cite model shapes of one class by a skelet al prototype tree learned by skeleton graph matching. Then a Bayesian inference is used to compute the similarity between a testing skeleton and each skelet al prototype tree. Bai @cite propose to integrate contour and skeleton by a Gaussian mixture model, in which contour fragments and skeleton paths are used as the input features. Unlike their method, ours encodes the contour and skeleton features into one shape descriptor according to the association between contour and skeleton. Therefore, we avoid the intractable step to finetune the weight between contour and skeleton models.
- Recent work in both imitation learning and policy search reinforcement learning have allowed robots to learn and execute complicated motor skills. Example skills include playing ball games @cite @cite @cite @cite , opening doors @cite @cite , scrubbing surfaces @cite @cite , and manipulating objects @cite @cite @cite . These learned skills adapt to different features of the task, e.g., the position of a ball or the goal location. However, the features used to adapt the skill executions are usually predefined and do not include irrelevant features. These methods therefore focus on determining how to adapt to the features, and not learning which features to adapt to.
- Some frameworks learn relevant features in order to select actions more accurately in otherwise ambiguous situations @cite @cite . These approaches often perform an implicit pose estimation of an object or part, which the robot can then use to adapt its actions. However, the features are usually learned for specific sets of objects and not to generalize between objects with different shapes.
- Another important challenge for skill learning is determining which objects are involved in a manipulation @cite . The set of relevant objects can often be extracted from demonstrations using visual cues, such as motionese @cite . The object selection problem is however distinct from the feature selection problem addressed in this paper, and not all of the features associated with an object will be relevant for generalizing a skill.
- Several works have investigated transfer and multi-task learning in the field of robotics @cite @cite @cite . These approaches often focus on transferring trajectories or controllers between different tasks. The different tasks are also often quite similar and share the same feature space, e.g., reaching for different locations in the task frame may be considered as different tasks. In our work, the tasks have distinct sets of features and the robot is learning a meta-level prior for transferring the relevance of features between skills. Meta features have previously been used to transfer knowledge about the relevance of features between tasks in applications such as predicting movie ratings, text classification, and object recognition @cite @cite @cite .
- Our feature generation process is based on decomposing objects into parts. The segmentation is guided by the demonstrations of the skill. Several other works have also used actions in order to improve segmentation performance @cite @cite @cite . However, most of these methods focus on segmenting scenes into objects rather than parts. Decomposing objects into parts has largely been based on geometric features, such as convexity @cite @cite @cite . recently proposed a method for segmenting objects into parts based on where it could be grasped by the robot @cite . Their approach uses an initial over-segmentation of an object model into supervoxels and merges these supervoxels into parts.
- The subject of automatically optimizing dialog policies is a hot topic, and many data-driven methods have been proposed among which RL-based ones are the most popular. There is some previous work on constraining the behavior of RL-based DM. In @cite Williams proposed to construct a hand-crafted DM and it produces a set of candidate actions for given dialog state, from which the best one is chosen by a POMDP-DM. Lison @cite proposed to use probabilistic rule' in specifying the transition and reward sub-models of the POMDP model. The probabilistic rules are human-readable and less parameterized than conventional probability distribution, thus reducing the free parameters of the POMDP model and allowing the system designers to make use of domain knowledge in designing DM. Our work bears some resemblance to @cite . But we used the dialog policy template to specify a policy directly and utilized GA to train the free parameters.
- proposed a hybrid learning method in @cite to learn a policy on an existing dialog corpus by combining the results of supervised and reinforcement learning. Pure RL on fixed dataset often shows irregular behavior due to the insufficient exploration problem. Supervised learning (SL) is used to mitigate the problem and the hybrid method shows better performance than pure SL or RL. In this regard the QVal fitness function is similar in spirit and the use of policy template can further constrain the DM behavior, thus is suitable for off-line on-corpus learning.
- In @cite , the authors present a compositional character model based on bidirectional LSTMs as a potential solution to these problems. A major benefit of this approach is that large word lookup tables can be compacted into character lookup tables and the compositional model scales to large data sets better than other state-of-the-art approaches. While @cite generate word embeddings from character representations, we propose to generate vector representations of entire tweets from characters in our tweet2vec model.
- Our work adds to the growing body of work showing the applicability of character models for a variety of NLP tasks such as Named Entity Recognition @cite , POS tagging @cite , text classification @cite and language modeling @cite @cite .
- Previously, @cite dealt with the problem of estimating rare word representations by building them from their constituent morphemes. While they show improved performance over word-based models, their approach requires a morpheme parser for preprocessing which may not perform well on noisy text like Twitter. Also the space of all morphemes, though smaller than the space of all words, is still large enough that modelling all morphemes is impractical.
- Hashtag prediction for social media has been addressed earlier, for example in @cite @cite . @cite also use a neural architecture, but compose text embeddings from a lookup table of words. They also show that the learned embeddings can generalize to an unrelated task of document recommendation, justifying the use of hashtags as supervision for learning text representations.
- Mostly close related to our method is @cite in which the authors used IMU sensor to get two rotation readings and to solve the unknown yaw angles by solving an 8-th degree polynomial; our experiments however shown that the method is sensitive to image noise and the reference direction noise. In @cite , proposed the start-of-the-art method to solve the problem by artfully select the angle-axis representation of rotation matrix, and this results the maximum 6 real solutions. A drawback is that when all Pl " u cker lines correspondences are from the same cameras in the first and second frames, numerical unstable problem happens when the rotation is small which is the usual case in real-world. We do not compare methods using Ackerman vehicle motion model, as this model is restrictive in practice, and a post-relaxation is often needed (c.f. @cite @cite ).
- Recently in the context of discontinuous Galerkin methods, hybrid meshes have been successfully used on acoustic wave equation problems @cite or on Maxwell equations @cite with significant speedups over tetrahedral meshes. For standard Galerkin methods, continuous finite element spaces for hybrid meshes have been introduced, such as in @cite . A succinct survey exposing various approaches with emphasis on the pyramidal element is available in the introduction of @cite . All these propositions involve special functions, such as rationals, as it is not possible to build a polynomial function basis on the pyramid which is conforming with tetrahedra and hexahedra polynomial function basis, as noticed in @cite .
- The idea of using non-conforming hexahedral-tetrahedral meshes is not new and has been successfully developed in the context of the discontinuous Galerkin Method in electromagnetic in @cite , @cite and @cite . For continuous Galerkin methods, constraints to ensure continuity of the divergence and of the rotational along non-conforming interfaces have been briefly proposed in @cite . In a engineering approach, hexahedra-tetrahedra non-conforming junction have been firstly discussed in @cite which proposes various multi-point constraints to ensure the function continuity or to minimize the error, depending of the finite element considered in their software. An extention of this approach @cite discards non-conforming hexahedral-tetrahedral junctions in favor of pyramidal elements. Our contribution is to give a formal approach to this problem and to deal with the geometric discontinuity arising with non-planar hexahedra faces, which was not considered in previous work to our knowledge.
- @cite has introduced a dataset of camera-captured document images. This dataset consist of @math pages with the text line information. In addition, ground truth text for each page is also provided. It is primarily developed for text line extraction and dewarping. It cannot be used for training of character recognition systems because there is no character, word, or line level text ground truth information available. @cite has proposed a dataset containing @math images of @math documents taken with different camera settings and focus. This dataset can be used for assessing the quality of images, e.g., sharpness score. However, it cannot be used for training of OCRs on camera-captured documents, as there is no character, word, or line level text ground truth information available. @cite has used a dataset of manually labeled documents which were submitted to Google for queries. However, the dataset is not publicly available, and therefore cannot be used for improving other systems.
- Similarly, @cite proposed automatic ground truth generation for OCR using robust branch and bound search (RAST) @cite . First, global alignment is estimated between the scanned and ground truth images. Then, local alignment is used to adapt the transformation parameters by aligning clusters of nearby connected components. @cite proposed an approach for ground truth generation of newspaper documents. It is based on synthetic data generated using an automatic layout generation system. The data are printed, degenerated, and scanned. Again, RAST is used to compute the transformation to align the ground truth to the scanned image. The focus of this approach is to create ground truth information for layout analysis.
- A second work which is related to ours appears in @cite , where the authors consider the problem of designing protocols for simultaneously optimizing storage performance as well as communication cost for updates. Contrary to our setting, the updates are modeled as insertions deletions in the file. A similarity with our work (like the work in @cite ) is that the destination node holds a coded form of the data (which can be considered as linear function of the uncoded data), and is interested in updating the coded data. The goal is to communicate and update the coded file in such a way that reconstruction repair properties in the storage system are preserved. Their protocol permits modifications to the structure of the storage code itself for optimizing the communication cost. Note that in our model we optimize the communication cost under the assumption that the destination is interested in the same function @math of the updated data as well.
- Maximally Recoverable Codes: The notion of maximal recoverability in linear codes was originally introduced in @cite . Low field size constructions of partial MDS codes (another name for MRCs with locality) for specific parameter sets codes appear in @cite , @cite , @cite . In the terminology of partial MDS codes, these three works respectively provide low field size constructions for the settings up to one, two and three global parities, and any number of local parities. Explicit constructions based on linearized polynomials for the case of one local parity and any number of global parities appear in @cite . Identifying low-field-size constructions of partial MDS codes for general parameter sets remain an open problem. Various approximations of partial MDS codes like Sector Disk codes @cite , @cite , @cite , STAIR codes @cite , and partial-maximally-recoverable-codes @cite have been considered in literature, and these permit low field size constructions for larger parameter sets. Other known results on maximally recoverable codes include expressions for the weight enumerators of MRCs with locality @cite , and the generalized Hamming weights (GHWs) of the MRSC @math in terms of the GHWs of the code @math @cite .
- Montanari and Richard @cite presented a @math -time algorithm that can certify that the optimal value of @math for a random @math -tensor is at most @math with high probability. Hopkins, Shi, and Steurer @cite improved it to @math with the same running time. They also asked how many levels of SoS are required to certify a bound of @math for @math .
- Our analysis asymptotically improves the aforementioned bound when @math is growing with @math , and we prove an essentially matching lower bound (but only for the case @math ). Secondly, we consider the case when @math is fixed, and give improved results for the performance of degree- @math SoS (for large @math ), thus answering in part, a question posed by Hopkins, Shi and Steurer @cite .
- Raghavendra, Rao, and Schramm @cite also prove results analogous to d-tensor:informal for the case of random polynomials (a model we do not consider in this work, and which appears to pose additional technical difficulties). This implied upper bounds for refuting random instances of constraint satisfaction problems using higher levels of the SoS hierarchy, which were shown to be tight via matching SoS lower bounds in @cite .
- * Results for worst-case tensors. It is proved in @cite that the @math -level SoS gives an @math approximation to @math in the case of arbitrary @math -tensors and an @math approximation to @math in the case of @math -tensors with non-negative entries (for technical reasons one can only approximate @math in the former case).
- The problem under study in the paper covers a wide range of applications. Several studies in the literature are closely related to this work. As previously discussed, Huber's robust detection problem can be treated as a special case of our problem. In @cite , Huber's test features a clipped version of the likelihood ratio test between the nominal densities that delivers performance which minimizes the worst-case probability of false alarm and miss. However, the clipped test has limited application since it requires parametric model for the nominal probability distributions. In @cite , the author provides a framework of the robust likelihood ratio test when the uncertainty region is described by the Kullback-Leibler divergence. In the parametric case, this problem is also known as the composite hypothesis testing problem [p. 169] levy2008principles . In this case, the generalized likelihood ratio test (GLRT) has optimal error performance when the probability distributions under all the hypotheses are in the same exponential family, c.f. [p. 204] levy2008principles , @cite . Again, this test requires complete parametric description of the probability distributions.
- Similar problems are also studied in the application of the signal detection in noise with uncertainty @cite or non-Gaussian noise @cite @cite . Noise uncertainty considers the case where the test designers do not know the noise statistics perfectly. For example, it might be known that the mean or the variance of the noise falls onto an interval but its exact value may be unknown. Non-Gaussian noise considers the case where the noise statistics is a mixture of Gaussian densities of various means and variances. The distribution function constraint can be applied whenever robustness is needed. Besides applications in signal processing, the proposed test has many practical applications, such as quality assurance in manufacturing, event forecasting, assessment of model fitting in finance, to name just a few.
- @cite and @cite showed how location-related data, using a GPS (Global Positioning System), can be used to determine the proximity of two NFC mobile phones. used a ten second window with location information collected every second, which was subsequently compared across various devices. The authors reported a high success rate in identifying the devices in close proximity to one another.
- @cite demonstrated the suitability of using ambient sound and light for proximity detection. Here, the authors analysed the sensor measurements -- collected for 2 and 30 seconds duration for light and audio respectively -- using a range of similarity comparison algorithms. Extensive experiments were performed in different physical locations, with a high success rate in detecting co-located devices.
- @cite used ambient temperature with an elliptic curve-based RFID and or NFC authentication protocol to determine whether two devices were co-located. Using this method, they were successful in establishing a secure channel; the proposal combines the timing channels in RFID, traditionally used in distance bounding protocols, in conjunction with ambient temperature. Their proposal, however, was not implemented and so there is no experimental data provided to evaluate its efficacy.
- @cite proposed the use of an accelerometer to provide assurance that the mobile phone is within the vicinity of the payment terminal. Their proposal requires the user to tap the payment terminal twice in succession, after which the sensor streams of the device and the payment terminal are compared for similarity. It is difficult to deduce the total time it took to complete one transaction in its entirety, but the authors have provided a sensor recording time range of 0.6--1.5 seconds.
- To overcome the unnecessary expansion in each iteration, @cite introduced the CCRP++ algorithm. The main advantage of CCRP++ is that it runs faster than CCRP. But the quality of solution is not good, because availability along a path may change between the times when paths are reserved and when they are actually used.
- The solutions produced by CCRP++ and QPER do not follow semantics of CCRP, i.e., the solution quality is not better than that of CCRP. Recently Gupta and Sarda @cite have given an algorithm called CCRP*, where evacuation plan is same as that of CCRP and it runs faster in practice. Instead of running Dijkstra's algorithm from scratch in each iteration, they resume it from the previous iteration.
- Deep learning architectures which have been recently proposed for the prediction of salient areas in images differ essentially by the quantity of convolution and pooling layers, by the input data, by pooling strategies, by the nature of the final classifiers and the loss functions to optimize, but also by the formulation of the problem. The attempt to predict visual attention reveals the binary classification problem of areas in images as "salient" and "non-salient". It corresponds to the visual experiment with free instructions, when the subjects are simply asked to look at the content. Shen @cite proposed a deep learning model to extract salient areas in images. It allows firstly to learn the relevant characteristics of the saliency of natural images, and secondly to predict the eye fixations on objects with semantic content. The proposed model is formed by three layer sequences of "filtering" and "pooling", followed by a layer of linear SVM classifier providing ranked "salient" or "non-salient" regions of the input image. With the filtering by sparse coding and the max pooling, this model approximates human gaze fixations.
- Classical text line segmentation algorithms are mostly based on image processing techniques and heuristics @cite @cite @cite @cite @cite @cite @cite . However, some methods were devised using statistical models and machine learing techniques such as hidden Markov models @cite , conditional random fields @cite , or neural networks @cite @cite @cite @cite . In our model, the line segmentation is performed implicitely and integrated in the neural network. The intermediate features are shared by the transcription and the segmentation models, and they are jointly trained to minimize the transcription error.
- In the field of computer vision, and particularly object detection and recognition, many neural architectures were proposed to both locate and recognize the objects, such as OverFeat @cite or spatial transformer networks @cite . Although systems are now able to detect multiple similar objects in a scene, most methods localize only one object, or several objects that are different. For scene text recognition, which is maybe the topic in computer vision closest to our problem, most systems still rely on a two-step process (localization, then recognition) @cite , even though some approaches jointly optimize character segmentation and word recognition @cite @cite @cite .
- Recently, many attention-based'' models were proposed to iteratively select in an encoded signal the relevant parts to make the next prediction. This paradigm, already suggested by Fukushima in 1987 @cite , was successfully applied to various problems such as machine translation @cite , image caption generation @cite @cite , speech recognition @cite @cite @cite , or cropped words in scene text @cite . In those works, the localization is implicitely performed inside the neural network.
- Other papers present similar methods to read short sequence of characters (mainly digits) with different implementations of the attention, e.g. DRAW @cite , RAM @cite , or recurrent spatial transformer networks @cite . We recently proposed an attention-based model to transcribe full paragraphs of handwritten text, which predicts each character in turn @cite .
- Outputing one token at a time turns out to be prohibitive in terms of memory and time consumption for full paragraphs, which typically contain about 500 characters. In the proposed system, the encoded image is not summarized as a single vector at each timestep, but as a sequence of vectors representing full text lines. It represents a huge speedup factor, and a comeback to the original MDLSTM-RNN architecture, in which the collapse layer is augmented with an MDLSTM attention network similar to the one presented in @cite .
- Information-centric networking (ICN) solutions have been proposed to convert networks into inherent content delivery systems @cite . Similarly, service-centric networking (SCN) Also known as service-oriented networking. @cite @cite @cite @cite @cite extends ICN principles to apply to services as well as content. Both ICN and SCN attempt to break away from statically binding to specific network resources. However, they only partly address the problems we have outlined in the specific cases of accessing content services: they do not naturally generalize to other scenarios, those involving switching of networks.
- The Experience-oriented network architecture (EONA) @cite is one in which application and content providers as well as infrastructure operators exchange information from their respective control loops to improve user experience. We take inspiration from EONA, but are concerned about the viability of its approach. In a world where data is the new oil, we can not imagine such cooperative exchange of information happening between parties with sometimes conflicting interests @cite . The authors do not provide a reasonable argument for how this would be realized.
- Closer to our proposal are recent efforts in the direction of enabling applications to express their requirements and allowing these to percolate down to the underlying network. Pyretic @cite is an open source Python framework that raises the level of abstraction of writing network policies, enabling the definition of sophisticated network structures through a high-level language. Merlin @cite is another declarative language that enables the specification of a global networking policy, which is expressed as a collection of logical predicates to identify traffic subsets and a set of statements indicating the action(s) to be taken on each subset. However, both Pyretic and Merlin focus on issues relating to unifying network administration rather than identifying and addressing application requirements.
- Other relevant efforts include: yanc @cite , an abstraction in Linux to facilitate network control logic to be written in any language; FlowOS @cite , a programming model to capture and process Internet flows; NOSIX @cite , an abstraction layer to enable portable deployments; and P4 @cite , a language to configure switches to process packets and match header fields.
- An important practical work is described by in @cite . An online forum for soldiers gradually changed its account model from anonymity with pseudonyms to full civil identity. analysed the different stages and found that removing anonymity options led to fewer antisocial comments and fewer comments in total. This work heavily influenced our analysis in selecting possible hypotheses.
- There is also a lot of literature describing factors influencing participation. Anonymity is there seldom a main focus, but it gets mentioned. An example for that is a main thread in the literature: The being used by as described in @cite . The theory sees two types of connection between the members of a community - Identity and Bond - influenced by different factors, , and for Identity and , and for Bond. Some of them can be linked to anonymity, SIDE theory does that explicitly with , as described in the introduction.
- There have been two trends in learning visual concept automatically. One relies on Internet search engines @cite @cite to collect images and train concept classifiers on those images. In particular, NEIL @cite uses a never-ending-learning algorithm to gather visual data from internet and build relationships between these concepts. However, the vocabulary dataset is fixed. LEVAN @cite discovers comprehensive vocabulary from Internet data and trains detectors for them using the images collected by search engine.
- The second trend discovers concepts from weakly labelled data. Specifically, VCD @cite proposes a concept mining and clustering algorithm based on visual and semantic similarity. However, it mainly focuses on general object concept learning. ConceptLearner @cite designs a scalable max-margin algorithm to discover and to learn visual concepts from weakly labeled image dataset @cite @cite . However, it lacks concept clustering.
- There is large literature on multimodal representation models. We only select a few representative systems to describe here. @cite adopts a concatenation strategy and use a convolutional neural network to extract features from images and skip-gram model for text. @cite constructs text feature vectors and image feature vectors separately and then mixes them by Singular Value Decomposition on their concatenation. @cite extends skip-gram model by taking visual information into account, which is called multimodal skip-gram. Image dispersion-based filtering in @cite improves multi-modal representations by approximating conceptual concreteness from images and filtering model input.
- The use of convolutional neural network (CNN) has brought huge improvement in action classification. @cite finetunes the CNN pre-trained on ImageNet and shows improvement over traditional methods. @cite designs a multi-task (person-detection, pose-estimation and action classification) model based on R-CNN. @cite develops a part-based approach by leveraging convolutional network features for action and attribute classification. They show top performance on PASCAL VOC human attribute and action classification. @cite develops an end-to-end deep convolutional neural network that utilizes contextual information of actions. Most previously leading methods in action classification for 2D images are based on part detectors. In particular, @cite trains action specific poselets and for each instance creates a poselet activation vector which is being classified using SVMs. In @cite , image regions are identified in different classes by using a dense sampling space and a random forest algorithm with discriminative classifiers. These works are all based on fully-supervised learning method and trained on fully-labeled data.
- A number of approaches, e.g., local metric learning and kernelized metric learning, have been suggested to learn multiple or nonlinear metrics from training data with complicated nonlinear manifold structure. In local metric learning, local metrics can be learned independently for each region or by considering the data manifold structure @cite @cite @cite . In kernelized metric learning, a kernel function is exploited to implicitly embed instances into the reproducing kernel Hilbert space (RKHS), and a Mahalanobis distance metric is then learned in the RKHS space @cite . Actually, kernelized method with Gaussian RBF kernel can also be treated as local learning approach. As pointed out in @cite , local learning are also shallow models, and generally are insufficient in coping with highly varying appearance and deformations of images. Another efficient local distance metric learning @cite was also proposed for classification and retrieval. To handle heterogeneous data, @cite propose a method using a random forest-based classifier to strengthen the distance function with implicit pairwise position dependence.
- On the other hand, deep convolutional models have been intensively studied and achieved extremely well performance. Compared with the multiple layer perceptron, CNN contains much less parameters to be learned, and can be efficiently trained using stochastic gradient descent. With the increasing of large scale training data and computational resources, deeper CNN and novel regularization methods had been developed, and deep CNN has gained great success in many visual recognition tasks, e.g., image classification @cite , object detection @cite , and scene labeling @cite .
- Despite the success of deep learning in variety of vision tasks, little studies were conducted on metric learning with deep architecture. @cite suggested a energy-based model (EBM) for discriminative similarity metric learning for image pairs. Stacked restricted Boltzmann machines (RBMs) had also been exploited to learn nonlinear transformation for data visualization and supervised embedding @cite . @cite proposed a deep nonlinear metric learning method by combining logistic regression and independent subspace analysis. @cite adopted the forward multi-layer neural network to learn deep metric for hand-crafted features. Compared with these approaches, the proposed DARI model considers the prominence of CNN in capturing salient and incorporates the Mahalanobis distance with the generated image features into one optimization target for distance metric and representation integration.
- One approach close to ours was proposed by @cite , which addresses the triplet-based similarity learning for image retrieval. However, our work have significant differences with that work. First, we derive our formulation from a novel angle, i.e. integrating feature learning and distance metric learning. Second, our learning method has advantage in the triplet generation and the batch-based gradient descent learning. Specifically, given @math training triplets containing @math distinct images ( @math ), their algorithm optimizes with @math forward and backward propagations, while only @math rounds is required for our approach because we derive to calculate the gradient over the images. Last, our deep architecture is specifically designed (only two conv layers are used) and we train our model from scratch, while they utilized the Alex's model @cite that is pre-trained on the ImageNet.
- One of most immediately similar variants is that where pattern variables are allowed. In contrast to the work here, these variables can act as substrings of arbitrary length. Keans and Pitt @cite give a family of polynomial time algorithms for learning the language generated by a single such pattern with a given number @math of pattern variables. Angluin @cite studies the inverse problem of generating a pattern, with a polynomial time algorithm for the case where the pattern contains a single pattern variable being the central result. We note that a central difference here is the repeated use of variables, allowing the same undefined substring to be repeated. The properties of these pattern languages have since been studied in some detail, far beyond the scope of this paper.
- Hermelin and Rozenberg introduce a further variant of the problem @cite , the problem. The input is a set of strings @math , which may include wildcard characters, and an integer @math . The goal is to find a string with hamming distance at most @math to each @math . The solution is required to have no wildcard characters. The examine a number of parameters: the length @math of the input strings, the number @math of input strings, @math , the number @math of characters in the alphabet, and the minimum number @math of wildcard characters in any input string. They show that the problem is in (with varying explicit running times) when parameterized by @math , @math , @math and @math . They also show that the special case where @math can be solved in polynomial time, whereas the problem is -hard for every @math .
- Bulteau @cite also give a survey of the parameterized complexity of a variety of more distantly related string problems, with similar multivariate parameterizations as in other work in this area. They cover, amongst others, , , , , , and .
- Introduced by Cannon and Cowen @cite , the problem is a geometric relative of where the input is two sets of points colored red and blue, with the goal of selecting a minimum set of blue points (centers) that cover'' the full set of blue points, in the sense that any blue point is closer to its nearest center than any red point. It is -hard with an @math -factor approximation algorithm, bearing a close similarity to .
- Proposals are quite important for object detection and diverse methods for object proposal generation are proposed. In case of detecting one particular category of near rigid objects (like faces or pedestrians) with fixed aspect ratio, sliding window mechanism is often used @cite @cite @cite . The main disadvantage is that the number of candidate windows can be about @math for an image, therefore limiting the complexity of classifier due to efficiency issues. When it comes to generating proposals covering general objects of various categories and in various shapes, sliding window approach becomes more computationally expensive.
- Many works are proposed to get more compact proposals, which can be divided into two types: the unsupervised grouping style and the supervised classification style. The most popular method in grouping style is the Selective Search @cite , which hierarchically groups super-pixels generated through @cite to form general object proposals. Other typical grouping style proposal methods include the EdgeBox @cite which is faster and MCG @cite which is more compact. With around 2000 proposals kept for each image, a recall rate of $98 In the supervised camp, the proposal generation problem is defined as a classification and or regression problem. Typical methods include the BING @cite and Multi-box @cite @cite . The BING uses the binary feature and SVM to efficiently classify objects from background. The Multi-box uses CNN to regress the object location in an end-to-end manner. A recently proposed promising solution is the Region Proposal Network (RPN) @cite , where a multi-task fully convolutional network is used to jointly estimate proposal location and assign each proposal with a confidence score. The number of proposals is also reduced to be less than 300 with higher recall rate. We use the RPN as the baseline proposal algorithm in CRAFT.
- Given object proposals, detection problem becomes an object classification task, which involves representation and classification. Browsing the history of computer vision, the feature representation is becoming more and more sophisticated, from hand-craft Haar @cite and HOG @cite to learning based CNN @cite . Built on top of these feature representations, carefully designed models can be incorporated. The two popular models are the Deformable Part Model (DPM @cite ) and the Bag of Words (BOW @cite @cite ). Given the feature representation, classifiers such as Boosting @cite and SVM @cite are commonly used. Structural SVM @cite @cite and its latent version @cite are widely used when the problem has a structural loss.
- In recent three years, with the revival of CNN @cite , CNN based representation achieves excellent performance in various computer vision tasks, including object recognition and detection. Current state-of-the-art is the R-CNN approach. The Region-CNN (R-CNN) @cite is the first to show that Selective Search region proposal and the CNN together can produce a large performance gain, where the CNN is pre-trained on large-scale datasets such as ImageNet to get robust feature representation and fine-tuned on target detection dataset. Fast R-CNN @cite improves the speed by sharing convolutions among different proposals @cite and boosts the performance by multi-task loss (region classification and box regression). @cite uses Region Proposal Network to directly predict the proposals and makes the whole pipeline even faster by sharing full-image convolutional features with the detection network. We use the Fast R-CNN as the baseline object classification model in CRAFT.
- Based on the two-step object detection framework, many works have been proposed to improve it. Some of them focus on the proposal part. @cite @cite find that using the CNN to shrink the proposals generated by grouping style proposals leads to performance gain. @cite @cite use CNN cascade to rank sliding windows or re-rank object proposals. CRAFT shares both similarities and differences with these methods. The common part is that we both the cascade'' strategy to further shrink the number of proposals and improve the proposal quality. The discrepancy is that those methods are based on sliding window or grouping style proposals, while ours is based on RPN which already has proposals of much better quality. We also show that RPN proposals and grouping style proposals are somewhat complementary to each other and they can be combined through our cascade structure.
- Some other works put the efforts in improving the detection network (R-CNN and Fast R-CNN are popular choices). @cite proposes the multi-region pipeline to capture fine-grained object representation. @cite introduces the Inside-Outside Net, which captures multi-scale representation by skip connections and incorporates image context via spatial recurrent units. These works can be regarded as learning better representation, while the learning objective is not changed. In CRAFT, we identify that current objective function in Fast R-CNN leads to flaws in the final detections, and address this by cascading another complementary objective function. In other words, works like @cite @cite that aim to learn better representation are orthogonal to our work.
- In a word, guided by the divide and conquer'' philosophy, we propose to further divide the two steps in current state-of-the-art object detection framework, and both tasks are improved considerably via a delicate design of network cascade. Our work is complementary to many other related works as well. Besides these improvements built on the two-step framework, there are also some works @cite @cite @cite on end-to-end detection framework that drops the proposal step. However, these methods work well under some constrained scenarios but the performance drops notably in general object detection in unconstrained environment.
- In addition to the pragmatics literature, the approach proposed in this paper relies extensively on recently developed tools for multimodal processing of language and unstructured representations like images. These includes both image retrieval models, which select an image from a collection given a textual description @cite , and neural conditional language models, which take a content representation and emit a string @cite .
- Software-based hardening techniques can be broadly divided into three categories: Thread-Level Redundancy (TLR) also called Redundant Multithreading (RMT), Process-Level Redundancy (PLR), and Instruction-Level Redundancy ( ). Redundant Multithreading (RMT). In RMT approaches @cite @cite , a hardened program spawns an additional thread for each original thread. At runtime, trailing threads are executed on separate spare cores or take advantage of the Simultaneous Multithreading (SMT) capabilities of modern CPUs. Similar to , RMT allows keeping only one memory state among replicas (assuming that memory is protected via ECC). However, RMT approaches heavily rely on the assumption of spare cores or unused SMT, which is commonly not the case in multithreaded environments where programs tend to use all available CPU cores.
- Process Level Redundancy (PLR). PLR implements the similar idea as RMT, but at the level of separate processes @cite @cite . In PLR, each process replica operates on its own memory state, and all processes synchronize on system calls. In multithreaded environments, allocating a separate memory state for each process raises a challenge of non-determinism because memory interleavings can result in discrepancies among processes and lead to false positives. Some PLR approaches resolve this challenge by enforcing deterministic multithreading @cite . PLR might incur a lower performance overhead than RMT but it still requires spare cores for efficient execution.
- Recent work on mainly concentrated on optimizations to trade-off fault coverage for lower overheads @cite @cite . In contrast to these new approaches, aims to utilize SIMD technology available on modern CPUs to achieve low performance overhead without compromising on fault coverage. A recent proposal has shown promising initial results when applying SIMD instructions to parallelize @cite . The scope of the work is however limited: (1) it only detects faults and does not provide recovery; (2) it only protects the floating-point unit; (3) it targets only single-threaded programs; and (4) hardening is performed manually at the level of the program's source code. In contrast, targets detection recovery of transient CPU faults for unmodified programs. Furthermore, protects the whole CPU execution including pointers, integers, and floating-point numbers.
- HAFT is a fault tolerance technique that couples with Hardware Transactional Memory (HTM) @cite . In this work, instructions are duplicated to provide fault detection, and an HTM mechanism roll-backs failed transactions to provide fault recovery. does not rely on a separate rollback mechanism, but rather masks faults using Triple Modular Redundancy.
- Triple Modular Redundancy (TMR) is a classical approach for achieving fault tolerance in mission-critical systems @cite . TMR detects faults by simple comparison of three replicas and performs fault recovery by majority voting, i.e., by detecting which replica differs from the other two and correcting its state. Consequently, it imposes an obvious restriction on the fault model: only one replica is assumed to be affected by the fault.
- While most of the software-based hardening techniques discussed above utilize only Dual Modular Redundancy (DMR), i.e., they can only detect but not correct faults, there are still a number of techniques based on TMR @cite @cite . In the context of , SWIFT-R @cite extends the fault detection mechanisms of SWIFT @cite by inserting three copies (instead of two) for each instruction and performing periodic majority voting to detect and correct faults. , in contrast, implements TMR without an increase in the number of instructions, since AVX registers are large enough to hold at least 4 copies of the data.
- Semantic parsing for open-domain question answering, which translates a question into a structured KB query, is a key component in question answering with a KB. While early approaches relied on building high-quality lexicons for domain-specific databases such as GeoQuery @cite , recent work has focused on building semantic parsing frameworks for general knowledge bases such as Freebase @cite @cite @cite @cite @cite .
- Semantic parsing frameworks for large-scale knowledge bases have to be able to successfully generate queries for the millions of entities and thousands of predicates in the KB, many of which are unseen during training. To address this issue, recent work relies on producing embeddings for predicates and entities in a KB based on their textual descriptions @cite @cite @cite @cite . A general interaction function can then be used to measure the semantic relevance of these embedded KB entries to the question and determine the most likely KB query.
- Most of these approaches use word-level embeddings to encode entities and predicates, and therefore might suffer from the out-of-vocabulary (OOV) problem when they encounter unseen words during test time. Consequently, they often rely on significant data augmentation from sources such as Paralex @cite , which contains 18 million question-paraphrase pairs scraped from WikiAnswers, to have sufficient examples for each word they encounter @cite @cite @cite .
- As opposed to word-level modeling, character-level modeling can be used to handle the OOV issue. While character-level modeling has not been applied to factoid question answering before, it has been successfully applied to information retrieval, machine translation, sentiment analysis, classification, and named entity recognition @cite @cite @cite @cite @cite @cite @cite @cite @cite . Moreover, demonstrate that gated-feedback LSTMs on top of character-level embeddings can capture long-term dependencies in language modeling.
- Lastly, encoder-decoder networks have been applied to many structured machine learning tasks. First introduced in , in an encoder-decoder network, a source sequence is first encoded with a recurrent neural network (RNN) into a fixed-length vector which intuitively captures its meaning , and then decoded into a desired target sequence. This approach and related memory-based or attention-based approaches have been successfully applied in diverse domains such as speech recognition, machine translation, image captioning, parsing, executing programs, and conversational dialogues @cite @cite @cite @cite @cite @cite @cite .
- In classical signal processing, a localized signal is constant over local connected regions separated by lower-dimensional boundaries. It is often related to concepts such as impulse function, step function, square wave and Haar basis @cite . Detecting localized signals has been considered through signal noise discrimination @cite , edge detection @cite , pattern matching @cite and support recovery of sparse signals @cite @cite , among others. We here look at the counterpart problem on graphs. A localized attribute (graph signal) is constant over a subgraph that is easily separated from the rest of the nodes. Similarly to localized signals in classical signal processing, a localized attribute emphasizes fast transitions (corresponding to boundaries) and localization in the graph vertex domain (corresponding to attributes that are nonzero in a local neighborhood).
- Our detection problem bears resemblance to many detection problems in the current graph-related literature, such as detecting a smooth graph signal or a localized graph signal under a specific noise model. For example, @cite @cite detects a cluster in a lattice graph that exhibits unusual behavior; @cite constructs a generalized likelihood test to detect smooth graph signals; @cite considers a general graph-structured normal means test; @cite considers combining data gathering and decision-making to design the quickest detection in the markov random field; @cite , constructs the uniform spanning tree wavelet statistic to approximate the epsilon scan statistic; and @cite @cite , considers the Lovasz extended scan statistic and spectral relaxation as relaxations of the combinatorial scan statistic.
- Our detection problem is also related to community detection, which, as one of the key topics in network science and graph mining, aims to extract tightly connected subgraphs in a network, also known as graph partitioning and graph clustering @cite @cite @cite . While the traditional community detection algorithms focus on the graph structure only @cite @cite , some recent studies tried to combine the knowledge of both graph structure and node attributes @cite as such attributes not only improve the accuracy of community detection, but also provide the interpretation of detected communities. However, as not all attributes are relevant for all communities, community detection accuracy may suffer. It is also computational inefficient to include a large number of attributes in the training phase @cite . We here aim to find useful attributes for improving community detection and aiding interpretation; as an example, in , the proposed statistics select useful keywords in a co-authorship network.
- There is little existing work on micro-video analysis, as the medium itself is new. Redi @cite explore the problem of finding creative micro-videos, inspired by similar studies of image quality assessment. Sano @cite analyze the problem of detecting loop micro-videos (that are designed to be played in a continuous six-second loop). Here we focus on analyzing general properties of micro-videos, with the explicit goal of constructing a new, large-scale benchmark for temporally-evolving tag prediction.
- treated MCTest as a structured prediction problem, searching for a latent answer-entailing structure connecting question, answer, and text. This structure corresponds to the best latent alignment of a hypothesis with appropriate snippets of the text. The process of (latently) selecting text snippets is related to the attention mechanisms typically used in deep networks designed for MC and machine translation @cite @cite @cite @cite . The model uses event and entity coreference links across sentences along with a host of other features. These include specifically trained word vectors for synonymy; antonymy and class-inclusion relations from external database sources; dependencies and semantic role labels. The model is trained using a latent structural SVM extended to a multitask setting, so that questions are first classified using a pretrained top-level classifier. This enables the system to use different processing strategies for different question categories. The model also combines question and answer into a well-formed statement using the rules of .
- investigated deep-learning approaches concurrently with the present work. They measured the performance of the Attentive Reader @cite and the Neural Reasoner @cite , both deep, end-to-end recurrent models with attention mechanisms, and also developed an attention-based convolutional network, the HABCNN. Their network operates on a hierarchy similar to our own, providing further evidence of the promise of hierarchical perspectives. Specifically, the HABCNN processes text at the sentence level and the snippet level, where the latter combines adjacent sentences (as we do through an @math -gram input). Embedding vectors for the question and the answer candidates are combined and encoded by a convolutional network. This encoding modulates attention over sentence and snippet encodings, followed by maxpooling to determine the best matches between question, answer, and text. As in the present work, matching scores are given by cosine similarity. The HABCNN also makes use of a question classifier.
- This work falls in the area of , which is an unsupervised learning problem @cite . Representation learning is concerned with building intermediate representations of data useful to solve machine learning tasks. It also involves @cite , as one applies and repurposes features that have been learned by solving the Jigsaw puzzle to other tasks such as object classification and detection. In our experiments we do so via the scheme, as in prior work @cite . Pre-training corresponds to the feature learning that we obtain with our Jigsaw puzzle solver. Fine-tuning is instead the process of updating the weights obtained during pre-training to solve another task (object classification or detection).
- There is a rich literature in unsupervised learning of visual representations @cite . Most techniques build representations by exploiting general-purpose priors such as smoothness, sharing of factors, factors organized hierarchically, belonging to a low-dimension manifold, temporal and spatial coherence, and sparsity. Unfortunately, a general criterion to design a visual representation is not available. Nonetheless, a natural choice is the goal of disentangling factors of variation. For example, several factors such as the object shapes, the object materials, and the light sources, combine to create complex effects such as shadows, shading, color patterns and reflections in images. Ideal features would separate each of these factors so that other learning tasks ( , classification based on just shape or surface materials) can be handled more easily. In this work we design features to separate the appearance from the arrangement (geometry) of parts of objects.
- If the data structure suggests that data points might concentrate around a manifold, then techniques can be employed @cite @cite . This representation allows to map directly smooth variations of the factors to smooth variations of the observations. Some of the issues with manifold learning techniques are that they might require computing nearest neighbors (which scales quadratically with the number of samples) and that they need a sufficiently high density of samples around the manifold (and this becomes more difficult to achieve with high-dimensional manifolds).
- In the context of Computer Vision, it is worth mentioning some early work on unsupervised learning of models for classification. For instance @cite @cite introduced methods to build a probabilistic representation of objects as constellations of parts. A limitation is the high computational complexity of these models. As we will see later, training the Jigsaw puzzle solver also amounts to building a model of both appearance and configuration of the parts.
- This learning strategy is a recent variation on the unsupervised learning theme that exploits labeling that comes for free'' with the data @cite @cite @cite . We make a distinction between labels that are easily accessible and are associated with a non-visual signal (for example, ego-motion @cite , but also one could consider audio, text and so on), and labels that are obtained from the structure of the data @cite @cite . Our work relates to the latter case as we simply re-use the input images and exploit the pixel arrangement as a label.
- Doersch al @cite train a convolutional network to classify the relative position between two image patches. One tile is kept in the middle of a @math grid and the other tile can be placed in any of the other @math available locations (up to some small random shift). In Fig. (c) we show an example where the relative location between the central tile and the top-left and top-middle tiles is ambiguous. In contrast, the Jigsaw puzzle problem is solved by observing all the tiles at the same time. This allows the trained network to intersect all ambiguity sets and possibly reduce them to a singleton.
- There is also a sizeable literature on solving Jigsaw puzzles computationally (see, for example, @cite @cite @cite ). However, these methods rely on the shape of the tiles or on texture especially in the proximity of the borders of the tiles. These are cues that we avoid when training the Jigsaw puzzle solver, as they do not carry useful information when learning a part detector.
- Recently, a new data initialization technique for training convolutional neural networks by Kr " a henb " u hl al @cite has been applied to @cite @cite @cite with a remarkable increase in performance for object detection on PASCAL VOC 2007. The performance of the method of Doersch al @cite gains considerably with this data initialization method, and goes from @math 4 @math 2.5 @math 1 @math 10$ hours).
- Inspired by statistical machine translation, @cite built a model in which the encoder RNN for source sentences is replaced by CNN features of images. LSTM was employed as a generative RNN of non-linear function. This workflow of feature extraction using CNN followed by caption generation using LSTM builds a foundation upon which most recent image captioning works are based.
- @cite took a similar workflow, but introduced attention-based model using standard back-propagation techniques, which learns to update the saliency while generating corresponding words.
- @cite expanded the CNN-LSTM architecture to activity recognition and video recognition by building long-term recurrent convolutional networks (LRCNs). Time-varying inputs are processed by CNN whose outputs are fed to a stack of LSTMs.
- @cite took a more linguistically inspired approach by training visual detectors for words with multiple instance learning, which learns to extract nouns, verbs, and adjectives from regions in the image. Maximum-entropy language model generates a set of candidates, which are re-ranked by sentence-level features and deep multimodal similarity model. Our model differs from this work in that we explicitly take local objects into consideration rather than approximating from multiple instances.
- Most of the above-mentioned works have represented images with CNN features extracted from the whole image, usually without paying explicit attention to local objects. In that regard, @cite show similar motivation with our work. They introduced DenseCap, which attempts to localize the regions in the image and incorporate it into captioning. However, their dense localization layer is trained on a dataset whose construction process is highly costly, with manual box-setting and labelling on crowdsourcing. Our method does not involve any manual labelling, and can work with any existing dataset.
- @cite exploited multimodal RNN to generate descriptions of image regions, aided by the alignment model of CNN over image regions and bidirectional RNN over sentences, which are intermingled via a multimodal embedding. This model also does not explicitly take local regions into representation, and instead relies on region convolutional neural network (RCNN) @cite to detect objects. However, since RCNN model is fine-tuned on only 200 classes of objects on PASCAL dataset @cite , it frequently fails to detect objects present in the image if those objects are not in the object classes of the dataset. Figure 2 shows examples of object detection using RCNN. On the other hand, since our model relies on selective search for object detection and region proposal, it is not limited by the number of object classes in the training dataset.
- As a method to approximate global non-invariant geometric statistics, @cite introduced spatial pyramid matching technique, a simple extension of bag-of-features representation, in which histograms for local features are aggregated in each sub-region.
- Although spatial pyramid can find useful global features from each level, it has been reported to be weak at high geometric variability, necessitating a combination with invariant features. On the other hand, VLAD coding is usually performed on locally invariable descriptors, such as SIFT, yet it does not preserve spatial information. In order to compensate for these mutual weaknesses, @cite introduced spatial pyramid VLAD, which combines the two. This technique will play a central role in the method introduced in our paper.
- @cite showed a far simpler approach for taking spatial information into account, by simply incorporating the coordinate information into the feature vector and augmenting it. We will also examine this approach and compare it to our model in Section 4.
- Some previous works @cite @cite @cite have applied similar methods to ours by extracting deep activation features from local patches at multiple scales, and coding them with VLAD or Fisher Vector. However, previous works mainly dealt with scene classification, texture recognition, and object classification, in which the necessity for explicitly dealing with local objects and spatial information is less pronounced. On the contrary, image captioning task requires that local objects be very clearly reflected in the captions, which we manage by novel application of spatial pyramid matching to region-level CNN features. To the best of our knowledge, our work is the first to apply such workflow to image captioning task.
- Previous pose estimation works can be divided into two groups. The first is to model the geometrical distribution of body joints @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite which can be viewed as post processing on detection score maps and prediction labels. They are mainly based on hand-crafted features. The Pictorial Structure Model @cite defined pairwise terms to represent relationship between body joint locations. Later, Yang al @cite proposed the flexible mixture-of-parts model to combine part detection results with a tree-structured model, which provided simple and exact inference. Nevertheless, it is believed that the tree-structured model is oversimplified''. In light of this, many works introduced more complex structures, and researchers have obtained improvement in performance. Loopy structure @cite , latent variable @cite , poselet @cite @cite and strong appearance @cite modeled structural information at different levels. They investigated different structures to model the spatial constraints among body joints on score maps. In our work, a bi-directional tree is used to model the correlation among feature maps. In the future, the investigations on structures in previous works can be incorporated in our framework to guide the message passing at the feature level.
- The second group focus on more powerful feature generators such as ConvNets @cite @cite @cite @cite @cite @cite @cite . The use of deep models brings large progress @cite @cite . DeepPose @cite used ConvNet to regress joint locations with multiple steps. Chen al @cite used ConvNet features and built up image-dependent pairwise relations to measure relationship among body joints. Fan al @cite combined local and global features to jointly predict joint locations. Tompson al @cite @cite implemented the multi-resolution deep model and Markov random field within an end-to-end joint training framework. Carreira and Malik @cite proposed to build up dependency among input and output spaces. In order to iteratively refine prediction results, they concatenated the body joint location predictions at the previous steps with the image as the input of current step. However, existing ConvNet models either learned the pair-wise relationship among body joints from score maps or did not learn pair-wise relationship. Learning relationship among parts at the feature level was not investigated.
- Semantic segmentation methods @cite @cite @cite @cite @cite @cite produce dense scene labels by running a network in a fully-convolutional manner over an input image, training with a per-pixel classification loss. @cite moves beyond per-pixel losses by framing CRF inference as a recurrent layer trained jointly with the rest of the network. The architecture of our transformation networks are inspired by @cite and @cite , which use in-network downsampling to reduce the spatial extent of feature maps followed by in-network upsampling to produce the final output image.
- A number of recent papers have used optimization to generate images where the objective is perceptual, depending on high-level features extracted from a convolutional network. Images can be generated to maximize class prediction scores @cite @cite or individual features @cite in order to understand the functions encoded in trained networks. Similar optimization techniques can also be used to generate high-confidence fooling images @cite @cite .
- Mahendran and Vedaldi @cite invert features from convolutional networks by minimizing a feature reconstruction loss in order to understand the image information retained by different network layers; similar methods had previously been used to invert local binary descriptors @cite and HOG features @cite .
- The work of Dosovitskiy and Brox @cite is particularly relevant to ours, as they train a feed-forward neural network to invert convolutional features, quickly approximating a solution to the optimization problem posed by @cite . However, their feed-forward network is trained with a per-pixel reconstruction loss, while our networks directly optimize the feature reconstruction loss of @cite .
- Gatys al @cite perform artistic style transfer, combining the of one image with the of another by jointly minimizing the feature reconstruction loss of @cite and a also based on features extracted from a pretrained convolutional network; a similar method had previously been used for texture synthesis @cite . Their method produces high-quality results, but is computationally expensive since each step of the optimization problem requires a forward and backward pass through the pretrained network. To overcome this computational burden, we train a feed-forward network to quickly approximate solutions to their optimization problem.
- Image super-resolution is a classic problem for which a wide variety of techniques have been developed. Yang al @cite provide an exhaustive evaluation of the prevailing techniques prior to the widespread adoption of convolutional neural networks. They group super-resolution techniques into prediction-based methods (bilinear, bicubic, Lanczos, @cite ), edge-based methods @cite @cite , statistical methods @cite @cite @cite , patch-based methods @cite @cite @cite @cite @cite @cite @cite @cite , and sparse dictionary methods @cite @cite . Recently @cite achieved excellent performance on single-image super-resolution using a three-layer convolutional neural network trained with a per-pixel Euclidean loss. Other recent state-of-the-art methods include @cite @cite @cite .
- Concurrent to this work, very similar results to ours are presented in a recent article by Kaplan, Leurent, Leverrier, and Naya-Plasencia @cite . While there has been an initial exchange of ideas in early stages of the work, the communication stopped after a while and our respective results have been obtained independently.
- One difference between our work and @cite is that we use different ways to handle the case in which Simon's promise is not fully satisfied, that is when there is an @math such that @math for all inputs @math , but there can be unwanted collisions @math where @math . These unwanted collisions may increase the number of runs of Simon's subroutine needed to obtain an independent system of equations. We observe that this does not constitute a problem when it comes to distinguishing @math from a random function. Instead, the authors of @cite give a more general analysis of Simon's algorithm, and introduce a quantity to measure the number of the aforementioned unwanted collisions, and they show how to bound the number of needed repetitions of the subroutine in terms of this quantity.
- Another difference between our works is the variant of CBC-MAC which we analyze. In our attack, we focus on MACs for fixed-size messages. In particular, we manage to forge a tag for a message (with a chosen prefix) by querying the MAC with messages of the same length. In @cite , they consider a slight variant of the basic CBC-MAC which can handle arbitrary-size messages by using two separate keys. While we show how to forge a tag for a message in which each block is non-zero by querying only messages which have at least one block of zeros, what they do is showing how to forge a tag for a message which is strictly longer than all messages queried in their attack.
- Moreover, the results from @cite illustrate that Simon's algorithm can be used to break not only CBC-MAC, but a whole range of modes of operation for authentication and authenticated encryption. Furthermore, Simon's algorithm can be used to speed up , a cryptanalytic technique against classical symmetric cryptoschemes.
- The study of computational geometry problems under uncertainty is a relatively new topic, and has attracted a lot of attention; see @cite and @cite for two surveys. Different uncertainty models and related problems have been investigated in recent years. See @cite @cite @cite @cite @cite @cite @cite @cite for example. The unipoint multipoint uncertainty model, which we use in this paper, was first defined in @cite @cite , and has been applied in many recent papers. For instance, in @cite , studied the stochastic minimum spanning tree problem, and computed its expected length. investigated the most likely convex hull problem over uncertain data in @cite ; the similar topic was revisited by in @cite to compute the probability that a query point is inside the uncertain hull. In @cite , Suri and Verbeek studied the most likely Voronoi Diagram (LVD) in @math under the unipoint model, and the expected complexity of LVD was further improved by in @cite , who explored the stochastic line arrangement problem in @math . In @cite , proposed efficient algorithms for the most likely skyline problem in @math and gave NP-hardness results in higher dimensions.
- Recently, in @cite , de studied the separability problem given a set of bichromatic imprecise points in @math in a setting that each point is drawn from an imprecision region.
- Both supervised and unsupervised machine learning methods have been applied to document classification and indexing. A popular method for representing documents in supervised learning is the approach, which represents documents by the words they contain, disregarding the order in which they occur. Instead of single words, also sequences of words () can be used to represent a document. The words or n-grams can be weighted by different schemes such as term frequency or TF-IDF @cite . While TF-IDF is unable to capture the semantic structures in documents, methods such as @cite and @cite try to overcome this weakness. Recently, also encyclopedic background knowledge has been leveraged for representing documents. For example, @cite represents the meaning of documents as weighted vectors of Wikipedia-based concepts. While originally intended for computing semantic relatedness, it has also been applied successfully to text classification (e.g. @cite ).
- One commonly used method for unsupervised text classification and indexing are . Topic models are statistical models which aim to discover latent topics in documents. The simplest one is , which was introduced by @cite . A supervised version of LDA, , was later presented by Blei and McAuliffe @cite . , another supervised topic model, was introduced by @cite . Labeled LDA constrains the latent topics which are to be learned to the labels of the documents in the training dataset. Topic models have also been used to create features for training supervised classifiers @cite .
- An ontology is a @cite Ontologies have been used as background knowledge for semantic annotation of documents (e.g. @cite , @cite ), mostly in the context of the Semantic Web. @cite presented the Open Biomedical Annotator, which is an ontology-based Web service for annotating documents with biomedical ontology concepts. The annotation process consists of two main steps: The first step, produces direct annotations by matching textual meta-data of the documents to ontology concepts. In the second step, the set of direct annotations is by using semantic relations of the ontology and by using existing mappings to other ontologies. I plan to build upon and extend this approach by incorporating textual and structural encyclopedic background knowledge.
- have shown that making the input sequences reversed is effective in a French-to-English translation task, and the technique has also proven effective in translation tasks between other European language pairs @cite . All of the NMT models mentioned above are based on sequential encoders. To incorporate structural information into the NMT models, proposed to jointly learn structures inherent in source-side languages but did not report improvement of translation performance. These studies motivated us to investigate the role of syntactic structures explicitly given by existing syntactic parsers in the NMT models.
- The attention mechanism @cite has promoted NMT onto the next stage. It enables the NMT models to translate while aligning the target with the source. refined the attention model so that it can dynamically focus on local windows rather than the entire sentence. They also proposed a more effective attentional path in the calculation of ANMT models. Subsequently, several ANMT models have been proposed @cite @cite ; however, each model is based on the existing sequential attentional models and does not focus on a syntactic structure of languages.
- The problem of centralized bandwidth-efficient repair of multiple node failures in a DSS employing has previously been considered by @cite . However, they restrict themselves to only MDS codes and they show existence of such codes only in the asymptotic regime where node size (amount of data stored on a node) tends to infinity.
- The seminal work of LSH @cite sheds a light on fast image search with theoretic guarantee. In a typical pipeline of hash code based image search @cite @cite , a set of hashing functions are generated either in an unsupervised manner or learned by perfectly separating similar dissimilar data pairs on the training set. The latter is also called supervised hashing since it often judges the similarity of two samples according to their semantic labels. For an unseen image, it finds the most similar images in the database by efficiently comparing the corresponding hash codes. It can be accomplished in sub-linear time using hash buckets @cite . Representative methods in this line include Binary Reconstructive Embedding (BRE @cite ), Minimal Loss Hashing (MLH @cite ), Kernel-Based Supervised Hashing (KSH @cite ), CCA based Iterative Quantization (CCA-ITQ @cite ), FastHash @cite , Graph Cuts Coding (GCC @cite ), .
- Implicit Discourse Relation Recognition Due to the release of Penn Discourse Treebank @cite corpus, constantly increasing efforts are made for implicit DRR. Upon this corpus, exploit several linguistically informed features, such as polarity tags, modality and lexical features. further incorporate context words, word pairs as well as discourse parse information into their classifier. Following this direction, several more powerful features have been exploited: entities @cite , word embeddings @cite , Brown cluster pairs and co-reference patterns @cite . With these features, Park and Cardie perform feature set optimization for better feature combination.
- Different from feature engineering, predicting discourse connectives can indirectly help the relation classification @cite @cite . In addition, selecting explicit discourse instances that are similar to the implicit ones can enrich the training corpus for implicit DRR and gains improvement @cite @cite @cite @cite @cite . Very recently, neural network models have been also used for implicit DRR due to its capability for representation learning @cite @cite .
- Alquier and Biau @cite consider learning high dimensional single index models. They provide estimators of @math using PAC-Bayesian analysis, which relies on reversible jump MCMC, and is slow to converge even for moderately sized problems. @cite learns high dimensional single index models with simple sparsity assumptions on the weight vectors, while @cite provide methods to learn SIM's in the matrix factorization setting. While these are first steps towards learning high dimensional SIM's, our method can handle several types of structural assumptions, generalizing these approaches to several other structures in an elegant manner. Restricted versions of the SIM estimation problem with (structured) sparsity constraints have been considered in @cite @cite , where the authors are only interested in accurate parameter estimation and not prediction. Hence, in these works the proposed algorithms do not learn the transfer function. We finally comment that there is also related literature focused on how to query points in order to learn the SIM, such as @cite .
- The class of SIM belongs to a larger set of semi-parametric models called multiple index models @cite , which involves learning a sum of multiple @math and corresponding @math . Other semi-parametric models @cite @cite @cite where the model is a linear combination of functions of the form @math are also popular, but our restrictions on the transfer function allow us to use simple optimization methods to learn @math .
- For @math and when the distribution is uniform over the unit sphere @math it is very easy to see that the halving or bisection leads to @math . By using the same halving method, one can in principle extend the result to any dimension @math . To do so, we need to carefully construct the version space (i.e., the set of hypotheses consistent with the queries and outcomes) at each iteration and then find a query that halves the volume (in the uniform case) or the density (in the general case if the distribution is known) @cite . Finding such a query in high dimension is very challenging.
- One very successful approach that does not suffer from the aforementioned computational challenge is pool-based active learning @cite , where instead of ideally halving the space, effective approaximations are performed. Notable algorithms are @cite and query-by-committee (QBC) @cite . In fact, our problem is closely related to learning homogeneous linear separators under the uniform distribution in the pool-based setting. This problem is very well understood and there exist efficient pool-based algorithms @cite @cite @cite . In particular, @cite presented an efficient perceptron-based algorithm that achieve a near-optimal query complexity. Similar results can be obtained under log-concave distributions @cite . Most of the pool-based methods require to have access to @math number of unlabeled samples in each iteration or otherwise they perform very poorly @cite @cite . This means that in order to have exponential guarantee in terms of sample complexity, we need to grow the pool size exponentially fast (note that there is no need to store all of these points). Moreover, with a few exceptions @cite @cite pool-based learning of linear separators in the noisy setting has been much less studied and the dependency of sample complexity on noise is not very well understood.
- The advanatages of collecting performance data from real users with the Navigation Timing API as opposed to perform very limited tests from only a few vantage points are outlined in @cite . The author discusses the various metrics provided by the API and shows an example on how to gain insights into the performance experienced by group of users by analyzing the collected data. Wprof @cite is a tool that analyzes the dependencies of objects on a website and computes the critical path of the page load time. The authors ran tests for popular websites from their campus network using one machine (3 Ghz clock speed) and find that for their measurement setup computing time accounts for 35 very recent study @cite (published after the data collection for this paper had been completed) presents a platform to run mobile network measurement experiments. It presents crowd-sourced data collected via the Navigation Timing API by 80 users and conclude that web performance can significantly benefit from faster processors. Unlike our study the effect of the network performance is not considered, in fact even wifi and cellular networks are mixed together in the results.
- Salient object detection can be performed either in a bottom-up fashion using low-level features @cite @cite @cite @cite @cite @cite @cite @cite @cite or in a top-down fashion via the incorporation of high-level knowledge @cite @cite @cite @cite @cite @cite @cite . Since this paper is focused on visual saliency based on deep learning, we discuss relevant work in this context below.
- A variety of network decompositions have been developed to solve distributed computing problems. @cite introduced the notion of @math -decompositions where each component has diameter @math and the contracted graph is @math -colorable. They give a deterministic procedure to obtain a @math -decomposition, which can be used to deterministically compute a @math -coloring and MIS in @math rounds. Panconesi and Srinivasan @cite showed how to obtain a @math -decomposition, yielding @math -time algorithms for @math -coloring and MIS. Linial and Saks @cite gave a randomized algorithm for obtaining a @math -decomposition in @math rounds. @cite gave a randomized algorithm for obtaining @math -decompositions in constant rounds.
- @cite investigated distributed coloring where the number of colors used depends on the chromatic number @math . Their algorithm requires @math colors and running time of @math for graphs with @math and @math .
- More efficient algorithms for @math -coloring exist for very dense graphs, e.g. a deterministic O( @math ) algorithm for growth bounded graphs (e.g. unit disk graphs) @cite , as well as for many types of sparse graphs @cite @cite @cite , e.g. for graphs of low arboricity. @cite described a @math -coloring algorithm for locally-sparse graphs. We will extend this result to cover list-colorings as well.
- As we have discussed, the MIS problem and the coloring problems are related. An MIS can be computed in @math rounds deterministically @cite and in @math rounds randomly @cite . More recently, Ghaffari @cite reduced the randomized complexity of MIS to @math . Whether an MIS can be obtained in polylogarithmic deterministic time or sublogarithmic randomized time remain interesting open problems.
- A generalization of MIS, known as an , has also been considered. A ( @math )-ruling set @math is a set of vertices such that two nodes @math have distance at least @math and for any node @math there exists a node @math with distance at most @math @cite . MIS is a special case, namely a @math -ruling set. A number of papers @cite @cite @cite use ruling sets to compute colorings in different kinds of graphs. A ruling set can be viewed as defining a network decomposition, such that any component has diameter at least @math and at most @math .
- As in standard RL, model-based deep reinforcement learning methods have generally been more efficient @cite @cite @cite @cite @cite , while model-free algorithms tend to be more generally applicable but substantially slower @cite @cite . Combining model-based and model-free learning has been explored in several ways in the literature. The method closest to our imagination rollouts approach is Dyna-Q @cite , which uses simulated experience in a learned model to supplement real-world on-policy rollouts. As we show in our evaluation, using Dyna-Q style methods to accelerate model-free RL is very effective when the learned model perfectly matches the true model, but degrades rapidly as the model becomes worse. We demonstrate that using iteratively refitted local linear models achieves substantially better results with imagination rollouts than more complex neural network models. We hypothesize that this is likely due to the fact that the more expressive models themselves require substantially more data, and that otherwise efficient algorithms like Dyna-Q are vulnerable to poor model approximations.
- Snodgrass and Onta n ' o n ( @cite @cite ) have used tile-to-tile transitions for the placement of tiles. Because of this, they do not have the problems of the vertical slice approach, but this brings a host of other issues. Namely, it can be far too likely for the system to generate sequences of tiles that violate implicit semantic constraints, such as generating ground tiles at the top of the level. Multiple attempts were made to reduce such problems via learning different chains for varying heights, or learning a hierarchical Markov chain ( @cite ). We build on the tile-to-tile approach for the work we report here as it is better able to generalize over a larger collection of levels. With the column based approach it is very easy to imagine that a new level might have a column that has never been seen before and so the system will be unable to generalize, but it is much harder to imagine a single-tile-to-tile transition that has never been seen (unless it is one that should never be seen like an enemy embedded inside a pipe).
- Procedural generation via neural networks has most been focused on image and text generation. Both have typically been byproducts of trying to train a network for some sort of classification process. Google's Inceptionism ( @cite ) took the product of training an image classifier and manipulated images to better match the set classification class, such as dogs. Facebook's Eyescream had image generation as the goal ( @cite ) and used adversarial networks, one network trying to generate images and the other trying to determine if what it was seeing was generated or real.
- Text generation from neural networks has been done by numerous people for various goals such as translation ( @cite ), text classification ( @cite ), and even trying to determine the output of code simply by reading the source code ( @cite ) and then writing what it believes to be the output.
- The work of Graves ( @cite ) went beyond text generation and used sequences of stylus locations and pressure to learn calligraphic character writing. Similarly, the DRAW system ( @cite ) used LSTMs to generate address signs learned from Google street view by using an attentional system to determine where to focus and then using that as the sequencing of where to place a virtual paint brush and what color to paint, e.g. it learns a sequence of @math .
- In Table , we compare our result to other reported results using segmental CRFs as well as recent end-to-end systems. Previous state-of-the-art result using segmental CRFs on the TIMIT dataset is reported in @cite , where the first-pass decoding was used to prune the search space, and the second-pass was used to re-score the hypothesis using various features including neural network features. Besides, the ground-truth segmentation was used in @cite . We achieved considerably lower PER with first-pass decoding, despite the fact that our CRF was zeroth-order, and we did not use any language model. Furthermore, our results are also comparable to that from the CTC and attention-based RNN end-to-end systems. The accuracy of segmental RNNs may be further improved by using higher-order CRFs or incorporating a language model into the decode step, and using beam search to reduce the search error.
- Risk prediction models have been widely used in many domains, including health care @cite , student performance evaluation @cite , and accounting fraud detection @cite . However, urban fire risk prediction has received relatively less attention, despite its obvious importance.
- Assignment problems are central in the literature of economics and computer science; the literature on one-sided matchings dates back to the seminal paper by Hylland and Zeckhauser @cite and includes many very influential papers @cite @cite in economics as well as a rich recent literature in computer science @cite @cite @cite . Serial Dictatorships (or their randomized counterparts) have been in the focus of much of this literature, mainly due to their simplicity and the fragile nature of truthfulness, which makes it quite hard to construct more involved truthful mechanisms. In a celebrated result, Svensson @cite characterized a large class of truthful mechanisms by serial dictatorships. Random Serial Dictatorship has also been extensively studied @cite @cite and recently it was proven @cite that is asymptotically the best truthful mechanism for one-sided matchings under the general cardinal preference domain.
- Most closely related to our problem is the @cite @cite (also known as the minimum online metric bipartite matching). In particular, as we explain in Section of the Appendix, results about the greedy algorithm in the online transportation problem imply bounds for the facility assignment problem. However, contrary to @cite , our analysis is , i.e. our results involve no asymptotics. Furthermore, compared to the related result in @cite , we remark that our analysis is substantially different due to the use of linear programming; our primal-dual technique could be applicable for greedy assignment mechanisms on other resource augmentation settings, beyond the specific problem.
- Compared to the related result in @cite , we remark that our analysis is substantially different due to the use of linear programming. This technique for the analysis of purely combinatorial algorithms has found applications in many different contexts such as facility location @cite , set cover @cite , online matching @cite , maximum directed cut @cite , wavelength routing @cite , and revenue optimization @cite . Like in our case, these techniques usually lead to tight analysis. Also note that while the connection between SD and RSD and the greedy algorithm for the online transportation problem is straightforward, the two problems are fundamentally different and hence non-greedy online competitive algorithms do not imply any bounds for our setting and non-serial truthful mechanisms do not imply any bounds for the online setting.
- Finally, there is some resemblance between our problem and the facility location problem @cite that has been studied extensively in the literature of approximate mechanism design, in the sense that in both settings, agents specify their most preferred positions on a metric space. Note that the settings are fundamentally different however, since in the facility location problem, the task is to identify the appropriate point to locate a facility whereas in our setting, facilities are already in place and we are looking for an assignment of agents to them.
- Past work presented a method for repairing a digital dictionary in an XML format using a dictionary markup language called DML @cite . It remains time-consuming and error-prone however to have a human exhaustively read through and manually correct a digital version of a dictionary, even with languages such as DML available for making corrections once errors are detected. The methods we present in the current paper automatically scan through and detect errors in dictionaries. The methods in the current paper can be used in concert with error correction techniques such as dictionary markup languages.
- Previous approaches have been presented for detecting structural errors in digitized dictionaries @cite @cite . The method in @cite works by linearizing the lexicon structure, converting the opening tags in XML into tokens and then considering the likelihoods of various strings of tokens using a language modeling approach. Anomalous branches of XML tags are flagged as structural errors. The method ignores the underlying text data within the dictionary and only detects structural errors. The methods in @cite outperform the method from @cite . The methods in @cite use a mixture of unsupervised methods, supervised machine learning methods, and system combination approaches. The highest-performing method uses a random forest system combination approach. The methods in @cite only detect structural errors. Errors in the textual data content within the XML elements are not detected. In contrast, the current paper presents methods that detect errors in the text (data) content of XML elements. The methods in the current paper also use different approaches and different sources of information than were used in @cite @cite and do not require training data, which is often not available. The error detection methods in the current paper can be used in concert with structural error detection methods.
- The study of Electrodermal Activity signals, or EDA signals, dates back to the early @math century ( e.g., @cite ) with the observation of a connection between changes in user skin conductance and psychological state. In recent years, this connection has been validated by examining brain function via fMRI and skin conduction via EDA concurrently in @cite , and by showing the specific regions of the brain that correspond with EDA changes and video recordings of sweat glands in @cite . The promise of EDA as a window into user psychology resulted in extensive work on evaluating the connection between EDA and user interactions @cite , stress detection @cite , content and audience segmentation @cite , and reaction to video content @cite ---to name only a few.
- Applications using EDA signal analysis rely on the extraction of a user's fine-grained responses embedded in the EDA signal called Skin Conductance Responses, or SCRs. These SCRs measure the expulsion of sweat triggered by a user's spike-like stimulus responses, which we call SCR events. SCR events are not explicitly observed in the EDA signal; we observe only the SCRs, which can be modeled as the convolution of the SCR events with a distinguishing impulse response. Significant prior literature has focused both on how to model the SCR impulse response and extract the SCR events from the observed EDA signal. Examples include a parametric sigmoid-exponential model @cite , a bi-exponential impulse response @cite , nonnegative deconvolution @cite , and a variational Bayesian decomposition methodology @cite . These prior techniques are limited by either computational complexity @cite or overly simple models that ignore or heuristically remove additional EDA signal components, such as the SCL, that disguise the SCR events @cite @cite .
- The authors of @cite treat the SCL as a constant estimated by averaging the skin conductance signal over the time windows when the estimated SCR (by deconvolution) is below a certain amplitude. The work of @cite presented a methodology to extract relevant SCR events while considering the SCL signal, but their matching pursuit-based technique used only a rough heuristic to remove this additional signal by deleting the two coarsest-scale components of a discrete-cosine transform applied to the skin conductance.
- More recent work has incorporated SCL in a more principled manner into the EDA signal model. The sparse representation of SCR signal was exploited in @cite . In this work, the SCL signal was modeled as a slowly varying linear signal, and the SCR signal was modeled as a sparse linear combination of atoms of a dictionary containing time shifts of variety of function shapes. A greedy method exploiting the sparsity was also proposed for extracting the SCR events signal. Recently, the authors of @cite proposed an approach which exploited sparsity from a Bayesian perspective in which the SCL signal was modeled as a sum of cubic B-spline functions, an offset and a linear trend, whereas the SCR signal was modeled by a sparse signal in the dictionary obtained by shifts of bilinear transformations of a Bateman function. Following the maximum (MAP) estimation principle, a convex formulation was obtained which can be solved efficiently. In contrast to these works @cite @cite we propose a model for the baseline signal that incorporates shifts in skin conductance due to changes in the positioning of the sensors due to motion, which is crucial when data is collected using wearables.
- Linearized ADMM solvers. For the problem structure of our interest here, one of the most natural algorithms is the splitting method known as the Chambolle-Pock algorithm (also known as Primal-Dual Hybrid Gradient, Arrow-Hurwicz method, or linearized ADMM) @cite . While this algorithm can give rise to a duality gap, it is significantly less general compared to our framework. In each iteration, it requires a complete solution of the proximal operators of both @math and @math , which can be computationally expensive. Its convergence rate is sensitive to the used step-size @cite . Our framework is not algorithm-specific, and holds for arbitrary iterate sequences. More recently, the SPDC method @cite was proposed as a coordinate-wise variant of @cite , but only for strongly convex @math .
- Stochastic coordinate solvers. Coordinate descent ascent methods have become state-of-the-art for many machine learning problems @cite @cite . In recent years, theoretical convergence rate guarantees have been developed for the primal-only setting, e.g., by @cite @cite , as well as more recently also for primal-dual guarantees, see, e.g., @cite @cite @cite . The influential Stochastic Here 'stochastic' refers to randomized selection of the active coordinate. Dual Coordinate Ascent (SDCA) framework @cite was motivated by the @math -regularized SVM, where coordinate descent is very efficient on the dual SVM formulation, with every iteration only requiring access to a single datapoint (i.e. a column of the matrix @math in our setup). In contrast to primal stochastic gradient descent (SGD) methods, the SDCA algorithm family is often preferred as it is free of learning-rate parameters, and has a fast linear (geometric) convergence rate. SDCA and recent extensions @cite @cite @cite @cite @cite require @math to be strongly convex.
- Under the weaker assumption of weak strong convexity @cite , a linear rate for the primal-dual convergence of SDCA was recently shown by @cite .
- In the Lasso literature, a similar trend in terms of solvers has been observed recently, but with the roles of primal and dual problems reversed. For those problems, coordinate descent algorithms on the primal formulation have become the state-of-the-art, as in @cite and extensions @cite @cite @cite . Despite the prototypes of both problem types---SVM for the @math -regularized case and Lasso for the @math -regularized case---being closely related @cite , we are not aware of existing primal-dual guarantees for coordinate descent on unmodified @math -regularized problems.
- Distributed algorithms. For @math -problems exceeding the memory capacity of a single computer, a communication-efficient distributed scheme leveraging this Lipschitzing trick is presented in @cite @cite .
- The Fraunhofer Ultra Low Delay (ULD) codec @cite is one of the only full-bandwidth codecs with an algorithmic delay comparable to the proposed codec. Its structure, however, is completely different from that of the proposed codec. ULD is based on time-domain linear prediction instead of the MDCT. It uses a pre-filter post-filter combination, whose parameters are transmitted in the bitstream, to shape the quantization noise. ULD frames are 128 samples with 128 samples of look-ahead, for a total algorithmic delay of 256 samples at 48 kHz (5.3 ms). One disadvantage of the linear-prediction approach is the difficulty of resynchronizing the decoder after a packet is lost @cite . In contrast, the proposed codec only uses inter-frame prediction for @math , so the decoder resynchronizes very quickly after packet loss. Changing the proposed codec to have completely independent packets would cost approximately 12 bits per frame.
- AAC-LD @cite is another low-delay audio codec whose total algorithmic delay can range from 20 ms to around 50 ms, depending on the sampling rate and bit reservoir size. However, its complexity is higher than that of the proposed codec.
- SWAR @cite , SIMD Within A Register, is the most closely related work to our bitslice floating point vector computation. SWAR uses logic operations to implement integer operations. Partitioned operations are the main focus of most of the hardware support for SWAR. Instead of keeping the field size as the size of desirable data types, we use one single bit to hold one bit of a data element so that the number of elements processed in parallel is not decided by the width of data type but the max width of registers (including SIMD registers). Meanwhile, we use logic operations to implement the actual hardware logic for a single bit of the data element.
- Lowering energy consumption is one of the major benefits of approximate computing. Disciplined approximate programming asks programmers to specify which parts of a program can be computed approximately. The approximate computation thus reduces the energy cost. An ISA extension is put forward to provide approximate operations and storage @cite . With this extension, hardware has freedom to save energy at the cost of accuracy. Our customizable precision BFP vector types and related operations can serve as a software ISA for approximate computation.
- Some programs may not need the dynamic range or the precision of FP arithmetic. For these programs, it is a general design practice to translate the floating-point arithmetic into a suitable finite fixed point presentation @cite . However, some programs may still require 6 bits or more in the exponent to preserve a reasonable degree of accuracy. In other words, these applications need more than the typical 32 bits of precision that fixed point arithmetic offers. Therefore, support for small, irregularly sized floating point makes our bitslice vector types a perfect fit for this kind of application.
- In a few applications, the underlying networks are evolving all the time @cite @cite . Rather than re-computing from scratch, incremental algorithms are more desirable in graph analysis tasks on dynamic networks. Maintaining PageRank values of nodes on an evolving graph was studied in @cite @cite . Hayashi @cite proposed to utilize a sketch of all shortest paths to dynamically maintain the edge betweenness value. The dynamics considered by the above work is a stream of edge insertions deletions, which is not suitable for influence computation. The dynamics of influence network is more complicated, because besides edge insertions deletions, influence probabilities of edges may also evolve over time @cite .
- Aggarwal @cite explored how to find a set of nodes that has the highest influence within a time window @math . They modeled influence propagation as a non-linear system which is very different from triggering models like the Linear Threshold model or the Independent Cascade model. The algorithm in @cite is heuristic and the results produced do not come with any provable quality guarantee.
- Chen @cite investigated incrementally updating the seed set for influence maximization under the Independent Cascade model. They proposed an algorithm which utilizes the seed set mined from the former network snapshot to efficiently find the seed set of the current snapshot. An Upper Bound Interchange heuristic is applied in the algorithm. However, the algorithm in @cite is costly in processing updates, since updating the Upper Bound vector for filtering non-influential nodes takes @math time where @math is the number of edges. Moreover, the SP1M heuristic @cite , which does not have any approximation quality guarantee, was adopted in @cite for estimating influence spread of nodes. Thus, the set of influential nodes, even when the size of the seed set is set to @math , does not have any provable quality guarantee.
- The problem of incorporation of contextual information has been an active research topic in computer vision @cite @cite @cite @cite @cite . To some extent, probabilistic graphical models address this issue in semantic segmentation and can be either a) used as a separate post-processing step @cite or b) trained end-to-end with CNNs @cite @cite @cite . In setting a) graphical models are unable to refine the parameters of the CNN and thus the errors from the CNN will essentially be presented during post-processing. On the other hand, in b) one need to carefully design the inference part in terms of usual neural networks operations, and it still relies on computing high-dimensional Gaussian kernels @cite . Besides that, Yu and Koltun @cite have recently shown that dilated convolution filters are generally applicable and allow to increase the contextual capacity of the network as well.
- In terms of improving the deconvolutional part of the network for dense predictions, there has been a prevalence of using information from lower layers: so-called Skip Architecture' @cite @cite and Multi-scale @cite are two notable examples. Noh al @cite proposed to train a separate deconvolutional network to effectively decode information from the original fully convolutional model. While these methods have given better results, all of them contain significantly more parameters than the corresponding baseline models.
- In a SMV setting, full support recovery guarantees for OMP with bounded noise signals as well as with Gaussian noises have been proposed in @cite . This work also provides criteria on the stopping criteria to guarantee that OMP terminates after having picked all the correct atoms. This contribution has then been slightly refined in @cite to provide conditions independent of the particular support that is to be recovered.
- Gribonval have investigated the performance of SOMP for a problem resembling ours in @cite . Their contribution has been to provide a lower bound on the probability of correct full support recovery when the signal to be estimated is sparse and its non-zero entries are statistically independent mean-zero Gaussian random variables.
- Face replacement for image and video can be roughly divided into two categories. A first category is @cite @cite @cite @cite @cite @cite , which aims to transfer expressions and emotions of a user (puppeteer) to a virtual character (puppet). Such methods are used to animate digital avatars in games, movies and video conferences. methods @cite @cite @cite @cite @cite @cite , on the other hand, try to exchange two faces in different images or videos such that the replaced result looks sufficiently realistic. Swapping different faces is useful for online identity protection, while swapping the same face (or parts of it) between different videos is interesting for dubbing, retargeting and video montage. At the intersection of both categories lies @cite , which replaces an actor's face by swapping it with that of a user, while at the same time preserving the actor's facial expressions and emotions. Here, the original facial performance needs to be accurately emulated (puppetry), and the new face with different identity needs to be inserted as naturally as possible in the original video (swapping).
- A first type of methods tracks a morphable 3D model of the face that parameterizes identity, facial expressions and other nuances. Such systems can produce accurate 3D textured meshes and can establish a one-to-one expression mapping between source user and target actor, thereby simplifying and speeding up expression transfer. The generation of such a model, however, can be time consuming and is either done by learning a detailed 3D multilinear model from example data spanning a large variety of identities and expressions @cite @cite , or by purposely building a person-specific blend shape model from scans of a specific actor using specialized hardware @cite @cite @cite @cite @cite . Moreover, the difficulty of stably tracking a 3D model over time generally necessitates a fair amount of manual interaction.
- The standard Frank-Wolfe algorithm: for some appropriate chosen @math requires @math iteration without additional conditions . In a recent paper, give a variant that requires @math iterations when @math is strongly convex and smooth, and @math is a polytope See also recent follow up work @cite . . Although the dependence on @math is much better, the geometric constant @math depends on the polyhedral set and can be very large. Moreover, each iteration of the algorithm requires further computation besides the linear optimization step.
- Stochastic Condition Gradient Sliding (SCGS), recently proposed by @cite , uses Nesterov's acceleration technique to speed up Frank-Wolfe. Without strong convexity, SCGS needs @math stochastic gradients, improving SFW. With strong convexity, this number can even be improved to @math . In both cases, the number of linear optimization steps is @math . The key idea of our algorithms is to combine the variance reduction technique proposed in with some of the above-mentioned algorithms. For example, our algorithm SVRF combines this technique with SFW, also improving the number of stochastic gradients from @math to @math , but without any extra conditions (such as Lipschitzness required for SCGS). More importantly, despite having seemingly same convergence rate, SVRF substantially outperforms SCGS empirically (see ).
- In contrast to using such hand-designed pipelines we provide a novel end-to-end trainable solution allowing the robot to automatically learn an appropriate belief state representation as well as the corresponding predict and update operations for an environment containing multiple objects with different appearance and potentially very complex behaviour. While we are not the first to leverage the expressive power of neural networks in the context of tracking, prior art in this domain primarily concerned only the detection part of the tracking pipeline @cite , @cite .
- The final field of related work is planning for reward cost optimization. We only focus on the literature on Monte-Carlo (MC) sampling based online POMDP solvers since this approach allows significant scale-up @cite . The POMCP solver @cite uses Monte-Carlo UCT tree search in online POMDP planning. Also, Somani et. al. @cite present the DESPOT algorithm, that improves the worst case performance of POMCP. Our initial experiments with POMCP and DESPOT showed that they run out of memory on even our small sized networks. A recent paper @cite introduced PSINET-W, a MC sampling based online POMDP planner. We have discussed PSINET's shortcomings in Section and how we remedy them. In particular, as we show later, HEALER scales up whereas PSINET fails to do so. ) . Further, a recent paper @cite looks at an extension of the same problem by considering the case that not all nodes in the network are known ahead of time (as opposed to our work where we only assume that some edges are not known ahead of time). However, unlike our work, they do not consider sequential selection of node subsets.
- Historically, automated machine learning pipeline optimization has focused on optimizing specific elements of the pipeline @cite . For example, grid search is the most commonly-used form of hyperparameter optimization, where practitioners apply brute force to explore a broad ranged sweep of model parameter combinations in search of the parameter set that allows the machine learning model to perform best. Recent research has shown that randomly exploring parameter sets within the grid search often discovers high-performing parameter sets faster than an exhaustive search @cite , suggesting that there is promise for intelligent search in the hyperparameter space. Bayesian optimization of model hyperparameters, in particular, has been effective in this realm and has even outperformed manual hyperparameter tuning by expert practitioners @cite .
- @cite and @cite showed how location-related data, namely using GPS (Global Positioning System), can be used to determine the proximity of two NFC mobile phones. use a ten second window with location information collected every second, which was subsequently compared across various devices. The authors report a high success rate in identifying the devices in close proximity to one another.
- @cite demonstrated the suitability of using ambient sound and light for proximity detection. Here, the authors analysed the sensor measurements -- collected for 2 and 30 seconds duration for light and audio respectively -- using a range of similarity comparison algorithms. Extensive experiments were performed in different physical locations, with a high success rate of detecting co-located devices.
- @cite based their proximity detection mechanism on the shared radio environment of devices -- the presence of WiFi access points and associated signal strengths -- using the application scenario of secure device pairing. In this work, they considered this approach to produce low error rates, recommending it as a proximity detection mechanism. While their paper did not focus on NFC-based mobile transactions, their techniques and methodology may still be applicable.
- @cite use ambient temperature with an elliptic curve-based RFID NFC authentication protocol to determine whether two devices are co-located. Using this method, they were successful in establishing a secure channel; the proposal combines the timing channels in RFID, traditionally used in distance bounding protocols, in conjunction with ambient temperature. Their proposal, however, was not implemented and so there is no experimental data provided to evaluate its efficacy.
- This work is different in scope in that it tries to deliver general-purpose location descriptors by modelling the distribution of demographics and localities affiliated with a location in a spatially smooth manner. @cite follow a similar aim, in the sense that they, as well, rely on usage information to describe urban spaces. There are, however, some key differences. While the authors categorize whole cities into 4 types, our method is able to discover intra-city patterns of neighborhoods or streets and employs a more fine-grained location taxonomy, permitting its use for the aim of semantic place description.
- Datta et.al. @cite first casted the image aesthetics assessment problem as a classification or regression problem. A given image is mapped to an aesthetic rating, which is usually collected from multiple subject raters. The rating is normally quantized with discrete values. The earliest work @cite , @cite extracted various handcrafted features, including low-level image statistics such as distributions of edges and color histograms, and high-level photographic rules such as the rule of thirds. A part of subsequent efforts, such as @cite , @cite , @cite , focus on improving the quality of those features. Generic image features @cite , such as SIFT and Fisher Vector @cite , have also been applied to predict aesthetics. However, empirical features cannot accurately and exhaustively represent the aesthetic properties.
- The human brain transforms and synthesizes a torrent of complex and ambiguous sensory information into coherent thought and decisions. Most aesthetic assessment methods adopt simple linear classifiers to categorize the input features, which is obviously oversimplified. Deep networks @cite attempt to emulate the underlying complex neural mechanisms of human perception, and display the ability to describe image content from the primitive level (low-level features) to the abstract level (high-level features). They are composed of multiple non-linear transformations to yield more abstract and descriptive embedding representations. The RAPID model @cite is among the first to apply deep convolutional neural networks (CNN) @cite to the aesthetics rating prediction, where the features are automatically learned. They further improved the model by exploring style annotations @cite associated with images. In fact, even the hidden activations from a generic CNN proved to work reasonably well for aesthetics features @cite .
- Most current work treat aesthetics assessment as a conventional classification problem: the user ratings of each photo are transformed into a ordinal scalar rating (by averaging, etc.), which is taken as the label of the photo. For example, RAPID @cite simply divided all samples as aesthetic or unaesthetic, and trained a binary classification model. However, it is common for different users to rate visual subjects inconsistently or even oppositely due to the subjective problem nature @cite . Since human aesthetic assessment depends on multiple dimensions such as composition, colorfulness, or even emotion @cite , it is difficult for individuals to reliably convert their experiences to a single rating, resulting in noisy estimates of real aesthetic responses. In @cite , Wu et.al. first proposed to represent each photo's rating as a distribution vector over basic ratings, constituting a structural regression problem. Gao et.al. @cite formulated the aesthetic assessment as a multi-label task, where multiple aesthetic attributes were predicted jointly via bayesian networks.
- This work relies on smoothing the objective function by convolving it with the Gaussian kernel. We have previously shown that this particular form of smoothing is optimal in a certain sense, by relating Gaussian convolution to a relaxation of the convex envelope. Although connection to the convex envelope is meaningful in the context of c nonconvex objective functions, there are side benefits in smoothing even when the objective function is convex. For example, smoothing a nonsmooth c convex objective function by convolution can improve the convergence rate of stochastic optimization algorithms @cite .
- As discussed in Section , smoothing can be considered as means to inject noise into the training process. The idea of noise injection is already used in methods such as SGD or dropout @cite in order to improve learning. The key advantage of our framework for noise injection, however, is that the noise injection can be achieved in closed form and without need of sampling. In order words, we can compute the effect of infinitely many noisified objective functions in closed form. This is similar to the idea of Marginalized Denoising Autoencoders (mDA) @cite , where the effect of infinitely many nosified inputs is marginalized to obtain a closed form expression. However, mDA limits the form of the injected noise. Specifically, the marginalized effect is only computable in a c linear reconstruction setup (nonlinearity is applied only after computation of the marginalized reconstruction). In addition, mDA performs noise injection layer by layer in a greedy fashion. In contrast, our framework is able to compute closed form expression for the entire deep network and allowing full nonconvexity of the associated optimization, up to reasonable approximation.
- A closely related work to ours is Annealed Gradient Descent @cite , where the objective landscape is also initially approximated by a smoother function and is gradually transformed to the original one. However, the unlike this work where Gaussian smoothing is theoretically motivated for nonconvex optimization @cite , in @cite coarse-to-fine approximation of the objective function is based on heuristically motivated procedure. More precisely, the latter uses vector quantization methods in order to generate a code book by which the coarse cost function is approximated. Another difference between these two works is that the representation of the smoothed function in our framework is simpler, as we directly obtain a closed form expression of the objective function. that is a simpler setup than approximation by codebook generation.
- Very recently, the diffusion process has been proposed for learning difficult probability distributions @cite . In forward time diffusion, the method converts any complex distribution into a simple distribution, e.g., Gaussian. It then learns the reverse-time of this diffusion process to define a generative model distribution. By sampling from such trained model, the authors have achieved inpainting of missing regions in natural images.
- The proposed HSCFVC combines the idea of supervised coding and FVC. Here we briefly review the work of supervised coding and the attempts to combine it with FVC. Using discriminative information to create an image representation is a popular idea in image classification. For example, label-supervised information has been used to learn discriminative codebooks for encoding local features @cite @cite @cite @cite @cite , either by using a separated codebook learning step @cite @cite or in an end-to-end fashion @cite @cite . Supervision information has also been applied to discover a set of middle-level discriminative patches @cite @cite @cite to train some patch detectors which are essentially local feature encoders. A CNN can also be seen as a special case of supervised coding methods if we view the responses of the filter bank in a convolutional layer as the coding vector of convolutional activations of the previous layer. From this perspective, a deep CNN can be seen as a hierarchical extension of supervised coding methods.
- As key technologies for 5G network, massive MIMO and small cells have been extensively studied in prior works. The fundamental PHY layer techniques of massive MIMO were introduced in @cite @cite . Beyond the PHY, upper layer technologies in a wireless network with massive MIMO are also considered, such as @cite @cite @cite . @cite , user association and resource allocation in a massive MIMO HetNet were investigated with the objectives of rate maximization and rate maximization with proportional fairness. @cite , a time-shift frame structure was proposed to mitigate inter-cell interference caused by pilot contamination in a multi-cell massive MIMO system. Since neighboring cells transmit pilots at different time instants, the inter-cell interference can be well mitigated.
- User association in HetNet has been widely investigated, such as @cite @cite @cite . @cite , user association and resource allocation were jointly considered to maximize the sum utility of users. Using dual decomposition, the proposed scheme can be implemented with a distributed algorithm, and the solution is shown to be near-optimal. @cite , user association was considered to minimize the maximum load among all BS's, several approximation algorithms were proposed with analysis on complexity and performance bound. @cite , user association is determined by the achievable rate of each user. The HetNet with dense small cell deployment has drawn increasing interests since new challenges arise when a large number of SBS's are deployed in a given area. An overview of hyper-dense HetNet was presented in @cite , several cooperative approaches were proposed in @cite and @cite to enhance the network performance.
- Jena rule is a rule format used only by inference engine in the Jena framework @cite . The rule language syntax is based on RDF. It uses the triple representation, which is similar to Notation3 except that a rule name can be specified in a rule. There are not any formula notation, and built-in functions are written in function terms.
- aims at predicting the probability that a specific user will respond (e.g., click) to an ad in a given context @cite @cite . Such context can be either a search keyword @cite , webpage content @cite , or other kinds of real-time information related to the underlying user @cite . From the modelling perspective, many user response prediction solutions are based on linear models, such as logistic regression @cite @cite and Bayesian probit regression @cite . Despite the advantage of high learning efficiency, these linear models suffer from the lack of feature interaction and combination @cite . Thus non-linear models such as tree models @cite and latent vector models @cite @cite are proposed to catch the data non-linearity and interactions between features. Recently the authors in @cite proposed to first learn combination features from gradient boosting decision trees (GBDT) and, based on the tree leaves as features, learn a factorisation machine (FM) @cite to build feature interactions to improve ad click prediction performance.
- deals with the learning problem where the learning data of the target task is expensive to get, or easily outdated, via transferring the knowledge learned from other tasks @cite . It has been proven to work on a variety of problems such as classification @cite , regression @cite and collaborative filtering @cite . Different from multi-task learning, where the data from different tasks are assumed to drawn from the same distribution @cite , transfer learning methods may allow for arbitrary source and target tasks. In online advertising field, the authors in a recent work @cite proposed a transfer learning scheme based on logistic regression prediction models, where the parameters of ad click prediction model were restricted with a regularisation term from the ones of user web browsing prediction model. In this paper, we consider it as one of the baselines.
- It is observed in @cite that spectral clustering based directly the adjacency matrix (or their normalized version) often result in inconsistent clustering in real data, such as the political blogs dataset @cite , a popular benchmark for testing community detection approaches. To address this issue, a new spectral clustering algorithm called SCORE is proposed in @cite . Specifically, the second to @math -th leading eigenvectors are divided by the first leading eigenvector elementwisely, and spectral clustering is applied to the resulting ratio matrix. In their theoretical results, an implicit assumption is that the number of communities @math is bounded by a constant, as implied by the condition (2.14) in @cite . In comparison, our convexified modularity maximization approach works for growing @math both theoretically and empirically. As illustrated in sec:synthetic , our method exhibited better performance on both the synthetic and real datasets considered there, especially when @math .
- There is a rich literature on the task of link prediction and recommendation, so rather than trying to be exhaustive, we refer the reader to the excellent survey by Liben Nowell and Kleinberg @cite and to some prior methods for the specific case of Wikipedia @cite @cite @cite @cite , as it is also our main evaluation dataset. The important point to make is that most prior methods base their predictions on the static structure of the network, whereas we focus on how users interact with this static structure by browsing and searching on it.
- Among the few link recommendation methods that take usage data into account are those by Grecu @cite and West al @cite . They leverage data collected through a human computation game in which users navigate from a given start to a given target page on Wikipedia, thereby producing traces that may be used to recommend new shortcut' links from pages along the path to the target page. Although these approaches are simple and work well, they are limited by the requirement that the user's navigation target be explicitly known. Eliciting this information is practically difficult and, worse, may even be fundamentally impossible: often users do not have any target in mind, , during exploratory search @cite or when browsing the Web for fun or distraction. Our approach alleviates this problem, as it does not rely on a known navigation target, but rather works on passively collected server logs.
- Our random walk model ( sec:random walks ) builds on a long tradition of Markovian web navigation models @cite @cite @cite . Among other things, Markov chains have been used to predict users' next clicks @cite @cite as well as their ultimate navigation targets @cite . Our model might be improved by using a higher order Markov model, based on work by Chierichetti al @cite , who found that human browsing is around 10 Further literature on human navigation includes information foraging @cite @cite @cite @cite , decentralized network search @cite @cite @cite , and the analysis of click trails from search result pages @cite @cite @cite .
- In the present work we assume that frequent indirect paths between two pages indicate that a direct link would be useful. While this is confirmed by the data ( fig:pindir_vs_pst ), research has shown that users sometimes deliberately choose indirect routes @cite , which may offer additional value @cite . Detecting automatically in the logs when this is the case would be an interesting research problem.
- There exist quite a few works using directional sensors. Jing and Abouzeid @cite formulate the coverage problem using directional sensors as Maximum Coverage using Minimum Sensors (MCMS) problem, and provide both centralized and decentralized greedy solutions. In @cite , Munishwar and Abu-Ghazaleh present new algorithm which improves the greedy approaches of @cite . The optimal solution to @math -coverage problem has been proved to be NP-hard by Fusco and Gupta in @cite . They also modelled the sensors to have a fixed viewing angle and overlapping pans. However, all of these works overlooked the coverage imbalance issue.
- Constrained counting, the problem of counting the number of solutions of a propositional formula, is known as . It is @math -complete @cite , where is the set of counting problems associated with decision problems. Theoretical investigations of have led to the discovery of deep connections in complexity theory @cite @cite , and there is strong evidence for its hardness @cite . It is also known that an efficient algorithm for constrained would yield a fully polynomial randomized approximation scheme (FPRAS) for -complete inference problems @cite -- a possibility that lacks any evidence so far and is widely disbelieved.
- In many applications of constrained counting, such as in probabilistic reasoning, exact counting may not be critically important, and approximate counts suffice. Even when exact counts are important, the inherent complexity of the problem may force one to work with approximate counters. Jerrum, Valiant, and Vazirani @cite showed that approximate counting of solutions of CNF formulas, to within a given tolerance factor, can be done with high confidence in randomized polynomial time using an oracle. A key result of @cite states that for many problems, generating solutions is inter-reducible with approximate counting; hence, they have similar complexity. Building on the Sipser's and Stockmeyer's early work @cite @cite , Bellare, Goldreich, and Petrank @cite later showed that in fact, an -oracle suffices for generating solutions of CNF formulas in randomized polynomial time. Unfortunately, these deep theoretical results have not been successfully reduced to practice. Our experience in implementing these techniques indicates that they do not scale in practice even to small problem instances involving few tens of variables @cite .
- Industrial approaches to constrained sampling in the context of constrained-random verification @cite either rely on Binary Decision Diagram (BDD)-based techniques @cite , which scale rather poorly, or use heuristics that offer no guarantee of performance or uniformity when applied to large problem instances @cite . In prior academic works @cite @cite @cite @cite , the focus is on heuristic techniques including Markov chain Monte Carlo (MCMC) methods and techniques based on random seeding of solvers. These methods scale to large problem instances, but either offer very weak or no guarantees on the uniformity of sampling, or require the user to provide hard-to-estimate problem-specific parameters that crucially affect the performance and uniformity of sampling @cite @cite @cite @cite .
- To overcome the scalability challenge, more efficient techniques for counting have been proposed. The large majority of approximate counters used in practice are , which provide lower or upper bounds but do not offer guarantees on the tightness of these bounds. Examples include @cite , @cite , and @cite , and @cite . Another category of counters is called . While these counters may be efficient, they provide no guarantees and the computed estimates may differ from the exact counts by several orders of magnitude @cite . Examples of guarantee-less counters include @cite , @cite , @cite , and @cite .
- To learn different user preferences, we collect data from many non-expert users using a crowdsourcing platform. Prior work has also leveraged crowdsourcing for data labeling or as an efficient platform for transferring human knowledge to robots, e.g., @cite @cite . For example, Sorokin2010 utilized crowdsourcing to teach robots how to grasp new objects @cite . Moreover, several researchers have used crowdsourcing to facilitate learning manipulation tasks from large numbers of human demonstrations @cite @cite @cite @cite . In the context of learning user preferences, jain2015icra recently presented a new crowdsourcing platform where non-experts can label segments in robot trajectories as desirable or not @cite . This is then used to learn a cost function for planning preferred robot trajectories in different indoor environments.
- We aim to discover latent patterns in the ratings matrix @math that enable us to make predictions about the preferences of users. For this, we take from factorization-based collaborative filtering @cite @cite . First, we decompose @math into a bias matrix @math and a residual ratings matrix @math : Each entry @math in @math is formulated as follows: where @math is a global bias term, @math is the bias of the pair @math , and @math is the bias of user @math . We compute @math as the mean rating over all users and object-pairs in @math , i.e., The bias @math describes how high or low a certain user @math tends to rate object-pairs compared to the average user. Similarly, @math captures the tendency of a pair @math to receive high or low ratings. For example, the pair @math , @math tends to receive generally high ratings compared to the pair @math , @math .
- Accordingly, our prediction @math for the rating of an object-pair @math by a user @math is expressed as We learn the biases and factor vectors from all available ratings in @math by formulating an optimization problem. The goal is to minimize the difference between the observed ratings @math made by users and the predictions @math of the system over all known ratings. Let the error associated with rating @math be We jointly learn the biases and factors that minimize the error over all known ratings, i.e., where @math denotes all object-pair and user biases and @math is a regularizer. To do so, we use L-BFGS optimization with a random initialization for all variables @cite . At every step of the optimization, we update the value of each variable based on the error gradient with respect to that variable, which we derive from .
- . Another line of related work is metric learning with CNNs using pairwise @cite @cite or triplet constraints @cite @cite @cite . The goal is to use a CNN with either pairwise (contrastive) or triplet loss to learn a feature embedding that captures the semantic similarity among images. Compared with traditional metric learning methods that rely on hand-crafted features @cite @cite @cite @cite , deep metric learning directly learns from data and achieves much better performance. Recently, it has been successfully applied to variety of problems including face recognition and verification @cite @cite , image retrieval @cite , semantic hashing @cite , product design @cite , geo-localization @cite and style matching @cite . In contrast with previous methods, we propose a novel strategy that enables the learning of continuous manifolds. In addition, we also bring humans in the loop and leverage their inputs during metric learning.
- Applications today that perform data processing from devices at the edge use a MapReduce-style programming model over immutable data @cite and subsequent optimizations for efficient, fault-tolerant processing over streams. @cite These solutions are appealing to the application developer: they present systems for performing efficient computation in a now-familiar programming paradigm.
- Alternative techniques have been presented by academia that focus on moving computation to the edge to alleviate the need for transmission of the entire data set. Directed @cite and digest @cite diffusion presented efficient, fault-tolerant approaches for dissemination of computations and their results. However, these systems do not expose a general programming model.
- Declarative approaches such as Tiny AGgregation, @cite have been proposed for data collection and aggregation across sensor networks. However, these approaches have presented abstractions that are not for general programming as they are specific to the details of aggregation in sensor networks. Finally, declarative approaches @cite have also been proposed for computation in large-scale peer-to-peer systems where clients own their own data. However, in an effort to make their language Turing complete, they relied on the programmer explicitly encoding the details around how statements would be evaluated, to ensure termination.
- Several groups @cite @cite @cite have studied the use of model-free algorithms with recurrent networks to solve POMDPs with discrete action spaces. @cite focused on relatively long-horizon ("deep") memory problems in small state-action spaces. In contrast, @cite modified the Atari DQN architecture @cite (i.e. they perform control from high-dimensional pixel inputs) and demonstrated that recurrent Q learning @cite can perform the required information integration to resolve short-term partial observability (e.g. to estimate velocities) that is achieved via stacks of frames in the original DQN architecture.
- Continuous action problems with relatively low-dimensional observation spaces have been considered e.g. in @cite @cite @cite @cite . @cite trained LSTM-based stochastic policies using Reinforce; @cite @cite @cite used actor-critic architectures. The algorithm of @cite can be seen as a special case of DPG where the deterministic policy produces the parameters of an action distribution from which the actions are then sampled. This requires suitable exploration at the level of distribution parameters (e.g. exploring in terms of means and variances of a Gaussian distribution); in contrast, SVG(0) also learns stochastic policies but allows exploration at the action level only.
- All works mentioned above, except for @cite , consider the memory to be internal to the policy and learn the RNN parameters using BPTT, back-propagating either TD errors or policy gradients. @cite instead take the view of @cite and consider memory as extra state dimensions that can can be read and set by the policy. They optimize the policy using guided policy search @cite which performs explicit trajectory optimization along reference trajectories and, unlike our approach, requires a well defined full latent state and access to this latent state during training.
- @cite describe a relational, Java-based, in-memory database that, for each query, dynamically generates new query-specific code. They created two versions of the query planner: an interpreted one using the iterator model and a compiled one. They demonstrated that using the compiled version removed the overhead of virtual functions in the interpreted version. In addition, the Java JIT compiler was much better at optimizing the generated code for each query than the interpreted version. On average, the compiled queries in their benchmark ran twice as fast as the interpreter.
- @cite generate C code from queries and load the compiled shared libraries to execute them. Their compilation process dynamically instantiates carefully handwritten templates to create source code specific for a given query and hardware. The performance of their dynamically generated evaluation plans are comparable to hard-coded equivalents.
- Neumann @cite describes an approach where the query is compiled to machine code using the LLVM compiler @cite . When generating code, the approach attempts to keep data in registers for as long as possible. Similar to how SQPyte interacts with the existing SQLite C code, this system also preserves complex parts of the database in C++ and calls into the C++ code from the LLVM code as needed. The resulting system is @math faster than the other databases benchmarked.
- @cite @cite @cite use generative programming techniques in Scala to dynamically compile queries using a Scala SQL query engine into C. This technique implicitly inlines parts of the DBMS into the query and shows good performance on the TPC-H benchmark suite relative to the DBX DBMS. However, because code written in Scala cannot inline the generated C, there is no equivalent of the PyPy SQPyte bridge we implemented.
- @cite created the Ferry glue language which serves as an intermediate language to translate subsets of other languages to SQL. That is various front-end languages can translate to that intermediate language, which is then lowered into SQL code. The goal is to reduce the impedance mismatch between languages and database when programming and to improve the efficiency of the interaction. Ferry influenced several works in other languages: @cite developed a Scala plugin that enables programmers to translate Scala-level constructs to Ferry; @cite introduced Switch which uses Ferry-like translation principles to allow seamless integration of Ruby and Ruby on Rails with the DBMS; @cite designed and implemented a Haskell library for database-supported program execution; and @cite created a Ferry-based LINQ-to-SQL provider. Since one of the main goals of Ferry is to reduce the number of times the PL DBMS boundary is crossed, the approach is complementary to the SQPyte approach of reducing the cost of the boundary crossings.
- @cite describe columnar objects, which is an implementation of an in-memory column store embedded into Python together with a seamless integration into the Python object system. With the help of the PyPy JIT compiler they produce efficient machine code for Python code that queries the data store. Compared to SQPyte their approach offers a much deeper integration of the database implementation into the host language, at the cost of having to implement the data store from scratch.
- Unipycation by @cite is a language composition of Prolog and Python that uses meta-tracing to reduce the overhead of crossing the boundary between the two languages. It composes together PyPy with Pyrolog, a Prolog interpreter written in RPython. As with SQPyte, the most effective optimisation is inlining.
- Our work is informed by prior work in three distinct areas: depth and sensor based approaches for physical interaction; movement-based play; and short duration breaks at work. Research has shown exergames, which combine exercise and digital games motivate people to be more physically active @cite . Our goal is to ameliorate the adverse effects of extended periods of sitting, an unavoidable consequence of modern work and lifestyles, using playful mechanisms. Increasing bodily movement by taking breaks from sitting is advocated as a means for increasing metabolism @cite and reducing the risk of cardiovascular disease @cite . Recent evidence suggests health consequences of too much sitting" are distinct from those of too little exercise" @cite . Substantial amount of research has been devoted to promoting physical activity in general through mobile devices with embedded sensors @cite @cite or through dedicated devices like the Nike Fuel Band Nike. http: www.nike.com us en c nikeplus-fuelband or the FitBit Fitbit. http: www.fitbit.com to calculate number of steps walked, distance run, or calories expended. However, there is limited work that promotes physical activity during prolonged sitting at work and we hope to fill this gap.
- Our work originated from the minimum risk training algorithms in conventional statistical machine translation @cite @cite @cite . Och describes a smoothed error count to allow calculating gradients, which directly inspires us to use a parameter @math to adjust the smoothness of the objective function. As neural networks are non-linear, our approach has to minimize the expected loss on the sentence level rather than the loss of 1-best translations on the corpus level. Smith and Eisner introduce minimum risk annealing for training log-linear models that is capable of gradually annealing to focus on the 1-best hypothesis. apply minimum risk training to learning phrase translation probabilities. leverage MRT for learning continuous phrase representations for statistical machine translation. The difference is that they use MRT to optimize a sub-model of SMT while we are interested in directly optimizing end-to-end neural translation models.
- The Mixed Incremental Cross-Entropy Reinforce (MIXER) algorithm @cite is in spirit closest to our work. Building on the REINFORCE algorithm proposed by Williams , MIXER allows incremental learning and the use of hybrid loss function that combines both REINFORCE and cross-entropy. The major difference is that leverage reinforcement learning while our work resorts to minimum risk training. In addition, MIXER only samples one candidate to calculate reinforcement reward while MRT generates multiple samples to calculate the expected risk. Figure indicates that multiple samples potentially increases MRT's capability of discriminating between diverse candidates and thus benefit translation quality. Our experiments confirm 's finding that taking evaluation metrics into account when optimizing model parameters does help to improve sentence-level text generation.
- More recently, our approach has been successfully applied to summarization @cite . They optimize neural networks for headline generation with respect to ROUGE @cite and also achieve significant improvements, confirming the effectiveness and applicability of our approach.
- The user intent behind a search query has been subject of intense study during the last decade. Previous research works have represented user intent by query classification, by existing knowledge base (e.g. Wikipedia) or by queries and click-through data. The most common approach consists in representing user intent by classifying a query into a pre-defined category of goals tasks or topics. Broder @cite has proposed three categories representing the users' goal: navigational, transactional and informational queries. This has been the basis of several research works attempting to automatically identify the users' goal into these three categories @cite @cite . Rose @cite and Baeza-Yates @cite studied the users' intent problem and have proposed slight changes to the Broder's taxonomy.
- @cite first described the task of segmenting out individual instances of a category. The metrics we use in this paper were detailed by @cite , who proposed non-parametric transfer of instance masks from the training set to detected objects, and by @cite who used convolutional neural nets (CNNs) @cite to classify region proposals. We use the terminology and metrics proposed by the latter in this paper. @cite used ideas from @cite to speed up the CNN-based proposal classification significantly.
- A simple way of tackling this task is to run an object detector and segment out each detected instance. The notion of segmenting out detected objects has a long history in computer vision. Usually this idea has been used to aid semantic segmentation, or the task of labeling pixels in an image with category labels. Borenstein and Ullman @cite first suggested using category-specific information to improve the accuracy of segmentation. @cite start from object detections from the deformable parts model @cite and paste figure-ground masks for each detected object. Similarly, @cite and Arbel ' a @cite paste figure-ground masks for poselet detections @cite . Recent advances in computer vision have all but replaced early detectors such as DPM and poselets with ones based on CNNs @cite @cite @cite and produced dramatic improvements in performance in the process. In the CNN era, @cite used features from CNNs to segment out R-CNN detections @cite .
- When producing figure-ground masks for detections, most of these approaches predict every pixel independently. However, this disregards the fact that pixels in the image are hardly independent of each other, and a figure-ground labeling has to satisfy certain constraints. Some of these constraints can be simply encoded as local smoothness: nearby pixels of similar color should be labeled similarly. This can be achieved simply by aligning the predicted segmentation to image contours @cite or projecting to superpixels @cite . More sophisticated approaches model the problem using CRFs with unary and pairwise potentials @cite @cite @cite . Later work considers extending these models by incorporating higher-order potentials of specific forms for which inference is tractable @cite @cite . A related line of work explores learning a generative model of masks @cite using a deep Boltzmann machine @cite . @cite show that inference in CRFs can be viewed as recurrent neural nets and trained together with a CNN to label pixels, resulting in large gains. Another alternative is to use eigenvectors obtained from normalized cuts as an embedding for pixels @cite @cite .
- Compound losses for training deep neural networks that are created by combining multiple losses are now commonplace. In the very deep GoogLeNet network @cite multiple cross entropy losses are distributed at different intermediate layers of the deep network in order to help avoid vanishing gradients. In contrast, our work supports multiple cross entropy losses at the top layer and for different reasons.
- In our work, we create multiple losses by constructing multiple top classification layers on top of a shared network representation. Each classification layer has one output neuron per class. The weights from the representation to this neuron are the classifier weights for this specific classifier. In order to enforce multiplicity among the classifiers of the same class, we add an orthogonality constraint, which is enforced either in the representation space or in a Fisher spectrum aligned space. A number of ways to encourage diversity in a classifier ensemble by enforcing orthogonality have been studied in the machine learning literature @cite and in computer vision @cite . However, note that in our case orthogonality does not lead to diversity since all classifiers end up presenting the same set of probabilities.
- A prominent example of the success of transfer learning can be seen in the task of face recognition. Starting with the work of @cite , a neural network has been employed for extracting representations from face images that are shown to outperform humans. @cite @cite @cite @cite further improve the state-of-art by extracting features from multiple face patches, incorporating architectures into the domain of deep face recognition that are inspired by recent architectures that are used for object recognition @cite , and most relevant to our work, combining, during training, both classification and contrastive loss. Another recent work @cite further improves the training criterion by using a triplet cost to increase the discriminability between identities. The idea presented here, of combining multiple copies of the same loss, was not pursued in previous works.
- The deep face networks mentioned above, are all trained on large scale proprietary datasets, which are not publicly available. @cite built a publicly available dataset by mining images from the internet. Furthermore, they demonstrated the quality of the data collected by training a state-of-the-art network on it. Their network architecture is similar to that of the VGG model @cite . JB is used to effectively enhance performance. In our work, we use the same architecture suggested in @cite as the basis of our face recognition experiments. We also employ JB to learn similarities for faces and other objects. A recent paper using JB outside the domain of face recognition is @cite .
- Historically, the most investigated feature was the human skin color @cite . If the input image contains too much skin colored pixels, it was taken as an indicator of nudity. However, skin color solely is not reliable since a face closeup image has a lot of skin pixels while being non-porn. So, researchers augmented the skin color with other constraints or shape features @cite . With the introduction of new computer vision models, porn detection algorithms became more accurate. For instance, introduced classification accuracy improvement by adopting the visual bag-of-words (BoW) model @cite . BoW tries to extract the most common patches that exist on a set of training images. For a detailed survey of existing methods, please refer to @cite .
- Recently, @cite introduced a BoW framework, the BossaNova representation, to classify pornographic videos. BossaNova relies on the HueSIFT descriptors. Since HueSIFT represents both color and shape, it outperformed standard BoW models that rely only on shape or edge cues. Additionally, @cite extended the BossaNova to be more suitable for video classification. They applied binary descriptors with BossaNova for even better accuracy on the same benchmark dataset.
- Due to the popularity of PEGs, many grammar developers have attempted the grammar specification for their interesting languages. While PEGs, in some sense, are more powerful than CFGs, several limitations on their expressiveness have been pointed out in @cite @cite .
- A few researchers have attempted to extend the expressive power of PEGs itself. Notably, Adams newly introduced Indent-Sensitive CFGs @cite and its PEG-version @cite to recognize indentation-based code layout. The idea is based on constraint-based annotations on all nonterminals and terminals. As we described in Section 4.5, Nez can define an INDENT table and provide similar (not the same) effects to the Indent-Sensitive CFGs, as shown in Table.
- Other related work has been carried out in the context of automated non-cooperative dialogue systems, where an agent may act to satisfy its own goals rather than those of other participants @cite . The game-theoretic underpinnings of non-cooperative behaviour have also been investigated @cite . Such automated agents are of interest when trying to persuade, argue, or debate, or in the area of believable characters in video games and educational simulations @cite @cite . Another arena in which strategic conversational behaviour has been investigated is negotiation @cite , where hiding information (and even outright lying) can be advantageous.
- Recent work on deep learning applied to games include the following. @cite train a deep convolutional network for the game of Go, but it is trained in a supervised fashion rather than trained to maximise a long-term reward as in this work. A closely related work to ours is a DRL agent for text-based games @cite . Their states are based on words, their policies are induced using game-based rewards, and their actions are based on directions such as go east west south north'. Another closely related work to ours is DRL agents trained to play ATARI games @cite . Their states are based on pixels from down-sampled images, their policies make use of game-based rewards, and their actions are based on joystick movements. In contrast to these previous works which are based on navigation commands, our agents are use trading dialogue moves (e.g. I will give you ore and sheep for clay', or I accept decline your offer'), which are essential behaviours for strategic interaction.
- This paper extends the recent work above on training strategic agents using reinforcement learning, which have either used small state-action spaces or focused on navigation commands rather than negotiation dialogue. The learning agents described in this paper use a high dimensional state representation (160 non-binary features) and a fairly large action space (73 actions) for learning strategic non-cooperative dialogue behaviour. To our knowledge, our results report the highest winning rates reported to date in the game of Settlers of Catan, see @cite @cite @cite . The comprehensive evaluation reported in the previous section is evidence to argue that deep reinforcement learning is a promising framework for training strategic interactive agents.
- There has been substantial progress in mouse tracking using color cameras. Many commercial software used for studying social interaction of mice, such as HomeCageScan, EthoVision, AnyMaze, PhenoTracker and MiceProfiler @cite use this technique to track up to two mice from color videos. A common issue with tracking is that it is highly sensitive to small color or texture perturbations over time. This often gives rise to failures, which makes a manual re-initialization of the system necessary and inevitable if used to track for long durations. There has been some recent progress in long-duration tracking, such as @cite @cite @cite , which can work robustly without such manual intervention. This is typically achieved with the help of invasive methods such as attaching RFID chips or applying dedicated fur dyes to maintain identities reliably over time.
- In behavior phenotyping, existing systems rely on discriminative learning methods such as SVM and conditional random fields (CRFs) @cite @cite @cite that directly focus on classifying behaviours or interactions. These discriminative methods are usually supported by extracting visual features from 2D videos, such as spatio-temporal interest points (STIP) @cite obtained by convolution of video sequence with filters designed to respond to salient points in the video volume, on which the visual bag-of-word features can be constructed. One key drawback is that these approaches are based on highly sophisticated features that are usually inscrutable by behavioral neuroscientists. This might lead to undesirable consequences when one tries to interpret the empirical findings, even if the classification performance is acceptable. An alternative scheme is to instead rely on single-frame based pose estimation, which is a lot easier for domain experts to understand and interpret.
- The recent advance of commodity depth imaging opens the door to drastic progress in this direction. These depth cameras are currently based on either structured illumination or time-of-flight (ToF) technologies. The most related work in the field @cite is a low-res body pose estimation that involves the 3D location and orientation of the entire mouse based an overhead Primesense depth camera. Our approach may be viewed as the next step to reveal high-resolution details of 3D mouse full-body skeleton. Note the mice dataset of @cite has also been studied in @cite for a similar low-resolution pose estimation purpose in 2D based on gray-scale images.
- Similar trends have been observed recently in computer vision problems related to humans, which have already demonstrated the unique value of utilizing depth cameras. These include depth-image based pose estimation of the human body @cite , head @cite , and hand @cite . Nonetheless, the set of challenges for mouse pose estimation are distinct and cannot be accomplished by a trivial re-implementation of these existing methods. A lab mouse is noticeably smaller than a typical human hand, is highly deformable and is highly agile attaining over 1m s maximum velocity @cite in a small enclosed space. For any practical camera setup, occlusions of body parts such as limbs and paws of mice are common problems. These factors also inspire us to consider dedicated cage and camera setups which will be described in later sections.
- There has been extensive prior work on the downlink transmission rate as a function of a finite CSI feedback rate. In toy setups where a base station (BS) serves a single user in a single cell by ignoring ICI, downlink transmission rates were characterized as a function of the codebook size and the SNR. For example, in @cite @cite , a channel codebook design method for a single-user transmission was proposed by using Grassmannian line packing. By employing random vector quantization (RVQ), the rate loss of a point-to-point MIMO system caused by using finite rate feedback was characterized in @cite . In @cite , by applying RVQ, the performance of conjugate beamforming was analyzed in a MISO system. In @cite , the performance of the finite rate CSI feedback was characterized, assuming temporally correlated channel and a principle of designing a channel codebook for this particular condition was proposed. The main limitation of @cite @cite @cite @cite @cite is that it assumed only a single pair of a BS and a user, in which important features of a cellular network, e.g., ICI are missing.
- Our core visual processing module is a Convolutional Neural Network (CNN) @cite @cite , which has emerged as a powerful model for visual recognition tasks @cite . The first application of these models to dense prediction tasks was introduced in R-CNN @cite , where each region of interest was processed independently. Further work has focused on processing all regions with only single forward pass of the CNN @cite @cite , and on eliminating explicit region proposal methods by directly predicting the bounding boxes either in the image coordinate system @cite @cite , or in a fully convolutional @cite and hence position-invariant settings @cite @cite @cite . Most related to our approach is the work of Ren al @cite who develop a region proposal network (RPN) that regresses from anchors to regions of interest. However, they adopt a 4-step optimization process, while our approach does not require training pipelines. Additionally, we replace their RoI pooling mechanism with a differentiable, spatial soft attention mechanism @cite @cite . In particular, this change allows us to backpropagate through the region proposal network and train the whole model jointly.
- Finally, the metrics we develop for the dense captioning task are inspired by metrics developed for image captioning @cite @cite @cite .
- CCS is distinct from classification. Classification is focused on sorting documents, such as for attributing authorship @cite @cite or counting types of news events @cite @cite . Text classification has been attempted using a wide array of machine learning methods such as naive bayes, linear discriminant analysis, or support vector machines @cite , which easily allow for large numbers of features (the words and phrases) to be incorporated. For comparisons of these different methods on text see @cite , @cite or @cite . For SVMs as an approach in particular for text, see the book of @cite . For such methods and these evaluations, however, the features themselves are not of primary interest, classification is. We instead attempt to extract meaning from documents by contrasting sets to each other. This is most similar to key phrase extraction, a literature in its own right. See, for example, @cite , @cite , or @cite .
- Interpreting text is a difficult task, and can be done in a variety of ways. For example, @cite use text to predict roll call votes with an LDA (Latent Dirichlet Allocation) algorithm @cite in order to understand how language of law is correlated with political support. @cite model political text to explore who dominates policy debates. Truly validating a finding is generally quite difficult and requires a great deal of effort to do properly, as these papers illustrate quite well. Our tools are primarily intended for exploration; validation is not within CCS's scope without additional human validation efforts or alternative techniques.
- Of the many approaches to text analysis, variations of LDA @cite in particular have recently been widely investigated and used. These consist of a Bayesian hierarchical model that describe documents in a corpus as a mixture of some number of different distributions on the words. They can reveal structure in a corpus and have been shown to capture human meaning. However, generating novel models for specific circumstances is difficult. Even mild changes to the model can be technically quite challenging and consist of an entire research topic in its own right. They are either computationally expensive or only solved approximately (via, e.g., variational methods). In the spirit of diversity in research approaches, we take a different path.
- Learning word representations using large text corpora has received a renewed interest recently due to the impressive performance gains obtained in downstream NLP applications using the word representations as features @cite @cite . Continuous bag-of-words (CBOW) and skip-gram (SG) methods proposed by @cite use the local co-occurrences of a target word and other words in its context for learning word representations. Specifically, CBOW predicts a target word given its context, whereas SG predicts the context given the target word. Global vector prediction (GloVe) @cite on the other hand first builds a word co-occurrence matrix and predicts the total co-occurrences between a target word and a context word. Unlike SG and CBOW, GloVe does not require negative training instances, and is less likely to affect from random local co-occurrences because it operates on global counts. However, all of the above mentioned methods are limited to using only a corpus, and the research on using semantic lexicons in the word representation learning process has been limited.
- Although much progress has been made to design aesthetic animal-like faces of these articulated robots, it is usually difficult to design the mechanic structures with multiple DOFs and jointly control them when displaying several complex facial behaviors such as mouth movement. In this case, the robot BERT2" @cite was designed with a hybrid face including a plastic faceplate and an LCD display, where eye brows, eyes and mouth movement were displayed via graphics animation. The recognizability of BERT2's facial expressions were evaluated and verified through human-robot interaction experiments. Compared to BERT2, our eBear has 10 DOFs on the mechanical faceplate including ears, and it is able to blend the expressive movement with speech during mouth animation, which incorporates the benefits of both mechanical and graphical facial elements.
- Many approaches have been proposed for solving the face alignment problem. The classic methods include the Active Appearance Model (AAM) @cite and its variants. AAM models faces by both appearance and shape information and optimises them in a holistic manner. Facial parts detection based method @cite , also known as Constrained Local Model, aim to detect facial parts with several part detectors, and use global constraints to estimate reasonable shapes. Discriminative regression methods @cite @cite @cite @cite regress expected shape update based on local appearance information extracted from all current estimated landmarks. Such methods have achieved state-of-the-art accuracy. Our framework allows these state-of-the-art approaches to be applicable to non-frontal cases via recommendation over multiple models. A number of existing studies propose to solve the multi-view face alignment problem @cite @cite . These studies differ significantly from our work since they confine the yaw angle of faces within the range of @math 45 @math @cite . @cite , the dataset is chosen from the 300-W dataset @cite with an approximate range of @math 45 @math , which is far smaller than the head pose range considered in this study.
- . In this work, the facial points are estimated after a rough estimation of head pose. The head pose is estimated according to the global appearance of the face image and regarded as the prior probability of each alignment model. This method works quite well and gains improvement on traditional face alignment benchmarks, LFPW @cite or LFW @cite . When this method is extended to dataset with extremely large pose variations, AFLW, the method fails due to erroneous head pose estimation, no matter how well each face alignment model is trained.
- . This study extends @cite by iteratively re-estimating the head pose and facial expression based on the current estimated shape, and then re-estimating the facial landmarks with the updated prior model probability. Further improvement is achieved in this work compared to @cite . Similar to @cite , the algorithm begins with head pose estimation based on global appearance. When applied to faces with large pose variation, if the initial head pose estimation is incorrect, the error is rather hard to be rectified during iterative optimisation since the initial shape is estimated with the wrong prior probability.
- . The study applies a mixture of models to handle different types of occluded faces. The method works well on occlusion benchmarks like COFW @cite . However, the method yields poor performance on profile-view cases because all regression models assume a limited pose variation. From our experiments, we observe that even if models are provided to handle large pose variations, suboptimal results may be still generated due to the failure in drawing consensus amongst recommended landmarks with large spatial variations.
- . Model recommendation has been proposed for action recognition @cite . The study shows that a recommendation system is important for choosing one model out of a large set of possibilities. Their method follows the conventional collaborative filtering scheme, where user's rating is required on a small subset of models, and the goal is to use that subset of ratings to predict the ratings of remaining models. This is in contrast to our task where user rating is not available.
- Instead of using the heuristic strategies, @cite formulated the key principle of CL as a concise SPL model. Formally, given a training dataset @math , in which @math and @math denote the @math observed sample and its label, respectively, @math denotes the loss function which calculates the cost between the ground truth label @math and the estimated one @math , and @math represents the model parameter in decision function @math . The SPL model includes a weighted loss term on all samples and a general self-paced regularizer imposed on sample weights, expressed as:
- where @math is the age parameter for controlling the learning pace, and @math represents the self-paced regularizer (SP-regularizer), whose intrinsic conditions have been theoretically abstracted by @cite @cite . Through jointly learning the model parameter @math and the latent weight @math by AOS with gradually increasing age parameter, more samples can be automatically included into training from easy to complex in a purely self-paced way.
- Multiple variations of this SPL learning regime, like self-paced reranking @cite , self-paced multiple instance learning @cite @cite , self-paced learning with diversity @cite and self-paced curriculum learning @cite , have been proposed under the format ). The effectiveness of this SPL paradigm , especially its robustness in highly corrupted data, has been empirically validated in various machine learning and computer vision tasks, such as object detector adaptation @cite , specific-class segmentation learning @cite , visual category discovery @cite , and long-term tracking @cite . For example, the SPL paradigm has been a major contributing factor to the leading performance of the CMU team in the challenging TRECVID MED competition organized by the NIST in 2014 @cite .
- NCRP has been demonstrated to have attractive properties in sparse estimation (as a penalty term) @cite @cite @cite and robust learning (as a loss term) @cite @cite both theoretically and practically, and attracted much attention in machine learning and statistics in recent years. Various NCRP realizations have also been proposed. Typical ones include the capped-norm based penalty (CNP) @cite @cite @cite , the minimax concave plus penalty (MCP) @cite , the smoothly clipped absolute deviation penalty (SCAD) @cite ,the logarithmic penalty (LOG) @cite and the nonconvex exponential penalty (EXP) @cite . The mathematical forms of these NCRP terms in one dimension cases are listed as follows @cite @cite : . SCAD: p_ , ^ SCAD (t)= . LOG: p_ , ^ LOG (t)= 1 (1+ |t|) EXP: p_ , ^ EXP (t)= 1 (1- (- |t|)).
- Albeit possessing elegant statistic properties and empirically verified to be effective in specific applications through finely designed solving strategy, involving such NCPR terms brings non-convexity to the model. This inclines to result in the issue that the algorithm easily get stuck to an undesired local minima of the problem @cite @cite .
- MM algorithms have wide applications in machine learning and statistical inference @cite . It aims to turn a complicated optimization problem into a tractable one by alternatively iterating the majorization and minimization steps. In particular, considering a minimization problem with objective @math , given an estimate @math at the @math iteration, a typical MM algorithm consists of the following two steps:
- Starting from the invention of the distributed representations for words @cite @cite @cite @cite @cite , CNNs and RNNs have gained significant successes on various NLP tasks, including sequential labeling @cite , sentence modeling and classification @cite @cite , paraphrase identification @cite , event extraction @cite @cite for CNNs and machine translation @cite @cite for RNNs, to name a few.
- blum2005practical (2005) proposed an early input perturbation framework (named SULQ), and the parameters of noise are refined by dwork2006calibrating (2006). dwork2014analyze (2014) proved the state-of-the-art utility bounds for @math -DP. hardt2012beating (2012) provided a better bound under the coherence assumption. In @cite @cite , the authors used a noisy power method to produce the principal eigenvector iteratively with removing the previous generated ones. hardt2014noisy (2014) provided a special case for @math -DP as well.
- : Existing metric learning methods can be mainly classified into two categories: unsupervised and supervised. Unsupervised methods seek a low-dimensional subspace to preserve the geometrical information of samples. Representative unsupervised metric learning methods include principal component analysis (PCA) @cite , locality preserving projections (LPP) @cite , locally linear embedding (LLE) @cite , and multidimensional scaling (MDS) @cite . Supervised methods learn a discriminative distance metric under which the intra-class variation is increased and the inter-class variation is decreased. Typical methods in this category include linear discriminant analysis (LDA) @cite , neighborhood component analysis (NCA) @cite , cosine similarity metric learning @cite , large margin nearest neighbor (LMNN) @cite , and information theoretic metric learning (ITML) @cite , discriminative deep metric learning (DDML) @cite and large margin local metric learning (LMLML) @cite .
- We do not know any previous work on closely related models. In one way, our model could be regarded as a continuum, syncronous-moves analog of the (discrete-space, asynchronous moves) TASEP which has been intensively studied @cite owing to its connections with other statistical physics models. The small literature on such analogs of TASEP (see @cite and citations therein) has focused on ergodic properties for processes on the doubly-infinite line. In another way, our phenomenon could be regarded as analogous to stop-and-go motion in traffic jams, for which realistic models would need to consider both positions and velocities of vehicles, although discrete-space discrete-time synchronous models have also been studied @cite . Academic literature such as @cite studying real world security lines does not address the specific wave" feature studied in this paper.
- Due in part to its importance, there has been an active line of work to address difficulties associated with cold-start users and items, where a common theme among them is to exploit auxiliary information about users or items besides the rating data that are usually available @cite . A feature based regression ranking model for predicting the values (rates) of user-item matrix in cold-start scenarios by leveraging all information available for users and items is proposed in @cite . The kernelized matrix factorization approach studied in @cite , which incorporates the auxiliary information into the MF. @cite joint factorization of the user-item and item-feature matrices by using the same item latent feature matrix in both decompositions is utilized. The FBSM model is introduced in @cite , which learns factorized bilinear similarity model for new items, given the rating information as well as the features of these items. Recently, @cite proposed a decoupling approach to transduct knowledge from side information to rating prediction which is able to handle both cold-start items and users problems in factorization based models.
- Substantial evidence for violations of the missing at random condition in recommender systems is reported in @cite and it has been showed that incorporating an explicit model of the missing data mechanism can lead to significant improvements in prediction performance.The first study of the effect of non-random missing data on collaborative ranking is presented in @cite . @cite an EM algorithm to optimize in turn the factorization and the estimation of missing values. Recently, in @cite a novel dynamic matrix factorization framework that allows to set an explicit prior on unknown values is introduced. However their algorithm requires a careful setting of the prior rating to be practical.
- Due to the importance of valuating a real estate property, there has been extensive research on automatic valuation @cite @cite @cite @cite @cite @cite . Most of that work used hedonic models, which assumed the price can be predicted from a combination (usually linear) of the property structured features @cite such as number of bedrooms and the location. Traditional hedonic models were based on human expertise, where the model parameters are usually hand-coded by experts, unlike our proposed method here, which uses data mining. There has been growing literature on the use of data mining techniques to analyze real estate data @cite @cite @cite @cite @cite , however, most of the previous work focused only on structured features and ignored textual features. We review sample of these works in the remainder of this section.
- One of the early works @cite used decision tree and neural network techniques to predict the sale price of a house. The analysis used data with 15 numerical features that represent the houses characteristics plus a categorical feature that corresponds to the address. The dataset consisted of 1000 records that were collected from the houses sales transactions in Miami, US. Unlike our work, the analysis focused only on properties for sale (did not include rentals), used only structured features (no text mining) and relied on a much smaller dataset (compared to our +50,000 records). A broader analysis was conducted in @cite , covering 295,787 transactions from four cities in the US. Again, only numerical features were used (although more extensive features were used, almost 200) and no textual features were used (also despite attempting to predict the price, no performance criterion was reported). A more recent work @cite proposed Adaptive Neuro Fuzzy Inference System (ANFIS) and tested the system over 360 records of past sales properties in Midwest, US. The dataset had 14 numerical features and again no textual feature was used.
- In the domain of global localization with a priori maps, @cite showed that finding the optimal (shortest) plan to re-localize a kidnapped robot with multiple hypothesis in a deterministic setting (no sensing or motion uncertainty) is NP-hard. At best a greedy localization strategy can be developed whose plan length is upper bounded by @math , where @math is the number of hypothesis and @math is the length of the optimal plan. Compared to @cite , we do not assume perfect sensing or actuation. In @cite , the authors develop an active localization method in a grid based scheme for a known map. Their planning method considers arbitrary targets in the robot's local coordinate frame as atomic actions (e.g., move 1m right and 4m forward). The optimal action is selected based on the path cost and the expected decrease in entropy at the target. Compared to @cite , our target selection methodology (Section and ) is active, i.e., M3P uses the a priori map information to select targets such that by visiting them, observation gap between belief modes is maximized resulting in successive disambiguation.
- ( 1 ) Supervised hashing with deep models: Learning high-level feature representations by building deep hierarchical models have shown great potential in various applications. Researchers have been adopting deep models to jointly learn image representations and hash codes from data. Kang al @cite proposed a deep multi-view hashing (DMVH) algorithm to learn hash codes with multiple data representations. Xia al @cite proposed learning image representations for supervised hashing by approximating the data affinity matrix with CNN features. Zhao al @cite proposed a Deep Semantic Ranking Hashing (DSRH) method to preserve multilevel semantic similarity between multi-label images. Erin Liong al @cite proposed a deep hashing method to explore the nonlinear relationships among data. Zhang al @cite proposed a Deep Regularized Similarity Comparison Hashing (DRSCH) method to allow the length of output bits to be scalable. Most of the works learn hash functions on top of a deep CNN architecture. In contrast, VDSH can be built from arbitrary vector representations. When CNN features are used, our method can be viewed as fine-tuning these networks for supervised hashing. Besides, the scale and depth of our DNNs are much larger than previous methods, which pose harder challenges for training.
- ( 2 ) DNNs: In the literature, many different DNN architectures ( LeNet @cite , AlexNet @cite , GoogLeNet @cite and VGG-VD @cite ) and weighting structures ( sparse network @cite , circulant structure @cite , low-rank approximation @cite ) have been proposed. Several techniques have been proposed to improve the generalization of networks such as dropout @cite and dropconnet @cite , which can be viewed as better regularization. Some techniques for speeding-up the training have been proposed as well such as distributed training @cite and batch normalization @cite . These architectures and methods, however, are trained using backprop, suffering from the same issues such as vanishing gradients.
- Ongoing efforts to overcome issues in backprop include variational Bayesian autoencoder @cite , auto-encoding target propagation @cite , and difference target propagation @cite . Carreira-Perpin 'a n and Wang @cite recently proposed a method for training deeply nested systems. Their method of auxiliary coordinates (MAC) breaks down the dependency in nested systems into equality constraints, so that the quadratic penalty method can be utilized as an efficient solver. Shen al @cite proposed a Supervised Discrete Hashing (SDH) method based on MAC which achieved the state-of-the-art on supervised hashing. Carreira-Perpin 'a n and Raziperchikolaei @cite proposed learning binary autoencoders for hashing as well using MAC.
- In contrast our ADMM-based method is more suitable and efficient for solving regularized loss minimization as has been shown in the Block-Splitting algorithm @cite . ADMM solves optimization (possibly nonconvex) problems with equality constraints by decomposing an objective into several disjoint sub-objectives using new auxiliary variables so that the original objective can be optimized iteratively using coordinate descent. With small additional computational cost we circumvent the need for relaxation of penalty related parameters as required in this context @cite .
- Application-specific information can be very useful to enable ad-hoc solutions, which dramatically decrease the cost of detection. Many techniques have been advocated. They include memory scrubbing @cite and ABFT techniques (see below).
- The very first idea of algorithm-based fault tolerance for linear algebra kernels is given by Huang and Abraham @cite . They describe an algorithm capable of detecting and correcting a single silent error striking a matrix-matrix multiplication by means of row and column checksums. This germinal idea is then elaborated by Anfinson and Luk @cite , who propose a method to detect and correct up to two errors in a matrix representation using just four column checksums. Despite its theoretical merit, the idea presented in their paper is actually applicable only to relatively small matrices, and is hence out of our scope. @cite and @cite present two relatively recent survey.
- The problem of algorithm-based fault-tolerance for sparse matrices is investigated by @cite , who suggest a way to detect a single error in an SpMxV at the cost of a few additional dot products . @cite suggest that this approach can be relaxed using randomization schemes, and propose several checksumming techniques for sparse matrices. These techniques are less effective than the previous ones, not being able to protect the computation from faults striking the memory, but provide an interesting theoretical insight.
- Early edge detectors were manually designed to use image gradients @cite @cite @cite and later texture gradients @cite . Of more relevance to this work are edge detectors trained in a data-driven manner. Since the work of @cite , which formulated edge detection as a binary classification problem, progressively more powerful learning paradigms have been employed, including multi-class classification @cite , feature learning @cite , regression @cite , structured prediction @cite , and deep learning @cite @cite @cite . Recently, @cite extended @cite to motion edge estimation. These methods all require strong supervision for training. In this work we explore whether unsupervised learning can be used instead (and as discussed select @cite @cite for our experiments).
- Motion plays a key role for grouping and object recognition in the human visual system @cite . In particular, @cite studied the visual skills of individuals recovering from congenital blindness and showed that motion cues were essential to help facilitate the development of object grouping and representation. Our work is inspired by these findings: we aim to learn an edge detector using motion cues.
- There is an emerging interest for learning visual representations using video as a supervisory signal, for example by enforcing that neighboring frames have a similar representation @cite , learning latent representations for successive frames @cite , or learning to predict missing or future frames @cite @cite . Instead of simply enforcing various constraints on successive video frames, Wang and Gupta @cite utilized object tracking and enforce that tracked patches in a video should have a similar visual representation. The resulting network generalizes well to surface normal estimation and object detection. As we will demonstrate, our approach can also serve as a novel unsupervised pre-training scheme. However, while in previous approaches the training objective was used as a surrogate to encourage the network to learn a useful representation, our primary goal is to train an edge detector and the learned representation is simply a useful byproduct.
- The framework proposed in this work belongs to the first category. There are two major differences between our approaches and the work in : (1) The architectures developed in are only based on CNN, whereas our models are based on bidirectional LSTMs, which are more capable of exploiting long-range sequential context information. Moreover, we also integrate the CNN structures on the top of biLSTM for better performance. (2) @cite tackle the question and answer independently, while the proposed structures develop an efficient attentive models to generate answer embeddings according to the question.
- Motivated by the remarkable success of deep learning for large-scale image classification @cite @cite , recent work show that the use of representations extracted from DCNN is quickly gaining ground over hand-crafted descriptors such as Fisher Vectors as favoured state-of-the-art global image descriptors for image instance retrieval @cite @cite . While neural networks have been around for several decades, their resurgence can be attributed to two key factors: availability of large training data, and large amounts of computing power, which makes training large and deep networks possible. E.g., the neural networks in @cite @cite have 7 and 19 layers respectively, and take weeks to train with millions of images on GPU.
- We propose Unsupervised Triplet Hashing (UTH), a scheme for learning binary embedding functions of high-dimensional representations. UTH consists of a two-stage deep learning pipeline. First, we use SRBMs to learn a first version of the binary embedding functions in a fashion similar to our previous work in @cite where we showed that the method is able to produce very compact hashes with good retrieval performances. However, SRBMs is purely based on data reconstruction and do not purposefully try to preserve the good metric properties of the original high-dimensional space. Therefore, we add a second step to fine-tune the network weights (embedding parameters). This second step uses triplets of weight-sharing networks and learns to preserve the ranking order of triplets of images. Unlike other approaches using triplet learning networks @cite @cite @cite , our approach is and does not require additional label data for the triplets. For our experiments, we use for our starting representation an intermediate layer from the 19-layer OxfordNet which is state-of-the-art for image classification.
- Our contributions are three-fold: We propose a method for learning binary embedding functions able to produce very compact hashes (down to 32 bits) for image instance retrieval. UTH expands our previous work on hash compression @cite with a fine-tuning step based on triplet networks to preserve the ranking information from the high-dimensional global descriptors. Unlike with other approaches, the learning-to-rank pipeline is fully unsupervised and does not require any additional label data. To our knowledge, this is the first work on learning to rank with unsupervised deep network applied to image instance retrieval. Through a thorough empirical evaluation on small and large datasets, we show that UTH can reduce the data size of uncompressed descriptors by 512 @math (256 bits hashes) without considerable retrieval performance loss. We also show that UTH is able to outperform other unsupervised schemes in the 32-256 bits range.
- The Tanaka functional and the wasserstein distance methods employed in our study are indeed related to another set of distances between probability measures, based on the Fourier transform. Those methods were introduced in @cite @cite , and we refer to @cite for a description of the links existing between those metrics and Wasserstein distances. Those Fourier-based arguments have proven useful to study the properties of the Boltzmann equation in the Maxwellian case, and they could probably also be used to study the dynamics of . The Fourier-based distance was used to study a range of models from econometrics @cite and opinion formation theory. Let us finally mention @cite , where probabilist methods are used to prove the contractivity of a large class of kinetic operators.
- Finally, let us mention two biological problems linked to the present article. The first class of biological problematic is cooperation in biological populations. Cooperation is a widespread behaviour: it exists at all levels of life, and appearance and stability of such phenomena is a challenging problem, and we refer to @cite @cite @cite . One of the mechanisms of cooperation is the exchange of goods. These exchanges could be modelled with an operator similar to the one we derived in . The second class of problems is linked to sexual reproduction, where the formation of gametes and recombinations implies that the DNA of the offspring is a combination of the genomes of both parents. A model used in biology to describe the effect of those events on a given phenotype is the so-called infinitesimal model, see @cite @cite . The properties of the operator appearing in this model are similar to the exchange operator described in .We believe the analysis methods developed here could also be useful in those two other contexts. In particular, they could be an interesting step towards a rigorous mathematical description of the hydrodynamic limits introduced in @cite .
- Most of the top-performing IQA models (full, reduced, and no-reference) have been extensively evaluated on two benchmark databases: the LIVE IQA Database which was designed in @math and the TID2008 Database, designed and released in @math . The LIVE IQA Database, one of the first comprehensive IQA databases, consists of @math images, much larger than the small databases that existed at the time of its introduction @cite - @cite . This legacy database contains @math pristine reference images and models five distortion types - jp2k, jpeg, Gaussian blur, white noise, and fast fading noise @cite . The TID2008 Database is larger, consisting of @math reference and @math distorted images over @math distortion categories. TID2013 @cite is a very recently introduced image quality database with an end goal to include the peculiarities of color distortions in addition to the 17 simulated spatial distortions included in TID2008. It consists of 3000 images and includes seven new types of distortions, thus modeling a total of 24 distortions. We refer the reader to @cite @cite for more details on the categories and severities of image distortions contained in this database.
- However, the highly variable ambient conditions and the wide array of display devices on which a user might potentially view images will have a considerable influence on her perception of picture quality. This greatly motivates our interest in conducting IQA studies on the Internet, which can enable us to access a much larger and more diverse subject pool while allowing for more flexible study conditions. However, the lack of control on the subjective study environment introduces several challenges (more in Sec. ), some of which can be handled by employing counter measures (such as gathering details of the subject's display monitor, room illumination, and so on) @cite .
- Appliance modeling in residential settings has several proposed benefits, including reduced power consumption, automated actuation of smart appliances subject to energy pricing, microgrid load balancing, and home security @cite . Additionally, personalized advisory tools have gained popularity to provide adaptive demand-response prediction @cite @cite . Bayesian modeling techniques for home appliance load modeling has been an emerging topic of interest @cite , and EDHMM-based models have previously been proposed for load disaggregation @cite . Markov modeling of uncertainties in demand and energy pricing has been studied in @cite , which presents a reinforcement learning based approach to optimal load scheduling.
- The related subjects of data-driven occupancy prediction @cite and user behavior modeling for energy demand predictions have also been studied in recent years. In @cite , a stochastic model to predict time-dependent user activity was presented, while in @cite , a data-driven approach was adapted for learning residential power profiles based on user-specific factors. Integration of suitable occupancy and user prediction techniques with ours is a clear direction for future work.
- More importantly, we believe every sentence in the review may not necessarily contribute to the classification of the review to the appropriate class. @cite We say that certain could be sufficient to represent and distinguish between the sentiment classes of a review. Representative features have been argued to be the key to effective classification technique. @cite @cite We emphasize that our approach is promising and can be easily integrated by any sentiment classification system regardless of the sentiment detection technique employed.
- According to , the sentiment analysis research mainly started from early 2000 by and . firstly used a few semantically words (e.g., excellent and poor) to label other phrases with the hit counts by queries through search engines. Then, researchers had also proposed several custom techniques specifically for sentiment classification, e.g., the score function based on words in positive and negative reviews @cite and feature weighting schemes used to enhance classification accuracy @cite . Besides, the other situation of sentiment analysis is to represent texts by vectors which indicate these words appear in the text but do not preserve word order. And a machine learning approach will be used for classification in the end. In such way, considered classifying documents according to standard machine learning techniques. In addition, subsequent research used more features in learning, making the main task of sentiment classification engineer an effective set of features @cite .
- However, compared to English sentiment analysis, there are relatively few investigations conducted on Chinese sentiment classification until 2005 @cite . presented a study on comparison of different machine learning approaches under different text representation schemes and feature weighting schemes. They found that SVM achieved the best performance. After that, found 6,000 or bigger for the size of features would be sufficient for Chinese sentiment analysis, and sentiment classifiers were severely dependent on domains or topics.
- As pointed out by Moser and Tardos @cite , it is not necessary to find a independent set, but a large independent set suffices. Following this idea, @cite gave a distributed algorithm where they instead compute so-called independent sets, where the probability that a node is not in the produced independent set @math or neighbouring a node in set @math is bounded by @math . They showed that this can be done in @math rounds, thus the dependency on @math in the total running time of the LLL algorithm is only @math . Recently, Ghaffari improved this further by showing that weakly maximal independent sets can be computed in @math rounds @cite .
- While there are numerous positive results, only a few lower bounds for LLL are known. Moser and Tardos point out that in their resampling approach, @math expected iterations of resampling are needed. Recently, Haeupler and Harris @cite conjectured that parallel resampling algorithms need @math time.
- While bringing concave valuations from Actuarial Risk Theory @cite into play, our framework encompasses general classes of @math -valuations, assuming the Risk-Positivity property, that also enjoy the Weak-Equilibrium-for-Expectation property, and a few additional properties. The @math -equilibria on which @cite @cite focused are a special case of our general framework.
- Fiat and Papadimitriou [Section 2] FP10 coined a notion termed as strict concavity, denoted here as @math -strict concavity, which is similar to but different than @math -strict concavity. It turns out that their difference is essential since @math is not @math -strictly concave while it is @math -strictly concave. See the Appendix for the definition of @math -strict concavity and a proof that @math is not @math -strictly concave. In fact, no concrete example of an @math -strictly concave valuation was given in @cite . Fiat and Papadimitriou [Theorem 3 & Observation 4] FP10 proved the sparsity of mixed @math -equilibria, when @math is @math -strictly concave: games with a mixed @math -equilibrium have measure @math . The sparsity of mixed @math -equilibria follows from [Theorem 1] BKS10 where it is established that @math is a Mean-Variance Preference Function [Claim 1] BKS10 . See for the definition of Mean-Variance Preference Functions and their relation to this work. Contrary to their sparsity, this work establishes that deciding the existence of mixed @math -equilibria is strongly @math -hard (Theorems and ).
- Several works were conducted in the context of tag-based access control policies for images @cite @cite @cite , showing some initial success in tying tags with access control rules. However, the scarcity of tags for many online images @cite , and the workload associated with user-defined tags precluded accurate analysis of images' sensitivity based on this dimension.
- As noted, the above approaches majorly involved visual feature such as SIFT, edges-direction coherence, and face detection, for privacy classification. Recently, the computer vision community has shifted towards convolutional neural networks for task such as object detection @cite @cite semantic segmentation @cite . Deep convolutional neural network has also acquired state of the art results on ImageNet (Highly challenging dataset used for object recognition) only using supervised learning @cite . described an approach to predicting the style of images by evaluating different visual features. They found that features learned from multi-layer network generally performed best when trained on 80K Flickr photographs.
- Like the task of AMR parsing, there have been various attempts to parse sentences into a logical form, given raw sentences annotated with such forms @cite @cite . The work by Zettlemoyer and Collins @cite attempts to map natural language sentences to a lambda-calculus encoding of their semantics. They do so by treating the problem as a structured learning task, and use a log-linear model to learn a Probabilistic (CCG) @cite , which is a grammar formalism based on lambda calculus.
- AMR aims to combine various semantic annotations to produce a unified annotation, but it mainly builds on top of PropBank @cite . PropBank has found extensive use in other semantic tasks such as shallow semantic parsing @cite ,
- In our work we used to build an AMR parser. comes from a family of algorithms called "Learning to Search (L2S)" that solves structured prediction problems by decomposing the structured output in terms of an explicit search space and then learning a policy that can take actions in this search space in the optimal way. Incremental structured perceptron @cite @cite , @cite , @cite , etc. @cite @cite @cite @cite @cite @cite are other algorithms that also belong to this family.
- Recent researches have focused on different aspects of SNSs that relate with families @cite @cite @cite . @cite @cite @cite @cite , researchers have investigated how Internet and SNSs psychologically affect human being. The work in @cite has focused on how family members negotiates in sharing resources to access the net. Our work is different because we investigate how one's imbalance SNS usage can affect the psychological wellbeing of other family members.
- @cite explored the link between socioeconomic factors and network structure using anonymized phone call records to reconstruct the national-level network of people living in the UK. Measures of socioeconomic development were constructed from the UK government's Index of Multiple Deprivation (IMD), a composite measure of prosperity based on income, employment, education, health, crime, housing of different regions within the country. They found that people living in more prosperous regions formed more diverse social networks, linking them to others living in distinct communities. On the other hand, people living in less prosperous communities formed less diverse, more cohesive social structures.
- @cite found that sentiment expressed in tweets posted around 78 census areas of London correlated highly with community socioeconomic well being, as measured by the Index of Multiple Deprivation (i.e., qualitative study of deprived areas in the UK local councils). In another study @cite they found that happy places tend to interact with other happy places, although other indicators such as demographic data and human mobility were not used in their research @cite .
- Other researcher used demographic factors and associated them to sentiment analysis to measure happiness in different places. For instance, @cite generated taxonomies of US states and cities based on their similarities in word use and estimates the happiness levels of these states and cities. Then, the authors correlated highly-resolved demographic characteristics with happiness levels and connected word choice and message length with urban characteristics such as education levels and obesity rates, showing that social media may potentially be used to estimate real-time levels and changes in population-scale measures, such as obesity rates.
- A number of innovative research works attempted to better understand human emotion and mobility. Some of these works focuses on geo-tagged location data extracted from Foursquare and Twitter. Researchers reported @cite @cite that Foursquare users usually check-in at venues they perceived as more interesting and express actions similar to other social media, such as Facebook and Twitter. Foursquare check-ins are, in many cases, biased: while some users provide important feedback by checking-in at venues and share their engagement, others subvert the rules by deliberately creating unofficial duplicate and nonexistent venues @cite .
- We can see that among the eight most relevant state-of-the-art techniques, only two @cite @cite have comparable false positive rates to ours ( @math ). A low false positive rate is paramount for a phishing detection technique, since this relates to the proportion of legitimate webpages to which a user will be incorrectly denied. The technique proposed by Ma @cite has a lower accuracy than in our system ( @math ). In addition, they use a testing set that does not represent real world distribution (3 legs 4 phishs) and use a cross-validation that does not assess scalability of the approach with a 1 1 ratio for learning to testing instances. Whittaker @cite report results similar to us in several metrics. However, they use a and their test set is actually than the training set (a sixth, at 1.5M)! Scalability and language-independence are likely to be poor since they use 100,000 mostly static features (bag-of-words).
- Table presents comparative performances results of our phishing detection system to the most relevant state-of-the-art systems. It presents the size of the testing sets used to evaluate each system and the provenance of the legitimate set, showing how representative the set is. For example, using popular websites (such as top Alexa sites) @cite @cite as the legitimate set is not representative. The ratio of training to testing instances indicates the scalability of the method and the ratio of legitimate to phishing instances shows the extent to which the experiments represents a real world distribution ( @math ) @cite @cite . We also identify the evaluation method (e.g., cross validation vs. training with old data and testing with new data). Finally, we present several metrics for assessing the classification performance. If data for any of the columns were missing from the original paper describing the system, we estimated them. For comparison purposes, if several experimental setups were proposed in a paper, we selected the most relevant to assess their practical efficacy using the following ordered criteria:
- We can see that among the eight most relevant state-of-the-art techniques, only two @cite @cite have comparable false positive rates to ours ( @math ). A low false positive rate is paramount for a phishing detection technique, since this relates to the proportion of legitimate webpages to which a user will be incorrectly denied. The technique proposed by Ma @cite has a lower accuracy than in our system ( @math ). In addition, they use a testing set that does not represent real world distribution (3 legs 4 phishs) and use a cross-validation that does not assess scalability of the approach with a 1 1 ratio for learning to testing instances. Whittaker @cite report results similar to us in several metrics. However, they use a and their test set is actually than the training set (a sixth, at 1.5M)! Scalability and language brand independence are likely to be poor since they use 100,000 mostly static features (bag-of-words).
- Binary embedding has not been fully recognized in the speaker recognition community. The limited research focuses on employing the advantages of binary codes in robustness and fast computing. For example, @cite proposed a time-spectral binary masking approach to improve robustness of speaker recognition in conditions with high interference. Besides, @cite presented a solution for large-scale speaker search and indexing under the i-vector model, where the search and indexing algorithm is based on LSH. The work proposed in @cite is more relevant to our proposal. By their approach, a universal background model (UBM) is employed to divide the acoustic space into subregions, and each subregion is populated with a set of Gaussian components. Each acoustic frame is then converted to a binary vector by evaluating the Gaussian components that the frame belongs to, and the frame-level vectors are finally accumulated to produce the segment-level speaker vector. Better robustness compared with the conventional GMM-UBM approach was reported by the authors.
- Our idea to use relations with an unknown value is inspired from fix-point based methods such as , which are used to reason about instantaneous feedback in cyclic combinational circuits and synchronous languages @cite @cite @cite . These methods deal with monotonic functions over the flat CPO with unknown as the least element. Because they assume total functions, they are limited to deterministic and input-receptive systems. Refinement is also absent from these frameworks.
- In @cite a model for synchronous nondeterministic systems is used for studying preservation of security properties by composition of systems, including delayed feedback. However this model does not treat non-input receptive systems.
- In @cite tweets collected using a set of keywords related to the H1N1 virus were used to monitor the public concern about the disease in the United States. Flu was the major concern in @cite @cite , where regression models were applied to estimate the disease incidence rate using data from Twitter and comparing to the reports provided by the official organizations. Regression models were also employed in a more recent work @cite , combining data from Twitter and search engines to estimate the flu incidence in Portugal.
- Our work also focuses on estimating the dengue epidemics, a neglected tropical disease, which is rarely explored in the context of social media channels. Differently from most works and similarly to @cite , our approach considers a fine geographic-scale analysis. However, we propose a latent shared-component generative model that accommodates different aspects of the scenario regarding the behavior of users in social media channels.
- There are three levels of BLAS, divided with respect to the complexity of operations. Level-1 (L1) BLAS targets vector operations in @math such as vector dot products and vector norms. Level-2 (L2) BLAS targets matrix-vector operations in @math such as matrix-vector multiplication. Level-3 (L3) BLAS @cite targets matrix operations in @math time such as matrix-matrix multiplications. The focus of this research is on L3 BLAS, which uses General Matrix Multiplication (GEMM) as the primary building block for the routines within the category. Therefore, the task of improving the performance of L3 BLAS can be reduced to the GEMM speed.
- As defined in @cite , a video highlight is a moment of major or special interest in a video. Generating highlight clips is thus different from the task of video summarization, which instead accounts for factors such as diversity'' and representativeness'' to convey a brief but comprehensive synopsis of a video. Despite this different goal, we review methods for video summarization in addition to video highlight detection because of similarities between the two topics.
- A comprehensive review of video summarization can be found in @cite . Among recent methods, several are guided by saliency-based properties such as attention @cite , interestingness @cite @cite @cite , and important people and objects @cite . However, the most salient frames in a video do not necessarily correspond to its highlights, which depend heavily on the video domain. Others aim to provide a comprehensive synopsis based on connectivity of sub-events @cite or diversity of video segments @cite @cite . While this helps to provide a complete overview of a video, a highlight clip instead is focused on only certain segments that are specific to the video domain, while discarding the rest.
- The one-class learning of our recurrent auto-encoder is related to works on novelty detection, which aim to identify outliers from an observed class. In @cite , novelty detection is performed for audio features using an auto-encoder with LSTM. Our concurrent work deals instead with RGB video data, for which meaningful features are more challenging to extract. We address this through temporal video segmentation, extraction of high-level spatial-temporal features for each segment, and then temporal pooling, before feeding to the auto-encoder. Moreover, we introduce a novel shrinking loss function to address the noisy training data in our context.
- There exist other unsupervised novelty detection techniques that could potentially be applied to our problem, such as the unsupervised one-class learning in @cite or outlier-robust PCA @cite . In our work, we chose the auto-encoder as our basic unsupervised framework because its properties are well-suited to our application, such as scalability, easy parallelization, and seamless integration with LSTM cells. How to customize other novelty detection techniques for video highlight detection is a potential direction for future investigation.
- Estimating the number of distinct values for data streams is a well studied problem. The problem of estimating result sizes of set expressions over multiple streams was concretely formulated by @cite . Motivated by the question of handling streams containing both insertions and deletions, their construction involves a 2-level hash function that essentially stores a set of counters for each bit-position of an HLL-type hash, and hence is inherently more resource intensive, both in terms of the space and update times.
- K'th Minimum Value sketches were introduced by Bar- @cite , and developed into an unbiased scheme that handles set expressions by @cite . Our own scheme is closely related to the schemes proposed and analyzed in Cohen and Kaplan @cite , and in Gibbons and Tirthapura @cite . Chen, Cao and Bu @cite propose a somewhat different scheme for estimating unique counts with set expressions that is based on a data-structure related to the probabilistic counting'' sketches of @cite , and also to the multi-bucket KMV sketches of @cite (with @math ). However, the guarantees proved by @cite are asymptotic in nature, and their system's union sketches are the same size as base sketches, and therefore do not provide the increased accuracy that is possible with a growing'' union rule as in @cite , in @cite , and in this paper's scheme.
- Bottom-k sketches @cite @cite are a weighted generalization of KMV that provides unbiased estimates of the weights of arbitrary subpopulations of identifiers. They have small errors even under 2-independent hashing @cite . A closely related method for estimating subpopulation weights is priority sampling @cite . Although this paper's Theta-Sketch Framework offers a broad generalization of KMV, it is not clear that it can support the entire generality of bottom-k sketches for weighted sets.
- This paper's Alpha Algorithm'' is inspired by the elegant Approximate Counting method of Morris @cite , that has previously been applied to the estimation of the frequency moments @math , for @math . By contrast, our task is to estimate @math . The Alpha Algorithm is able to do this because its Approximate Counting process is tightly interleaved with another process that removes duplicates from the input stream while maintaining a small memory footprint by using feedback from the approximate counter.
- @cite gave a streaming algorithm for the problem that outputs a @math -approximation with constant probability, using @math bits of space. This improves over the bit-complexity of HLL by roughly a @math factor (and avoids the assumption of truly random hash functions). Like HLL, it is not known how to extend the algorithm to handle @math queries for non-trivial properties @math , and the algorithm does not appear to have been implemented @cite .
- Tirthapura and Woodruff @cite give sketching algorithms for estimating @math queries for a special class of properties @math . Specifically, they consider streams that contain tuples of the form @math , where @math is a numerical parameter, and the subpopulation @math is specified via a lower or upper bound on @math .
- In very recent work, Cohen @cite and Ting @cite have proposed new estimators for (called "Historical Inverse Probabililty" (HIP) estimators in @cite ). Any sketch which is generated by hashing of each element in the data stream and is not affected by duplicate elements (such as HLL, KMV, Adaptive Sampling, and our Alpha Algorithm) has a corresponding HIP estimator, and @cite @cite show that the HIP estimator reduces the variance of the original sketching algorithm by a factor of 2. However, HIP estimators, in general, can only be computed when processing the stream, and this applies in particular to the HIP estimators of KMV and Adaptive Sampling. Hence, they do not satisfy the mergeablity properties necessary to apply to multi-stream settings.
- Relational data can also be used to learn distributed representations of entities in knowledge graphs, entities which may correspond to or can be mapped to words. A general approach is to implicitly embed the graph structure through vertex embeddings and rules (or transformations) for traversing it. bordes2011learning scored the similarity of entities under a given relationship by their distance after transformation using pairs of relationship-specific matrices. socher2013reasoning describe a neural network architecture with a more complex scoring function, noting that the previous method does not allow for interactions between entities. The TransE model of bordes2013translating (and extensions such as wang2014knowledge , fan2014transition , and lin2015learning ) represents relationships as , motivated by the tree representation of hierarchical relationships, and observations that linear composition of entities appears to preserve semantic meaning @cite . These approaches are uniquely concerned with relational data however, and do not consider distributional semantics from free text. faruqui2015retrofitting and johansson-nietopina:2015:NAACL-HLT describe methods to modify pre-existing word embeddings to align them with evidence derived from a knowledge base, although their models do not learn representations .
- Finally, we note that a recent extension of word2vec to full sentences @cite using a fast generative model exceeds the scope of our model in terms of sentence modeling, but does not explicitly model latent relationships or tackle transfer learning from heterogeneous data sources.
- Distribution clustering can be done subject to different affinity definitions. For example, Bregman clustering pursues the minimal distortion between the cluster prototype---called the Bregman representative---and cluster members according to a certain Bregman divergence @cite . In comparison, D2-clustering is an extension of K-means to discrete distributions under the Wasserstein distance @cite , and the cluster prototype is an approximate Wasserstein barycenter with sparse support. In the D2-clustering framework, solving the cluster prototype or the centroid for discrete distributions under the Wasserstein distance is computationally challenging @cite @cite @cite . In order to scale up the computation of D2-clustering, a divide-and-conquer approach has been proposed @cite , but the method is ad-hoc from optimization perspective. A standard ADMM approach has also been explored @cite , but its efficiency is still inadequate for large datasets. Although fast computation of Wasserstein distances has been much explored @cite @cite @cite , how to perform top-down clustering efficiently based on the distance has not. We present below the related work and discuss their relationships with our current work.
- The centroid of a collection of distributions minimizing the average @math th-order power of the @math Wasserstein distance is called Wasserstein barycenter @cite . In the D2-clustering algorithm @cite , the 2nd order Wasserstein barycenter is simply referred to as a prototype or centroid; and is solved for the case of an unknown support with a pre-given cardinality. The existence, uniqueness, regularity and other properties of the 2nd order Wasserstein barycenter have been established mathematically for continuous measures in the Euclidean space @cite . But the situation is more intricate for discrete distributions, as will be explained later.
- Given @math arbitrary discrete distributions each with @math support points, their true Wasserstein barycenter in theory can be solved via linear programming @cite @cite . This is because the support points of the Wasserstein barycenter can only locate at a finite (yet huge) number of possible positions. However, solving the true discrete barycenter quickly becomes intractable even for a rather small number of distributions containing only 10 support points each. An important theoretical progress has been made by @cite who proved that the actual support of a true barycenter of @math such distributions is extremely sparse, with cardinality @math no greater than @math . However, the complexity of the problem is not reduced practically because so far there is no theoretically ensured way to sift out the optimal sparse locations. On the other hand, the new theory seems to backup the practice of assuming a pre-selected number of support points in a barycenter as an approximation to the true solution.
- In applications on high-dimensional data, it is desirable to optimize the support points rather than fix them from the beginning. This however leads to a non-convex optimization problem. Our work aims at developing practical numerical methods. In particular, the method optimizes jointly the locations and weights of the support points in a single loop without resorting to a bi-level optimization reformulation, as was done in earlier work @cite @cite .
- Recently, a series of works have been devoted to solving the Wasserstein barycenter given a set of distributions (e.g. @cite @cite @cite @cite @cite ). How our method compares with the existing ones depends strongly on the specific data setting. We discuss the comparisons in details below and motivate the use of our new method in D2-clustering.
- First of all, it is interesting to note that if the distributions do not share the support set, IBP @cite has the same memory complexity @math (for caching the distance matrix per instance) as our approach. In addition, B-ADMM @cite , based on which our approach is developed, has the following advantages: (1) It yields the exact OT and distance in the limit of iterations. Note that the ADMM parameter does not trade off the convergence rate. (2) It exhibits different convergence behaviors, accommodating warm starts and early stops (to be illustrated later), valuable traits for the task of D2-clustering. (3) It works well with single-precision floats, thus not pestered by the machine precision constraint. In contrast, this issue is serious for IBP, preventing it from robustly outputting adequately accurate discrete Wasserstein barycenters with sparse support (See @cite and our experiments). Here the adequately accurate'' means close to a local minimizer of sum of the (squared) Wasserstein distances.
- Our main algorithm is inspired by the B-ADMM algorithm of Wang and Banerjee @cite for solving OT fast. They developed the two-block version of ADMM @cite along with Bregman divergence to solve the OT problem when the number of support points is extremely large. Its algorithmic relation to IBP @cite is discussed in . The OT problem at a moderate scale can in fact be efficiently handled by state-of-the-art LP solvers @cite . As demonstrated by the line of work on solving the barycenter, optimizing the Wasserstein barycenter is rather different from computing the distance. Our modified B-ADMM algorithm is for solving the Wasserstein barycenter problem. Naively adapting the B-ADMM to Wasserstein barycenter does not result in a proper algorithm. The modification we made on B-ADMM is necessary. Although the modified B-ADMM approach is not guaranteed to converge to a local optimum, it often yields a solution very close to the local optimum. The new method is shown empirically to achieve higher accuracy than IBP or its derivatives.
- have already modeled the impact of data aggregation schemes in WSN @cite . More specifically, they showed the impact of network density on data aggregation using directed diffusion with a greedy algorithm to construct trees. In their work, they proposed the in-network aggregation (along with the greedy tree) and compared the perfect aggregation (which saves the headers and merges the content in new packets with the same length) with the linear aggregation (which also saves the headers, but appends the content in larger payloads). According to simulation results, the greedy algorithm can reduce @math 43 in high-density networks. Finally, the authors concluded that, in high-density networks, more energy is saved by the greedy algorithm, and the delays are as good as using the opportunistic algorithm.
- More recently, focused on calculating the most energy efficient deployment strategy for WSN using an integral programming model @cite . They used a regular hexagonal cell architecture @cite @cite to model the location of sensor nodes in the plane, which, in fact, is similar to a ring model with @math . Using this model, they formulated the energy consumption of sensor nodes and GW based on the energy used to transmit, receive and process data in sensor nodes. Finally, they also evaluated the impact of data aggregation in the energy consumption of the sensor nodes.
- The works above inspired our model for data transmissions. However, different from our proposal, they focused only on data aggregation and did not evaluate the effects of DPS in WSN . Concerning the evaluation of DPS , were the first authors to introduce statistical methods to choose which prediction model better fits a certain environment @cite . They created a formula to estimate the Prediction Cost ( @math ), which considers the percentage of transmitted measurements ( @math ) and the user desired level of accuracy ( @math ) @cite . More recently, an extended formula was designed and implemented in real sensor nodes to compare the savings using several prediction methods, such as the , ES and ARIMA methods @cite @cite . The new formula is more generic than the original proposed and also considers the prediction models' memory footprint ( @math ) as a significant computational cost for sensor nodes:
- There have been a number of papers that look at the problem of using MapReduce to compute marginals. Probably the closest work to what we present here is in @cite . This paper expresses the goal of minimizing communication, and of partitioning the work among reducers. It does not, however, present concrete bounds or algorithms that meet or approach those bounds, as we shall do here.
- @cite considers constructing a data cube using a nonassociative aggregation function and also examines how to deal with nonuniformity in the density of tuples in the cube. Like all the other papers mentioned, it deals with constructing the entire data cube using multiple rounds of MapReduce. We consider how to compute only the marginals of one order, using one round. We may assume that locally, at each reducer, higher-order marginals are computed by aggregating lower-order marginals for efficiency, but this method does not result in additional MapReduce rounds.
- @cite looks at using MapReduce to form a data cube from data stored in Bigtable. @cite and @cite are implementations of known algorithms in MapReduce. Finally, @cite talks about extending MapReduce to compute data cubes more efficiently.
- Bottou and Gallinari proposed to decompose neural networks into cooperating modules @cite @cite . Decomposing more general algorithms or models into collections of interacting agents dates back to the shrieking demons that comprised Selfridge's Pandemonium @cite and a long line of related work @cite @cite @cite @cite @cite @cite @cite @cite . The focus on components of neural networks as players, or rational agents, in their own right developed here derives from work aimed at modeling biological neurons game-theoretically, see @cite @cite @cite @cite @cite .
- A related approach to semantics based on general value functions can be found in Sutton @cite , see remark . Computation graphs as applied to backprop are the basis of the Python library Theano @cite @cite @cite and provide the backbone for automatic algorithmic differentiation @cite @cite .
- Grammars are a technical term in the theory of formal languages relating to the Chomsky hierarchy @cite . There is no apparent relation between that notion of grammar and the one presented here, aside from both relating to structural rules governing composition. Formal languages and deep learning are sufficiently disparate fields that there is little risk of terminological confusion. Similarly, the notion of semantics introduced here is distinct from semantics in the theory of programming languages.
- Although game theory was originally developed to model human interactions @cite , it has been pointed out that it may be more directly applicable to interacting populations of algorithms, so-called @cite @cite @cite @cite @cite @cite . This paper goes one step further to propose that games played over first-order communication protocols are a key component of the foundations of deep learning.
- A source of inspiration for the essay is Bayesian networks and Markov random fields. Probabilistic graphical models and factor graphs provide simple, powerful ways to encode a multivariate distribution's independencies into a diagram @cite @cite @cite . They have greatly facilitated the design and analysis of probabilistic algorithms. However, there is no comparable framework for distributed optimization and deep learning. The essay is intended as a first step in this direction.
- More sophisticated active learning algorithms have been widely studied, achieving optimal rates for piecewise constant functions in @cite and for the linear support vector machine in @cite . In both cases, the algorithm begins by uniformly sampling the entire feature space. Again considering the problem of interest, the central basin of Lake Erie has an area of approximately 14,000 km @math , making this approach infeasible. In contrast, the algorithm studied in @cite was used in @cite to measure the hydrodynamics of Lake Wingra in Madison, WI, which has an area of 1.3 km @math .
- Several approaches have been used to study load balancing in HetNets, including stochastic geometry @cite @cite , game theory @cite and system-level simulations @cite @cite . Meanwhile, in industry, proactive load balancing is accomplished by UE association towards the small cells @cite @cite . Biasing refers to artificially adding a bias value (e.g., @math dB) to received signal power from small cell layer at UEs. Our initial study on load balancing @cite formulated a network utility maximization (NUM) problem for user association in HetNets with single-antenna BSs, where resources are equally allocated among users in the same cell. The equal resource allocation can be suboptimal if the user associations happen on a much slower time scale than the channel variations. In general, the user association and scheduling (resource allocation) problems are coupled, and it is quite difficult to jointly optimize them.
- . A key benefit of massive MIMO is that the extra diversity afforded by the large antenna array averages out the fast fading, and thus the instantaneous rate stabilizes to the long-term mean which changes on slow time scales. As shown in @cite , the instantaneous rates can be predicted with peak-rate proxies, which are independent of scheduled instances. This property allows the decoupling of user association and scheduling, which is exploited to achieve near-optimal load balancing in massive MIMO HetNets with cellular transmission (where data for each user is transmitted from a single BS) @cite .
- . MIMO techniques also provide the option of serving a user at high rates from multiple BSs -- referred to as coordinated multi-point transmission (CoMP), which is proposed as one of the core features in LTE-Advanced @cite @cite @cite . The set of BSs that cooperatively serve the same user is referred in this paper as a . Paper @cite studies how to determine the BS clusters, while @cite @cite @cite @cite @cite @cite @cite investigate jointly optimized designs involving some of the following aspects: BS cluster selection, beamforming (e.g., coordinated beamforming and joint transmissions), user scheduling and power allocation. In studies such as @cite @cite @cite @cite , the complexity of proposed algorithms can become prohibitive as number of antennas increases. On the other hand, an efficient suboptimal precoder for the single-cell scenario with large antenna arrays is proposed in @cite .
- . As the macrocells that users are offloaded from now become strong interferers for these offloaded users, the increased interference eats into the gains offered by load balancing. This motivates us to jointly consider user association and interference management. Besides CoMP, another popular interference management approach is to leave some macro resource blocks (RBs) blank, similar to enhanced intercell interference coordination (eICIC) in 3GPP @cite . The key difference between RB blanking in our work and eICIC is that eICIC focuses on the time domain, while in this work blanking is applied in both time and frequency domains. We call the RBs where macro BSs are muted the , while the rest of RBs are called . Several works have considered the joint problem of user association and RB blanking. For example, @cite @cite proposes a dynamic approach adapting the muting duty cycle to load variations, while @cite @cite @cite @cite @cite consider a more static approach.
- For general multi-cell massive MIMO HetNets, the joint optimization of user association and interference management including joint transmission and resource blanking is still an open issue. In this work, we combine various aspects of resource allocation and interference management including resource blanking, joint transmission, association, user scheduling, etc. for massive MIMO deployments. We focus on a distributed-MIMO form of CoMP, which allows local precoding at each BS and does not require channel state information (CSI) exchanges among cooperating BSs @cite . We call this specific form of CoMP as Local Joint Transmission (LJT). LJT allows us to develop a systematic resource allocation approach for CoMP (including cellular transmission as a special case). Other interference management approaches (e.g. @cite ) can also be adopted at the cost of additional complexity and overheads (e.g., schemes with joint precoding as discussed in @cite ), but such designs are beyond the scope of this paper.
- Some previous works have demonstrated that interaction between scene and objects have the capability to promote each other @cite @cite @cite @cite . The typical idea is to build the relationship between scenes and objects using a graphical model such as Markov Random Field or Conditional Random Field @cite . Though these works have achieved superior results, they are based on hand-crafted features, which means the feature extraction and classification in these works are not in a unified optimization framework. Compared to these works focusing on simultaneously labeling, our work is more close to Object Bank @cite since we focus on regularizing scene classification with semantic segmentation. Object Bank proposed a high-level representation for scene classification by encoding the images with combination of a large amount of object detectors. However, the feature extraction and scene classification in Object Bank are still optimized separately, and it requires pre-training a large number of object detectors. Recently, the superior results achieved with deep learning methods suggest that learning features with a fully trainable architecture may be a better choice. In this paper, we implemented the scene classifier using a fully trainable deep architecture with a single semantic segmentation branch encoding all object-level information.
- As for the conventional deep learning methods, the most successful CNN model in scene classification is Place-CNN @cite , which is trained on 2.5 million labeled images belonging to 476 scene classes using the well-known architecture Alexnet @cite . Before @cite , the performance of CNN on scene classification was within the range of performances of some hand-crafted features based implementations. As pointed out in @cite , one reason of the relatively poor result on scene classification of CNN is due to the larger diversity of scene-centric images compared to object-centric images, which means scene classification has higher requirement on generalization ability. By encouraging CNN to classify the scene through implicit understanding of object existence, we developed a scene classifier also based on Alexnet and achieves better generalization ability than Place-CNN with only 5 thousand training images.
- To the best of our knowledge, considering multiple tasks is rarely exploited in deep learning methods. An exception is the refinement in DeepID-Net @cite , which took the image classification result to refine the object detection. More specifically, they introduced another separated network for image classification and concatenated the estimated image probability with the estimated object probability for a further classification, which means the information of two tasks are only combined after independent training, instead of simultaneous training as implemented in this paper.
- We first mention works that generate snippets for native RDF documents. Ge @cite , and Penin @cite focus on the generation of snippets for ontology search. Bai @cite generate snippets for a semantic web search engine.
- @cite , the authors first identify a topic thanks to an off-line hierarchical clustering algorithm. Next, they compute a list of RDF sentences (i.e. sets of connected RDF statements) semantically close to the topic. Finally, they rank the selected RDF statements by considering both structural properties of the RDF graph and lexical features of the terms present in the ontology (by way of a Wordnet-based similarity measure).
- @cite , the authors first transform the RDF graph into a term association graph in which each edge is associated with a set of RDF sentences. Their objective is to produce a compact representation of the relationships existing between the terms of the query. These relationships are to be found in the RDF graph. To do this, they decompose the term association graph into maximum r-radius components in order to avoid long distance relations between query terms. Next, they search sub-snippets in these components (i.e. connected subgraphs that link some of the query-terms). Finally, they select some of the sub-snippets to form the final snippet.
- @cite , the authors first assign a topic to the RDF document (they use a property such as p:primaryTopic if it exists, otherwise they rely on a heuristic based on the comparison of the URI of the candidates topic-nodes with the text of the URL of the RDF document). Next they design a ranking algorithm for RDF statements. Particularly, they introduce the notions of correlative (e.g. foaf:surname and foaf:family ) and exclusive (e.g. foaf:name and foaf:surname ) properties. Finally, they use this ranking algorithm to give the user a set of relationships between the query-related statements and the topic-related statements.
- To sum up, we agree with Ge @cite that the main benefit of possessing highly structured data from an RDF graph is the possibility to find non-trivial relationships among the query terms themselves, and also between the query terms and the main concepts of the document. Moreover, we agree with Penin @cite and Bai @cite about the necessity to design a ranking algorithm for RDF statements that considers both the structure of the RDF graph and lexical properties of the textual data. However, we find ourselves in an inverted situation with genuine text extracted from classical Web pages, and RDF graphs automatically generated from these Web pages.
- Moreover, a study made in 2012 @cite on the over 40 million websites of the Common Crawl corpus http: commoncrawl.org shows that 5.64 However, most of the existing approaches that can be used to rank the resources of graphs coming from the Web of data are not well adapted to this task. Thus, OntologyRank @cite (used by Swoogle) introduces a modified version of PageRank with a teleportation matrix that takes into account the types of the links between ontologies. Similarly, PopRank @cite offers a modified PageRank that considers the different types of predicates between resources. RareRank @cite introduces a modified PageRank with a teleportation matrix that takes into account topical relationships between resources as available from ontologies. The approach introduced in @cite modifies the teleportation matrix by taking into account the ranking of the Web pages within which the resources were discovered. Since this approach can be applied to our context, we include it to our evaluations (see section ). Finally, TRank @cite addresses the task of ranking entity types given an initial entity and its textual context.
- In this section, we provide an overview on existing models for periodic interval-based, slotless protocols, such as BLE. As already mentioned, exact models only exist for the trivial case of @math . This implies that the duration of one scan window must be larger than the time between two advertising packets. Therefore, every scan attempt is successful and the discovery latency is limited to roughly @math . The first known model (for @math ) has been presented along with the introduction thew STEM-B protocol @cite , which is one of the first PI-based protocols that have been proposed. In @cite @cite and @cite , this solution has been adopted to the BLE protocol to account for multiple channels, but the limitation to @math remains.
- Usually, PI-based protocols are considered to be non-deterministic, since no upper bound could be determined until now. To overcome the lack of determinism, it has been proposed to configure the BLE protocol such that the parameter values fulfill the CRT @cite . However, such configurations restrict the range of possible valuations unnecessarily. In summary, to the best of our knowledge, no valid model for PI-based protocols for the general case ( @math ) has been presented until now.
- Contemporary techniques for localization and tracking in confined environments are very basic. They are typically based on manual reporting of the employee's location using paging phones or video surveillance @cite @cite . Moreover, there are few proposals in the literature, based on fingerprinting @cite @cite @cite @cite , trilateration @cite @cite , centroid @cite and Bayesian filtering @cite @cite .
- More specifically, in @cite , a fingerprinting technique was proposed, in which seven relevant parameters (including mean excess delay, total received power, and delay spread) were learned offline from wideband impulse responses measured at hundreds of locations. Then, these 7D vectors were used as the input to an artificial neural network pattern-matching algorithm. The measurements were conducted in a gallery of the CANMET mine, a former gold mine located in Quebec, Canada. This method was then improved in @cite by using more receivers with known positions. The fingerprinting techniques @cite @cite , based on WiFi signals, have been also applied in subway tunnels in Seoul, S. Korea. The main problem of these algorithms is that they are not well suited for dynamic propagation environments (e.g., caused by movement of heavy machinery) in which the fingerprints have to be updated very frequently.
- In @cite @cite , ultra-wideband (UWB) measurements were used for positioning. They were motivated by a high ranging accuracy in cluttered environments and low-cost implementation of the communication system. To solve the trilateration problem, many types of algorithms have been applied, including linearized least-squares, Gauss-Newton and bounding-box methods. The measurements were performed in the same environment as the one studied in @cite . The main drawbacks of these algorithms are that the sensor nodes have to be precisely deployed and maintained, and that the algorithms are sensitive to outliers.
- In @cite , a centroid algorithm was proposed, in which the miner's location was found by averaging the coordinates of the detected anchors. The algorithm is a part of a structure-aware self-adapting (SASA) WSN, which is capable of detecting structure variations caused by mine collapses. The main problem of this approach is that it requires a high density of uniformly deployed sensor nodes.
- In @cite , Bayesian point-mass (grid-based) filtering was applied to track mine vehicles. The main goal was monitoring and control of ore extraction from the draw points in a mine in Australia. Since the draw points are very close to each other, high tracking accuracy is required. The main problem of this approach is that it requires many grid points in order to obtain sufficiently accurate estimates.
- Finally, the results of the measurement campaign @cite , carried out in a basement tunnel of Link "oping university and an iron-ore mine in Kiruna, Sweden, indicated that UWB time-of-arrival (TOA) allows very accurate ranging in line-of-sight (LOS) and non-LOS (NLOS) scenarios caused by thin obstacles. However, if the direct path is blocked by a thick tunnel wall, the TOA-based ranging leads to a relatively large bias. Moreover, the analysis showed that NLOS conditions cannot be accurately discriminated from LOS conditions, which means that (Bayesian) soft-decision algorithms are required for accurate ranging and positioning in these environments. The previously described state-of-the-art algorithms assume that the positions of the sensors are perfectly known, which is not necessarily the case due to imprecise placement and or sensor drops caused by vibrations or wall collapses. Although probably not available in confined environments nowadays, we also envision that uncertain sensors' positions can be an outcome of some (cooperative) sensor network localization algorithm @cite @cite . One possible solution to this problem is to manually and periodically verify that the sensors positions are correct. However, this approach may be too costly and even infeasible in some areas due to the on-going activities.
- Hidden Markov models are widely used in simulating the state space based disease progression @cite @cite @cite @cite , since HMMs actually mimic the behavior of an SIS model with discretized timestamps and are convenient in terms of the development of inference algorithm for different variants. In our work, we also adopt this basic framework to design our model. Other work like @cite @cite proposed a group lasso formulation or multi-task learning framework using optimization, and is far from our domain. A recent work by @cite learned a continuous time series model looking at trajectories for different patients, but ignoring interactions between chains. Our work attempts to unify social network modeling of individuals using a hierarchical structure. @cite utilized a fixed social network analysis on susceptible-infectious-recovered (SIR) models to identify high-risk individuals. @cite 's work on close proximity interactions (CPIs) of dynamic social networks at a high school indicated immunization strategies are more credible if extra contact data is provided.
- The idea of using hierarchies to improve model flexibility is extensively studied in topic modeling, in models such as latent Dirichlet allocation (LDA) @cite @cite @cite . @cite used a sigmoid link function, introduced in Relational Topic Model to learn fixed networks of documents. These, and further works have exemplified a trend in data-driven machine learning applications -- hierarchical modeling is used to make inferences when the data structure is complex. Our work can be considered as a hierarchical extension of either GCHMMs @cite or topic HMMs @cite with a nested transition function. From this viewpoint, we can also interpret our graphical model in terms of hierarchical LDA modeling.
- Within this first group, Liu @cite formulate salient object detection as a foreground-background segmentation problem and learn a conditional random field to combine local, regional and global features for saliency detection: multi-scale contrast, center-surround histogram and color spatial-distribution, producing binary saliency masks.
- Achanta @cite propose a frequency-tuned method that defines pixel saliency using color differences from the average image color and a Gaussian blurred version of the image.
- A limitation of these methods is that they tend to highlight edges around salient objects but do not obtain high saliency values for the complete objects. A review of other pixel-based models can be found in @cite .
- Cheng @cite propose a method that simultaneously evaluates global contrast differences and spatial coherence. They first segment the image into regions using a graph-based segmentation technique. The saliency of a region is calculated using a global contrast score measured by the region's contrast and spatial distances to other regions in the image.
- Perazzi @cite segment the image using an adaptation of SLIC superpixels, and compute two measures of contrast that rate the uniqueness and spatial distribution of superpixels. The two maps are combined and the saliency of each pixel is defined as a weighted combination of the saliency of its surrounding regions.
- Zhu @cite construct an undirected graph by connecting adjacent SLIC superpixels and formulate the salient object detection problem as the optimization of the saliency values of the superpixels. The cost function is defined in terms of background and foreground weights that measure the boundary connectivity and the contrast of superpixels.
- Margolin @cite integrate pattern and color distinctness using Principal Component Analysis to find components that best explain the variance in the data. They apply PCA to represent the set of patches of an image and use this representation to estimate distinctness, combined with standard techniques for color uniqueness and organization priors.
- Yan @cite use a hierarchy of three levels. They first generate an over-segmentation of the image by a watershed-like method and then apply an iterative merging process based on the scale and the color of the regions. Saliency maps are created for the three levels and merged into a final map by hierarchical inference. This inference is equivalent to applying a weighted average to all single saliency maps with optimal weights for each region.
- Liu @cite propose a framework termed as saliency tree. They first over-segment the image using gPb-UCM and generate an initial saliency map based on color contrast and spatial sparsity of regions and object prior. Next, a BPT is created using the saliency of the regions to define the merging criterion and merging order. Based on the analysis of the tree structure a final pixel-based saliency map is derived.
- The main idea of clustering is to group similar objects. In order to discover which data are similar, several distance (or dissimilarity) measures were defined in the literature. In this paper, we use the terms similarity'' and distance'' in inverse concepts. In the case of time series distance measures, the distance measures can be classified into four categories @cite : shape-based, edit-based, feature-based, and structure-based.
- A large body of work in computer vision has focused on the problem of object detection and pose estimation, including instance and category recognition, rigid and articulated objects, and coarse (quantized) and accurate (6D) poses. Pose estimation has been an active topic, ranging from template-based approaches @cite @cite , sparse feature-based approaches @cite , and dense approaches @cite @cite . In the brief review below, we focus on techniques that specifically address CNNs and analysis-by-synthesis.
- In our work we build upon the framework of @cite . As in @cite we use the regression-classification random forest to obtain the predictions described above. We also use their optimization scheme, but replace the energy function with a novel one, based on a CNN, that is trained. The key difference is that while energy function in @cite has only a few parameters which can be trained via discriminative cross-validation procedure, the CNN has around 600K which we train with a maximum likelihood procedure. We show that this richness of parameters makes remarkable difference, and practical challenges such as occlusion and noise are much better dealt with. This approach will be described in the next section.
- Although the notion of using a non-trivial Gaussian process prior mean is not new, it is usually expected to encode domain expert knowledge or known structure in the response surface. Only one recent work, to the best of the authors' knowledge, has considered using the prior mean as a regularization term and it was primarily to avoid selecting points along boundaries and in corners of the bounding box @cite .
- The classical secretary problem was introduced more than 50 years ago (e.g., @cite ). Since its introduction, many variants and extension of that problem have been proposed and analyzed @cite @cite @cite @cite . The problem that is closest to the problem implied from our online model is the submodular knapsack secretary problem @cite @cite @cite . An instance of this problem consists of a set of @math secretaries that arrive in a random order, each of which has some intrinsic cost. An additional ingredient of the input is a monotone submodular set function that quantifies the value gained from any subset of secretaries. The objective is to select a set of secretaries of maximum value under the constraint that their overall cost is no more than a given budget parameter. Note that our model extends this setting by having a more general influence function that is submodular over the integer lattice. Essentially, this adds another layer of complexity to the problem as we are not only required to decide which secretaries to select, but we also need to assign them budgets.
- Many approaches have been studied for the problem of graph clustering, including hierarchical clustering, divisive clustering, spectral methods, random walk (for a survey, see @cite @cite ). To evaluate the quality of a clustering @cite regardless of the cluster number, the modularity criterion @cite is now widely accepted in the literature and has even been treated as an objective function in clustering algorithms @cite @cite . This criterion aims to obtain dense clusters where the within-cluster edge density is significantly above the expected edge density in case of random edges following the same vertex degree distribution. Actually, not all graphs follow a @cite , with a structure consisting of natural clusters. Yet, all clustering algorithms output a partition into clusters for any input graph. While the clustering setting is relevant in many domains, our approach does not rely of such cluster tendency assumption and may have a wider range of application. This is illustrated experimentally in .
- More expressive graph models aim at searching a partition of both the source and target vertices into clusters, with different types of interaction between clusters. The cross-product of the two partitions of vertices form a partition of the edges into or . This modeling approach is called blockmodeling and has been thoroughly studied for decades. In early approaches @cite @cite , non-stochastic blocks are considered, with a focus on predefined types of block patterns. The blockmodel is searched either indirectly using a (dis)similarity measure between pairs of vertices and then applying a standard clustering algorithm, or directly by optimizing an ad hoc function measuring the fit of real blocks to the corresponding predefined types of blocks. The limit of these approaches is that they do not cope with the stochastic nature of many real world datasets.
- One feasible solution is proposed in @cite , namely, distributed spectral embedding (DSE). For DSE, a spectral embedding scheme is first performed on each view, respectively, producing the individual low-dimensional representations. After that, a common compact embedding is finally learned to guarantee that it would be similar with all single-view's representations as much as possible. Although the spectral structure of each view can be effectively considered for learning a multiview embedding via DSE, the complementarity between different views is still neglected.
- To effectively and efficiently learn the complementary nature of different views, multiview spectral embedding (MSE) is introduced in @cite . The main advantage of MSE is that it can simultaneously learn a low-dimensional embedding over all views rather than separate learning as in DSE. Additionally, MSE shows better effectiveness in fusing different views in the learning phase.
- However, both DSE and MSE are based on nonlinear embedding, which leads to a serious computational complexity problem and the problem @cite . In particular, when we apply them to classification or retrieval tasks, the methods have to be re-trained for learning the low-dimensional embedding when new test data are used. Due to their nonlinearity nature, this will cause heavily computational costs and even become impractical for realistic and large-scale scenarios.
- Joachims @cite proposed a SVM method to optimize multivariate nonlinear performance measures, including F-score, AUC etc. This method takes a multivariate predictor, and gives an algorithm to train the a multivariate SVM in polynomial time for large classes so that the potentially non-linear performance measures can be optimized. Moreover, the translational SVM with hinge loss function can be treated as a special case of this method.
- @cite proposed a smoothing strategy for multivariate performance score optimization., in particular PREBP and AUC. The proposed method combines Nesterov's accelerated gradient algorithm and the smoothing strategy, and obtains an optimization algorithm. This algorithm converges to a given accurate solution in a limited number of iterations corresponding to the accurate.
- Mao and Tsang @cite proposed a generalized sparse regularizer for multivariate performance measure optimization. Based on the this regularizer, a unified feature selection and general loss function optimization is developed. The formulation of the problem is solved by a two-layer cutting plane algorithm, and the convergence is presented. Moreover, it can also be used to optimize the multivariate measures of multiple-instance learning problems.
- @cite proposed to learn a nonlinear classifier for optimization of nonlinear and nonsmooth performance measures by novel two-step approach. Firstly, a nonlinear auxiliary classifiers with existing learning methods is trained, and then it is adapted for specific performance measures. The classifier adaptation can be reduced to a quadratic program problem, similar to the method introduced in @cite .
- Classic random forests introduced by Breiman @cite combine several decision trees @cite with bagging @cite . The main idea of random forests is based on the early work of @cite on the random subspace method, the feature selection work of @cite , the way of random split selection of @cite . Based on the seminal work of Breiman @cite , @cite suggests that it is best to average across sets of trees with different structures but not any of the constituent trees. @cite present a unified, efficient model of random decision forests which can be applied to a number of machine learning, computer vision and medical image analysis tasks. With the development of random forests in recent years, they have been applied to a wide variety of real world problems @cite , @cite , @cite , @cite .
- Despite the successful applications of random forests in practice, the mathematical properties behind them have not been well understood. For example, the early theoretical work of @cite , which is essentially based on mathematical heuristics, is not formalized to rigorous theory.
- In theory, there are two main properties of theoretical interests related to random forests. One is the consistency of the models, that whether it can converge to an optimal solution as the data set grows infinitely large. The other is the rate of convergence. Our paper mainly focuses on consistency, which @cite has proved that Breiman's random forests cannot guarantee.
- To design consistent random forests, many researchers have struggled in this trend. Meinshausen @cite has shown that an algorithm of random forests for quantile regression is consistent; Ishwaran and Kogalur @cite have shown the consistency of their survival forests model; @cite show the consistency of an online version of random forests, while @cite presents a new random regression forests. These consistent models can be applied to either regression, survival or online settings, but not to batch classification settings where all the training data can be used together for learning. In this paper, we propose a novel random forests model based on the cooperative game theory for multi-class classification problems. The consistency of the proposed algorithm is also proved.
- Two more closely related papers to our work are @cite and @cite . @cite proves the consistency of some popular averaging classifiers, including random forests. Specifically, the authors take @cite as a weighted layered nearest neighbor classifier from the perspective of taxonomy proposed by @cite . Unfortunately, this property prevents the consistency of random tree classifiers. To remedy the inconsistency of tree classifiers, the authors suggest the technique introduced in @cite . Moreover, @cite has also proposed a scale-invariant version of random forests with consistency. Recently, @cite presents a new model of random forests, which is similar to the original algorithm of @cite . The main difference between these two models is in how random features are selected. @cite requires a second independent data set to evaluate the importance index of each feature and uses this property to prove the consistency for their algorithm, while the model of @cite doesn't need the second data set. In this paper, we use the Banzhaf power index to evaluate the power of each feature by traversing all possible feature coalitions, but not employing the second data set. The consistency of the proposed algorithm is theoretically guaranteed.
- The task of visual domain adaptation for object classification has been studied in unsupervised and semi-supervised settings. In this section we briefly survey only the domain adaptation techniques that are related to the present work. A recent extended report @cite by Gopalan surveys domain adaptation techniques for visual recognition.
- Different from subspace based techniques, in @cite , it is shown that the image representation learned by convolutional neural networks on Imagenet dataset can be transferred for other tasks with relatively smaller dataset by using fine tuning the deep network. However, these require annotations for the target dataset. In very recent work a more sophisticated technique has been proposed by Zhang @cite where a deep transfer network is learned that learns a shared feature subspace that matches conditional distributions. However, this is not applicable for detecting objects.
- There have been many works on the verification of programs manipulating mutable data structures in general and the use of separation logic, e.g., @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . In the following, we discuss those which are closer to our approach.
- The natural proof approach DRYAD @cite @cite can prove automatically the correctness of programs against the specifications given by separation logic formulas with inductive definitions. Nevertheless, the lemmas are still supposed to be provided by the users in DRYAD, while our approach can generate the lemmas automatically. Moreover, DRYAD does not provide an independent solver to decide the entailment of separation logic formulas, which makes difficult to compare the performance of our tool with that of DRYAD. In addition, the inductive definitions used in our paper enable succinct lemmas, far less complex than those used in DRYAD, which include complex constraints on data variables and the magic wand.
- The method of cyclic proofs introduced by @cite and extended recently in @cite proves the entailment of two SL formulas by using induction on the paths of proof trees. They are not generating the lemma, but the method is able to (soundly) check intricate lemma given by the user, even ones which are out of the scope of our method, e.g., lemmas concerning the predicate @math which is defined by unfolding the list segments from the end, instead of the beginning. The cyclic proofs method can be seen like a dynamic lemma generation using complex reasoning on proof trees, while our method generates lemma statically by simple checks on the inductive definitions. We think that our lemma generator could be used in the cyclic proof method to cut proof trees.
- The tool SLIDE @cite @cite provides decision procedures for fragments of SL based on reductions to the language inclusion problem of tree automata. Their fragments contain no data or size constraints. In addition, the EXPTIME lower bound complexity is an important obstacle for scalability. Our previous work @cite introduces a decision procedure based on reductions to the membership problem of tree automata which however is not capable of dealing with data constraints.
- The tool GRASShopper @cite is based on translations of SL fragments to first-order logic with reachability predicates, and the use of SMT solvers to deal with the latter. The advantage is the integration with other SMT theories to reason about data. However, this approach considers a limited class of inductive definitions (for linked lists and trees) and is incapable of dealing with the size or multiset constraints, thus unable to reason about AVL or red-black trees.
- The truncation point approach @cite provides a method to specify and verify programs based on separation logic with inductive definitions that may specify truncated data structures with multiple holes, but it cannot deal with data constraints. Our approach can also be extended to cover such inductive definitions.
- * -0.1in The JL transforms refer to a class of transforms that obey the JL lemma @cite , which states that any @math points in Euclidean space can be embedded into @math dimensions so that all pairwise Euclidean distances are preserved upto @math . Since the original Johnson-Lindenstrauss result, many transforms have been designed to satisfy the JL lemma, including Gaussian random matrices @cite , sub-Gaussian random matrices @cite , randomized Hadamard transform @cite , sparse JL transforms by random hashing @cite @cite . The analysis presented in this work builds upon the JL lemma and therefore our method can enjoy the computational benefits of sparse JL transforms including less memory and fast computation.
- Until now, very few researchers considered the problem of resolving conflicts in multi-party privacy management for Social Media. @cite proposed a method to define privacy policies collaboratively. In their approach all of the parties involved can define strong and weak privacy preferences. However, this approach does not involve any automated method to solve conflicts, only some suggestions that the users might want to consider when they try to solve the conflicts manually.
- The work described in @cite is based on an incentive mechanism where users are rewarded with a quantity of numeraire each time they share information or acknowledge the presence of other users (called co-owners) who are affected by the same item. When there are conflicts among co-owners' policies, users can spend their numeraire bidding for the policy that is best for them. Then, the use of the Clark Tax mechanism is suggested to obtain the highest bid. As stated in @cite , users may have difficulties to comprehend the mechanism and specify appropriate bid values in auctions. Furthermore, users that earned much numeraire in the past will have more numeraire to spend it at will, potentially leading to unilateral decisions.
- In @cite users must manually define for each item: the privacy settings for the item, their trust to the other users, the sensitivity of the item, and how much privacy risk they would like to take. These parameters are used to calculate what the authors call privacy risk and sharing loss on segments --- they define segments as the set of conflicting target users among a set of negotiating users. Then, based on these measures all of the conflicting target users in each segment are assigned the same action. That is, all of the conflicts that a set of negotiating users have would be solved either by granting or denying access. Clearly, not considering that each individual conflict can have a different solution leads to outcomes that are far from what the users would be willing to accept. Moreover, due to how the privacy risk and sharing loss metrics are defined, solutions are likely to be the actions preferred by the majority of negotiating users, which can be many times far from the actual behaviour of users as shown in Section .
- Finally, the problem of negotiating a solution to multi-party conflicts, has also been recently analysed from a game-theoretic point of view @cite @cite . These proposals provide an elegant analytic framework proposing negotiation protocols to study the problem and the solutions that can be obtained using well-known game-theoretic solution concepts such as the Nash equilibrium. However, as shown in @cite , these proposals may not always work well in practice, as they do not capture the social idiosyncrasies considered by users in the real life when they face multi-party privacy conflicts, and users' behaviour is far from perfectly rational as assumed in these game-theoretic approaches --- e.g., refer to @cite @cite .
- The CKR-relaxation makes use of the observation that can be viewed as a partition problem where the goal is to partition the node set @math into @math parts @math to minimize @math subject to the constraint that for @math , @math . Submodular Multiway Partition ( ) is a generalization from the setting of graphs to arbitrary submodular functions. Here we are given a non-negative submodular function @math over the ground set @math along with terminals @math . The goal is to partition @math into @math to minimize @math subject to the constraint that @math for @math . If @math is symmetric, as in the case of the undirected graph cut function, we obtain the Symmetric Submodular Multiway Partition ( ) problem. These problems were considered by Zhao, Nagamochi and Ibaraki @cite who analyzed greedy-splitting algorithms, and more recently by Chekuri and Ene @cite who used a Lov 'asz-extension based convex relaxation. Interestingly, the convex relaxation when specialized to yields the CKR-relaxation.Chekuri and Ene @cite obtained a @math -approximation for and @math -approximation for . Ene, Vondr 'ak and Wu @cite improved the bound for to @math and also obtained lower bound results in the oracle model.
- There has been a recent push toward obtaining fast, practical algorithms for submodular maximization problems arising in a variety of applied settings. Research in this direction has yielded a variety of techniques for speeding up the continuous greedy algorithm for monotone maximization @cite @cite , as well as new approaches for non-monotone maximization based on insights from both the continuous greedy and double greedy algorithms @cite @cite . Of particular relevance to our results is the case of maximization under a matroid constraint. Here, for monotone functions the fastest current sequential algorithm gives a @math approximation using @math value queries. For non-monotone functions, Buchbinder al @cite give an @math -approximation in time @math , where @math is the time required to compute a perfect matching on bipartite graph with @math vertices per side. They also give a simple, combinatorial @math -approximation in time @math . In comparison, the sequential algorithm we present here is faster by a factor of @math , at the cost of a slightly-weaker @math -approximation.
- There are three types of local feature detection. (1) Classification methods include Support Vector Machine (SVM) classifier @cite @cite based on various image features such as Gabor @cite , SIFT @cite @cite , HOG @cite and multichannel correlation filter responses @cite . (2) Regression-based approaches are also widely used. For instance, Support Vector Regressors (SVRs) are used in @cite with a probabilistic MRF-based shape model and Continuous Conditional Neural Fields (CCNF) are used in @cite . (3) Voting-based approaches are also introduced in recent years, including regression forests based voting methods @cite @cite @cite and exemplar based voting methods @cite @cite .
- * Query processing techniques for in-memory data Research in main memory database management systems commenced more than three decades ago. Early work investigated join algorithms for main memory database systems by @cite and an investigation of in-memory query processing techniques by Lehman and Carey @cite . As the CPU architecture evolved, additional hardware operations were shown to become performance bottlenecks. @cite studied the performance bottlenecks of four commercial database management systems and highlighted the importance of minimizing L2 data cache stalls and L1 instruction cache stalls. @cite proposed to make the join algorithm cache-conscious to improve locality and join performance by adding a partitioning step before the join. @cite proposed radix-based query processing techniques to improve in-memory performance by reducing cache and TLB misses.
- As multi-core CPU architectures became the norm, the database research community explored alternatives to the established query processing paradigm. @cite proposed a pipelined model for query processing where operators map to hardware cores and can be shared among multiple queries. @cite developed a push-based dataflow engine for query processing. @cite investigated techniques to dynamically generate code for query execution. Neumann @cite proposed to further improve performance by compiling queries into native machine code using LLVM. @cite designed a system to process thousands of concurrent queries by sharing computation and intermediate results.
- There is also a lively and ongoing debate on efficient parallel in-memory equi-join techniques. @cite compare sort-merge and radix-hash joins in a multi-core, main-memory environment and claimed that radix-hash join was superior to sort-merge join. @cite break input data into small fragments and assign each to a thread to run entire operator pipelines. By controlling the dispatch of data fragments, query execution can be parallelized dynamically in a NUMA-aware fashion. Polychroniou and Ross @cite proposed a family of main-memory partitioning algorithms that includes hash, radix and range partitioning. @cite studied data shuffling in a NUMA system and improved the performance by employing thread binding, NUMA-aware thread allocation and relaxed global coordination. @cite optimized the hash join for NUMA architectures by optimizing the physical representation of the hash table. @cite introduced concise hash tables which significantly reduce memory consumption while still offering competitive performance.
- Researchers also have proposed cost models for in-memory data processing. @cite proposed a hierarchical model to capture all levels of a cache hierarchy and show that this can accurately capture the total response time of quick sort, hash join and partitioning operations after calibrating for the CPU cost of each algorithm.
- Prior research has extensively studied parallel and distributed query optimization techniques. R* was an early research prototype that optimized queries to minimize data transfer costs in a shared-nothing environment @cite . @cite proposed to generate parallel query plans in two phases: first generate the optimal query plan for a single node, and then to parallelize it for a multi-processor environment. Hong and Stonebraker @cite showed that two phase optimization produces good query plans for shared-memory systems. Researchers have also studied parallel hash join algorithms. DeWitt and Gerber @cite introduced multi-processor versions of popular hash join algorithms for a single join and evaluated their advantages and disadvantages in a multi-processor environment. Recently, @cite propose a novel distributed multi-way join algorithm that has been theoretically proven to have better performance than evaluating the multi-way join as a series of binary joins.
- The performance of different multi-join evaluation strategies has also been extensively studied. Schneider and DeWitt @cite studied multi-way join query processing through hash-based methods in the Gamma parallel database system. They observed that trees with shallow hash-join build phases and a long probe pipeline ( right-deep'' trees) can provide significant performance advantages in large multiprocessor database machines. The key advantages of right-deep query trees are the ample parallelization potential, as each build subtree can be executed concurrently, and that no intermediate results need to be materialized in the long probe pipeline because results are streamed from one operator to the next.
- Right-deep trees introduce the problem of processor allocation to each join sub-tree during parallel evaluation, and @cite use a greedy algorithm to generate query plans for multi-join queries that determines the order of each join, but also the number of parallel joins and the processor assignment to each join. Ioannidis and Kang @cite show that optimization for bushy and left-deep trees is easier than optimizing left-deep trees alone. @cite proposed bushy trees with right-deep pipelines, which they call segmented right-deep trees, for efficient execution of multi-join queries. Query evaluation strategies for shared-nothing main-memory database management systems have been examined in the context of the PRISMA DB system @cite . Wilschut and Apers have identified synchronization costs as the performance bottleneck from parallelism @cite . Recently, @cite have studied how to allocate resources and deploy query plans on multi-cores in the context of their operator-centric SharedDB system.
- Liu and Rundensteiner @cite introduced segmented bushy query trees for the efficient evaluation of multi-join queries. Pipeline delays have also been studied in research by Deshpande and Hellerstein @cite who propose to reduce pipeline delays by plan interleaving between plan fragments. @cite have proposed a technique to optimize bushy join trees for snowstorm queries in the Oracle database system.
- Machine learning techniques have also been explored to predict the response time of a query. IBM's DB2 introduced LEO, a learning optimizer, to adjust the query optimizer's prediction based on the observed query cost @cite @cite . @cite proposed a statistical learning approach, COMET, to adapt to changing workloads for XML operations. @cite used machine learning techniques to predict the performance of concurrent workloads. @cite introduced two learning-based models, a plan-level model and an operator-level model, and contributed a hybrid model to predict query performance. @cite combined regression tree models and scaling functions to estimate the resource consumption of individual operators.
- The problem of surface reconstruction from point cloud has been studied in literature for more than two decades. A comprehensive review of all these works has beyond the scope of our paper. More discussion and comparison on different surface reconstruction methods can be found in @cite . We only give an overview of implicit function based reconstruction with a focus on RBF-based formulations.
- Some prior works (e.g., @cite @cite ) deduced from the statistical-learning perspective avoid generating offset-points in surface reconstruction, where normals were directly used in a variational formulation. Recently, @cite derive an implicit function from the Hermite-Birkhoff interpolation with RBFs. They enhance the flexibility of HRBF reconstruction by ensuring well-posedness of an approach combining regularization. However, given a set with @math points and @math normal vectors, these methods give a @math linear system to be solved, which limits the number of points can be involved in the reconstruction.
- There are schemes for finding a subset of optimal' centers from the point set in order to obtain fast reconstructions. @cite proposed a greedy algorithm that iteratively appends centers which are corresponding to the maximal residual of the current RBF fitting until a desired accuracy is reached. Samozino presented the reconstruction with voronoi centered RBFs @cite . @cite proposed a reconstruction method which combines an adaptive POU approximation with least-squares RBF fitting. Different from @cite , we adopt a quadric error function based on positions and normals of a point and its neighbors instead of local quadratic approximation. Therefore, our center selection step can generate a good spherical cover in a non-iterative way. The selected centers of spheres describe the input shape with a small error.
- Our main motivation to study the chain stems from its connection with the ubiquitous stochastic gradient descent (SGD) algorithm. In general this algorithm takes the form @math where @math is a centered i.i.d. sequence. Standard results in approximation theory, such as @cite , show that if the variance of the noise @math is of smaller order than the step-size @math then the iterates @math converge to the minimum of @math on @math (for a step-size decreasing sufficiently fast as a function of the number of iterations). For the specific noise sequence that we study in , the variance is exactly equal to the step-size, which is why the chain deviates from its standard and well-understood behavior. We also note that other regimes where SGD does not converge to the minimum of @math have been studied in the optimization literature, such as the constant step-size case investigated in @cite @cite .
- The chain is also closely related to a line of works in Bayesian statistics on Langevin Monte Carlo algorithms, starting essentially with @cite . The focus there is on the unconstrained case, that is @math . In this simpler situation, a variant of Theorem was proven in the recent paper @cite . The latter result is the starting point of our work. A straightforward way to extend the analysis of Dalalyan to the constrained case is to run the unconstrained chain with an additional potential that diverges quickly as the distance from @math to @math increases. However it seems much more natural to study directly the chain . Unfortunately the techniques used in @cite cannot deal with the singularities in the diffusion process which are introduced by the projection. As we explain in Section our main contribution is to develop the appropriate machinery to study .
- Connections with ICA were discussed in Sec. and the relationship to InfoMax was discussed in Sec. . The information bottleneck (IB) @cite is another information-theoretic optimization for constructing representations of data that has many mathematical similarities to the objective in Eq. , with the main difference being that IB focuses on supervised learning while ours is an unsupervised approach. Recently, the IB principle was used to investigate the value of depth in the context of supervised learning @cite . The focus here, on the other hand, is to find an information-theoretic principle that justifies and motivates deep representations for unsupervised learning.
- The codebookless model for directly modeling the statistics of local features has been studied in past decades. Rubner al @cite introduced signatures for image representation, and proposed the Earth Mover's Distance for image matching which is robust but has high computational cost. Tuzel al @cite for the first time used covariance matrices for representing regular image regions, and employed Affine-Riemannian metric which suffers from high computational cost @cite . Gaussian model as image descriptor has been used for visual tracking @cite , in which Gaussian models are matched based on the Riemannian metric, involving expensive operations to solve generalized eigenvalue problem. Going beyond Gaussian, Gaussian mixture model (GMM) is more informative and is used in image retrieval @cite . However, GMM suffers from some limitations, such as high computational cost of matching methods and lacking of general criteria for model selection.
- Our work is motivated by @cite @cite and @cite . Carreira al @cite @cite modeled the free-form regions obtained by image segmentation with estimating the second-order moments. By using Log-Euclidean metric @cite , the method in @cite @cite can be combined with a linear classifier, which has shown competing recognition performance on images with less background clutter (e.g., Caltech101 @cite ). Different from @cite @cite , we employ a Gaussian model to represent the whole image. It is well-known that a covariance matrix can be seen as a Gaussian model with fixed mean vector. Compared to @cite @cite , our CLM contains both the first-order (mean) and second-order (covariance) information. Note that the first-order statistics has proven important in image classification @cite @cite . Moreover, the manifold of Gaussian models and that of covariance matrices are quite different, and the embedding method in our CLM makes Gaussian models can be handled flexibly and conveniently.
- Nakayama al @cite also represented an image with a global Gaussian for scene categorization. However, they matched two Gaussian models by using the Kullback-Leibler (KL) divergence, and hence kernel-based classifiers have to be used. This method is not scalable and has high computational cost. In contrast to @cite , our metric is decoupled which allows a linear classifier to be combined, which makes our method more efficient and scalable than the KL kernel based one in @cite . Moreover, compared with the ad-hoc linear kernel (Euclidean baseline) in @cite , our method takes advantage of the geometry structure of Gaussian models and brings large performance improvement.
- There is another line of research on codebookless model methods. Grauman al @cite proposed a pyramid match kernel to map feature sets to multi-resolution histograms, and employed histogram intersection kernel for classification. Bo al @cite presented efficient match kernels to map local features into a low dimensional space, and adopted a linear classifier. Boiman al @cite developed an image-to-class distance between the sets of local features, and employed a nearest neighbor classifier. Yao al @cite proposed a codebook-free approach by using a large number of randomly generated image templates for image representation, and developed a bagging-based classifier.
- Previous research has focused on finding infinite-horizon stationary policies for constrained MDPs. Due to the constraints, the optimal policies might no longer be deterministic and stationary @cite . @cite gives an example of a transient multi-chain MDP with state constraints and shows that the Bellman principle fails to hold and the optimal policy is not necessarily stationary. In the presence of constraints, randomization in the actions can then be necessary for obtaining optimal policies @cite @cite . For stationary policies to be optimal, specific assumptions on the underlying Markov chain are often introduced @cite . Stationary policies for these specific models can be found by using algorithms based on Linear Programming (LP) @cite or Lagrange multipliers ( @cite @cite ). However, finding optimal policies in the broader class of randomized policies for CMDPs can be very expensive computationally and there is no previously known algorithm for the general case See @cite for very specific examples where solutions can be obtained. @cite @cite .
- Our work is closer in spirit to @cite which studies how humans navigate through Wikipedia in search of information. They proposed an algorithm to predict the user's intended target page, given the click log. In contrast, we study users' edit patterns and differentiate between users based on the pages he she has edited. Other studies look at users' web navigation and surfing behavior @cite @cite and why users re-visit certain pages @cite . By using patterns in edit histories and egocentric network properties, @cite proposes a method to identify the social roles played by Wikipedia users (substantive experts, technical editors, vandal fighters, and social networkers), but don't identify vandals.
- New standardization efforts such as OMG Application Programming Interfaces for Knowledge Bases (OMG API4KB) www.omgwiki.org API4KB aim at the accessability and interoperability of heterogenous ontologies via standardized programming interfaces. OntoMaven can act as one possible implementation of the development support functionalities of OMG's API4KB. With its Maven-based approach for structuring the development phases into different goals providing different functionalities during the software development project's life cycle, OntoMaven supports in particular agile ontology development methods, such as COLM @cite , as well as development methods which are inherently based on modularization such as aspect-oriented ontology development @cite .
- Since the inception of DDoS flooding attacks, several defense mechanisms have been proposed to date in the literature @cite . This section highlights the defense mechanisms against two main DDoS flooding attacks, followed by a discussion on the application of Mapreduce Hadoop to combat network anomalies, Botnet and DDoS related attacks.
- Analysis of logs and network flows for anomaly detection has been a problem in the information security for decades. New big data technologies, such as Hadoop, has attracted the interest of the security community for its promised ability to analyze and correlate security-related heterogeneous data efficiently and at unprecedented scale and speeds @cite . In the rest of the section, we review some recent techniques (other than @cite , discussed in ) where Hadoop based frameworks are used to build affordable infrastructures for security applications.
- Temporal and spatial traffic structures are essential for anomaly detectors to accurately drive the statistics from network traffic. Hadoop divides the data into multiple same size blocks, and distributes them in a cluster of data nodes to be processed independently. This could introduce a difficulty in analysis of network traffic where related packets may be spread across different block, thus dislocating traffic structures. Hashdoop @cite resolve this potential weakness by using hash function to divide traffic into blocks that preserve the spatial and temporal traffic structures. In this way, Hashdoop conserves all the advantages of the MapReduce model for accurate and efficient anomaly detection of network traffic.
- RNN regularization has recently been shown to be achievable using Dropout @cite by regularizing a subset of the recurrent connections in deep RNNs @cite @cite . Previously, it was shown that weight decay regularization only provided small improvement @cite and dropout noise was detrimental when applied to all connections due to the compounding of errors over time @cite . In this work, we show that this problem can also be solved using a deterministic approach by penalizing gate activations from deep RNNs. As a result, RNNs can now benefit from multiple regularization techniques in varying architectures.
- In recent years, there has been a wealth of evidence that attention-based techniques can improve the performance of machine learning models. Examples of this include work on capturing visual structure through a sequence of glimpses through images @cite @cite @cite @cite @cite @cite , and networks that learn how to attend to and control a separate memory @cite @cite @cite . In certain cases the models are trained with supervision on the gates @cite @cite , however in many cases there is no supervised data for the attentional component. Several surrogate objectives have been suggested for learning where to focus, including setting a prior on observation spacing that makes a tradeoff between exploration and exploitation @cite , using reinforcement learning @cite to optimize a visual tracking strategy @cite , or leaving this part semi-supervised through the primary objective. Our work resembles the observation prior of @cite , where we favor input gates being closed and penalize deviation with a penalty of our choosing. Similarly to the annealed Dropout from @cite , we also consider a gradual increase in the sparsity penalty during training to encourage early exploration.
- Object detection has been actively studied for the last few decades. One of the most popular approaches is part-based models due to their strength in handling pose variations and occlusions. Deformable Part Model (DPM), proposed by Felzenszwalb al @cite , is a flexible model composed of object parts combined with deformation cost to manage severe deformations. Poselets, proposed by Bourdev al @cite , is another part-based model demonstrating competitive performance. Poselets has numerous part-wise HOG detectors covering wide pose variations. Activated parts vote the location of objects.
- The state-of-the-art method in object detection is the Region-CNN (R-CNN), which represents a local region by CNN activations @cite . Specifically, R-CNN framework proceeds as follows. First it extracts local regions which probably contain an object by using an object proposal methods @cite . The local regions, called object proposals, are warped into a fixed size and fed to a pre-trained CNN. Then each proposal is represented by mid-level CNN activations (e.g. 7th convolutional layer) and evaluated by separated classifiers (e.g. SVMs). The object proposals are then merged and fed to a bounding box regressor @cite to correct miss-localizations. Despite its efficiency and successful performance, it has a limitation that proposal quality highly affects detection performance. If the proposal model fails to propose a suitable candidate, the rest procedures will not have the opportunity to detect it. For this reason, @cite @cite proposed a new class-agnostic proposal method with a CNN regression model to improve the proposal quality while reducing the number of proposals. Also, R-CNN is a cascaded model composed of individual components as object proposal, feature extraction, classification, and bounding box regression, therefore these should be individually engineered for the best performance.
- Apart from R-CNN framework, there is another CNN-based approach, which considers object detection as a regression problem. Szegedy al @cite trains a CNN which maps an image to a rectangular mask of an object. Sermanet al @cite also employ a similar approach but their CNN directly estimates bounding box coordinates. These methods are free from object proposals, but it is a still debatable to leave all to a CNN trained with a mean-square cost to produce an exact bounding box.
- The second line of work that is very relevant to DMNs is that of attention and memory in deep learning. Attention mechanisms are generally useful and can improve image classification @cite , automatic image captioning @cite and machine translation @cite @cite . Neural Turing machines use memory to solve algorithmic problems such as list sorting @cite . The work of recent months by on memory networks @cite focuses on adding a memory component for natural language question answering. They have an input (I) and response (R) component and their generalization (G) and output feature map (O) components have some functional overlap with our episodic memory. However, the Memory Network cannot be applied to the same variety of NLP tasks since it processes sentences independently and not via a sequence model. It requires bag of @math -gram vector features as well as a separate feature that captures whether a sentence came before another one.
- Various other neural memory or attention architectures have recently been proposed for algorithmic problems @cite @cite , caption generation for images @cite @cite , visual question answering @cite or other NLP problems and datasets @cite .
- In contrast, the DMN employs neural sequence models for input representation, attention, and response mechanisms, thereby naturally capturing position and temporality. As a result, the DMN is directly applicable to a broader range of applications without feature engineering. We compare directly to Memory Networks on the bAbI dataset @cite .
- There are many different approaches to : some build large knowledge bases (KBs) with open information extraction systems @cite , some use neural networks, dependency trees and KBs @cite , others only sentences @cite . A lot of other approaches exist. When QA systems do not produce the right answer, it is often unclear if it is because they do not have access to the facts, cannot reason over them or have never seen this type of question or phenomenon. Most QA dataset only have a few hundred questions and answers but require complex reasoning. They can hence not be solved by models that have to learn purely from examples. While synthetic datasets @cite have problems and can often be solved easily with manual feature engineering, they let us disentangle failure modes of models and understand necessary QA capabilities. They are useful for analyzing models that attempt to learn everything and do not rely on external features like coreference, POS, parsing, logical rules, etc. The DMN is such a model. Another related model by combines neural and logical reasoning for question answering over knowledge bases and visual question answering.
- is a very useful classification task and recently the Stanford Sentiment Treebank @cite has become a standard benchmark dataset. Kim @cite reports the previous state-of-the-art result based on a convolutional neural network that uses multiple word vector representations. The previous best model for on the Wall Street Journal section of the Penn Tree Bank @cite was Sogaard @cite who used a semisupervised nearest neighbor approach. We also directly compare to paragraph vectors by @cite .
- The episodic memory in humans stores specific experiences in their spatial and temporal context. For instance, it might contain the first memory somebody has of flying a hang glider. Eichenbaum and Cohen have argued that episodic memories represent a form of relationship (i.e., relations between spatial, sensory and temporal information) and that the hippocampus is responsible for general relational learning @cite . Interestingly, it also appears that the hippocampus is active during transitive inference @cite , and disruption of the hippocampus impairs this ability @cite .
- The episodic memory module in the DMN is inspired by these findings. It retrieves specific temporal states that are related to or triggered by a question. Furthermore, we found that the GRU in this module was able to do some transitive inference over the simple facts in the bAbI dataset. This module also has similarities to the @cite and its Bayesian extensions @cite which were developed to analyze human behavior in word recall experiments.
- There is significant prior work on cognitive radio and compressed sensing (CS); we have focused on the literature that is most relevant to our current problem framework. Centralized schemes for the tracking of sparse time-varying processes have been examined in @cite @cite @cite and distributed CS has been studied in @cite @cite for signals. In contrast to these two veins, we study distributed CS for time-varying signals. Performance guarantees for recursive reconstruction of sparse signals under the assumption of slow support changes is studied in @cite ; however, joint sensing and control is not examined. Recovery of static binary sparse signals via CS has been investigated in @cite @cite . Compressive spectrum sensing has been studied in @cite , for a static setting, and in @cite , for a dynamic setting with noiseless measurements, but without scheduling. We do not focus on recovery guarantees herein, but rather embed CS into a control framework wherein the number of measurements is adapted based on prior information in order to drive traffic scheduling decisions.
- Active sensor scheduling and adaptation @cite encompass applications such as target tracking @cite @cite , physical activity detection @cite , and sequential hypothesis testing @cite . All these prior works including ours @cite @cite @cite assume that the underlying state is given by nature and is not controlled. In contrast, in this work, states are affected by scheduling decisions, via interference and collisions generated by the SUs to the PUs, and we design joint sensing, estimation and scheduling schemes in wireless networks, which account for the cost of acquisition of state information and its impact on the overall network performance. Complexity reduction of POMDPs via exponential family principal components analysis enables planning on a small dimensional manifold in @cite . Model reduction of complex Markov chain models using the KLD as a metric is investigated in @cite . In contrast, we develop a belief compression method based on Neyman-Pearson formulation of the compressive spectrum sensing problem. Our scheme captures relevant features of the dynamic spectrum access problem, without having to learn key statistics. As in @cite , the KLD measure is also used to project the true belief onto the low-dimensional manifold.
- In this work, we assume that the PUs employ a retransmission process, which induces structure in the PU signal. This structure has been exploited in @cite to design adaptive SU access techniques, and in @cite to design smart interference cancellation techniques that exploit redundancy introduced by retransmission. In this work, instead, we exploit the structure in the signal as a result of sparse network dynamics, to design compressive spectrum sensing techniques and sparse recovery schemes. We extend the model studied in @cite to include a more general traffic model for the SU network, and propose a low-complexity solution based on the aforementioned partially myopic scheduling scheme.
- One of the most well known image guided upsampling methods is based on the Markov Random Fields (MRF) @cite . It is further extended in @cite which we denote as NLM-MRF. Both methods along with other similar methods such as @cite @cite @cite @cite @cite @cite @cite which adopt the certain cues on the corresponding available color image as the reference to upsample the low resolution depth map. They all assume that the color edge co-occurs with the depth discontinuity. To clearly demonstrate the essential constrains of the existing methods, we brief the existing modeling as follows.
- Past work on user geolocation falls broadly into two categories: text-based and network-based methods. Common to both methods is the manner of framing the geolocation prediction problem. Geographic coordinates are real-valued, and accordingly this is most naturally modelled as (multiple) regression. However for modelling convenience, the problem is typically simplified to classification by first pre-partitioning the regions into discrete sub-regions using either known city locations @cite @cite or a tree partitioning @cite @cite . In the tree methods, the resulting discrete regions are treated either as a flat list (as we do here) or a nested hierarchy.
- Text-based approaches assume that language in social media is geographically biased, which is clearly evident for regions speaking different languages @cite , but is also reflected in regional dialects and the use of region specific terminology. Text based models have predominantly used bag of words features to learn per-region classifiers @cite @cite , including feature selection for location-indicative terms @cite .
- Topic models have also been applied to model geospatial text usage @cite @cite , by associating latent topics with locations. This has a benefit of allowing for prediction over continuous space, i.e., without the need to render the problem as classification. On the other hand, these methods have high algorithmic complexity and their generative formulation is unlikely to rival the performance of discriminative methods on large datasets.
- Although online social networking sites allow for global interaction, users tend to befriend and interact with many of the same people online as they do off-line @cite . Network-based methods exploit this property to infer the location of users from the locations of their friends @cite @cite . This relies on some form of friendship graph, through which location information can be propagated, e.g., using label propagation @cite @cite . A significant caveat regarding the generality of these techniques is that friendship graphs are often not accessible, e.g., secured from the public (Facebook) or hidden behind a heavily rate-limited API (Twitter).
- Next, we would like to turn our decision method into a verified proof procedure within a proof assistant. The deepest result needed is the Prime Number Theorem (PNT). As have formalised a proof of PNT within Isabelle HOL @cite , we are hopeful that a verified version of our procedure can be built in Isabelle HOL @cite in the near future. To this end, it is useful to observe that PNT is not needed by the restriction of our method to deciding the rationality of real algebraic numbers like @math and @math . Thus, a simpler tactic could be constructed for this fragment.
- Other related problems include the enumeration of graphs with a given degree sequence ( @cite ), the enumeration of symmetric matrices with nonnegative coefficients and constant row sum ( @cite ), and the enumeration of graphs with degree parities, investigated by @cite .
- Also close to our work, @cite and @cite consider the machine learning problem associated to Neyman-Pearson hypothesis testing. In a similar setup, they consider the situation where one does not have access to the underlying distributions, but only has i.i.d. samples from each hypothesis. This work generalizes that goal from the Neyman-Pearson setting to generalized likelihood ratio tests and emphasizes the connection with classification. @cite take on a different problem (tests of statistical independence) by using machine learning algorithms to find scalar maps from the high-dimensional feature space that achieve the desired statistical goal when the fundamental high-dimensional test is intractable.
- In terms of joint loss minimization and semi-supervised learning, our work can be linked to @cite and @cite , with the main advantage being the easiness to extend a Convnet with a Deconvnet and thereby enabling the utilization of unlabeled data. @cite has analyzed the regularization effect with similar architectures in a layer-wise fashion.
- One recent work ( @cite , @cite ) has been proposed to adopt deep auto-encoders to support supervised learning in which completely different strategy is employed to harness the lateral connection between same stage encoder-decoder pairs, however. In that work, decoders receive the entire pre-pooled activation state from the encoder, whereas decoders from SWWAE only receive the where'' state from the corresponding encoder stages. Further, due to a lack of unpooling mechanism incorporated in the Ladder networks, it is restricted to only reconstruct the top layer within generative pathway ( @math model), which looses the "ladder" structure. By contrast, SWWAE doesn't suffer from such necessity.
- . Recurrent Neural Networks (RNNs) have a long history of applications in various sequence learning tasks @cite @cite @cite . Despite their early successes, the difficulty of training simple recurrent networks @cite @cite has encouraged various proposals for improvements to their basic architecture. Among the most successful variants are the Long Short Term Memory networks @cite , which can in principle store and retrieve information over long time periods with explicit gating mechanisms and a built-in constant error carousel. In the recent years there has been a renewed interest in further improving on the basic architecture by modifying the functional form as seen with Gated Recurrent Units @cite , incorporating content-based soft attention mechanisms @cite @cite , push-pop stacks @cite , or more generally external memory arrays with both content-based and relative addressing mechanisms @cite . In this work we focus the majority of our analysis on the LSTM due to its widespread popularity and a proven track record.
- . While there is an abundance of work that modifies or extends the basic LSTM architecture, relatively little attention has been paid to understanding the properties of its representations and predictions. @cite recently conducted a comprehensive study of LSTM components. evaluated GRU compared to LSTMs @cite . @cite conduct an automated architecture search of thousands of RNN architectures. @cite examined the effects of depth . These approaches study recurrent network based only on the variations in the final test set cross entropy, while we break down the performance into interpretable categories and study individual error types. Most related to our work is @cite , who also study the long-term interactions learned by recurrent networks in the context of character-level language models, specifically in the context of parenthesis closing and time-scales analysis. Our work complements their results and provides additional types of analysis. Lastly, we are heavily influenced by work on in-depth analysis of errors in object detection @cite , where the final mean average precision is similarly broken down and studied in detail.
- The Binary-coded decimal encoding (BCD) encodes each digit in a group of 4 or 8 digits, so-called tetrades. Improvements include the Chen-Ho @cite encoding, which manages to encode 3 digits on 10 bits (declets). It has the nice particularity of being extremely efficient to process (no multiplications, no divisions), and of being friendly to decimal computations. However, it does not preserve order. The Densely packed decimal (DPD) encoding @cite is an improvement upon the Chen-Ho encoding.
- The Rishe Encoding @cite is the closest match to decimalInfinite we found in literature in terms of problem solving. It supports arbitrarily large, small and precise decimals, is compact, and is also compatible with a bitwise lexicographic comparison. However, it relies on an arbitrary choice of intervals (128) based on the use case. decimalInfinite does not rely on such a choice and scales up continuously, regardless of how large, small or precise decimals are.
- The Rishe encoding was itself proposed as an improvement upon the Matula-Kornerup encoding @cite , which relies on a representation of the decimal as a continuous fraction. The latter did not scale up with exponents. The exponent part of Rishe scales up logarithmically in the decimal, while that of decimalInfinite scales up double-logarithmically (see Section ).
- Some of the methods on cold-start problem focus on the improvement of prediction accuracy, due to the lack of enough rating data of new users. Theses methods gain better prediction performance by applying different strategy. For example, @cite presented a new similarity measure with optimization based on neural learning, which shows the much better results than current metrics, such as cosine similarity measure. @cite showed an interesting phenomenon that to link a cold-start item to inactive users will give this new item more chance to appear in other users recommendation lists. @cite applied probabilistic method to select the @math neighbours from the entire candidate list, rather the @math nearest candidate, to avoid the low prediction accuracy due to the lack of rates data. @cite proposed an approach which incorporates classification methods in a pure CF system while the use of demographic data help for the identification of other users with similar behaviour.
- Some of the solutions to scalability problem proposed the methods based on incremental updates of user-to-user and item-to-item similarity. These methods achieve faster and high-quality recommendation than the traditional CF. @cite proposed an incremental method which quickly updates user's similarity list when the user adds rates new items in the recommender systems. @cite presented the temporal relevance measure for ratings at different time steps and developed online evolutionary collaborative filtering algorithms by introducing this measure into @math NN algorithms and incrementally computing neighbourhood similarities, which achieve both better time and space complexity. Inspired by @cite , @cite developed the user-based incremental similarity update method to an corresponding item-based method. @cite proposed a practical item-based CF algorithm on big data environment, with the super characteristics such as robust to the implicit feedback problem, scalable incremental update and real-time pruning.
- Unfortunately, the current methods on cold-start problem and scalability problem do not work well on a special case: the new users, with enough recommendation data, have the same rating list ( @math Nearest Neighbour ( @math NN) attack @cite can be taken as an example of our special case, which creates @math same fake users with at least 8 rated items into the recommender system). The reasons are the solutions to cold-start problem only work on the new users which have not been gathered sufficient information, and the methods concentrating on scalability problem only work on the old users who have already have a similarity list. Naturally, when facing the special case, the above methods have to apply the traditional similarity computation method which yields in @math time complexity. Considering the number of users in a recommender system, @math , is usually very large, the computational cost of the above method will be very large. Therefore, it is necessary to gain a faster algorithm to build the new users similarity list in our special case.
- Edge detection is a well studied problem with a rich history. Traditional methods considered step edges and relied on local gradients to detect them @cite @cite @cite . In contrast, recent methods addressed the problem of boundary detection in natural images @cite @cite @cite @cite @cite @cite . These methods rely on supervised learning of complex boundary features that account for intensity, color and texture. Despite the high accuracy achieved by these methods on natural images, as shown experimentally in Sec. , they exhibit poor performance in noisy images with faint edges.
- In this paper we focus on the problem of detecting step edges at high levels of noise. As mentioned in the introduction, this is an important task in a variety of imaging domains. One of the first proposed methods for step edge detection was the Sobel operator @cite , which does so by thresholding the gradient magnitude at each pixel separately. Marr & Hildreth @cite proposed to detect edges by identifying the zero crossings of a 2D Laplacian of a Gaussian filter applied to the image. These local approaches for edge detection are very efficient, essentially with linear-time complexity in the total number of pixels. However, due to their local nature, they are sensitive to noise and exhibit poor performance at low SNR. One exception is the algorithm suggested by Canny @cite , which extends Sobel by hysteresis thresholding of the local gradient magnitudes. This post-processing operation significantly improves its robustness to noise. We thus choose the Canny algorithm as a baseline for comparison to our work.
- Another set of edge detection algorithms utilize a Wavelet-based bank of filters that vary in length and width, see a review in @cite . Given the availability of fast-wavelet transforms, these methods are also quite fast. However, a key difference from our approach is that these wavelet methods do not adapt to the shape of actual curved edges. Hence, their performance at very low SNRs is sub-optimal.
- Several recent studies proposed to use matched filters to improve the detection of faint edges. @cite proposed an efficient algorithm for detecting straight edges, requiring @math operations for an input image with @math pixels. An even faster version for detecting long straight edges was proposed in @cite , with sub-linear run-time in the image size. An algorithm for detecting faint edges was proposed by @cite . Relying on the Beamlet data structure @cite , their method applied dynamic programming to a hierarchical, quad-tree partition of the image. The time complexity of their algorithm is @math , which translates into a non-practical runtime of several minutes on typical images.
- In our work, we detect curved edges with a significantly reduced complexity. The two key ideas are to instead construct a tree partition, and perform a sophisticated processing on it. We present two variants of our algorithm, which both require a memory of @math . The first, more stringent variant has time complexity @math , whereas the second faster one incurs a slight loss of detection accuracy, but has even faster runtime at @math operations. For illustrative purposes, the run time of our latter algorithm, implemented in C++, is roughly 2 seconds on a small @math image and 10 seconds on a @math image. In contrast, the runtime of @cite , as reported in their paper is several minutes per image.
- Very recently, @cite considered statistical aspects of leverage-based sampling algorithms (called in @cite ). Assuming a standard linear model on @math of the form of Eqn. ), the authors developed first-order Taylor approximations to the statistical RE of different estimators computed with leverage-based sampling algorithms, and they verified the quality of those approximations with computations on real and synthetic data. Taken as a whole, their results suggest that, if one is interested in the statistical performance of these randomized sketching algorithms, then there are nontrivial trade-offs that are not taken into account by standard WC analysis. Their approach, however, does not immediately apply to random projections or other more general sketching matrices. Further, the realm of applicability of the first-order Taylor approximation was not precisely quantified, and they left open the question of structural characterizations of random sketching matrices that were sufficient to ensure good statistical properties on the sketched data. We address these issues in this paper.
- Subsequent work by Pilanci and Wainwright @cite also considers a statistical perspective of sketching. Amongst other results, they develop a lower bound which confirms that using a single randomized sketching matrix @math can not achieve a better PE than @math . This lower bound complements the upper bounds developed in this paper. Their main focus is to use this insight to develop an iterative sketching scheme which yields bounds on the SPE when an @math sketch is applied repeatedly.
- Substantial progress have been done in the last decade in mean-field games in the non-cooperative setup. However, very little is known about cooperative mean-field games. In @cite we have introduced cooperative mean-field type games in which the state dynamics and the payoffs depend not only on the state and actions but also on their probability measure. We establish a time-dependent payoff allocation procedure for coalitions of mean-field type. The allocated payoff considers not only fairness property but also the cost of making the coalition. Both time consistency and subgame perfectness solution concept equations are established.
- Methods for detecting events in Twitter rely on a rich body of work dealing with event, topic and burst detection from textual streams. In a seminal work, @cite studies time gaps between messages in order to detect bursts of email messages. Assuming that all messages are about the same topic, he proposes to model bursts with hidden Markov chains. @cite propose (On-line Latent Dirichlet Allocation), a dynamic topic model based on . It builds evolutionary matrices translating the evolution of topics detected in a textual stream through time, from which events can be identified. @cite propose to detect and then cluster bursty words by looking at where the frequency of each word in a given time window is positioned in the overall distribution of the number of documents containing that word.
- The is a normalized word frequency metric, similar to the @math metric, for identifying words that are particular to a fixed length time window and not salient in others. However, individual words may not always be sufficient to describe complex events because of the possible ambiguity and the lack of context. To cope with this, @cite propose a different normalized frequency metric, , for identifying event-related n-grams. For a given n-gram and time window, it consists in computing the normalized frequency, @math , of that n-gram with regard to the frequency of the other n-grams in this window. The of a n-gram in a particular time window is then obtained by normalizing the value of @math in this time window with regard to the values calculated in the others.
- @cite propose an online variation of . The idea is to incrementally update the topic model in each time window using the previously generated model to guide the learning of the new model. At every model update, the word distribution in topics evolves. Assuming that an event causes a sudden change in the word distribution of a topic, authors propose to detect events by monitoring the degree of evolution of topics using the Jensen-Shannon divergence measure. @cite note that topic modeling methods behave badly when applied to short documents such as tweets. To remedy this, they propose (joint Event and Tweets ). It expands tweets with the help of a search engine and then aligns them with re transcriptions of events provided by traditional media, which heavily influences the results. Globally, topic modeling based methods suffer from a lack of scalability, which renders their application to tweet streams difficult. However, works by @cite reveal that dynamic topic models don't effectively handle social streams in which many events are reported in parallel.
- is among the first tools developed for visualizing events from tweets. It displays a single word cloud that describes all the detected events, as well as a single stacked area chart that plots the evolution of the relative volume of tweets for each event. @cite propose , a system that allows for a finer understanding of the detected events in comparison with . It displays a list of events, each event being described by a set of words and a chart that plots the evolution of the volume of related tweets. is a tool that offers similar functionalities with more sophisticated visualizations, such as word clouds to describe events instead of sets of words. @cite describe , a tool whose interface revolves around a timeline of events. The user can click an event to see the tweets published during the related time interval, or to see the most cited URLs in these tweets. Let us also mention work by @cite , in which a heatmap describes the distribution of events across time.
- The advent of massive online computer science courses has made the problem of automated reasoning with large code collections an important problem. There have been a number of recent papers @cite @cite @cite @cite @cite @cite on using large homework submission datasets to improve student feedback. The volume of work speaks to the importance of this problem. Despite the research efforts, however, providing quality feedback at scale remains an open problem.
- To incorporate functionality, @cite proposed a method that discovers program modifications that do not appear to change the semantic meaning of code. The embedded representations of programs used in this paper also capture semantic similarities and are more amenable to prediction tasks such as propagating feedback. We ran feedback propagation on student data using methods from and observe that embeddings enabled notable improvement (see section 6.3).
- Embedding programs has many crossovers with embedding natural language artifacts, given the similarity between the AST representation and parse trees. Our models are related to recent work from the NLP and deep learning communities on recursive neural networks, particularly for modeling semantics in sentences or symbolic expressions @cite @cite @cite @cite .
- Several hardness assumptions imply that it is hard to agnostically learn halfspaces. Feldman, Gopalan, Khot and Ponnuswami @cite have shown that based on the security of the Ajtai-Dwork cryptosystem. Kalai, Klivans, Mansour and Servedio showed the same conclusion @cite based on the hardness of learning parity with noise. Daniely and Shalev-Shwartz @cite derived the same conclusion based on the hardness of refuting random @math -SAT formulas. Klivans and Kothari @cite showed that assuming that learning sparse parity is hard, it is hard to learn halfspaces even when the marginal distribution is Gaussian. We note that all these results only rule out exact algorithms, but say nothing about approximation algorithms. We note however that by @cite @cite , an algorithm with non trivial performance of @math -realizable distributions will result with quasi-polynomial time algorithm for learning constant depth circuits.
- When we restrict the algorithms to return a halfspace classifier, the problem of learning halfspaces is essentially equivalent @cite to the computational problem of minimizing disagreements . In this problem we are given a sample @math , and the goal is to find a vector @math that minimizes the fraction of pairs with @math . The optimal fraction is called the error of the sample. As a standard" and basic computational problem, much is known about it. The problem have been shown @math -hard already in Karp's famous paper @cite . Soon after the discovery of the PCP theorem, Arora, Babai, Stern and Sweedyk @cite have shown that assuming that no quasi-polynomial time algorithm can solve @math -hard problems, there is no efficient algorithm with an approximation ratio of @math . Later on, the corresponding maximization problem was considered by several authors @cite @cite @cite @cite @cite , culminating with Feldman, Gopalan, Khot and Ponnuswami @cite who showed that assuming that no quasi-polynomial time algorithm can solve @math hard problems, no efficient algorithm can distinguish samples with error @math from samples with error @math .
- Statistical queries (SQ) algorithms @cite is a class of learning algorithms whose interaction with @math is done only via statistical queries . Concretely, an SQ-algorithm can specify any function @math and an error parameter @math , and receive a number @math satisfying @math . In this model, an algorithm is efficient if it makes polynomially many queries with error parameters satisfying @math (besides that, the algorithm is not restricted). While this class is strictly smaller than the class of all algorithms, most known algorithms admit an SQ version. In addition, as opposed to general algorithms, unconditional lower bounds are known for SQ algorithms @cite for several learning problems. In particular, it is known that it is hard to agnostically learn halfspaces using SQ-algorithm (e.g. @cite ). We note that as with previously known lower bounds for general algorithms, these results only rule out exact algorithms.
- In a recent paper @cite (see also @cite ) we, together with Linial and Shalev-Shwartz, developed a new framework to prove hardness of learning based on hardness on average of CSP problems. Yet, in @cite we were not able to use our technique to establish hardness results that are based on a natural assumption on a well studied problem. Rather, we made a rather speculative hardness assumption, that is concerned with general @math problems, most of which were never studied explicitly. We recognized it as the main weakness of our approach, and therefore the main direction for further research. About a year after, Allen, O'Donnell and Witmer @cite refuted the assumption of @cite . On the other hand we @cite were able to overcome the use of our speculative assumption, and prove hardness of learning of DNF formulas (and other problems) based on a natural assumption on the complexity of refuting random @math -SAT instances, in the spirit of Feige's assumption @cite . The current paper continues this line of work.
- One approach for solving the optimization problem defined by @math involves using a linear program (LP) relaxation. The optimization problem can be posed using a LP with a large number of constraints and relaxed to obtain a tractable LP over the @cite . Several message passing methods have been motivated in terms of this LP @cite . There are also recent methods which use message passing in the inner loop of an algorithm that converges to the optimal solution of the local polytope LP relaxation @cite @cite . In we characterize the fixed point of @math using a different LP.
- Removing camera shake blur is one of the most challenging problems in image processing. Although in the last decade several image restoration algorithms have emerged giving outstanding performance, their success is still very dependent on the scene. Most image deblurring algorithms cast the problem as a deconvolution with either a known (non-blind) or an unknown blurring kernel (blind). See e.g., the review by Kundur and Hatzinakos @cite , where a discussion of the most classical methods is presented.
- Several attempt to first estimate the degradation operator and then applying a non-blind deconvolution algorithm. For instance, @cite accelerates the kernel estimation step by using fast image filters for explicitly detecting and restoring strong edges in the latent sharp image. Since the blurring kernel has typically a very small support, the kernel estimation problem is better conditioned than estimating the kernel and the sharp image together. @cite @cite , the authors concluded that it is better to first solve a maximum a posteriori estimation of the kernel than of the latent image and the kernel simultaneously. However, even in non-blind deblurring, i.e., when the blurring kernels are known, the problem is generally ill-posed, because the blur introduces zeros in the frequency domain. Thereby avoiding explicit inversion, as here proposed, becomes critical.
- Two or more input images can improve the estimation of both the underlying image and the blurring kernels. Rav-Acha and Peleg @cite claimed that if the direction of the blurs are different. @cite the authors proposed to capture two photographs: one having a short exposure time, noisy but sharp, and one with a long exposure, blurred but with low noise. The two acquisitions are complementary, and the sharp one is used to estimate the motion kernel of the blurred one.
- Close to our work are papers on multi-image blind deconvolution @cite @cite @cite @cite @cite . @cite the authors showed that given multiple observations, the sparsity of the image under a tight frame is a good measurement of the clearness of the recovered image. Having multiple input images improves the accuracy of identifying the motion blur kernels, reducing the illposedness of the problem. Most of these multi-image algorithms introduce cross-blur penalty functions between image pairs. However this has the problem of growing combinatorially with the number of images in the burst. This idea is extended in @cite using a Bayesian framework for coupling all the unknown blurring kernels and the latent image in a unique prior. Although this prior has numerous good mathematical properties, its optimization is very slow. The algorithm produces very good results but it may take several minutes or even hours for a typical burst of 8-10 images of several megapixels. The very recent work by Park and Levoy @cite relies on an attached gyroscope, now present in many phones and tablets, to align all the input images and to get an estimation of the blurring kernels. Then, a multi-image non-blind deconvolution algorithm is applied.
- A popular technique in astronomical photography, known as or , is to take a series of thousands of short-exposure images and then select and fuse only the sharper ones @cite . Fried @cite showed that the probability of getting a sharp lucky short-exposure image through turbulence follows a negative exponential. Thus, when the captured series or video is sufficiently long, there will exist such a frame with high probability.
- Classical selection techniques are based on the brightness of the brightest speckle @cite . The number of selected frames is chosen to optimize the tradeoff between sharpness and signal-to-noise ratio required in the application.
- Others propose to measure the local sharpness from the norm of the gradient or the image Laplacian @cite @cite @cite @cite . Joshi and Cohen @cite engineered a weighting scheme to balance noise reduction and sharpness preservation. The sharpness is measured through the intensity of the image Laplacian. They also proposed a local selectivity weight to reflect the fact that more averaging should be done in smooth regions. Haro and colleagues @cite explored similar ideas to fusion different acquisitions of painting images. The weights for combining the input images rely on a local sharpness measure based on the energy of the image gradient. The main disadvantage of these approaches is that they only rely on sharpness measures (which by the way is not necessarily trivial to estimate) and do not profit the fact that camera shake blur can be in different directions in different frames.
- @cite introduced a selection scheme for astronomic images, based on the relative strength of signal for each spatial frequency in the Fourier domain. From a series of realistic image simulations, the authors showed that this procedure produces images of higher resolution and better signal to noise ratio than traditional lucky image fusion schemes. This procedure makes a much more efficient use of the information contained in each frame.
- @cite were one of the first to suggest a category-independent method to propose putative objects. Their method involved sampling regions from multiple segmentations of the image. More recently, @cite and @cite propose using bottom-up object proposals as a first step in recognition. Expanding on the multiple segmentations idea, Selective search @cite uses regions from hierarchical segmentations in multiple color spaces as object proposals. CPMC @cite uses multiple graph-cut based segmentations with multiple foreground seeds and multiple foreground biases to propose objects. GOP @cite replaces graph cuts with a much faster geodesic based segmentation. MCG @cite also uses multiple hierarchical segmentations from different scales of the image, but produces proposals by combinatorially grouping regions. Edge boxes @cite uses contour information instead of segments: bounding boxes which have fewer contours straggling the boundary of the box are considered more likely to be objects.
- Many object proposal methods also include a ranking of the regions. This ranking is typically based on low level region features such as saliency @cite , and is sometimes learnt @cite @cite . Relatively simple ranking suffices when the goal is a few thousand proposals as in MCG @cite , but to narrow the list down to a few hundred as in CPMC @cite requires more involved reasoning. DeepBox aims at such a ranking.
- Multibox @cite @cite directly produces object proposals from images using a sophisticated neural network. In contemporary work, Faster R-CNN @cite uses the same large network to propose objects and classify them. DeepMask @cite also uses a very deep network to directly produce segment proposals. In comparison, our architecture is quite lightweight and can be used out of the box to rerank any bottom-up proposals.
- Finally, we direct the reader to @cite @cite for a more thorough evaluation of bottom-up proposal methods.
- Equipped with these tools we can then complete the proof of the area law ) in . This is accomplished by using an extension of an argument from @cite to reduce the claim to the eigencorrelator localization bound ).
- The trace identity ) for Fermion systems seems to originate from @cite , see also @cite for a more detailed presentation and @cite for a survey of subsequent results. In these works ) was used as a tool in the study of the ground state entanglement for the XY chain in constant magnetic field, In particular, the dimension reduction from @math to @math accomplished by ) (or @math to @math for the subsystem) allowed numerical predictions for the limit of large subsystems @math (and in the thermodynamic limit of an infinite chain @math ). These were later proven rigorously, see @cite for most complete results as well as references therein. Generally, it was found that the ground state entanglement for the anisotropic chain remains bounded in the size @math of the subsystem, i.e. satisfies an area law. Phase transitions with divergent entanglement appear for @math and for the isotropic chain @math . In particular, @cite shows that in the latter case the ground state entanglement grows as @math .
- : Image retrieval @cite @cite is a classical problem in this setting. It is not only an active research topic, but also fascinating to commercial image search engines and photo-sharing websites since they would like to better capture data streams on the Internet and thus better serve user's information need. Over the years, various techniques ( click-through data) have been integrated to improve search engine results. Note that, using pretrained models ( CNN @cite ) to clean up web data also falls into this category, since extensive human supervision has already been used.
- Our work is also closely related to another trend in computer vision: learning and exploiting visual representation via CNNs @cite @cite @cite @cite . However, learning these CNNs from noisy labeled data @cite @cite is still an open challenge. Following the recent success of convolutional networks and curriculum learning @cite @cite @cite , we demonstrate that, while directly training CNNs with high-level or fine-grained queries ( random proper nouns, abstract concepts) and noisy labels ( Flickr tags) can still be challenging, a more learning approach might provide us the right solution. Specifically, one can bootstrap CNN training with easy examples first, followed by a more extensive and comprehensive learning procedure with similarity constraints to learn visual representations. We demonstrate that visual representations learned by our algorithm performs very competitively as compared to ImageNet trained CNNs.
- Finally, our paper is also related to learning from weak or noisy labels @cite @cite @cite @cite @cite . There are some recent works showcasing that CNNs trained in a weakly supervised setting can also develop detailed information about the object intrinsically @cite @cite @cite @cite @cite . However, different from the assumptions in most weakly-supervised approaches, here our model is deprived of clean human supervision altogether (instead of only removing the location or segmentation). Most recently, novel loss layers have also been introduced in CNNs to deal with noisy labels @cite @cite . On the other hand, we assume a vanilla CNN is robust to noise when trained with simple examples, from which a relationship graph can be learned, and this relationship graph provides powerful constraints when the network is faced with more challenging and noisier data.
- Numerous techniques exist and a number of different ways exist to try to solve this problem. The methods can be classified into (i) manual, (ii) automatic or semi-automatic depending mainly on matches or features. In the (i) manual methods the registration is performed manually selecting correspondences between each image and the 3D geometry. This technique is often used for medical applications @cite . Others instead, have used features in order to automate the process, but finding consistent correspondences is a very complex problem. Due to the different appearance of photographs and geometric models, (ii) automatic methods are limited to some specific models and information. For example, line features are mostly used for urban environments @cite , @cite ; and silhouette information is used when the contour of the objects is visible in the images and the 3D model projected onto an image plane @cite @cite @cite .
- Nevertheless there are 3D scanners which provide also reflectance images and the registration is performed in a 2D space @cite , @cite . On the other hand, some authors perform their registration in a 3D space reconstructing the 3D object from the 2D images and aligning both 3D objects @cite , @cite , @cite . This procedure is carried out in two steps: (1) 3D reconstruction and (2) point cloud alignment. Through the widely used Structure from Motion technique (SfM), a 3D reconstruction and intrinsic and extrinsic camera parameters are recovered without making any assumptions about the images in the scene. The registration is usually performed by selecting correspondences @cite that minimize the distances between a set of points.
- Our work is based on SfM approach and the use of the SICP algorithm @cite to register both point clouds with the only constraint being to locate them relatively close to each other.
- The ImageCLEF medical image annotation tasks of 2005-2007 by @cite have 9,000 training and 1,000 testing two-dimensional images, converted to @math pixel thumbnails with @math labels. Local image descriptors and intensity histograms are used as a bag-of-features approach in that work for this scene-recognition-like problem. Unsupervised LDA based matching from lung disease words (e.g., fibrosis, emphysema) to two-dimensional image blocks from axial CT chest scans of 24 patients is studied by @cite . The works of @cite @cite using generative models of combining words and images under a very limited word image vocabulary has also motivated this study.
- Graphical models have been employed to predict image attributes by @cite @cite , or to describe images by @cite using manually annotated datasets.Automatic label mining on large, unlabeled datasets is presented by @cite @cite , however the variety of the label-space is limited to image text annotations. We analyze and mine the medical image semantics on both document and sentence levels, and deep CNNs of @cite @cite are adapted to learn them from image contents.
- The margin of victory problem is the same as the optimization version of the destructive bribery problem introduced by @cite @cite . However, to the best of our knowledge, there is no prior work on estimating the cost of bribery by sampling votes.
- @cite also study manipulation under partial information setting. However, they focused on whether or not there exists a and shows that it is for many common voting rules. Given some partial votes, a dominating manipulation is a non-truthful vote that the manipulator can cast which makes the winner at least as preferable (and sometimes more preferable) as the winner when the manipulator votes truthfully. The dominating manipulation problem and the WM and the SM problems do not seem to have any apparent complexity theoretic connection. For example, dominating manipulation problem is for all the common voting rules except plurality and veto, whereas, the SM problem is easy for most of the cases (see table table:partial_summary ). @cite studies manipulation under voting rule uncertainty. However, in our work, the voting rule is fixed and known to the manipulators.
- Two closely related problems that have been extensively studied in the context of incomplete votes are and @cite . In the PW problem, we are given a set of partial votes @math and a candidate @math , and the question is whether there exists an extension of @math where @math wins, while in the NW problem, the question is whether @math is a winner in all extensions of @math . Following the work of , a number of special cases and variants of the PW problem have been studied in the literature @cite @cite @cite @cite @cite @cite @cite . Clearly, the WM problem is a specialization of the PW problem where there exist at least one empty vote (see proposition pw_hard ). On the other hand, the SM problem is significantly different from the NW problem --- the NW problem is known to belong to the complexity class , whereas the SM problem does not belong to unless @math (since, the SM problem is for some voting rules, for example the Borda voting rule). However, the SM problem clearly belongs to the complexity class @math .
- There has been work on unsupervised learning of disparity or motion between frames of videos using neural network models. These methods typically use multiplicative interactions to model relations between a pair of images. Disparities and optical flow can then be inferred from the latent variables. Taylor al @cite approach the task with factored gated restricted Boltzmann machines. Konda and Memisevic @cite use a special autoencoder called synchrony autoencoder'. While these approaches work well in a controlled setup and learn features useful for activity recognition in videos, they are not competitive with classical methods on realistic videos.
- The problem of matrix completion is to recover a low-rank matrix from a subset of entries @cite , which is given by: where @math is the data matrix, @math is the recovered matrix, and @math is the index set of observed entries. Th optimization problem is not only NP-hard, but requires double exponential time complexity with the number of samples @cite . To solve the above problem, one alternative is to use nuclear norm as a relaxation to the rank function: where @math denotes the sum of singular values of matrix @math . @cite developed a first-order procedure to solve the convex problem ), namely singular value thresholding (SVT). @cite minimized the rank minimization by the singular value projection (SVP) algorithm. @cite solved the problem by first trimming each row and column with too few entries, then compute the truncated SVD of the trimmed matrix. Under certain conditions, it showed accurate recovery on the order of @math samples ( @math is the number of samples, @math is the rank of recovered matrix). With the rapid development of matrix completion problem, some more efficient methods have been proposed @cite @cite @cite @cite .
- However, all the methods mentioned above use the nuclear norm as the surrogate to the rank, whose exact recovery can be guaranteed only when the data are sampled uniformly, which is not practical in real world applications. On the other hand, recent empirical on max-norm @cite shows promising results for non-uniform data if one utilize the max-norm as a surrogate @cite . Notably, for some specific problems, such as collaborative filtering, @cite proved that the generalization error bound for max-norm is better than the nuclear norm. More recently, @cite reported encouraging results on the subspace recovery task (which is closely relevant to matrix completion). Since the social trust data is non-uniformly sampled, we believe that a max-norm regularized formulation can better handle the challenge than the nuclear norm. Our formulation is also inspired by a recent theoretical study on matrix completion with 1-bit measurement @cite , which established a minimax lower bound on the general sampling model and derived the optimal convergence rate in terms of Frobenius norm loss. Furthermore, there are several practical algorithms to solve max-norm regularized or max-norm constrained problems, see @cite and @cite for example.
- The seminal work of Gurevich @cite on Abstract State Machines (ASMs, formerly called evolving algebras'') aimed to find a precise formal definition of the notion of algorithm. The major discovery was that all computing formalisms were bound to a specific abstraction level, which implied that almost always encodings were required @cite @cite , so the major breakthrough of ASMs was due to the abstract notion of state, which is defined by general Tarski structures.
- The sequential ASM thesis @cite characterizes sequential algorithms in terms of three postulates: sequential time, abstract state and bounded exploration. Moreover, it establishes the characterization theorem for sequential algorithms stating that algorithms defined this way are exactly captured by sequential ASMs, i.e. a well-defined abstract machine model that relies on the parallel execution of updates on the abstract states, if certain conditions guarding the updates are satisfied. Thus, also sequential ASMs support bounded parallelism, where the bound is a priori fixed by the algorithm and does not depend on the state.
- Following this work, many other extensions of the sequential ASM thesis to different classes of algorithms have been explored. These include the parallel ASM thesis @cite @cite for parallel algorithms, in which the bound on the parallel branches in a computation is dropped. The postulates are significantly more complex, as multisets and multiset operations must be provided explicitly in the background @cite to permit branching and synchronization. While sequential time and abstract state postulates are preserved, the bounded exploration postulate has been replaced by a set of postulates that permit to distinguish between local and global states. There is still a debate in the ASM community, if these postulates can be simplified to obtain an axiomatization for parallel algorithms that is as intuitive as the one for sequential algorithms. A simplified parallel ASM thesis has been conjectured in @cite .
- Robustness discrete optimization with cost uncertainty was initially studied by Kouvelis and Yu @cite and Yu and Yang @cite . These works mainly consider the min-max model, where the goal is to find a solution that minimizes the worst-case cost according to the given set of cost functions. See the paper of Aissi, Bazgan and Vanderpooten @cite for a survey. A closely related class of multi-budgeted problem has received considerable attention recently (see e.g. @cite @cite @cite @cite and references therein).
- An interesting class of problems with uncertainty in the feasible set was introduced by Dhamdhere, Goyal, Ravi and Singh @cite . In this two-stage models the feasibility condition is only fully revealed in the second stage. While resources can be bought in both stages, they are cheaper in the first stage, in which only partial information about the feasible set is available. This model was subsequently studied by several other authors (see @cite @cite @cite ). Different two-stage model was proposed in @cite @cite for the shortest path problem. Several other important network design problems are motivated by robust optimization. Such problems include the minimum @math -edge connected spanning subgraph problem @cite @cite and the survivable network design problem @cite @cite . Various other robust variants of classical combinatorial optimization problems were proposed. For a survey of these results we refer the reader to the theses of Adjiashvili @cite and Olver @cite .
- A detailed overview of current GPS accuracy is provided in the quarterly GPS Performance Analysis Report for the Federal Aviation Administration. A good introduction to the GPS in general, and to its error sources and quality parameters in particular, has been provided by @cite .
- The above-mentioned research has mainly focused on describing and understanding GPS measurement error. In addition to this, filtering and smoothing approaches have been proposed for recording movement data, in order to reduce the influence of errors on movement trajectories. A concise summary of these approaches can be found in @cite . @cite tested smoothing methods that best preserve travelled distance, speed, and acceleration. The authors found that Kalman filtering resulted in the least difference between the true movement and its representation.
- The formula for the law, @math , provides a theoretical distribution of expected first digits, shown in Table 1. On the surface, Benford's Law is quite counter-intuitive. Why would numbers beginning with 1 be any more common than those beginning with 9? However, the law holds across many variations in measurement @cite . Temperatures that follow Benford's Law do so regardless of whether they are measured in Farenheit, Celsius, or Kelvin. Distances follow whether measured in miles, kilometers, or smoots.
- Most persuasively, Hill provided a proof for the Benford's Law in 1995 @cite . He suggests that skeptics jot down all the numbers that appear on the front pages of several news papers or randomly select data from the Farmer's Almanac as a simple experiment and personal demonstration that the law holds @cite . Benford's Law has been shown to describe many naturally occurring sets of numbers. In addition to those already mentioned, there is some research specifically relevant to this paper. It is known that Benford will often apply to systems that follow a power law distribution @cite ; power laws are commonly found in social network structure @cite and social media @cite . While no work has investigated how well Benford's Law describes social networks (online or offline) or social media, research has shown it describes human behavior online through price distributions in eBay auctions @cite .
- A sequence of two papers offers an alternative, and substantially more complex, non-iterative algorithm for matrix balancing in the @math norm. Schneider and Schneider @cite gave an @math -time algorithm based on finding maximum mean-weight cycles in a graph; the running time was improved to @math using Fibonnaci heaps and other techniques by Young, Tarjan and Orlin @cite . This is asymptotically faster than the @math worst case running time we establish for the iterative algorithm; and for some graphs (e.g., the cycle) it is faster than the actual running time of the iterative algorithm.
- A different notion of matrix balancing is often known as Sinkhorn balancing after Sinkhorn @cite , who proposed a natural iterative algorithm analogous to that of Osborne-Parlett-Reinsch. The goal here is to find a scaling matrix @math such that @math has prescribed row and column sums. A polynomial time algorithm for this problem was given by Kalantari and Khachian @cite , while the convergence rate of Sinkhorn's iterative algorithm was studied by several authors @cite @cite @cite @cite . In particular Linial, Samorodnitsky and Wigderson @cite gave strongly polynomial bounds (after a non-Sinkhorn preprocessing step) and derived a surprising approximation scheme for the permanent of a non-negative matrix.
- : Ref. PNC06 @cite first proposed PNC to increase the throughput of a two-way relay network (TWRN). In TWRN, two end nodes exchange information via a relay. PNC doubles the throughput of a TWRN operated with the traditional scheduling scheme PNC06 @cite . PNC has been studied and evaluated in depth: we refer the interested readers to liew2015primer,popovski2006anti,Nazer2011ReliablePNC,FPNCPhycom12,RPNCSRIF13 @cite @cite @cite @cite @cite and the references therein for details. Following the tradition of PNC06 @cite , prior PNC work focused almost exclusively on networks. By contrast, NCMA was the first attempt to apply PNC in networks (i.e., multiple access in wireless networks) NCMA1,NCMA2 @cite @cite . There has been existing work focusing on high-order modulated PNC BICM-ID-PNC,wang2012constellation,yang2010modified @cite @cite @cite . This work, however, was theoretical in nature and assumed perfect synchronization (e.g., no phase offset between the concurrent transmitting signals). The practical implementation of such synchronized systems is much more challenging than the current system studied in our paper.
- : Besides NCMA NCMA1,NCMA2 @cite @cite , there have been other efforts to apply network coding in multiple access networks. The major difference between NCMA and this work is that NCMA tries to decode more than one equation per reception (e.g., tries to decode both the PNC packet and the individual native packets) from one overlapped packet, while most other work targets to get one equation per reception (either the PNC packet or one native packet). For example, RAwithPNC,Cocco2011,SeekandDecode @cite @cite @cite explored forming linear equations from the collided packets to derive source packets. Refs. LivaGraph,CodedRandomAccess @cite @cite treated the collisions in multiple access channel as erasure-correcting codes on graphs. But Cocco2011,LivaGraph,CodedRandomAccess @cite @cite @cite only form one equation for each overlapped packet, whereas NCMA can form one or two equations for each overlapped packet depending on the instantaneous channel condition. Furthermore, the decoding in RAwithPNC, SeekandDecode @cite @cite is based on PHY-layer equations only, while NCMA makes use of another MAC-layer channel coding based on the PHY-layer PNC packets.
- : As with distributed MIMO systems, in MIMO-NCMA, the AP also has multiple antennas. Distributed MIMO can enable spatially separated transmitters to form a virtual MIMO system for multiple access. In the literature, SAM2009, JMB2012, BalanAirSyncToN2013 @cite @cite @cite studied distributed MIMO systems to increase the system throughput, and we refer to them as MIMO-MUD since they only focus on MUD decoding, without incorporating PNC decoding. In this paper, we consider classical MIMO-MUD with zero-forcing (ZF), minimum mean square error (MMSE) decoders as our benchmarks. More sophisticated distributed MIMO decoders can be found in tse2005fundamentals @cite .
- is often modeled as a 3D box (cuboid) @cite @cite @cite @cite @cite . A box provides a good approximation to many rooms, and it has few parameters so that accurate estimation is possible from single RGB images @cite @cite @cite or panoramas @cite . Even when a depth image is available, a box layout is often used (e.g., @cite ) due to the difficulty of parameterizing and fitting more complex models. Others, such as @cite @cite , estimate a more detailed Manhattan-structured layout of perpendicular walls based on visible floor-wall-ceiling boundaries. Methods also exist to recover axis-aligned, piecewise-planar models of interiors from large collections of images @cite or laser scans @cite , benefitting from more complete scene information with fewer occlusions. We model the walls, floor, and ceiling of a room with a collection of axis-aligned planes with cutouts for windows, doors, and gaps, as shown in Figure . Thus, we achieve similar model complexity to other methods that require more complete 3D measurements.
- are also often modeled as 3D boxes when estimating from RGB images @cite @cite @cite @cite @cite or RGB-D images @cite . But cuboids do not provide good shape approximations to chairs, tables, sofas, and many other common objects. Another approach is to fit CAD-like models to depicted objects. Within RGB images, @cite @cite find furniture instances, and @cite recognize chairs using HOG-based part detectors. In RGB-D images, Song and Xiao @cite search for chairs, beds, toilets, sofas, and tables by sliding 3D windows and enumerating all possible poses. Our approach does not aim to categorize objects but to find an approximate shape for any object, which could be from a rare or nonexistent category in the training set. We take an exemplar-based approach, conjecturing that a similar looking object from a different category can often still provide a good approximation to 3D extent. Incorporating category-based 3D detection, such as @cite , is a promising direction for future work.
- Our use of is inspired by the SuperParsing method of Tighe and Lazebnik @cite , which transfers pixel labels from training images based on retrieval. Similar ideas have also been used in other modalities: @cite transfers depth, Guo and Hoiem @cite transfers polygons of background regions, and @cite transfers clothing items. Exemplar-based 3D modeling is also employed by Satkin and Hebert @cite to transfer 3D geometry and object labels from entire scenes. Rather than retrieving based on entire images (which is too constraining) or superpixels (which may not correspond to entire objects), we take an approach of proposing a bag of object-like regions and resolving conflicts in a final compositing process that accounts for fidelity to observed depth points, coverage, and consistency. In that way, our approach also relates to work on segmentation @cite @cite and parsing @cite @cite of RGBD images and generation of bags of object-like regions @cite @cite @cite . We also incorporate a per-object 3D alignment procedure that reduces the need to match to objects with exactly the same scale and orientation.
- that have been developed for scene interpretation from RGB-D images do not provide 3D scene interpretations but could be used to improve or extend our approach. For example, @cite infer support labels for regions, and @cite segment images into objects and assign category labels.
- Consider two arbitrarily connected communities with internal adjacency matrices @math and @math and network sizes @math and @math , respectively. The external connections between these two communities are characterized by an @math adjacency matrix @math , where each entry in @math is a Bernoulli( @math ) random variable. Let @math . The overall @math adjacency matrix of the community structure can be represented as . The widely used stochastic block model @cite is a special case of ) when the two community structures are generated by connected Erdos-Renyi random graphs parameterized by the within-community connection probability @math ( @math ). Our network model is more general since we only assume random connection probability @math on the external edges and we allow the within-community adjacency matrices @math to be arbitrary. In this paper we consider the noisy setting in which the adjacency matrix @math is corrupted by a random adjacency matrix @math such that the observed adjacency matrix is @math . The adjacency matrix @math is generated by a Erdos-Renyi random graph with edge connection probability @math . Note that this model only allows random insertions and not deletions of edges.
- Community detectability has been studied under the stochastic block model with restricted assumptions such as @math , @math and fixed average degree as the network size @math increases @cite @cite @cite @cite @cite . The planted clique detection problem in @cite is a special case of the stochastic block model when @math and @math . A less restricted stochastic block model is studied in @cite where a universal phase transition in community detectability is established for which the critical value does not depend on the community sizes. A similar model to our network model is studied in @cite for interconnected networks. However, in @cite the subnetworks are of equal size and the external edges are known (i.e., non-random). Phase transitions in spectral community detection under noiseless network setting is studied in @cite .
- In recent years, Grassmann manifold @cite has attracted great interest in research community, such as subspace tracking @cite , clustering @cite , discriminant analysis @cite , and sparse coding @cite @cite . Mathematically Grassmann manifold @math is defined as the set of @math -dimensional subspaces in @math . Its Riemannian geometry has been recently well investigated in literature @cite @cite @cite .
- . Representation by Full Columns Rank Matrices @cite : [ G (p,d) R ^ d p _* GL(p) ]
- . The Orthogonal Representation @cite : [ G (p,d) O (d) O (p) O (d-p) ] . Symmetric Idempotent Matrix Representation @cite : [ G (p,d) P R ^ d d : P^T = P, P^2= P, rank ( P) = p . ] Many researchers adopt this representation for learning tasks on Grassmann manifold, for example, Grassmann Discriminant Analysis @cite , Grassmann Kernel Methods @cite , Grassmann Low Rank Representation @cite , etc.
- . The Stiefel Manifold Representation @cite : Denote by @math the set of all the @math dimension bases, called Stiefel manifold. We identify a point @math on Grassmann manifold @math as an equivalent class under orthogonal transform of Stiefel manifold. Generally we have [ G (p,d) ST (p,d) O (p). ]
- Recently, computer vision has been expected to player a larger role within autonomous driving. However, due to its history of relatively low precision, it is typically used in conjunction with either other sensors or other road models @cite @cite @cite @cite . Cho @cite uses multiple sensors, such as LIDAR, radar, and computer vision for object detection. They then fuse these sensors together in a Kalman filter using motion models on the objects. Held @cite , uses only a deformable parts based model on images to get the detections, then uses road models to filter out false positives. Carafii @cite uses a WaldBoost detector along with a tracker to generate pixel space detections in real time. Jazayeri @cite relies on temporal information of features for detection, and then filters out false positives with a front-view motion model.
- In contrast to these object detectors, we do not use any road or motion-based models; instead we rely only on the robustness of a neural network to make reasonable predictions. In addition, we currently do not rely on any temporal features, and the detector operates independently on single frames from a monocular camera. To make up for the lack of other sensors, which estimate object depth, we train the neural network to predict depth based on labels extracted from radar returns. Although the model only predicts a single depth value for each object, Eigen have shown how a neural network can predict entire depth maps from single images @cite . The network we train likely learns some model of the road for object detection and depth predictions, but it is never explicitly engineered and instead learns from the annotations alone.
- Along the same line of studies, we also proposed a system for efficient software updates distribution, over untrusted distribution networks in @cite . We used CP-ABE to guarantee flexible access control. In this work, we only provided evaluations measured on a laptop device with two 2.4 GHz Intel Core 2 Duo CPUs. Based on our experimental results, the CP-ABE encryption and decryption time for five attributes are 77.47 ms and 32.62 ms, respectively. We realized that not providing a proper feasibility assessment is a big limitation for this type of proposals. In this paper, we aim to fill this gap and pave the way for developing further solutions assuming the feasibility of ABE for mobile devices.
- A study similar to the one we are going to discuss in this paper was carried out in @cite . The authors evaluated the performance of CP-ABE and KP-ABE in terms of execution time, data overhead, energy consumption, CPU and memory usage. They implemented these two ABE schemes using Java on a laptop with a 1.60 GHz Intel Quad-Core i7 2677M CPU and a smartphone runs Android 4.04 with a 1.60 GHz Intel Atom Z2460. The authors stated that applying ABE on Android smartphone devices is not practical with acceptable performance. In this paper, we show that this conclusion mostly depends on the specific implementation provided in @cite , and it does not hold in general. Finally, in @cite proposed an alternative approach for efficient ABE decryption. In their solution, a part of the ABE decryption is outsourced to a third party cloud, highly reducing the load on the client device. However, while representing a good option to facilitate the ABE operations on devices with limited resources, this solution requires additional resources compared to in-device decryption, such as a third-party cloud entity, as well as Internet connectivity.
- was studied predominantly on small-scale data and often using qualitative methods. The attitude of email users towards work email was investigated through organizational surveys @cite , finding that the social nature of the message is a stronger motivation to reply than the importance'' of the message. Also, survey respondents tended to reply to about only a third of the messages in their inbox. User studies and targeted interviews about rhythms in email usage, including intervals of replying, have uncovered the role of user expectation in relation with the replying behavior @cite . In particular, when a user perceives that the response has been delayed too much, the resulting triggers a follow-up action in the thread.
- Quantitative studies targeted to specific application-oriented tasks such as classification of emails into folders @cite @cite were conducted mainly on the open Enron email data @cite , which is one of the few complete temporal data on email communication whose structure has been studied extensively @cite . However, Enron's email communication patterns were very specific as they were limited to the context of the company and by its evolution and dramatic fall. This peculiarity held back researchers from drawing conclusions on general patterns of email use. Other less known, small-scale email datasets, such as the email corpora from OSS projects @cite can provide interesting traces on user interaction, but they do not allow any generalization of the findings. Email communication networks have been also used to investigate the propagation of computer viruses @cite and word-of-mouth advertising @cite , or to solve specific tasks such as expert finding within organizations @cite .
- Information overload in email was studied since the 90's. In contrast to Whittaker and Sidner @cite , who defined as the use of email as a tool for task management, archival, and communication, we follow the definition of overload as not being able to keep up with the volume of incoming email @cite @cite . Unlike previous qualitative studies that focused on the perception of overload @cite , we focus on quantitative measures of overload and its observed effects on users' behavior.
- Research of ways how discoveries can be automated or facilitated by machines dates back to the dawn of the digital computer era. The work @cite provides a comprehensive analysis of the discovery process operationalised as creative problem solving. It reviews several classic machine discovery systems and the heuristics used by them, and also mentions several properties of worthy discoveries like novelty and value. A more recent related work @cite reviews the major approaches to studying the process of scientific discovery, provides another survey of automated discovery systems and analyses additional features of relevant discoveries, such as surprise. The works @cite @cite review still more machine discovery systems and heuristics, and identify features like refutability and simplicity as essential to discoveries. One of the most recent and relevant works from this area is @cite . It builds on @cite @cite and introduces formalisations of several discovery features. In particular, it models novelty and value using metric spaces, and surprise using Bayesian probabilities.
- Discovery features discussed in the referenced works conform to our virtue definitions, although most of them do not provide a systematic formalisation, only rather application-specific implementations. For instance, refutability and simplicity as reviewed in @cite directly correspond to our virtues. Surprise and novelty discussed in the other works can be modelled by putting emphasis on radical claims as addressed by the conservatism virtue, only using different distance metrics for each of the respective features. We believe that our approach presents a new way to formalising discovery features that is consistent with related state of the art, but is more systematic, comprehensive and extensible. In addition, we provide an actionable set of measures implemented in the context of knowledge graphs. This enables universal applicability of our research, which is not the case in most of the rather specific afore-mentioned approaches.
- As a recent survey @cite shows, the applicability of existing ontology learning approaches to (semi)automated knowledge discovery is still quite limited. Many of the techniques are dependent on manually curated resources. They also introduce a lot of assumptions during the extraction process (based on, for instance, linguistic facts valid only in the context of a particular language or discourse). This limits their universal applicability. Another problem is that the more complex knowledge representation the learned ontologies use, the more restrictive they are about their meaning. This typically leads to brittleness the often inherently vague and contextual nature of the knowledge they represent. This can easily cause problems in machine-aided knowledge discovery scenarios where we typically want to represent the knowledge implied by the input data in as unbiased way as possible. Another practical limitation is that most ontology learning system do not scale very well as reported in @cite .
- More recent works related to machine discovery using knowledge graphs include @cite @cite @cite which contain also comprehensive reviews of prior similar approaches. The approach elaborated in @cite presents methods for knowledge discovery in RDF @cite data based on user-defined query patterns and analytical perspectives. Our approach complements @cite by offering means for automated analysis and refinement of knowledge graphs using application-independent, well-founded features.
- Google's Knowledge Vault @cite presents a web-scale approach to probabilistic knowledge fusion that uses graphs represented in the RDF format. It tackles the scalability vs. accuracy trade-off of the manual and automatic approaches to construction of knowledge graphs. This is done by refining statements extracted from the web content using models learned from pre-existing highly accurate knowledge bases like YAGO or Freebase. Additional details and broader theoretical context of the approach introduced in @cite is given in @cite , which offers a comprehensive review of relational machine learning approaches in the context of RDF-compatible knowledge graphs. The main advantage of our approach the works @cite @cite is that we are not critically dependent on the background knowledge model. In addition, we present a complementary well-founded approach to determining which relationships in automatically extracted knowledge graphs are worth preservation. Having said that, the techniques reviewed in @cite can certainly provide valuable hints on future extensions of our approach to graphs with oriented edges representing more than one type of relationships ( RDF graphs).
- Concurrent data structures in the fast path of the frontend as well as the backend of scalloc are lock-free @cite . A recent trend towards semantically relaxed concurrent data structures @cite @cite opens doors for new design ideas and even greater performance and scalability so that hierarchies of spans (buffers in general) can be avoided and may be utilized globally across the whole system. The concurrent data structures in scalloc are pools, allowing in principle every data structure with pool semantics to be used. However, unlike segment queues @cite and @math -Stacks @cite , Distributed Queues @cite with Treiber stacks @cite , as used in scalloc, do not require dynamically allocated administrative memory (such as sentinel nodes), which is important for building efficient memory allocators. The data structures for reusable spans within a TLAB are implemented using locks but could in principle be replaced with wait-free sets, which nowadays can be implemented almost as efficiently as their lock-free counterparts @cite .
- Many concepts underlying scalloc such as size classes, hierarchical allocation (local allocation buffers, spans), and local memory caching in so-called private heaps (span ownership) have already been introduced and studied in thread-local variants @cite @cite . Scalloc borrows from some of these concepts and integrates them with lock-free concurrent data structures, and introduces new ideas like virtual spans.
- Allocators implementing private heaps without returning remotely freed objects to the allocating threads suffer from unbounded blowup fragmentation in producer-consumer workloads @cite . Hence, it is necessary to transfer remotely freed memory back to the heap it was allocated on.
- A concept related to virtual spans called spaces appears in the Memory Management Toolkit (MMTk) for managed languages @cite . We note that the generally poor performance of SuperMalloc for concurrent workloads shows that virtual spans alone do not suffice for achieving competitive temporal and spatial performance and scalability. Virtual spans alone only simplify allocator design as they enable uniform treatment of small and big objects, and reduce memory consumption. As shown in our experimental evaluation, the combination of virtual spans with a scalable backend and a high-performance frontend is crucial for achieving competitive performance and scalability.
- Visual saliency computation can be categorized into bottom-up and top-down methods or a hybrid of the two. Bottom-up models are primarily based on a center-surround scheme, computing a master saliency map by a linear or non-linear combination of low-level visual attributes such as color, intensity, texture and orientation @cite @cite @cite @cite @cite . Top-down methods generally require the incorporation of high-level knowledge, such as objectness and face detector in the computation process @cite @cite @cite @cite @cite .
- Recently, much effort has been made to design discriminative features and saliency priors. Most methods essentially follow the region contrast framework, aiming to design features that better characterize the distinctiveness of an image region with respect to its surrounding area. In @cite , three novel features are integrated with a conditional random field. A model based on low-rank matrix recovery is presented in @cite to integrate low-level visual features with higher-level priors.
- Saliency priors, such as the center prior @cite @cite @cite and the boundary prior @cite @cite , are widely used to heuristically combine low-level cues and improve saliency estimation. These saliency priors are either directly combined with other saliency cues as weights @cite @cite @cite or used as features in learning based algorithms @cite @cite @cite . While these empirical priors can improve saliency results for many images, they can fail when a salient object is off-center or significantly overlaps with the image boundary. Note that object location cues and boundary-based background modeling are not neglected in our framework, but have been implicitly incorporated into our model through multiscale CNN feature extraction and neural network training.
- Convolutional neural networks have recently achieved many successes in visual recognition tasks, including image classification @cite , object detection @cite , and scene parsing @cite . @cite pointed out that features extracted from Krizhevsky's CNN trained on the ImageNet dataset @cite can be repurposed to generic tasks. @cite extended their results and concluded that deep learning with CNNs can be a strong candidate for any visual recognition task. Nevertheless, CNN features have not yet been explored in visual saliency research primarily because saliency cannot be solved using the same framework considered in @cite @cite . It is the contrast against the surrounding area rather than the content inside an image region that should be learned for saliency prediction. This paper proposes a simple but very effective neural network architecture to make deep CNN features applicable to saliency modeling and salient object detection.
- Localization on utilizing the RSS of signal has been intensively studied for many years. A popular approach is the Fingerprinting-based approach that establishes the mapping of RSS-to-location for localization. Radar @cite is the first proposed method on Fingerprinting-based localization that estimates the location of the target by the knowledge of APs' location, transmit power and the floor map. Horus @cite built a probability distribution in RSS-to-location mapping and located the target by matching the mapping with maximum likelihood. A variety of improvements or extensions have also been made on Fingerprinting-based localization, such as considering the mobility constraints @cite and incorporating ambient signatures from environment @cite . Another approach on utilizing the RSS is the Model-based approach that directly build the relationship between RSS and distance e.g. the log-distance path loss model @cite . Efforts like locating by APs @cite and learning the ability of measurement @cite have been made based on the model of RSS-to-distance.
- There has been much work on dealing with the SLAM problem in robotics fields. Usually, in the scenario of SLAM, a robot is designated to explore the area of interest for simultaneous localization and mapping with equipped sensors (e.g. camera, laser and sonar). The localization problem is to determine the relative location of the robot in the area, and the mapping problem is to locate the objects surrounding the robot. For example, FootSLAM @cite built the floor plan with shoe-mounted IMU sensors, and WiFi-SLAM @cite used the Gaussian process latent variable model to build RSS-based connectivity graphs.
- The phenomenology of these codes under AMP decoding, in particular the sharp phase transitions different between MAP and AMP decoding, has many similarities with what appears in low density parity check codes (LDPC) @cite . It is actually in the context of LDPC codes that spatial coupling has been introduced @cite @cite in order to deal with this phase transition phenomenon that blocks the convergence of low-complexity message-passing based decoders. These similarities are not a priori trivial because LDPC codes are codes over finite fields, the sparse superposition codes work in the continuous framework. Furthermore LDPC codes are decoded by loopy belief-propagation (BP) whereas sparse superposition codes are decoded by AMP which is a Gaussian approximation of loopy BP. However, they arise due to a deep connection to compressed sensing, where these phenomena (phase transition, spatial coupling, etc) have been studied as well @cite @cite @cite @cite and we shall make use of this connection extensively.
- The AMP algorithm, which stands at the roots of our approach, is a simple relaxation of loopy BP. While the principle behind AMP has been used for a long time in physics under the name of Thouless-Anderson-Palmer equations @cite , the present form has been originally derived for compressed sensing @cite @cite @cite and is naturally applied to sparse superposition codes as this scheme can be interpreted as a compressed sensing problem with structured sparsity. The state evolution technique @cite is unfortunately not yet fully rigorous for the present AMP approach, due to the structured sparsity of the signal, but in spite of that, we conjecture that it is exact.
- The third category takes a hybrid approach for learning the discriminative sparse representation. In these approaches, the dictionaries are designed to have a set of shared atoms in addition to class-specific atoms. @cite extended the SRC algorithm by appending an intra-class face variation dictionary to the training data. This extension achieves promising results in face recognition with a single training sample per class. Zhou and Fan @cite employ a Fisher-like regularizer on the representation coefficients while learning a hybrid dictionary. Wang and Kong @cite learned a hybrid dictionary to separate the common and particular features of the data. Their approach additionally encouraged the class-specific dictionaries to be incoherent during the optimization process. @cite proposed to learn a multi-level dictionary for hierarchical visual categorization. To some extent, it is possible to reduce the size of the dictionary using the hybrid approach, which also results in reducing the classification time in comparison to the approaches that fall under the first category. However, it is often non-trivial to decide on how to balance between the shared and the class-specific parts of the hybrid dictionary @cite , @cite .
- Existing structured P2P systems can be classified into two broad categories: Distributed Hash Table (DHT)-based systems and tree-based systems. Examples of the former, which constitute the majority, include Chord, CAN, Pastry, Symphony, Tapestry @cite and P-Ring @cite . In general, DHT-based systems support exact match queries well and use (successfully) probabilistic methods to distribute the workload among nodes equally. Since hashing destroys the ordering on keys, DHT-based systems typically do not possess the functionality to support straightforwardly range queries, or more complex queries based on data ordering (e.g., nearest-neighbour and string prefix queries). Some efforts towards addressing range queries have been made in @cite @cite , getting however approximate answers and also making exact searching highly inefficient. The most recent effort towards range queries is the P-Ring @cite . P-Ring is fully distributed and fault-tolerant, provides load-balancing and supports both exact match and range queries, achieving @math range search performance in average case ( @math is the number of peers, @math is the of the ring and @math is the answer size) and @math in worst case.
- YouTube had been the subject of study in many papers from different perspectives: from users' behavior @cite to the social network @cite , from video popularity dynamics @cite to protocols aspects @cite @cite .
- Considering the YouTube delivery infrastructure, a large body of work verified its evolution over time @cite @cite @cite @cite @cite @cite . They show a highly dynamic system which keeps changing over time due to continuous upgrades in the infrastructure @cite @cite or due to the dynamicity of the cache selection policies @cite @cite @cite @cite . Some of the findings are already outdated. For instance, the load-balancing policy based on HTTP redirections which is described in @cite @cite is no longer in place, and YouTube dismissed the naming scheme described in @cite at the end of 2011. In this work, we do not aim to offer an updated view of YouTube. We rather aim at offering a methodology that allows to automatically identify changes in both the infrastructure, e.g., the appearance of new , and in the day to day management of the infrastructure, e.g., a change in the load-balancing algorithm that may affect millions of customers.
- In some sense, our contribution is in line with the body of works focusing on anomaly detection, for which @cite @cite offer good surveys. Most of the works in this area target anomalies in a security context, e.g., the design of intrusion detection systems. Most of them fall in the supervised'' category, i.e., given a baseline is built, the proposed methodologies highlight deviations from it. To the best of our knowledge, only @cite targets large scale anomaly detection in operational networks. However, it presents a supervised system, which relies on data from passive probes, topology information and routing tables to feed a classic forecasting system, which finally compares its predictions to the actual measurements to pinpoint deviations. , on the other hand does not assume any knowledge of a baseline, and leverages unsupervised algorithms to automatically unveil changes. We specifically design it to target the YouTube CDN, for which the ground truth is a moving target that is very difficult to know.
- Finally, other approaches as @cite aim at measuring the distance among different time snapshots by considering the sample distributions obtained from them. However, directly relying on distributions to perform the comparison considerably complicates the detection of the behind the changes. Instead, extracts and compares clustering patterns, which are simpler to process in an automatic manner, and allow to immediately pinpoint the (i.e., the clusters) responsible for possible deviations.
- Several attempts have been made in recent years in order to produce a subset of an ensemble that performs as well as, or better than, the original ensemble. The purpose of ensemble pruning is to search for such a good subset. This is particularly useful for large ensembles that require extra memory usage, computational costs, and occasional decreases in effectiveness. @cite recently amalgamated a survey of ensemble pruning techniques where they classified such techniques into four categories: ranking based, clustering based, optimization based, and others. Ranking based methods, that are relevant to us in this paper, are conceptually the simplest. Since using the predictive performance to rank models is too simplistic and does not yield satisfying results @cite @cite , ranking based methods employ an evaluation measure to rank models. Kappa statistic measure @math was used in @cite for pruning AdaBoost ensembles. For bagging ensembles, however, kappa has proven to be non-competitive @cite . For bagging ensembles, @cite developed an efficient and effective pruning method based on orientation ordering where the classifiers obtained from bagging are reordered and a subset is selected for aggregation.
- Because of the vital role diversity plays on the performance of ensembles, it had received a lot of attention from the research community. G. @cite summarized the work done to date in this domain from two main perspectives. The first is a review of the various attempts that were made to provide a formal foundation of diversity. The second, which is more relevant to this paper, is a survey of the various techniques to produce diverse ensembles. For the latter, two types of diversity methods were identified: implicit and explicit. While implicit methods tend to use randomness to generate diverse trajectories in the hypothesis space, explicit methods, on the other hand, choose different paths in the space deterministically. In light of these definitions, bagging and boosting in the previous section are classified as implicit and explicit respectively.
- G. @cite also categorized ensemble diversity techniques into three categories: starting point in hypothesis space, set of accessible hypotheses, and manipulation of training data. Methods in the first category use different starting points in the hypothesis space, therefore, influencing the convergence place within the space. Because of their poor performance of achieving diversity, such methods are used by many authors as a default benchmark for their own methods @cite . Methods in the second category vary the set of hypotheses that are available and accessible by the ensemble. For different ensembles, these methods vary either the training data used or the architecture employed. In the third category, the methods alter the way space is traversed. Occupying any point in the search space, gives a particular hypothesis. The type of the ensemble obtained will be determined by how the space of the possible hypotheses is traversed.
- Regardless of the diversity creation technique used, diversity measures were developed to measure the diversity of a certain technique or perhaps to compare the diversity of two techniques. @cite presented a theoretical analysis on six existing diversity measures: disagreement measure @cite , double fault measure @cite , KW variance @cite , inter-rater agreement @cite , generalized diversity @cite , and measure of difficulty @cite . The goal was not only to show the underlying relationships between them, but also to relate them to the concept of margin, which is one of the contributing factors to the success of ensemble learning algorithms.
- Link prediction The link prediction problem in networks comes in many flavors and variants. Unsupervised methods for link prediction in social networks were extensively evaluated by Liben Nowell and Kleinberg @cite , who found the Adamic--Adar measure @cite to perform best. More recently approaches based on network community detection @cite @cite @cite and random walks @cite were considered for predicting missing links. Supervised link prediction @cite was also studied by the relational learning community @cite @cite , but scalability remains a challenge with these approaches.
- While the above works focused mostly on the identification of missing links in social networks, there is also a rich line of work on the identification of missing links among Wikipedia articles @cite @cite @cite @cite and on linking existing webpages to Wikipedia @cite @cite @cite . Generally these approaches focus on building models of Wikipedia's graph structure, while also performing keyword extraction and word sense disambiguation.
- Much research has followed in Kleinberg's wake, so we focus on the most directly related projects: data sets such as ours were previously analyzed by West and Leskovec, who characterize human strategies in successful navigation tasks @cite and train machine learning models capable of navigating automatically @cite , and by Helic al @cite and Trattner al @cite , who explore heuristic navigation algorithms based on hierarchical knowledge representations.
- Games with a purpose Games with a purpose' @cite were popularized by von Ahn and colleagues, a seminal early example being the for labeling images @cite . Wikispeedia was originally designed as a game with a purpose for computing the semantic relatedness between concepts @cite . Further relevant work was done by Ageev al @cite , who developed a human computation game for collecting data in which users are asked to find the answers to as many factual questions ( , What is the highest peak in the Western Hemisphere?') as possible within a given amount of time, using Web search queries that may optionally be followed by click based navigation. As in our navigation data sets, the goal is explicitly known here, but not in the form of a specific target page but rather in the form of a specific answer string.
- We differentiate our approach from those aiming to discover new regions @cite or redefine neighborhood boundaries @cite using geotagged data. @cite presented a model to estimate the geographical center of a search engine query and its spatial dispersion, where the dispersion measure was used to distinguish between local and global queries. @cite used -means clustering to separate geotagged photos into geographic regions, where each region was described by the most representative tags. This work was followed up by Rattenbury and Naaman @cite , in which the authors proposed new methods for extracting place semantics for tags. While these prior works principally attempted to identify new regions or arbitrary spaces, we instead aim to find the unique characteristics of regions in a known hierarchy. For each region at each level in the hierarchy we aim to find a specifically descriptive set of tags, drawn from the unstructured vocabulary of community-generated annotations.
- Hollenstein and Purves @cite examined Flickr images to explore the terms used to describe city core areas. For example, they found that terms such as downtown are prominent in North America, cbd (central business district) is popular in Australia and citycenter is typical for Europe. The authors further manually categorized tags in the city of Z " u rich according to a geographical hierarchy; even with local knowledge they found that doing so was tedious because of tag idiosyncrasies and the diversity of languages. In contrast, our method enables us to automatically obtain such classifications. Moreover, we are not just able to distinguish between local and global content, but can classify a tag according to a geographical hierarchy with an arbitrary number of levels.
- Hierarchical mixture models can be used for building complex probability distributions. The underlying hierarchy, that encodes the specificity or generality of the data, might be known a priori @cite or may be inferred from data by assuming a specific generative process, such as the Chinese restaurant process @cite for Latent Dirichlet Allocation @cite @cite , where the regions and their hierarchy are learned from data. We emphasize that, in this paper, we do not try to learn latent geographical regions, but rather aim to describe regions that are known a priori. Moreover, these regions are structured according to a known hierarchy.
- There is a long tradition of exploring the potential design space of smart home' technologies through study of natural behaviours in the home (e.g. @cite ), and investigating the ways in which families respond to the opportunity to program and configure existing digital technologies @cite . Sticky notes are often included in the kit of materials for cultural probe studies (e.g. @cite ), and the sticky note metaphor has been literally rendered in home technology probes (e.g. @cite ). The refrigerator door, as a location where sticky notes and other papers are placed, is a regular focus of smart home technology, both as a versatile augmented display surface for research attention @cite and more literally in commercial products such as the Samsung WiFi-enabled fridge http: www.samsung.com us topic apps-on-your-fridge . One can imagine that sticky notes themselves might be used as an EUP technology in future (e.g. @cite ), and there are many previous systems that have augmented sticky notes in ways that might achieve this (e.g. as reviewed by @cite ).
- A text-based CAPTCHA presents an obfuscated word in the form of an image, and asks the user to read and rewrite it, usually in a text box. @cite proposed the first text-based CAPTCHA in 2002. After this first proposal, several other researchers worked on this kind of design. Several researchers focused on improving the resiliency against automated attacks @cite @cite @cite . Currently, text-based CAPTCHAs are the most widely used @cite . In the following, we report two examples of text-based CAPTCHAs that, as for CAPTCHaStar, do not require a keyboard to submit the answer: iCaptcha @cite and DDIM-CAPTCHA @cite .
- CAPTCHA designers reacted to these attacks proposing several improvements to mitigate their effectiveness. Some examples follow (between parenthesis we indicate the attack for which the mitigation strategy could be effective): Add more layers of interaction between user and CAPTCHA (could be effective for threat A01 above). Add more distortion to the letters, e.g., warping, scaling, rotating (against A02 and A06). Use of English-like words (for the sake of usability) or totally random words (against A03). Add more pollution to the image, e.g., ticker lines over the letters, confusing background (against A04 and A05). Increment noise, e.g., degrading the quality of the resulting image (against A07). Unfortunately, some of these mitigation strategies have been shown to be ineffective @cite @cite .
- Image-based CAPTCHAs usually ask the user to recognize an image or to interact with on-screen objects to find a solution. Unlike text-based CAPTCHAs, every image-based design is substantially different from each other. For this reason, a user who faces a CAPTCHA design for the first time needs a little more effort to understand how it works. Studies suggest that image-based CAPTCHAs are more appreciated by users @cite . Indeed, image-based CAPTCHAs usually have a high success rate and they are less challenging than text-based ones @cite . In the following, we report some examples of image-based CAPTCHA that we could group in three sub-categories: static, motion, and interactive.
- One of the representative static image-based CAPTCHAs was Asirra @cite , which was discontinued in fall 2014. Asirra asks the user to distinguish between cats and dogs, on twelve different photos randomly taken from an external website. Another static image-based CAPTCHA is Collage @cite : it requests to click on a specific picture, among six pictures randomly taken. Deep CAPTCHA @cite prompts the user with six 3D models of real world objects and it asks to sort them by their size. Some designers focus on CAPTCHA that requires video recognition rather than static image recognition. For example, Motion CAPTCHA @cite shows the user a randomly chosen video from a database, then it asks the user to identify the action performed by the person in the video. Similarly, YouTube Videos CAPTCHA @cite leverages on real video in YouTube service, and it asks the user to write three tags related to the content of the video.
- Interactive CAPTCHAs mitigate the relay attack threat. For example, Noise CAPTCHA @cite presents a transparent noisy image overlapped to a noisy background. The user needs to drag this image until he can recognize a well formed text. Cursor CAPTCHA @cite changes the appearance of mouse cursor into another random object. The user needs to overlap the cursor on the identical object placed in a random generated image. Jigsaw CAPTCHA @cite reprises the classical jigsaw puzzle. Indeed, the user needs to correctly rearrange the pieces of a jigsaw. Finally, PlayThru @cite asks the user to solve a randomly generated mini-game. These mini-games require to drag objects on their correct spots.
- Obtaining bounds for easy data in the experts setting is typically achieved by adaptively tuning a , which is a parameter found in many algorithms. Schemes for the learning rate on-line are built by @cite @cite @cite @cite @cite @cite . These schemes typically choose a monotonically decreasing sequence of learning rates to prove a certain regret bound.
- Other approaches try to multiple learning rates. The motivations and techniques here show extreme diversity, ranging from by @cite @cite , and by @cite to by @cite and by @cite . The last scheme is of note, as it does not aggregate to reproduce a bound of a certain form, but rather to compete with the optimally tuned learning rate for the Hedge algorithm.
- Various models of the file-synchronization problem have been considered in the literature -- see Table @math for a summary. Our work here differs from each of those works in significant ways. For instance, in our model the encoder knows both files, hence we design one-way communication protocols (rather than the multi-round protocols required in the models where the encoder and the decoder each has one version of the file as in @cite @cite @cite @cite @cite @cite ); hence our protocols are information-theoretically near-optimal (however for two-way communication model, computationally efficient schemes which achieve rates with constant factors to the lower bounds are already challenging). The one-way communication model studied in @cite @cite is the closest to our RPES-LtRRID model. For the information-theoretical lower bound, we differ from @cite by considering both insertions and deletions, and arbitrary alphabet. The achievability scheme in @cite matches the lower bound up to first order term for the random source edit model, whereas our scheme is universal" for both RPES-LtRRID and APES-AID models in our work. The literature on insertion deletion channels and error-correcting codes is also quite closely related -- indeed, we borrow significantly from techniques in @cite @cite @cite .
- Hand pose estimation is an old problem in Computer Vision, with early references from the nineties, but it is currently very active probably because of the appearance of depth sensors. A good overview of earlier work is given in @cite . Here we will discuss only more recent work, which can be divided into two main approaches.
- The first approach is based on generative, model-based tracking methods. @cite @cite use a 3D hand model and Particle Swarm Optimization to handle the large number of parameters to estimate. @cite also considers dynamics simulation of the 3D model. Several works rely on a tracking-by-synthesis approach: @cite considers shading and texture, @cite salient points, and @cite depth images. All these works require careful initialization in order to guarantee convergence and therefore rely on tracking based on the last frames' pose or separate initialization methods---for example, @cite requires the fingertips to be visible. Such tracking-based methods have difficulty handling drastic changes between two frames, which are common as the hand tends to move fast.
- The second type of approach is discriminative, and aims at directly predicting the locations of the joints from RGB or RGB-D images. For example, @cite and @cite rely on multi-layered Random Forests for the prediction. The former uses invariant depth features, and the latter uses clustering in hand configuration space and pixel-wise labelling. However, both do not predict the actual 3D pose but only classify given poses based on a dictionary. Motivated by work for human pose estimation @cite , @cite uses Random Forests to perform a per-pixel classification of depth images and then a local mode-finding algorithm to estimate the 2D joint locations. However, this approach cannot directly infer the locations of hidden joints, which are much more frequent for hands than for the human body.
- @cite proposed a semi-supervised regression forest, which first classifies the hands viewpoint, then the individual joints, to finally predict the 3D joint locations. However, it relies on a costly pixel-wise classification, and requires a huge training database due to viewpoint quantization. The same authors proposed a regression forest in @cite to directly regress the 3D locations of the joints, using a hierarchical model of the hand. However, their hierarchical approach accumulates errors, causing larger errors for the finger tips.
- Even more recently, @cite uses a CNN for feature extraction and generates small heatmaps'' for joint locations from which they infer the hand pose using inverse kinematics. However, their approach predicts only the 2D locations of the joints, and uses a depth map for the third coordinate, which is problematic for hidden joints. Furthermore, the accuracy is restricted to the heatmap resolution, and creating heatmaps is computationally costly as the CNN has to be evaluated at each pixel location.
- The hand pose estimation problem is of course closely related to the human body pose estimation problem. To tackle this problem, @cite proposed per-pixel semantic segmentation and regression forests to estimate the 3D human body pose from a single depth image. @cite recently showed it was possible to do the same from RGB images only, by combined body part labelling and iterative structured-output regression for 3D joint localization. @cite recently proposed a cascade of CNNs to directly predict and iteratively refine the 2D joint locations in RGB images. Further, @cite used a CNN for part detection and a simple spatial model, which however, is not effective for high variations in pose space.
- Learning abstract features (with neural networks in many cases) has been extensively studied in recent years and has proved effective in many applications. For instance, numerous studies @cite @cite @cite @cite @cite have shown that deep neural networks perform well for complex computer vision classification tasks, while many demonstrate that success can be achieved with deep learning architectures for audio classification tasks as well @cite @cite . These well-performing deep neural networks have a variety of core ideas, ranging from restricted boltzmann machines that utilize an energy model @cite @cite @cite , to sparse autoencoders that introduce an unsupervised denoising'' mechanism to remove insignificant, noisy signals from data @cite @cite @cite , to using convolution as an effective way to learn representative features robust to geometric locations of images @cite @cite .
- The main advantage of such methods is that they have a strong capability of unravelling the hidden hierarchical structure of data to derive representative features. Moving from a shallower architecture to a deeper architecture, these models progressively detect essential components of the data from local parts like strokes in human handwriting, to global compositions such as digits or objects. Among the variations of neural networks, inspired by biological processes @cite , convolutional networks in particular excel in finding such abstract features that are robust to geometric variations in images @cite . Interestingly, such advantages of convolutional neural networks are present not only in vision tasks, but also in speech recognition @cite @cite @cite and natural language processing @cite @cite .
- Now we consider the periodical time-series prediction problem for data such as daily traveling distances or daily household power consumptions. To tackle this problem, conventionally statistical models such as autoregression and its variants are strongly favored. While in the past decade, realizing there is abstract and structural information beneath the raw numeric values in the time-series, researchers have experimented to discover such patterns by clustering or motif'' discovery @cite @cite @cite . Though conceptually similar, these motifs'' usually are concrete subsequences that are restricted by specific mathematical definitions, which differentiate themselves from the concept of abstract, representative snippets in our paper. However, how to design a method that can find abstract patterns as well as predict future values, that meanwhile is robust to various temporal distortions and misalignment, is yet to be answered. Inspired by the success of convolutional neural networks, we investigate using convolution-based neural networks to address this problem.
- CRFs are used in many semantic segmentation algorithms to regularize output labels. The Hierarchical Conditional Random Field (HCRF) @cite uses different levels of quantization, from pixels to segments. They operate under the assumption that there is unlikely to be a single optimal quantization level that is suitable for all object categories.
- Beyond regularizing just neighboring pixels, several works model the relationships between all pixels. In the Dense CRF semantic segmentation of @cite , mean field approximation and Gaussian filtering is used to make inference in fully connected models practicable. Further, @cite demonstrates that using a Dense CRF to infer all test images at once gives better results than inferring one image at a time. Our approach shows that a 4-connected neighbor CRF model can achieve results comparable to the fully connected model (both were tested on the MSRC-21 dataset).
- Co-occurrence statistics had been exploited in semantic segmentation systems to boost accuracy. The HCRF was improved further in @cite by incorporating a co-occurrence potential as per-image context information, into the CRF energy function. We propose a simpler system that also uses the co-occurrence statistics, but incorporates them in a different manner. Our system achieves comparable average recall scores to @cite , without tuning our parameters per-dataset.
- Gonfaus al @cite proposed another improvement to the HCRF, by adding a new consistency potential to the model, called the harmony potential. The harmony potential encodes all possible combinations of labels, allowing regions to have more than one class, which was a perceived limitation of HCRF models. Further, in @cite , they introduced three more cues into the local unary potential to improve recall scores over their previous version. While certainly worthwhile, these algorithms, @cite @cite @cite @cite , achieve ever better results by adding complexity to their models. Our system maintains a very plain model, using a simple 4-connected CRF with potts pairwise potentials to encourage harmonization of the neighboring pixels. Our proposed simple system outperforms @cite on both average and global recall scores.
- A sophisticated model was successfully demonstrated in @cite , where the problems of semantic segmentation, object detection, and scene classification were cast as one holistic CRF model. Their parameters are learned via a structured learning algorithm, and inference is accomplished by a convergent message-passing algorithm. The model exploits various cues, such as scene type, co-occurrence statistics, the shape and location of the object, and different quantization levels to boost the segmentation result. In contrast, our proposed system exploits some of these important cues as context, but integrates them together with a much simpler model, achieving accuracy that approaches that of the more sophisticated model. Most recently, CNNs have also been exploited in a more sophisticated semantic segmentation framework @cite . They compute feature vectors for each proposed region using two CNN's, trained especially on bounding boxes and free-form versions of the region. Thereafter, the concatenated feature vectors are passed through a linear SVM classifier to get a per-class potential. The final class label for each region is assigned via non-maximum suppression. Although, the system performed very well on the PascalVOC 2012 dataset, it did so at the expense of algorithm complexity.
- Current state-of-the-art localization techniques can only provide location estimates with an average accuracy around 10m @cite , which is not suitable for lane-level localization. To overcome this, researchers proposed different techniques @cite @cite @cite that are based on using an external accurate GPS device (L1 GPS or DGPS) for localization and combining it with other sensors to provide more accurate lane-level location estimates. For example, in @cite authors fuse a high-accuracy GNSS receiver, an odometer, and a gyroscope, along with an enhanced digital map (that describes the road geometry using a particle filter-based algorithm) to position the user based on a combined GNSS-dead-reckoning approach and map matches her to her lane. Similarly, in @cite authors fused an L1-GPS device, camera and an enhanced digital map that stores lane markings information using a dynamical Kalman filter with map matching to get an accurate user position.
- Computer vision techniques have been proposed to detect the car lane. For example, in @cite authors use the iPhone camera to detect the lane markings. Using cameras for lane detection, however, is highly susceptible to errors due to various factors such as lighting condition (e.g. night time, sun glare, headlight glare, shadows from nearby buildings, etc), bad weather conditions (e.g. snow, rain), and other environmental noise (e.g., faded lane marks, surrounding objects like buildings, parked cars, etc). Moreover, both camera and GPS have high energy requirements for the limited phone battery.
- The study of non-scalar waves on black hole backgrounds has focused primarily on Maxwell's equations: Sterbenz and Tataru @cite showed local energy decay for Maxwell's equations on a class of spherically symmetric asymptotically flat spacetimes including Schwarzschild. Blue @cite established conformal energy and pointwise decay estimates in the exterior of the Schwarzschild black hole; Andersson and Blue @cite proved similar estimates on slowly rotating Kerr spacetimes. These followed earlier results for Schwarzschild by Inglese and Nicolo @cite on energy and pointwise bounds for integer spin fields in the far exterior of the Schwarzschild black hole, and by Bachelot @cite , who proved scattering for electromagnetic perturbations. Finster, Kamran, Smoller and Yau @cite proved local pointwise decay for Dirac waves on Kerr. There are further works which in particular establish bounds for certain components of the Maxwell field, see Donniger, Schlag and Soffer @cite and Whiting @cite . Dafermos @cite , @cite studied the non-linear Einstein-Maxwell-scalar field system under the assumption of spherical symmetry.
- The framework in which we describe resonances was introduced by Vasy @cite . In the scalar setting, this can directly be combined with estimates at normally hyperbolic trapping @cite @cite @cite to obtain resonance expansions for scalar waves. On exact Kerr-de Sitter space, Dyatlov proved a significant strengthening of this in @cite , obtaining a full resonance expansion for scalar waves, improving on the result of Bony and H "afner @cite in the Schwarzschild-de Sitter setting, which in turn followed S 'a Barreto and Zworski @cite . Vasy @cite proved the meromorphic continuation of the resolvent of the Laplacian on differential forms on asymptotically hyperbolic spaces, and the fact that the underlying analysis of @cite works on sections of vector bundles just as it does on functions is fundamental for the present paper.
- For the stochastic block model, most previous work focuses on the dense'' regime with an average degree diverging as the size of the graph @math grows, (see, e.g., @cite @cite and the references therein). For the sparse'' regime with bounded average degrees, a sharp phase transition threshold for reconstruction was conjectured in @cite by analyzing the belief propagation algorithm. The converse part of the conjecture was rigorously proved in @cite . The achievability part is proved independently in @cite @cite . In addition, it is shown in @cite that a variant of spectral method gives a positively correlated partition when above the threshold by an unknown constant factor. More recently, it is shown in @cite that a semidefinite program finds a correlated partition when above the threshold by some large constant factor.
- The labeled stochastic block was first proposed and studied in @cite and a new reconstruction threshold that incorporates the extra labeling information was conjectured. Simulations further indicate that the belief propagation algorithm works when above the threshold, but reconstruction algorithms that provably work are still unknown.
- Finally, @cite and @cite make use of both long term and short term context to re-rank search results. Main feature they use are previously clicked urls.
- A graph model related to uniform random intersection graphs is the so-called : each item in a pool is assigned to each node independently with the same probability, and two nodes establish an undirected edge upon sharing at least one item. Note that here the number of items on each node follows a binomial distribution. This graph model has also been studied in the literature as noted below. For connectivity, Rybarczyk presents a zero--one law @cite and later obtains the stronger result of the asymptotically exact probability @cite . For @math -connectivity, she establishes zero--one laws @cite @cite , and we compute the asymptotically exact probability @cite . For perfect matching containment, Rybarczyk provides a zero--one law @cite and derives the asymptotically exact probability @cite . For Hamilton cycle containment, Efthymioua and Spirakis @cite , and Rybarczyk @cite @cite show zero--one laws.
- @cite explored a rich data set, comprising of 14.254 accident cases described with 48 attributes containing information related to road users (drivers, pedestrians and passengers), vehicles and road. In their study, two predictive modeling methods were used: CART and Random Forests. The experimental results done using CART analysis to assess the injury risk, scored with respect to the area under the ROC curve (AUC) a result of 0.8827. While running Random Forests, the authors also found that the age of the victim, victim occupation, among others, were the attributes with the most predictive power.
- A number of existing studies on information dissemination in wireless networks (e.g. @cite ) focused on connected networks, where a network is said to be at a time instant if and only if (iff) there is at least one multi-hop path connecting any pair of nodes. In mobile networks, it is often unnecessary or impractical to require that a network is connected @cite @cite , due to fast-changing network topology or channel randomness. Hence this paper studies information dissemination in MANETs from a percolation perspective, as described formally in Section .
- Epidemic broadcast schemes are popularly used for information dissemination in MANETs. In @cite , studied the information dissemination process using a Susceptible-Infectious (SI) epidemic scheme, where every node carries the received information and forwards it to all nodes coming into the radio range. The SI epidemic scheme is a reliable but costly scheme due to a lack of a proper mechanism to stop the transmission. Considering a Susceptible-Infectious-Recovered (SIR) epidemic scheme, our previous work @cite studied the information dissemination process in a MANET. The SIR epidemic scheme postulates that nodes need to keep transmitting for a prescribed time period before recovery (i.e. stopping the transmission); nevertheless a long continuous transmitting period required by the scheme can be difficult or costly to implement in reality. Then in a conference version of this work @cite , we took shadowing effect into account and proposed the opportunistic broadcast scheme. This paper takes a further step by taking fast fading effect into consideration and investigating the optimal design for the opportunistic broadcast scheme that minimises the resource consumption.
- In @cite , studied the speed of information propagation in a mobile network where nodes move independently at random over a square area. They obtained an upper bound on the flooding time, which is the maximum time required for all nodes of the network to be informed. In @cite , studied the information propagation speed in mobile networks where nodes are uniformly distributed in a bounded area. The nodes were assumed to move following an i.i.d. random trajectory. An upper bound on the information propagation speed, viz. ratio of the distance traveled by information over a given amount of time, was obtained.
- Different from the aforementioned two studies on the information propagation speed (i.e. @cite @cite ), this paper focuses on analysing the fraction of nodes that can receive the information. We choose to focus on the fraction of recipients nodes because it is a key performance metric for information broadcast in a network using the SIR scheme. Compared with the Susceptible-Infectious (SI) forwarding scheme used in @cite @cite , using the SIR scheme a relay node does forward the received packets indefinitely, which reflects the real world scenarios where mobile devices usually have limited energy supply and or buffer size. On the other hand, unlike the SI-like schemes, the SIR scheme does not guarantee that the information can be received by in a network, and consequently the fraction of recipients becomes a key performance metric, which is studied in this paper.
- Moreover, a number of existing work in this area (e.g. @cite @cite @cite @cite ) considered the unit disk model, under which two nodes are directly connected if and only if the Euclidean distance between them is not larger than the radio range @math . Differently, this work takes shadowing and fading effects into consideration and shows that channel randomness can significantly affect the performance of information dissemination in mobile ad-hoc networks.
- There are other broadcast schemes for MANETs beside epidemic schemes. In @cite , reviewed some gossip-based algorithms that can be suitable candidates for information dissemination in MANETs. They pointed out that the design of energy and bandwidth efficient information dissemination schemes for MANETs is a challenging and open problem. A recent work @cite of proposed a novel data forwarding strategy which exploits the transient social contact patterns in social networks. They identified that the low nodal density and the lack of global information are two major challenges in effective data forwarding in delay tolerant networks. We further note that in @cite , Xiang and first proposed an energy efficiency model for wireless cellular networks, and on that basis built an analytical relationship among the wireless traffic, wireless channel model and the energy efficiency. Their work suggested that the energy efficiency optimization of wireless networks should consider the wireless channel randomness. In this paper, these challenges are tackled by introducing a distributed energy-efficient broadcast scheme taking into account channel randomness.
- Unsupervised object discovery has long been attempted in computer vision. Sivic al @cite and Russell al @cite apply statistical topic discovery models. Grauman and Darrel @cite use partial correspondence and clustering of local features. Kim and Torralba @cite employ a link analysis technique. Faktor and Irani @cite propose clustering by composition. Unsupervised object discovery, however, has proven extremely difficult in the wild''; all of these previous approaches have been successfully demonstrated in a restricted setting with a few distinctive object classes, but their localization results turn out to be far behind weakly-supervised results on challenging benchmarks @cite @cite @cite .
- Region proposals have been used in many of the methods discussed so far, but most of them @cite @cite @cite @cite @cite @cite use relatively a small number of the best proposals (typically, less than 100 for each image) to form whole object hypotheses, often together with generic objectness measures @cite . In contrast, we use a large number of region proposals (typically, between 1000 and 4000) as primitive elements for matching without any objectness priors. While many other approaches @cite @cite @cite also use correspondences between image pairs to discover object regions, they do not use an efficient part-based matching approach such as ours. Many of them @cite @cite @cite are driven by correspondence techniques, , the SIFT flow @cite , based on generic local regions. In the sense that semi-local or mid-level parts are crucial for representing generic objects @cite @cite , we believe segment-level regions are more adequate for object matching and discovery. The work of Rubio al. @cite introduces such a segment-level matching term in their cosegmentation formulation. Unlike ours, however, it requires a reasonable initialization by an objectness measure @cite , and does not scale well with a large number of segments and images.
- As mentioned in section domain adaptation is widely studied in many fields including Natural Language Processing, Speech Processing and Computer Vision @cite @cite @cite . A survey on recent advances in domain adaptation in natural language processing and computer vision can be found in @cite @cite @cite . Subspace based approaches are most popular for solving the visual domain shift problem @cite @cite @cite . The same principal lies behind these approaches. They first determine separate subspaces for source and target data and then project the data onto these subspaces and or a set of intermediate sampled subspaces with the aim of making the feature point domain invariant. In @cite , a method is proposed to sample subspaces along the geodesic between source and target subspace on the Grassmann manifold. Once sampling is done then features are projected onto those sampled subspaces and a classifier is trained on the projected features. In @cite , the geodesic flow kernel is proposed to capture the incremental details in subspaces between source and target subspace along the geodesic. Instead of using intermediate subspaces, @cite proposes to learn a transformation to directly align the source subspace to the target subspace.
- Only few works have looked at the use of hierarchies in the context of domain adaptation. In @cite , Nguyen propose to adapt a hierarchy of features to exploit the richness of visual data. The intent behind this work is similar to our work, in that semantic closeness and context information are exploited to boost domain adaptation performance. Taking this idea forward a recent work on hierarchical adaptive structural SVM for domain adaptation has been proposed in @cite . They organize multiple target domains into a hierarchical structure (tree) and adapt the source model to them jointly. Others have used statistical methods for hierarchical domain adaptation, e.g. in @cite a hierarchical Bayesian prior is used to solve the domain shift problem in natural language and speech processing. However, the previous works have assumed a single common subspace between source and target, while our approach makes use of the hierarchical structure among the different classes to learn separate subspaces.
- Recently the most popular graph crawling is random walk-based sampling, including simple random walk with re-weighting (RWRW) @cite @cite and Metropolis-Hastings random walk (MHRW) @cite . RWRW is considered as a special case of Respondent-Driven Sampling (RDS) @cite if only one neighbor is chosen in each iteration and revisiting nodes is allowed. It is also biased to sample high degree nodes, but the bias can be corrected by the Hansen-Hurwitz estimator shown in @cite @cite . RWRW was not only used to sample OSNs @cite @cite , but also P2P networks and Web @cite @cite . MHRW is based on the Metropolis-Hastings (MH) algorithm and provides unbiased samples directly @cite @cite . Some studies @cite @cite have shown that RWRW estimates are more accurate than MHRW estimates.
- . Researchers have proposed some methods to improve the sampling efficiency against random walk-based sampling, including the FS @cite and RWuR @cite methods which we apply as showcases in this work. Besides, @cite presented a weighted random walk method to perform stratified sampling with a priori estimate of network information. @cite proposed a non-backtracking random walk which forbids the sampler to backtrack to the previously visited node, and they theoretically guaranteed the technique achieves higher efficiency than a simple random walk. Our work concentrates on how to combine the existing statistics (sampling methods) efficiently and thus is complementary to their approaches.
- It is worth mentioning that, @cite designed a multi-graph sampling technique for the social networks which have multiple relation graphs. Their technique improves the convergence rate of the sampler by walking along a union graph of all relations. But it does not distinguish the efficiencies of walking on different relation graphs. In this paper, we propose the two-stage framework to select an inferred most efficient one from multiple graphs to improve sampling efficiency further.
- Exact probabilistic reasoning has a close connection with propositional logic and weighted model counting @cite @cite @cite @cite . The model counting problem, #SAT, is the problem of computing the number of models for a given propositional formula, i.e., the number of distinct truth assignments of the variables for which the formula evaluates to TRUE . In its weighted version, each boolean variable @math has a weight @math when set to TRUE and a weight @math when set to FALSE . The weight of a truth assignment is the product of the weights of its literals. The weighted model counting problem then asks the sum of the weights of all satisfying truth assignments. There are two important streams of research for exact weighted model counting and exact probabilistic reasoning that relate to SPNs: DPLL-style exhaustive search @cite and those based on , e.g., Binary Decision Diagrams (BDDs), Decomposable Negation Normal Forms (DNNFs) and Arithmetic Circuits (ACs) @cite @cite @cite .
- The SPN, as an inference machine, has a close connection with the broader field of knowledge representation and knowledge compilation. In knowledge compilation, the reasoning process is divided into two phases: an offline compilation phase and an online query-answering phase. In the offline phase, the knowledge base, either propositional theory or belief network, is compiled into some tractable target language. In the online phase, the compiled target model is used to answer a large number of queries efficiently. The key motivation of knowledge compilation is to shift the computation that is common to many queries from the online phase into the offline phase. As an example, ACs have been studied and used extensively in both knowledge representation and probabilistic inference @cite @cite @cite . recently showed that ACs and SPNs can be converted mutually without an exponential blow-up in both time and space. As a direct result, ACs and SPNs share the same expressiveness for probabilistic reasoning.
- Another representation closely related to SPNs in propositional logic and knowledge representation is the deterministic-Decomposable Negation Normal Form (d-DNNF) @cite . Propositional formulas in d-DNNF are represented by a directed acyclic graph (DAG) structure to enable the re-usability of sub-formulas. The terminal nodes of the DAG are literals and the internal nodes are AND or OR operators. Like SPNs, d-DNNF formulas can be queried to answer satisfiability and model counting problems. We refer interested readers to and for more detailed discussions.
- Since their introduction by , SPNs have generated a lot of interest as a tractable class of models for probabilistic inference in machine learning. Discriminative learning techniques for SPNs have been proposed and applied to image classification @cite . Later, automatic structure learning algorithms were developed to build tree-structured SPNs directly from data @cite @cite @cite @cite . SPNs have also been applied to various fields and have generated promising results, including activity modeling @cite , speech modeling @cite and language modeling @cite . Theoretical work investigating the influence of the depth of SPNs on expressiveness exists @cite , but is quite limited. As discussed later, our results reinforce previous theoretical results about the depth of SPNs and provide further insights about the structure of SPNs by examining the structure of equivalent BNs.
- Given these models, there are several types of matrix sketches. We describe them here with a bit more specificity than in the Introduction, with particular attention to those that can operate in the row-update model we focus on. Specific exemplars are described which are used in out empirical study to follow. The first approach is to the matrix @cite @cite @cite , by retaining a small number of non-zero. These algorithms typically assume to know the @math dimensions of @math , and are thus not directly applicable in out model.
- Hashing : A variant of this approach @cite uses an extra sign-hash function to replicate the count-sketch @cite with matrix rows (analogously to how does with the MG sketch). Specifically, the sketch @math is initialized as the @math all zeros matrix, then each row @math of @math is added to row @math as @math , where @math and @math are perfect hash functions. There is no harm in assuming such functions exist since complete randomness is na " vely possible without dominating either space or running time. This method is often used in practice by the machine learning community and is referred to as feature hashing" @cite . Surprising new analysis of this method @cite shows this approach is optimal for construction bounds and takes @math processing time for any matrix @math .
- It is worth noting that under the construction model, and allowing streaming turnstile updates to each element of the matrix, the hashing algorithm has been shown space optimal by Clarkson and Woodruff @cite (assuming each matrix entry requires @math bits, and otherwise off by only a @math factor). It is randomized and it constructs a decomposition of a rank @math matrix @math that satisfies @math , with probability at least @math . This provides a relative error construction bound of size @math bits. They also show an @math bits lower bound. Our paper shows that the row-update model is strictly easier with a lower upper bound.
- Although not explicitly described in their paper @cite , one can directly use their techniques and analysis to achieve a weak form of a non-construction projection bound. One maintains a matrix @math with @math columns where @math is a @math matrix where each entry is chosen from @math at random. Then setting @math , achieves a @math , however @math is rank @math and hence that is the only bound on @math as well.
- We discuss related work in two main categories: distributed gradient descent optimization and parallel online aggregation. We emphasize the novelty brought by this work when compared to our previous papers on parallel online aggregation @cite @cite and incremental gradient descent in GLADE @cite @cite .
- * Distributed gradient descent optimization. There is a plethora of work on distributed gradient descent algorithms published in machine learning @cite @cite @cite @cite . All these algorithms are similar in that a certain amount of model updates are performed at each node, followed by the transfer of the partial models across nodes. The differences lie in how the model communication is done @cite @cite and how the model is partitioned for specific tasks, e.g., neural networks @cite and matrix factorization @cite . Many of the distributed solutions are immediately applicable to multi-core shared memory environments. The work of R 'e et al @cite @cite @cite is representative in this sense. Our work is different from all these approaches because we consider concurrent evaluation of multiple step sizes and we use adaptive intra-iteration approximation to detect convergence. Moreover, IGD is taken by default to be the optimal gradient descent method, while BGD is hardly ever considered. We provide a thorough comparison between BGD and IGD, and show that -- with our optimizations -- BGD always outperforms IGD.
- Many Big Data analytics systems and frameworks implement gradient descent optimization. Most of them target distributed applications on top of the Hadoop MapReduce framework, e.g., Mahout @cite , MLlib @cite , while others provide complete stacks, e.g., MADlib @cite , Distributed GraphLab @cite , and Vowpal Wabbit @cite . With no exception, IGD is the only method implemented in all these systems. As a first step, the techniques we present in this paper can be incorporated into any of these systems, as long as multi-threading parallelism and partial aggregation are supported. More important, we provide strong evidence that BGD deserves full consideration in any Big Data analytics system.
- * GLADE PF-OLA. The novelty of our work compared to the PF-OLA framework @cite @cite comes from applying online aggregation estimators to complex analytics, rather than focusing on standard SQL aggregates---the case in previous literature. We are the first to model gradient descent optimization as an aggregation problem. This allows us to design multiple concurrent estimators and to define halting mechanisms that stop the execution when model update and loss computation are overlapped. Moreover, the integration of online aggregation with speculative step evaluation allows for early identification of sub-optimal step sizes and directs the system resources toward the promising configurations. None of the existing systems, including GLADE PF-OLA, support concurrent hyper-parameter evaluation or concurrent estimators. Our previous work on gradient descent optimization in GLADE @cite @cite is limited to IGD. In this paper, we also consider BGD and propose general methods applicable to distributed gradient descent optimization.
- To enhance the quality of the Fourier inversion of the sampled data, great effort has been done in applications to improve the reconstruction process by changing the reconstruction basis or adapting the sampling process. For example in @cite the authors introduced which allows the user to directly sample wavelet coefficients instead of Fourier coefficients by adapting the acquisition device. However, as for now this method also has several shortcomings such as low signal-to-noise ratio @cite . On the other hand, the proposed method GS can be understood as a post processing method that does not require to change the acquisition device since the sparsifying transform will be incorporated in the reconstruction process and not in the sampling process.
- The use of GS to study the reconstruction problem from Fourier measurements has already been investigated for wavelets in 1D @cite and 2D @cite . The authors of the respective works showed, that for dyadic multiresolution analysis (MRA) wavelets the stable sampling rate is linear, meaning up to a constant one has to sample as many Fourier coefficients as one wants to reconstruct wavelet coefficients. Moreover, it was shown in @cite that a defiance of this rate leads to unstable approximations.
- As for practical applications such as MRI, wavelets are often used as a reconstructions system and it has been shown that these often lead to superior results in terms of lower sampling rates and reduced artifacts @cite . We will provide some numerical tests with MR data and deterministic subsampling patterns in the numerics section using shearlets and compare these to reconstruction methods based on wavelets and Fourier inversion to show the strength of using shearlets as a reconstruction system for the recovery problem from Fourier measurements.
- It has been shown @cite that the set of persistence diagrams with the Wasserstein distance is a complete and separable metric space, and thus provides a suitable setting for probability and statistics. Unfortunately the Fr 'echet mean is not necessarily unique. For a slightly adjusted metric, there is an algorithm @cite that converges to an element of the Fr 'echet mean set, though it does not have good computational properties. The discontinuity of this procedure can be remedied by using a probabilistic approach @cite .
- Persistence diagrams can be used for statistical inference. Hypothesis testing using persistence diagrams has been considered for brain MRI data in @cite and more abstractly in @cite . Furthermore, confidence sets for persistence diagrams have been obtained in @cite .
- The persistence landscape allows the use of more statistical machinery. The bootstrap has been applied to obtain confidence bands for the persistence landscape @cite and the average persistence landscape of subsamples has been studied @cite . The persistence landscape has been used to study protein binding @cite and as a kernel for topological machine learning and compared to the recent multi-scale kernel for persistence diagrams @cite .
- Relevance feedback @cite @cite @cite has been shown to be an effective way to help retrieval systems improve retrieval performance. Besides the commonly used relevance feedback mechanism in which users are asked to judge whether a document is relevant or not, there has been some work on soliciting user feedback on document features. The term-based feedback mechanism, in which users are asked to identify relevant terms, has been studied by several researchers @cite @cite @cite @cite @cite . Recently, faceted feedback has been proposed for users to identify suitable faceted constraints on semi-structured documents to help improve retrieval performance @cite @cite .
- This paper differs from the prior work by focusing on a different task: adaptive information filtering. Some of the techniques we tried in this paper are motivated by the prior work. The new user profile learning algorithm proposed in this paper is motivated by @cite , however, with significant differences. First, our algorithm is designed to incorporate two types of user feedback, that is, to learn from labeled instances and features simultaneously in order to fit the filtering task where users may provide mixed types of feedback. In our algorithm, we use a unified loss function to combine user feedback on both instances and features. Secondly, our model is designed to capture the sufficiency and necessity of user-labeled features. The assumption of our model is users can identify important features and an important feature should have a high correlation with the document label. To measure this correlation, we propose the concepts of sufficiency and necessity and explicitly capture them in our algorithm.
- Many ideas in this paper are inspired by previous work on Random Camera @cite . However, the key difference between our paper and the previous work is that in @cite no lens is used and hence all the lights from all directions in the lightfield get mixed up which is difficult to invert. In our setup, we place a lens between the world and the camera sensor, which makes the problem significantly easier and more tractable to solve. Also similar ideas appears in the single pixel camera'' @cite where measurements of the light are randomized for compressed sensing.
- The idea that some everyday objects can accidentally serve as a camera has been explored before. It is in shown in @cite that an photograph of a human eye reflects the environment in front of the eye, and this can be used for relighting. In addition, a window or a door can act like a pinhole, in effect imaging the world outside the opening @cite .
- Modifying graphs and comparing the outcome with the respective source graph is a common approach to tackle a variety of questions. In the field of social network analysis, networks are modified in order to simulate measurement errors and to examine the robustness of centrality measures. An empirical network was altered by Bolland @cite and Pearson correlation was used to measure the robustness. Random samples have been taken from empirical networks by Costenbader and Valente @cite to investigate the stability of various centrality measures. Four error types have been applied to Erd o s-R 'enyi graphs by @cite to measure centrality robustness regarding different types of accuracy measures. Six types of measurement errors have been applied to real-world and generated networks by @cite in order to examine the robustness of node-level network measures by means of Spearman's rho.
- Albert and Barabsi @cite examined the error and attack tolerance of random graphs with respect to node removal. Based on centrality measures, @cite removed nodes and edges from real-world networks and random graphs to investigate the attack vulnerability regarding the average inverse of the geodesic length and the size of the largest connected subgraph. The behavior of very large social networks and web graphs with respect to vertex removal based on various removal strategies and comparison methods based on the neighborhood function has been studied by @cite . @cite measured the impact of random errors to real-world networks and generated graphs by means of stochastic quantifiers.
- It has been shown before ( @cite ) that joint training is important for the success of part-based models in object detection. Differently from them, however, we share parts among multiple classes and define a joint optimization in which multiple classifiers are learned concurrently. In particular, the same part can vote strongly for a subset of the classes and against another subset. The most closely related work to ours is ( @cite ). Their model has two sets of parameters; a dictionary of visual words @math and a set of weights @math that specifies the importance the visual words in each category. Similar to what we do here, @cite trains @math and @math jointly (visual words would be the equivalent of part filters in our terminology). However, they assume that @math is non-negative. This assumption does not allow for negative parts'' as we describe in Section .
- There are several approaches to the problem of sampling graphs with a given degree sequence, though none is known to be efficient for all degree sequences. The configuration model of Bollob ' a s @cite gives expected polynomial time uniform sampling if @math . McKay and Wormald @cite adapted the configuration model to give an algorithm which performs uniform sampling from @math in expected polynomial time when @math .
- Jerrum and Sinclair @cite used a construction of Tutte's to reduce the problem of sampling from @math to the problem of sampling perfect matchings from an auxilliary graph. The resulting Markov chain algorithm is rapidly mixing if the degree sequence @math is : see @cite . Stable sequences are those in which small local changes to the degree sequences do not greatly affect the size of @math . Many degree sequences which satisfy the conditions of Theorem are stable; however, not all stable sequences satisfy the conditions of Theorem . (For example, if @math and @math then @math is stable @cite but then @math , which is not large enough for Theorem .)
- Steger and Wormald @cite gave an easily-implementable algorithm for sampling regular graphs, and proved that their algorithm performs asymptotically uniform sampling in polynomial time when @math (where @math denotes the degree). Kim and Vu @cite gave a sharper analysis and established that @math suffices for efficient asymptotically uniform sampling. Bayati, Kim and Saberi @cite extended Steger and Wormald's algorithm to irregular degree sequences, giving polynomial-time asymptotically uniform sampling when @math . From this they constructed a sequential importance sampling algorithm for @math . Recently, Zhao @cite described and analysed a similar approach to that of @cite , in a general combinatorial setting. Zhao shows that for sampling from @math , when @math , his algorithm performs asymptotically uniform sampling in time @math .
- Finally we note that Barvinok and Hartigan @cite showed that the adjacency matrix of a random element of @math is close'' to a certain maximum entropy matrix'', when the degree sequence is . The definition of tame depends on the maximum entropy matrix, but a sufficient condition is that @math and @math for some constants @math . Some degree sequences satisfying this latter condition are stable sequences, and many of these degree sequences also satisfy the condition of Theorem . It would be interesting to explore further the connections between stable degree sequences, tame degree sequences and the mixing rate of the switch Markov chain.
- It is not known whether the corresponding counting problem (exact evaluation of @math ) is @math -complete. There are several results giving asymptotic enumeration formulae for @math under various conditions on @math : see for example @cite @cite @cite and references therein.
- Almazan al @cite further explore the notion of word embeddings, creating a joint embedding space for word images and representations of word strings. This is extended in @cite where Gordo makes explicit use of character level training data to learn mid-level features. This results in performance on par with @cite but using only a small fraction of the amount of training data.
- Many low rank approximation schemes are available, including QR decomposition, Independent Component Analysis, truncated Singular Value Decomposition, and Non-negative Matrix Factorization @cite .
- While these basic methods are unsupervised, there is a growing interest in incorporating prior knowledge or user guidance into these frameworks @cite . For example, in semi-supervised clustering applications, user guidance is often given by partial labeling information, which can be incorporated using hard constraints @cite @cite @cite . Typical constraints used in this case are Must-Link and Cannot-Link, enforcing that two data points must or cannot be in the same cluster, respectively. For example, @cite present an integrated framework for clustering non-homogenous data, and show how to turn Must-Link and Cannot-Link constraints into dependent and disparate clustering problems. Recently, researchers have also considered interactive matrix factorization schemes for topic modeling that can take into account user feedback on the quality of the decomposition @cite (topic refinement, merging or splitting) and semi-supervised NMF with label information as constraints @cite . Constraint clustering @cite is another example of this approach. Alternatively, regularizations or penalty terms are also used to obtain solutions with certain desired properties such as sparsity @cite @cite , convexity @cite , temporal structural properties and shift invariances @cite @cite .
- Over the years, two different types of approximate MRF energy minimization methods have been proposed. The first class of such methods consists of move-making techniques that were inspired by the success of the graph cut algorithm at solving binary problems in computer vision. These techniques include @math -expansion, @math - @math swap @cite and multi-label moves @cite @cite @cite . The core idea of these methods is to reduce the original multi-label problem to a sequence of binary graph cut problems. Each graph cut problem can then be solved either optimally by the max-flow algorithm @cite if the resulting binary energy is submodular, or approximately via a roof dual technique @cite otherwise. The second type of approximate energy minimization methods consists of message passing algorithms, such as belief propagation (BP) @cite , tree-reweighted message passing (TRW) @cite @cite and the dual decomposition-based approach of @cite , which TRW is a special case of.
- As mentioned above, our algorithm is inspired by the IRLS method. Recently, several methods similarly motivated by the IRLS have been proposed to minimize different objective functions. For instance, in @cite , the @math norm (for @math ) was minimized by iteratively minimizing a weighted @math cost function. @cite , an iterated @math algorithm was introduced to optimize non-convex functions that are the sum of convex data terms and concave smoothness terms. More recently, a general formulation (not restricted to weighted @math or @math minimization) was studied together with the conditions under which such iteratively reweighted algorithms ensure the cost to decrease @cite . In the next section, we propose an extension of this formulation that will later allow us to tackle the case of multi-label MRFs.
- So far, the problem of justifying answer sets has not received much attention, even though the need for justifications has been expressed @cite . According to @cite , a justification should provide only the information that are relevant to the item being explained'', making it easier understandable. We incorporate this in ABAS Justifications by not using the whole derivation of a literal, but only the underlying facts and NAF literals necessary to derive the literal in question.
- The two approaches for justifying why a literal is or is not part of an answer set which are most related to ABAS Justifications are Argumentation-Based Answer Set Justifications and off-line justifications. @cite are a predecessor'' of ABAS Justifications using the ASPIC+ argumentation framework @cite instead of ABA. @cite explain why a literal is or is not part of an answer set by making use of the well-founded model semantics for logic programs. In the following Sections and , we look at these two related approaches in more detail and compare them to ABAS Justifications. In , we look at a number of other, less closely related explanation approaches.
- An off-line justification graph includes all intermediate literals in the derivation of the literal in question. However, following @cite we argue that for a justification it is sufficient to include the most basic relevant literals, without considering intermediate steps. Especially in the case of large logic programs, where derivations include many steps, an off-line justification will be a large graph with many positive and negative dependency relations, which is hard to understand for humans. In contrast, an ABAS Justification only contains the basic underlying literals, i.e. facts and NAF literals necessary to derive the literal in question, making the justification clearer. However, if the intermediate steps were required, they could be easily extracted from the arguments in the Attack Trees underlying an ABAS Justification.
- Argumentation-Based Answer Set Justification @cite is the first work that applies argumentation theory to answer set programming, and in particular for the justification of answer sets. There, the ASPIC+ argumentation framework @cite is used instead of ABA.
- Most closely related is the concurrent work of Girshick al @cite , who also combine a DPM with ConvNet features in a model called DeepPyramid DPM (DP-DPM). Their work, however, is limited to integrating fixed pretrained ConvNet features with a DPM. We independently corroborate the conclusion that using ConvNet features in place of HoG greatly boosts the performance of DPMs. Furthermore, we show how using a post-NMS online training loss improves response ordering and addresses errors from the NMS stage. We also perform joint end-to-end training of the entire system.
- The basic building blocks of our model architecture come from the DPMs of Felzenszwalb al @cite and Zhu al @cite @cite , and the ConvNet of Krizhevsky al @cite . We make crucial modifications in their integration that enables the resulting model to achieve competitive object detection performance. In particular, we develop ways to transfer the ConvNet from classification to the detection environment, as well as changes to the learning procedure to enable joint training of all parts.
- Some recent works have applied ConvNets to object detection directly: Sermanet al @cite train a network to regress on object bounding box coordinates at different strides and scales, then merge predicted boxes across the image. Szegedy al @cite regress to a bitmap with the bounding box target, which they then apply to strided windows. Both of these approaches directly regress to the bounding box from the convolutional network features, potentially ignoring many important spatial relationships. By contrast, we use the ConvNet features as input to a DPM. In this way, we can include a model of the spatial relationships between object parts.
- In the R-CNN model, Girshick al @cite take a different approach in the use of ConvNets. Instead of integrating a location regressor into the network, they instead produce candidate region proposals with a separate mechanism, then use the ConvNet to classify each region. However, this explicitly resizes each region to the classifier field of view (fixed size), performing significant distortions to the input, and requires the entire network stack to be recomputed for each region. Instead, our integration runs the features in a convolutional bottom-up fashion over the whole image, preserving the true aspect ratios and requiring only one computational pass.
- End-to-end training of a multi-class detector and post-processing has also been discussed in Desai al @cite . Their approach reformulates NMS as a contextual relationship between locations. They replace NMS, which removes duplicate detections, with a greedy search that adds detection results using an object class-pairs context model. Whereas their system handles interactions between different types of objects, our system integrates NMS in a way that creates an ordering of results both of different classes and the same class but different views. In addition, we further integrate this into a full end-to-end system including ConvNet feature generation.
- Cost-sharing connection games were introduced by @cite , and have been widely studied since. The fair cost-sharing mechanism has been studied in @cite . A key property of fair cost-sharing games is the fact that they admit an exact potential function and thus possess a pure strategy NE @cite @cite . Settings with coalitions in congestion games have been studied by Holzman and Law-Yone @cite @cite , and @cite studied the existence and quality of SE in cost-sharing games. The characterization of network topologies that admit SE in congestion games with monotone cost functions has been later established in @cite . The PoA measure has been introduced by Koutsoupias and Papadimitriou @cite to study the quality of NE in games. The analogue of the PoA with respect to SE (called the strong PoA) was introduced and analyzed by @cite . The SPoA in cost-sharing games was studied in @cite @cite , and was also studied with respect to various additional settings; see, e.g., @cite @cite . The consideration of capacities in cost-sharing games was first suggested by Feldman and Ron @cite .
- In @cite a multi-scale structure was developed to tackle random embedding problems which can be recast in co-ordinate percolation framework. As a corollary of a general embedding theorem, it was proved there that an i.i.d. Bernoulli sequence can almost surely be embedded into another in a Lipschitz manner provided that the Lipschitz constant is sufficiently large. It also led to a proof of rough isometry of two one-dimensional Poisson processes as well as a new proof of Winkler's compatible sequence problem. In this work we build upon the methods of @cite , using a similar multi-scale structure, but with crucial adaptations. An earlier proof of Theorem 1 appeared in @cite with a very difficult multi-scale argument. Our proof is different and we believe gives a clearer inductive structure. We also believe that our proof can be adapted to deal with this problem on several other graphs, as well as in the case where there are multiple random walks.
- Thirunarayan and Immaneni in @cite also developed a hybrid query language to unify web of data and web of documents, This approach improves both: 1) information retrieval from Semantic Web through keyword-based search and 2) semantic search of hyperlinked web documents through the exploitation of inheritance hierarchy. Their lucene-based SITAR (Semantic InformaTion Analysis and Retrieval) system provided enhanced retrieval from combined data sources such as AIFB SEAL. SITAR contains information about researchers that combines both structured and unstructured data.
- Lei et. al. @cite developed a semantic search engine called SemSearch, which is another ontology-driven system for keyword-based search over documents. However, SemSearch provides flexibility in query interpretation by also providing search support using Lucene, for keywords not present in the ontology. In the case of complex queries that contain multiple keywords, facts from the ontology are used as templates for query interpretation, similar to the approach in KIM @cite .
- Recently, Convolutional Neural Networks (CNNs) have been shown to perform well in a variety of vision tasks with millions of annotated training images and thousands of categories, including classification @cite , detection @cite and segmentation @cite . Notably, Krizhevsky @cite and Szegedy @cite achieved great progress in the classification task with large and deep supervised CNN training. Girshick @cite proposed to fine-tune the pre-trained Krizhevsky's network with the PASCAL VOC dataset and achieved the state-of-the-art object detection performance. However, the large performance increase achieved by these methods is only possible due to massive efforts on manually annotating millions of images which can provide good coverage of the space of possible appearance variations.
- Our learning framework is partially similar to the one-shot learning @cite which learns visual object classifiers by using very few samples. Most of the one-shot learning methods are based on the feature representation transfer @cite , similar geometric context @cite or cross-modal knowledge transfer @cite . However, their performance is far from that of the state-of-the-art object classifiers. By continuously learning from video context, our framework can achieve the state-of-the-art detection results.
- Another related learning pipeline is self-paced learning @cite which learns first from easy samples and then from complex ones. Various vision applications based on self-paced learning have been proposed very recently @cite @cite . For example, Tang @cite adapted object detectors learned from images to videos by starting with easy samples. Jiang @cite addressed the data diversity. Our method is a slightly-supervised self-paced learning framework, where very few samples are used as seeds and more instances are iteratively accumulated and learned.
- Topic models such as Latent Dirichlet Allocation (LDA) @cite and PLSA (Probabilistic Latent Semantic Analysis) @cite have been widely used in many different fields such as social analysis @cite @cite @cite , event detection @cite @cite , text classification @cite @cite or facet mining @cite @cite . for their ability in discovering topics latent in the document collection. Standard topic models suffer the disadvantage that researchers only use word co-appearance frequency for topic modeling without considering word correlations. However similar or synonymous two terms are, if they do not co-appear in the document, they can hardly be classified into the same topic. This disadvantage is largely magnified in multi-lingual topic modeling where different languages never co-occur with each other.
- SGD- and ALS-based matrix factorization techniques for recommender systems have been extensively studied in the context of the Netflix Prize @cite @cite . These techniques have been extended to work on implicit feedback data @cite and to optimize metrics different from RMSE @cite @cite . A large body of work has been conducted with respect to parallelizing and distributing matrix factorization. This includes work on the scalability of the algorithms itself, e.g. by introducing biased sampling for SGD to avoid conflicting updates during concurrent execution @cite @cite @cite or by proving convergence under a minor amount of update conflicts @cite . Furthermore, distributed implementations have been proposed and evaluated on MapReduce-based @cite , graph-parallel @cite @cite and specialized systems @cite .
- One of the earliest studies to demonstrate that scale free networks are more robust against random failures was conducted by @cite . The authors also discuss the vulnerability of scale free networks to targeted attacks. Cohen @cite @cite study the resilience of internet under random and targeted attacks on nodes. For the case of random attacks, they conclude that even after $100 Focusing on the random failure of nodes and edges, although previous researchers had predicted completely different behavior for Poisson and power law networks, in practice the differences, are vital but not huge. Our results re-enforce these results specially for the case of social networks. The authors also invalidate the explanation that targeted attacks are very efficient on power-law networks because they remove many links, random removal of as many links also result in breakdown of the network. Networks with Poisson degree distribution behave similarly in case of random node failures and targeted attacks, it must be noted that their threshold is significantly lower in the second case. This goes against the often claimed assumption that, because all nodes have almost the same degree in a Poisson network, there is little difference between random node failures and targeted attacks.
- Resilience has not been extensively studied for social networks. Moreover, studies focus on networks that are either only scale free or their sizes are not comparable to online social networks readily available around us. Considering the new findings that deviate with the previous results, we get a strong motivation to further investigate resilience of different types of complex networks with a focus on social networks. Our empirical results reaffirm most of these findings of @cite where our focus is on semantically different social networks.
- Another related work is @cite , in which the optimal arm selection strategy is derived for the infinite time horizon learning problem, when the arm rewards are parametrized with known priors, and the future rewards are discounted. However, in the Gittins' formulation of the MAB problem, the parameters of the arms are different from each other, and the discounting allows the learner to efficiently solve the optimization problem related to arm selection by decoupling the joint optimization problem into @math individual optimization problems - one for each arm. In contrast, we do not assume known priors, and the learner in our case does not solve an optimization problem but rather learns the global parameter through its reward observations.
- Another seemingly related learning scenario is the experts setting @cite , where after an arm is chosen, the rewards of all arms are observed and their estimated rewards is updated. Hence, there is no tradeoff between exploration and exploitation and finite regret bounds can be achieved in an expert system with finite number of arms and stochastic arm rewards. However, unlike in the expert setting, the GMABs achieve finite regret bounds while observing the reward of the selected arm. Hence, the arm reward estimation procedure in GMABs requires forming reward estimates by collectively considering the observed rewards from all the arms, which is completely different than in the expert systems, in which the expected reward of an arm is estimated only by using the past reward observations from that arm.
- The nonlinearity of @math makes the bilateral filter computationally expensive in its standard form. However, they remain attractive as a number of works have been dedicated to accelerate them. Paris and Durand derived criteria for downsampling in space and intensity to come up with a fast approximation of the bilateral filter @cite . A constant-time algorithm for fast bilateral filtering has been proposed in @cite @cite . achieved substantial acceleration at the cost of quantization @cite .
- Modifications of the bilateral filters have found widespread use in a number of image processing tasks such as denoising @cite , illumination compensation @cite , optical-flow estimation @cite , demoaiscking @cite , edge detection @cite , etc.
- Considerable work has also been done on optimizing the parameters of the bilateral filter for improving denoising performance. Peng and Rao used Stein's unbiased risk estimate (SURE) to find the optimal parameters of the Gaussian bilateral filter @cite @cite . Kishan and Seelamantula achieved this goal for a bilateral filter with a raised cosine range kernel @cite . Chen and Shu used Chi-square unbiased risk estimate (CURE) for optimizing bilateral filter parameters in squared magnitude MR images @cite @cite .
- A large body of research on stress detection focused on physiological measurements to infer stress levels (see @cite , @cite , @cite ). Heart-rate variability, galvanic skin response, respiration, muscle activity and temperature are among the most relevant features. However, despite providing reliable insights on stress levels, this approach has major limitations because it comprises wearable sensors that need to be carried at all times to allow for continuous monitoring.
- Among the different changes in physiological parameters that happen during stressful situations, variation in speech production has inspired a number of studies using acoustic sensing on smartphones. Research on stress detection based on voice analysis considered different speech characteristics such as pitch, glottal pulse, spectral slope and phonetic variations. For example, Lu and colleagues @cite proposed StressSense, an Android application for stress detection from human voice in real-life conversation, and they achieved 81 .
- However, these methods depend on sound quality, which is not granted in natural settings (e.g., crowded public places, noisy outdoor), and the correlation between speech and emotion is subjected to large individual differences @cite . Hence, our performance of 72.28 to stress detection. Other studies focused on the video analysis of behavioural correlates of psychological stress @cite . These systems, despite providing an unobtrusive method for stress monitoring, cannot be employed in a large variety of real world and mobile environments and pose privacy concerns related to the recording of people's behaviour.
- A promising approach that can overcome the major shortcomings of stress detection based on physiological measures and on audio video analysis is activity recognition from smartphone usage patterns. Studies in this field have been mainly focused on the understanding of relational dynamics of individuals @cite . Recently studies have started to investigate how smartphone usage habits can provide insights into users' affective state @cite and stress levels @cite . LiKamWa and colleagues @cite proposed MoodScope, a mobile software system that recognizes the users' mood, but not stress states, from smartphone usage analysis. They collected usage data and self-reported mood in a two months longitudinal study and used them to train mood models. Smartphone usage data consisted in phone calls, SMSes, e-mail messages, application use, web browsing histories and location changes, while self-reported mood was collected from users' input at least four times a day. MoodScope reached a 66 calls and categorized applications as the most useful features for mood discrimination.
- Bauer and Lukowicz @cite focused on mid-term stress detection, monitoring 7 students during a two week exam session followed by two weeks of non-stressful period. The recorded data were related to participants' mobility patterns and social interactions, and included users' location, Bluetooth proximity, phone calls and SMSes. These features allowed to detect an average behaviour modification of 53 of this study is the small number of subjects. Our multifactorial approach outperforms the approach proposed by @cite although a direct comparison may be not adequate given the different focus: our approach tend to daily classify people as "not stressed" or "stressed", while Bauer and Lukowicz try to detect stressful situations.
- In 2013, Sano and Picard @cite reported an accuracy performance in stress recognition of 75 phones and wearable sensors. However, the limited number of subjects used in their experiments (18) and the limited number of days (5) make preliminary the results of this study.
- Other work has examined classification with multiple labels, although the labels are generally not considered to be dependent on each other and multiple correct labels are not considered. Multi-label classification considers problems with multiple outputs, but no dependency between the outputs is modeled. Tsoumakas al @cite give an overview of multi-label classification. They define two main approaches for multi-label classification. The first approach is problem transformation, where the given data is transformed into a single problem that already has a well defined solution. The second approach is algorithm adaptation, where current algorithms are modified to solve the multi-label classification problem. Recent work has looked at correlations between labels in multi-label classification to improve accuracy @cite . Read @cite @cite introduced chain classifiers for supporting these correlations which could be viable for supporting MOD problems.
- While MOD learning is relation approximation, this should not be confused with relational learning. Statistical Relational Learning @cite @cite and Multi-Relational Learning @cite both handle relational data, not relation approximation. These relational learning models learn a function from relational data and handle specially formatted and structured data.
- Palatucci al @cite showed that the attribute description of an instance or category is useful as a semantically meaningful intermediate representation to bridge the gap between low level features and high-level classes. Thus, the attributes facilitate transfer and zero-shot learning to alleviate issues of the lack of labeled training data, by expressing classes in terms of well-known attributes. This is followed by Lampert al @cite @cite that extended the work to animal categorization by introducing Direct Attributes Prediction (DAP) and Indirect Attributes Prediction (IAP).
- Unlike @cite @cite @cite , Parikh and Grauman @cite introduced relative attributes to perform zero-shot learning. This approach captures the relationships between images and objects in terms of human-nameable visual properties. For example, the models capture that animal @math is taller' than animal @math , or subject @math is happier' than subject @math . This allows a richer language of supervision and description than the commonly used categorical (binary) attributes. Though relative attributes seem efficient for zero-shot learning, the dataset needs to be intra-class ( the images in the dataset must belong to a set of object classes that are visually similar). Also, a binary or relative relationship between all classes needs to be defined beforehand. Such a process will require extensive human supervision efforts and the decision is always subjective.
- Unsupervised learning of log-linear models has been widely used in natural language processing, including word segmentation , morphological segmentation , POS tagging , grammar induction , and word alignment . The contrastive estimation (CE) approach proposed by @cite is in spirit most close to our work. CE redefines the partition function as the set of each observed example and its noisy neighbors''. However, it is still intractable to compute the expectations of non-local features. In contrast, our approach cancels out the partition function and introduces top- @math sampling to approximate the expectations of non-local features.
- Contrastive learning has received increasing attention in a variety of fields. @cite proposes contrastive divergence (CD) that compares the data distribution with reconstructions of the data vector generated by a limited number of full Gibbs sampling steps. It is possible to apply CD to unsupervised learning of latent-variable log-linear models and use top- @math sampling to approximate the expectation on posterior distributions within each full Gibbs sampling step. The noise-contrastive estimation (NCE) method casts density estimation, which is a typical unsupervised learning problem, as supervised classification by introducing noisy data. However, a key limitation of NCE is that it cannot be used for models with latent variables that cannot be integrated out analytically. There are also many other efforts in developing contrastive objectives to avoid computing partition functions . Their focus is on choosing assignments to be compared with the observed data and developing sub-objectives that allow for dynamic programming for tractable sub-structures. In this work, we simply remove the partition functions by comparing pairs of observed and noisy examples. Using noisy examples to guide unsupervised learning has also been pursued in deep learning .
- The fact that image measurements are inherently noisy, and hence estimation from them requires statistical methodology, has been recognised by many researchers. Statistical methods for estimating the uncertainty in 3-D, such as finding the Cramer-Rao Lower Bound (CRLB), have previously been investigated @cite @cite @cite , though researchers often transform the measurement or linearise the system before estimating the uncertainty, thereby losing the underlying statistical sensor characteristics in the process. There is a consensus in that the uncertainty in triangulated estimates from stereo cameras, or monocular sequences, is non-Gaussian and not trivially estimated @cite . Furthermore, the importance of fusing the estimates given by several complementary cameras is recognised as an instrumental way of reducing the uncertainty in triangulated estimates @cite @cite .
- The concept of binocular disparity, defined by the difference in the location of an object in two images, arose from research into mammalian visual systems to reflect the horizontal separation of the left and right eyes @cite . Perception of depth is obtained in stereopsis as a consequence of this binocular disparity. The same concept is applied to problems in computer vision for extracting depth information from stereo cameras and researchers have designed algorithms for 3-D estimation from cameras by considering the disparity space as a state space @cite @cite @cite @cite @cite .
- Recent developments in the Sensor Fusion community have enabled practitioners to overcome the computational limitations of combinatorial data association approaches by modelling the system as an integrated multi-object Bayesian estimation problem. A Bayesian solution to the multi-object filtering and estimation problem can be found with Finite Set Statistics (FISST) @cite , a set of mathematical tools developed from point process theory, random finite sets, and stochastic geometry.
- The FISST approach to multi-sensor multi-object tracking has attracted significant international attention in the Sensor Fusion community due to the success of practical implementations of first-moment multi-object approximation filters, known as Probability Hypothesis Density (PHD) filters @cite @cite @cite .
- Camera calibration refers to the estimation of the parameters of the imaging process, such that when two or more views of the same scene are available, the original @math -D scene and its dimensions can be reconstructed by solving an inverse problem. How accurately the original scene can be reconstructed depends on the number of parameters that can be estimated and consequently different calibration methods exist. If some ground-truth knowledge about the scene is provided, e.g., a calibration object with known Euclidean @math -D coordinates, the Euclidean calibration can be performed directly @cite . Alternatively, the so called stratified approach is used @cite , which gradually refines the calibration from projective to Euclidean.
- In practice, a calibration object is not always available and hence the stratified approach, which relies only on the information extracted from the images, is more appropriate. Projective calibration is usually achieved by structure-from-motion techniques @cite which unrealistically assume perfect knowledge of measurement correspondences as an input to the calibration process. This in turn means that such projective calibration implicitly assumes that the estimated correspondences were updated with the correct measurements and the corresponding points are known in at least a certain number of images. The possibility of incorrect data association or correspondence is not considered as such cases are pruned from the input data and similarly, the possibility of incorrect estimation of the number of correspondences is also not considered. As a consequence, useful information is removed from the input data before the calibration process even begins.
- In addition to the use of genetic algorithms @cite , brute force @cite has been proposed to search for a braids of manageable size (up to @math exchanges). Other methods such as the Solovay-Kitaev algorithm @cite @cite @cite provide bounds on the accuracy and length of the braids. However, these methods do not allow the user to tune the balance between the accuracy and the length as pioneered in @cite . The Boltzmann distribution has played an important role in the theoretical analysis of EDAs and other authors works have analyzed the relationship between the function structure and the dependencies in the distribution @cite @cite @cite . Other problems from physics have been previously treated with EDAs. In particular, spin glass models with different types of interactions and topologies have been addressed @cite @cite @cite . Two important differences between the braid problem and the spin glass models that makes it particularly challenging is that its fitness function is multiplicative and that the representation is non binary. In fact the cardinality of the variables can increase with the number of generators.
- Handling semantic concepts to express situations and their associated risk level, the first approach compute the risk using concepts. This approach permits to get the risk of the situation directly from the risk of each of its concepts. The second approach compute the risk using the semantic similarity between the current situation and situations stocked in the system as it is done in @cite , and it comes from the assumption that similar situations have the same risk level. The third approach is computing the risk using the variance of the reward. In this case, we assume that risky situations get very low number of user's clicks. (2) We propose an algorithm called CBIR-R-greedy that include the risk computing in its management of the exr exp trade-off. High exploration (resp. high exploitation) is achieved when the current user situation is "not risky" (resp. "risky");
- Models for opinion dynamics have been studied in a variety of fields, including sociology, physics, computer science, and engineering. The wide range of models @cite can be divided into two broad approaches: dynamical systems and agent-based modeling. Agent-based models in which each social element or entity acts based on actions or positions of others have been studied extensively. Agents take actions to pursue an objective such as social welfare maximization, individual benefit, or learning @cite @cite @cite @cite .
- Bounded-confidence opinion dynamics @cite @cite have been proposed as agent-based dynamics for modeling opinion formation. Unlike previous models for social dynamics where interactions among social agents are governed by an underlying graph (fixed or random, but independent of opinions), bounded confidence models make interactions opinion-dependent. In the Hegselmann-Krause model @cite every agent updates its opinion (modeled as a scalar) by averaging all other opinions within a certain distance (threshold) from its own. This dynamics has been studied extensively via numerical techniques as well as analytically. @cite proved that this dynamics converges in finite time and provided upper and lower bounds for the convergence times in terms of number of agents. @cite and Nedic and Touri @cite studied multi-dimensional (vector) Hegselmann-Krause dynamics.
- Multiple variations of this dynamics and their evolutions have been studied, e.g., effect of different initial conditions @cite , noise in the updates @cite @cite , heterogeneous thresholds among agents @cite , or mediating interactions using an underlying social graph @cite ; Lorenz @cite presents a survey of this line of work. Lorenz @cite also proposed a weighted Hegselmann-Krause dynamics with opinion dependent weights, and Hendrickz @cite studied conditions for order-preservation of opinions in this dynamics. In this work we consider Hegselmann-Krause dynamics with opinion-dependent weighted updates and study convergence time of this dynamics.
- In this paper we interpret the Hegselmann-Krause dynamics in terms of utility optimization. There is a significant body of work on utility-maximization in multi-agent settings. @cite considers a large population linear Quadratic Gaussian (LQG) problem where agents interact via coupled cost. There are other works involving control theoretic, game theoretic and optimization based approaches. @cite presents a survey of recent developments. Here we study an engineering problem where the society follows Hegselmann-Krause dynamics and a strategic entity wants to form opinions in its favor. This problem of opinion modulation modification is analogous to marketing and political campaigns.
- The strategy to migrate traffic from the cellular networks to the free and fast device-to-device networks, so as to mitigate the overloaded cellular network is referred to as mobile data offloading @cite @cite . @cite formulated the problem of offloading data with various sizes and delay tolerances as a submodular function maximization problem. Our work is along the same lines in the sense that users try to rely on device-to-device transmission and avoid generating cellular network traffic as much as possible, thereby achieving data offloading.
- It is helpful to find a unified mathematical model to capture the fundamental natures of node mobility. @cite summarized several common methods of mobility modelling. The most widely used and studied model is the random waypoint model. A host randomly chooses a destination (waypoint) and moves toward the destination in a straight line, with a constant speed randomly selected in @math . After reaching the destination, a host pauses for a random period before moving to the next destination. A recent investigation on the pattern of individual mobility @cite reveals that individual mobility patterns are largly indistinguishable after correcting for differences in travel distances and the inherent anisotropy of each trajectory.
- In this section we outline perspectives on student attrition that have been explored so far in the literature on MOOCs. Much of this work successfully leverages effective feature engineering and advanced statistical methods. However, the biggest limitation of most of these emerging works is that they focus solely on discussion forum behavior or video lecture activity, but do not fuse and take them into account. Some of these works have grown out of research on predicting academic progress of students and identifying students those who are at dropout risk @cite @cite @cite @cite @cite .
- Some prior research has focused on deriving social positioning metrics within discussion forums to understand influencing factors that lead to differently motivated behaviors of students. For example, @cite @cite used aggregate post-reply discussion forum graph per week, with an aim to investigate posting behavior and collaborative aspects of participation through operationalizations of social positioning. However, we work at a much finer granularity in the current study and our focus is on individual student modeling instead. We capture not only forum participation trajectory, but also video lecture viewing activity of every student in their participation week. Modeling the combined interaction footprint as an activity network, allows us to decipher the type of engagement and organization of behavior for each student, which are reflective of attrition.
- Distributed learning methods can be roughly divided into two categories: the decentralized gossip-type algorithms @cite and the algorithms constructed on the map-reduce framework @cite . Gossip algorithms do not require a center node to aggregate the results, but suffer from a high communication cost.
- Recently, several online robust learning algorithms were proposed to process the data in a sequential manner @cite . Online learning methods partially mitigate the scalability issue of robust learning by reducing the memory cost for machines. However, the time complexity of those methods still depends linearly on the sample size, hardly affordable in practice when dealing with ultra-large datasets.
- Most of the works related to the CA identification problem use genetic algorithms as a tool to extract update rule and neighborhood from spatio-temporal patterns produced in CA evolution @cite @cite @cite . In @cite a genetic algorithm was employed to learn probabilistic rules directly from experimental data for two-dimensional binary-state CA. Several methods were proposed that use genetic algorithms to learn CA rules and neighborhood size for image processing tasks @cite @cite .
- Although application of genetic algorithms was a dominant approach to CA identification, some non-genetic techniques are also available. Adamatzky @cite proposed several approaches to extracting rules for different classes of CA. @cite developed a form of hill climbing and backtracking to identify CA rules. In @cite a deterministic sequential floating forward search method was used to select rules of CA. Another approach is based on parameter estimation methods from the field of system identification @cite . A framework for solving the identification problems for both deterministic and probabilistic CA was presented in @cite .
- Chunking was proposed for speedup purposes in @cite . Here only a specific estimator was considered, a certain additive nonparametric regression model.
- Chunked estimation was then investigated in another special case, that of linear regression, in @cite , motivated by memory requirement problems rather than speed. The authors allowed their chunk averaging to be weighted, and they derived the optimal weights. They also showed that certain statistics would be @math distributed under the assumption of normally-distributed populations, and studied the resulting chunked estimators via simulation. However, that work was restricted to linear regression, not general types of estimators, and was not aimed at parallel processing, i.e. speedup.
- @cite developed a variant of CA, essentially what will be called C without the A'' in Section , with the goal of speedup. Some of the same authors did some finite-sample analysis of CA in the linear regression case in @cite .
- @cite developed a bootstrap approach related to CA, based on applying the bootstrap method to a number of small random subsamples of the original data. Since any bootstrap method requires the user to choose the values of hyperparameters''---size and numbers of subsamples---the authors proposed an adaptive method to choose these parameters. They derived some asymptotic results, but also noted that their method will fail in settings in which the ordinary bootstrap fails. On the other hand, they indicated how their method can be used for time series data, something other CA methods have not yet been extended to.
- The main convergence result proved in this paper can be rephrased as follows: In the presence of the log-normal shadowing of sufficiently high variance, the statistics of the propagation loss of a single user with respect to different base stations are essentially invariant with respect to the exact geographic positioning of the base stations, whether regular or not, for a wide class of empirically homogeneous networks. Even in perfectly hexagonal case they appear as though they were realized in a Poisson network model. Brown @cite first observed this by simulation, later confirmed independently by B aszczyszyn and Karray @cite who suggested using classical convergence results of random translations of point processes; see Daley and Vere-Jones [Section 11.4] daleyPPII2008 . This approach was first adapted and applied in this setting by B aszczyszyn, Karray and Keeler @cite , which forms part of the results presented here.
- The fact that an arbitrary network can be approximated (from the point of view of a single user) by a Poisson model is very useful. This latter model has already been extensively studied. In particular, it enjoys the following very useful property, here referred to as propagation invariance , which stems from using a power-law as the path-loss function: The statistics of the propagation loss of a single user with respect to different base stations (which we called the propagation process Also called path-loss process with fading'' in @cite . ) depend on the fading distribution through only one moment. This property, provides considerable tractability and has led to the concept of so-called equivalent networks from the perspective of the typical user; cf @cite , where extensions to heterogeneous networks are also presented.
- This convenient property has been observed independently in physics models @cite @cite and network models @cite @cite @cite ; see @cite and references therein for further details. For example, compare [Corollary 10] equivalence2013 and [Lemma 2] panchenko2007guerra , which effectively both give the same equivalence result for, in our setting, the marked propagation process of the typical user. In the physics context, these invariance results imply Bolthausen-Sznitman invariance property (cf [Eq. (2.26)] panchenko2013sherrington ) for the Poisson-Dirichlet process, which is used to study the Sherrington-Kirkpatrick model for spin glasses (types of disordered magnets). This latter process It should not be confused with the better known Poisson-Dirichlet process of Kingman @cite . In fact both processes are special cases of two-parameter Poisson-Dirichlet process extensively studied in @cite . is exactly the so-called signal-to-total-interference ratio (STIR) process, which in turn is trivially related to the SIR process in (interference-limited) Poisson networks @cite @cite .
- Recently several characteristics related to the propagation process in the Poisson model have been studied. For example, closed and semi-closed expressions for the coverage probability in so-called multi-tier network models @cite @cite @cite @cite @cite , with the concept of @math -coverage being later introduced @cite @cite . More recently, these models have been advanced by studying the coverage probability under signal coordination @cite @cite and interference cancellation schemes @cite @cite , with a recent contribution being the joint probability density of the order statistics of the process formed from the SINR values of the typical user @cite . The recently observed relation between the SINR values and a type of Poisson-Dirichlet process (cf @cite ) can potentially bring some further progress to this subject.
- Jindal and Liu first studied the deceptive opinion problem and trained models using features based on the review text, reviewer, and product to identify duplicate opinions, i.e., opinions that appear more than once in the corpus with similar contexts. (2010) propose an alternative strategy to detect deceptive opinion spam in the absence of a gold standard. Yoo and Gretzel gathered 40 truthful and 42 deceptive hotel reviews and manually compare the linguistic differences between them. created a gold-standard collection by employing Turkers to write fake reviews, and follow-up research was based on their data @cite @cite @cite @cite @cite . For example, looked into syntactic features from Context Free Grammar parse trees to improve the classifier performance. A step further, Feng and Hirst make use of degree of compatibility between the personal experiment and a collection of reference reviews about the same product rather than simple textual features.
- In addition to exploring text or linguistic features in deception, some existing work looks into customers' behavior to identify deception @cite . For example, delved into group behavior to identify group of reviewers who work collaboratively to write fake reviews.
- The process of action recognition can be generally divided into two main steps, action representation and action classification. Action representation consists of feature extraction and feature selection. Features can be extracted from input sources such as depth maps, skeleton and or RGB images. Regardless of the input source, there are two main approaches, space-time approach and sequential approach @cite @cite @cite , to the representation of actions. The space-time approach usually extracts local or holistic features from space-time volume, without explicit modelling of temporal dynamics. By contrast, the sequential approach normally extracts local features from each frame of the input source and models the dynamics explicitly. Action classification is the step of learning a classifier based on action representation and classifying any new observations using the classifier. For space-time approaches, discriminative classifier, such as Support Vector Machine (SVM), is often used for classification. For the sequential approach, generative statistical models, such as Hidden Markov Model (HMM), are commonly used. Our method belongs to the skeleton-based space-time volume approach. In this section, we mainly review the existing work of skeleton-based action representation for action recognition.
- For the skeleton-based sequential approach, @cite proposed a feature called Histograms of 3D Joint Locations (HOJ3D) as a representation of postures. The HOJ3D essentially encodes spatial occupancy information relative to the root joint, e.g. hip centre. A modified spherical coordinate system is defined on the root joint and the 3D space is divided into @math bins. The HOJ3D is reprojected using Linear Discriminant Analysis (LDA) to reduce dimensionality and then clustered into @math posture visual words which represent the prototypical poses of actions. HMMs are adopted to model the visual words and recognize actions. Radial distance is adopted in this spherical coordinate system which makes the method to some extend view-invariant.
- PubMed generates related research for articles in their catalogue. They trained a probabilistic topic-based model to learn article relatedness based on article meta-data (e.g. title, abstract and MeSH terms). In testing on the TREC 2005 genomics track (including MEDLINE abstracts), the probabilistic topic-based model is shown to slightly outperform BM25, with 0.399 vs. 0.383 precision at 5, respectively @cite . By taking a random week long sample of logs, PubMed show that out of two million user sessions, 19
- There is an ample work on computing relatedness between ranked lists of items, such as to mine correlations or anti-correlations between lists ranked by different attributes. Arguably, the two most prominent similarity measures are Kendall's tau and Spearman's Footrule. @cite study comparing incomplete top- @math lists, that is, lists capturing a subset of a global set of items, rendering the lists incomplete in nature. In the scenarios motivating our work, like similarity search favorite preference rankings, lists are naturally incomplete, capturing, e.g., only the top-10 movies of all times. In this work, we focus on the computation of Kendall's Tau distance.
- Helmer and Moerkotte @cite present a study on indexing set-valued attributes as they appear for instance in object-oriented data -bases. Retrieval is done based on the query's items; the result is a set of candidate rankings, for which the distance function can be computed. For metric spaces, data-agnostic structures for indexing objects are known, like the M-tree by @cite @cite ; but Kendall's Tau over incomplete list is not a metric.
- @cite consider matchmaking between users in a dating portal. The attributes considered are scalar (e.g., age, weight, and height) or categorical (e.g., married, smoking, and education) and focus is put on feature selection and learning for effective match making. Work on rank aggregation @cite @cite aims at synthesizing a representative ranking that minimizes the distances to the given rankings, for a given input set of rankings.
- Cascaded regression treats shape estimation as a regression problem. It starts from raw estimates of landmark positions, and learns regressors that map shape dependent features into pose increments iteratively. Examples of cascaded regression method include the approach by Cao al @cite , which employs boosted nonlinear regression with shape dependent pixel difference features. Burgos-Artizzu al @cite builds a cascaded regression model with an occlusion detection and voting strategy to cope with severe occlusion. Xiong and De la Torre @cite address regression by learning generic descent directions, and perform linear mapping on non-linear SIFT features, which achieves state-of-the-art results.
- Detection based approaches detects object parts independently and then estimates pose and or shape directly from the detections @cite @cite or through flexible part models @cite @cite @cite . These methods are effective at detecting and localising articulated objects from multiple views in challenging scenarios. Sun al @cite propose a cascaded deep convolutional network for five-points face alignment. The network detects approximate locations of the landmarks in the lower cascade and refine the estimations in higher cascade. State-of-the-art result is recently achieved by Zhang al @cite @cite using a deep convolutional network trained for facial landmark detection together with heterogeneous but subtly correlated tasks, like head pose estimation and facial attribute inference. The model achieves the state-of-the-art result on the 300-W benchmark dataset (mean error of 9.15
- Torralba and Efros @cite raise an important question: are the datasets deployed for computer vision studies unbiased representations of the visual world? They showed that even large number of training images are employed, an image classification model can still over-fit if it is trained on a single dataset with bias. The over-fitting would severely hamper cross-dataset generalisation. A number of studies focus on undoing this bias by transfer learning @cite @cite @cite or through other means like max-margin based learning framework or subspace alignment method @cite @cite . To our knowledge, our work is among the first studies that investigates the problem of dataset bias in face alignment domain. We wish to show that using existing databases independently for training test would risk a closed-world' evaluation environment. To allow improved cross-dataset generalisation, we devise a novel transductive alignment method to bridge the annotation gap between diverse datasets, which in turn facilitates seamless databases fusion for domain adaptive face alignment.
- It is worth noting that in the recent work by Smith and Zhang @cite , they have independently presented an alternative way to combine multiple face landmark datasets with different landmark definitions into a super dataset.
- In information visualization there are a large variety of techniques for visualizing clusters of objects, some of which simply map objects to (colored) points so that spatial proximity indicates object similarity @cite @cite , others explicitly visualize clusters or general sets as regions in the plane @cite @cite . These approaches are visually similar to Euler diagrams @cite @cite , however, they do not give hard guarantees on the final set layout, e.g., in terms of intersection regions or connectedness of regions, nor do they specifically consider the simultaneous embedding of two or more clusterings or partitions.
- is a concept in graph drawing that combines a planar graph layout with a drawing of the clusters of a single hierarchical clustering. Clusters are represented as regions bounded by simple closed and pairwise crossing-free curves. Such a layout is called if no edge crosses a region boundary more than once @cite .
- The theory of @cite was applied in several contexts and with different types of knowledge, such as First-Order Logic clauses @cite @cite and visual relationships in object recognition @cite . In the case of manifold regularization based constraints @cite , our on-line learning system was evaluated using heterogenous data, showing promising results @cite . Finally, the notion of constraint is used in this paper to model motion coherence, thus resembling what is usually done in optical flow algorithms @cite @cite . Motion estimation in DVAs is part of the feature extraction process; some qualitative results can be found in @cite .
- The notion of defect class'' or fault class'' is really important in the field of fault tolerance and software testing. In fault tolerance, according to foundations of the field @cite , . Indeed, one is tolerant with respect to a certain class of fault. 's elementary dimensions of fault classes @cite provide a coarse-grain framework for characterizing the bugs addressed by an automatic repair approach.
- In software testing, and in particular in the field of mutation testing, a fault class'' or fault model'' describes kinds of programmer mistakes (in a particular language, domain, etc) @cite . Each mutation operator is intended to simulate one of those mistakes. Tolerating bugs, simulating faults, repairing bugs: in all cases, there is a real need to describe the classes of bugs that are handled by a novel technique.
- In the research on self-healing software, which is close to automatic software repair, the need for a fault model has been clearly stated by Koopman @cite : ..
- Having sound evaluation methods is essential for science. In many fields, different fallacies have been described (e.g. @cite in medicine). In software engineering, many authors discussed potential fallacies, such as Glass in his book @cite . Recently, Bird and colleagues @cite have extensively discussed the biases of datasets used in bug predication research. Posnett, Filkov and Devanbu @cite have published a paper on the presence of ecological fallacies in empirical software engineering research (focusing on sample size, zonation, and class imbalance). Both papers discuss the intimate relation between the dataset construction and the conclusiveness of the evaluation.
- Along the two dimensions of state repair and runtime repair, let us now survey important related work. As early as 1980, Taylor and colleagues @cite introduced robust data structures'' which are able to repair their own state at runtime. Demsky and Rinard @cite proposed a similar approach for data structure repair @cite , @cite invented a complex repair strategies for register values and memory locations of x86 binary programs @cite , @cite focused on repairing service-oriented software. Lewis and Whitehead's paper @cite also performs state repair, by runtime modification of the state of event-driven programs.
- On behavioral repair, beyond the now classical work by Weimer and colleagues @cite @cite , there is also earlier (e.g. @cite @cite ) and concurrent work on this topic (e.g. @cite @cite ). Those contributions focus on synthesizing source code to fix bugs, the code being meant to be committed into a version control system. However, behavioral repair is also relevant on binary code @cite . Moreover, as stated above, behavioral repair can also happen at runtime, the application communities of Locasto and colleagues @cite , for instance, share behavioral patches at runtime for fixing faults.
- It is only recently that natural language processing has been actively applied to social media data. The inherent difficulties of this domain have been well explored @cite @cite . There has been increasing interest in improving NLP components for this domain. and suggested a new set of part-of-speech tags tailor made for Twitter. Their tagger achieved improved accuracies on tweets, compared to other existing taggers. Named entity recognition is of particular interest in Social Media. Current state-of-the-art systems perform well on formal texts @cite @cite @cite ; however, the task becomes much harder in the social media domain. adapted many NLP components to Twitter data, including a tagger, a shallow parser, and an entity chunker. For movie entities, their dictionary-based approach outperformed their statistical approach, which implies that a new NER system may need to be designed for this type of entities.
- (2004) @cite classify proposals based on knowledge from network oracles or route computation and determination. A routing evaluation framework is provided considering the different knowledge oracles to evaluate the amount of knowledge each proposal requires in specific application scenarios.
- Zhang (2006) @cite provides a classification with two main categories, deterministic and stochastic, which also considers the type of knowledge used for routing. The main goal is to solely categorize routing solutions based on the information used to perform data exchange.
- Likewise and Zhang, D'Souza and Jose (2010) @cite classify solutions considering the required knowledge. Proposals are divided into three major categories based on flooding, history, and, special devices. It is important to note that this taxonomy succeeds in including the new social-aware routing trend observed in the last three years.
- The most recent classification proposed by (2010) @cite groups opportunistic routing proposals according to the message exchange scheme they employ: forwarding, replication, and coding. Authors also identify different types of utility functions that can be applied to such schemes. Additionally, Disruption Tolerant Networks (DTN) are classified based on characteristics that have major impact on routing such as connectivity and mobility. Authors succeed in mapping routing solutions to the different DTN types, which allows them to evaluate proposals accordingly.
- We observe that the main goal of these classifications is uniquely to identify the different families of routing solutions. Some of them @cite @cite provide evaluation principles that simply aid designers to identify the application requirements in order to propose the right'' algorithm. We, on the other hand, use our classification to identify common aspects among the analysed solutions to propose a fair way to assess routing performance independently of the amount of needed knowledge and application scenario.
- Regarding evaluation models, we highlight a proposal based on theory to design and evaluate least cost routing protocols. (2010) @cite use a formalized metric (i.e., foremost) to determine journeys (i.e., future temporary connections between nodes that may provide a path over time) that quickly reach the destination.
- Aligning with the above procedures, various traffic load balancing algorithms have been proposed to optimize the network utilities. The most practical traffic load balancing approach is the cell range expansion (CRE) technique that biases users' receiving SINRs or data rates from some BSs to prioritize these BSs in associating with users @cite . Owing to the transmit power difference between MBSs and SCBSs, a large bias is usually given to SCBSs to offload users to small cells @cite . By applying CRE, a user associates with the BS from which the user receives the maximum biased SINR or data rate. Although CRE is simple, it is challenging to derive the optimal bias for BSs. Singh @cite provided a comprehensive analysis on traffic load balancing using CRE in HetNet. The authors investigated the selection of the bias value and its impact on the SINR coverage and the downlink rate distribution in HetNet.
- The traffic load balancing problem can also be modeled as an optimization problem and solved by convex optimization approaches. Ye @cite modeled the traffic load balancing problem as a utility maximization problem and developed distributed user association algorithms based on the primal-dual decomposition. Kim @cite proposed an @math -optimal user association algorithm to achieve flow level load balancing under spatially heterogeneous traffic distribution. The proposed algorithm may maximize different network utilities, e.g., the traffic latency and the network throughput, by properly setting the value of @math . In addition, game theory has been exploited to model and solve the traffic load balancing problems. Aryafar @cite modeled the traffic load balancing problem as a congestion game in which users are the players and user association decisions are the actions.
- The above solutions, though effectively balance the traffic loads to maximize the network utilities, do not consider the green energy utilization as a performance metric in balancing traffic loads. As green energy technologies advance, powering BSs with green energy is a promising solution to save on-grid power and reduce the carbon footprints @cite . It is desirable to recognize green energy as one of the performance metrics when balancing the traffic loads. Zhou @cite proposed a handover parameter tuning algorithm for target cell selection, and a power control algorithm for coverage optimization to guide mobile users to access the BSs with renewable energy supply. Considering a mobile network powered by multiple energy sources, Han and Ansari @cite proposed to optimize the utilization of green energy for cellular networks by optimizing BSs' transmit powers. The proposed algorithm achieves significant on-grid power savings by scheduling the green energy consumption along the time domain for individual BSs, and balancing the green energy consumption among BSs. The authors have also proposed a user association algorithm that jointly optimizes the average traffic delivery latency and the green energy utilization @cite .
- C &W: @cite introduce a convolutional neural network architecture that is capable of learning a language model and generating word embeddings from unlabeled data. The model can be fine-tuned for supervised NLP tasks.
- HLBL: @cite introduce the log-bilinear language model. It is a feed-forward neural network with one linear hidden layer and a softmax output layer. The model utilizes linear combination of word type representations of preceding words to predict the next word. @cite modify this model to reduce computational cost by introducing a hierarchical structure. The architecture is then named the hierarchical log-bilinear language model.
- GCA NLM: @cite introduce an architecture using both local and global context via a joint training objective. The training is very similar to @cite . They represent a word context by taking the weighted average of the representations of word types in a fixed size window around the target word token. Following @cite , they cluster word context representations for each word type to form word prototypes. These prototypes capture homonymy and polysemy relations.
- LR-MVL: @cite present a spectral method to induce word embeddings. They perform the Canonical Correlation Analysis on the context of a token. They provide an algorithm to represent a target word with different vectors depending on its context. The objective function they define is convex. Thus, the method is guaranteed to converge to the optimal solution.
- Skip-Gram NLM: @cite propose a two neural models to induce word embeddings. The first architecture is Continuous Bag-of-Words where the words in a window surrounding the target is used to classify the target word. The second one is continuous Skip-Gram model in which the target word is used to classify its surrounding words. @cite show that these representations reflect syntactic and semantic regularities.
- SCODE Word Embeddings: @cite introduce the SCODE framework, an extension of the CODE @cite framework. @cite obtains word type representations from co-occurrence data generated by using neighbors of words. @cite extend this work by generating co-occurrence data using probable substitutes of words. In Section , we explain this framework in detail. Here, we review studies extending that work.
- @cite used SCODE word embeddings for Word Sense Induction. They achieved the best results in Semeval 2013 Shared Task @cite . @cite is the first study exploiting SCODE embeddings in a supervised setup by using them as word features.
- Several recent works @cite @cite @cite @cite @cite @cite are closely related to ours. Objectness measure @cite combines multiple visual cues to score the windows, and then produces the object proposals by sampling windows with high scores. Based on @cite , Rahtu et. al. @cite proposed another category-independent cascaded method for proposal generation, where the proposal candidates are sampled from super-pixels, which are generated using a segmentation method, according to a prior object localization distribution and then ranked using structured learning with learned features. The idea of grouping super-pixels segments to generate proposals is also used in @cite @cite @cite @cite with different grouping criteria. More empirical comparisons of different proposed object proposal generation methods are presented in @cite .
- Speaker detection in video is an active research topic. Speaker diarization is the problem of determining who spoke when based on audio signals @cite . The problem of speech recognition based on visual information has been studied by gordan2002support and saenko2004articulatory,saenko2005visual . Visual and audio data are often used together in speaker localization ( @cite @cite ). Generally speaking, methods based on training data are more accurate but are more time-consuming. Out of efficiency consideration, we developed a method without involving a training process but still achieved robust detection results because of our use of several novel features and cues.
- A preliminary version of this work appeared in our conference paper @cite . The present paper contains derivations related to routing reliability and efficiency omitted in @cite , as well as new results on the scaling of communication radius required to limit the worst-case disagreement between the actual and desired direction for forwarding. The latter are required to ensure that our prescriptions for routing reliability and scalability are realizable.
- For the routing scheme in this paper and the preceding references, mobility is a nuisance that increases routing overhead. However, when delay in message delivery is not an issue, Grossglauser and Tse have shown in @cite that mobility can actually help us get around the transport capacity limits derived by Gupta and Kumar @cite . In a similar spirit, mobility can be exploited to reduce the overhead of location updates, as argued in @cite @cite . However, this is not the regime of interest to us, since we are interested in delivering packets to their destinations with minimal delay.
- Approximate message passing: AMP is an iterative algorithm that solves a linear inverse problem by successively converting the matrix channel problem into scalar channel denoising problems with additive white Gaussian noise (AWGN). AMP has received considerable attention because of its fast convergence and the state evolution (SE) formalism @cite @cite , which offers a precise characterization of the AWGN denoising problem in each iteration. AMP with separable denoisers has been rigorously proved to obey SE @cite . However, for non-i.i.d. signals we may want to explore non-separable denoisers. @cite provide numerical results demonstrating that SE accurately predicts the phase transition of AMP when some well-behaved non-separable minimax denoisers are applied, and conjecture that SE holds for AMP with a broader class of denoisers. A compressive imaging algorithm that applies non-separable image denoisers within AMP appears in @cite . A potential challenge of implementing AMP is to obtain the Onsager correction term @cite , which involves the calculation of the derivative of a denoiser. @cite leverage a Monte Carlo technique to approximate the derivative of a denoiser when an explicit input-output relation of the denoiser is not available, and provide numerical results showing that SE holds for AMP with their approximation.
- Universal denoising: Our proposed denoiser is inspired by an approach based on context quantization @cite , where a universal denoiser for a stationary ergodic signal involves multiple separable denoisers for conditionally independent subsequences. Sivaramakrishnan and Weissman @cite have shown that their universal denoiser based on context quantization can achieve the MMSE for stationary ergodic signals with bounded components.
- The boundedness condition of Sivaramakrishnan and Weissman @cite is due to their density estimation approach, in which the empirical distribution function is obtained by quantizing the bounded range of the signal. Such boundedness conditions may be undesirable in certain applications. We overcome this limitation by replacing their density estimation approach with GM model learning. Our second contribution is a universal denoiser that does not require the input signal to be bounded; we conjecture that our universal denoiser achieves the MMSE under some technical conditions.
- Fitting Gaussian mixture models: Figueiredo and Jain @cite propose an algorithm that fits a given data sequence with a GM model. The algorithm employs a cost function that resembles the minimum message length (MML) criterion, and the parameters are learned using a modified expectation-maximization (EM) algorithm.
- Our approach to coordinating MDPs contrasts with those of multiagent MDPs @cite and dec-MDPs @cite in finding exact solutions, which face complexity problems for large-scale problems such as ours @cite . Instead, we offer an approximation method that collapses the state space of each agent down to only features that are available locally, and uses averaged effects of other agents for coordination. This is similar in spirit to @cite where effects of actions are estimated by agents (but without the central coordination, as in our work).
- The problem of medical resource allocation is perhaps best addressed to date by @cite @cite which also integrates a health-based utility function to address fairness based on the severity of health states. This model does not, however, consider temporal dependency when determining allocations and our approach of considering future events provides a broader consideration of possible uncertainty. Markov decision processes have been used to model elective (non-emergency) patient scheduling in @cite .
- A key assumption to group users based on the similarity of their followers' friends is that following is an expression of topical interests or demographic similarity, rather than personal contacts. address the question whether Twitter is more of a social network or a news media @cite . Looking at trending topics they find that the majority (over 85 early study of why people use Twitter is presented in @cite . The authors find that people use microblogging to talk about their daily activities and to seek or share information''. Given the findings of , the information sharing fraction has likely increased since which, again, benefits our approach. A similar conclusion that Twitter users have a very small number of friends compared to the number of followers and followees they declare'' is reached in @cite , further indicating that the majority of follow links is likely related to topical interests, not real-life personal connections. This observation is also confirmed in @cite where find that mutual following is a signal for real life friendship.
- Given its global popularity and the relative ease of data acquisition through open APIs https: dev.twitter.com , Twitter has been used before for studies in Computational Social Science and some examples are discussed in the following. use a crawl of 228K Twitter profiles to test whether established sociological theories of real-life networks hold in Twitter'' @cite . They find that, much like individuals in real-life communities, social brokers [...] are opinion leaders who tweet about diverse topics, have geographically wide networks, and express not only positive but also negative emotions''. Garcia- link cultural variables such as Pace of Life, Individualism and power to tweeting behavior from several countries @cite . They find strong correlations between these variables and online behavior. Though our current paper only partly falls under the umbrella of Computational Social Science, we hope that this work still illustrates the potential of using co-following information for such studies.
- Our mapping and visualization of similar accounts in is conceptually similar to community detection which also identifies groups of related accounts. Classical community detection looks at the global network and tries to find areas with unusually high triadic closure @cite or otherwise unusual linkage patterns. A survey of the area and experimental comparisons can be found in @cite and @cite respectively. Related are notions of graph centrality that also require the whole graph @cite . The approach in @cite is arguably the most similar to ours as it combines local structural information with node similarity in an iterative manner. The similarity used, however, requires direct co-links and even direct links. Our approach differs in a number of ways. First, we are not interesting in clustering grouping users but only understand the relative positions of main ones. Second, we do not require a global view of the network but, in return, we cannot argue about things such as node centrality or PageRank. Third, we do not want to find communities induced by friends-of-friends type links. Rather we strive for a similarity-only based approach that can easily be transferred to domains without any friends-of-friend links.
- Determining which of two alternatives a Twitter user is more likely to follow is related to friend recommendation or link prediction as, in a sense, we are suggesting which of the two links should be formed. Intuitively, transitivity and mutuality of links are important signals for link prediction @cite but, as discussed previously, we do not want to use such three people you follow also follow X'' information as it leads to a different type of application, closer to community detection. User similarity based on user attributes has also been used as a feature for link prediction @cite @cite @cite . But this work still partly relies on mutuality and transitivity, which is equivalent to re-enforcing partisan camps without noting any existing similarities in terms of shared interests. For example, such approaches would most likely fail to pick up the similarity between @Puma and @TheAppleInc that we observe in .
- Related to our analysis in on mapping and visualization of similar users is previous work on segmenting online user populations as our methods can also be applied for audience analysis on Twitter. For web search a clustering based market segmentation is presented in @cite . The authors include demographic variables in their analysis which could also be obtained for Twitter through the help of machine learned classifiers. For web browsing a similar study, also including demographic variables, is presented in @cite . In our work we do not explicitly segment millions of Twitter users but, rather, use regular Twitter users to segment a small number of important accounts.
- A component of our numerical evaluation here entails assessing the performance of our approach in a stylized image processing task of saliency map estimation. We note that several recent works have utilized techniques from the sparse representation literature in salient region identification, and in compressive imaging scenarios. A seminal effort in this direction was @cite , which proposed a model for feature identification via the human visual cortex based on parsimonious (sparse) representations. More recently, @cite applied techniques from @cite @cite and low-rank-plus-sparse matrix decomposition @cite @cite in a procedure to identify salient regions of an image from (uncompressed) measurements. Similar sparse representation techniques for salient feature identification were also examined in @cite . An adaptive compressive imaging procedure driven by a saliency map'' obtained via low-resolution discrete cosine transform (DCT) measurements was demonstrated in @cite . Here, unlike in @cite @cite , we consider salient feature identification based on compressive samples, and while our approach is similar in spirit to the problem examined in @cite , here we provide theoretical guarantees for the performance of our approach. Finally, we note several recent works @cite @cite that propose methods for identifying salient elements in a data set using compressive samples.
- The semiclassical limit is, in a sense, the exact opposite to the weakly-nonlinear small-amplitude limit. Indeed, the formal semiclassical limit is given by the strongly nonlinear hyperbolic system . There is at least one other paper in the literature on the subject of semiclassical analysis of the Dirichlet initial-boundary-value problem on the half-line for the defocusing nonlinear Schr "odinger equation, namely a paper of Kamvissis @cite , which directly stimulated our interest in this problem. Like we do, Kamvissis considers general nonhomogeneous Dirichlet boundary data together with homogeneous initial conditions, and he applies the steepest descent methodology for Riemann-Hilbert problems to deduce general properties of the solution in the semiclassical limit. Our Corollary is consistent with Theorem 5 of @cite (the main result of that paper) albeit in the simplest case of genus @math . On the other hand, it is less clear whether the vacuum domain @math described by our Corollary is a special case of Kamvissis' Theorem 5.
- Inertial tracking systems, on the other hand, can be self-contained and require no external reference. They use technologies such as accelerometers, gyroscopes, and compasses to sense their change in orientation @cite . While devices equipped with such sensors (e.g., Wiimote, air mice, and smartphones) are capable of serving as a 3D pointing device, they have only been used to translate objects on a fixed 2D plane (e.g., the TV screen). 3DTouch, with 5 degrees of freedom, does not only serve as a 3D pointing device, but also enables users to rotate and translate objects in 3D space.
- Early work in instrumenting the human finger was conducted in the 3DUI research community. Ring Mouse @cite is a small, ring-like device, with two buttons, worn along the index finger. It uses ultrasonic tracking, but generates only position information. With a similar design to that of Ring Mouse, FingerSleeve uses a 6-DOF magnetic tracker to report position and orientation @cite . The drawback of these devices is that they are not self-contained, relying on an external tracking system.
- In this section we point out similarities and differences of this paper with respect to related approaches in @cite , @cite , @cite , @cite and @cite . @cite introduced proposals with dynamics governed by the SDE where @math . Sufficient conditions for absolute continuity for both cases are given in @cite . The pulling term @math in the SDE forces @math to hit @math at time @math and the case @math takes the drift of the process into account when proposing bridges. However, for both @math and @math the resulting bridges may not resemble true bridges. An illuminating example is given in the introductory section of @cite .
- Without loss of generality we may assume we have have one observation @math . If @math in , a slight adjustment of the Euler discretisation of the bridge (introduced by @cite ) sets where @math and @math (the adjustment being the addition of the term @math in front of @math ). If @math is small, the right hand side defines a discrete time Markov chain approximation @math to @math termed the Modified Brownian Bridge. We can define a mapping @math by the relation [ ( X^ , , X^ ) =g( , W_1, , W_N), ] where @math and @math . This implies that parameters in the diffusion coefficient can be updated conditional on @math , instead of @math ; thereby preventing degeneracy of the data-aug -men -ta -tion algorithm. This is the approach of @cite . @cite use the same algorithm using Euler discretisation of @math in case @math (so no correction in the diffusion coefficient appears in the discretisation). Note that there is no formal proof that these methods work as @math .
- @cite extend the reparametrisation of the diffusion bridges in @cite (cf. section ) to reducible diffusions and otherwise decouple parameter and bridge by first discretising and then using the transformation formula in Euclidean coordinates for the discretised bridges. The use of the driving Brownian motion of the continuous time bridge proposal as innovation process was proposed by @cite .
- A number of studies have been conducted to understand the propagation of spam on OSM, many of which revealed heavy usage of URLs to spread malicious content. in their research identified distinctive features to detect spammers on Twitter @cite . Researchers also evaluated the effectiveness of popular blacklists in evading spam and observed it to be inefficient. On Twitter, checking blacklists becomes even slower because of the URL shortening services used to obfuscate long URLs. Using these services, a spammer can complicate the process of detection by using chains of multiple shortenings @cite . in 2011 developed a system called Monarch which classifies a URL submitted to any web service as malicious or benign in real time @cite . This system relies on the features of URL landing page (like page content, hosting infrastructure, etc.) and detected malicious links with an accuracy close to 91 another real time suspicious URL detection technique on Twitter proposed by , authors addressed the problem of conditional URL redirects @cite . A combined feature set of and was used and authors attained an accuracy of 86.3
- There is little research done in the area of malicious short URL characterization. One such work that presents short URL based features to detect malicious accounts is given by in year 2013 @cite . In their experiment, they investigated the creators of 600,000 short Bitly URLs and associated click traffic from different countries and referrers. Based on the analysis, they classify a link as spam non-spam using only 3 click traffic based features with maximum accuracy of 90.81 After reviewing the above literature, it is evident that a lot of work has been done in the identification and analysis of malicious URLs. Surprisingly, very little work has been done in analyzing only suspicious short URLs to expose the gaps in security mechanisms adopted by a specific URL shortener. Our work significantly differs from the prior studies, as we focus on understanding in depth, the loopholes in spam detection mechanisms of a URL shortening service. We also propose and evaluate a semi supervised classification framework for spam detection in URL shorteners.
- By far the most common technique to geotag digital media @cite is through a process of geo-parsing text, identifying toponyms in text or metadata, and toponym resolution, mapping from a name of a place to an unambiguous geographic location @cite . With respect to digital media, geotagging using text has been applied to general webpages @cite @cite , Wikipedia pages @cite and visual media such as photos posted on Flickr with geo-referenced tags @cite .
- Toponyms are usually stored in a dictionary called a gazetteer. Toponym resolution is a challenging task for a number of reasons: misspellings; the use of abbreviation, which is particularly prevalent in social media microtext services such as Twitter; evolving new and changing location names, for example through urban construction or expansion or through changing vernacular; and, through ambiguities in the use of toponyms. Geocoding literature focuses particularly on the case of topoonym ambiguities of toponyms, of which there are two types @cite : geo geo, where a given toponym can be used to describe multiple distinct locations, and geo non-geo, where a toponym also has a non-geographic meaning.
- Natural language processing and machine learning techniques are used to identify mispellings and abbreviations and to add local terms for automatic gazetteer enrichment @cite @cite @cite . Methods for disambiguation include modeling co-occurrences of toponyms on Wikipedia @cite .
- Other approaches construct their own generative, realtional location dictionaries by data-mining rich, online geo-referenced data, such as using geo-tags on Flickr @cite @cite . The work of @cite extends these by adding temporal and image features to the textual cues to geotag images using a multi-feature approach. The work of @cite and @cite improve these approaches by defining location priors and, in the case of @cite , using statistical term selection. The joint modelling of location-based and topic-based content on social media posts in @cite is used to build geographic language models, that give better geotagging results than geographic terms alone, and in @cite to group geographic regions by topic-similarity.
- The work in @cite uses co-occurrences of toponyms in tags of geotagged Flickr photos, but expands the scope of closeness between toponyms to include tags made by the same user and by contacts within the user's social network. This assumes that links in a social network strongly correlate to users' location, in other words that closeness centrality in the social network is significant. This is not an assumption that we have seen in other toponym resolution publications, but it is a central thesis of our proposed methods. The results of @cite show that toponym resolution is improved by including the posts of contacts, but that adding the posts of contacts of contacts does not perform as well.
- As @cite pointed out, mobile cyber-physical system applications might be present on common sensory devices (e.g., on smartphones). Malware infected devices might be connected to form a botnet (as described by Dagon, Gu and Lee @cite ). With smartphones and similar devices, such a botnet might be built upon one of many network interfaces present in such devices and even on input and output sensors as we describe in this article, although we do not specifically experiment with smartphones and focus at interconnecting laptops. A near-field communication botnet approach based on bluetooth is presented by Singh, Jain, Traynor and Lee @cite . We, instead, use audio communications for our botnet-like approach in near-field communications to construct a covert acoustical mesh network consisting of malware infected drones.
- Hasan, Saxena, Tzipora, Shams and Dustin @cite present a botnet approach where different types of physical emanations are used for command-and-control in botnets. In contrast to these authors, we are not only looking at command-and-control messages, but we build a whole covert mesh network from acoustical emanations.
- For eavesdropping on a computer user's keystrokes by using a computers's physical emanations, several techniques have been proposed. Halevi and Saxena @cite present a study on acoustical emanations from keyboards where the keyboard input is recovered from the sound of a keystroke. Raguram, White, Goswami, Monrose and Frahm @cite discuss a setup where typed input is recovered from environmental reflections, while Frankland @cite describes information leakage over keyboard LEDs that are manipulated to carry an optical signal. Moreover, Balzarotti, Cova and Vigna @cite show that keystrokes can be automatically captured by recording the keyboard with a camera and analyzing the video stream. We also gather keystrokes from a computer user, but we record the keystrokes directly on the infected victim (i.e., the user's computer) where a malware prototype needs to be placed in preparation of an attack. This malware prototype contains both a keylogger software and a network stack for covert acoustic communication in order to spread the recorded keystrokes over the covert network.
- Using audio as a networking technology has been described by Madhavapeddy, Scott, Tse and Sharp @cite , by Yan, Zhou, Shi and Li @cite and by Lopes and Aguiar @cite . Very recently, Nandakumar, Chintalapudi, Padmanabhan and Venkatesan @cite have presented a study on acoustic peer-to-peer near-field-communication (up to 15-20 cm), where eavesdropping by third parties is prevented by the means of signal jamming. Based on the concept of audio networking, we construct a wireless mesh network, not with the ambition to compete with wireless communication standards, but to prove that covert and stealthy communication is possible with audio networking and that critical information can be leaked over a covert acoustical mesh network.
- The clustering method introduced here builds on principles similar to k-hop clustering @cite . Another related approach is nonparametric hierarchical link clustering @cite , which is more general and sophisticated than our simple method, yet its Python implementation we have experimented with proved to be intractable when used in our experiments. A comprehensive overview of semantic similarity measures applicable in is provided in @cite . The similarities used in our experiments were the cosine and Wu-Palmer ones, chosen as representatives of the vector space-based and taxonomy-based similarity types.
- The most relevant tools and approaches for RDF data analysis are @cite @cite @cite @cite . Perhaps closest to our work is @cite that computes a set of statistics and histograms for a given RDF dataset. The statistics are, however, concerned mostly about distributions of statements, instances and explicit statement patterns. This may be useful for tasks like SPARQL query optimisation, but cannot directly answer questions that motivate our work. Graph summaries @cite propose high-level abstractions of RDF data intended to facilitate formulation of SPARQL queries, which is orthogonal to our approach aimed at quantitative characteristics of the data itself. Usage-based RDF data analysis @cite provides insights into common patterns of utilising RDF data by agents, but does not offer means for actually analysing the data. Finally, the recent approach @cite is useful for knowledge discovery in RDF data based on user-defined query patterns and analytical perspectives. Our approach complements @cite by characterising application-independent features of RDF datasets taken as a whole.
- The problem we tackle in this paper is related to a few other problems that have been studied before. Near-duplicate detection of web pages @cite is important in search because web pages are often copied or mirrored with only minor differences (e.g., ads or navigation bars); it would be desirable to return only the canonical'' versions in search results. In fact, the algorithm that we use in this paper, minhash @cite , was originally developed for exactly this purpose. Another closely-related problem is plagiarism detection @cite , or more generally, text reuse'' @cite . In contrast to near-duplicate detection, the focus is usually on smaller segments of text as opposed to entire documents. However, similar approaches such as shingling are applicable to both problems.
- Other similar formulations of the problem are what the data mining community calls pairwise similarity search or all pairs'' search @cite and what the database community calls set similarity join @cite . The task is the same: given a potentially large collection of objects, identify all pairs whose similarity is above a threshold according to some similarity metric.
- There are two classes of solutions to the above problems: in the index-based approach, an inverted index is constructed from objects in the collection and a traversal of the index allows the similar pairs to be extracted, e.g., @cite @cite ; with the hash-based approach, the basic idea is to use locality-sensitive hashing (LSH) to identify similar pairs based on hash collisions, e.g., minhash @cite . Of course, hybrid solutions are also possible. Scaling up these solutions has been accomplished by MapReduce @cite @cite . Similarly, our approach takes advantage of minhash using a MapReduce implementation in Hadoop.
- In both the coloring model and the hardcore model the reconstruction threshold is far from the Kesten-Stigum bound for large @math . In the coloring model close to optimal bounds on the reconstruction threshold @cite @cite were obtained by first showing that, when @math is small, the information on the root is sufficiently small. Then a quantitative version of @cite establishes that the information on the root converges to 0 exponentially quickly. In this work, we show that the hardcore model behaves similarly.
- Popularity of content on the Web, like news articles @cite , blog posts @cite @cite and posts in online discussion forums @cite , vary on different temporal scales. For example, content on micro-blogging platforms, like Twitter is very volatile, and pieces of content become popular and fade away in a matter of hours @cite . Blogging and micro-blogging networks show temporal and topological patterns which largely exhibit power law behavior @cite @cite . have studied the growth and decay of trending topics on Twitter @cite . Temporal trends displayed by users have been analysed and the decay factor has been found to decrease in a power law fashion. Yang and Leskovec examined patterns of temporal behavior for hashtags in Twitter @cite . They presented a stable time series clustering algorithm and demonstrated the common temporal patterns that tweets containing hashtags follow.
- U-relations and p- @math WSA were developed in the influential MayBMS project. http: maybms.sourceforge.net As implied by its design principles, e.g., compositionality and the ability to introduce uncertainty, MayBMS ' query language @math @cite fits well to hypothesis management. Noteworthy, the repair @math key operation gives rise to alternative worlds as maximal-subset repairs of an argument key. We shall look at it from the point of view of p-DB design, for which no methodology has yet been proposed.
- Also related to the @math -DB vision is the topic of conditioning a p-DB. It has been firstly addressed by Koch and Olteanu motivated by data cleaning applications @cite . They have introduced the assert operation to implement, as in AI, a kind of knowledge compilation, viz., world elimination in face of constraints (e.g., FDs). For hypothesis management, nonetheless, we need to apply by asserting observed data (not constraints). In @math , we present an example that settles the kind of conditioning problem that is relevant to @math -DB . In order to provide a concrete feel of our vision, in the next sections we present preliminary results on our methodology for constructing an @math -DB on top of MayBMS .
- QPBF is also a general form of the energy functions of binary MRFs. State-of-the-art methods for binary MRF labeling include BP @cite , TRW @cite @cite and graph cuts @cite . An important conclusion is that any submodular QPBF can be exactly minimized by solving an st-mincut problem on a directed graph @cite . For nonsubmodular QPBFs, the roof duality @cite is used to find an optimal partial labeling, which is called QPBO and two variants P and I @cite @cite . Our algorithm is in contrast to the directed graph formulation @cite and QPBO(P,I) @cite . First, the undirected graph characterization maps a QPBF to a symmetric set function, thus making us easily suppress the supermodular part, which is important for the optimality of our algorithm. Second, we use extended SSP to handle nonsubmodular terms. As a result, our algorithm solves a minimum cut problem of an undirected graph, which needs only half the number of nodes of QPBO(P,I).
- In the past 40 years numerous algorithms have been developed to infer FSMs (equivalently regular grammars) from observed sequences of events @cite @cite @cite . These sequences are referred to as , and are recorded from the system under analysis. The challenge is to derive from the set of traces a FSM that accurately captures the set of all valid sequences of events, even if they do not belong to the initial set of traces.
- Such techniques have previously been applied to proof planning. Jamnik @cite used an Inductive Logic Programming technique to infer what are ultimately regular expressions from well chosen sets of proof methods. For example, if we have the following two proofs (where a-d are proof methods): [a, a, c, d] and [a, b, d] they may be generalized as the following: [a*, [b|c], d] .
- To combat this problem, this paper explores the use of the Extended Finite State Machines @cite as a means of modelling examples of successful proofs. EFSMs extend the traditional FSM. Transitions are labelled with guards on an underlying data store (although the update functions on the store are not explicitly modelled).
- In recent years, algorithms have been developed to infer EFSMs from traces of events @cite @cite , where events are paired with a selection of variable values. In this work we choose the EFSMInfer tool by Walkinshaw @cite , which has been shown to be reasonably accurate when applied to the task of reverse-engineering models of software modules. We provide a brief overview of the essential steps of the approach below.
- Given a set of traces (see Definition ), the approach first infers the guard conditions. For each symbol @math the trace is scanned, identifying every instance where @math is applied, the variable values @math at that instance, and the label of the subsequent step in the trace. This is used to construct a training set where, with the use of standard machine learning algorithms (e.g. decision tree learners like @cite ), it is possible to construct a model that predicts from a given pair label and data configuration what the subsequent label will be. In terms of EFSMs, this gives us @math , and implies some constraints on the order in which particular configurations of labels and variables can occur.
- The subsequent task is to derive an underlying state transition model that obeys and incorporates these data guards. To achieve this EFSMInfer applies an augmented version of the standard FSM state merging algorithm (Lang's Blue-Fringe algorithm @cite ). The set of traces is first arranged as a prefix tree @cite , where traces with the same prefix also share the same path from the root. Subsequently, states in the tree are merged according to the likelihood that they represent the same state, based on the similarity of their outgoing paths.
- Since this model incorporates data, the merging process includes a step to ensure that the model remains consistent with the data guards. Each transition in the tree is mapped to its corresponding variable configurations. Pairs of states are only merged if the resulting model completely obeys the data classifiers (guard conditions) that were obtained in the previous step. If the inferred data model predicts that the data value for a given transition is followed by a label @math , any merge involving the target state can only occur if the resulting state machine contains an outgoing transition that is labelled by @math . After each merge, the resulting state machine is further post-processed to ensure that each transition is deterministic @cite .
- EFSMInfer has several optional parameters. The most important parameter is the choice of data classifier algorithm, which is used to infer the guards on the transitions. For this, EFSMInfer incorporates several standard algorithms that were implemented as part of the Weka @cite toolset. In our experiments, we will adopt the default parameters in EFSMInfer.
- Bilevel network pricing problems have been considered extensively in the literature; see @cite @cite @cite . In these works, the leader prices edges of some subgraph and the followers choose shortest paths with respect to edge costs defined as the sum of travel costs and prices. This problem has been proved to be NP-hard; see @cite and @cite @cite @cite for further hardness and approximability results.
- Some of these techniques were already adapted in non-disjunctive ASP solvers like Smodels @math @cite , @cite , Smodels @cite , @cite , and @cite . More in detail, differs from @cite that are based on a rewriting into a propositional formula and an external SAT solver. differs from @cite and the Smodels variants, which features a native implementation of all inference rules. Our new solver is more similar to , but there are differences concerning the restart policy, constraint deletion and branching heuristics. adopts as default a policy based on the sequence of thresholds introduced in @cite , whereas employs by default a different policy based on geometric series. Concerning deletion of learned constraints, adopts a criterion inspired by , while implements a technique introduced in Glucose @cite . Moreover, adopts a branching heuristic based on BerkMin @cite with a variant of the MOMS criterion which estimates the effect of the candidate literals in short clauses.
- Cyber epidemic dynamics was rooted in biological epidemic dynamics @cite @cite . The first cyber epidemic models @cite @cite were limited by their homogeneity assumption that each computer node has the same effect on the others. Recently, models that are more appropriate for studying cyber security problems have been proposed @cite @cite @cite @cite @cite @cite @cite @cite . As we will elaborate later, the basic idea underlying these models is to use a graph-theoretic abstraction to represent the attack-defense structure, and use parameters to represent attack and defense capabilities. Cyber epidemic dynamics is a special kind of cybersecurity dynamics @cite ,
- We will use the cyber epidemic dynamics model in @cite as the starting point of our study. This model @cite describes reactive adaptive defense (i.e., the defender aims to adjust its defense to control contain the global security state). We extend this model to accommodate MTD, a kind of proactive defense , and the resulting model is analyzed using different skills @cite @cite . We mention that the effect of dynamic structures in cyber epidemic models is studied in @cite , where the structure dynamics however follows a deterministic and periodic process, rather than adaptively scheduled by using (for example) MTD. We also mention that the effect of dynamic semi-heterogeneous structures (i.e., clustered networks), rather than arbitrary heterogeneous structures, is studied in @cite . These studies @cite @cite consider static parameters only and do not have any of the measures we propose to use.
- Most storage-related research focuses on the (logical) network layer, while the physical layer functionality is usually ignored due to the fact that many storage systems in big data centers are wired. Nonetheless, the authors are aware of some interesting works considering the physical layer. In @cite , a so-called partial downloading scheme is proposed that allows for data reconstruction with limited bandwidth by downloading only parts of the contents of the helper nodes. In @cite , the use of a forward error correction code (, LDPC code) is proposed in order to correct bit errors caused by fading. In @cite , optimal storage codes are constructed for the error and erasure scenario. The present paper deviates from the previous work in that it addresses the actual of the transmitted repair data in order to fight the effects caused by fading.
- Isolated from the storage point of view, on the other hand, a plethora of research has been carried out during the past two decades in wireless communications (see @cite and the references therein). Motivated by this work, we will introduce the notion of , a class of codes that should be able to resist fading of the signals during repair transmissions, while also maintaining the repair property of the underlying storage code.
- Assignment games were introduced by @cite and thoroughly analyzed by @cite . A (non-comprehensive) list of further work on assignment games includes the study of strategic incentives @cite , entry @cite , convergence via decentralized processes @cite @cite @cite , elongation of the core @cite and its dimensions @cite , and median stable matchings @cite .
- Analysis of random instances of the linear sum assignment problem was first carried by @cite , and was subsequently improved by @cite @cite @cite . For a survey on the topic and other related literature see @cite .
- Several popular SG technologies are known for development of SG frameworks and instances, for example, WS-PGRADE gUSE DCI-bridge @cite , ASKALON @cite , MOTEUR @cite , etc. They use different enabling components and technologies: web application containers (Tomcat, Glassfish, etc.), portal or web application frameworks (Liferay, Spring, Drupal, etc.), database management systems (MySQL, etc.), workflow management systems @cite . Here the most promising SG framework, namely WS-PGRADE gUSE DCI-bridge bundle (which is an open-source project), is described with emphasize on some examples of its usage in several scientific communities like physics, materials science, nanotechnologies (Fig. ).
- To overcome these limitations, introduced the Sparse Representation based Classification (SRC) method to represent the query image @math based on the over-complete dictionary @math with sparse coefficients, The above @math -norm minimization problem is non-convex and actually NP-hard. It has been proved in @cite @cite that problem ) is equal to @math -minimization problem under certain conditions: To deal with the noises, the @math -minimization problem is extended to the following formulation: where @math is a given tolerance. It is generally regarded that SRC is an extension of NN and NFS. The difference is that the coding of @math is performed over the over-complete dictionary @math instead of its subset. With a sufficient number of training samples, SRC with random projection-based features can outperform NFCs on conventional features. The sparse model is also more robust and effective for object recognition in the case that objects are corrupted by outliers.
- However, SRC is under the assumption that the query image @math and the training images are well aligned. It is indicated that with sufficient training samples that cover nearly all the possible variations, @math can be correctly represented and robust to variations @cite . Thus, SRC may fail in the case that the query image is misaligned or the dictionary has a small number of samples. Meanwhile, due to the sparsity, SRC may have the instability problem when samples are highly correlated. If the subjects correlated to the query image look similar, the SRC method tends to select one at random for representation rather than selecting them all. It indicates that SRC fails to capture the correlation structure of the dictionary which is critical in face recognition @cite .
- Some works have focused on the above problem and provided several solutions. @cite proposed locality constraints in spatial sparse representation. @cite maximized the intra-individual correlations to address the pose difference for face recognition. imposed the @math -norm constraint on the coefficients and proposed CRC (Collaborative Representation based Classification with regularized least square). In @cite , Zhang pointed out that the success of SRC comes from the collaborative representation of @math over all training samples. The @math -norm is supposed to take advantage of data correlation @cite . Thus, the query image @math is represented by the over-complete dictionary with @math -norm rather than @math -norm to constrain the coding vector in CRC. The objective function is defined as: The @math -minimization guarantees that CRC gets a stable result with a much denser representation vector. However, CRC does not perform sample selection for representation. It may not perform well when the training data are not highly correlated.
- A very recent paper by @cite has independently investigated the same topic (parameterized complexity of strong backdoor detection for tractable semantic classes) and some of their results seem close to ours. In particular, one of their theorems (Theorem 5) is similar to our Proposition , but is less general as they assume the target class to be an union of atomic classes. They also study the case where @math is bounded and @math is the parameter, as we do, but their result (Theorem 6) is more specific and can be shown to be implied by our Theorem .
- Most approaches are based on appliances' signatures, , specific features such as the real reactive power, current, and voltage of running appliances @cite . These methods need the support of high sampling rate and build either steady or transient signal features of appliances with labeled training datasets. The signal features are treated as the appliances' signatures @cite @cite , based on which event detection schemes are developed to detect appliances' on off as well as different running states. The detected events are ascribed to certain appliances' activities via classification @cite @cite @cite . In addition to time-domain signal features, spectral analysis has also been adopted to search for appliances' signatures in the frequency domain @cite @cite @cite .
- The performance of signature based approaches depends greatly on the uniqueness of an appliance's signature. In practice, however, the signatures of different appliances may overlap with each other, causing inaccurate event detection. Even for the same type of appliances, it may be hard to obtain the widely acceptable signature @cite . In other words, it is hard to generalize the signature learned from a particular device's operating data. Consequently, even though a method may have a good performance over a specific group of appliances, it may suffer in other datasets, caused by the over-fitting problem due to over-specific signatures. Due to these difficulties, it is not easy to use signature based methods for unambiguous appliance detection and classification.
- A number of methods made use of state transition in appliances' activities for energy disaggregation. Recently, the Hidden Markov Model (HMM) was adopted to model the state transition patterns of appliances. The hidden states of each appliance at each time instant are predicted by inference algorithms, such as the Viterbi algorithm, with the observed emission probabilities @cite @cite . Non-negative sparse coding was proposed to solve the energy disaggregation problem in @cite . It was further discussed in @cite , in which a training process was needed to obtain the basis vector related to the state transition patterns of different appliances. Although some other works, such as @cite , were not to solve the energy disaggregation problem, they also utilized the appliance state transition information and their results may be helpful for energy disaggregation.
- Finally, in the above two types of approaches, optimization algorithms were used to search for optimal solutions. Generally, the objective was to minimize the difference between the predicted value and real aggregation value. For example, in @cite , Least Square Estimation (LSE) was used to find the tightest fit for the aggregated waveform. Nevertheless, as we will disclose in this paper, such solutions usually do not match well the true switching events of appliances, leading to inaccurate energy disaggregation results.
- Recently, @cite considered scenarios in which an attacker designs attacks carefully such that the conventional bad data detection algorithms are not capable of detecting them. Inspired by their work, many other papers targeted this problem @cite @cite @cite @cite @cite @cite @cite @cite @cite . An adversary attack has an impact on the real-time and day-ahead electricity markets. Such situations have been studied by @cite @cite , among others.
- @cite assume a Bayesian model on the state variables and consider a binary detection problem. In particular, they assume that the state variables have a zero-mean Gaussian distribution. @cite impose a linear state-space representation on the power system state evolution and propose a decoder that corrects for the compromised meters. In their plant model, they assume they know the state transition and measurement matrices.
- The EM algorithm @cite is a two-step iterative estimation method that has been proposed in @cite to perform channel estimation in case of superimposed signals. In a first step called Expectation'' (E step), the preamble part is extracted from the received signal. Then for each user @math , vector @math is derived as the preamble of user @math ( @math ) multiplied by its estimated channel coefficient @math (of previous iteration) and additioned with a certain percentage @math of the difference between the received preamble part ( @math ) and the reconstructed preambles of both users @math and @math , as shown below where @math refers to the index of the preamble.
- use the method of autocorrelation to estimate channel parameters in @cite . However, @math is chosen so small that it induces negligible phase variation during a time slot. Moreover, @cite takes advantage of clean packets in CRDSA to get a good estimation of @math and @math . Difficulties arise, however, in other random access methods like MuSCA, where finding clean packets in the frame is a rare situation. Note that @cite presents a system adapted to an environment affected by high phase noise.
- More generally, the theorem of Banaszczyk @cite shows that for any convex set @math with Gaussian measure at least @math and any set of vectors @math of length @math , there exist signs @math so that @math .
- Model-driven engineering in a collaborative manner has been successfully applied in the embedded systems community. Efforts, for instance, include transforming between different UML models and SysML @cite , modeling in SysML and transforming these models to the simulation tool Orchestra @cite , integration of modeling and simulation in Cosmic Cadena @cite , or modeling of reactive systems and integration of various verification tools in Syspect @cite .
- Recent surveys on verification methods for hybrid systems @cite , modeling and analysis of hybrid systems @cite , and modeling of cyber-physical systems @cite , reveal that indeed many tools are available for modeling and analyzing hybrid systems, but in a rather isolated manner. Supporting collaboration on formal verification by distributing tasks among members of a verification team in a model-driven engineering approach has not yet been the focus. Although current verification tools for hybrid systems ( , PHAVer @cite , SpaceEx @cite ), as well as those for arithmetic ( , Z3 @cite ) are accompanied by modeling editors of varying sophistication, they are not yet particularly well prepared for collaboration either. Developments in collaborative verification of source code by multiple complementary static code checkers @cite , modular model-checking ( , @cite ), and extreme verification @cite , however, indicate that this is indeed an interesting field. Most notably, usage of online collaboration tools in the Polymath project has led to an elementary proof of a special case of the density Hales-Jewett theorem @cite .
- The Unified Modeling Language (UML @cite ) is a standardized language for object-oriented modeling. But without extension it is not well suited for modeling hybrid systems @cite . Therefore, the profiling mechanism of UML was used to extend the standardized UML languages SysML @cite for modeling hardware and software components of complex systems and MARTE @cite for modeling real-time and embedded systems. These and other extensions @cite @cite @cite increase the support for hybrid modeling in UML. However, those profiles augment the UML Statechart formalism, since their languages base on hybrid automata as underlying principle. We, instead, use hybrid programs and therefore extend UML Activity Diagrams since they are a more natural way of modeling. Examples for integrating formal notations with informal ones can be found outside the hybrid systems community, for instance, UML-B @cite , TRIO @cite , or VeriAgent @cite .
- In summary, we address model-driven engineering and formal verification as follows. Unlike @cite @cite @cite , who focus on exchanging models, we also facilitate collaboration on formal verification. Unlike @cite @cite @cite , who focus on one aspect of verification, we provide modeling and collaboration tools that should make it easier for domain experts to work in verification teams and exchange models and verification results between different tools. Unlike @cite @cite , who focus on verification tools, we also work on modeling support and collaboration. Unlike @cite @cite @cite , who define a hybrid automaton semantics for UML Statecharts, we define a hybrid program semantics for UML Activity Diagrams. Unlike @cite @cite @cite , who combine formal models with semi-formal modeling in UML for discrete systems, we define a UML profile for hybrid systems modeling.
- The Kaczmarz method was first introduced in the 1937 work of Kaczmarz himself @cite . Since then, the method has been revitalized by researchers in computer tomography, under the name (ART) @cite @cite @cite @cite . Deterministic convergence results for the method often depend on properties of the matrix that are difficult to compute or analyze @cite @cite @cite @cite . Moreover, it has been well observed that random choice of row selection often speeds up the convergence @cite @cite @cite @cite .
- Recently, Strohmer and Vershynin @cite derived the first provable convergence rate of the Kaczmarz method, showing that when each row is selected with probability proportional to its norm the method exhibits the expected linear convergence of . This work was extended to the inconsistent case in @cite , which shows linear convergence to within some fixed radius of the least squares solution. The almost-sure guarantees were recently derived by Chen and Powell @cite . To break the convergence barrier, relaxation parameters can be introduced, so that each iterate is over or under projected onto each solution space. Whitney and Meany prove that if the relaxation parameters tend to zero that the iterates converge to the least squares solution @cite . Further results using relaxation have also been obtained, see for example @cite @cite @cite @cite . An alternative to relaxation parameters was recently proposed by Zouzias and Freris @cite as the REK method described by . Rather than alter the projection step, motivated by ideas of Popa @cite they introduce a secondary step which aims to reduce the residual.
- The Kaczmarz method has been extended beyond linear systems as well. For example, Leventhal and Lewis @cite analyze the method for systems with polyhedral constraints and inequalities, which was also extended to the block case @cite , and Richt 'a rik and Tak 'a c @cite build on these results for general optimization problems.
- Another important aspect of research in this area focuses on accelerating the convergence of the methods. Geometric brute force methods can be used @cite , additional row directions may be added @cite , or instead one can select of rows rather than a single row in each iteration. The block version of the Kaczmarz method is originally due to work of Elfving @cite and @cite . Its convergence rates were recently studied in @cite and analyzed via pavings by Needell and Tropp @cite . The block Kaczmarz method is of course a special instance in a broader class of block projection algorithms, see for example @cite for a more general analysis and @cite for a presentation of other block variants.
- To use block methods effectively, one needs to obtain a suitable partition of the rows (and or columns). Popa constructs such partitions by creating orthogonal blocks @cite @cite @cite , whereas Needell and Tropp promote the use of row pavings to construct the partition @cite .
- Construction of pavings has been studied for quite some time now, and most early results rely on random selection. The guarantee of lower and upper paving bounds has been derived by Bourgain and Tzafriri @cite and Kashin and Tzafriri @cite , respectively. Simultaneous guarantees were later derived by Bourgain and Tzafriri @cite with suboptimal dependence on the matrix norm. Recently, Spielman and Srivastava @cite and Youssef @cite provided simple proofs of the results from @cite and @cite , respectively. Vershynin @cite and Srivastava @cite extend the paving results to general matrices with arbitrary row norms; see also @cite @cite . Proposition follows from the work of Vershynin @cite and Tropp @cite , and is attributed to the seminal work of Bourgain and Tzafriri @cite @cite . For particular classes of matrices, the paving can even be obtained from a random partition of the rows with high probability. This is proved by Tropp @cite using ideas from @cite @cite , and is refined in @cite .
- First, we remark that our result @math , for each integer @math complements previous known result that @math @cite , which can be proved as follows. First, a model of first-order sentence with only one variable remains a model after cloning elements, thus @math only includes the empty set, and sets of form @math . In another paper we show that the class of spectra of two-variable logic with counting quantifiers is exactly the class of semilinear sets, and closed under complement @cite . Using the same methods, one can show that @math is the class of finite and cofinite sets, thus separating @math from @math . On the other hand, three variables are enough to simulate an arbitrary Turing machine, so it is not difficult to construct a set in @math which is not even semilinear, say, e.g., @math , hence, separating @math from @math .
- Lynch showed that @math , where @math denotes the class of sets of positive integers (written in unary form) accepted by non-deterministic multi-tape Turing machine in time @math , where @math is the input integer @cite . The converse is still open and seems difficult. A proof for @math seems to require that model checking for first-order sentences (of arity @math ) on structures with universe of cardinality @math can be done in @math . However, a result by Chen, et. al. states that checking whether a graph of @math vertices contains a @math -clique, which is of constant arity @math , cannot be done in time @math unless the exponential time hypothesis fails @cite @cite @cite .
- Another body of related works is those by Grandjean, Olive and Pudlak which established the variable hierarchy for spectra of sentences using relation and function symbols @cite @cite @cite @cite @cite . Let @math denote the spectra of first-order sentences using up to @math variables with vocabulary consisting of relation and function symbols, and @math denote the restriction of @math to sentences written in prenex normal form with universal quantifiers only and using only @math variables. In his series of papers, Grandjean showed that @math , for each positive integer @math , where @math denotes the class of sets of positive integers accepted by non-deterministic RAM in time @math , and @math is the input integer @cite @cite @cite . By Skolemisation, it is shown that @math , for all @math [Theorem 3.1] GrandjeanO04 . Combined with Cook's hierarchy of non-deterministic time @cite and the known inclusions @math , for each function @math , see @cite , it implies @math , for all @math .
- This does not imply our hierarchy here: @math . Obviously every function can be translated into a relation in first-order logic. However, such translation requires at least one new variable for each function. It is not clear whether there is a translation in which the number of new variables introduced depends only on the arity of the functions, and not on the number of functions. At this point we should also remark that @math can be much more expressive than @math . Take, for example, @math . The class @math consists of only empty set and sets of the form @math , whereas the class @math contains PRIMES, the set of prime numbers @cite .
- Compression has been extensively studied in various database systems, and has been considered as an effective way to reduce the data footprint and improve the overall query processing performance @cite @cite . In terms of efficient storage and retrieval of RDF data, various approaches described in @cite are geared toward efficient storage and transfer, as opposed to having direct access to the data for efficient processing. RDF data compression as used with the most popular triple stores, such as RDF-3X @cite , is performed on the basis of a single dictionary table. This method does not avail of potential speed-up by parallel implementations. Various distributed solutions used to manage RDF data have been proposed in the literature @cite @cite . Nevertheless, their main focus is on data distribution after all the statements have been encoded. There exists only two efficient methods focused on parallel compression of RDF data. One is based on parallel hashing @cite and the other uses the MapReduce model @cite .
- @cite adapt the linear probing method on their Cray XMT machine, and realize the parallel encoding on a single dictionary through parallel hashing, exploiting specialized primitives of the Cray XMT. Their evaluation has shown that their method is highly efficient and the run-time is linear with the number of used cores. This method requires that all data is kept in memory and is deeply reliant on the shared memory architecture of the Cray XMT, making it unsuitable for commodity distributed memory systems. They report an improvement of 2.4 to 3.3 compared to the MapReduce system on an in-memory configuration. By comparison, on similar datasets, our approach outperforms the MapReduce system a factor of 2.6 to 7.4, both on-disk and in-memory.
- Further, have shown that the history of an article and discussion pages in Wikipedia contain valuable information for administrators and moderators. In @cite the authors conclude that collectives in Wikipedia follow their self-imposed rules regarding well defined and formalized processes, such as featured articles. discussed multiple different aspects and the importance of consensus finding on Wikipedia and the Social Semantic Web, by analyzing the history of articles in said systems, further strengthening the need for tools and analyses to be able to better understand and support digital collaborative endeavors.
- A number of tools, such as the OntoWiki @cite , the MoKi @cite , Soboleo @cite or PoolParty @cite support collaborative ontology engineering, focusing on supporting and augmenting different aspects of collaborative development processes of ontologies. For example, Semantic MediaWikis @cite add semantic capabilities to traditional Wiki systems. They are intended to help users navigating the Wikis by introducing more meaningful semantic links and support of richer queries. Some of the Semantic Wikis available today focus on with semantic links in order to allow more meaningful navigation and to support richer queries. Semantic Wikis usually associate a page to a particular instance in the ontology, and the semantic annotations are converted into properties of that instance. As an ontology represents a formalized and abstract version of a specific domain, disagreements between authors on certain subjects can occur. Similar to face-to-face meetings, these collaborative ontology-engineering projects need tools that augment collaboration and help contributors in reaching consensus especially when modeling (controversial) topics of the real world.
- , and its extensions for collaborative development, such as and iCAT @cite (see Figure for a screenshot of the iCAT ontology-editor interface) are prominent tools that are used by a large community worldwide to develop ontologies in a variety of different projects. Both and provide a robust and scalable environment for collaboration and are used in several large-scale projects, including the development of ICD-11 @cite .
- P " @cite , and @cite have created and further developed , a tool to browse an ontology and visualize aspects of its history. PragmatiX also provides quantitative insights into the creation process. The authors applied it to the analysis of the ICD-11 project.
- @cite investigated the hidden social dynamics that take place in collaborative ontology-engineering projects from the biomedical domain and provided new metrics to quantify various aspects of the collaborative engineering processes. investigated the change-logs of collaborative ontology-engineering projects, showing that contributors exhibit specific roles, which can be used to group and classify these users, when contributing to the ontology. investigated if the location and specific structural features can be used to determine if and where the next change is going to occur in the Gene Ontology http: www.geneontology.org . have used association-rule mining to analyze user editing patterns in collaborative ontology-engineering projects. The approach presented in this paper uses Markov chains to extract much higher detailed user-interaction patterns incorporating a variable number of historic editing information.
- We build on a number of previous approaches for training object detectors from weakly-labeled data. In nearly all cases, the task is formulated as a multiple instance learning (MIL) problem @cite . In this formulation, the learner has access to an image-level label indicating the presence or absence of the target class, but not its location (if it is present). The challenge faced by the learner is to find the sliver of signal present in the positive images, but absent from the negative images. The implicit assumption is that this signal will correspond to the positive class.
- Although there have been recent works on convex relaxations @cite @cite , most MIL algorithms start from an initialization and then perform some form of local optimization. Early efforts, such as @cite @cite @cite @cite @cite @cite @cite , focused on datasets with strong object-in-the-center biases (e.g. Caltech-101). This simplified setting enabled clarity and focus on the MIL formulation, image features, and classifier design, but masked the vexing problem of finding a good initialization in data where such helpful biases are absent.
- More recent work, such as @cite @cite , attempts to learn detectors, or simply automatically generate bounding box annotations from much more challenging datasets such as PASCAL VOC @cite . In this data regime, focusing on initialization is crucial and carefully designed heuristics, such as shrinking bounding boxes @cite , are often employed.
- If any bin is as good as the others, putting items inside a solution to this problem is trivial, as one will always be interested in using the largest bin. We thus suppose that bin utilization induces a certain cost associated to this bin. This assumption leads to the Variable Sized Bin Packing with Cost Problem (VSBPCP). The reader might already observe that in the BPP the cost of packing is always the same regardless of the bins chosen. This fact explains that the new problem is NP-hard @cite . Intuitively, one can consider that the cost of packing varies in function of a bin capacity. From this point of view we are no longer interested in minimizing the number of bins used but in minimizing the global packing cost as a voluminous bin which remains 'almost empty' may be more expensive in use than several little bins 'almost totally' full. Solving the VSBPCP we have at our disposal @math classes of bins and the infinite number of bins of any class available.
- The vocabulary provides a discrete partitioning of the feature space by visual words. Typically, either flat kmeans @cite @cite or hierarchical kmeans @cite is employed to train a vocabulary in an unsupervised manner. Improved methods include incorporating contextual information into the vocabulary @cite , building super-sized vocabulary @cite @cite @cite , making use of the active points @cite , etc. Feature-to-feature matching is a key issue in the BoW model. The baseline approach employs a coarse word-to-word matching, resulting in undesirable low precision. To improve precision, some works analyze the spatial contexts @cite @cite @cite of SIFT features, and use the spatial constraints as solution to refining matching. Another line of works extracts binary signatures from SIFT descriptors @cite or its contexts @cite @cite . The feature matching is thus refined by a further check of the Hamming distance between binary signatures. In this paper, however, we argue that even if two features are adjacent in the feature space, the corresponding images are probably very different. Therefore, we are supposed to look one step further by estimating a joint similarity on both image- and feature-level from clues in multiple vocabularies.
- The recent papers @cite @cite are of particular relevance to us since they establish a connection between learning sparse polynomials and compressed sensing. The authors show that the problem of learning a sparse polynomial is equivalent to recovering the unknown sparse coefficient vector using linear measurements. By applying techniques from compressed sensing theory, namely Restricted Isometry Property (see @cite ) and incoherence (see @cite ), the authors independently established results for reconstructing sparse polynomials using convex optimization. The results have near-optimal sample complexity. However, the running time of these algorithms is exponential in the underlying dimension, @math . This is because the measurement matrix of the equivalent compressed sensing problem requires one column for every possible non-zero monomial.
- On the practical side, we give an application of the theory to the problem of hypergraph sketching. We generalize a prior work @cite that applied the compressed sensing approach discussed before to graph sketching on evolving social network graphs. In our algorithm, while the sample complexity requirements are higher, the time complexity is greatly reduced in comparison. We test our algorithms on a real dataset and show that the algorithm is able to scale well on sparse hypergraphs created out of Yahoo! messenger dataset by filtering through time and location stamps.
- The results reported in this paper are connected to previous work on the development of an abstract fixed point theory for non-monotonic operators. Pioneering in this respect is the work of Fitting @cite who used the abstract framework of lattices and operators on lattices in order to characterize all major semantic approaches of logic programming. Despite its abstract nature, Fitting's work is centered around the theory of logic programming.
- A more recent work that is also connected to our approach is reported in @cite . In that paper the authors consider the case of product lattices as-well-as stratifiable operators on such lattices. In our terminology, an operator @math is stratifiable iff for all @math and for all @math , if @math then @math . The authors demonstrate @cite [Theorem 3.5] that for every stratifiable operator @math it holds that every fixed point of @math can be constructed using the fixed points of a family of operators called the components of @math (intuitively, to every sublattice @math of the product lattice @math there corresponds a subfamily of the components). However, the least fixed point of @cite [Theorem 3.5] is with respect to the pointwise partial order that is defined on the product lattice @math while in our case @math is not necessarily pointwise. Moreover, our construction does not only apply to product lattices but to all lattices that satisfy the axioms of Subsection . For example, the non-standard product model of Subsection does not fall within the scope of the results developed in @cite .
- This section reviews the supervised compact codebook creation methods in @cite @cite @cite , with the focus on @cite which inspires our work. As shown in @cite , compact codebook creation can essentially be casted as a large-scale discrete optimization problem, subject to a criterion related to the discriminative power of the resultant compact codebook. Due to the difficulty of efficient and global optimization, hierarchically merging visual words is often adopted in the literature. That is, two words are identified at each level of the hierarchy such that merging them will optimize a given criterion. Let @math denote a visual codebook consisting of @math words. Let @math be the resultant codebook after merging the @math th and @math th words. The corresponding histogram for the @math th sample is denoted by @math , and its @math th bin is @math , where @math . Also, @math is the class label of a training sample. In this paper, the criteria in @cite @cite @cite are termed AIB, CSM and UVD in short, respectively.
- : In @cite , the mutual information, @math , between @math and class labels @math is used to measure its discriminative power as where @math denotes the @math th word of @math and @math and @math are estimated with the @math th bins of training histograms. At each level @math , the words @math and @math whose mergence maximizes @math are identified and merged. As noted in @cite , this criterion can be related to agglomerative information bottleneck @cite .
- : In @cite , the scatter-matrix-based class separability, @math , is used to measure the goodness of @math as where @math and @math are the within-class scatter matrix and the total scatter matrix, respectively. @math denotes the trace of a matrix. They are computed with training histograms @math . At each level, the words @math and @math whose mergence minimizes @math are identified and merged To facilitate the subsequent analysis, we use the minimization of @math here. Because of the identity @math , it is equivalent to @cite which maximizes @math . .
- We remark here that the issue of delayed CSIT has recently sparkled a significant interest in terms of improvements that can be achieved in terms of Degrees of Freedom at high SNRs @cite @cite . However, that line of work does not deal with HARQ protocols and is directed towards multi-user scenarios, while in our case we consider relatively low SNR and a single link.
- Application-level defenses alter the sequence of HTTP requests and responses to further obfuscate the user's activity. For example, HTTPOS uses HTTP pipelining, HTTP Range requests, dummy requests, extraneous HTTP headers, multiple TCP connections, and munges TCP window sizes and maximum segment size (MSS) fields @cite . Tor has also released an experimental version of Firefox that randomizes the order in which embedded objects are requested, and the level of pipelining used by the browser during the requests @cite . Both schemes were defeated by Cai, et al @cite .
- In the supervised paradigm, key term extraction is formulated as a problem. Each term is encoded using different feature representations such as @cite @cite , @cite , and @cite , @cite , etc. Several machine learning algorithms have been used by different groups, e.g., na " i ve Bayes by , and , decision trees by Ercan and Cicekli , support vector machines by and , and conditional random fields by .
- In the unsupervised paradigm, the key term extraction problem is framed as a problem. Dominant ranking strategies include: @cite , @cite , and @cite , etc.
- Graph-based methods to key term extraction are inherently unsupervised, where the philosophy is to build a network of words phrases, and then rank the nodes using some kind of centrality measure. Researchers used variants of PageRank @cite @cite , HITS @cite , and other measures like degree, betweenness and closeness @cite . Use of noun phrase networks has been reported in @cite .
- Server selection algorithms have a rich history of both research and actual implementations over the past two decades. Several server selection algorithms have been proposed and empirically evaluated, including client-side algorithms that use historical performance feedback using probes @cite @cite . Server selection has also been studied in a variety of contexts, such as the web @cite @cite , video streaming @cite , and cloud services @cite . Our work is distinguished from the prior literature in that we theoretically model the Go-With-The-Winner'' paradigm that is common to many proposed and implemented client-side server selection algorithms. Our work is the first formal study of the efficacy and convergence of such algorithms.
- First, these check-in sources do not usually contain very much ancillary categorical information about a trip such as mode, weather, purpose, number of passengers, etc. Thus, if one wants to know the effect of external factors, one may be limited by resources to determine all of them accurately @cite .
- Models of saliency can be either pre-specified or learned from eye tracking data. In the former category falls the basic saliency model @cite that combines information from multiple channels into a single saliency map. Information maximization @cite provides an alternative criterion for building saliency maps. These can be learned from low-level features @cite or from a combination of low, mid and high-level ones @cite @cite @cite . Saliency maps have been used for scene classification @cite , object localization and recognition @cite @cite @cite @cite @cite and action recognition @cite @cite . Comparatively little attention has been devoted to computational models of saliency maps for the dynamic domain. Bottom-up saliency models for static images have been extended to videos by incorporating motion and flicker channels @cite @cite . All these models are pre-specified. One exception is the work of @cite , who train an interest point detector using fixation data collected from human subjects in a free viewing (rather than specific) task.
- Datasets containing human gaze pattern annotations of images have emerged from studies carried out by the human vision community, some of which are publicly available @cite @cite @cite @cite @cite and some that are not @cite (see @cite for an overview). Most of these datasets have been designed for small quantitative studies, consisting of at most a few hundred images or videos, usually recorded under free-viewing, in sharp contrast with the data we provide, which is large scale, dynamic, and task controlled. These studies @cite @cite @cite @cite @cite @cite @cite @cite could however benefit from larger scale natural datasets, and from studies that emphasize the task, as we pursue.
- The problem of visual attention and the prediction of visual saliency have long been of interest in the human vision community @cite @cite @cite . Recently there was a growing trend of training visual saliency models based on human fixations, mostly in static images (with the notable exception of @cite ), and under subject free-viewing conditions @cite @cite . While visual saliency models can be evaluated in isolation under a variety of measures against human fixations, for computer vision, their ultimate test remains the demonstration of relevance within an end-to-end automatic visual recognition pipeline. While such integrated systems are still in their infancy, promising demonstrations have recently emerged for computer vision tasks like scene classification @cite , verifying correlations with object (pedestrian) detection responses @cite @cite . An interesting early biologically inspired recognition system was presented by @cite , who learn a fixation operator from human eye movements collected under video free-viewing, then learn action classification models for the KTH dataset with promising results. Recently, under the constraint of a first person perspective, Fathi et. al. @cite have shown that human fixations can be predicted and used to enhance action recognition performance.
- Our adversarial channel model can be seen as a special class of arbitrarily varying channel. For this class we can remove some of the restrictions of the above line of work. More specifically, (i) we do not assume shared randomness between the sender and the receiver, (ii) we consider an integrated adaptive eavesdropping and jamming adversary and assume that to corrupt the next symbol, the adversary uses all its knowledge up to that point; (iii) we allow adversary to choose the message distribution and so the error is worst case; and finally (iv) the secrecy measure in our case is in terms of statistical distance between the adversary's views of two adversarially chosen messages. We note that in @cite , secrecy is measured as the mutual information of a random message (uniform distribution on messages) and the adversary's view. Our security definition using statistical distance is equivalent to the mutual information security when the message distribution is adversarially chosen @cite .
- Low-level audio features are typically extracted from short time frames of the musical clip, then some temporal integration is done. Sometimes an early integration is performed, by taking statistics (mean, variance, covariance, ) of the feature vector over longer segments, or over the entire song ( @cite @cite ). Sometimes late integration is performed, for instance: each short segment is classified and for the entire musical clip a majority vote is taken over the multiple segments' declared labels ( @cite ). Such late integration systems require more computing time, since the classification operation should be done to every frame, instead of to a single vector representation per song.
- Encoding of low-level features using a pre-calculated codebook was examined for audio and music. Quantization tree ( @cite ), vector quantization (VQ) ( @cite @cite @cite ), sparse coding with the LASSO ( @cite ) and other variations ( @cite @cite ) were used to represent the features at a higher level. Sparse representations were also applied directly to time domain audio signals, with either predetermined kernel functions (Gammatone) or with a trained codebook ( @cite @cite ). As alternative to the heavy computational cost of solving optimization criteria (like the LASSO) greedy algorithms like matching pursuit have also been applied ( @cite @cite @cite ).
- Heterogeneous and multi-layer systems have been proposed. The bag of systems approach combined various generative models as codewords ( @cite ). Multi-modal signals (audio and image) were combined in a single framework ( @cite ). Even the codebook training scheme, which was usually unsupervised, was combined with supervision to get a boosted representation for a specific application ( @cite @cite ). Deep belief networks were used in @cite , also combining unsupervised network weights training with supervised fine tuning. @cite audio features were processed in two layers of encoding with codebooks.
- Several works invested in comparing different encoding schemes for audio, music and image. Nam al examined different variations of low-level audio processing, and compared different encoding methods (VQ, the LASSO and sparse restricted Boltzman machine) for music annotation and retrieval with the CAL500 dataset @cite . Yeh al reported to find superiority of sparsity-enforced dictionary learning and @math -regularized encoding over regular VQ for genre classification. @cite Coates and Ng examined the usage of different combinations of dictionary training algorithms and encoding algorithms to better explain the successful performance of sparse coding in previous works. They concluded that the dictionary training stage has less of an impact on the final performance than the encoding stage and that the main merit of sparse coding may be due to its nonlinearity, which can be achieved also with simpler encoders that apply some nonlinear soft thresholding. @cite Coates al examined various parameters of early feature extraction for images (such as the density of the extracted patches) and showed that when properly extracting features, one can use simple and efficient algorithms (k-means clustering and single layer neural network) and achieve image classification performance as high as other, more complex systems.
- @cite define a generic notion of and give their semantics in terms of categories of matrices, under special (blocked) composition schemes. They make implicit use of what we have identified as the standard biproduct (enabling blocked matrix algebra) to formalize column and row-wise matrix join and fusion, but the emphasis is on iteration theories which matricial theories are a particular case of.
- Using wireless networks as indoor locating infrastructure, there are different ways to utilize the wireless signal. One way is to use the propagation model of RF signal as a ranging reference. Some research works studied the RF attenuation model in indoor environments @cite , so that distances from a target to a set of beacons can be inferred from the amount of RF attenuations. Then least square estimation or multilateration methods @cite are applied to the distance set to estimate the position of the target. Although this method is simple to calculate, the positioning accuracy is coarse, because even empirical propagation model cannot capture the dynamics of indoor environments.
- To improve the positioning accuracy, based approach was proposed to model the diverse fading signatures of radio signal @cite @cite @cite . This method contains an offline and an online phase. In offline phase, @math training locations are selected in the sensing field, which are denoted by @math . Suppose there are @math beacons (WiFi APs or wireless sensors) in the sensing field, which are denoted by @math . In the training phase, the RSS values of all beacons at each training location @math will be measured over a period of time, so that a vector of location @math is constructed as @math . When only mean value of RSS is considered, @math represents the average RSS value from @math . When signature distribution is considered, @math can be probabilistic density function (pdf) of RSS from @math . The signature vectors of all training locations are stored as a database, called , denoted by @math .
- In the online positioning phase, a mobile target measures its current RSS vector @math and finds the best match (Euclidean distance in signal space) of @math in @math to estimate the position of the target. In mean value type radio-map, matching can be conducted by Nearest Neighbor algorithm @cite . In pdf type radio-maps, maximum likelihood estimation and Bayesian estimation can be applied. When radio-map is trained in fine granularity and the environments are not highly dynamic, the positioning accuracy of radio-map based method can be in 2-3 meters resolution.
- But the positioning accuracy may become worse in hostile environment such as in ship cabins, where the shadowing and multi-path fading effects are severe and the RSS signatures change over time. Another problem is that the radio-map calibration process is general time consuming and laborious, which generally needs deliberate training method @cite .
- A more accurate approach is to utilize the speed difference of signal propagation to measure distances from transmitters to receivers, so as to conduct indoor locating more accurately @cite @cite . Ultrasound and acoustic signals are the generally exploited low speed signals. In the case of measuring time of arrival, the transmitter broadcasts low-speed signal (ultrasound or acoustic) and RF signal simultaneously. The receiver receives the RF signal to synchronize timer with the transmitter and then measures the traveling time of the low-speed signal to estimate distance from the transmitter. Only when a receiver @math is within the communication range of the low speed signal (denoted by @math , and generally small) of the transmitter @math , can a distance @math be measured. When a set of distances, which is denoted by @math are obtained, least square estimation or multilateration is applied for position calculation. TOA-based positioning can provide centimeter level positioning accuracy @cite . But because the short transmission range of the low speed signals (ultrasound and acoustic), and the requirement of more than three non-collinear distances for location estimation, TOA-based positioning requires dense deployment of TOA beacons, which poses high cost to the positioning system.
- A number of information systems have been proposed and deployed on scientific infrastructures. For example, early versions of the Globus software included a Metacomputing Directory Service (MDS) @cite for storing information in a hierarchical directory service accessed via the Lightweight Directory Access Protocol (LDAP). The Globus project then transitioned to a MDS based on web service (SOAP WSDL) technologies @cite .
- The Condor system contains a Collector service that contains classads that describe hosts. These descriptions contain standard information, but can also include custom information. This service is uses as part of Condor matchmaking @cite .
- The beneficial effect of mobility onto the capacity of ad hoc networks appeared in @cite under the assumption of mobility occurs in the unitary disc. Capacity results were extended to the case of clustered networks in @cite : indeed, the presence of a clustered hierarchical structure, is showed to lower the minimum asymptotic degree required to maintain asymptotic connected graphs. The reduction per node obeys to a factor @math , where @math is the cluster size.
- Novel contributions: Customary models used in ad hoc networks consider either a fixed topology, i.e., the case of a static ad hoc network, or a fully disconnected network, i.e., the DTN case. The analysis proposed in this paper is able to explore the intermediate case when connectivity is still intermittent as assumed in DTNs, but, whenever a connected component is present, messages can span several hops with very small delay. Moreover, if compared to the analysis in @cite , the continuous time model we employ has the key advantage that the closed forms and bounds proposed in this paper can be specialized to span different conditions of the sub-critical regime, depending on the ratio between the contact time duration and the intermeeting times.
- Our motivation is the double-sided greedy algorithms of Buchbinder that we wish to formalize. Independently, Bar-Noy and Lampis @cite gave a @math deterministic online greedy algorithm for the Max-Di-Cut problem matching the deterministic approximation obtained by Buchbinder for all unconstrained non-monotone submodular maximization problems. Interestingly, their algorithm is shown to be the de-randomization of the simple @math random-cut algorithm! Bar-Noy and Lampis give an improved @math approximation for Max-Di-Cut when restricted to DAGs. Furthermore, they provide a precise online model with respect to which this approximation bound is essentially optimal.
- Deep learning architectures have been successfully applied to a wide range of application: characters recognition @cite , object recognition @cite , natural language processing @cite or image classification @cite
- In speech, one of the first phoneme recognition system based on neural network was the Time Delay Neural Network @cite . It was extended to isolated word recognition @cite . At the same time, the hybrid HMM ANN architecture approach @cite @cite was developed, leading to more scalable systems. Recently, the deep belief network @cite approach has been found to yield good performance in phone recognition @cite . It was also extend to context-dependent phonemes in @cite . However, these systems used complex hand-crafted features, such as MFCC. Later, there has been growing interests in using short-term spectrum as features. These intermediate'' representations (standing between raw signal and classical'' features such as cepstral-based features) have been successfully used in speech recognition applications, for example in unsupervised feature learning @cite , acoustic modeling @cite and large vocabulary speech recognition @cite @cite . Convolutional neural networks also yield state-of-the-art results in phoneme recognition @cite . Although these systems are able to learn efficient representations, these features are still used as input for hybrid systems.
- The work presented in @cite successfully investigates features learning from raw speech for phoneme recognition. Here also, these features are used as input for a different system. Finally, convolutional neural networks have shown the capability of learning features from raw speech and estimate phoneme conditional probabilities in a single system @cite .
- Reeb graphs are used in image analysis @cite as simple representations of shapes. The Reeb graph is an abstract graph that describes the linking structure of connected components in level sets of a real-valued function on a complex. Briefly, a complex is a union of vertices, edges, triangles, tetrahedra and so on. Formal Definition of a Reeb graph in section doesn't specify any canonical way to visualize this abstract graph in a low dimensional space.
- Our methods for book embeddings were inspired by basic embeddings of graphs @cite and by a reduction of the topological classification for graphs embedded in @math to word problems in finitely presented semigroups @cite .
- An alternative method to improve the strength of texture descriptor is to perform efficient preprocessing. For example, in face recognition, Vu and Caplier @cite @cite applied the LBP operators upon three edge distribution maps across different directions, and reported state-of-the-art performance. However, to the best of our knowledge, with regard to feature extraction in texture classification, no such efficient preprocessing method exists (in @cite , the DLBP features and Gabor filters are extracted separately). This is the motivation for the algorithm presented in this paper.
- Interest in model acquisition has a long history. Many of the scanning researches share the same pipeline: surface acquisition, registration of the obtained surfaces and finally fusion to obtain high quality model. In the earliest researches all of these stages were performed separately and offline @cite . Thus interest was mostly focused in quality and stability improvement of each of this stages to allow finer model acquisition with less human intervention.
- First real-time scanning applications used to store point cloud instead of fully polygonized model, this allowed to avoid costly fusion step and perform ICP registration as usual @cite . During the scanning process some representation of the reconstructed model was rendered to the user, so it can control the scanning process. Then enough data was acquired user terminated the scanning process and offline global registration and fusion algorithms were performed to obtain high quality model. With such software almost anyone can digitize real-world objects.
- With the rise of computation power real-time implementation of the costly fusion step become practicable @cite . Performing fusion in real-time decreases the time necessary to obtain high quality model, simplifies and pleases scanning process for the user, improves the registration quality.
- Although training with the full training set is common practice, there are a few notable exceptions. @cite propose a method of pruning a training set and show its effectiveness in removing outliers and hard-to-learn examples. Within the context of a face recognition task, they introduce outliers (background samples labeled as faces) to the training set to test their algorithm. They train multiple classifiers using different partitions of the training set and identify which examples create more disagreement across classifiers, labeling them as troublesome. In our experiments we do not introduce any artificial noise because we can show that current datasets contain a large number of hard-to-learn examples for current recognition models, and prunning them can be beneficial.
- Robust learning algorithms provide an alternative way of differentially treating training examples, by assigning different weights to different training examples or by learning to ignore outliers @cite . For example, @cite have shown that treating bounding boxes as noisy observations can provide significant improvements to the performance of the final classifier.
- Some methods for pruning training examples are designed to work with specific learning techniques. For boosting, Vezhnevets and Barinova @cite proposed an algorithm that eliminates training examples that are not correctly classified during the training, considering them as confusing examples. They showed their approach to be effective for improving classification performance when the classes have a large degree of overlap. For SVMs, the loss function can be changed to become more robust to outliers and bad training examples @cite @cite .
- consider a software's requirements as a basis for prioritization of test cases. @cite prioritized test cases based on four factors: requirements volatility, customer priority, implementation complexity, and fault proneness of the requirements. Krishnamoorthi at al. @cite adopted a similar approach. Their prioritization is based on six factors: customer priority, changes in requirement, implementation complexity, completeness, traceability and fault impact. However, a potential weakness of requirement-based approaches is that requirement properties are subjective and thus estimations might be biased.
- Researchers used a number of other criteria to prioritize test cases. prioritized test cases based on historical change records in @cite . They proposed a methodology for determining the effect of a software feature change and then prioritized regression test cases by gathering software change records and analyzing them through singular value decomposition. @cite introduced distribution-based filtering and prioritized test cases based on the distribution of the profiles of test cases in the multi-dimensional profile space. @cite prioritized test cases for web applications. They prioritized test suites by test lengths, frequency of appearance of request sequences and systematic coverage of parameter-values and their interactions. @cite introduced a prioritization technique based on data-flow analysis. They focused on the definition and use of program variables for the data-flow analysis. @cite prioritized test cases using relevant slices. @cite prioritized test cases in a black box environment.
- However, none of the above solutions considered dependencies among faults in prioritizing test cases for regression testing. In software testing, it is known that some faults are the consequences of other faults (leading faults). So, intuitively, test cases that revealed the leadings faults should be executed first in a regression testing in order to get an early confirmation that software is free from dependent faults. @cite we took the first step to prioritize regression testing based on fault dependency. We proposed an algorithm to prioritize test cases based on fault dependency. We also proposed a metric Average Average Percentage Fault Dependency Detected (APFDD) to quantify how rapidly a prioritized test suite can detect dependencies among faults. However, that work only considered @math -hop neighborhood or dependencies of faults. This paper leverages a fault network for prioritization.
- The automatic discovery of polynomial invariants for imperative programs has received a lot of attention in recent years. Mller-Olm and Seidl generate invariant polynomial equalities of bounded degree by backwards propagation @cite . This can be seen as an extension to Karr's algorithm @cite , which uses only linear arithmetic. Seidl, Flexeder and Petter apply the backwards-propagation method to programs over machine integers, i.e., programs whose variables range over the domain @math @cite .
- Coln combines the two aforementioned approaches by doing the fixed point computation on ideals with linear algebra @cite . He introduces the notion of pseudo-ideals to ensure termination of the fixed point computation while retaining the expressiveness of generated invariants.
- Polynomial program invariants can also be derived without using Grbner basis computations @cite . use backwards analysis and variable substitution on template polynomials for an incomplete approach.
- The constraint solving approach that generates invariant polynomial equalities using templates was proposed by Sankaranarayanan, Sipma and Manna @cite . Invariant generation is a central ingredient to our synthesis method, so we want the invariant generation process to be as complete as possible. Therefore we extend their approach by using a more general condition for the invariant (see also rem-ssm-comparison ) that enables us to state a completeness criterion.
- Polynomial lasso programs have also received some attention regarding the analysis of their termination properties. Bradley, Manna and Sipma use finite difference arithmetic to compute lexicographic polynomial ranking functions for polynomial lasso programs @cite .
- The deterministic orienteering problem was introduced by @cite . It has several applications, and many exact approaches and heuristics have been applied to this problem, see eg. the survey @cite . The first constant-factor approximation algorithm was due to @cite . The approximation ratio has been improved @cite @cite to the current best @math .
- @cite studied a generalization of the stochastic knapsack problem, to the setting where the reward and size of each item may be correlated, and gave an @math -approximation algorithm and adaptivity gap for this problem. Recently, Ma @cite improved the approximation ratio to @math .
- The correlated stochastic orienteering problem was studied in @cite , where the authors obtained an @math -approximation algorithm and an @math adaptivity gap. They also showed the problem to be at least as hard to approximate as the deadline orienteering problem, for which the best approximation ratio known is @math @cite .
- Siegel states [Abstract] siegel04hash about his scheme that it is far too slow for any practical application''. This and the @math evaluation time has lead researchers to seek simpler and faster schemes. Several works @cite @cite @cite have been focused on the case of smaller independence @math . These works have all been position sensitive like ours. Fix @math . We are looking at functions @math , to be composed with a simple tabulation hash function @math . The evaluation time is @math , so we want @math to be small.
- Thorup and Zhang @cite found an explicit deterministic construction of a @math -unique @math which also has better constants than the scheme from @cite . By Lemma , the resulting hash function is exactly @math -independent. Simple tabulation is by itself @math -independent, but @cite is motivated by applications needing 4 and 5-independence. For @math and @math , @cite gets down to @math . For general @math , using @math , @cite gets @math .
- Klassen and Woelfel @cite focus mostly on @math , where for arbitrary @math they get @math . For general @math , their bound is @math .
- We note that the twisted tabulation in @cite has a similar flavor to the above schemes, but it does not yield independence above 3. The main target of @cite is to get strong Chernoff style bounds.
- The above works @cite @cite @cite thus need @math for independence @math . This contrasts our Theorem thm:main which gets @math with independence @math . Apart from the case @math , @math from @cite , our new scheme is probably also the easiest to implement, as we are just applying simple tabulation twice with different parameters.
- Decentralized digital currencies have been proposed before Bitcoin, starting with @cite and followed by peer-to-peer currencies, e.g. @cite @cite ; see @cite @cite for short surveys. None of these are centered around a global log; therefore, their techniques and challenges are unrelated to this work.
- Recent work @cite addresses the lack of incentive for disseminating transactions between miners, since each of them prefers to collect the transaction fee himself. This is unrelated to the mining incentive mechanism we discuss.
- A recent survey @cite identifies incentive-compatibility as a critical property of the system, and describes a potential scenario that could lead to a single entity controlling a majority of the mining power. Our work demonstrates that the Bitcoin system is not incentive compatible, and shows an imminent vulnerability.
- A widely cited study @cite examines the Bitcoin transaction graph to analyze client behavior. The analysis of client behavior is not directly related to our work.
- Optimal tuning of parameters to obtain the smallest final reconstruction error has been the focus of major research in CS, machine learning, and statistics. The methods considered in the literature fall into the following three categories: [(i)] The first approach is based on obtaining an upper bound for the reconstruction error and setting the parameters to obtain the smallest upper bound. For many of the algorithms proposed in the literature, there exists a theoretical analysis based on certain properties of the matrix, such as RIP @cite @cite , Coherence @cite , and RSC @cite . These analyses can potentially provide a simple approach for tuning parameters. However, they suffer from two issues: (i) Inaccuracy of the upper bounds derived for the risk of the final estimates usually lead to pessimistic parameter choices that are not useful for practical purposes. (ii) The requirement of an upper bound for the sparsity level @cite @cite , which is often not available in practice.
- @cite are the first to consider a stream of nodes to be placed onto partitions on the fly. Many heuristics are proposed and tested, from intuitive ones (considering balance) to more advanced ones (considering clustering coefficient); we re-implement the best performing one in this paper to compare it to our proposal. All approaches are one pass; a placed vertex is never moved afterward. Their paper assumes a full knowledge model, where the graph to be streamed has to be present on one machine prior to the execution of the proposed heuristics.
- The work of Dawid and Skene @cite laid a solid foundation in the field of crowdsourcing. Extension of the framework under a Bayesian setting were investigated by @cite @cite @cite .
- The model of Dawid and Skene implicitly assumes that a worker performs equally well across all items in a common class. In practice, however, it is often the case that one item is more difficult to label than another. To address this heterogeneous issue, @cite propose a minimax entropy principle for crowdsourcing. The observed labels are modeled jointly by the worker confusion matrices and item confusion vectors through an exponential family model. Moreover, it turns out that the probabilistic model can be equivalently derived from a natural assumption of objective measurements of worker ability and item difficulty. Such objectivity arguments have been widely discussed in the literature of mental test theory @cite @cite .
- Though the framework of Dawid and Skene has been widely used and well extended in crowdsourcing on the algorithmic side, there has been no theoretical work addressing convergence and optimality issues under the Dawid-Skene setting. To the best of our knowledge, the only exception is the work of @cite . They proposed a belief propagation algorithm using a Haldane prior A Haldane prior assumes each worker's ability is either @math or @math with equal probabilities. on workers' abilities and derive its rate of convergence under the one-coin model. They essentially reveal the exponent @math in the rate of convergence under the assumption that workers' abilities are bounded, while in our work both @math and @math play important roles on the exponent in various regimes. In addition, they consider the question of task assignment which is not addressed in our paper.
- In addition to showing that Dawid-Skene estimator achieves the minimax rate, we are also interested in studying whether its generalization also shares optimality. For example, what if we consider both worker confusion matrices and item confusion vectors @cite @cite . The technique used in this paper cannot be directly extended to that setting. We will consider this harder problem in our future work.
- GRE adopts the well-known vertex-centric programming model @cite . Essentially, it is reminiscent of the classic actor model @cite . Previously, the most representative vertex-centric abstractions are Pregel @cite and GraphLab @cite , whose comparison with GRE was summarized in Table. . Here we describe how GRE evolves.
- For shared memory environment, there are also numerous graph-parallel frameworks. Ligra @cite proposes an abstraction of edgeMap and vertexMap which is simple but efficient to describe traversal-based algorithms. GraphChi @cite uses a sliding window to process large graphs from disks in just a PC. X-Stream @cite proposes a novel edge-centric scatter-gather programming abstraction for both in-memory and out-of-core graph topology, which essentially, like GRE and PowerGraph , leverages vertex factorization over edges. GRE 's computation on local machine is highly optimized for massive edge-grained parallelism, based on technologies such as vLock @cite fine-grained data synchronization and FastForward @cite thread-level communication.
- MTurk's standard method of task self-selection has led to relatively few studies on task routing to better match workers to tasks, though work considered task assignment in other venues, such as Wikipedia @cite . Others have studied the cooperative refinement and task routing among on-line agents with regard to prediction tasks @cite . Bernstein at al. @cite investigate task routing in terms of real-time crowdsourcing. Though informative, these studies do not address finding strong candidates for a particular task from a task requester's viewpoint. Other work on task markets chains together of different worker competencies for problem solving @cite .
- @cite present a task assignment model based on random graph generation and a message-passing inference algorithm, in order to route tasks to crowd workers under homogeneous labeling tasks. @cite attempt to generalize this model to allow heterogeneous tasks by applying on-line primal-dual techniques. However, neither study answers the question of how to predict the unobserved workers' performance, which is critical to task routing in practice. @cite consider a related task routing task that seeks to engage people or automated agents to both contribute solutions and route tasks onward.
- Jung and Lease @cite study MF methods to improve the quality of crowdsourced labels, using a PMF model to infer unobserved labels in order to reduce the bias of the existing labels. They do not consider task routing. investigate matrix completion for crowdclustering, and more recently inferring user preferences @cite @cite . @cite consider workers' task selection preferences and propose a task recommendation model based on PMF. However, they motivated their approach on conceptual grounds and did not evaluate it. Most recently, @cite investigate task routing of multiple tasks across a common pool of workers.
- In the domain of ODE models, there exist several analytic methods for effective analysis under parameter uncertainty. They build on static analysis (stoichiometric analysis, flux balance analysis) as well as dynamic numerical methods (simulation, monitoring by temporal formulae, sensitivity analysis) implemented in tools (e.g. @cite @cite @cite ). Robustness analysis with respect to functionality specified in terms of temporal formulae has been introduced recently @cite @cite . There exist two major approaches how to define and analyse robustness. If only parameters of the model are perturbed, we speak of a to robustness. This approach has been explored by Fainekos & Pappas @cite , further extended by A. Donz ' e et al @cite and implemented in the toolbox Breach @cite . Another option could be to perturb the model structure i.e. the reaction topology, as this is done in many gene knock-out biological experiments. Such changes are in principle discrete and the problem of robustness computation for such perturbations would reduce to solving many individual instances of the same problem for each discrete topology. However identifying model behaviour shared among individual perturbations can lead to more efficient analysis @cite .
- Finally, the SINGLE algorithm is formally related to the Joint Graphical Lasso (JGL) . The JGL was designed with the motivation of improving network inference by leveraging information across related observations and data sets. However, while the JGL focuses on stationary network estimation the SINGLE algorithm is designed to estimate dynamic networks. This manifests itself in two main differences to the overall objective functions of each of the algorithms. Firstly, the SINGLE algorithm only employs the Fused Lasso penalty as the Group Lasso penalty proposed in @cite cannot be used in the context of temporal homogeneity. This is due to the fact that the Group Lasso penalty encourages all coefficients to either be zero or non-zero in unison and therefore ignores temporal behaviour. Secondly, while both algorithms contain a Fused Lasso penalty the nature of these penalties are vastly different. In the case of the JGL there is no natural ordering to observations and therefore are present between all networks (i.e., the penalty is of the form @math ). This is not the case in the SINGLE algorithm where there is a chronological ordering. This results in a penalty of the form @math .
- The text recognition problem has been addressed in the literature on multiple levels: character recognition @cite @cite , word recognition @cite @cite and text detection @cite @cite . Most previous works focused on a single stage of the pipeline, with few looking into the end-to-end systems, namely @cite @cite @cite .
- The character recognition problem is a classification problem that is generally addressed with the use of strong classifiers such as Convolutional Neural Nets (CNNs) @cite @cite , deformable parts models @cite or manually-engineered feature-extraction followed by a classifier @cite .
- The word recognition problem is, much like phone recognition and handwriting recognition, a sequence recognition problem. Previous works have addressed this problem using CNNs @cite , Conditional Random Fields (CRFs) @cite @cite and Pictorial Structures (PS) @cite . Most of the work in this area has relied on segmentation-free lexicon-dependent approaches. The use of the lexicons helps tackle the high confusion inherent in the text recognition problem. However, despite the argument for the validity of task-specific lexicon use in @cite , it is clear that we ultimately wish to recognize text with a very general lexicon. To do so, we require word recognizers that scale well in the size of the lexicon. The works of @cite @cite @cite are the only works we know of that show how their methods scale with lexicon size.
- The text detection problem is defined such that, given a natural image, the goal is to output bounding boxes on all words in the image. Abstractly speaking, the problem is an instance of the object detection problem, followed by segmenting text regions into their constituent words. Previous works investigated different approaches for text detection, typically trading off precision, recall, training time and time consumed for manually designing features. Pre-trained CNNs @cite @cite applied in a multi-scale sliding window fashion are highly accurate but very time consuming. Viola-Jones style classifiers remedy the slowness in CNNs, but have long training times and require manually-engineered features @cite @cite . Alternative methods that cleverly exploit the nature of text such as Maximally Stable Extremal Regions (MSERs) @cite and Stroke Width Transform @cite generally have lower accuracy but are fast to compute. Such methods were used successfully to detect text as in @cite @cite @cite .
- Another motivating application of this work is containment of rumor or misinformation diffusion in large social networks. Research on this topic is still at its infancy. Budak considered the problem of limiting the spread of misinformation in social networks @cite . Their approach was to convince a small set of users in the online social network to spread good'' rumors that cancel out the influence of bad ones. Convincing people to spread good'' rumors in social networks, albeit an interesting idea, may not be feasible in practice. The strategy we consider in this work, however, does not require involvement of individual users.
- The percolation theory has established the critical threshold for wide-scale epidemic spreading and has been widely applied to study epidemic spreading in diverse network structures, such as small-world networks @cite , heterogeneous networks @cite , and sensor networks @cite . In the context of scale-free networks, selectively immunizing those highly connected nodes is an effective approach to slowing down epidemic spread in such networks @cite @cite . In both problems of containing social-based malware and misinformation spread in social networks, however, the key challenge that faces node immunization is the difficulty of interacting with individual users due to the distributed nature of social networks. An alternative approach would be to achieve node immunization by sanitizing all messages that come to or come from those nodes to be immunized. In our problem, however, this may not be the most cost-effective approach because highly connected nodes could generate a large number of communication messages.
- The or FICAS @cite is a distributed data-flow architecture for composing software services. Composition of the services in the FICAS architecture is specified using the Compositional Language for Autonomous Services (CLAS), which is essentially a sequential specification of the relationships among collaborating services. This CLAS program is then translated by the build-time environment into a control sequence that can be executed by the FICAS runtime environment.
- In previous work @cite @cite we proposed , a proxy-based architecture based on a centralised control flow, distributed data flow model. Our prior work @cite has also focused on decentralised service choreography models. @cite , an architecture for of composite Web services defined in BPEL is proposed.
- Workflow partitioning is an approach to divide a workflow into several sub-workflows, which are then executed on different sites. The most mature workflow partitioning mechanisms are contained in Pegasus @cite : a partitioner component decomposes an abstract workflow into smaller sub-workflows which are then mapped onto computational (usually Grid) resources.
- @cite submits jobs represented as a DAG to a Condor pool of resources. Intermediate data are not transferred via a workflow engine, instead they are passed directly from vertex to vertex. DAGMan removes the workflow bottleneck as data are transferred directly between vertices in a the DAG. @cite is an open-source problem solving environment. It is designed to define, process, analyse, manage, execute and monitor workflows. Triana can distribute sections of a workflow to remote machines through a connected peer-to-peer network. @cite is a middleware product that supports the exposure of data resources on to Grids. This middleware facilitates data streaming between local OGSA-DAI instances.
- A Newtonian version of the future stability theorem of @cite was proved in 1994 by Brauer, Rendall, and Reula @cite . They studied Newtonian cosmological models equipped with a positive cosmological constant and containing a perfect fluid verifying the equation of state @math where @math is the Newtonian mass density, and @math are constants. They proved that the uniform quiet fluid states of constant positive density are globally future-stable.
- Detection is the most standard way to deal with security and privacy problems. There are many works in the area and many different ways to detect malware. For example, @cite presented MyPageKeeper, a Facebook application that protects Facebook users from socware. MyPageKeeper is based on a Support Vector Machine (SVM) classifier that uses a main feature specific keyword occurrence in a post made by an application. MyPageKeeper was able to identify socware posts and alert the user with 97 Defensio @cite is a Facebook application from Websense that monitors posts in a users profile and determines whether they are legitimate, spam, or malicious. Defensio also uses SVM to detect malicious posts and in addition they could delete them. Abu- @cite used Defensio as a platform to study malicious links. They found that about 9 2012, Rahman, et al @cite improved his previously mentioned work. Rahman, et al developed the FRAppE: A tool that can identify malicious applications by using the application information as features. Some examples include the number of permissions required, the domain reputation of redirect URI, and others. FRAppE can detect malicious applications with 99.5
- The term polygon simplification'' is also used in connection with operations that are used to compress polygons and to reduce noise in the representation. A well-known algorithm to smoothen polygonal chains is the Douglas-Peucker algorithm @cite . @cite address several variations of the approach to fatten existing polygonal chains and approximate the chain inside the fattened region. There also exists work on constructing simple polygons that contain given ones and fulfill certain properties, as a generalization of the convex hull of simple polygons @cite ; the main objective there is to approximate the shape.
- There are various efforts in the literature that addressed fairness in the general domain of wireless networks @cite @cite @cite . Authors in @cite proposed a set of algorithms for the assignment of max-min fair shares to nodes in a wireless ad hoc network. The work in @cite proposed schedulers that provide deterministic and probabilistic fairness by decoupling throughput optimization and fairness guarantees as two distinct blocks. Authors in @cite proposed a fair framework for ad hoc wireless networks via a contention resolution algorithm. However, none of these works takes into account the unique features of the CRN concept such as the requirement to ensure PU protection; therefore, they are unsuitable for implementation in CRNs.
- The work in @cite , for instance, concentrated on the spectrum overlay paradigm by modeling the interference as a multi-channel contention graph (MCCG). Authors in @cite proposed a spectrum decision framework for centralized CRNs by considering application requirements and current spectrum conditions. Unlike our work, the work in @cite focused on inter-cell spectrum sharing and proposed a joint spectrum and power allocation framework. Both @cite and @cite are based on the spectrum overlay paradigm since they handle spectrum availabilities as a binary decision, i.e., a spectrum band is either available or not. Our work is distinct in principle from all works in the literature about overlay spectrum sharing because we take into account the maximum allowed interference power at PUs.
- Most of the scheduling works about underlay spectrum sharing in the literature focused on rate and power allocation without emphasizing other criteria such as frequency allocation, possibly having multiple antennas, ensuring reliable communication, and joint temporal throughput fairness. The work in @cite proposed a joint scheduling and power control scheme for centralized CRNs. Authors in @cite focused on capacity maximization in multi-hop CRNs. None of the works in @cite @cite focused on issues such as fairness or multiple antennas.
- Our proposed scheduler in this paper has resemblance with OFDM systems. In fact, this resemblance is not surprising because OFDMA is seen as a promising candidate technology for future CRNs @cite . For instance, authors in @cite proposed a dynamically configurable framework that combines maximum rate, max-min fairness, and proportional fairness policies to maximize the number of satisfied users, whereas the work in @cite derived opportunistic scheduling policies with utilitarian fairness for OFDM systems. The work in @cite proposes a resource allocation algorithm for multi-user OFDM-based cognitive radio systems with proportional rate constraints. None of these works consider joint temporal and throughput fairness or the case where users potentially have multiple antennas.
- * Non Photo-realistic Rendering Applications. The inspiration for the initial version of Zahir was the software @cite (RTSC). RTSC renders line drawings from 3D surfaces in a beautiful and optimal way. However, because it's a demonstration program, it's limited to static white objects (the models don't load any material or texture information). Those limitations inspired us to start working on a more flexible framework.
- Another similar tool is @cite . FreeStyle is a stylized line-renderer for 3D models. It's different from RTSC, because while RTSC extracts contours as iso-curves from surfaces, FreeStyle extracts silhouette and crease edges. The extracted edges are connected and stylized using procedures defined by the user. This software produces pretty pictures and is very customizable, but is not suited for interactive applications. On the other hand, it's very flexible and it can be used by artists because the styling can be written in the Python programming language.
- Several researchers have attempted to design a secure password manager. However, none of them has considered the effect of a network attacker. PwdHash transparently produces a different password for each site by using cryptographic hash functions @cite , hence preventing a web attacker from compromising multiple accounts from the same user using the same password. Passpet aims to protect the user's login credentials from phishing attackers by associating each trusted website with user-assigned labels @cite . Internet users' password strength, as well as their password management habits, has also been extensively studied in previous literature @cite @cite @cite @cite . Most of the existing research has found that the majority of passwords on the Internet are weak and that users tend to reuse existing passwords. The attack described in our work does not target the weaknesses of these web passwords, but rather, it exploits a vulnerability in the design of several commercial password managers.
- The closest model to our model is the colored packets with deadline problem @cite . In that model we are given a sequence of incoming packets. Each packet is characterized by a color and a deadline (all packets have the same value). The goal is to find a schedule that maximizes the number of packets that were transmitted before the deadline, such that there is a transition time-slot between the transmission of packets of different colors. An algorithm with competitive ratio of @math and hardness result of @math are shown in @cite , when @math is the number of colors and @math is the minimum of the difference between the expiration and the arrival time of packets.
- In the bounded delay model @cite we are given a sequence of incoming packets. Each packet is characterized by a value and a deadline. The goal is to find a schedule that maximizes the number of packets that were transmitted before the deadline. The earliest deadline first (EDF) strategy is known to achieve an optimal throughput when all the packets have the same value. For arbitrary values the following results are known. A deterministic competitive algorithm of about @math @cite @cite and a hardness result of @math @cite @cite @cite . A randomized competitive algorithm of @math @cite @cite and a hardness result of @math @cite . For additional papers related to the bounded delay model see @cite @cite @cite .
- In the FIFO queue model @cite we are given a sequence of incoming packets. The packets are placed in a FIFO queue with bounded buffer size. The challenges for an online algorithm are to decide whether to accept or discard arriving packet, and to choose a queue for transmission in each time unit. The problem was studied in @cite @cite @cite @cite @cite . Another related problem is the sorting buffer problem @cite . In that problem we are given a server with unbounded capacity and an incoming sequence of requests. Each request corresponds to a point in a metric space. The goal is to serve all requests while minimizing the total distance traveled by the server. This problem can be interpreted as a multi-port device problem. This model was studied in @cite @cite @cite @cite @cite .
- Firstly, server based checkpointing strategies are subject to single point of failure @cite . To address this issue, multi-server checkpointing strategies have been introduced @cite . However, these centralized server strategies tend to be less scalable on complex and heterogeneous environments.
- Thirdly, an attempt to checkpoint a large process involves large overheads and greater time to write the checkpoint to a stable storage system. In order to mitigate this issue, distributed commit protocols @cite and diskless checkpointing @cite strategies based on memory and processor redundancy have been developed. These strategies tend to be ineffective if the checkpoint size and the number of nodes in the distributed system is large.
- Fourthly, most checkpoint strategies require a cold restart, that is, a complete reload of all processes associated with the parallel job @cite . In this case, processors that did not suffer a failure might also require a reload of the process executing on it.
- Fifthly, in mobile agent technology, checkpointing can prevent the loss of an agent and prevent blocking. In this case single failure does not prevent the progress of a mobile agent execution. However, checkpointing does not satisfy the exactly-once property, leading to multiple executions of an agent @cite .
- On an implementation level, checkpointing based fault tolerance has opened avenues for implementing middleware approaches that aim to add an additional interface or a sandwich layer between hardware and software layers @cite . To improve efficiency of checkpointing, additional checkpointing strategies over custom implementations have been adopted in such middleware layers like MPI (Message Passing Interface), a few of which are referenced here.
- In @cite , the concept of automatic checkpointing is introduced in LAM MPI middleware. The strategy records the context of an application periodically, identifies failed nodes and restarts MPI processes only on failed nodes, hence allowing continuity of the executing application by taking advantage of the computing done previously.
- In @cite , DREAM (Dynamic Robust Embedding Allocation Middleware) based on Robust MPI (R-MPI) as a library component is proposed. In @cite , to address challenges in diskless checkpointing, algorithm-based fault tolerance (ABFT) using Fault Tolerant MPI (FT-MPI) is introduced. Recovery from failure in the middle of computations is performed by maintaining a checksum relationship.
- In @cite , to address the scalability issue of checkpointing in MPI applications, an asynchronous replication strategy is introduced that distributes replication overhead over all participating nodes in the computation.
- In @cite , fault tolerant MPI comprising a replicated system controller, a node controller and checkpoint server tested on a parallelized weather model is introduced. The fault tolerant version is designed to address single point failures, ensure consistency of checkpoint files and robustness of fault detection hierarchy.
- In @cite , for computationally intensive applications using MPI, two approaches for checkpoint based fault tolerance is proposed. Firstly, segment-level solution, an extension of a checkpoint library for sequential codes. Secondly, variable-level solution, a manual solution determined by the programmer that inserts safe points and specifies data to be stored during checkpointing into program code.
- In @cite , an extension to MPI is proposed that consists of two steps to achieve fault tolerance. Firstly, failure diagnosis, detection of the location of a failed component. Secondly, failure recovery, a step towards reassigning tasks of a failed component to fully functional system nodes.
- Research on multi-agent based fault tolerance is reported in @cite @cite @cite @cite @cite . Though research has been pursued on multi-agents focusing on fault tolerance, it is surprising that there has been little effort towards extending and implementing such ideas for large scale parallel computing systems.
- Enabling many processors to access different parts of a single array is a cornerstone of data parallel programming models. OpenMP @cite is the de-facto standard for shared-memory multiprocessing. Its API offers various data parallel directives for handling the access to arrays, e.g. in conjunction with parallel-for loops. Threading Building Blocks @cite is a C++ library which offers a wide variety of algorithmic skeletons for parallel programming patterns with array manipulations. Chapel @cite is a parallel programming language for high-performance computation offering concise abstractions for parallel programming patterns. Data Parallel Haskell @cite implements the model of nested data parallelism (inspired by NESL @cite ), extending parallel access also to user-defined data structures. In difference to our work, these approaches focus on efficient computation but not on safety guarantees for concurrent access, which is our starting point.
- The concept of views is an application of readers-writers locks first introduced by Courtois, Heymans and Parnas @cite , tailored to the concept of slices.
- There has been a body of work on studying the Ulam distance between strings @cite @cite @cite @cite . For permutations, the Ulam distance is twice the size of the complement of the longest common subsequence. Note that Ulam distance between a permutation and the identity permutation is basically the distance to monotonicity. There has been a recent sublinear time algorithm for approximating the Ulam distance between two permutations @cite . We again note that the previous techniques for distance approximation play a role in these results. Our results may be helpful in getting better approximations for these problems.
- Due to increasingly overloaded mobile 3G networks, the offload of non-realtime traffic has recently raised significant academic interest @cite . Although manual traffic offload has de facto been a success story (for up to 65
- For the automation of traffic offload, a feasible selection of traffic seems to be required. Siris and Kalyvas @cite have followed up on this idea by offloading delay tolerant traffic based on a designed mobility prediction. Thus, a concentration on nomadic users for mobile traffic offload, e.g., in home scenarios, combined with mass deployment of accessible Wi-Fi hotspots may be of highest interest. In addition, application-awareness seems to be necessary in order to optimally target user requirements, e.g., through Quality of Experience (QoE) trials for applications which have important delay constraints Apple Not Throttling iPhone or iPad Cellular Throughput via Carrier Bundles'': http: www.anandtech.com show 7037 apple-not-throttling-iphones-ipads-cellular-throughput-via-carrier-bundles- , last accessed: June 16, 2013 .
- Differentiated by the following list of driving actors, @cite have compiled a set of Wi-Fi Femto offload Value Network Configurations (VNCs): mobile operators, broadband access operators (fixed line), combined fixed and mobile operators, the access aggregator role (e.g., FON http: www.fon.com , last accessed: June 14, 2013 ), service providers and device manufacturers, as well as site venue owners. Investigating market convergence effects, the present paper will concentrate on the fixed mobile operator and access aggregator cases. These VNCs have been elaborated by @cite in order to illustrate two-sided market issues towards a successful market evolution, e.g., w.r.t. required platform subsidies. Cost aspects for offloading base station traffic to femto cells have specifically been targeted by @cite . However, to the best of our knowledge, the conflict of business interests and market challenges involved in offloading traffic from mobile networks to unlicensed spectrum Wi-Fi solutions has not yet been targeted in literature.
- The problem of video delivery optimization in wireless networks has been studied in many works, for instance, see @cite @cite @cite @cite @cite @cite @cite @cite which utilize extensions of Network Utility Maximization (NUM) framework (see @cite ). The main focus of @cite and @cite is real-time interactive video which present the challenge of meeting strict delivery deadlines. Papers @cite and @cite study video delivery optimization in wireless networks considering simpler QoE models, and do not explicitly incorporate rebuffering (nor cost) into their respective optimization frameworks, and instead control rebuffering through network congestion control. Using static QoE models, @cite and @cite study the resource allocation component for video delivery accounting for user dynamics. A major weakness of the aforementioned papers is the limited nature of the associated QoE models (that are essentially just the mean quality) and their lack of flexibility in managing incorporating user preferences related to rebuffering and cost.
- The purpose of refresh policies is to recrawl known pages that have changed in order to keep a search engine's index fresh. Usually, such policies are based on some model, which predicts changes on Web pages. In pioneering works @cite @cite @cite @cite @cite , the analysis of pages' changes was made in the assumption of a time-homogeneous Poisson process, i.e., it was assumed that the pages change rate does not depend on time. However, in @cite , it was noted that there are daily and weekly trends in the pages change rate. Then, a history-based estimator, which takes such trends into account, was proposed in @cite . A more sophisticated approach based on machine learning is used in @cite , where the page's content, the degree of observed changes and other features are taken into account.
- For our specific task, refresh policies can be used to find links to ephemeral new pages, that appeared on already known pages (content sources). So, pages changes are relevant for us only if new links to such new pages can be found. Interestingly, this simplifies the estimation of pages change rate as one can easily understand, given two successive snapshots of a page, that two new links appeared, while it is much harder to know if the page's text changed once or twice. This fact allows us to use a simple estimator for the rate of new links appearance, which reflects timely trends. Of course, more sophisticated methods (e.g., using machine learning @cite ), can be applied here, but it was out of focus of the current paper.
- One of the important early papers on inverse boundary value problems is by Calder 'on @cite . He considered an isotropic body @math from which one would like to deduce the electrical conductivity @math by doing electrical measurements on the boundary. If we keep the voltage @math fixed as @math on the boundary, then the stationary state of @math can be modeled by the boundary value problem The weighted normal derivative @math is the current flux going out of @math . Calder 'on asked whether knowing the boundary measurements, or Dirichlet-Neumann map @math , is enough to determine the conductivity @math inside the whole domain @math . This is called the . inverse problem!Calder 'on He showed the injectivity of a linearized problem near @math .
- Sylvester and Uhlmann solved the problem in dimensions @math at least three for smooth conductivities bounded away from zero @cite . They constructed solutions!complex geometric optics|textbf , that is, solutions of the form where the complex vectors @math satisfy where @math are perpendicular vectors satisfying @math . Using a well-known orthogonality relation for the potentials @math and @math , called the Alessandrini identity @cite Alessandrini's identity , they got and after taking @math they saw that the Fourier transforms of @math and @math are the same, so the potentials are so too. Note that the only part that requires @math in this solution is the existence of the three vectors @math .
- There are also some results for the inverse boundary value problem of the Schr "odinger equation whose potential is not assumed to be of the conductivity type. Jerison and Kenig proved, according to @cite , that if @math with @math , @math , then the Dirichlet-Neumann map @math determines the potential @math uniquely. The case @math was open until the paper of Bukhgeim. In @cite , he introduced new kinds of solutions to the Schr "odinger equation, which allow the use of stationary phase. This led to an elegant solution of this long standing open problem. There is a point in the argument that requires differentiability of the potentials. Imanuvilov and Yamamoto published the paper @cite in arXiv after the writing of this thesis. They seem to have fixed that problem and hence proven uniqueness for @math , @math . non-smooth potential L@ @math potential|see non-smooth potential
- Some more recent results in two dimensions have concerned partial data, stability and reducing smoothness requirements for the conductivities and potentials. Notable results of partial data include Imanuvilov, Uhlmann, Yamamoto @cite and Guillarmou and Tzou @cite . In the first paper the authors consider the Schr "odinger equation in a plane domain and in the second one on a Riemann surface with boundary. The results of both papers state that knowing the Cauchy data on any open subset on the boundary determines the potential uniquely if it is smooth enough.
- Stability seemed to be proven first for the inverse problem of the conductivity equation. Liu @cite showed it for potentials of the conductivity type. Barcel 'o, Faraco and Ru 'iz @cite showed stability for H "older continuous conductivities. Clop, Faraco and Ru 'iz generalized it to @math , @math , in @cite . For the Schr "odinger equation, there's the result of Novikov and Santacesaria for @math potentials in @cite .
- Lastly, we cite very briefly some reconstruction methods reconstruction . This paragraph is certainly very incomplete as reconstruction was not the focus of the thesis. Nachman gave the first result for the conductivity equation for @math in @cite and later for @math in @cite . In the recent paper @cite , the authors show a numerical reconstruction method for piecewise smooth conductivities in 2D. For a more in-depth survey, see the introduction in that same paper. The case of the Schr "odinger equation in the plane seems to be more elusive. Bukhgeim mentioned a reconstruction formula at the end of @cite , but as far as we know, there are no published numerical methods for reconstructing the potential in 2D. There is a reconstruction formula using only the boundary data explicitly in @cite though.
- Online social networks, in general, have been studied in detail by various researchers in the computer science community. conducted a large scale measurement study and analysis of Flickr, YouTube, LiveJournal, and Orkut @cite . Their results confirmed power-law, small-world, and scale-free properties of online social networks. In a more recent work by , authors performed a detailed analysis of the Google+ network, and identified some key differences and similarities between Google+, and existing social networks like Facebook, and Twitter @cite . performed a large-scale analysis of the entire Facebook social graph and found that 99.91 showed that the value had dropped to 3.74 degrees of separation in the entire Facebook network of active users.
- Although there has been a lot of work done on the network structure, and user characteristics on various social networks, to the best of our knowledge, little work has been done on image-based social networks; in particular, Pinterest. @cite used Cultural Analytics visualization techniques to study 550,000 images taken by users of the social photo sharing application, Instagram. Authors of this work compared the images from New York City and Tokyo, and found differences in local color usage, cultural production rate, etc. harvested geo-referenced and tagged metadata associated with 8 million Flickr images, and considered how large numbers of people named city core areas @cite . Authors of this work exploited the fact that the terms used to describe city centers, such as Downtown, are key concepts in everyday or vernacular language. The study covered six cities around the world, viz. Zurich, London, Sheffield, Chicago, Seattle, and Sydney.
- Parameter estimation for non-identical Poisson distributions has been studied in the context of Generalized Linear Models (GLMs). However, our model is inherently different from the exponential family of GLM models that has been studied in @cite @cite @cite @cite . In particular the GLM model corresponding to the Poisson distributed data studied in the literature has the following form: Therefore, the log likelihood has the form: In contrast, in the setting we are interested in, the observations are modeled as follows: and the log likelihood function has the form:
- More generally @cite describes a unified framework for analysis of regularized @math estimators in high dimensions. They also mention extension of their framework to GLMs and describe strong convexity" of the objective function as a sufficient condition to obtain consistency of M-estimators under Model I. As we described this requirement of strong-convexity is not consistent with our model. In addition the statistical aspects in that work requires that the components of the sensing matrix be characterized by , which we do not require here.
- Statistical guarantees for sparse recovery in settings similar to model II have been provided in @cite @cite @cite in the context of photon limited measurements. They assume that the observations are distributed as follows @math where elements of the signal @math and sensing matrix are positive, and the sensing matrix satisfies the so-called Flux Preserving assumption: @math
- The latter assumption arises in some photon counting applications, like imaging under Poisson noise, where the total number of expected measured photons cannot be larger than the intensity of the original signal. The upper bound on reconstruction error of the constrained ML estimator is given in the paper @cite . Surprisingly, the upper bound scales linearly with the number of measurements. However, this sounds reasonable under the Flux Preserving assumption. In fact this behavior is due to the fact that for a fixed signal intensity, more measurements lead to lower SNR for each observation. As a result, unlike conventional compressive sensing bounds, the estimates do not converge to the ground truth with increasing the sample size. Nevertheless, Flux Preserving constraint does not arise in our setting and consequently the application and methods of analysis are different.
- There are quite a few interesting system level and network architectural level studies of VoD streaming services. An interesting work showed the correlation between user engagement and video quality @cite . They conclude that the time spent in buffering (in this paper we call freeze'') has the largest impact on user engagement. The work @cite proposes to use a global view of client and network conditions to dynamically optimize the video delivery to achieve better QoE for Internet video services. These works help understand Internet-based VoD systems in general. Our work focus on the early departure behavior (not affected by QoE), and the improvement of rate allocation strategy based on real measurement, which is a specific problem not considered in the above works.
- The early departure behavior has been considered in some previous works. For example, it was studied as a activity for users to shop for videos they like @cite , based on data collected on a campus network. Another work @cite reported statistics of video playback aborts - 60 Other works studied access patterns by users. For example, user arrival patterns are analyzed and modeled in @cite . Some works focus on the transition probability of different user behaviors. In @cite , the @math -means technique is used to retrieve and cluster user behaviors using a Markovian model based on a movie trailer database. Another work @cite probes the relationship between several types of user behavior and uncovers that the behavior of one individual user in a video streaming session has strong correlation with the user's behaviors in previous streaming sessions. While these works tell us more about user behavior, their results are not particularly helpful in designing our rate allocation algorithms.
- Youtube is one of the largest video provider in the world, and attracted some academic measurement studies. @cite investigated the application flow control technique utilized by YouTube. They reveal and describe the basic properties of YouTube application flow control, which is block sending. It also showed that block sending is widely used by YouTube servers. The authors also examined how the block sending algorithm interacts with the flow control provided by TCP. Paper @cite studied the network characteristics of the two most popular video streaming services, Netflix and YouTube. Paper @cite crawled the YouTube site for four months, collecting more than 3 million YouTube videos' data. It is reported that Youtube videos have noticeably different statistics compared to traditional streaming videos, ranging from length and access pattern, to their growth trend and active life span.
- * -0.2cm (Top) Curves show the ( @math ) number of distinct hash table indices (buckets) within a Hamming ball of radius @math , for different code lengths. With 64-bit codes there are about 1B buckets within a Hamming ball with a 7-bit radius. Hence with fewer than 1B database items, and a search radius of 7 or more, a hash table would be less efficient than linear scan. Using hash tables with 128-bit codes is prohibitive for radii larger than 6. (Bottom) This plot shows the expected search radius required for @math search as a function of @math , based on a dataset of 1B SIFT descriptors. Binary codes with 64 and 128 bits were obtained by random projections (LSH) from the SIFT descriptors @cite . Standard deviation bars help show that large search radii are often required. * -0.2cm
- The second problem is to find all codes in a dataset @math that are within a fixed Hamming distance of a query, sometimes called the Approximate Query problem @cite , or Point Location in Equal Balls (PLEB) @cite . A binary code is an @math -neighbor of a query code, denoted @math , if it differs from @math in @math bits or less. We define the @math -neighbor search problem as: find all @math -neighbors of a query @math from @math .
- One way to tackle @math -neighbor search is to use a hash table populated with the binary codes @math , and examine all hash buckets whose indices are within @math bits of a query @math ( , @cite ). For binary codes of @math bits, the number of distinct hash buckets to examine is As shown in Fig. (top), @math grows very rapidly with @math . Thus, this approach is only practical for small radii or short code lengths. Some vision applications restrict search to exact matches ( @math ) or a small search radius ( @cite @cite ), but in most cases of interest the desired search radius is larger than is currently feasible ( see Fig. (bottom)).
- Our work is inspired in part by the multi-index hashing results of Greene, Parnas, and Yao @cite . Building on the classical Turan problem for hypergraphs, they construct a set of over-lapping binary substrings such that any two codes that differ by at most @math bits are guaranteed to be identical in at least one of the constructed substrings. Accordingly, they propose an exact method for finding all @math -neighbors of a query using multiple hash tables, one for each substring. At query time, candidate @math -neighbors are found by using query substrings as indices into their corresponding hash tables. As explained below, while run-time efficient, the main drawback of their approach is the prohibitive storage required for the requisite number of hash tables. By comparison, the method we propose requires much less storage, and is only marginally slower in search performance.
- . Our work is also related to studies of the AL and related algorithms in conventional, centralized optimization. There is a vast literature on the subject, and many authors considered inexact primal minimizations (see @cite @cite @cite and the references listed in the following paragraphs.) Before detailing the existing work, we point to main differences of this paper with respect to usual studies in the literature. First, when analyzing inexact AL methods, the literature usually assumes that the primal problems use arbitrary initialization. In contrast, we initialize the inner primal algorithm with the previous primal variable. Consequently, our results and the results in the literature are different, the algorithms in the literature typically be convergent only to a solution neighborhood, e.g. @cite @cite . Second, except for recent papers, e.g., @cite @cite , the analysis of inexact AL is usually done with respect to dual sub-optimality. In contrast, we are interested in the primal sub-optimality measures. Third, convergence rates are usually established at the outer iteration level, while we--besides the outer iterations level--establish the rates in the number of iterations.
- . The ADMM method has been proposed in the 70s @cite @cite and has been since then extensively studied. References @cite @cite @cite show locally linear or superlinear convergence rates of AL methods. Reference @cite analyzes convergence of the ADMM method using the theory of maximal set monotone operators, and it studies its convergence under inexact primal minimizations. Recently, @cite @cite show that the ADMM method converges globally linearly, for certain more general convex costs than ours. (The most related work to ours on ADMM is actually the work on distributed ADMM in @cite @cite that we have already commented on above.)
- From this section onwards, following notation similar to @cite , we let @math denote the dataset of documents, with size @math , and @math the data dictionary of all unique words from documents in @math , with size @math . Given a document @math and a word @math , @math denotes the number of words in @math and @math indicates that the word @math is found in @math .
- In VSM, the data dictionary, consisting of all unique words that appear in at least one document in @math , is first constructed. Sometimes, @math -grams, which are phrases of words, are also included in the dictionary; however, the benefit of these additional phrases is still up for debate @cite . Given the data dictionary, each document can be represented as a vector in the real-valued vector space with dimension equaling the size of the dictionary. Two common methods for associating a document to a vector are explained below.
- The standard approach in literature for text categorization is that one, or more, feature selection or extraction technique is first applied to a data matrix, since the original data matrix is often extremely high-dimensional. A learning algorithm, independent of the dimension reduction process, is then used for classification @cite . The novel approach in this paper is that we consider a new classifier based only on extracted class specific words, which naturally reduces time complexity and the dimension of the dataset. In other words, the Domain-Specific classifier both performs dimension reduction and classifies, in consecutive and dependent steps.
- Locality was identified as a repair cost metric for distributed storage systems independently by Oggier @cite , Gopalan @cite and PaPailiopoulos @cite using different terms. In @cite , Gopalan introduced the concept of symbol locality of linear codes and established a tight bound for the redundancy in terms of the message length, the distance, and the locality of information coordinates. A generalized concept, i.e., @math locality, was addressed by Prakash @cite . It was proved in @cite that the minimum distance @math of an @math linear code @math is upper bounded by where @math and @math are the length and dimension of @math respectively. It was also proved that a class of codes known as pyramid codes @cite achieve this bound. Since an @math code is also an @math code, ) also presents an upper bound for the minimum distance of @math codes.
- Locality of general codes (linear or nonlinear) and bounds on the minimum distance for a given locality were presented in parallel and subsequent works @cite @cite . An @math code (systematic or not) is also termed a , and @math codes that achieve the minimum distance bound are called .
- It was proved in @cite that there exists optimal locally repairable linear codes when @math and @math . Under the condition that @math , a construction method of optimal locally repairable vector codes was proposed in @cite , where maximal rank distance (MRD) codes were used along with MDS array codes. For the special case of @math , Tamo @cite proposed an explicit construction of optimal LRCs when @math or @math Except for the special case that @math , no results are known about whether there exists optimal @math code when @math .
- Early work on attack graph solving relied on model checking techniques @cite @cite , with their inherent scalability restrictions; or on monotonicity assumptions @cite @cite @cite that are not able to express situations in which compromised resources are lost due to crashes, detection or other unforeseen circumstances.
- Due in part to its importance, there has been an active line of work on efficient optimization methods for solving and . Since the regularization term is non-smooth and hard to solve, many methods aim to solve the dual problem of : which has a smooth objective function with bounded constraints. @cite propose a block-coordinate descent method to solve the dual problem , by updating one row and column of @math at a time. They show that the dual of the corresponding row subproblem can be written as a standard Lasso problem, which they then solve by Nesterov's first order method. @cite follow the same strategy, but propose to use a coordinate descent method to solve the row subproblems instead; their method is implemented in the widely used R package called glasso . In other approaches, the dual problem is treated as a constrained optimization problem, for which @cite apply a projected subgradient method called , while @cite proposes an accelerated gradient descent method called .
- Other first-order methods have been pursued to solve the primal optimization problem . @cite apply Nesterov's first order method to after smoothing the objective function; @cite apply an augmented Lagrangian method to handle the smooth and nonsmooth parts separately; the resulting algorithm is implemented in the software package. @cite , the authors propose to directly solve the primal problem by a greedy coordinate descent method called . However, each coordinate update of has a time complexity of @math , which becomes computationally prohibitive when handling large problems. We will show in this paper that after forming the quadratic approximation, the time complexity of one coordinate update can be performed in @math operations. This trick is one of the key advantages of our proposed method, .
- The theoretical literature on high-dimensional statistical models is vast and rapidly growing. Estimating sparse linear regression models is the most studied problem in this area, and a source of many fruitful ideas. Limiting ourselves to linear regression, earlier work investigated prediction error @cite , model selection properties @cite @cite @cite @cite , @math consistency @cite @cite . . Of necessity, we do not provide a complete set of references, and instead refer the reader to @cite for an in-depth introduction to this area.
- The problem of quantifying statistical significance in high-dimensional parameter estimation is, by comparison, far less understood. Zhang and Zhang @cite , and B "uhlmann @cite proposed hypothesis testing procedures under restricted eigenvalue or compatibility conditions @cite . These papers provide deterministic guarantees but --in order to achieve a certain target significance level @math and power @math -- they require @math . The best lower bound @cite shows that any such test requires instead @math . (The lower bound of @cite is reproduced as Theorem here, for the reader's convenience.)
- In other words, the guarantees of @cite @cite can be suboptimal by a factor as large as @math . Equivalently, in order for the coefficient @math to be detectable with appreciable probability, it needs to be larger than the overall @math error. Here we will propose a test that --for random designs-- achieves significance level @math and power @math for @math .
- @cite develop a test for the hypothesis that a newly added coefficient along the LASSO regularization path is irrelevant. This however does not allow to test arbitrary coefficients at a given value of @math , which is instead the problem addressed in this paper. These authors further assume that the current LASSO support contains the actual support @math and that the latter has bounded size.
- Belloni, Chernozhukov and collaborators @cite @cite consider inference in a regression model with high-dimensional data. In this model the response variable relates to a scalar main regressor and a @math -dimensional control vector. The main regressor is of primary interest and the control vector is treated as nuisance component. Assuming that the control vector is @math -sparse, the authors propose a method to construct confidence regions for the parameter of interest under the sample size requirement @math . The proposed method is shown to attain the semi-parametric efficiency bounds for this class of models. The key modeling assumption in this paper is that the scalar regressor of interest is random, and depends linearly on the @math -dimensional control vector, with a sparse coefficient vector (with sparsity again of order @math . This assumption is closely related to the sparse inverse covariance assumption of @cite (with the difference that only one regressor is tested).
- Most recently, there is a considerable interest on two-stage approaches for multiple kernel learning @cite @cite which perform competitively as the one-stage approaches @cite @cite . In particular, Kar @cite studied generalization guarantees for the following regularization formulation for learning similarity (kernel) function: where @math is the positive linear combination of base kernels @math and @math is a regularization term which, for instance, can be the Frobenius norm or the @math norm. Specifically, Kar @cite established elegant generalization bounds for the above two-stage multiple kernel learning using techniques of Rademacher averages @cite @cite @cite and U-statistics @cite @cite . The empirical error term ) in our formulation ) is not a U-statistics term and the techniques in @cite @cite can not directly be applied to our case.
- Jain et al. @cite and Kar et al. @cite introduced an extended framework of @cite @cite in the general setting of supervised learning. The authors proposed a general goodness criterion for similarity functions, which can handle general supervised learning tasks and also subsumes the goodness of condition of @cite . There, efficient algorithms were constructed with provable generalization error bounds. The main distinction between these work and our work is that we aim to learn a similarity function while in their work a similarity function is defined in advance.
- The two-dimensional DGFF is particularly interesting because its continuum version (CGFF) is invariant under conformal transformations of the underlying domain. This offers a framework for analyzing scaling limits of certain critical models. For instance, the level sets of the DGFF on the triangular lattice can be linked with the Schramm-Loewner process SLE @math (Schramm and Sheffield @cite ), the height function associated with domino tilings scales to the CGFF (Kenyon @cite ) etc. A complication associated with the CGFF is that, by its scale-invariant nature, the field'' exists only as a random distribution on an appropriate function space.
- Let us now move to the subject of interest in the present paper, which is the behavior of the extreme values of the DGFF in the limit as @math . A particular aspect of this, the maximum has been studied very intensely. Indeed, Bolthausen, Deuschel and Giacomin @cite showed that @math . More recently, building on Bolthausen, Deuschel and Zeitouni @cite , Bramson and Zeitouni @cite proved that the family @math is tight when @math is as in . Finally, in a very recent development, Bramson, Ding and Zeitouni @cite proved the following result:
- : As Ofer Zeitouni informed us, although not explicitly derived in @cite , the representation of the limiting distribution as a Laplace transform of a random variable @math , i.e., formula , can be extracted from their work as well.
- In two other recent papers, Ding @cite and Ding and Zeitouni @cite have studied the tails of the maximum. Specifically, for the upper tail they derived for some @math . For the lower tail, they got the estimates for some @math . The recent work of Bramson, Ding and Zeitouni @cite controls the asymptotic form of the upper tail of @math including the multiplicative constant. Indeed, recasting Proposition 2.2 of @cite into a simpler form, we get:
- The existing work has not been limited to the maximum @math only. Indeed, Ding and Zeitouni @cite have also studied the structure of the level sets close to the maximal value (or @math above) where @math . In particular, they derived exponential estimates (in @math ) on the size of @math and controlled distances between the points of @math . We restate these results nearly verbatim:
- Broadening our discussion to subjects that are not primarily concerned with the behavior of the extreme points, let us mention also the work of Daviaud @cite who studied the size of the set where the field exceeds a constant times @math . His principal result is that, for @math , and @math in probability. The extreme level sets (at levels of order @math ) thus exhibit a non-trivial fractal structure. The reader should notice the striking similarity with the level sets for @math independent Gaussians with variance @math .
- : In a very recent posting, Chatterjee, Dembo and Ding @cite show that an analogous result to holds for very general Gaussian fields.
- : In a recent posting @cite , Arguin and Zindy have extended some of their conclusions to the DGFF. Notwithstanding, the question of pure atomicity for @math remains open.
- A version of the Gibbs measure appears also in the studies of Gaussian multiplicative chaos by Robert and Vargas @cite , Allez, Rhodes and Vargas @cite and Duplantier, Rhodes, Sheffield and Vargas @cite @cite ; see Rhodes and Vargas @cite for a recent review of this subject going back to Kahane @cite . In particular, the papers @cite @cite are concerned with the construction of a which corresponds to the @math limit of (unnormalized) measure This is an object closely related to the @math -derivative of @math at @math . On the basis of various conjectural statements, the references @cite @cite predict the derivative martingale to appear in the place of our @math . In Theorem we thus prove a version of this prediction for the DGFF. (The apparent discrepancy in the factor @math in front of the variance in the exponents of and comes from the fact that the measure @math in has been scaled by a factor of @math .)
- : Since the first version of the present paper was circulated, two new papers have appeared dealing with log-correlated Gaussian fields (over continuum space) in any @math . First, a paper of Madaule @cite , where the Laplace transform representation was shown for the law of the maximum, and a paper by Acosta @cite where tightness of the maximum was shown under somewhat more general conditions than those of @cite .
- Building further upon this beautiful structure, Arguin, Bovier and Kistler @cite @cite @cite and independently A "id 'ekon, Berestycki, Brunet and Shi @cite have recently managed to control the full distribution of the extreme points of the set @math as @math . The limit point process is a cluster process associated with Gumbel law of intensity @math , again quite analogously to what we show (modulo the clusters) for the DGFF in Theorem . A key fact (proved in @cite ) is the separation of time scales: If @math and @math are close to @math , then the corresponding Brownian paths split either right at the beginning (i.e., at a time @math ) or or right at the very end (i.e., at a time @math ). The splittings in time @math give rise to a residual'' randomness in the problem; this is the origin of the random variable @math .
- In general, @math will grow linearly with @math but for properly centered and normalized splitting processes --- the so called boundary cases --- the growth of @math is sublinear. Here McDiarmid @cite showed that the correct order is @math while Bachmann @cite and Bramson and Zeitouni @cite proved tightness of @math under regularity conditions on the tail of the splitting process. Hu and Shi @cite and Addario-Berry and Reed @cite then established @math is tight. Pursuing the strategy that proved to be so useful for the BBM, Biggins and Kyprianou @cite showed the convergence of the corresponding derivative martingale @math while A "id 'ekon @cite established a representation for the limiting law of @math as the Laplace transform of @math .
- The overview of large-scale graph engines is presented in @cite , which contains graph systems designed to achieve different goals - from offline analytics system to online low-latency systems.
- @cite provided a theoretical comparison of BSP and MapReduce models. In terms of graph processing, they noticed, that Breadth First Search algorithm (for the shortest path computation) cannot be efficiently implemented by means of the MapReduce model. In this paper, we go forward and focus more on an empirical comparison for the real world data sets, using available implementations as well as evaluation for additional graph problem - collective classification. The general conclusions remain the same: BSP usually appears to be better model for solving graph problems than MapReduce. The results included in this paper provide quantitative analyses supporting that statement.
- Many pricing mechanisms have been proposed to manage quality of services (QoS) in networks, see e.g., surveys @cite @cite @cite . For instance, in @cite , Honig and Steiglitz propose a usage-based pricing mechanism, and analyze it using a model with delay-sensitive users. Their results show how to find the price that maximizes the provider's revenue by solving a fixed-point equation. A similar model is used in @cite where Ba s ar and Srikant analyze the many-users limit. They show that, as the number of users increases, the provider's revenue per unit of bandwidth increases and conclude that this gives providers an incentive to increase their network capacity. In a number of papers, e.g., @cite @cite @cite , pricing mechanisms based on multiple classes of customers with different priorities are proposed and analyzed in terms of equilibrium achieved and optimal price per class. In @cite @cite , Shen and Ba s ar investigate the performance of non-linear pricing in a model similar to @cite and find an improvement of up to 38 contrast, in this paper, we investigate linear pricing mechanisms that leverage the time variability of user demand using a single priority class.
- A few papers have proposed mechanisms with prices dependent on congestion levels. In @cite , Paschalidis and Tsitsiklis propose a congestion-based pricing mechanism in the context of loss networks (i.e., phone). They provide a dynamic programming formulation of the revenue maximization problem and of the welfare maximization problem. Then, they show that this dynamic congestion pricing mechanism can be well approximated by a simpler static time-of-day pricing. An alternative mechanism called Trade & Cap'', was recently proposed by Londo n o, Bestavros and Laoutaris @cite . It works in two phases. First, users engage in a trading game where they choose an amount of bandwidth slots to buy for hard-constraints traffic. In the second phase, the remaining bandwidth is allocated to users as bandwidth, in proportion of their remaining buying power''. They show that this mechanism smoothes the aggregate demand to a certain level. In their model, users have a cost function that increases linearly with the total demand in a given slot. In this paper, we consider simpler one-phase pricing mechanisms with fixed parameters. Our model also differ from these papers in that users have elastic demand and their utility is an arbitrary function of the congestion level.
- For general restless bandit problems, there is a rich literature; however, very little is known about the structure of optimal policies for this class of problems in general. In @cite it has been shown that the Gittins index rule (see @cite , @cite for the definition of the Gittins index rule) is not optimal for a general restless bandit problems. Moreover, this class of problem is PSPACE-hard in general @cite . In @cite Whittle introduced an index policy (referred to as Whittle's index) and an indexability condition"; the asymptotic optimality of the Whittle index was addressed in @cite . Issues related to Whittle's indexability condition were discussed in @cite @cite @cite @cite @cite . For the two-state channel sensing problem, Whittle's index was computed in closed-form in @cite , where performance simulation of that index was provided. For some special classes of restless bandit problems, the optimality of some index-type policies was established under certain conditions (see @cite @cite ). Approximation algorithms for the computation of optimal policies for a class of restless bandit problems similar to the one studied in this paper were investigated in @cite .
- Reinforcement learning has long been applied to the robot soccer domain. For example, Andou @cite uses observational reinforcement learning" to refine a function that is used by the soccer agents for deciding their positions on the field. Riedmiller @cite use reinforcement learning to learn low-level soccer skills, such as kicking and ball-interception. Nakashima @cite propose a reinforcement learning meth-od called fuzzy Q-learning", where an agent determines its action based on the inference result of a fuzzy rule-based system. The authors apply the proposed method to the sce-nario where a soccer agent learns to intercept a passed ball.
- Arguably, the most successful application is due to Stone @cite . They propose the keepaway task", which consists of two teams, the keepers and the takers, where the former tries to keep control of the ball for as long as possible, while the latter tries to gain possession. Our solution to the soccer dribbling task follows closely the solution proposed by those authors to learn the keepers' behavior. Iscen and Erogul @cite use similar solution to learn a policy for the takers.
- Gabel @cite propose a task which is the opposite of the soccer dribbling task, where a defensive player must interfere and disturb the opponent that has possession of the ball. Their solution to that task uses a reinforcement learning algorithm with a multilayer neural network for function approximation.
- Kalyanakrishnan @cite present the half-field offense task", a scenario in which an offense team attempts to outplay a defense team in order to shoot goals. Those authors pose that task as a reinforcement learning problem, and propose a new learning algorithm for dealing with it.
- More closely related to our work are reinforcement learn-ing-based solutions to the task of conducting the ball (, @cite ), which can be seen as a simplification of the dribbling task since it usually does not include adversaries.
- There has also been recent research in the physics community on preventing cascading failures. In the model used for these results, each vertex in the network starts with a fixed capacity. When a vertex is deleted, some of its load'' (typically defined as the number of shortest paths that go through the vertex) is diverted to the remaining vertices. The remaining vertices, in turn, can fail if the extra load exceeds their capacities. Motter, Lai, Holme, and Kim have shown empirically that even a single node deletion can cause a constant fraction of the nodes to fail in a power-law network due to cascading failures @cite @cite . Motter and Lai propose a strategy for addressing this problem by intentional removal of certain nodes in the network after a failure begins @cite . Hayashi and Miyazaki propose another strategy, called emergent rewirings, that adds edges to the network after a failure begins to prevent the failure from cascading @cite . Both of these approaches are shown to work well empirically on many networks. However, unfortunately, they perform very poorly under adversarial attack.
- A responsive approach was followed by the authors in @cite @cite , which proposed a simple line algorithm for self-healing to maintain network connectivity. This algorithm has obvious drawbacks with regard to properties such as diameter maintenance but has served as a useful starting point for our research.
- Using mini-batches in stochastic learning has received a lot of attention in recent years. E.g. @cite reported experiments showing that applying small mini-batches in Stochastic Gradient Descent (SGD) decreases the required number of iterations. @cite and @cite gave an analysis of SGD with mini-batches for smooth loss functions. @cite studied SGD and accelerated versions of SGD with mini-batches and @cite studied SDCA with mini-batches for SVMs. @cite studied dual averaging in distributed networks as a function of spectral properties of the underlying graph. However, all of these methods have a polynomial dependence on @math , while we consider the strongly convex and smooth case in which a @math rate is achievable. It should be noted that one can use our results for Lipschitz functions as well by smoothing the loss function (see @cite ). By doing so, we can interpolate between the @math rate of non-accelerated method and the @math rate of accelerated gradient.
- It is interesting to note that most There are few exceptions in the context of stochastic coordinate descent . See for example @cite @cite of these papers focus on mini-batches as the method of choice for distributing SGD or SDCA, while ignoring the option to divide the data by features instead of by examples. A possible reason is the cost of opening communication sockets as discussed in sec:parallel .
- There are various practical considerations that one should take into account when designing a practical system for distributed optimization. We refer the reader, for example, to @cite @cite @cite @cite @cite .
- The more general problem of distributed PAC learning has been studied recently in @cite @cite . See also @cite . In particular, they obtain algorithm with @math communication complexity. However, these works consider efficient algorithms only in the realizable case.
- : Various research efforts have focused on understanding the behavior of several commercially deployed HAS systems. One such example is @cite , where the authors characterize and evaluate HTTP streaming players such as Microsoft Smooth Streaming, Netflix, and Adobe OSMF via experiments in controlled settings. The first measurement study to consider HAS streaming in the multi-client scenarios is @cite . The authors identify the root cause of the player's rate oscillation problem as the existence of on-off patterns in HAS traffic. In @cite , the authors measure behavior of commercial video streaming services, i.e., Hulu, Netflix, and Vudu, when competing with other long-lived TCP flows. The results revealed that inaccurate estimations can trigger a feedback loop leading to undesirably low-quality video.
- The restricted isometry property @cite @cite (or the related restricted eigenvalue @cite or compatibility conditions @cite ) have been used to establish guarantees on the estimation and model selection errors of the Lasso or similar approaches. In particular, Bickel, Ritov and Tsybakov @cite show that, under such conditions, with high probability,
- The same conditions can be used to prove model-selection guarantees. In particular, Zhou @cite studies a multi-step thresholding procedure whose first steps coincide with the Gauss-Lasso. While the main objective of this work is to prove high-dimensional @math consistency with a sparse estimated model, the author also proves partial model selection guarantees. Namely, the method correctly recovers a subset of large coefficients @math , provided @math , for @math . This means that the coefficients that are guaranteed to be detected must be a factor @math larger than what is required by our results.
- Also related to model selection is the recent line of work on hypothesis testing in high-dimensional regression @cite @cite . These papers propose methods for testing hypotheses of the form @math . In order to achieve a given significance level, they require --again-- large coefficients, namely @math (see @cite for a discussion of this point). In @cite , we investigate a hypothesis testing method that achieves any given significance level @math for @math , with @math a constant that depends on @math . Although the testing procedure can be used for general setting, the guarantee on its statistical power is provided only for some random Gaussian designs in an asymptotic sense. A very recent paper by van de Geer, B "u hlmann and Ritov @cite proposes a similar procedure and gives conditions under which the procedure achieves the semiparametric efficiency bound. Their analysis allows for general Gaussian and sub-Gaussian designs. However, it requires a sample size @math , namely the square of the optimal sample size.
- Let us finally mention that an alternative approach to establishing model-selection guarantees assumes a suitable mutual incoherence conditions. Lounici @cite proves correct model selection under the assumption @math . This assumption is however stronger than irrepresentability @cite . Cand 'es and Plan @cite also assume mutual incoherence, albeit with a much weaker requirement, namely @math . Under this condition, they establish model selection guarantees for an ideal scaling of the non-zero coefficients @math . However, this result only holds with high probability for a random signal model' in which the non-zero coefficients @math have uniformly random signs.
- Finally, model selection consistency can be obtained without irrepresentability through other methods. For instance @cite develops the adaptive Lasso, using a data-dependent weighted @math regularization, and @cite proposes the Bolasso, a resampling-based techniques. Unfortunately, both of these approaches are only guaranteed to succeed in the low-dimensional regime of @math fixed, and @math .
- The model of et al @cite provides valuable insight into how much performance can be achieved with regard to the typical limiting factors memory bandwidth and arithmetic throughput. This allows for a first assessment of how far the performance of the code at hand deviates from the maximum sustainable performance (the light speed'').
- Performance modeling and prediction especially in the context of LBM are an ongoing research topic of many groups in engineering and computer science @cite @cite . Auto-tuning was used, e.g., in @cite for a magnetohydrodynamics LBM. The ILBDC LBM code we use in the present work has been optimized previously @cite and its sustained performance is close to predictions from performance models @cite .
- Research in the direction of energy-saving hardware and software mechanisms focuses on models and algorithms for dynamic voltage and frequency scaling (DVFS) and dynamic concurrency throttling (DCT) @cite . With frequency scaling not only the computational performance of a core changes, but as a side effect also cache and memory bandwidth can be influenced @cite @cite . Any realistic modeling effort must take these effects into account. The ECM model @cite @cite is a refinement of the roof line model and allows a more accurate prediction of the intra-chip scaling properties of a parallel code. In @cite we have used it to model a specific variant of an LBM solver. Together with a simple power model derived for the Sandy Bridge chip we were able to explain the performance saturation and energy to solution characteristics of this solver. The analysis in this paper goes much deeper, and considers a more advanced propagation method. It also extends beyond the single chip to fathom the energy and performance properties of the solver in distributed-memory parallel runs on up to 128 nodes.
- With energy efficiency moving into the focus of HPC research, modeling the power dissipation of chips has received much attention in recent years. A power model similar to the one used in this paper has been introduced in @cite . It concentrates, however, on microscopic quantities such as the energy cost for transferring data or for performing floating-point operations, and does not address multi-core issues. It would be interesting to reconcile both models for a more holistic view on chip power, but this is left for future work.
- There is large body on work on regret style analysis for prediction. Numerous works including @cite @cite have examined the optimal amount of regret achievable with respect to two or more experts. A good reference for the results in this area is @cite . It is well known that in the case of static experts, the optimal regret achievable is exactly equal to the Rademacher complexity of the predictions of the experts (chapter 8 in @cite ). Recent works such as @cite @cite @cite have extended this analysis to other settings. Measures other than the standard regret measure have been studied in @cite The question of what can be achieved if one would like to have a significantly better guarantee with respect to a fixed expert or a distribution of experts was asked before in @cite @cite . Tradeoffs between regret and minimum payoff were also examined in @cite , where the author studied the set of values of @math for which an algorithm can have payoff @math , where @math is the payoff of the best arm and @math are constants.
- Numerous papers (for example @cite @cite @cite ) have implemented algorithms inspired from regret style analysis and applied it on financial and other types of data.
- Pictorial Structure Model (PSM) @cite is one of the most successful deformable models. A tree structure graphical model is used, and pairwise terms are based on the relative distances between corresponding body parts. @cite proposed a mixtures-of-parts model for articulated pose estimation. Instead of modeling both location and orientation of body limbs as rigid parts (, @cite ), they used non-oriented pictorial structures with co-occurrence constraints. Their work relies on the geometry to define clusters (called types" in their paper). Therefore, the representation is less from Marr's point of view.
- Following this research direction, @cite used predefined symbols for a simultaneous detection of body parts and estimation of human poses. @cite used compatibility maps in a tree structure. Latent nodes encode compatibility between parts, and accuracy is improved because incompatible poses are pruned. However, their types" variables are solely based on the geometry, and do not encode visual information.
- @cite proposed to use compositional parts to provide more precise results, but his method has a higher computation cost due to the loopy graph structure. @cite cast human pose into AND OR graphs, and performed human parsing using top-down scheme. Rich appearance models were adopted to estimate human parts. @cite proposed to use poselets for human recognition. Each poselet refers to a combined part that is distinctive in training images. Please note that these poselets do not characterise geometric contexts in modeling pairwise distributions, which makes it less effective to fully capture the body dynamical structures.
- The RC model was first introduced by van der Meulen @cite in 1971. Despite the significant research efforts, the capacity of the general RC is still unknown. In their seminal work @cite , Cover and El Gamal proposed a general outer bound, now known as the max-flow min-cut outer bound or cut-set for short, and two achievable schemes: decode-and-forward (DF) and compress-and-forward (CF). The cut-set outer bound was shown to be tight for the degraded RC, the reversely degraded RC and the semi-deterministic RC @cite , but it is not tight in general @cite .
- Relay networks were also studied in @cite , where an iterative algorithm was proposed to determine the optimal fraction of time each HD relay transmits receives by using DF under deterministic switching mechanism.
- There is a large body of work on techniques for adaptive video streaming. At the application layer, channel-aware prefetching schemes invoke a traffic burst at high channel gain @cite @cite . At the link layer, cross-layer schedulers to jointly adapt video quality and wireless resource allocation have been proposed @cite @cite .
- Compared to this work, our approach differs two-fold. First, it does not adapt the video quality but adjusts the size of the video play-out buffer along with the wireless data rate. Unlike @cite and @cite , this enables to trade off buffer size with the amount of allocated resources. Second, our adaptation scheme is . Unlike any of the above approaches, our joint buffer-rate allocation is based on a prediction of the user's average channel gain. Using this prediction enables our scheme to plan its resource allocation ahead. This enables to compensate for upcoming channel outages (e.g., when a user drives through a tunnel) by pre-loading the video buffer in advance. Although this idea of anticipation has been applied for software interfaces @cite and cognitive radios @cite , it has not been proposed for media streaming so far.
- Further benefits of our work are its generality and low computational complexity. Being based on bit rates, our buffer model captures arbitrary video and audio codecs, while being independent of subjective quality metrics. This level of tractability is not provided by the Utility-based formulation in @cite . Finally, our buffer model enables to incorporate the dynamics of streaming media traffic into a Linear Program. Such low complexity is not provided by the combinatorial approach in @cite .
- Zhu et. al. adopted the SIR model and proposed a sample path counting approach for source detection @cite . They prove that the source node on infinite trees minimizes the maximum distance to the infected nodes. They assume that the infected and susceptible nodes are indistinguishable. Lokhov et. al. use a dynamic message-passing algorithm to estimate the probability that a given node produces an observed snapshot. They use a mean-field-like approximation to compute the marginal probabilities and an assumption of sparse contact network @cite .
- The formulation of spectrum learning problem as a restless multi-armed bandit process is investigated in @cite -- @cite . It is proved that when channels are identical and independent the myopic policy is the optimal policy @cite -- @cite , and that this policy is a special case of Whittle's index policy for the restless bandit problem which can be computed for non-identical channels as well @cite . An asymptotically optimal policy is proposed in @cite for a more realistic case where the policy does not require any prior statistical knowledge about the traffic pattern and the channels are different. Despite all these work that assume a non-adversarial environment, this paper investigates the spectrum learning problem in an adversarial environment where active attackers aim to reduce the throughput of the cognitive radio network.
- The security of machine learning algorithms that have been applied to applications such as intrusion detection systems (IDS) and spam filters is investigated thoroughly in @cite - @cite . In these papers, the authors discuss how an adversary can maliciously mistrain a learning system in an IDS and how an attacker may contaminate the knowledge base of a spam email filtering system to bypass the filtering. In @cite and @cite different kinds of attacks against machine learning algorithms are introduced and a variety of potential defenses against those attacks are proposed.
- In the context of cognitive radios, security of machine learning algorithms that are used for signal classification are addressed in @cite and @cite , but the types of learning algorithms that are analyzed is different from the algorithms in this paper. To the best of our knowledge, this paper describes the first analysis of a reinforcement learning algorithm's vulnerability against belief manipulation attacks to cognitive radios.
- Our work is also motivated by the work on sketching in the computer science community; this literature deals with the idea of compressing high-dimensional data vectors via projection to low-dimensions while preserving pertinent geometric properties. The celebrated Johnson-Lindenstrauss Lemma @cite is one such result, and the idea of sketching has been explored in various contexts @cite @cite . The idea of using random bipartite graphs and their related expansion properties, which motivated our approach to the problem, have also been studied in past work @cite @cite @cite .
- While most of the work on sparse recovery focuses on sensing matrices where each entry is an i.i.d. random variable, there are a few lines of work that explore structured sensing matrices. For instance, there have been studies of matrices with Toeplitz structure @cite , or those with random entries with independent rows but with possibly dependent columns @cite @cite . Also related is the work on deterministic dictionaries for compressed sensing @cite , although those approaches yield results that are too weak for our setup.
- One interesting aspect of our work is that we show that it is possible to use highly constrained sensing matrices (i.e. those with tensor product structure) to recover the signal of interest. Many standard techniques fail in this setting. Restricted isometry based approaches @cite and coherence based approaches @cite @cite @cite fail due to a lack of independence structure in the sensing matrix. Indeed, the restricted isometry constants as well as the coherence constants are known to be weak for tensor product sensing operators @cite @cite . Gaussian width based analysis approaches @cite fail because the kernel of the sensing matrix is not a uniformly random subspace and hence not amenable to a similar application of Gordon's ( escape through the mesh'') theorem. We overcome these technical difficulties by working directly with combinatorial properties of the tensor product of a random bipartite graph, and exploiting those to prove the so-called nullspace property @cite @cite .
- Sparse matrix multiplication is one of the most heavily used kernels in scientific computing and has therefore received attention from several groups @cite @cite @cite @cite . Multiple storage formats, optimisation strategies and even auto-tuning frameworks exist to improve spMVM performance on a wide range of multi-core architectures @cite . On modern HPC architectures hybrid programming methods are being investigated to better utilise the hierarchical hardware design by reducing communication needs, memory consumption and improved load balance @cite . In particular, task-based threading methods have been highlighted by several researchers, where dedicated threads can be used to overlap MPI communication with local work @cite @cite @cite .
- While there is a wide literature that proposes frameworks to interact with commercial controllers like the WiiMote (eg: @cite ) and general purpose sensor nodes (eg. @cite ) there are only few works proposing frameworks composed by both hardware and software libraries for prototyping orientation and motion sensing. In @cite the authors propose a framework composed by a wireless sensor board and gesture recognition algorithms. While their work on the algorithms is still interesting, their sensor hardware is now considerably outdated and the sensors used in FreeIMU are considerably more accurate, faster and easier to interface.
- @cite is a small sensor board containing an accelerometer, a microcontroller and wireless communication which has been designed for ease-of-use. While this device may be interesting for inexperienced developers, it may be also constraining compared to Arduino, as also noted by the authors. Our framework also aims to ease sensor based interaction prototyping, however we concentrated on not loosing the flexibility and extensibility of the Arduino platform while providing wider sensor input.
- Clustering on graphs has been studied extensively due to its numerous applications in different domains. The works in @cite @cite have given comprehensive overviews of the advancements in this field over the last few decades. The algorithms that are based on spectral techniques on graphs are of particular interest, typical examples being spectral clustering @cite @cite @cite and modularity maximization via spectral method @cite @cite . Specifically, these approaches propose to embed the vertices of the original graph into a low dimensional space, usually called the spectral embedding, which consists of the top eigenvectors of a special matrix (graph Laplacian matrix for spectral clustering and modularity matrix for modularity maximization). Due to the special properties of these matrices, clustering in such low dimensional spaces usually becomes trivial. Therefore, the corresponding clustering approaches can be interpreted as transforming the information on the original graph into a meaningful subspace representation. Another example is the Principal Component Analysis (PCA) interpretation on graphs described in @cite , which links the graph structure to a subspace spanned by the top eigenvectors of the graph Laplacian matrix. These works have inspired us to consider the subspace representation in Section .
- In respect of transfer learning and deep learning, there has been some similar work with the proposed schemes. For example, in @cite , Glorot adopted a domain adaptation scheme that is exactly the same as Scheme 2 of this paper for the sentiment classification problem. In @cite , Collobert and Weston proposed a joint training scheme for the multitask learning problem of natural language processing, whose key idea is similar with Scheme 3. The architecture of @cite is also successfully applied to machine translation @cite . However, Scheme 3 is different from @cite @cite in that our Scheme 3 pre-train the top hidden-layer of DDNN with set @math , while the architecture of @cite @cite try to learn a subspace of word mapping in the top-hidden layer with a strong constraint that one word in the lookup table of @math should have a matching word in that of @math .
- In respect of the VAD study, the distribution difference between different noise scenarios has been mentioned in traditional VADs. For example, in @cite , Chang used different statistical models for modeling the speech and noise distributions in different noise scenarios. Another related topic with domain adaptation is the online learning methods @cite , they update the model parameters according to the historical domain information of the speech signals. Traditional statistical-model-based VADs @cite can also be regarded as unsupervised online learning methods. But to our knowledge, how to combine multiple features effectively is still an open problem in the online learning methods. On the other side, although the domain-adaptation-based VAD works in batch mode, it can combine multiple features effectively and yield a high accuracy without a requirement of heavy manual labeling.
- From a object oriented software engineering perspective, the classical design pattern @cite comes close to our suggestion by permitting a to request an to execute a given on a .
- Since this work is concerned with constructing modular subsystems, the work of the Ptolemy project @cite is relevant, although that is more focused on composition of heterogeneous systems.
- Since the goal of this paper is to focus on the implementation and scalability of BTER, we limit our discussion to the most salient related models. A more thorough discussion of related work can be found in the original paper on BTER @cite .
- The majority of graph models add edges one at a time in a way that each random edge influences the formation of future edges, making them inherently unscalable. The classic example is Preferential Attachment @cite , but there are a variety of related models, e.g., @cite @cite . These models are more focused on capturing qualitative properties of graphs and typically are difficult to match to real-world data @cite . Perhaps the most relevant is @cite , which creates a graph with power law degree distribution and then rewires'' it to improve the clustering coefficients. Another related model, the clustering model proposed Newman @cite , assigns individuals'' to groups'' (a bipartite graph with individual and group nodes) and then creates a graph of connections between individuals by assigning connection probabilities to each group; in other words, each group is modeled as an Erd o s--R 'enyi graph.
- A widely used model for modeling large-scale graphs is the Stochastic Kronecker Graph (SKG) model, also known as R-MAT @cite @cite . The generation process is easily parallelized and can scale to very large graphs. Notably, SKG has been selected as the generator for the Graph 500 Supercomputer Benchmark @cite and has been used in a variety of other studies @cite @cite @cite @cite @cite @cite @cite . Unfortunately, SKG has some drawbacks.
- The @cite is a resampling method that can be used to estimate the bias of an estimator, in order to reduce this bias. Based on this, Tibshirani and Tibshirani @cite propose an estimator for @math for model selection in classification. Inevitably---see Section ---the resulting estimate is still biased, and it is typically more variable that the original estimate. Also specifically considering model selection for classifiers, @cite propose a smoothed version of nested CV. The resulting estimator performs similar to normal (nested) CV, which in turn is shown to typically be more accurate than the approach by Tibshirani and Tibshirani. In this paper we focus on CV and the ME, which are by far the most widely used.
- In another work, @cite eschew inverted indexes completely and incrementally build document-centered representations, from which postings list are dynamically constructed and cached only in response to queries. The assumption is that more heavyweight'' index processes will run periodically (e.g., every 30 minutes), so that all other data structures can be considered transient. Although this design appears to be justified for the particular search environment explored (corporate intranet), these assumptions do not appear to be workable for our setting.
- Another interesting point in the design space is represented by Google's Percolator architecture @cite , which is built on top of Bigtable @cite ---a distributed, multi-dimensional sparse sorted map based log-structured merge trees. Percolator supports incremental data processing through observers , similar to database triggers, which provide cross-row transactions, whereas Bigtable only supports single-row transactions. This architecture represents a very different design from our system, which makes a fair comparison difficult. The performance figures reported by the authors suggest that Earlybird is much faster in indexing, but in fairness, this is an apples-to-oranges comparison. Percolator was designed to encompass the entire webpage ingestion pipeline, handling not only indexing but other aspects of document processing as well---whereas Earlybird is highly specialized for building in-memory inverted indexes.
- Finally, a few notes about our strategy for allocating postings slices from fixed-size pools: there are some similarities we can point to in previous work, but some important differences as well. With the in-place update strategy where extra space for postings is pre-allocated, it is not much of a stretch to implement fixed block sizes that are powers of two. @cite allocate space for on-disk postings in sizes of 16, 32, 64, 128, @math 8192. However, a few important differences (beyond in-memory vs. on-disk): copy postings each time a new block is allocated to preserve contiguity, whereas we don't. In addition, the paper leaves open the method by which those blocks are allocated---whereas we describe a specific implementation based on fixed slice sizes in large pools (supporting efficient memory allocation, compact pointer addressing, etc.).
- Tracing the lineage of various storage allocation mechanisms further back in time, we would arrive at a rich literature on general-purposes memory allocation for heap-based languages (e.g., malloc in C). According to the taxonomy of @cite , Earlybird's allocation strategy would be an example of segregated free lists, an approach that dates back to the 1960s. Of course, since we're allocating memory for the very specific purpose of storing postings, we can accomplish the task much more efficiently since there are much tighter constraints, e.g., no memory fragmentation, fixed sizes, etc. Nevertheless, it would be fair to think of our work as a highly-specialized variant of general purpose memory allocators for heap-based languages.
- The work in @cite presents the results of tests carried out in a cluttered book store and in a through-wall scenario with @math targets having separated trajectories. The authors use a particle filter @cite to track the two targets, measuring an RMSE of @math m in the book store and of @math m in the through-wall scenario. The number of targets is assumed to be known a priori. Due to the computational complexity of the particle filter method, localization and tracking are not performed in real-time.
- In particular, Jain, Klauck and Santha have shown @cite such a direct sum theorem for deterministic and randomised decision tree complexity. As discussed in @cite , previous work had dismissed this question as uninteresting, but the proof is not immediate. In the deterministic case, the result of @cite is proven by showing that, given a decision tree @math computing @math independent outputs, one can produce @math independent trees @math , one for each output, and lower bounding the depth of @math in terms of the depth of the trees @math . This approach does not seem suitable for proving a composition theorem, as given a tree computing @math , it is not necessarily possible to produce individual trees for @math . On the other hand, the composition theorem given here in fact implies a direct sum theorem for decision tree complexity (see Section for this and other corollaries).
- Similar, but more complicated, composition theorems to the present work have been proven for quantities which are important in the study of quantum query complexity. H yer, Lee and S palek @cite proved that the so-called non-negative weight adversary bound (which we will not define here; see @cite for details) obeys a composition theorem. Interestingly, their proof is based on generalising the query model to weighted queries, similarly to the approach taken here. A composition theorem was later proven for the general adversary bound (in two parts; a lower bound by H yer, Lee and S palek @cite , and an upper bound by Reichardt @cite ), which was then used by Reichardt to infer a composition theorem for quantum query complexity @cite .
- Minimizing I Os. Several studies focus on minimizing I Os required for recovering a single failure in erasure codes. Their approaches mainly focus on a disk array system where the disk access is the bottleneck. Authors of @cite @cite propose optimal single failure recovery for RAID-6 codes. Khan @cite show that finding the optimal recovery solution for arbitrary erasure codes is NP-hard. Note that the performance gains of the above solutions over the conventional recovery are generally less than 30 gain in single failure recovery (see ).
- Authors of @cite @cite @cite @cite Although the proposed scheme of @cite is also called CORE, it refers to Cross Object Redundancy and builds on local recovery codes, which have very different constructions from regenerating codes considered by our work. have proposed local recovery codes that reduce bandwidth and I O when recovering lost data. They evaluate the codes atop a cloud storage system simulator (e.g., in @cite ), Azure Storage (e.g., in @cite ) and HDFS (e.g., in @cite @cite ). It is worth noting that the local recovery codes are non-MDS codes with additional parities added to storage, so as to trade for better recovery performance. All these studies focus on optimizing single failure recovery. Our work differs from them in several aspects: (i) we consider optimal minimum storage regenerating codes that are MDS codes, (ii) we consider recovering both single and concurrent failures, (iii) we experiment regenerating codes that require storage nodes to perform encoding operations.
- Minimizing recovery bandwidth. Regenerating codes @cite minimize the recovery bandwidth for a single failure in a distributed storage system. There have been many theoretical studies on constructing regenerating codes (e.g., @cite @cite @cite @cite @cite ). In contrast with the above solutions that minimize I Os, most regenerating codes typically read all stored data to generate encoded data. Implementation studies of regenerating codes recently receive attention from the research community, such as @cite @cite @cite @cite . Note that the studies @cite @cite @cite do not integrate regeneration codes into a real storage system, while NCCloud @cite implements a storage prototype based on non-systematic regenerating codes.
- Cooperative recovery. Several theoretical studies (e.g., @cite @cite @cite @cite ) address concurrent failure recovery based on regenerating codes, and they focus on recovery of lost data on new nodes. They all consider a cooperative model , in which the new nodes exchange among themselves their data being read from surviving nodes during recovery. Authors of @cite @cite prove that the cooperative model achieves the same optimal recovery bandwidth as ours, but they do not provide explicit constructions of regenerating codes that achieve the optimal point. Authors of @cite @cite provide such explicit implementations, but they focus on limited parameters and the resulting implementations do not provide any bandwidth saving over erasure codes. A drawback of the cooperative model requires coordination among the new nodes to perform recovery, and its implementation complexities are unknown. Extending it for degraded reads is also non-trivial, as clients simply request lost data instead of recovering lost data on new nodes.
- In this context, the model--driven paradigm is recognized to be an effective approach for the design of software. In @cite , the authors point out the importance of resource awareness in robotics applications; they describe a development process and a meta--model for robotics systems that are focused on the non-functional properties of the components. The techniques of meta--modeling and domain specific languages are exploited in @cite to design a programming environment independent of the target robot, to facilitate the specification and reuse of control programs. In @cite , the authors present an execution environment based on the scripting language Lua, to support the implementation of internal for modeling expressive state machines for robot coordination. The work focuses particularly on dynamic memory management issues, not to violate real time constraints during the interpretation (execution) of the state machines.
- Key to our paper is the work by @cite . In fact, we adopt the same opinion-dynamics model as in @cite , where individuals selfishly form opinions to minimize their personal cost. However, focus on quantifying the social cost of this lack of central coordination between individuals, i.e., the , and they consider a network-design problem with the objective of reducing the social cost at equilibrium. Our work on the other hand, focuses on designing a promotion campaign so that, at equilibrium, the overall positive inclination towards a particular opinion is maximized.
- Closely related to our work, there is early work on DC Nets @cite @cite which aims to provide a cryptographic means to hide who sends messages, the use of Raptor codes @cite to implement an asynchronous unidirectional one-to-one and one-to-many covert channel using spam messages, and anonymous data aggregation @cite for distributed sensing and diagnostics. In preserving the privacy of web-based email @cite one can take advantage of a spread-spectrum approach for hiding the existence of a message, but it is not secure against a global attacker. Membership-concealment @cite can also be used to hide the real-world identities of participants in an overlay network, but this doesn't suffice for AdLeaks.
- There is extensive literature reporting research on intelligent agents that learn from expert advice. Many examples feature robotic agents that learn simple tasks from different forms of human feedback. Examples include the robot Leonardo that is able to learn new tasks by observing changes induced in the world (as perceived by the robot) by a human demonstrating the target task @cite . During learning, Leonardo provides additional feedback on its current understanding of the task that the human user can then use to provide additional information. We refer the survey works of @cite @cite for a comprehensive discussion on learning from demonstration.
- In this paper, as already mentioned, we adopt the inverse reinforcement learning (IRL) formalism introduced in the seminal paper by @cite . One appealing aspect of the IRL approach to learning from demonstration is that the learner is not just mimicking'' the observed actions. Instead, the learner infers the purpose behind the observed behavior and sets such purpose as its goal. IRL also enables the learner to accommodate for differences between itself and the demonstrator .
- The appealing features discussed above have led several researchers to address learning from demonstration from an IRL perspective. @cite explored inverse reinforcement learning in a context of , where the purpose of the learning agent is to replicate the behavior of the demonstrator, but is only able to observe a sequence of states experienced during task execution. The IRL formalism allows the learner to reason about which tasks could lead the demonstrator to visit the observed states and infer how to replicate the inferred behavior. syed08icml have further explored this line of reasoning from a game-theoretic perspective, and proposed algorithms to learn from demonstration with provable guarantees on the performance of the learner.
- @cite introduced (BIRL), where the IRL problem is cast as a Bayesian inference problem. Given a prior distribution over possible target tasks, the algorithm uses the demonstration by the expert as evidence to compute the posterior distribution over tasks and identify the target task. Unfortunately, the Monte-Carlo Markov chain (MCMC) algorithm used to approximate the posterior distribution is computationally expensive, as it requires extensive sampling of the space of possible rewards. To avoid such complexity, several posterior works have departed from the BIRL formulation and instead determine the task that maximizes the likelihood of the observed demonstration .
- The aforementioned maximum likelihood approaches of @cite and @cite take advantage of the underlying IRL problem structure and derive simple gradient-based algorithms to determine the maximum likelihood task representation. Two closely related works are the of @cite and the of @cite . While the former selects the task representation that maximizes the likelihood of the observed expert behavior, under the maximum entropy distribution, the latter explores a gradient-based approach to IRL, but the where the task representation is selected so as to induce a behavior as similar as possible to the expert behavior.
- Finally, @cite propose a learning algorithm that reduces imitation learning to a classification problem. The classifier prescribes the best action to take in each possible situation that the learner can encounter, and is successively improved by enriching the data-set used to train the classifier.
- (CBA), proposed by @cite , also enables a robot to learn a task from a human user by building a mapping between situations that the robot has encountered and the adequate actions. This work already incorporates a mechanism that enables the learner to ask the expert for the right action when it encounters a situation in which it is less confident about the correct behavior. The system also allows the human user to provide corrective feedback as the robot executes the learned task. Related ideas are further explored in the architecture of @cite . The querying strategy in CBA can be classified both as and as [see discussions in the survey works of][] settles09tr,dasgupta11tcs . Stream-based, since the learner is presented with a stream of samples (in the case of CBA, samples correspond to possible situations) and only asks for the labels ( correct actions) of those samples it feels uncertain about. Mellow, since it does not seek highly informative samples, but queries any sample that is at all informative.
- @cite propose another closely related approach that, however, uses a different criterion to select which situations to query. EMG-AQS (Expected Myopic Gain Action Querying Strategy) queries the expert for the correct action in those states where the expected is potentially larger. Unfortunately, as discussed by @cite , the determination of the expected gain of information requires extensive computation, rendering EMG-AQS computationally costly. On a different line of work, @cite @cite address imitation learning using a no-regret framework, and propose algorithms for direct imitation learning with provable bounds on the regret. Finally, @cite use active learning in a metric approach to learning from demonstration.
- Our approach in this paper is a modified version of our original active sampling algorithm . We depart from the generalized binary search (GBS) algorithm of @cite and adapt it to the IRL setting. To this purpose, we cast IRL as a (multi-class) classification problem and extend the GBS algorithm of @cite to this multi-class setting. We analyze the sample complexity of our GBS-IRL approach, thus providing the first active IRL algorithm with provable bounds on sample complexity. Also, to the extent of our knowledge, GBS-IRL is the first aggressive active learning algorithm for non-separable, multi-class data .
- We conclude this discussion of related work by pointing out that all above works describe systems that learn from human feedback. However, other forms of expert advice have also been explored in the agent learning literature. @cite @cite have explored how a learning agent can improve its performance by observing other similar agents, in what could be seen as implicit'' imitation learning. In these works, the demonstrator is, for all purposes, oblivious to the fact that its actions are being observed and learned from. Instead, the learned observes the behavior of the other agents and extracts information that may be useful for its own learning (for example, it may extract useful information about the world dynamics).
- In a more general setting, @cite discuss how different forms of supervisory information can be integrated in a reinforcement learning architecture to improve learning. Finally, @cite @cite introduce the tamer paradigm, that enables a reinforcement learning agent to use human feedback (in addition to its reinforcement signal) to guide its learning process.
- The focus on (the negative side of) expressibility for approximate counting problems appears in @cite , where logsupermodular functions are shown not to express non-logsupermodular functions in the context of @math s.
- Under an asynchronous timing model, every node in the network has a clock which ticks according to a Poisson process with a rate of @math . This is equivalent to a single clock that ticks according to a Poisson process with a rate of @math , where @math is the number of nodes in the network @cite . In practice, this means that at each tick an average of @math nodes are chosen independently and uniformly at random to transmit their state values.
- Sharing a similar motivation as ours, existing research on wireless networks has seen some focus on algorithm design and analysis based on evolutionary game theory. In @cite , used evolutionary dynamics to analyze node behavior over time in the context of two problems, namely channel access in slotted ALOHA systems and decentralized power control. Evolutionary coalition games were used to study the problem of network formation in @cite and @cite while dynamic routing games were studied in @cite . In their work in @cite , studied the evolution of misbehavior among secondary users in a cognitive radio network.
- A key aspect of our work which distinguishes it from existing research is the fact that we study node cooperation behavior in the context of information dissemination in wireless networks. Our approach aims to identify the mutual impact that the evolution of node cooperation and the information dissemination process have on each other. As with the problem of information dissemination in vehicular networks considered here, there exist other scenarios in which free-riding is the only rational strategy for nodes to follow. Indeed, existing research has explored the use of evolutionary game theoretic techniques to study node cooperation in scenarios such as peer-to-peer file sharing @cite @cite @cite . However, the distinguishing feature of our work is that our model of node behavior is particularly suited for wireless network scenarios since we take into consideration aspects like node interactions limited to immediate neighborhood and the intrinsic ability of the wireless medium to boost cooperation.
- The work most closely related to the current work, in that it presents an extremal result relating crossings, volume, and number of edges, is that of Bose al @cite , who show that the maximum number of edges in a crossing-free @math -D geometric grid graph, @math , with vertex set @math , is exactly For a fixed volume, @math , maximizing gives @math , in which case becomes @math . We state this here as lemma since we make use of it several times. boser immediately yields the upper-bound @math (see the beginning of counting ). It also yields the lower-bound @math since, if a geometric grid graph @math has @math we can remove an edge an edge from @math that eliminates at least one crossing. Since this can be repeated until @math has @math edges, this implies that @math has at least @math crossings.
- Finally, we note that Bukh and Hubard @cite have defined a form of crossing number for 3-dimensional geometric graphs that are not necessarily grid graphs. In their definition, a 4-tuple of vertex-disjoint edges form a if there is a line that intersects every edge in the 4-tuple. The , @math , of a 3-d geometric graph, @math , is the number of space crossings formed by @math 's edges. They show that a 3-d geometric graph @math with @math vertices and @math edges has a space crossing number @math . An easy lifting argument shows that this bound on the space crossing number almost implies the Crossing Lemma; specifically, it shows that the number of crossings in a graph with @math vertices and @math edges drawn in the plane is @math .
- In @cite , Arcara, Bertram, Coskun and Huizenga study the birational geometry of the punctual Hilbert schemes @math of length @math subschemes of @math . This is very related to the case @math of Question and (i.e. to Theorem ) because, in this case, @math is a locally closed subscheme of @math . Let us describe the similarities and differences between these two situations.
- Unlike the varieties @math , @math is always log Fano ( @cite Theorem 2.5). This immediately implies that it is a Mori dream space by @cite , answering the analogue of Question for @math . Since @math is of Picard rank @math , it is possible to run its MMP in two directions. One of these is trivial: we get a contraction, the Hilbert-Chow morphism. As for @math , it is the other one that is interesting to describe.
- The analogue of Question is much more complicated in the case of @math . Indeed, the non-trivial boundary of @math is difficult to describe: it depends on @math in a complicated and interesting fashion ( @cite , @cite Theorem 1.4 and Table 1).
- Here is another difference between @math and @math . The trivial contraction of @math is the fibration over @math , that associates to @math its degree @math equation @math , and the non-trivial contraction that starts the MMP for @math is closely related to a Hilbert-Chow morphism (see for instance the proof Lemma ). On the contrary, the trivial contraction of @math is the Hilbert-Chow morphism and the non-trivial contraction that starts the MMP for @math is given by considering the degree @math equations of a length @math subscheme ( @cite Proposition 3.1).
- We begin with an overview of modern processor architectures and recap advances over the past few decades. The broadest trend is perhaps the multi-core revolution @cite : the relentless march of Moore's Law continues to increase the number of transistors on a chip exponentially, but experts widely agree that we are long past the point of diminishing returns in extracting instruction-level parallelism in hardware. Instead, adding more cores appears to be a better use of increased transistor density. Since prediction is an embarrassingly parallel problem, our techniques can ride the wave of increasing core counts.
- A less-discussed, but just as important trend over the past two decades is the so-called memory wall'' @cite , where increases in processor speed have far outpaced improvements in memory latency. This means that RAM is becoming slower relative to the CPU. In the 1980s, memory latencies were on the order of a few clock cycles; today, it could be several hundred clock cycles. To hide this latency, computer architects have introduced hierarchical cache memories: a typical server today will have L1, L2, and L3 caches between the processor and main memory. Cache architectures are built on the assumption of reference locality---that at any given time, the processor repeatedly accesses only a (relatively) small amount of data, and these fit into cache. The fraction of memory accesses that can be fulfilled directly from the cache is called the cache hit rate , and data not found in cache is said to cause a cache miss . Cache misses cascade down the hierarchy---if a datum is not found in L1, the processor tries to look for it in L2, then in L3, and finally in main memory (paying an increasing latency cost each level down).
- Managing cache content is a complex challenge, but there are two main principles that are relevant to a software developer. First, caches are organized into cache lines (typically 64 bytes), which is the smallest unit of transfer between cache levels. That is, when a program accesses a particular memory location, the entire cache line is brought into (L1) cache. This means that subsequent references to nearby memory locations are very fast, i.e., a cache hit. Therefore, in software it is worthwhile to organize data structures to take advantage of this fact. Second, if a program accesses memory in a predictable sequential pattern (called striding), the processor will prefetch memory blocks and move them into cache, before the program has explicitly requested the memory locations (and in certain architectures, it is possible to explicitly control prefetch in software). There is, of course, much more complexity beyond this short description; see @cite for an overview.
- The database community has explored in depth the consequences of modern processor architectures for relational query processing @cite @cite @cite @cite @cite . In contrast, these issues are underexplored for information retrieval and data mining applications. This is one of the first attempts at developing architectural-conscious runtime implementations of machine learning algorithms. Researchers have explored scaling the training of tree-based models to massive datasets @cite @cite , which is of course an important problem, but orthogonal to the issue we tackle here: given a trained model, how do we make predictions quickly?
- The impact of data and control hazards can be substantial: an influential paper in 1999 concluded that in commercial RDBMSes at the time, almost half of the execution time is spent on stalls @cite . Of course, this was before the community was aware of the issue, and so systems have become much more efficient since then. Which is worse'', data or control hazards? Not surprisingly, the answer is, it depends. However, with a technique called predication @cite @cite , which we explore in our work, it is possible to convert control dependencies into data dependencies (see ). Whether predication is worthwhile, and under what circumstances, remains an empirical question.
- Another optimization that we adopt, called vectorization, was pioneered by database researchers @cite @cite : the basic idea is that instead of processing a tuple at a time, a relational query engine should process a vector'' (i.e., batch) of tuples at a time to take advantage of pipelining. Note that this sense of vectorization is distinct from, but related to, explicit SIMD instructions that are available in many processor architectures today. Vectorization increases the opportunities for optimizing compilers to generate specialized SIMD instructions automatically. Our work represents the first application of vectorization to optimizing machine learning algorithms that we are aware of.
- Beyond processor architectures, the other area of relevant work is the vast literature on learning to rank @cite , application of machine learning techniques to document ranking in search. Our work uses gradient-boosted regression trees (GBRTs) @cite @cite @cite , a state-of-the-art ensemble method. The focus of most learning-to-rank research is on learning effective models, without considering efficiency, although there is an emerging thread of work that attempts to better balance both factors @cite @cite . In contrast, we focus exclusively on runtime ranking performance, assuming a model that has already been trained (by other means).
- A broad concern with human labeling, such as relevance judging, is ensuring label consistency such that systems can be effectively trained and evaluated. A decade ago, Voorhees showed that reliable evaluation of search systems could be achieved despite significant variations in the underlying relevance judgments @cite . @cite took this idea further: could we forgo human labeling entirely (i.e., blind evaluation) by sampling relevant documents randomly after pooling outputs from all systems participating in a shared task evaluation? While such evaluation indeed correlated positively and significantly with use of human judgments, predicted performance was less reliable for the best systems. Aslam and Savell @cite achieved comparable correlation by simply scoring each system by its mean Jaccard Coefficient over the set of retrieved documents vs. those retrieved by each other system. Wu and Crestani exploited similar reference count'' popularity, modeling expected correlation between relevance and the rank and frequency at which documents are retrieved by systems @cite , similar to rank fusion @cite . Several other blind evaluation methods have also been explored @cite @cite @cite @cite .
- CS result shows that, if @math , a sub-Gaussian random matrix It includes Gaussian and Bernoulli random matrices etc. @cite . @math can satisfy the RIP with high probability @cite @cite .
- Recently, structured sparsity theories demonstrate that when there is some structured prior information (e.g. group, tree, graph) in @math , the measurement bound could be reduced @cite @cite . Suppose @math is in the union of subspaces @math , then the @math -RIP can be extended to the @math -RIP @cite :
- @math -RIP property has been proved to be sufficient for robust recovery of structured-sparse signals under noisy conditions @cite . The required number of measurements @math has been quantified for a sub-Gaussian random matrix @math that has the @math -RIP @cite :
- Now we consider structured sparse data. Following @cite , if a @math -sparse data @math can form a tree or can be sparsely represented as a tree under one orthogonal sparse basis @math (e.g. wavelet), and the @math non-zero components naturally form a subtree, then it is called tree-sparse data.
- For both case, we have @math . Similar conclusion has been drawn in previous articles @cite @cite . So far, we have reviewed standard sparsity and tree sparsity on single channel data. For multi-channel data that contains @math channels or vectors (i.e. @math ), each of which is standardly @math -sparse, the bound for the number of measurement should be @math . If each channel is tree-sparse and independently, the measurement bound for a sub-Gaussian random matrix @math is @math .
- Similar as tree-sparse data, joint-sparse data has the property. It has to be clarified that joint sparsity does not rely on tree sparsity. The former utilizes the structure across different channels, while the later utilizes the structure within each channel. Previous works implies that the minimum measurement bound for such joint sparse data is @math @cite @cite @cite .
- There have been hash tables designed with SSDs including the work presented in @cite in the context of data intensive networked systems, @cite in the context of wimpy nodes, @cite is in the context of data de-duplication, @cite energy efficient memory sensors, and @cite persistent storage as write and or read caches. In @cite designs a tree index for flash devices. However, @cite does not address duplicate keys thus it cannot handle a counting hash.
- In this work, we design a counting hash table that maintain frequencies, and this has not been addressed thus far. We store the hash table on the SSD which is not seen in the designs of @cite @cite . Unlike most of the existing strategies that rely on simple memory-based buffering schemes, we design a novel combination of memory- and disk- based buffering scheme. Our method leverages the strengths of SSDs (fast sequential random reads, fast sequential writes) to effectively address the weaknesses in SSDs (random writes, write endurance). We would like to emphasis that the works presented in this section do not handle a counting hash table , which is required by algorithms like TF-IDF, but our proposed hash table designs will.
- The work of was motivated by the pioneering work of who studied of the disk which arise as limiting objects for uniform triangulations of regular @math -gons as @math . In the case of uniform random triangulations, the process which encodes the limit triangulation is the Brownian excursion, and the scaling limit of the sequence of dual trees is the Brownian continuum random tree introduced in @cite @cite @cite .
- Among the recent work on laminations of the disk, one can mention @cite where Curien and Kortchemski showed that the Brownian triangulation is also the scaling limit of other random subsets of the disk, in particular non-crossing trees (sets of non-crossing chords which form a tree) @cite , and dissections (non-crossing sets of chords) under the uniform distribution. By sampling tessellations according to a Boltzmann weight depending on the degree of the faces, obtained limit laminations which are not triangulations and are encoded by excursions of stable spectrally positive L 'evy processes (with L 'evy measure concentrated on @math ). Finally, have studied geodesic laminations of the Poincar 'e disk. They construct and study the unique random tiling of the hyperbolic plane into triangles with vertices on the boundary whose distribution is invariant under M "obius transformations and satisfies a certain spatial Markov property.
- The methods proposed in the literature for similarity search via nearest neighbors (NN) can be, broadly speaking, classified to data or space partitioning methods and Locality Sensitive Hashing (LSH) techniques. Space partitioning methods, such as @math -D trees @cite , and data partitioning methods, such as R-trees @cite and SR-trees @cite , solve the NN problem by pruning the set of candidates for each query using branch and bound techniques. However, they do not scale well with the data dimension, and can in fact be even slower than a brute force sequential scan of the data set when the dimension exceeds @math @cite . For further details on these methods the reader is referred to the survey by Fodor @cite .
- Locality Sensitive Hashing (LSH) was introduced by Indyk and Motwani in order to solve high dimensional similarity search problems @cite . LSH indexing methods are based on LSH families of hash functions for which near points have a higher likelihood of hashing to the same value. Then, @math -NN problem can be solved by using multiple hash tables. @cite showed that in the Euclidian space @math hash tables suffice, which was later improved, by @cite , to @math (for some @math ), and further, by Andoni and Indyk @cite , to @math which almost matches the lower bound proved by @cite . LSH families are also known for several non-Euclidian metrics, such as Jaccard distance @cite and cosine similarity @cite .
- The main problem with LSH indexing is that to guarantee a good search quality, it requires a large number of hash tables. This entails a large index space requirement, and in the distributed setting, also a large amount of network communication per query. To mitigate the space inefficiency, Panigrahy @cite proposed Entropy LSH which, by also looking up the hash buckets of @math random query offsets'', requires just @math hash tables, and hence provides a large space improvement. But, Entropy LSH does not help with and in fact worsens the network inefficiency of conventional LSH: each query, instead of @math network calls, one per hash table, requires @math calls, one per offset. Our Layered LSH scheme exponentially improves this and, while guaranteeing a good load balance, requires only @math network calls per query.
- @cite describe the Sum and Cauchy schemes which map LSH buckets to peers in p2p networks in order to minimize network costs. However, in contrast to Layered LSH, no guarantees on network cost and load balance are provided. In this paper, we show via MapReduce experiments on the Wiki data set that Sum distributes data unevenly and thus may load some of the reduce tasks. In addition we also describe experiments which demonstrate that Layered LSH compares favorably with Cauchy on this data set.
- LSH families are known for several other metric spaces like min-wise independent permutations for Jaccard similarity @cite and SimHash for dot product or cosine similarity @cite . As noted earlier, LSH based indexing schemes require a large number of hash tables for accurate search results and a simple distributed implementation of the LSH indexing scheme (Simple LSH in Section , and described in @cite ) will cause a large load on the network while processing queries.
- Tools from algebraic coding theory have been widely used for constructing secrecy schemes @cite . In addition, the notion of providing security by exploiting the fact that the adversary has incomplete access to information is also central to several secure network coding schemes and wiretap models. Ozarow and Wyner @cite introduced the wiretap channel II, where an adversary can observe a set @math of his choice out of @math transmitted symbols, and proved that there exists a code that achieves perfect secrecy. A generalized version of this model was investigated by Cai and Yeung in @cite , where they introduce the related problem of designing an information-theoretically secure linear network code when an adversary can observe a certain number of edges in the network. Their results were later extended in @cite @cite @cite @cite . A more practical approach was presented by Lima al in @cite . For a survey on the theory of secure network coding, we refer the reader to @cite .
- The setting considered in this paper is related to the wiretap channel II in that a fraction of the source symbols is hidden from a possible adversary. Oliveira al investigated in @cite a related setting in the context of data storage over untrusted networks that do not collude, introducing a solution based on Vandermonde matrices. The MDS coding scheme introduced in this paper is similar to @cite , albeit the framework developed here is more general.
- The work @cite , modeled large scale terrain modeling using GPs. It proposed the use of non-stationary kernels (neural network) to model large scale discontinuous spatial data. A performance comparison between GPs based on stationary (squared exponential) and non-stationary (neural network) kernels as well as several other standard interpolation methods applicable to alternative representations of terrain data, was reported. The non-stationary neural network kernel was found to be superior to the stationary squared exponential kernel and at least as good as most standard interpolation techniques for a range of terrain (in terms of sparsity complexity discontinuities). The work presented in this paper builds on this GP representation. However, it addresses the problem of simultaneous modeling multiple heterogeneous quantities of interest, in the context of geological resource modeling. This requires the modeling and usage of the correlations between these quantities towards improving predictions of each of them - an instance of data fusion using Gaussian processes.
- An extensive review of kernel methods applied in modeling vector valued functions was presented in a recent survey paper @cite . The paper discusses different approaches to develop kernels for multi-task applications and draws parallels between regularization perspective of this problem and a Bayesian one. The latter perspective is discussed through Gaussian processes. The work presented in this paper focuses on one of the approaches reviewed in @cite ; specifically, it addresses modeling and information fusion of multi-task geological data using Gaussian processes developed using the process convolution approach. The paper presents a detailed empirical study of the approach applied to a large scale real world problem in order to evaluate its efficacy for information fusion, to understand the modeling capabilities of different kernels (chosen apriori) with such data and to understand broader approach-related questions from an application perspective. The paper also ties together past works of the authors within the process convolution theme.
- Our approach is related to the local regression method @cite @cite . More recent related work is @cite that uses smoothing techniques in high-dimensional lasso regression in the context of temporal data. Another recent approach proposed by @cite achieves code locality by approximating data points using a linear combination of nearby basis points. The main difference is that traditional local regression techniques do not involve basis learning. In this work, we propose to learn the basis or dictionary along with the regression coefficients locally.
- In contrast to previous sparse coding papers we propose to use marginal regression for learning the regression coefficients, which results in a significant computational speedup with no loss of accuracy. Marginal regression is a relatively old technique that has recently reemerged as a computationally faster alternative to lasso regression @cite . See also @cite for a statistical comparison of lasso regression and marginal regression.
- Since the introduction of parametricity for system F @cite @cite , it has been extended to many logical systems based on Type Theory. Among others, we can cite system @math by Vytiniotis and Weirich @cite and a large subset of PTSs by Bernardy @cite @cite . In all these presentations, no sort is impredicative, and parametricity relations live either in a meta-logic or in a different sort than propositions. To our knowledge, this is the first time parametricity relations live in an impredicative sort representing propositions, making them more usable in a system like .
- Bernardy @cite also explain two possible ways to handle inductive definitions: one by translating induction principles, and one by defining a new inductive data-type as the translation of the initial data-type. Our approach is close to the second method proposed by @cite . We also show how to translate fixpoint definitions, which are more common than inductive principles.
- Parametricity and parts of the abstraction theorem have been formalized for deep embeddings of logical systems in @cite and in @cite @cite . Our approach is different: we do not want to have a formal proof of the abstraction theorem (in a first step), but we want to have a practical tool that actually computes results produced by the abstraction theorem. This does not compromise soundness anyway, since the terms produced by this tool are type-checked by 's kernel.
- Several recent works have considered hashing variations that utilize less randomness in place of assuming perfectly random hash functions; indeed, there is a long history of work on universal hash functions @cite , and more recently min-wise independent hashing @cite . Specific recent related works include results on standard one-choice balls and bins problems @cite , hashing with linear probing with limited independence @cite , and tabulation hashing @cite ; other works involving balls and bins with less randomness include @cite @cite . As another example, Woelfel shows that a variation of V " o cking's results hold using simple hash functions that utilize a collection of @math -wise independent hash functions for small @math , and a random vector requiring @math space @cite .
- Another related work in the balls and bins setting is the paper of Kenthapadi and Panigrahy @cite , who consider a setting where balls are not allowed to choose any two bins, but are forced to choose two bins corresponding to an edge on an underlying random graph. In the same paper, they also show that two random choices that yield @math bins are sufficient for similar @math bounds on maximum loads that one obtains with @math fully random choices, where in their case each random choice gives a contiguous block of @math bins.
- Interestingly, the classical question regarding the average length of an unsuccessful search sequence for standard double hashing in an open address hash table when the table load is a constant @math has been shown to be, up to lower order terms, @math , showing that double hashing has essentially the same performance as random probing (where each ball would have its own random permutation of the bins to examine, in order, until finding an empty bin) when using traditional hash tables @cite @cite @cite . These results appear to have been derived using different techniques than we utilize here; it could be worthwhile to construct a general analysis that applies for both schemes.
- In terms of CPU optimization, Chandramowlishwaran @cite have done extensive work on thread-level parallelization with NUMA-aware optimizations and manual SIMD vectorization. They performed benchmarks on four different microarchitectures; Harpertown, Barcelona, Nehalem-EP, and Nehalem-EX, and exploit the simultaneous multithreading features of the Nehalem processors. With the various optimizations, they were able to double the performance on Harpertown and Barcelona processors, while obtaining a 3.7x speed-up on Nehalem-EX, compared to their previous best multithreaded and vectorized implementation. Some of the optimizations are specific to the kernel-independent FMM (KIFMM) @cite , and do not apply to FMMs based on Cartesian, spherical harmonics, or planewave expansions. It would be interesting to compare highly optimized kernels for these different types of expansions (including KIFMM) to determine at what order of expansion they cross over.
- For GPU optimization for uniform distributions, Takahashi @cite developed a technique to turn the originally memory bound M2L kernel into a compute bound GPU kernel by exploiting the symmetry of the interaction stencil. There are alternative techniques to make the M2L kernel compute bound on GPUs, by recalculating the M2L translation matrix on-the-fly @cite . However, the fact that the technique by Takahashi precalculates the M2L translation matrix and still allows the M2L kernel to be compute bound, means it is that much more efficient. Increasing the number of particles per leaf @cite or calculating the M2L kernel on the CPU @cite is an easy optimization strategy for heterogenous architectures. The value of this work lies in the fact that they tackle the GPU-M2L optimization problem head on, and actually succeed in making it compute bound. One limitation of their method is that the particle distribution must be somewhat uniform in order to exploit the symmetry of the M2L translation stencil. Though, classical molecular dynamics is an important application that be able to benefit from this method.
- For GPU optimization of non-uniform distributions for low accuracy, Bedorf @cite developed a highly optimized treecode bonsai https: github.com treecode Bonsai that runs entirely on GPUs. For three significant digits of accuracy in the force calculation for the Laplace kernel, their code is an order of magnitude faster than any other treecode or FMM on GPUs. Their code takes less than 50 milliseconds to calculate one million particles (including the tree construction) on an NVIDIA Fermi GF110 series GPU for the aforementioned accuracy, whereas other codes take about 500 milliseconds to achieve the same accuracy for one million particles. By using warp unit execution and () functions they have eliminated all synchronization barriers from their CUDA kernel. Furthermore, by using mask operations and prefix sums, they have eliminated all conditional branching from their code. However, an application which requires high accuracy may find this code difficult to use, since the order of expansion is not adjustable past @math . Nonetheless, anyone who has experience optimizing fast N-body codes knows that optimizing for low accuracy is much more challenging than optimizing high accuracy codes, so the impact of this work is remarkable.
- For MPI parallelization of an adaptive tree, Winkel @cite propose a request based point-to-point communication scheme that dedicates a thread for communication on each node, while the evaluation is done using the remaining threads simultaneously, in their code PEPC https: trac.version.fz-juelich.de pepc . Meanwhile, Jetley @cite used CHARMM++ to dynamically load-balance on large scale heterogenous environments, obtaining large gains by task aggregation and memory pooling, in their code ChaNGa http: software.astro.washington.edu nchilada . These efforts to provide an alternative to bulk-synchronous communication models could eventually become advantageous, but the lack of comparison against current state-of-the-art bulk-synchronous methods @cite @cite that also scale to the full node of the largest supercomputers, makes it difficult to predict exactly when they will become advantageous (if they do at all).
- As another aspect of MPI parallelization of an adaptive tree, Winkel use a 64-bit global Morton key and partition the domain to equally distribute the number of keys, while using the traversal time of the previous step as weights. This technique will only allow a maximum depth of the global tree of 21 levels before the 64-bit key overflows. Current large scale simulations already exceed this depth @cite , and the method of Winkel will require the use of multiple integers to store the key in such cases. Looking into the future, it would seem that the use of ever more integers to store the key will slow down the sorting of these keys, but the fact that Bedorf @cite can sort over a billion 96-bit keys per second using the back40computing library http: code.google.com p back40computing means that this remains a viable option. It would be interesting to compare this approach with alternative techniques that eliminate the use of global keys altogether @cite .
- An example of auto-tuning efforts in FMMs is given by Yokota & Barba @cite , where the dual tree traversal approach by Dehnen @cite was combined with an auto-tuning mechanism by selecting between M2L and M2P kernels to produce a treecode-FMM hybrid code, and by selecting between M2L and P2P kernels to optimize the number of particles at the leaf at which P2P is performed. This is an advantage when developing a black-box library that runs on both CPUs and GPUs without having to worry about selecting the optimal number of particles per leaf for each architecture. This code -- exaFMM https: bitbucket.org rioyokota exafmm-dev -- inherits all the techniques of falcON such as the dual tree traversal, mutual interaction, error aware local optimization of the opening angle @math , and a multipole acceptance criterion based on @math and @math @cite . At the same time, it adds features such as arbitrary order of expansion by using template metaprogramming, periodic boundary conditions, CUDA version of all FMM kernels, and a scalable MPI implementation. The present work is a further extension of this code to thread-level and SIMD parallelism.
- Last but not least, there have been a few recent attempts to use event-driven runtime systems for optimizing the data flow of parallel FMM codes. Ltaief & Yokota @cite used QUARK http: icl.cs.utk.edu quark index.html to schedule the threads dynamically according to a directed acyclic graph that represents the data flow of exaFMM 's dual tree traversal. Dekate take a similar approach but using ParalleX http: stellar.cct.lsu.edu tag parallex for the runtime system and a classical Barnes-Hut treecode @cite for the fast @math -body solver. Agullo use StarPU http: runtime.bordeaux.inria.fr StarPU along with the black-box FMM @cite on a CPU+GPU environment. Peric 'as use OmpSs http: pm.bsc.es ompss with exaFMM . These are all preliminary results, and large gains from using runtime systems have yet to be reported.
- The prior work most closely related to the problem investigated here is that by Lai, Poor, Xin, and Georgiadis @cite , in which the authors also examine the problem of quickest search across multiple populations, but do not focus on quantifying the sample complexity. The authors show that the S-SPRT (also termed a CUSUM test) minimizes a linear combination of the expected number of samples and the error probability. Complementary to this, our contributions include providing tight lower bounds on the expected number of samples required to achieve a desired probability of error, and then showing the sample complexity of the S-SPRT comes within a constant of this bound. This quantifies how the number of samples required to find an atypical population depends on the distributions @math and @math and the probability @math , which was not explicitly investigated in @cite . As a by-product, this proves the optimality of the S-SPRT.
- An instance of the quickest search problem was also studied recently in @cite , where the authors investigate the problem of finding a biased coin with the fewest flips. Our more general results are derived using different techniques, and cover this case with @math and @math as Bernoulli distributions. In @cite , the authors present a bound on the expected number of flips needed to find a biased coin. The bound derived from instantiating our more general theory (see example 2 and Corollary ) is a minimum of 32 times tighter than the bound in @cite .
- Work @cite considers problem with complex-valued quantities and @math equal to a down-sampled discrete Fourier operator, for which it establishes both the necessity and sufficiency of Condition 1 to the solution uniqueness of . Their proof uses the Hahn-Banach separation theorem and the Parseval formula. Work @cite lets the entries of matrix @math and vector @math in problem have complex values and gives a sufficient condition for its solution uniqueness. In regularization theory, Condition 1 is used to derive linear error bounds under the name of range or source conditions in @cite , which shows the necessity and sufficiency of Condition 1 for solution uniqueness of in a Hilbert-space setting. More recently, @cite constructs the set and then states that the set of vectors that can be recovered by problem is exactly characterized by the closure of @math if the measurement matrix @math satisfies the so-called (GP) condition, namely, for any sign vector @math , the set of columns @math of @math satisfying that any @math -dimensional affine subspace of @math , @math , contains at most @math points from the set @math . This paper claims that the result holds without the GP condition.
- To our knowledge, there are few conditions addressing the solution uniqueness of problems --. The following conditions in @cite , @cite are sufficient for @math to the unique minimizer of for @math : However, they are not necessary as demonstrated by the following example. Let A=, b=, =1 and consider solving the Lasso problem, which is a special case of problem : 1 2 |Ax-b |_2^2+ |x |_1. One gets the solution @math and @math . However, the inequality in condition ) holds with equality. In general, conditions becomes necessary in case @math happens to be a full rank square matrix. This assumption, however, does not apply to a sparse solution @math . Nevertheless, we summarize the result in the following corollary, whose proof is given at the end of Section .
- Very recently, work @cite investigates the solution uniqueness of and presents the following result. In Theorem , the necessity part for almost every @math '' is new. Indeed, it is not for every @math . An example is given in ) with a unique solution @math and @math , but @math does not full column rank. On the other hand, we can figure out a special case in which the full column rankness of @math becomes necessary for all @math in the following corollary, whose proof is given at the end of Section .
- Completing missing values in databases using Bayesian Networks has been addressed previously @cite @cite @cite @cite @cite . But most methods focus on completing the missing values so as to preserve the original data statistics so that other data mining techniques can be applied to it. We concentrate on retrieving relevant possible answers in the presence of missing values on single and multiple attributes. For example, @cite uses association rules to impute the value of the missing attributes, whereas we use Bayes Networks to impute the value as well as retrieve results.
- Work on querying inconsistent databases usually focuses on fixing problems with the query itself @cite @cite . If the query has an empty resultset, or if the query does not include all relevant keywords, it can be automatically augmented to fix those shortcomings. The objective of this work is to deal with shortcomings of the data --- our query rewriting algorithms help retrieve useful tuples even in the presence of multiple missing values in them.
- Recently Br "a nd 'e n @cite settled this conjecture in the affirmative giving spectrahedral representations of @math for @math of size @math . For each @math Br "a nd 'e n constructs a graph @math together with edge weights @math that are linear forms in @math so that where @math is the @math edge-weighted Laplacian of @math . Since @math is linear in the edge weights, and the edge weights are linear forms in @math , is a spectrahedral representation of size @math . With the exception of two distinguished vertices, the vertices of @math are indexed by all @math -tuples (for @math ) consisting of distinct elements of @math . Hence @math showing that Br "a nd 'e n's spectrahedral representation of @math has size @math . While Br "a nd 'e n's construction is of considerable theoretical interest, these representations (unlike ours) are not practical for optimization due to their prohibitive size.
- A spectrahedral representation of @math is implicit in the work of @cite that studies the relationships between matroids and hyperbolic polynomials. observe that if @math is a matroid represented by the rows of a totally unimodular matrix @math then @math is the basis generating polynomial of @math . In particular, the uniform matroid @math is regular and has @math as its basis generating polynomial, yielding a symmetric determinantal representation of @math and hence a spectrahedral representation of @math .
- From a computational perspective, G " u ler @cite showed that if @math has degree @math and is hyperbolic with respect to @math then @math is a self-concordant barrier function (with barrier parameter @math ) for the hyperbolicity cone @math . As such, as long as @math and its gradient and Hessian can be computed efficiently, one can use interior point methods to minimize a linear functional over an affine slice of @math efficiently. Renegar [Section 9] renegar2006hyperbolic gave an efficient interpolation-based method for computing @math (and its gradient and Hessian) whenever @math (and its gradient and Hessian) can be evaluated efficiently. G " u ler and Renegar's observations together yield efficient computational methods to optimize a linear functional over an affine slice of a derivative relaxation of a spectrahedral cone. Our results complement these, giving a method to solve optimization problems of this type using existing numerical procedures for semidefinite programming.
- @cite considered algorithms that can only access the database by making a polynomial number of statistical queries'' (essentially counting queries). They showed that such algorithms cannot be a one-shot sanitizer (even ignoring privacy constraints) that approximately answers certain simple families of counting queries with high accuracy.
- Finally, Dwork, Naor, and Vadhan @cite gave information-theoretic lower bounds for , which take @math queries as input, but whose answers to each query do not depend on the other @math input queries. They showed that (even computationally unbounded) stateless sanitizers can answer at most @math queries with non-trivial accuracy, while satisfying differential privacy. The Laplace Mechanism is a stateless sanitizer that answers @math queries, and thus their result is tight in this respect. Although their result is information theoretic, and considers a highly restricted type of sanitizer, their techniques are related to ours. We elaborate on this connection in the appendix.
- There are also some existing work on repeater selection and relay operation scheduling in mmWave WPANs. Repeater selection was investigated in @cite , with the objective to maximize data rate for each transmitter and receiver pair by determining the best link allocation. @cite , Lan explored time slot scheduling for relay operations in the scenario of directional antenna on mmWave devices and formulated the throughput maximization problem as an integer programming problem. However, both schemes do not consider robustness in presence of uncertain link blockage.
- Our work is also related to multihop routing in wireless networks with directional antenna @cite @cite @cite with two key differences. First, relays in our work are dedicated devices that do not generate or receive application layer packets. Second, we allow at most 2-hop paths between any mmWave transmitter-receiver pair, considering the fact that mmWave WPANs are deployed in small indoor environment with stringent QoS requirement.
- Biometrics, i.e., using human characteristics for identification and verification purposes has been an active research area for many years @cite . Conventionally, it is divided into two categories @cite : physiological and behavioral biometrics. While physiological biometrics rely on static physical attributes, such as fingerprints, hand geometry, facial features, or DNA, behavioral biometrics aim at identifying invariant features of the human behavior during different activities such as speaking, typing, or walking @cite .
- Early behavioral biometrics have been based on keystroke dynamics and mouse movements. In @cite , the authors introduce a keyboard system that captures timing and pressure characteristics to identify users based on entering telephone numbers and PIN inputs. The work in @cite improved the original system along different dimensions. The average error rates vary between 5 dynamics gained lots of popularity through @cite , where it was used to augment password authentication with additional security. A survey on the large body of literature on authentication with keystroke dynamics is given in @cite .
- Touchalytics resembles the field of on-line signature authentication or on-line signature verification @cite @cite in that it extracts temporal features of human gestures on planar surfaces. @cite , 49 temporal and geometric features are extracted from 5,603 signatures of 105 subjects. In newer contributions such as @cite , pressure is also used to compute features. Most approaches achieve an equal error rate (EER) between 1 complexity of gestures differs between signature verification and touchalytics. Compared to touch strokes, signatures are rather complex, which enables the extraction of more sophisticated features that support the authentication task. The main differences are the tools used and the availability of data. While in touchalytics the recording of all kinds of raw features is implicit for this interaction and is therefore always available, in signature verification only the spatial features are available without explicitly augmenting either pen or paper with extra sensors. This renders touchalytics more suitable for continuous authentication in practice.
- The two papers that are probably most related to our contribution are @cite and @cite . Both methods try to match recorded touch data with historical touch data of the user. In @cite , the authors augment a gesture-based authentication method with a behavioral classifier that supports the authentication. Users that know the secret gesture cannot authenticate unless they carry it out in the very same way as the true user does it. In @cite a set of 22 multi-touch gestures are used to authenticate 34 users on an iPad. The authors achieve EERs of 7 are significant differences in the problem setting of these two papers and our contribution. While in @cite @cite , a defined entry-point is required for the user to authenticate, we aim at an implicit and continuous scenario. Second, in our authentication scheme, the users can interact with the screen as they like, while in @cite @cite touch trajectories are compared with a particular (secret) gesture.
- Paul, Boutsidis, Magdon-Ismail, and Drineas @cite implemented our subspace embeddings and found that in the TechTC-300 matrices, a collection of 300 sparse matrices of document-term data, with an average of 150 to 200 rows and 15,000 columns, our subspace embeddings as used for the projection step in their SVM classifier are about 20 times faster than the Fast JL Transform, while maintaining the same classification accuracy. Despite this large improvement in the time for projecting the data, further research is needed for SVM classification, as the JL Transform empirically possesses additional properties important for SVM which make it faster to classify the projected data, even though the time to project the data using our method is faster.
- The MWRC is an extension of the two-way relay channel (TWRC), i.e., @math . Since there is an embedded relay channel from user 1 to user 2 via the relay, and vice versa, coding strategies proposed for relay networks @cite @cite , i.e., complete-decode-forward, This coding strategy is commonly known as decode-forward. We modify the name to distinguish this coding strategy and the functional-decode-forward coding strategy to be discussed next. compress-forward, and amplify-forward, have been modified for the TWRC @cite @cite .
- Besides the aforementioned classical relaying strategies for relay networks, @cite and @cite used the idea of network coding @cite for the TWRC and proposed that the relay performs a modulo-two summation of the users' messages, and broadcasts the summation. After getting the sum of the messages, each user can then obtain the other user's message by subtracting its own message from the sum. The network-coding-like operation can also be achieved at the modulation level, if amplify-forward is carried out using binary phase-shift keying (BPSK) @cite or minimum-shift keying (MSK) modulation @cite .
- Since the relay needs to broadcast only the modulo-sum of the messages (or in general a function defined such that, having the function and its message, each user is able to decode the message of the other user), the relay may directly decode the function that it will broadcast without needing to decode the individual messages. This strategy is known as functional-decode-forward and also compute-and-forward. This strategy was studied on the binary TWRC using binary linear codes @cite , and on the AWGN TWRC using lattice codes @cite @cite @cite . The scenario of having a relay decoding a linear function of messages from multiple senders was studied by Nazer and Gastpar @cite @cite .
- Complete-decode-forward, compress-forward, and amplify-forward for the TWRC were subsequently extended to the MWRC by G "und " @cite . We extended functional-decode-forward to the binary MWRC @cite . The idea was to split the uplink into @math blocks, and when the user pairs @math transmit in the blocks respectively. In each block, the relay decodes a function (in this case, the modulo-two sum) of the two corresponding messages, and broadcasts the function back to the users. If each user can decode the @math functions from the relay, it can then decode the messages of all the other users. In this paper, we extend this strategy to the AWGN MWRC. We propose a rotated scheme such that for each message to be sent, the uplink is again split into @math blocks, but different pairs of users are chosen from @math for each message in a round-robin fashion. This modification enables each user to transmit at a higher power when active while keeping the average transmitted power to @math . For each block, the two active users transmit using lattice codes as they do in the AWGN TWRC @cite .
- While the aforementioned functional-decode-forward for the binary MWRC @cite and for the AWGN MWRC (this paper) deals with users transmitting at the same rate, we have recently extended this coding strategy to the finite field MWRC where the users transmit at possibly different rates @cite . Note, however, that the rotation'' of the functional-decode-forward scheme introduced in this paper is not required for the binary MWRC or the finite field MWRC.
- The restricted MWRC was studied by G "und " @cite and @cite , where each user's transmitted signals can only depend on it message; however, the unrestricted MWRC is studied in this paper where each user's transmitted signals can depend on both its message and its previously received signals.
- It was shown by G "und " @cite that for the restricted AWGN MWRC, complete-decode-forward performs poorly at high SNR, compress-forward achieves rates within a constant fraction of a bit of the capacity at all SNRs, and amplify-forward is always worse than compress-forward at all SNRs. For the case where all nodes transmit at the same power, the same authors noted that complete-decode-forward achieves the capacity with a smaller number of users''.
- The bounds for unknown competition were improved in @cite to @math , optimizing the algorithm of @cite . Additionally, @math -approximate deterministic algorithms were given for a synchronized model where a carrier-sense primitive was assumed to be available. Finally, an @math -approximate randomized algorithm without a carrier-sense primitive was very recently proposed in @cite . A lower bound of @math was also given.
- Local broadcasting is related to the radio broadcasting problem in more classical models @cite @cite @cite , to initialization and wake-up problems in wireless networks @cite @cite as well as coloring problems on disc graphs @cite @cite .
- Recently, the SINR model has received considerable attention in the algorithms community, starting with the work of Moscibroda and Wattenhofer @cite . Constant-approximation factors are now known for capacity problems, both with fixed power @cite @cite and power control @cite . See the survey of @cite . Distributed algorithms have been given for dominating sets @cite , scheduling @cite @cite , coloring @cite , and connectivity and capacity @cite .
- Algorithms that leverage model symmetries to solve computationally challenging problems more efficiently exist in several fields. Most of the work is related to the computation of symmetry breaking predicates to improve SAT solver performance @cite @cite . The construction of our symmetry detection approach is largely derived from that of symmetry detection in propositional theories @cite @cite . More recently, similar symmetry detection approaches have been put to work for answer set programming @cite and integer linear programming @cite . Poole introduced the notion of lifted probabilistic inference as a variation of variable elimination taking advantage of the symmetries in graphical models resulting from probabilistic relational formalisms @cite . Following Poole's work, several algorithms for lifted probabilistic inference were developed such as lifted and counting belief propagation @cite @cite , bi-simulation-based approximate inference @cite , general purpose MCMC algorithm for relational models @cite and, more recently, first-order knowledge compilation techniques @cite @cite . In contrast to existing methods, we present an approach that is applicable to a much larger class of graphical models.
- There has been an extensive line of work on approximation algorithms for the @math -means problem ( @cite @cite @cite @cite @cite @cite ). The current best guarantee is a @math -approximation algorithm of @cite (with a much simpler analysis in @cite ) if polynomial dependence on @math and the dimension @math is desired. For constant @math , @cite give a PTAS for the @math -means problem. Another popular algorithm for @math -means is the Lloyd's heuristics ( @cite ). This heuristics, combined with a careful seeding of centers, has been shown to have good performance if the data is well separated (see @cite ), or to provide @math -approximation in general @cite . The separation-based results of @cite were improved by @cite .
- The idea of autonomic networking was introduced by IBM @cite . A similar concept underlies the self-organizing networks (SON) @cite . Recently, SON approach has been extensively studied for the application in 4G LTE mobile networks @cite . In @cite authors argue that some existing protocols like TCP or Open Shortest Path First (OSPF) may be treated as basic solutions for autonomic networking.
- The central network resource allocation problem is the network utility maximization problem (NUM), which is also discussed in this paper. As formulated in @cite , it provides the basis for further considerations. An extensive survey of the utility-based approach applied to the analysis of network resource allocation can be found in @cite . Moreover, in @cite it was shown that the NUM framework suits well for developing a self-managing mechanism for the Internet.
- A survey of the most important approaches in autonomic network management may be found in @cite and @cite . However, it is worth noting that the most common concepts towards self-management are related to control theoretic approach @cite , biological inspired mechanisms @cite and game theory @cite .
- Many interesting results in algorithmic game theory applied to computer networks have been obtained in the last ten years. In @cite two TCP clients are interpreted as players in the prisoner dilemma game. Another game-theoretical analysis of TCP is given in @cite , focusing on the Vegas version of this protocol. In @cite it is shown that the noncooperative games for flow control problems have Pareto-inefficient Nash equilibria.
- In @cite the co-existence of different congestion avoidance protocols is considered. It is shown that some properties related to the NUM approach do not hold in the presence of heterogeneous congestion signals. Such a situation is explained through game theoretical framework.
- Bottleneck games are a similar class of routing games, in which a different payoff function is used @cite . Although the Nash equilibria for such games usually exist, their performance (estimated via Price of Anarchy values) is usually poor. A game with a relatively low Price of Anarchy is proposed in @cite . In @cite two types of bottleneck games are considered, for splittable and unsplittable flows. It is also shown that for both proposed games the Price of Anarchy is unbounded. However, it is proven that under some mild conditions the Nash equilibrium is socially optimal.
- Work @cite considers both congestion game and bottleneck game, in the application to the routing problem. It also proposes a new routing game specifically for the elastic flows. All three approaches are compared. Basing on one example and two real network experiments, some advantages of the introduced game are shown.
- In @cite the approach to resource allocation for the networks with quality of service (QoS) based on Differentiated Services @cite architecture is proposed. The sources (flows) are players. They choose one QoS class and the transmission rate in this chosen class. The players' payoffs are proportional to the transmission rate if their QoS requirements are satisfied and zero otherwise. For the proposed noncooperative game, a simple algorithm computing Nash equilibrium is presented. The extension of this concept is given in @cite .
- The joint problem of QoS routing and capacity allocation problem is considered in @cite . In the proposed game, two groups of players are introduced, namely capacity players (each related to one link) and network users (each related to one pair of source and destination). Each capacity player divides its capacity among given Class of Services to minimize overall congestion over the associated link. On the other hand each user splits their traffic among all available paths so as to maximize a degree of satisfaction.
- Furthermore, another type of games called auctions @cite seems very suitable for computer network applications @cite . In @cite classic Vickerey-Clarke-Groves (VCG) mechanism @cite , together with the so-called Kelly mechanism (based on results obtained in @cite ), is used for the network resource allocation. In @cite the capacity allocation problem is stated as an auction game between flows (users), seen as buyers, and network operator, seen as an auctioneer. A distributed algorithm to find efficient Nash equilibrium is proposed. The presented mechanism is described as VCG-like, since, on the contrary to the classic VCG auction, it does not require a full valuation function.
- Currently, the game-theoretical framework is also extensively studied in the context of wireless networks @cite . For instance, in @cite bandwidth allocation problem for a class of wireless networks is investigated. The uniqueness of Nash equilibrium for some particular network topologies is shown. It is also stated that some of the presented results may be also generalized for different cases. In @cite the problem of choosing an access point by a mobile user is considered from the perspective of this approach. Similar frameworks for issues in wireless networks categorized under corresponding OSI Layers (namely: physical, data link, network and transport layers) are presented in @cite . The VCG auctions were also applied to the wireless networks, e.g. in @cite it is proposed for the resource allocation problem in multimedia wireless networks.
- In @cite coalitional games for a communication systems are considered. A classification of such games distinguishing three main types of cooperative games is given. It is stressed that the need of autonomic and self-organizing networks implies the necessity of developing distributed algorithms which enable each network device to make independent decision concerning network management. Application examples of cooperative games in computer networks, mainly wireless, are discussed. Presented arguments corroborate the game-theoretic approach as a promising solution for autonomic (and self-organizing) networks.
- The solution approach presented in this paper, can be seen as an efficient decentralized heuristic for the network utility maximization problem. There exists a large body of work on this subject; most of these works however focus on exact algorithms, formulated as gradient-based procedures @cite , Lagrangian methods @cite , dual decompositions @cite , Newton-type procedures @cite and interior-point methods @cite . Since the NUM problem is basically a convex optimization problem, it can be solved in polynomial time. However, due to large scale of practical instances, exact solutions often require considerable computational effort. Moreover, since many models involve uncertain parameters, it is justified to consider approximate solutions. For general multicommodity flow problems approximation algorithms were considered in @cite . For a special type of flow control problems, where formulation can be stated with the use of positive linear programs, a distributed approximation algorithm was given in @cite . Our solution is more general, as a wider class of utility functions are allowed, however only for a restricted subset of instances the approximation bounds are proven; for general network topologies we consider our algorithm a heuristic method, and evaluate its effectiveness via computational experiments.
- Much work has been done on polytope decomposition, volume computation, and intersection---mostly, however, on convex polytopes represented by vertices (via convex hull) or by (intersecting) half spaces. That representations do not allow non-convex polytopes and some of the above mentioned problems (even vertex enumeration) are NP-hard if no fixed dimension upper bound is given @cite . By using instead, vertices, edges, faces etc. are already enumerated as a precondition and the above problems can be avoided.
- A number of research projects concentrate their efforts on domain-specific languages and compilers. Among them, the SPIRAL project @cite and the Tensor Contraction Engine (TCE) @cite , focused on signal processing transforms and tensor contractions, respectively. As described throughout this paper, the main difference between our approach and SPIRAL is the inference of properties. Centered on general dense linear algebra operations, one of the goals of the FLAME project is the systematic generation of algorithms. The FLAME methodology, based on the partitioning of the operands and the automatic identification of loop-invariants @cite @cite , has been successfully applied to a number of operations, originating hundreds of high-performance algorithms.
- The approach described in this paper is orthogonal to FLAME. No partitioning of the operands takes place. Instead, the main idea is the mapping of operations onto high-performance kernels from available libraries, such as BLAS @cite and LAPACK.
- The success of crowdsourcing platforms on the web has generated a great deal of interest from researchers. Several studies have measured aspects of Amazon's Mechanical Turk, including worker demographics @cite @cite and task pricing @cite @cite @cite . There are studies that explore the pros and cons to use MTurk for user study @cite .
- Many studies address the problem of how to maximize accuracy from inherently unreliable turkers. The most common approach is to use majority voting @cite @cite , although this scheme is vulnerable to collusion attacks by malicious turkers @cite . Another approach is to pre-screen turkers with a questionnaire to filter out less reliable workers @cite . Finally, @cite proposes using a tournament algorithm to determine the correct answer for difficult tasks.
- In this study, we propose using crowdsourcing to solve a challenging OSN security problem. However, many studies have demonstrated how crowdsourcing can be used by attackers for malicious ends. Studies have observed malicious HITs asking turkers to send social spam @cite , perform search engine optimization (SEO) @cite , write fake reviews @cite , and even install malware on their systems @cite .
- Flip distance between triangulations of a convex polygon and rotation distance between binary trees have been well studied in the past. Several results deal with the combinatorics of the flip operation. @cite proved that for large values of @math , the flip distance between two triangulations of an @math -gon is at most @math and that occasionally @math flips are necessary. Other results deal with the , i.e., the graph where nodes correspond to triangulations of an @math -gon and an edge between two nodes denotes the fact that the corresponding triangulations are one flip apart. For example, Lucas @cite showed that the flip graph is hamiltonian. See also Eppstein @cite .
- Flips have been studied in more general settings as well. @cite proved that any two triangulations of a simple polygon can be transformed into one another using flips. They proved that the same holds even for two triangulations of a simple polygon with points inside it. The latter is also known in the literature as a , or simply a . This is because the graph whose nodes correspond to the points and edges correspond to the edges of the triangulation and of the polygon is a maximal planar graph (also called a triangulation) where one particular face is not necessarily a triangle. Lawson @cite proved an upper bound of @math flips needed in any such flip sequence. @cite proved that the bound is tight asymptotically.
- Our work falls within the scope of natural algorithms, a recent attempt to investigate biological phenomena from an algorithmic perspective @cite @cite @cite .
- The question of how important it is for individual processors to know their total number has recently been addressed in the context of locality. Generally speaking, it has been observed that for several classical local computation tasks, knowing the number of processors is not essential @cite . On the other hand, in the context of local decision, some evidence exist that such knowledge may be crucial for non-deterministic distributed decision @cite .
- In this paper, we focus on the case of close hypotheses and study the existence of LMPITs for the covariance structure of Gaussian data. We extend @cite to the general case of vector-valued observations that may or may not have the same covariance matrix under the null hypothesis. Instrumental in deriving these results is the application of Wijsman's theorem, which requires identifying the invariances of each problem and integrating the distribution of the transformed observations under a measure on the corresponding group of transformations. Although we focus on the case of complex Gaussian vectors, which is motivated by its applications in radar and spectrum sensing problems, the presented results can be easily proved for real vectors, which is left as an exercise for the interested reader. Specifically, the main contributions of this work are the following:
- Correlation Test : For vectors with different covariances under the null hypothesis, we show that the LMPIT is given by the Frobenius norm of the sample coherence matrix. The block diagram of the LMPIT is depicted in the lower part of Figure , where we can see that the whitening is different for each vector. We should also point out that the Frobenius norm of the coherence matrix, or similar squared-sum test statistics, have been previously proposed as approximations of the GLRT that might be computationally simpler or have some other advantage @cite @cite @cite @cite . Here, we prove for the first time that the Frobenius norm of the coherence matrix is in fact the LMPIT for this problem.
- Recent years have witnessed a significant research interest in data cleaning and enhancing data quality. A variety of approaches have been proposed with focus on noise elimination, missing value prediction, and noisy value correction. Some of them work directly on detecting and removing data corruptions but without fixing them, such as outlier detection @cite and noise removal @cite . On the other hand, some focus on fixing those corruptions alone, such as value imputation @cite . More recently, integrity constraints-based approaches have been proposed to capture and fix data corruptions such that the resulting database @math is either consistent and minimally differs from original database @math or certain errors in @math get fixed. These methods heavily use the editing rules which are generated from the (conditional) functional dependencies ( CFD s or FD s), (conditional) inclusion dependencies ( IND s or CIND s) or matching dependencies ( MD s) found from the data @cite @cite @cite @cite .
- Moreover, an interesting paper conducted a further analysis of the aforementioned methodologies and proposed a triplet of necessary standards that any theory aiming to provide a consistent prediction of elections using content from the Social Media should follow @cite . The authors recommend that firstly the prediction theory should be formed as a well defined algorithm, secondly the analysis should be aware of the different characteristics arising in the Social Web and thirdly that the method must include experimental justification on why it works.
- The probabilistic topic modeling task can be interpreted as a labeling problem, in which the objective is to assign a set of thematic topic labels, @math , to explain the observed elements in document-word matrix, @math . The notations @math and @math are the word index in vocabulary and the document index in corpus. The notation @math is the topic index. The nonzero element @math denotes the number of word counts at the index @math . For each word token @math , there is a topic label @math . Integrating out the document-specific topic proportions @math and topic distribution over vocabulary words @math in LDA yields the joint probability of the collapsed LDA @cite , where @math is the gamma function, and @math are fixed symmetric Dirichlet hyperparameters @cite .
- The synchronous BP @cite updates all messages simultaneously at iteration @math based on the messages at previous iteration @math . The asynchronous schedule updates the message of each variable in a given order; and the updated messages in turn are used to update other messages of neighboring variables at each iteration @math . We use RBP @cite for the informed schedule of asynchronous message passing. The basic idea of RBP is to select the best update order based on the residuals @math defined as the @math -norm of difference between two @math -tuple message vectors at successive iterations. Here, we choose the @math norm with @math , where @math is the number of word counts. If we sequentially update messages in descending order of @math at each iteration, the RBP algorithm theoretically converges faster or more often to a fixed point than synchronous BP @cite . Intuitively, RBP always updates messages with the largest residuals first, which in turn influence their neighboring messages efficiently. So, RBP accelerates the convergence speed of synchronous BP. More details on theoretical and experimental analysis of RBP's convergence property can be found in @cite @cite .
- The classical methods of line spectral estimation, often called linear prediction methods, are built upon the seminal interpolation method of Prony @cite . In the noiseless case, with as little as @math measurements, Prony's technique can identify the frequencies exactly, no matter how close the frequencies are. However, Prony's technique is known to be sensitive to noise due to instability of polynomial rooting @cite . Following Prony, several methods have been employed to robustify polynomial rooting method including the Matrix Pencil algorithm @cite , which recasts the polynomial rooting as a generalized eigenvalue problem and cleverly uses extra observations to guard against noise. The MUSIC @cite and ESPRIT @cite algorithms exploit the low rank structure of the autocorrelation matrix.
- Cadzow @cite proposed a heuristic that improves over MUSIC by exploiting the Toeplitz structure of the matric of moments by alternately projecting between the linear space of Toeplitz matrices and the space of rank @math matrices where @math is the desired model order. Cadzow's technique is very similar @cite to a popular technique in time series literature @cite @cite called Singular Spectrum Analysis @cite , which uses autocorrelation matrix instead of the matrix of moments for projection. Both these techniques may be viewed as instances of structured low rank approximation @cite which exploit additional structure beyond low rank structure used in subspace based methods such as MUSIC and ESPRIT. Cadzow's method has been identified as a fruitful preprocessing step for linear prediction methods @cite . A survey of classical linear prediction methods can be found in @cite @cite and an extensive list of references is given in @cite .
- In contrast to linear prediction methods, a number of authors @cite @cite @cite have suggested using compressive sensing and viewing the frequency estimation as a sparse approximation problem. For instance, @cite notes that the Lasso based method has better empirical localization performance than the popular MUSIC algorithm. However, the theoretical analysis of this phenomenon is complicated because of the need to replace the continuous frequency space by an oversampled frequency grid. Compressive sensing based results (see, for instance, @cite ) need to carefully control the incoherence of their linear maps to apply off-the-shelf tools from compressed sensing. It is important to note that the performance of our algorithm improves as the grid size increases. But this seems to contradict conventional wisdom in compressed sensing because our design matrix @math becomes more and more coherent, and limits how fine we can grid for the theoretical guarantees to hold.
- We circumvent the problems in the conventional compresssive sensing analysis by directly working in the continuous parameter space and hence step away from such notions as coherence, focussing on the geometry of the atomic set as the critical feature. By showing that the continuous approach is the limiting case of the Lasso based methods using the convergence of the corresponding atomic norms, we justify denoising line spectral signals using Lasso on a large grid. Since the original submission of this manuscript, Cand es and Fernandez-Granda @cite showed that our SDP formulation exactly recovers the correct frequencies in the noiseless case.
- Queueing models is the preferred approach to derive analytical results to analyze traffic metrics @cite . Priority service disciplines, such as the preemptive and non-preemptive service disciplines, are the most common approaches to implement service differentiation in communication networks. Furthermore, in an OSA network, the CR users must stop transmitting on an operating channel if the channel's primary user (PU) is detected or if the channel quality is unacceptable due, for example, to deep fading or interference. In the queueing model, the operating channel is the server of the queue. To achieve our objective, we must therefore analyze queueing models with priority service disciplines in the presence of frequent queue server interruptions.
- @cite we addressed several of those problems in a new queueing model for a single class of CR traffic for general operating and interruption period lengths. However, this model was limited to constant service time. @cite , an optimal threshold for the queue length to decide whether a packet should join the queue or not is derived. However, the model is again not general and can not be used to analyze traffic metrics.
- To the best of our knowledge @cite is one of the few papers discussing a queueing system with multiple classes of traffic in cognitive radio networks. The authors analyze a T-preemptive scheme and, similarly to the other work on opportunistic spectrum access networks, the queueing analysis does not consider general interruption lengths and it is specific to the priority service disciplines considered.
- The first class of techniques deal with prefetching objects ahead of use. An example of this is the approach of @cite , who place a prefetch pointer in linked list nodes to prefetch later nodes early enough to avoid cache miss penalties during traversals. Dynamic approaches are also possible such as that of who profile a program to detect frequently occurring streams of accesses @cite .
- In the context of dispatching problems, FIFO has been studied extensively in the literature since the early work by Winston @cite , @cite , and others. Often the number of tasks per server is assumed to be known, cf., e.g., (JSQ) dispatching policy @cite . Even though FIFO queues have received the most of the attention, also other scheduling disciplines have been studied. For example, Gupta et. al consider JSQ with (PS) scheduling discipline in @cite .
- Only a few optimality results are known for the dispatching problems. Assuming exponentially distributed interarrival times and job sizes, @cite shows that JSQ with FIFO minimizes the mean waiting time when the number in each queue is available. Also @cite argues for the optimality of JSQ FIFO when the number in each queue is available, while the Round-Robin (RR), followed by FIFO, is shown to be the optimal policy when it is only known that the queues were initially in the same state. @cite proves that RR FIFO is optimal with the absence of queue length information if the job sizes have a non-decreasing hazard function. The RR results were later generalized in @cite . Whitt @cite , on the other hand, provides several counterexamples where JSQ FIFO policy fails. @cite and Harchol- @cite assume that the dispatcher is aware of the size of a new job, but not of the state of the FIFO queues, and propose policies based on job size intervals (e.g., short jobs to one queue, and the rest to another). @cite later showed that such a policy is the optimal size-aware state-independent dispatching policy for homogeneous servers.
- Also the MDP framework lends itself to dispatching problems. Krishnan @cite has utilized it in the context of parallel M M s-FIFO servers so as to minimize the mean sojourn time, similarly as Aalto and Virtamo @cite for the traditional M M 1 queue. Recently, FIFO, LIFO, SPT and SRPT queues were analyzed in @cite with a general service time distribution. Similarly, PS is considered in @cite @cite . The key idea with the above work is to start by an arbitrary state-independent policy, and then carry out FPI step utilizing the value functions (relative values of states).
- Buhrman @cite show gaps between quantum and classical winning probability for games where the players are each given inputs and attempt, without communication, to produce outputs that satisfy some predicate. In the classical case they use shared randomness and in the quantum case, they use shared entanglement. Winning probabilities are linear so these translate to large Bell inequality violations.
- V 'ertesi show that there is a distribution with boolean outputs @math , based on partially entangled states, such that (in our language) @math @cite . Therefore, our results imply that @math . Since the states are nearly separable, however @math for large enough @math .
- Lower bounds for communication complexity of simulating distributions were first studied in a systematic way by Degorre @cite . These bounds are shown to be closely related to the nuclear norm and factorization norm @cite , and the dual expressions are interpreted as Bell inequality violations.
- Following up on the results in this paper, @cite used the notion of efficiency of zero-communication protocols to show that the information cost is bounded below by a relaxation of the partition bound which is larger than the smooth rectangle and @math bounds. Jain and Yao @cite followed up with a strong direct product theorem for the communication complexity of all functions for which an optimal lower bound can be shown using the smooth rectangle bound. Using a similar notion of zero-communication protocols, Gavinski and Lovett @cite showed that the log rank conjecture is equivalent to an upper bound which is polylogarithmic in the rank on the zero-communication cost. The notion of zero-communication cost that they use is the non-constant efficiency (Definition and Lemma .)
- Many data structures and scheduling algorithms have been proposed for advance reservations. Most of them are suitable for AR requests with immediate deadlines, and only few of them were specifically designed for AR requests with general deadlines. For AR request with immediate deadlines, such data structure as array @cite , linked-list @cite , trees @cite @cite @cite and queues @cite @cite have already been widely studied. These data structures are primarily used for admission control and focused on finding out whether it's feasible for the scheduler to accept an AR request to start at a definite time and keep on running for a given period. In @cite the author presents a good summary and comparison of them when they are used for single- or multi-processor AR jobs with immediate deadline constraint. However, they are not specifically designed for AR requests with general deadline constraint. Based on existing scheduling theory and algorithms for jobs with or without deadlines, some variants of scheduling algorithms for jobs with advance reservations have been studied in Grid-like systems @cite @cite @cite @cite and their impact on the users and the systems were investigated in terms of turnaround time, slowdown, or utilization.
- Additionally, in IHA all rewards are calculated based solely on the current sensor data. For many tasks, especially interaction tasks, this is not sufficient to determine whether the phenomena that should be reinforced is occurring unless the temporal horizon of experiences has been chosen to match the task @cite . In many cases, it is necessary to examine a sequential history of the sensor data for relationships between an agent's actions and those of the agent with which it interacts. We hypothesize that fluid turn-taking requires attention to the recent history of both one's own and the other's actions in order to anticipate and prepare for the shift in roles. In light of this, EIHA incorporates a short term memory over the recent history of sensor data relevant to the regulation of turn-taking (to be described in ).
- Annotations have a long research history, and unsurprisingly the research perspectives and interpretations of what an is supposed to be vary widely. @cite provide a comprehensive study on the contours and complexity of annotations. A representative discussion on how annotations can be used in various scholarly disciplines is given by Bradley @cite . He describes how annotations can support interpretation development by collecting notes, classifying resources, and identifying novel relationships between resources.
- The different forms and functions that annotations can take are analyzed by Marshall @cite . She distinguishes between and annotations, whereby formal annotations follow structural standards and informal ones are unstructured. Furthermore, Marshall divides into annotations that are intended for sharing and annotations of personal nature, often interpretable only by the original creator. Further divisions defined by Marshall with regard to the function of an annotation include vs. , vs. , vs. , vs. and vs. vs. . The difference between personal and public annotations in a digital environment is further investigated in a study by Marshall and Brush @cite . They derive design implications for annotation systems, e.g. regarding find and filtering requirements, and user interface strategies for processing and sharing annotations.
- The idea of publishing user annotations on the Web is not new. Annotea @cite was specified more than a decade ago and defines a data model and protocol for uploading, downloading, and modifying annotations. Since the Web has changed over time and now also comprises non-document Web resources, the Annotea model soon became insufficient for many annotation use cases, as we explained at the beginning of this article. Annotea extensions, such as Co-Annotea @cite or LEMO @cite , were developed to deal with the Annotea shortcomings and to take into account emerging architectural styles, such as RESTful Web Services, or Linked Open Data. Recent Web annotation model specification efforts include the M3O ontology @cite , the Annotation Ontology @cite , and the Open Annotation Model, which we presented in this article.
- In the off-line setting, the threshold for @math -orientations with @math can be shown (see, e.g. @cite ) to coincide with the appearance of a giant component in the random graph, which is known to happen at @math with high probability @cite . Several groups of researchers @cite @cite @cite independently established the thresholds for @math -orientations for every @math . Generalizing in another direction, Fernholz and Ramachandran @cite and Cain, Sanders, and Wormald @cite showed thresholds for @math -orientations for @math , and gave expected linear time algorithms for computing an orientation. This result was later extended to @math by @cite . Gao and Wormald @cite established thresholds for @math -orientations, given that @math is a sufficiently large constant (depending on @math and @math ). Independently of our work, Lelarge @cite recently developed new technical machinery for this problem, which handles all combinations of the parameters @math , @math , and @math that satisfy @math .
- One of the eye catching features of the time evolution of the @math -metric, as presented in Figure , is the large fluctuation of @math at early stages of the project development. This is in accordance with results reported in @cite . There, it was shown that young open source software projects display an accelerated growth rate while mature projects stabilize their dynamics and can grow further in a sustainable regime.
- Another possible, complementary, reason for fluctuations are refactoring events, where software is usually rewritten or restructured in order to improve multiple features such as functionality, flexibility, reusability or structural quality. Such events could lead to the sudden jumps observed in Figure along the time evolution of a software project. In @cite , refactoring metrics are proposed which take into account the dynamics of changing code. This line of research is well aligned with our purposes and can be easily adapted and augmented by our network perspective on software development processes.
- For an early attempt of the application of network science to the analysis of software engineering processes we recommend @cite , which also contains a short review of classical approaches used in the software engineering literature. Finally, a recent article published in the journal used a similar network approach, though with a different metric, to study modularity of code and its relation to module survival, drawing a parallel to ecological systems and making use of a predator-prey model variation @cite .
- In sparse recovery analysis, we want to know how good is our estimator @math in comparison to the target @math . Consider the standard @math regularization method ), two types of theoretical questions are of interests. The first is support recovery; that is, whether @math . The second is parameter estimation; that is, how small is @math . The support recovery problem is often studied under the so-called irrepresentable condition (some types also referred more generally as coherence condition) @cite @cite @cite @cite , while the parameter estimation problem is often studied under the so-called restricted isometry property (or RIP) as well as its generalizations @cite @cite @cite @cite @cite @cite . Related ideas have been extended to more complex structured sparse regularization problems such as group sparsity @cite @cite and certain matrix problems @cite @cite @cite . Closely related to parameter estimation is the so-called oracle inequality , which is particularly suitable for the dual-certificate analysis considered here.
- Finally, we would like to point out that while this paper successfully unifies the irrepresentable (or incoherence) conditions and RIP conditions under the general method of dual certificate, our analysis does not subsume some of the more elaborated analysis such as @cite and @cite as special case. Those studies employed a different generalization of RIP which we may refer to as the invertibility factor approach using the terminology of @cite . It thus remains open whether it is possible to develop an even more general theory that can include all previous sparse recovery analysis as special cases.
- provide extensible syntax in the OBJ3 language in the form of mixfix operators. In OBJ3, types (or sorts) play some role in disambiguation, but their papers do not describe the parsing algorithm. There is more literature regarding Maude @cite , one of the descendents of OBJ3. Maude uses the SCP algorithm of , which is bottom-up and bidirectional, much like our island parser. However, we have not been able to find a paper that describes how types are used for disambiguation in the Maude parser.
- Several systems use Ford's Parsing Expression Grammar (PEG) formalism @cite . PEGs are stylistically similar to CFGs; however, PEGs avoid ambiguity by introducing a prioritized choice operator for rule alternatives and PEGs disallow left-recursive rules. We claim that these two restrictions are not appropriate for composing DSLs. The order in which DSLs are imported should not matter and DSL authors should be free to use left recursion if that is the most natural way to express their grammar.
- The MetaBorg @cite system provides extensible syntax in support of embedding DSLs in general purpose languages. MetaBorg is built on the Stratego XT toolset which in turn used the syntax definition framework SDF @cite which uses scannerless GLR to parse arbitrary CFGs. Like Isabelle, the MetaBorg system performs type-based disambiguation to prune ill-typed parse trees from the resulting parse forest. Our treatment precedence and associativity is based on their notion of disambiguation filter @cite . We plan to explore the scannerless approach in the future. look into the problem of composing DSLs and investigate methods for composing parse tables. We currently do not create parse tables, but we may use these ideas in the future to further optimize the efficiency of our algorithm.
- In the above results on rectilinear duals and rectangular duals, the areas of the polygons are not considered; that is, these results deal with the unweighted version of the problem. The weighted version dates back to 1934 when Raisz described rectangular cartograms @cite . Algorithms by van Kreveld and Speckmann @cite and Heilmann @cite yield representations with touching rectangles but the adjacencies may be disturbed and there may also be a small distortions of the weights. Recently, Eppstein @cite characterized the class of planar graphs that have area-universal rectangular duals. The construction of the actual cartogram, given the area-universal rectilinear dual and the weight function, can be accomplished using a result by Wimer @cite , which in turn requires numerical iteration.
- There also exist CoM analyses for other types of structured matrices. For example, @cite derived concentration bounds for two types of block diagonal compressive matrices, one in which the blocks along the diagonal are random and independent, and one in which the blocks are random but equal. Shortly after our own development of CoM inequalities for compressive Toeplitz matrices (a preliminary version of Theorem appeared in @cite ), Yap and Rozell @cite showed that similar inequalities can be derived by extending the CoM results for block diagonal matrices. Our Theorem and the associated discussion, however, is unique to this paper. We subsequently extended these CoM results for block diagonal matrices to the observability matrices that arise in the analysis of linear dynamical systems @cite .
- Distributed controller synthesis is undecidable @cite even for reachability or simple safety conditions @cite @. A number of decidable subproblems have been proposed either by restricting the communication structures between components, such as pipelined, or by restricting the set of properties under consideration @cite @cite @cite @cite ; these restrictions usually limit applicability to a wide range of problems @. Schewe and Finkbiner's @cite bounded synthesis work on LTL specifications: when using automata-based methods, it requires that each process shall obtain the same information from the environment. The method is extended to encode locality constraints to work on arbitrary structures. Distributed priority synthesis, on one hand, its starting problem is a given distributed system, together with an additional safety requirement to ensure. On the other hand, it is also flexible enough to specify different communication architectures between the controllers such as master-slave in the multiprocessor scheduling example. To perform distributed execution, we have also explicitly indicate how such a strategy can be executed on concrete platforms.
- Starting with an arbitrary controller Katz, Peled and Schewe @cite @cite propose a knowledge-based approach for obtaining a decentralized controller by reducing the number of required communication between components. This approach assumes a fully connected communication structure, and the approach fails if the starting controller is inherently non-deployable.
- Bonakdarpour, Kulkarni and Lin @cite propose methods for adding for fault-recoveries for BIP components. The algorithms in @cite @cite are orthogonal in that they add additional behavior, for example new transitions, for individual components instead of determinizing possible interactions among components as in distributed priority synthesis @. However, distributed synthesis as described by @cite on distributed synthesis is restricted to local processes without joint interactions between components @.
- It is also important to reason that distributed priority synthesis, similar to our existing work, enables a direct execution of the resulting system (as synthesized priorities are artifacts compatible with the BIP model). The synthesized system is flexible in execution: it can be executed under a centralized controller or priorities can be equipped locally on each component (for distributed execution). Furthermore, priority synthesis is an approach which restricts existing behaviors predefined in the system, while works in @cite @cite target to add additional behaviors (i.e., non-existing transitions). Lastly, by releasing constraints inherent in BIP systems, we also study how (i) the use of knowledge and (ii) the release of transitivity constraints can assist the finding of distributed controllers.
- Lately, the problem of deploying priorities on a given architecture has gained increased recognition @cite @cite ; the advantage of priority synthesis is that the set of synthesized priorities is always known to be deployable. As a continuing toolchain, there are many existing works which try to deploy priorities on a given architecture @cite @cite , but these methods suffer from the fact that the set of priorities may be inherently undeployable. From these failures, our observation is that as priorities are control artifacts used to achieve certain specification, there can be different sets of priorities which achieve the same goal. What is required is to such a priority set respecting the architectural constraints. Therefore, our method reduces efforts in designing algorithms for priority deployment.
- Prebuffering video data has been studied in @cite @cite @cite . These techniques are focused on calculating the correct amount of prebuffered data based on a mathematical model to avoid the occurrence of a buffer underflow once the video playout has started. However, these techniques do not take into account of the varying playout rates and the resulting reduction in prebuffering into their models. Furthermore, prebuffering introduces unwanted delay into the system and is known to have an impact on the user perceived quality of the video @cite .
- Sender rate adaptation techniques such as encoder rate control @cite @cite and scalable rate control @cite achieve video continuity by ensuring the video rate matches the network bandwidth. However, these approaches do not take into account of the network delay variation that might occur (e.g. due to network jitter). Moreover, it has been demonstrated that coupling AMP with sender rate adaptation techniques can further reduce the prebuffering requirements @cite .
- AMP has been studied extensively for audio applications, @cite @cite @cite @cite @cite @cite . These audio AMP techniques depend on audio scaling techniques such as WSOLA @cite , which allows audio to be scaled without changing its pitch. A recent work by has shown that audio and video scaling can be done in real-time @cite . In this paper, we focus on AMP for video.
- AMP has also been integrated into the design of packet schedulers @cite @cite @cite @cite @cite . These techniques tend to slowdown the playout rate for important video packets. This ensures that the more important video packets have a higher probability of meeting its deadline and thus avoid being dropped. We do not focus on packet scheduling in this paper and our proposed framework could complement any packet scheduling scheme.
- Another aspect of AMP being studied is the smoothness of transition between playout rate adjustment @cite @cite @cite . The goal of these approaches is to ensure that adjustments made to the playout rate is done as smoothly as possible so as to reduce any noticeable effects to the viewers. We do not focus on rate smoothing in the current paper and again any smoothing scheme can be used within the framework.
- @cite and @cite both examined the trade-off between delay and buffer underflow using a two state Markov models. further proposed AMP-Initial, AMP-Robust and AMP-Live. AMP-Initial slows down the playout rate until the buffer reaches a certain target level, this produces a lower perceived initial buffering delay to the viewer. AMP-Robust slowdowns the playout rate if the current buffer occupancy falls below the target buffer level while AMP-Live slowdowns or speedups the playout rate to maintain the target buffer level.
- This work was encouraged by the reading of weber Data Mining Approach to Strategy Prediction and the fact that they provided their dataset. They tried and evaluated several machine learning algorithms on replays that were labeled with strategies (supervised learning). There are related works in the domains of opponent modeling @cite @cite @cite . The main methods used to these ends are case-based reasoning (CBR) and planning or plan recognition @cite @cite @cite @cite @cite . There are precedent works of Bayesian plan recognition @cite , even in games with @cite using dynamic Bayesian networks to recognize a user's plan in a multi-player dungeon adventure. Also, Chung05 describe a Monte-Carlo plan selection algorithm applied to Open RTS.
- LTW used CBR to perform dynamic plan retrieval extracted from domain knowledge in Wargus (Warcraft II clone). CBR_Planning base their real-time case-based planning (CBP) system on a plan dependency graph which is learned from human demonstration. In @cite @cite , they use CBR and expert demonstrations on Wargus. They improve the speed of CPB by using a decision tree to select relevant features. HsiehS08 based their work on CBR and LTW and used StarCraft replays to construct states and building sequences. Strategies are choices of building construction order in their model.
- SchaddBS07 describe opponent modeling through hierarchically structured models of the opponent behaviour and they applied their work to the Spring RTS (Total Annihilation clone). HTNPlanning use hierarchical task networks (HTN) to model strategies in a first person shooter with the goal to use HTN planners. OBRecog improve the probabilistic hostile agent task tracker (PHATT @cite , a simulated HMM for plan recognition) by encoding strategies as HTN.
- In order to annonymize the metering data, Efthymiou and Kalogridis @cite proposed a third party key escrow policy and uses several pseudonymous IDs instead of unique identifiers. Rial and Danesiz @cite proposed a privacy preserving protocol for smart meters using zero knowledge proof @cite , which ensures correct payment of fees with disclosing the details about consumption data. The protocol is implemented into smart meters and is generic enough to consider different billing settings like electronic traffic pricing, pay-as-you-drive car insurance etc.
- All the above schemes relied on a central key and attribute distribution center, which is prone to failures. Chase @cite proposed a multi-authority (same as multi-KDC) protocol, where several KDCs generate and distribute keys and attributes. There is also a central trusted authority who coordinates the multiple KDCs. To completely do away with central authority, Chase and Chow @cite proposed a scheme where the authorities can coordinate amongst themselves, but do not require a central authority. The drawback of this protocol was that the access structure was specific and required each user to have at least one attribute from each KDC. Both these scheme were KP-ABE.
- Rigorous work. The @math -NAESAT problem is well-known to be NP-complete in the worst case for any @math . In fact, the NP-complete problem of @math -coloring a @math -uniform hypergraph (with @math ) simply is the special case of @math -NAESAT without negations. The results in @cite are actually phrased in terms of hypergraph @math -coloring but carry over to @math -NAESAT directly.
- The main contribution of is the improved bound. In fact, the upper bound in ) can be obtained in several different ways. Achlioptas and Moore @cite state without proof that the (quite intricate) enhanced first moment argument from @cite @cite can be used to show that @math . This is indeed plausible as, in terms of the statistical mechanics intuition (which was unknown to the authors of @cite @cite ) this argument amounts to computing the first moment of the number of . Alternatively, generalizing work of Franz and Leone @cite , Panchenko and Talagrand @cite proved that the variational problem that results from the SP formalism yields a rigorous upper bound on @math , which is conjectured to be tight for any @math . The variational problem can be solved asymptotically in the large- @math limit (unpublished), yielding the upper bound stated in . In this paper we obtain the upper bound by a relatively simple third argument that has a neat combinatorial interpretation.
- The proofs of the lower bounds in @cite @cite and in the present paper are non-constructive in the sense that they do not entail an efficient algorithm for finding a NAE-solution The best current algorithm for random @math -NAESAT is known to succeed for @math , a factor of @math below @math @cite .
- From a statistical mechanics point of view, many random CSPs are similar to random @math -NAESAT. In particular, the physics methods suggest the existence of a condensation phase in most random CSPs (e.g., random @math -SAT graph @math -coloring). While @cite provided the prototype for the second moment arguments in these and other problems, the technical details in random graph @math -coloring @cite or random @math -SAT @cite are quite a bit more intricate than in random @math -NAESAT.
- Parts of our proof require a precise analysis of geometry of the solution space @math . This analysis harnesses some of the ideas that were developed in previous work @cite @cite @cite @cite (e.g., arguments for proving the existence of clusters or of rigid variables''). However, we need to go beyond these previous arguments significantly in two respects. First, we need to generalize them to accommodate the parameter @math that controls the cluster sizes. Second, we need rather precise quantitative information about the cluster structures.
- Survey Propagation guided decimation. The SP formalism has given rise to an efficient message passing algorithm called ( SPD') @cite . Experimentally, SPD seems spectacularly successful at solving, e.g., random @math -SAT for small values of @math . Unfortunately, no quantitative analysis of this algorithm is currently known (not even a non-rigorous one). The basic idea behind SPD is to approximate the marginals of the SP distribution (i.e., the probability that a given variable is true' in a solution drawn from the SP distribution) via a message passing heuristic. Then a variable @math is selected according to some rule and is assigned a value based on the (approximate) marginal. The entire procedure is repeated on the decimated'' problem instance where @math has been eliminated, until (hopefully) a solution is found.
- Abstraction and hierarchical decomposition are standard techniques in planning and other related search problems. The use of macro-operators dates back as far as Sacerdoti's early work on the planning system @cite which introduced abstraction hierarchies, whereby a problem could first be solved at a high level of abstraction while ignoring lower-level details. The idea has been re-expressed in many different ways through the history of planning -- far too many to review in detail. This present work was particularly inspired by the generic types' of longfox02 in which they similarly detected substructures in a task-planning problem and solved them using structure-specific planners.
- Hierarchical planning has been applied to path-planning before with abstractions such as approximate cell decomposition @cite @cite , generalised Voronoi graphs @cite @cite and general ad-hoc hierarchical maps @cite @cite @cite , but the structures identified in these examples do not carry over well to the multi-robot scenario.
- Other faster solutions to the multi-robot problem are available if we can assume the existence of garage'' locations for each robot @cite or other kinds of temporary free space @cite @cite . The method we present here makes no such assumption and is thus more general in application. There does not appear to be any previous work which provides a complete abstraction-based planner for the general multi-robot problem.
- The work that bears most similarity to our own is not explicitly in robot path planning, but in solving the Sokoban puzzle @cite @cite . That domain is significantly more constrained than ours (the map is necessarily an orthogonal grid and the stones can only move when they are pushed by the man) but the method they employ is similar. Dividing a map up into and they use the strongly-connected-component algorithm to identify equivalent arrangements of boulders in each subpart. Equivalent arrangements are then treated as a single abstract state -- corresponding to a configuration in our formulation -- which is used as the state in a global search. The particular structures they represent are different, but the general ideas of partitioning into independent local subproblems and identifying abstract states from strongly connected components, are the same as those employed in this work.
- The only previous work addressing the impact of uTP on BitTorrent completion time is our own recent work @cite , that however employs ns2 simulations unlike in this work. Interestingly, some of the observations of this study are in agreement with @cite , e.g., showing a larger completion time for increasing buffer occupancy on the data plane. Yet, we point out that @cite does not consider an hardcoded preference for uTP, nor bidirectional uTP connections: hence, an interesting difference with respect to the current work is that @cite forecasted heterogeneous performance for heterogeneous swarms (i.e., larger completion time for TCP peers) that we have shown not to hold on practice.
- In the past decade community detection has received a lot of attention from researchers spanning a wide range of domains and has consequently produced a wide range of approaches to the problem; for a comprehensive review of these approaches see @cite . Although there is no commonly accepted formal definition for community detection, a lot of approaches focus on the optimisation of the Newman-Givan modularity function @cite . The Newman-Givan modularity function, @math , of a network partition made up of @math disjoint communities is given by: where @math is the number of edges in community @math , @math is the number of edges in the network and @math is the total degree of the nodes in community @math . Commonly used as it is, the modularity function has been found to suffer from a number of issues such as a resolution limit @cite and degeneracy of solutions @cite . Rather than seek to find a globally optimum set of clusters, the proposed approach addresses the degeneracy issue by attempting to find clusters which are able to predict the class of a node.
- In part, due to the lack of formal problem definition, community detection lacks a commonly accepted evaluation procedure. To address this @cite proposes a benchmark graph generator and in later work @cite goes on to give a comparative analysis of a selection of community detection algorithms. Recognising that different community detection algorithms produce desirable outputs under different circumstances, the work in @cite considered the problem of selecting an appropriate community detection algorithm based on the properties of the network and community structure. These methods however rely on there being a single partition, which in many cases is unreasonable. The work presented here instead considers that there may be alternative partitions and instead searches for the partition relevant to a particular context.
- The main inspiration for this work comes from supervised Latent Dirichlet Allocation (sLDA) @cite which, as the name suggests, is a supervised extension of the topic modelling approach LDA @cite . Latent Dirichlet Allocation is a method for clustering a corpus of documents into mixed membership topics. The sLDA model extends the LDA approach to identify topics in documents which not only best describes the document structures but also to predict a known response variable (i.e. a classification or regression target) associated with each document.
- There are two main types of approaches for XML data mining in the literature. The first type of approaches applies relational data mining tools on XML data by mapping XML documents to relational data model and storing them in a relational database @cite . The second type of approaches applies data mining techniques directly onto native-XML data @cite @cite @cite . We are interested with the second type of approaches, specifically mining frequent itemsets and association rules from XML data. Wan and Dobbie provide XQuery implementation of the well-known Apriori algorithm @cite , to extract association rules from XML documents without any pre-processing or post-processing @cite . Their algorithm is adapted to simple and well-defined XML format. This algorithm is extended with pre-processing step in order to mine more complex and irregular XML documents @cite . Authors actually transform complex documents into a format that can be mined by Wan algorithm using XSLT. propose XMINE @cite , a tool to extract XML association rules from XML documents. The XMINE operator is based on XPath and XQuery to express complex mining tasks on the content and the structure of XML data.
- Detection and tracking of topics have been studied extensively in the area of topic detection and tracking (TDT) @cite . In this context, the main task is to either classify a new document into one of the known topics (tracking) or to detect that it belongs to none of the known categories. Subsequently, temporal structure of topics have been modeled and analyzed through dynamic model selection @cite , temporal text mining @cite , and factorial hidden Markov models @cite .
- All the above mentioned studies make use of textual content of the documents, but not the social content of the documents. The social content (links) have been utilized in the study of citation networks @cite . However, citation networks are often analyzed in a stationary setting.
- As mentioned above, the use of computers in creative tasks---for independent generation and or in assistance roles---is not a new idea. Computation has a particularly rich history in music composition, in both generative and assistant roles @cite . Route setting for indoor climbing is viewed by its practitioners as a creative task on par with music composition, requiring substantial expertise in order to create routes that are both of an appropriate difficulty and interesting to climb. A climbing route is a prescribed sequence of dynamic movements: a sequence of symbols from a complex language not unlike a dance or a tonal music composition, both of which can also be viewed as symbol sequences. All three of these domains---climbing, music, and dance---have strong notions of style,'' but that notion is very hard to formalize, even for experienced practitioners.
- Chaographer uses similar ideas to create variations on movement sequences, but with slightly different implementation of the mathematics and some necessary domain-specific changes @cite . In Chaographer , a symbol describes the state of 23 joints, which combine to articulate a body position. The nearest-neighbor calculation is generalized to the full dimension of the state space---without the directional restriction in Dabby's work---and care is taken that the initial condition falls on or near the attractor, which removes some of choice issues and their implications. The resulting movement sequence variations are essentially shuffled concatenations of subsequences of the original; the stylistic consonance derives from the subsequence structure, while the novelty derives from the chunking and shuffling. Chaographer 's companion tool, MotionMind , uses simple machine-learning strategies to smooth the potential dissonance that can occur at the subsequence boundaries @cite . MotionMind uses transition graphs and Bayesian networks to capture the patterns in a corpus of human movement, then uses those data structures to find a series of movements that create stylistically consonant interpolations. In a simple Turing Test, the chaotic variations were found to be only marginally less aesthetically appealing to human judges than those created by human choreographers @cite .
- The centrality of the multiclass learning problem has spurred the development of various approaches for tackling the task. Perhaps the most straightforward approach is a reduction from multiclass to binary, e.g. the one-vs-rest or all pairs constructions. The more direct approach we choose, in particular, the multiclass predictors of the form given in , has been extensively studied and showed a great success in practice --- see for example @cite @cite @cite .
- As mentioned before, ShareBoost follows the forward greedy selection approach for tackling the hardness of solving . The greedy approach has been widely studied in the context of learning sparse predictors for linear regression. However, in multiclass problems, one needs sparsity of groups of variables (columns of @math ). ShareBoost generalizes the fully corrective greedy selection procedure given in @cite to the case of selection of groups of variables, and our analysis follows similar techniques.
- Obtaining group sparsity by greedy methods has been also recently studied in @cite @cite , and indeed, ShareBoost shares similarities with these works. We differ from @cite in that our analysis does not impose strong assumptions (e.g. group-RIP) and so ShareBoost applies to a much wider array of applications. In addition, the specific criterion for choosing the next feature is different. In @cite , a ratio between difference in objective and different in costs is used. In ShareBoost, the L1 norm of the gradient matrix is used. For the multiclass problem with log loss, the criterion of ShareBoost is much easier to compute, especially in large scale problems. @cite suggested many other selection rules that are geared toward the squared loss, which is far from being an optimal loss function for multiclass problems.
- Multipath routing have been discussed since the early years of networking. A large body of literature is available addressing different issues of multipath routing. For example, @cite @cite @cite @cite describe the way to deduce loop-free multipath routes in a hop-by-hop basis. Some other, such as @cite @cite , transform the multipath routing problem into constrained optimization to find the optimal way to forward traffic for highest throughput and lowest congestion. QoS routing is addressed in @cite @cite @cite @cite @cite , which is to perform multipath routing to satisfy QoS guarantees.
- The results of this paper are a direct extension of the theory we introduced in @cite , and they stand on a similar foundation. Our paper @cite contains a detailed discussion from several historical perspectives.
- The general area of rigidity with symmetry has been somewhat active in the past few years, but the results here are independent of much of it. For completeness, we review some work along similar lines. A specialization of our [Theorem A] MT10 is due to Ross @cite @cite . Schulze @cite @cite and Schulze and Whiteley @cite studied the question of when incidental'' symmetry induces non-generic behaviors in finite frameworks, which is a different setting than the forced'' symmetry we consider here and in @cite . Ross, Schulze, and Whiteley @cite have studied the present problems, but they do not give any combinatorial characterizations. Borcea and Streinu @cite have proposed a kind of doubly generic'' periodic rigidity, where the combinatorial model does not include the colors on the quotient graph.
- Finding similar diverse solutions has been studied in other areas such as propositional logic @cite , constraint programming @cite @cite , and planning @cite . Let us briefly describe related work in each area, and discuss the similarities and the differences compared with our approach.
- As for the computational complexity analysis, Proposition 1 of @cite shows that in the general case distance-sat is -complete. However, determining whether @math has a model that disagrees with a complete interpretation @math on at most @math variables, where @math is a constant, is in .
- In @cite @cite , the authors study various computational problems related to finding similar diverse solutions. The main decision problems studied in @cite are the following:
- which are similar to @math -distant set (resp. @math -close set ): @cite asks for a set of @math solutions @math -distant close to one solution, whereas @math -distant close set asks for a set of solutions that is @math -close distant to a set of previously computed solutions. Also, the distance measure considered in @math Distant @math Set (resp. @math Close @math Set ) is computable in polynomial time; in @math -distant set (resp. @math -close set ) deciding whether the distance of a set of solutions is less than a given @math is assumed to be in .
- which are similar to @math -distant solution and @math -close solution . On the other hand, @cite asks for a solution @math -close to a partial solution rather than a set of solutions. Also, the distance measure considered in these problems is computable in polynomial time. However, with @math containing a single solution and @math being computable in polynomial time, @math -distant solution (resp. @math -close solution ) becomes essentially the same as @math Distant (resp. @math Close ).
- As for the computational complexity analysis of these problems, the authors find out that they are all -complete; these results comply with the ones presented in subject to conditions under which the problems of @cite @cite above are equivalent to the problems we study in this paper.
- Considering partial Hamming distance as in @cite , present an offline method (similar to our method) that applies clustering methods, and two online methods: one based on reformulation (similar to Online Method 1), the other based on a greedy algorithm (similar to Online Method 2) that iteratively computes a solution that maximizes similarity to previous solutions. The computation of a @math -close solution is due to a Branch & Bound algorithm (similar to the idea behind Online Method 3) that propagates some similarity diversity constraints specific to the given distance function. Our offline online methods are inspired by these methods of @cite @cite .
- The authors study these problems considering domain-independent distance measures computable in polynomial time (such as Hamming distance or set difference). They present a method (similar to our Online Method 1), where they add global constraints to the underlying constraint satisfaction solver of the GP-CSP planner @cite . As another method they present a greedy approach (similar to our Online Method 2), where they add global constraints which forces solver to compute @math -diverse solutions in each iteration until it computes @math solutions. They also present a method (similar to our Online Method 3) which modifies an existing planner's @cite heuristic function and computes @math @math -similar solutions in the search level.
- Also, our ASP-based methods tools may be preferred when it is easier to represent the main problem in ASP, due to advantages inherited from the expressive representation language of ASP, such as being able to define the transitive closure. Some sample applications include phylogenetic network reconstruction @cite and wire routing @cite @cite .
- ASP-based methods for computing similar diverse or close distant solutions to a given problem are probably most useful for existing well-studied ASP applications, such as phylogeny reconstruction @cite @cite or product configuration @cite , to be used with domain-specific measures.
- Table gives a summary of the major results in the top- @math frequent document retrieval problem. The time complexities are simplified by assuming that we are using the full text index proposed by Belazzougui and Navarro, of size @math bits and @math , where @math is the @math th order empirical entropy of @math @cite . We also assume @math for some @math and @math is any constant.
- The two inequalities in ) state the best previous bounds on the threshold for hypergraph @math -colorability from the paper of Achlioptas and Moore @cite , which provided the prototype for the second moment analyses in other sparse random CSPs (e.g., @cite @cite ). Since the second moment method is non-constructive, there is the separate algorithmic question: for what densities can a @math -coloring of a random hypergraph be constructed in polynomial time ? The best current algorithm is known to succeed up to @math for some constant @math , i.e., up to a factor of about @math below the @math -colorability threshold @cite .
- @cite the geometry of the set @math of @math -colorings of the random hypergraph was investigated (among other things). It was shown that @math shatters into exponentially small well-separated clusters' for densities @math . extends this picture up to @math . In addition, @cite also proved that in the regime @math a typical @math -coloring @math of @math is in the sense that for most vertices @math any @math -coloring @math with @math has Hamming distance @math from @math . Our analysis, most notably the study of the structure of a typical local cluster' in , builds substantially on the concepts of shattering and rigidity from @cite , but we will have to to elaborate them in considerably more detail to get close quantitative estimates.
- In many random CSPs other than random hypergraph @math -coloring the best current bounds on the thresholds for the existence of solutions derive from the second moment method as well. The most prominent examples are random graph @math -coloring @cite and random @math -SAT @cite . But the second moment argument extends naturally to a range of symmetric' random CSPs @cite . It would be interesting to see if how our techniques can be generalized in order to prove the existence of a condensation transition in these other problems, particularly random graph @math -coloring. However, since even the standard second moment analysis is quite involved in this case of random graph @math -coloring, such a generalization will be technically challenging.
- In statistical mechanics the condensation transition was first predicted (using non-rigorous techniques) for the random @math -SAT and the random graph @math -coloring problems @cite . For random hypergraph 2-coloring the statistical mechanics prediction for the condensation threshold was derived in @cite . The structure of the condensed phase is described using a non-rigorous framework called . Interestingly, it was also conjectured that the structure of the condensed phase for large @math is very similar to the structure of the random subcube model @cite . Our proofs verify this for random hypergraph 2-coloring.
- When @math is the local randomizer, SRM is a little reminiscent to the so-called Fast Johnson-Lindenstrauss Transform (FJLT) @cite . However, SRM employs a simpler matrix @math . In FJLT, this matrix @math is a completely random matrix with sparse distribution. It is unknown if there exists an efficient implementation of such a sparse random matrix. SRM is relevant for practical applications because of its high performance and fast computation.
- Random Convolution convolving the input signal with a random pulse followed by randomly subsampling measurements is proposed in @cite as a promising sensing method for practical applications. Although there are a few other methods that exploit the same idea of convolving a signal with a random pulse, for examples: Random Filter in @cite and Toeplitz structured sensing matrix in @cite , only the Random Convolution method can be shown to approach optimal sensing performance. While sensing methods such as Random Filter and Toeplitz-based CS methods subsample measurements structurally, the Random Convolution method subsamples measurements in a random fashion, a technique that is also employed in SRM. In addition, the Random Convolution method introduces randomness into the Fourier domain by randomizing phases of Fourier coefficients. These two techniques decouple stochastic dependence among measurements and thus, giving the Random Convolution method a higher performance.
- @cite were the first to study the geometric RAC drawing problem and proved that any graph with @math vertices that admits a geometric RAC drawing has at most @math edges. @cite presented bounds on the number of edges of polyline RAC drawings with at most one or two bends per edge. @cite presented acyclic planar digraphs that do not admit upward geometric RAC drawings and proved that the corresponding decision problem is @math -hard. @cite proved that it is @math -hard to decide whether a given graph admits a geometric RAC drawing (i.e., the upwardness requirement is relaxed). Di @cite presented tradeoffs on the maximum number of bends per edge, the required area and the crossing angle resolution. @cite characterized classes of complete bipartite graphs that admit geometric RAC drawings. Van Kreveld @cite showed that the quality of a planar drawing of a planar graph (measured in terms of area required, edge-length and angular resolution) can be improved if one allows right-angle crossings. Eades and Liotta @cite proved that a (i.e., @math ) is also @math -planar, i.e., it admits a drawing in which every edge is crossed at most once.
- Skyline queries have been imported to databases from the maximum vector problem or Pareto curve @cite in computational geometry. The first algorithm was proposed by @cite . BNL @cite uses a nested loop approach by repeatedly reading the set of tuples. SFS @cite improves it by sorting the data based on a monotone function. LESS @cite combines the best features of these external algorithms; however, its performance depends on the availability of pre-sorted data. A approach to partition the data so as to fit into the main memory was proposed in @cite . Using index structures, algorithms such as NN @cite and BBS @cite have been proposed.
- The idea of caching query results to optimize subsequent query processing was first studied in @cite @cite . Several algorithms have been proposed in @cite @cite @cite @cite that uses semantic caching efficiently and effectively for general applications. Also dynamic caching policies have been studied @cite .
- The other recent work @cite on learning submodular functions was motivated by bundle pricing and used passive supervised learning as a model for learning consumer valuations of added options. In particular, they have a polynomial-time algorithm that can learn (using random examples only) monotone , non-negative, submodular functions within a multiplicative factor of @math over distributions. As our machinery breaks down over non-product distributions, none of our results hold in this setting. For product distributions, Balcan and Harvey gave the first @math -time algorithm that can learn (using random examples only) monotone , non-negative, Lipschitz submodular functions with minimum value @math within a multiplicative factor of @math .
- @cite gave a detailed analysis of the communication performance of Cell BE using micro-benchmark, which encouraged our work on GUPS. They focused on the bus performance and did not give the result when large data set was used.
- @cite presented a software thread idea for list-ranking, which induced us for the GUPS implementation. For the SSCA2, the irregular data size makes much trouble for thread partition. So eventually we used a software pipeline method instead.
- @cite designed a delicate lock-free BFS algorithm on Cell BE. The algorithm depends on a bitmap in SPU's on-chip memory. During the optimization of SSCA2 over Cell BE, we found the main obstacle was the global atomic update. Each atomic operation will pause the pipeline with idle waiting. However it is not easy to design a lock-free algorithm due to the amount of globally random data updates. The process of SSCA2 need @math and @math set to be updated at the same time during the BFS expansion. These data structure are too large to fit in the on-chip memory.
- In @cite SSCA2 was porting to an innovative many-core platform, which split cores for memory operations and graph analysis.
- The research community agrees on the potential impact that custom IDEs and collaboration tools can have on effective distributed software development @cite . Recent years have shown a trend towards supporting IDEs into the web-browser. Some prominent examples of web IDEs are Cloud9 ( Cloud9IDE ), CodeRun Studio ( CodeRunStudio ), and Codeanywhere ( Codeanywhere ). As the browser is the natural workbench for web applications, most web IDEs target languages for web development (e.g., Javascript) rather than general purpose programming languages such as Eiffel or Java. Another limitation of most commercial web IDEs currently available is their focus on supporting run-of-the-mill functionalities that are standard in stand-alone IDEs, as opposed to embracing new development and communication modes.
- Some research prototypes of web IDEs have experimented novel approaches to collaborative development. Besides CloudStudio, the project Collabode @cite supports real-time code sharing among developers through a web IDE. Unlike CloudStudio, however, Collabode does not introduce new notions of CM, and it is mainly intended for developers simultaneoutly working on the same piece of code with the same view (similarly to CloudStudio's interweave mode).
- A different small group of research tools such as Syde @cite and CollabVS @cite introduce new models of CM, where more abstract and flexible change analyses are possible. Syde works on abstract syntax trees of the code, and defines changes as abstract operations on trees. Crystal @cite is based on the idea of constantly trying to merge the software artifacts of different developers in order to detect conflicts as early as possible. These concepts are quite novel, yet still ultimately centered on the notion of conflict (and conflict resolution). CloudStudio's focus is instead on providing programmers with real-time code change awareness, which can prevent many conflicts from arising in the first place.
- In a translation-based approach, all parts of the model are mapped into a single constraint language for which highly efficient off-the-shelf solvers are available. Hence, related work has mostly focussed on the translation of constraints to SAT (cf. @cite @cite ). Translation into ASP, however, can be more general than translation into SAT: Every nogood can be syntactically represented by a clause, but other ASP constructs are also possible, such as cardinality and weight constraints @cite . ASP was put forward as a novel paradigm for modelling and solving CSP in @cite , where straightforward encodings to represent generic constraints via either allowed or forbidden combination of values has been presented. Preliminary work on translating CASP into ASP was conducted in @cite , but they did not consider what level of consistency was achieved by their translation.
- Decompositions of all-different into simple arithmetic constraints such that bound and range consistency can be achieved were proposed in @cite . There is no polynomial-sized decomposition that achieves domain consistency @cite .
- The primary variant of matroid secretary problem (RO-AA-MN model) was introduced in @cite . In the following, let @math denote the total number of elements and @math the rank of the matroid. An @math -approximation for the RO-AA-MN model was given in @cite . It was also conjectured that a constant-factor approximation should exist for this problem and this question is still open. Very recently, Chakraborty and Lachish @cite improved @cite by giving an @math -approximation algorithm. Constant-factor approximations were given in @cite for some special cases such as partition matroids and graphic matroids with a given explicit representation. Further, constant-factor approximations were given for transversal matroids @cite @cite and laminar matroids @cite . However, even for graphic matroids in the RO-AA-MK model when the graphic matroid is given by an oracle, no constant factor is known.
- in @cite also posed as an open problem whether there is a constant-factor approximation algorithm for the following two models: Assume that a set of @math numerical values are assigned to the matroid elements using a random one-to-one correspondence but that the elements are presented in an adversarial order (AO-RA in our notation). Or, assume that both the assignment of values and the ordering of the elements in the input are random (RO-RA in our notation). The issue of whether the matroid is known beforehand is left somewhat ambiguous in @cite .
- While traditional distributed algorithms research has focused on computation in static networks, the analysis of dynamic network topologies has gained importance both in practice and theory. @cite offer an extensive review of this literature.
- Next to @cite the line of research most relevant to this work is network coding for gossip problems @cite @cite @cite @cite and most specifically work by Haeupler @cite . Since its introduction @cite @cite network coding has revolutionized the understanding of information flow in networks and found many practical applications (see, e.g., the books @cite @cite ).
- Random linear network coding and its distributed implementation considered in this paper were introduced by @cite and shown to achieve capacity for multicast. Its performance for the distributed @math -token dissemination problem has been intensively studied in combination with gossip algorithms under the name of algebraic gossip or rumor spreading. The first such analysis @cite @cite studied the performance of algebraic gossip in the random phone call model, i.e., the complete graph in which each nodes sends a message to a random neighbor in each round. Follow-on work @cite @cite @cite @cite has analyzed the distributed network coding gossip algorithm on general static networks. Haeupler @cite gives a very simple analysis technique (reviewed in ) that can be used to show order optimal stopping times in practically all communication models. Most interestingly this holds true even if, as studied here and in @cite , a fully adaptive adversary changes the topology in every round. In the setting considered here this would imply an optimal @math linear time algorithm for the @math -token dissemination problem. Unfortunately, these prior results do not directly apply for two subtle but important reasons:
- First, @cite , as well as all prior work on algebraic gossip, assumes that the additive overhead of the network coding header, which is linear in the number of coded packets, is negligible compared to the size of a packet. This assumption is backed up by many practical implementations in which this overhead is indeed less than one percent. But a rigorous theoretical treatment, like that of @cite , must account for this overhead which may be significant if message-sizes are small.
- Secondly, in all prior literature including @cite , it is also assumed that tokens are uniquely numbered indexed and that this index is known to any node that starts with a token. This is needed to allow nodes to specify in the coding header which packets are coded together in a message. In this paper such an assumption would be unacceptable. For example, for the task of counting the number of nodes in a dynamic network @cite having the IDs consecutively indexed would essentially amount to assuming that a solution to the counting problem is already part of the input.
- In this paper we address both points explicitly. Accounting for the coding overhead leads to interesting trade-offs and poses new algorithmic challenges like the need for many tokens in one node so that they can be grouped together to a smaller number of larger meta-tokens'' that require fewer coefficients. To this end we consider intermediate message sizes @math that can range between logarithmic size @cite to (super)linear size @cite @cite @cite @cite @cite @cite . We furthermore do not assume any token indexing or other extra coordination between nodes but show how to bootstrap the token dissemination algorithms to find such an indexing.
- The question of finding valid -colorings of random graphs online was first considered by Friedgut , who proved a threshold result for the case where @math is a triangle and @math colors are available @cite . @cite @cite , the greedy strategy was analyzed for the edge-coloring setting, and results similar to Theorem were derived for the case of @math colors. @cite , we presented the upper bound approach via deterministic two-player games discussed above; this approach was applied by Balogh and Butterfield to derive new upper bounds for the case where @math is a triangle and @math colors are available @cite . It would be very interesting to determine whether a general result analogous to Theorem holds for the edge-coloring setting. Various edge-coloring Builder-Painter games were studied in the context of deterministic Ramsey theory. The smallest number of moves Builder needs to win in the deterministic edge-coloring game without any restrictions is called the and was studied by many researchers @cite @cite @cite @cite @cite @cite @cite . Variants of the game where Builder is subject to various restrictions were studied in @cite @cite @cite .
- use information about node positions, distances between nodes, or angular relationships to detect network boundaries and holes. Accordingly, these approaches are limited to situations where GPS devices or similar equipment are available. Unfortunately, in many realistic scenarios this is not the case. Examples for geometrical approaches are Fang @cite , Martincic @cite , and Deogun @cite .
- try to recognize boundary nodes by low node degree or similar statistical properties. As long as nodes are smoothly distributed, this works quite well as boundary nodes usually have less neighbors than interior nodes. However, as soon as node degrees fluctuate noticeably, most statistical approaches produce many misclassifications. Besides, these algorithms often require unrealistic high average node degrees. Prominent statistical approaches are Fekete @cite @cite , and Bi @cite .
- Problem @math can be cast, in principle, as a composite optimization problem of the form eq:conv_comp x X , h(c(x)), by setting @math and @math as follows: eq:h_and_c h(s,v)= s ^2+J(v), c(x)=(F(x)-y,x). Problem has been deeply studied in @cite @cite @cite @cite @cite @cite from different point of views, mainly in the case @math and @math , but the hypotheses significantly differ, as well as the obtained results. More specifically, in @cite , the assumptions are too general to capture the features of problem @math , and allow only to get convergence results much weaker than the ones obtained in Theorem . Regarding all the remaining papers, as a matter of fact, the following special case of inclusion problem is treated eq:conv_incl c(x) C, C = h. In particular, the existence of an @math such that @math is always assumed. That hypothesis is of course reasonable if we think of @math as a kind of norm, but if we take it as in , then @math and we are lead to the condition [ ,x X, F(x)=y and x J ] which is too demanding for our original problem @math .
- Here the problem is to solve the nonlinear equation eq:eq F(x)=y in the ill-posed case. Typically a solution is found by introducing a regularization term weighted with a positive parameter. There are two possible approaches. The first one employs iterative methods which deal directly with problem , see @cite . In this case an iterative process is set up by minimizing at each step a simplified regularized problem (generally linear) having the structure of @math --- with a weight for @math varying at each iteration. Within this class of methods, one popular choice is the iteratively regularized Gauss-Newton method , see @cite @cite @cite @cite . Anyway, we remark that, despite the name, that algorithm is different from any kind of Gauss-Newton optimization method above, since it is not designed to optimize'' any objective functional @math , but, in fact, it directly looks for an exact solution of the equation .
- An alternative approach is the classical Tikhonov method @cite replacing by the minimization of the associated Tikhonov functional. A problem of the same type of @math arises, with @math weighted by a properly chosen parameter. Up to our knowledge, besides our method, the papers by Ramlau and Teschke, e.g. @cite @cite , are the only ones providing algorithms for the minimization of such type of functionals. In addition, the scheme they propose is different from ours and in general it converges only weakly. On the other hand, our algorithm assumes the derivative @math to be injective with closed range --- a hypothesis not suitable for handling the ill-posed case.
- Types were originally invented as simple names for sets of values that form part of a programming language @cite . As programming languages grew more sophisticated in keeping up with the complexity of the problems they were employed to solve, types evolved into more than simple sets of values and became an object of study in and of itself.
- It was found independently by Hindley @cite and Milner @cite that polymorphic types were useful for structuring programs. Since the early contributions of Hindley and Milner many alternatives and extensions to their approach have been suggested in the literature @cite @cite @cite @cite @cite @cite .
- More recently other uses of types are being explored that enrich type systems in various ways with constraints and qualifiers that can express certain invariants and in this way further the role of types in writing safe and correct programs @cite @cite .
- In @cite an efficient indexing scheme for storing and retrieving concept hierarchies over a fully decentralized system is given, even if folksonomies are not taken into account.
- A p2p infrastructure for tagging systems () is proposed in @cite ; in particular, the authors design a scheme to maintain feature vectors for characterization of users and resource of a tagging environment on a DHT. Feature vectors may be useful for calculating the similarity between users or for constructing algorithms for ranked retrieval.
- PINTS comes as a building block of Tagster @cite , a distributed content sharing and tagging system where the user-resource-tag graph is stored in a DHT. A dedicated storing index is used for each tagging relation, so each edge in the graph is stored at different overlay responsibility areas. For this reason, one lookup for each edge retrieval is needed, and this could make the navigation expensive in systems with a huge number of tags and objects. Furthermore, navigational aspects between related tags is not explicitly taken into account.
- In @cite , an hybrid structured-unstructured p2p approach is described. The scheme does not explicitly model a folksonomy with inter-tag relations and with the possibility to navigate through related labels.
- Tag-based navigation is taken into account in @cite , where a centralized web service discovery system based on folksonomies is presented. Tags, together with variables, are used to assign semantic information to input and output messages of the service operations. The key feature of this work is the possibility to exploit the subsumption relation between variable types to compose the discovery activity as an acyclic navigational workflow.
- Even if the work described in @cite is not related to p2p, it worths to be cited here because of the link the authors put between taxonomies and folksonomies. It has inspired us to further insights on query convergence.
- St "uzle @cite describes the simplest case of ACO parallelisation, in which independently instances of the ACO algorithm are run on different processors. Parallel runs have no communication overhead, and the final solution is taken as the best-solution over all independent executions. Improvements over non-communicating parallel runs may be obtained by exchange information among processors. Michel and Middendorf @cite present a solution based on this principle, whereby separate colonies exchange pheromone information. In more recent work, Chen @cite divide the ant population into equally-sized sub-colonies, each assigned to a different processor. Each sub-colony searches for an optimal local solution, and information is exchanged between processors periodically. Lin @cite propose dividing up the problem into subcomponents, with each subgraph assigned to a different processing unit. To explore a graph and find a complete solution, an ant moves from one processing unit to another, and messages are sent to update pheromone levels. The authors demonstrate that this approach reduces local complexity and memory requirements, thus improving overall efficiency.
- In terms of GPU-specific designs for the ACO algorithm, Jiening @cite propose an implementation of the Max-Min Ant System (one of many ACO variants) for the TSP, using C++ and NVIDIA Cg. They focus their attention on the tour construction stage, and compute the shortest path in the CPU. In @cite You discusses a CUDA implementation of the Ant System for the TSP. The tour construction stage is identified as a CUDA kernel, being launched by as many threads as there are artificial ants in the simulation. The tabu list of each ant is stored in shared memory, and the pheromone and distances matrices are stored in texture memory. The pheromone update stage is calculated on the CPU. You reports a 20x speed-up factor for benchmark instances up to 800 cities. Li @cite propose a method based on a fine-grained model for GPU-acceleration, which maps a parallel ACO algorithm to the GPU through CUDA. Ants are assigned to single processors, and they are connected by a population-structure @cite .
- Calibration for circular tomography devises is a variant of sensor localization, a problem that has been extensively studied for the past decade @cite @cite . In sensor localization, given the (i.e., which sensors are in the communication range of which others), the objective is to devise an algorithm that can infer the global position of the sensors. In practice, several methods are deployed as a means of obtaining this local information: the Signal Strength @cite , the Angle of Arrival (AOA) @cite , and the Time Difference of Arrival (TDOA) @cite . Our problem is naturally related to sensor localization when estimated TDOAs are used to measure the pairwise distances between nearby nodes. One should note that due to energy constraints, each node has a small communication range compared to the field size they are installed. As a result, only nodes within the communication range of each other can communicate and hence estimate their pairwise TDOA's. This situation is depicted in Fig. .
- The first sensor localization algorithm we consider is @cite . This algorithm has two phases. First, the Euclidean distance of far off sensors (i.e., the ones that are not in each other's communication range) are approximated by the shortest path between them. It was recently shown that having local connectivity, the shortest path is a reliable estimate of far off sensors @cite . Second, to estimate the relative positions of sensors, multidimensional scaling is applied to the approximated distance matrix. However, one can easily see that given faraway sensor's distances, the shortest path is a very coarse estimate of the distance between the close-by sensors. This makes perform very poorly in our setting.
- The most famous work that optimizes performance measures is @cite . By taking a multivariate prediction formulation, it finds the classifier in the function space directly. Our proposed CAPO works in a different manner and employs auxiliary classifiers to help find the target classifier in the function space. Furthermore, CAPO is a framework that can use different types of auxiliary classifiers. If nonlinear auxiliary classifier is used, the obtained classifier will also be nonlinear. This is very helpful, because nonlinear classifier is preferred in many applications while training nonlinear is computationally expensive. In summary, compared with , CAPO can provide the needed nonlinearity whilst keeping even improving computational efficiency.
- Another related work is A-SVM @cite , which learns a new SVM classifier by adapting auxiliary classifiers trained in other related domains. CAPO differs from A-SVM in several aspects: 1) CAPO aims to optimize specific performance measures, while A-SVM considers hinge loss; 2) The auxiliary classifiers of CAPO are used to help find the target classifier in the function space, while A-SVM is proposed for domain adaptation @cite and it employs auxiliary classifier to extract knowledge from related domains, similar ideas can be found in @cite . Generally speaking, classifier adaptation techniques which try to obtain a new classifier based on existed classifiers, were mainly used for domain adaptation in previous studies @cite @cite . Here, we use classifier adaptation to optimize specific performance measures, which is quite different.
- Ensemble learning is the learning paradigm which employs multiple learners to solve one task @cite , and it achieves state-of-the-art performance in many practice applications. In current work, the final classifier generated by CAPO is an ensemble constituting of auxiliary classifiers and the delta function. But, different from conventional ensemble methods, the component classifiers of CAPO are of two kinds and generated in two steps: first, auxiliary classifiers are trained; then a delta function which is designed to correct the decision of auxiliary classifiers is added such that the concerned performance measure is optimized.
- From the feature augmentation perspective, the nonlinear auxiliary classifiers construct nonlinear features that are augmented to the original features, so that the final classifier can have nonlinear generalization performance. This is like @cite which tries to change the representation of data by creating new features.
- Curriculum learning @cite is a learning paradigm which circumvents a challenging learning task by starting with relatively easier subtasks; then with the help of learnt subtasks, the target task can be effectively solved. It was first proposed for training neural networks in @cite , and is closely related to the idea of twice learning'' proposed in @cite , where a neural network ensemble was trained to help induce a decision tree. The study in @cite shows promising empirical results of curriculum learning. Our proposed CAPO is similar to curriculum learning since it also tries to solve a difficult problem by starting with relatively easier subtasks, but they are quite different because we do not provide a curriculum learning strategy.
- Since exact local alignment is too slow in practice, most heuristic homology search algorithms are based on a two-phase filtration technique @cite @cite @cite @cite . First, candidate sequences are selected that share a common pattern ( seed'') of matching characters with the query. These candidates (or hits'') are then further investigated by an exact method. Initially, (e.g., perfectly matching DNA 11-mers in the initial BLAST implementation) were used. PatternHunter (PH) by Ma et al. @cite was the first tool to systematically advocate and investigate : PH looks for 18-mers with at least 11 matching positions distributed as 111*1**1*1**11*111 , where 1 denotes a necessary match and * denotes a don't care position (match or mismatch). Over time, various seed models have been proposed in the literature, including consecutive seeds @cite @cite , spaced seeds @cite @cite @cite @cite , subset seeds @cite , vector seeds @cite , and indel seeds @cite .
- A good seed exhibits high sensitivity for alignments that model evolutionarily related biosequences, and low sensitivity values for alignments that represent unrelated sequences. The latter property ensures that the seed does not detect too many . Random hits decrease the efficiency of the filtration phase, since they are checked in vain for a significant alignment. An interesting finding was that the PH approach led to an increase in both sensitivity and filtration efficiency, compared to seeds of contiguous matches. Based on the observations in @cite , the advantages of spaced seeds over consecutive seeds have been subsequently evaluated by many authors @cite @cite @cite .
- An extension to single seed models is the design of a multiple seed. This is a set of spaced seeds to be used simultaneously, such that a similarity is detected when it is found by (at least) one of the seeds. The idea to use a family of spaced seeds for BLAST-type DNA local alignment has been suggested by Ma et al. @cite and was implemented in PatternHunter II @cite . It has also been applied to local protein alignment in @cite . Recent approaches @cite @cite approximate the sensitivity of multiple spaced seeds by means of correlation functions. Since finding optimal multiple seeds is challenging, most authors concentrate on the design of efficient sets of seeds, leading to higher sensitivity than optimal single seeds @cite @cite @cite .
- Of course, the problem has different solutions depending on the specific compression algorithm @math that is taken into consideration. In the following, we shall focus on the so-called BV compression scheme @cite used within the WebGraph framework, which incorporates the main ideas adopted in earlier systems and is a standard for handling large web-like graphs. In particular, the framework strongly relies on similarity and locality to achieve its good compression results; for this reason, we believe that most compressed structures that are based on the same properties will probably display a similar behaviour.
- The only coordinate-free compression algorithm we are aware of The quite extensive survey in @cite shows that many other approaches to web-graph compression, not quoted here, either fail to compress social networks, or are strongly dependent on the initial ordering of the graph. is that proposed by Apostolico and Drovandi in @cite ; Our experiments show in fact a very limited variation in compression (10--15 URL ordering or from a random permutation, except for the altavista-nd dataset, which however is quite pathological. they exploit a breadth-first search (BFS) to obtain an ordering of the graph and they devise a new compression scheme that takes full advantage of it. Their algorithm has a parameter, the , which can be tuned to obtain different trade-offs between compression performance and time to retrieve the adjacency list of a node: at level 8 they attain better compression performances than those obtained by BV with Gray orderings and have a similar speed in retrieving the adjacency list. Even in this optimal setting, though, their approach is outperformed by the one we are going to present (see Table ).
- The performance of codes with random scheduling of disjoint generations was first theoretically analyzed in @cite by , who referred to them as chunked codes . Chunked codes allow convenient encoding at intermediate nodes, and are readily suitable for peer-to-peer file dissemination. In @cite , the authors used an adversarial schedule as the network model and characterized the code performance under certain restrictions on the chunk(generation) size when the length of the information to be encoded tends to infinity. Coding with overlapping generations was first studied in @cite and @cite with the goal to improve throughput. Reference @cite studied a head-to-toe'' overlapping scheme in which only contiguous generations overlap for a given number of information packets, and analyzed its asymptotic performance over a line network when the length of information goes to infinity. Another overlapping scheme with a grid structure was proposed in @cite , analyzed for short lengths (e.g., @math generations) and simulated for practical lengths. When properly designed, these codes show improved performance over codes with disjoint generations. In our work, we offer an analysis of coding over disjoint and overlapping generations for finite but practically long information lengths.
- By investigating the relationship between neocortex size and group size in primates, Dunbar @cite predicted the number of group size in human beings was 150, which was notable as Dunbar's number. According to him, human beings can only maintain a small fraction of relationships within the circle of Dunbar's number, and other relationships beyond that circle are not reciprocated or personalized. Dunbar's number targets on real-world social networks when first put forward. However, recent works in online social networks have displayed similar interesting observations @cite @cite . pointed out that time spent using social media, including online social sites, was not associated with larger offline networks @cite . Potential time and cognitive constraints were also considered in their work. Other work of online social networks related to Dunbar's number will be further discussed in Section .
- Recently, researchers have done intensive study in online social networks. They measured the property of online social networks from different perspectives. Phenomena such as small world, power-law, high clustering, assortativity have been observed in different social sites, which are believed to be the common properties of online social networks. studied the largest online social networks Cyworld in South Korea @cite . They experimented on the whole data of Cyworld and discovered some unique characters of this site. They found an interesting phenomenon that most user connections were not active and attributed it to Dunbar's number. used data from Flickr, YouTube, LiveJounal and Orkut, conducting measurement analysis in a large scale @cite . They incorporated various complex network measurements such as degree distribution, clustering coefficients, degree correlations, connected core etc. in the research. analyzed Facebook users in North American colleges or universities @cite . Their results on degree distribution showed that the number of people who have few hundreds of friends remained stable, but it started to drop sharply once the friends number exceeded 250, which also coincided with Dunbar's number.
- GPUs were first used to perform LQCD calculations in @cite . This pioneering study predated various programmability improvements, such as the C for CUDA framework, and hence was implemented using the OpenGL graphics API. It targeted single GPU devices. The QUDA library @cite was discussed extensively in @cite @cite , where the primary techniques and algorithms for maximizing the efficient use of memory bandwidth were presented for a single GPU device. LQCD on GPUs has also been explored in @cite , which focused on questions of fine grained vs. coarse grained parallelism on single GPU devices. In addition, there are several as yet unpublished efforts aimed at exploiting GPUs for LQCD underway.
- LQCD has also been implemented on other heterogeneous devices, primarily on the Cell Broadband Engine. Efforts in this direction have been reported in @cite @cite as part of the QCD Parallel Computing on the Cell Broadband Engine'' (QPACE) project and elsewhere @cite @cite .
- Outside the context of LQCD, general challenges of implementing message passing on heterogeneous architectures have been considered for GPUs in @cite and for the RoadRunner supercomputer in @cite . An effort to provide a general message passing framework utilizing CUDA, MPI, and POSIX threads is also underway at Jefferson Laboratory @cite .
- Grohe @cite , answering a question of Downey and Fellows @cite , proved that the crossing number is fixed-parameter tractable. In particular, for any fixed number of crossings his algorithm computes an optimal drawing in @math time. Building upon the breakthrough result of Mohar @cite for embedding graphs into a surface of bounded genus, Kawarabayashi and Reed @cite gave an improved fixed-parameter algorithm with running time @math .
- In their seminal paper, Leighton and Rao @cite , combining their separator theorem with the framework of Bhatt and Leighton @cite , gave the first non-trivial approximation algorithm for crossing number. More precisely, for a given bounded-degree graph @math their algorithm computes a drawing with at most @math crossings. This was later improved to @math by @cite . Note that in general, these algorithms imply only a @math -approximation.
- Computing the crossing number of a graph was shown to be NP-complete by Garey and Johnson @cite . Hlin e n ' y @cite showed that the problem remains NP-complete even on cubic graphs. Combining the reduction from @cite with the inapproximability result for Minimum-Linear-Arrangement of @cite , it follows that there is no PTAS for crossing number unless NP-hard problems can be solved in randomized subexponential time. Using a reduction from Min-Uncut, and assuming the Unique Games Conjecture, one can obtain that the crossing number is hard to approximate within any constant factor.
- @cite gave a @math -approximation for projective graphs of bounded degree, and Hlin e n ' y and Salazar @cite a @math -approximation for toroidal graphs of bounded degree.
- Motivated by the problem of storing records in a 2-dimensional array, @cite studied strategies that minimize average access time between successive queries; among other results, they described an optimal solution for the continuous version of our problem: What shape of area @math minimizes the average Manhattan distance between two interior points? Independently, @cite solved this problem in the setting of a city, inspiring the subtitle of this paper. The optimal solution is described by a differential equation, and no closed-form solution is known.
- @cite consider the discrete problem of selecting a subset of @math nodes from a network with @math nodes to minimize their average pairwise distance. They prove a @math -approximation for metric distances and prove hardness of approximation for arbitrary distances. @cite solve the geometric version of this problem, giving an efficient processor allocator for the Cplant setting described above, and a polynomial-time approximation scheme (PTAS) for minimizing the average Manhattan distance. For the reverse problem of maximizing the average Manhattan distance, see @cite .
- Our problem is a special case of the quadratic assignment problem (QAP): Given @math facilities, @math locations, a matrix containing the amount of flow between any pair of facilities, and a matrix containing the distances between any pair of locations. The task is to assign every facility to a location such that the cost function which is proportional to the flow between the facilities multiplied by the distances between the locations is minimized. For a survey see @cite . The cost function in our problem and in the QAP are the same if we define the distances as the Manhattan distances between grid points and if we define all flows to be one. The QAP can not be approximated within any polynomial factor unless @math ; see @cite . @cite considered the metric version of this problem with the flow matrix being a 0 1 incidence matrix of a graph. They state some inapproximability results as well as a constant-factor approximation for the case in which the graph has vertex degree two for all but one vertex.
- The reverse version of the discrete problem, where the goal is to maximize the average distance between points, has also been studied: In the maximization version, called the , the objective is to pick @math points from a set of size @math so that the pairwise distance is maximized. When the edge weights need not obey the triangle inequality, @cite give an @math -approximation. @cite improve this guarantee to a constant factor in the special case when @math and @cite give a PTAS when @math and @math .
- When the edge weights obey the triangle inequality, @cite give a @math -approximation that runs in @math time and @cite give a @math -approximation that runs in @math time. For points in the plane and Euclidean distances, @cite give an approximation with performance bound arbitrarily close to @math . For Manhattan distances, @cite give an optimal algorithm for fixed @math and a PTAS for general @math . Moreover, they provide a @math -Approximation for Euclidean distances.
- Another related problem is called or . The goal is to separate a graph into @math clusters to minimize the sum of pairwise distances between nodes in the same cluster. For general graphs, @cite show that this problem is NP-hard to approximate within any constant factor for @math . In a metric space the problem is easier to approximate: @cite give a @math -approximation, @cite gives a PTAS for @math , and @cite give an @math -approximation for general @math .
- Numerous models and analyses have been proposed for wireless networks with TCP-controlled traffic, but very few consider propagation delays. In @cite , RTT is considered in modelling the TCP traffic in a WLAN. However, the authors' interest was in showing that 802.11e supports features that can be exploited to overcome certain TCP performance anomalies.
- @cite and @cite provide a model for single rate AP-STA WLAN assuming zero RTT and consider file transfers from a server located in the LAN. An extension of this model in @cite considers two rates of association with long file uploads from STAs to a local server. The multirate case, with @math rates, is analysed in @cite . @cite considers the single rate case, but allows simultaneous TCP uploads and downloads with arbitrary maximum window sizes. @cite and @cite analyze TCP-controlled uploads and downloads in the presence of UDP traffic. However, the effect of RTT on the network performance is ignored. The letter @cite gives the average value analysis of TCP performance with upload and download traffic without considering RTT. In @cite , finite buffer AP with TCP traffic in both upload and download direction is analysed with delayed and undelayed ACK cases. They consider server system located on the Ethernet to which the AP is connected and hence number of packets in flight'' outside the WLAN is ignored.
- @cite provides an analysis for a given number of STAs and maximum TCP receive window size by using the well known @math persistent model proposed in @cite . However both @cite and @cite do not consider the effect of RTT on the performance. In @cite , a queueing model is proposed to compute the mean session delay of HTTP sessions in the presence of short-lived TCP flows and the impact of TCP maximum congestion window size on this delay is studied.
- Substantial work has already been done in the control community in formulating various results of classical thermodynamics in a more mathematical framework. In @cite @cite , the second law of thermodynamics is derived and a control-theoretic heat engine is obtained (in @cite these results are generalized). In @cite , a rigorous dynamical systems approach is taken to derive the laws of thermodynamics using the framework of dissipative systems @cite @cite . In @cite , it is shown how the entropy flows in Kalman-Bucy filters, and in @cite Linear-Quadratic-Gaussian control theory is used to construct heat engines. In @cite @cite @cite , the problem of how lossless systems can appear dissipative (compare with @cite @cite @cite above) is discussed using various perspectives. In @cite , how the direction of time affects the difficulty of controlling a process is discussed.
- Although subgraph isomorphism is known to be NP-complete, it is solvable in polynomial time for small subgraphs. For example, all triangles and four-cycles can be found in an @math -vertex graph with @math edges in @math time @cite @cite . All cycles up to length seven can be counted (but not listed) in @math time @cite , where @math is the exponent for the asymptotically fastest known matrix multiplication algorithm @cite . In addition, fast matrix multiplication has also been used for other problems of finding and counting small cliques in graphs and hypergraphs @cite @cite @cite @cite @cite . Also, in planar graphs, the number of copies of any fixed subgraph may be found in linear time @cite @cite . These previous approaches run too slowly for the iterative nature of ERGM Markov Chain Monte Carlo simulations, however.
- From the extensive literature on association in WLANs, several themes can be discerned. In the first group, a newcomer STA assesses each AP (that it can hear) by finding out the per-STA throughput received by STAs associated with it currently @cite , @cite and @cite . In these papers, the authors derive expressions for aggregate AP throughput (with UDP traffic), taking into account the number of STAs associated, as well as the probability of packet loss due to collisions. From this, the per-STA throughput can be calculated, and the newcomer associates with the AP for which the per-STA throughput is the highest.
- In the second group, we have association policies that start from the throughputs that individual STAs are getting at present @cite , @cite , @cite , @cite and @cite . For each STA associated with an AP, the ratio of throughput to the physical rate of association is obtained, and this fraction is added over all the STAs. This leads to a figure of merit for each AP, and the newcomer chooses the AP with the highest figure of merit.
- In the third group, the selection of the AP is made by considering Jain's fairness index @math , @math and @math , corresponding to @math , @math and @math respectively. The AP selected is the one for which the Jain Fairness Index is the highest @cite . In the fourth group, @math is mapped to an expression for AP load @math , and the AP which is least loaded is selected. A variation considers a map from the vector of throughputs @math and the vector of rates of association to the load @cite . Finally, some association policies base their choice on maximizing the minimum element of @math , @math and @math ( max-min fairness''), or maximizing the sum of the logarithms of the elements of @math , @math , @cite , @cite , @cite , @cite , and @cite .
- Recently, some high-dimensional problems have been studied from a minimax point of view. Wainwright @cite @cite provides minimax lower bounds for the problem of support estimation ( @math ). @cite and Rigollet and Tsybakov @cite have provided minimax upper bounds and lower bounds for @math and @math over @math balls for general fixed designs @math when the variance @math is known (see also Ye and Zhang @cite and Abramovich and Grinshtein @cite ). Arias- @cite and @cite have computed the asymptotic minimax detection boundaries for the testing problem @math for some specific designs. However, their study only encompasses reasonable dimensional problems ( @math grows polynomially with @math ). Some minimax lower bounds have also been stated for testing ( @math ) and prediction ( @math ) problems with Gaussian design @cite @cite . All the aforementioned results do not cover the ultra-high dimensional case and do not tackle the problem of adaptation to both @math and @math .
- Support estimation. In a non-ultra high dimensional setting it is known @cite that under some assumptions on the design @math (e.g. each component of @math is drawn from iid. standard normal distribution) the support of a @math -sparse vector @math is recoverable with high probability if where @math is a numerical constant. In an ultra-high dimensional setting, even if it is not possible to estimate the support of @math with high probability. Observe that the condition ) is much stronger than ). In fact, it is not even possible to reduce drastically the dimension of the problem without forgetting relevant variables with positive probability. More precisely, for any dimension reduction procedure that selects a subset of variables @math of size @math with some @math (described in Proposition ), we have @math with probability away from zero (see Proposition ). Thus, it is almost hopeless to have a reliable estimation of the support of @math even if @math is large. This impossibility of dimension reduction for ultra-high dimensional problems is numerically illustrated in Section .
- The authors of @cite justify their set-based approach by the following statement: Our approach can be understood as a compromise since we avoid a propagation history by imposing an implicit set semantics on persistent constraints. The distinction between linear and persistent constraints, however, allows us to restrict the set behavior to those constraints, whereas the multiset semantics is preserved for linear constraints.
- Linear logical algorithms @cite (LLA) is a programming language based on bottom-up reasoning in linear logic, inspired by logical algorithms @cite . The first implementation of logical algorithms was realized in CHR with rule priorities @cite .
- Our proposed operational semantics @math is related to LLA @cite , but displays significant differences: Firstly, the notion of a constraint theory with built-in constraints is absent in LLA. Secondly, LLA rules are restricted such that persistent propositions cannot be derived multiple times, whereas @math makes no such restriction and solves this problem via the irreflexive transition system. Thirdly, LLA requires a strict separation of propositions into linear and persistent ones. In @math a CHR constraint can occur in the linear store, in the persistent store, or both.
- Due to the topic's importance, wireless network jamming has been extensively studied in the applied research fields @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite , both from the attacker's perspective @cite @cite @cite @cite as well as from the defender's perspective @cite @cite @cite @cite @cite @cite @cite @cite ---also in multi-hop settings (e.g. @cite @cite @cite @cite @cite ).
- Traditionally, jamming defense mechanisms operate on the physical layer @cite @cite @cite . Mechanisms have been designed to avoid jamming as well as detect jamming. Spread spectrum technology has been shown to be very effective to avoid jamming as with widely spread signals, it becomes harder to detect the start of a packet quickly enough in order to jam it. Unfortunately, protocols such as IEEE 802.11b use relatively narrow spreading @cite , and some other IEEE 802.11 variants spread signals by even smaller factors @cite . Therefore, a jammer that simultaneously blocks a small number of frequencies renders spread spectrum techniques useless in this case. As jamming strategies can come in many different flavors, detecting jamming activities by simple methods based on signal strength, carrier sensing, or packet delivery ratios has turned out to be quite difficult @cite .
- Recent work has also studied against jamming, including coding strategies @cite , channel surfing and spatial retreat @cite @cite , or mechanisms to hide messages from a jammer, evade its search, and reduce the impact of corrupted messages @cite . Unfortunately, these methods do not help against an adaptive jammer with full information about the history of the protocol, like the one considered in our work.
- In the multi-channel version of the problem introduced in the theory community by Dolev @cite and also studied in @cite @cite @cite @cite @cite @cite @cite , a node can only access one channel at a time, which results in protocols with a fairly large runtime (which can be exponential for deterministic protocols @cite @cite and at least quadratic in the number of jammed channels for randomized protocols @cite @cite if the adversary can jam almost all channels at a time). Recent work @cite also focuses on the wireless synchronization problem which requires devices to be activated at different times on a congested single-hop radio network to synchronize their round numbering while an adversary can disrupt a certain number of frequencies per round. @cite study robust information exchange in single-hop networks.
- Our work is motivated by the work in @cite and @cite . In @cite it is shown that an adaptive jammer can dramatically reduce the throughput of the standard MAC protocol used in IEEE 802.11 with only limited energy cost on the adversary side. @cite initiated the study of throughput-competitive MAC protocols under continuously running, adaptive jammers, but they only consider single-hop wireless networks. We go one step further by considering where different nodes can have different channel states at a time, e.g., a transmission may be received only by a fraction of the nodes. It turns out that while the MAC protocol of @cite can be adopted to the multi-hop setting with a small modification, the proof techniques cannot. We are not aware of any other theoretical work on MAC protocols for multi-hop networks with provable performance against adaptive jamming.
- Recently, researchers considered submodular functions on non-distributive lattices. It is known that a lattice is non-distributive if it contains as a sublattice either the pentagon @math or the diamond @math . Krokhin and Larose @cite proved tractability for the pentagon case, using nested applications of a submodular minimization algorithm. The case of the diamond was considered by Kuivinen @cite , who proved pseudo-polynomiality of the problem. The case of general non-distributive lattices is still open.
- Combining multimorphisms Finally, we mention that some constructions, namely Cartesian products and Malt'stev products , can be used for obtaining new tractable classes of binary multimoprhisms from existing ones @cite . Note, Krokhin and Larose @cite formulated these constructions only for lattice multimorphisms @math , but the proof in @cite actually applies to arbitrary binary multimorphisms @math .
- The Ricci flow was introduced by Hamilton @cite . There he effectively used it to solve the Poincar ' e conjecture for 3-manifolds with positive Ricci curvature. By following his approach, Perelman @cite @cite @cite finally solved the Poincar ' e conjecture (see also @cite @cite @cite ). There he used @math -functional as a crucial tool. At the same stage, he also studied the heat equation in @cite in relation with the geometry of Ricci flows. It suggests that analysing the heat equation is still an efficient way to investigate geometry of the underlying space even in the time-dependent metric case. This general principle has been confirmed in recent developments in this direction. In connection with the theory of optimal transportation, McCann and Topping @cite showed contraction in the @math -Wasserstein distance for the heat equation under backwards Ricci flow on a compact manifold. Topping's result @cite can be regarded as an extension of it to contraction in the normalized @math -transportation cost (see @cite also). By taking @math , he recovered the monotonicity of Perelman's @math -entropy, which is one of fundamental ingredients in Perelman's work.
- The problem of event segmentation has been studied in the past. See @cite for a review of recent techniques for the formation of event memories in robots. Ramoni proposed a method to cluster robot activities using Markov chain models @cite . @cite a batch maximum likelihood estimator is used to fit a sequence of time-indexed models to raw data. The incremental version of this algorithm is based on thresholding the likelihood of the current model along time. The spatio-temporal segmentation of video have been researched in @cite , applying motion model clustering, and in @cite using hierarchical clustering of the 3D space-time video stream. Gesture segmentation and recognition has been addressed in @cite employing hidden-Markov models (HMM).
- Improving the performance of stencil codes by temporal blocking is not a new idea, and many publications have studied different methods in depth @cite @cite @cite @cite @cite @. Conventional temporal blocking performs multiple updates on a small block of the computational domain before proceeding to the next block @cite @. This strategy has the important drawback that in-cache stencil updates are not naturally overlapped with loads and stores to main memory, leading to under-utilization of the memory interface. Cache-oblivious algorithms as proposed by @cite have the attractive property of being independent of cache sizes, but they come at the cost of irregular block access patterns, which cause many data TLB misses. This was shown for a 3D lattice Boltzmann (LB) application kernel in Ref. @cite @.
- However, the explicit use of shared caches provided by modern multicore CPUs has not yet been investigated to great detail. Ref. @cite describes a wavefront'' method similar to the one introduced here. However, that work was motivated mainly by the architectural peculiarities of multi-core CPUs, and does not elaborate on specific optimizations like avoiding boundary copies and optimizing thread synchronization. Our investigation is more general and explores a much larger parameter space. Finally there is, to our knowledge, as yet no systematic analysis of the prospects of temporal blocking on hybrid architectures, notably clusters of shared-memory compute nodes.
- A number of languages have been developed to describe software architectures, including @cite @cite @cite . Typical of these is Acme @cite , which is intended to fulfil three roles: to provide an architectural interchange format for design tools, to provide a foundation for the design of new tools and to support architectural modelling. The Acme language supports the description of components joined via connectors, providing a variety of communication styles. Components and connectors may be annotated with properties that specify various attributes. Acme also supports a logical formalism based on relations and constraints that permits computational or run-time behaviour to be associated with the description of architectures. Acme does not, however, support the deployment of systems from the architectural descriptions, nor does it express constraints on physical resources.
- The SmartFrog framework @cite is similar to this work in its motivation, to address the problems of describing, deploying and managing complex, distributed assemblies of software components. SmartFrog consists of a declarative language for describing component collections and component configuration parameters, and a runtime environment which activates and manages the components to deliver and maintain running systems. In SmartFrog each component transitions through life-cycle states in lock-step with all other components in the deployment. The SmartFrog life-cycle service model is similar to @cite which also utilizes constraint-based deployment. However, they advocate propagative deployment and maintain global constraints through the use of a consensus algorithm.
- Hein and Ritter @cite describe a model driven system for the evaluation of global constraints in a distributed system. In their work an application is developed as a collection of CORBA components. Their system introspects each of the components to discover their state and structure. This information is used to create a snapshot of the application which is checked for consistency against the model.
- The system described in this paper is most like @cite which is an environment for a adapting distributed applications to heterogeneous environments. The framework relies on declarative specifications (like Deladas), a runtime environment called and an AI planner (described below). The specifications contain descriptions of components that specify the required and implemented interfaces and conditions that must hold for component instances. The runtime provides three distinct pieces of functionality: a proxy mechanism, wrapper components on each host and cache coherence mechanisms. The proxy mechanism causes requests for services to be sent to the AI planner which decides on the appropriate selection and placement of components. The wrapper abstracts over node specifics allowing (Java) components to be loaded onto nodes in a generic manner. The cache coherency mechanisms provide directory level cache coherence amongst nodes sharing replicated data.
- Another paper by some of the same authors @cite focuses on the problem of initial placement, which they call the (CPP). The authors describe an AI planning algorithm called for solving the CPP which is defined to have five elements: the network topology, the component deployment behaviour, the application framework, the link crossing behaviour and the CPP goal. The network topology is described as a set of links between nodes; the application is defined as sets of interfaces and components. The link crossing behaviour is defined by functions which describe properties of inter-machine links. Finally, the CPP goal describes the desired state of the system with respect to placement. The algorithm utilises AI planning techniques to reduce the size of the search space in order to achieve scalability. is implemented as a pluggable component of the framework described above.
- @cite exploits a planner to generate workflows for Grid applications. The planner searches alternative deployment plans using heuristics to attempt to find high quality deployment solutions.
- Forward Secret Privacy. Forward Secret Privacy (PFS) @cite is the property ensuring that a session key derived from a pair of long-term public and private keys will not be compromised, if one of the long-term private keys is compromised in the future. An authenticated Diffie-Hellman (DH) key exchange protocol provides this property since the compromise of the long term key does not provide any information about the DH components that were used. However, a PFS system does not guarantee retroactive privacy. In fact, messages are not time-bounded, hence, the receiver or any party obtaining the key can always decrypt messages without any restriction of time.
- Vanish. Similar to Vanish @cite , EphPub aims at providing retroactive privacy, even if data storage is not trusted, compromised or stolen. However, as discussed in , the Vanish architecture is vulnerable to Sybil attacks @cite . Authors of Vanish have proposed countermeasures to this specific attack @cite . However, as discussed in @cite , this has only raised the bar for the attacker, due to the vulnerability of DHTs to Sybil attack, thus, leaving as open question whether or not DHTs are the best choice for key-share storage.
- Observe that the main flaw of Vanish design is that DHTs are assumed to be resistant to crawling. On the contrary, monitoring all DNS resolvers is realistically infeasible. Further, EphPub provides several improvements in terms of robustness and usability. First, it allows users to select the expiration time: by choosing domain names matching desired lifetime period, users assign their content a more precise expiration time (while in Vanish this is assigned by the specific DHT implementation and due to network churn, unless using a so-called refreshing proxy'' that re-pushes key shares). Also, as we show next, given its simple nature, EphPub does not require users to install any additional software (e.g., DHT client). Thus, it can be easily ported on any architecture and deployed even on mobile phones. Indeed, researchers are already advocating efficient solutions to guarantee retroactive privacy, for instance, in location sharing applications over smartphones @cite .
- Non-linear factor analysis has been studied for decades in the psychometrics literature Another instance of the whatever you do, somebody in psychometrics already did it long before'' law: http: www.stat.columbia.edu @math cook movabletype archives 2009 01 a .html . A review is provided by @cite . However, most of the classic work is based on simple parametric models. A modern approach based on Gaussian processes is the Gaussian process latent variable model of @cite . By construction, factor analysis cannot be used in applications where one is interested in learning functions relating latent variables, such as in causal inference. For embedding, factor analysis is easier to use and more robust to model misspecification than SEM analysis. Conversely, it does not benefit from well-specified structures and might be harder to interpret. @cite discusses the interplay between factor analysis and SEM. Practical non-linear structural equation models are discussed by @cite , but none of such approaches rely on nonparametric methods. Gaussian processes latent structures appear mostly in the context of dynamical systems (e.g., @cite ). However, the connection is typically among data points only, not among variables within a data point, where on-line filtering is the target application.
- Other Related Work: A combination of stochastic and online components appear in many different settings @cite @cite @cite @cite which are not immediately relevant to the call-out problem. We note that the bandwidth-like constraints (where the constraint is on a parameter different than the obtained value, as is the case for call-outs) has not studied in the bandit setting (see @cite @cite ) because the horizon is constrained. Finally, bidding and inventory optimization problems studied in the context of ad exchanges @cite @cite @cite , are not related to the call out optimization problems.
- @cite @cite propose the SpaseLoc heuristic. It is limited to @math and uses an solver for small localized subproblems. They then sew these subproblems together. So & Ye @cite show that the problem of solving a noiseless with a unique solution can be phrased as an and thus can be solved in polynomial time. They also give an efficient criterion for checking whether a given instance has a unique solution for @math .
- The problem with given embedding dimension @math is NP-hard embedding dimension, @math @cite @cite @cite . However, from our numerical tests it appears that random problems that have a unique solution can be solved very efficiently. This phenomenon fits into the results in @cite @cite .
- Several attribute grammar (AG) systems, namely JastAdd @cite , Silver @cite and LISA @cite , successfully use aspects to attach attribute evaluation productions to context-free grammar rules. AGs are a generic language for specifying computations on ASTs. They are well-suited for tasks such as specifying translators in general, which require a lot of expressive power. But the existence of more problem-oriented tools such as Pretzel @cite suggests that the generic formalism of AGs may not be the perfect tool for problems like generating pretty-printers. In fact, to specify a pretty-printer with AGs one has to produce a lot of boilerplate code for converting an AST into a string in concrete syntax. As we have shown in Example , facilitates creation of such problem-oriented tools providing the syntactical means (grammatical aspects) to avoid tangled grammars and unnecessary duplication.
- There is another approach to the problems we address: parser generators such as SableCC @cite and ANTLR @cite can work on annotation-free grammars and produce parsers that build ASTs automatically. In this way the problems induced by using annotations are avoided. The disadvantage of this approach is that the ASTs must be processed manually in a general-purpose programming language, which makes the development process less formal and thus more error-prone.
- , and its dual counterpart , have been previously studied in many geometric intersection graphs other than rectangle graphs. Gavril @cite gave a polynomial-time algorithm for both of these problem in chordal graphs, intersection graphs of subgraphs of a tree. Apostolico @cite gave a polynomial-time algorithm for these two problems in intersection graphs of chords on a circle, which were later improved by Cenek and Stuart @cite , while Golumbic and Hammer @cite gave a polynomial-time algorithm for intersection graphs of arcs on a circle which was later improved in @cite . A good survey of many generalizations of these results can be found in @cite @cite . Hochbaum and Maass and later Chleb ' k and Chleb ' i kov ' a considered intersection graphs of @math -dimensional boxes in @math @cite @cite , while Erlebach @cite considered intersection graphs of general fat objects in the plane. @cite @cite , approximation algorithms were suggested for and in the class of multiple-interval graphs.
- Our work falls under the general scenario where the source-destination communication is aided by a relay or a helper. Relevant results include the work of @cite , where multiple users communicate with a common receiver in the presence of an eavesdropper, and the transmit power allocation policy is determined that maximizes the secrecy sum-rate. In @cite , a source, destination, eavesdropper and relay model is considered, in which the relay transmits a noise signal in order to jam the eavesdropper. The rate-equivocation region is derived to show gains and applicable scenarios for cooperation, with the equivocation denoting the uncertainty of the eavesdropper about the source message.
- A generalization of @cite and @cite was proposed in @cite , in which the helper transmits signals from another source encoder. In @cite , inner and outer bounds on the rate-equivocation region were derived for the four-node model for both discrete memoryless and Gaussian channels. In @cite , the secrecy rate of orthogonal relay and eavesdropper channels was studied.
- DeLine proposed four desiderata that should be satisfied by spatial software navigation @cite . In the same work are presented, which satisfy the properties #1 and #4 ( continuous space and global overlays).
- Venolia and Cherubini ran a series of surveys and interviews on why and how developers use visual depictions of their code, @cite . They found that diagrams that document design decisions were often externalized in temporary drawings and then subsequently lost. Most of the diagrams had a transient nature because of the high cost of changing whiteboard sketches to electronic renderings.
- In the software visualization literature however, topic maps are rarely used. Except for the use of graph splatting in RE Toolkit by Telea al @cite , we are unaware of their prior application in software visualization.
- CodeCity is an explorative environment based on the city metaphor @cite . CodeCity employs the nesting level of packages for their city's elevation model, and uses a modified tree layout to position packages and classes. Order is based on size, so the layout is not stable over time. CodeCity is not integrated into an IDE, but built in top of the Moose reverse-engineering platform that offers post-mortem analysis of abstract models only.
- VERSO is an explorative environment that is also based on the city metaphor @cite , very similar to CodeCity. VERSO employs a treemap layout to position their elements, which provides a more stable layout. However, VERSO is also limited to post-mortem analysis of abstract models only.
- For @math the map @math was introduced by R. Gaio and D. A. Salamon in @cite . (More precisely, their definition relies on a choice of an @math -compatible almost complex structure @math on @math and a unitary trivialization of @math .)
- * Work by M. Entov and L. Polterovich and by V. L. Ginzburg Let now @math be a closed (spherically) monotone symplectic manifold and @math a torus acting on @math in a Hamiltonian way, with moment map @math . Then by Theorem 1.7 in the article @cite by M. Entov and L. Polterovich the pre-image @math of the special element of @math under @math is strongly (i.e. symplectically) non-displaceable.
- Assume that the action of @math on @math is free. Then by Lemma below @math is a closed, monotone regular coisotropic submanifold. Hence if @math is non-zero for some @math then it follows from Theorem that @math is not leafwise displaceable (and hence not displaceable). Thus in this case we obtain a stronger statement than in Theorem 1.7 in @cite , provided that also @math .
- In his recent paper @cite (Theorem 1.5) V. L. Ginzburg proved an upper bound on the minimal Maslov number of a closed, stable, displaceable coisotropic submanifold.
- McCann and Topping @cite (see Topping @cite and Lott @cite also) showed contraction in the Wasserstein metric for the heat equation under backwards Ricci flow on a compact manifold. More precisely, they showed that the following are equivalent: @math evolves under backwards super Ricci flow, i.e. @math .
- Whenever @math and @math are two non-negative unit-mass solutions of the heat equation [ u t = 1 2 g(t) u - ( 1 2 g t ) u ] (the term @math comes from the change in time of the volume element), the function @math is non-increasing. Here [ W_2(t, , ) := ( M M d_ g(t) (x,y)^2 (dx, dy) )^ 1 2 ] is the @math -Wasserstein distance of two probability measures @math and @math on @math . (The infimum is over all probability measures @math on @math whose marginals are @math and @math .) It means that backwards super Ricci flow is characterised by the contractivity property for solutions of the heat equation. Moreover, in recent work by Topping @cite and Lott @cite (see Brendle @cite also) the heat equation and the theory of optimal transport are efficiently used to derive several monotonicity results including a new proof for the monotonicity of Perelman's reduced volume. These facts indicate that it would be effective for deeper understanding of Ricci flow to study the heat equation in conjunction with backwards Ricci flow and the theory of optimal transport.
- 2. When there is no duplication of the amplitudes of solitons, the tropical analogue of the Jacobian @math obtained in @cite has an interpretation from the tropical geometry point of view @cite @cite . In this paper we are naturally led to a higher rank version of @math and the relevant tropical analogue of the Riemann theta function. We will call them tropical @math ' rather casually without identifying an underlying tropical geometric objects hoping not to cause a too much embarrassment.
- Prasad looked at how interrupt coalescence altered standard network tomography methods, and discussed some methods to detect the presence of coalescence in measured signals in @cite . Our work is different because we do not simply show the effect of the measurement system, but instead we use our knowledge of the measurement system and its inherent effect to design strategies for a specific networking problem, namely DOS attack detection. We chose DOS attack detection because recent work @cite @cite has shown malware and specifically DOS attacks can produce periodic traffic. Thus DOS attack detection becomes a problem of periodic signal detection, which is a common signal processing task. For comparison we use an attack detection method, called the periodic attack detector (PAD), proposed by He in @cite , which is was shown to produce good detection performance in @cite . The PAD is different from our method because it is not designed for a specific measurement system.
- The potential of uncertain data processing has achieved increasing interest in diverse application fields, e.g., sensor monitoring @cite , traffic analysis and location-based services @cite , etc.
- By now, uncertain data management has been established as an important branch of research within the database community, with increasing tendency. Existing approaches in this field of modelling of, managing of and query processing on uncertain data can be categorized into diverse directions, including probabilistic databases @cite @cite @cite @cite , indexing of uncertain data @cite @cite @cite and probabilistic query processing @cite @cite @cite @cite @cite @cite @cite .
- Probabilistic databases usually relate to probabilistic relational data, i.e. relations with uncertain tuples @cite , and use the possible worlds semantic @cite which is a probability distribution on all possible database instances; a database instance corresponds to a subset of uncertain tuples. In the general model, the possible worlds are constrained by rules that are defined on the tuples in order to incorporate object (tuple) correlations @cite . The ULDB model proposed in @cite and used in the @cite system supports uncertain tuples with alternative instances which are called x-tuples. Relations in ULDB are called x-relations containing a set of x-tuples. Each x-tuple corresponds to a set of tuple instances which are assumed to be mutually exclusive, i.e. no more than one instance of an x-tuple can appear in a possible world instance at the same time. This probabilistic data model closes the gap between two prevalent uncertainty models, the @cite and the @cite . An x-tuple is able to model an object with attribute value uncertainty; i.e., the instances of an x-tuple represent the probability value distribution of the corresponding uncertain attribute.
- In this paper, we adopt this concept to model uncertain vector objects. An uncertain vector object would correspond to an x-tuple of alternative uncertain instances of the object. Several approaches for indexing uncertain vector objects have been proposed @cite @cite @cite @cite . They mainly differ in the uncertainty model supported and in the type of supported similarity queries. In @cite , the Gauss-tree is introduced, which is an index for managing large amounts of uncertain objects with their uncertain attribute represented by a Gaussian distribution function. The proposed system aims at efficiently answering like Give me all persons in the database that could be shown on a given image with a probability of at least 10 which are based on a Bayesian setting. Later, in @cite an approach for incrementally retrieving the @math most likely uncertain objects that might be placed in a given query interval is proposed. Note that this definition is sematically different than the problem studied in this paper.
- In @cite , objects which have the highest probability of being located inside a given query range are reported. In contrast, the approaches for managing uncertain vector objects proposed in @cite @cite @cite support arbitrarily shaped probability distribution functions for uncertain object attributes. Similar to @cite , the approaches in @cite @cite focus on probability computations based on query predicates according to a given query range and, thus, are not applicable for our problem. Although @cite studies probabilistic ranking of objects according to their distance from a reference query point, the solutions are limited to existentially uncertain spatial data with a single alternative.
- We can categorize existing probabilistic querying approaches according to the uncertainty model they use. While probabilistic similarity queries over uncertain vector data are dedicated to the attribute value uncertainty model @cite @cite , probabilistic top- @math query approaches are usually associated with tuple uncertainty data in probabilistic databases @cite @cite @cite @cite . There exists a third probabilistic query category concerning spatially extended uncertain data as proposed in @cite . But there is only little work in this direction.
- There is a huge literature on abductive logic programming with and without constraints, see for example @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . The closest systems to CIFF are the @cite and @cite . The latter has also been developed as an extension of the IFF proof procedure to handle numerical constraints as in CLP, but with focus on the specification and verification of interactions in open agent societies. The main features of are the support of dynamical happening of events during computations, universally quantified variables in abducibles, the concept of fulfilment and violation of expectations, given a set of events, and integrity constraints of a specialised form which requires to include in their body at least one specific social construct (an event or an expectation). Instead, CIFF is intended as a general purpose abductive proof procedure, keeping the spirit of the original IFF proof procedure and conservatively adding numerical constraints.
- The , as remarked in @cite , is a combination of three existing abductive proof procedures, namely the IFF proof procedure @cite , the ACLP proof procedure @cite and, most importantly, the SLDNFA proof procedure @cite , of which the is a direct descendant. The is the state-of-the-art of abductive logic programming with constraints, borrowing the most interesting features from the above cited proof procedures. In Section we give a detailed comparison between CIFF and the .
- In Section , we present some experimental results on concrete examples and in comparison with the and the aforementioned answer set solvers. Note that @cite gives an extensive experimental comparison between Hyprolog, another relevant system for abductive logic programming, and CIFF, some ASP systems and the . Whereas CIFF is a meta-interpreter, Hyprolog avoids meta-interpretation by directly extending Prolog to incorporate abduction and constraint handling a la CHR @cite . However, Hyprolog has restrictions on the use of negation, as mentioned in @cite .
- - The framework requires that integrity constraints are in denial form. Logically, implicative integrity constraints can be written in denial form, since [ (B H) ((B H) ). ] However, the operational treatement of the two representations of integrity constraints is rather different in CIFF and in the . For example, given a CIFF integrity constraint [ a b ] (where @math and @math are abducibles) and an empty query, CIFF computes the empty set of abducibles, whereas, given the equivalent denial [ a b ] and the same query, the computes two alternative answers: the empty set of abducibles and @math . Indeed, assuming @math renders the original implication true. However, in some applications this treatment leads to unintuitive behaviours. For example, if @math is alarm and @math is evacuate , then, with the , evacuate is a possible answer independently of whether alarm has been observed or not. This and other examples are discussed in @cite .
- Comparison with Answer Set Programming Answer Set Programming (ASP) (see, e.g. @cite @cite @cite ) and Abductive Logic Programming with Constraints (ALPC) are strongly interconnected mechanisms for representing knowledge and reasoning. This interconnection arises at first glance, just noting that ASP is based on the Answer Set Semantics @cite , an evolution of the stable models semantics @cite (which in turn is used as the core semantics for many abductive proof procedures, e.g. @cite @cite @cite ) and that abduction can be modeled in ASP, as shown e.g. in @cite .
- As expected (and as shown in Section below), delegating the checks to a finite-domain constraint solver results in performances an order of magnitude faster than any answer set solver. Note that the ASP community is aware of this problem and recently some work has been initiated on integrating ASP with constraint solvers, in an effort to reduce the grounding size and speed computation (e.g., @cite @cite ), but for limited forms of constraints and restricted combinations of logic programs and constraints.
- In this performance comparison we restricted our attention to three systems: the @cite and two state-of-the-art answer set solvers, namely the DLV system @cite and SMODELS @cite .
- The most recent attempt to generalize the degree (along with betweenness and closeness) relied on mixing total weights (strength) with the cardinality (degree) @cite . The generalized degree based on this method is the @math -degree that we described earlier: @math -degree = @math . Although the generalization methodology were applied to the betweenness and closeness centrality measures, only the degree generalization was evaluated. More importantly, the generalization has a tuning parameter without clear guidelines regarding how to set it, unlike our methodology which is parameterless. Also the connection to the original unweighted measure does not depend on the weights, but on the tuning parameter (when @math equals 0). As a result, the @math -generalization is not really a generalization, but rather a new class of weighted measures.
- Bram Cohen, the protocol's creator, first described its main mechanisms and their design rationale @cite . Several measurement studies attempted to characterize the protocol's properties by examining real BitTorrent traffic. Izal @cite measured several peer characteristics derived from the tracker log of the Red Hat Linux 9 ISO image, including the proportion of seeds and leechers and the number and geographical spread of active peers. They observed that, while there is a correlation between upload and download rates, the majority of content is contributed by only a few leechers and the seeds.
- There have also been some simulation studies attempting to better understand BitTorrent's system properties. Felber @cite performed an initial investigation of the impact of different peer arrival rates, peer capacities, and peer and piece selection strategies. Bharambe @cite utilized a discrete event simulator to evaluate the impact of BitTorrent's core mechanisms and observed that rate-based tit-for-tat incentives cannot guarantee fairness. They also showed that the rarest-first algorithm outperforms alternative piece selection strategies. Lastly, Tian @cite studied peer performance toward the end of the download and proposed a new peer selection strategy that enables more peers to complete their download, even after the departure of all the seeds.
- The works most related to ours are Mahdian and Saberi @cite , Cole, Dobzinski, and Fleischer @cite and Lavi and Nisan @cite . Mahdian and Saberi @cite is the only other work that we are aware of to study mechanisms in which the supply is unknown and arrives online. They study the sale of multiple types of goods to bidders who desire only a single item, and wish to design mechanisms to maximize revenue. They consider only the adversarial supply setting, and allow extracting all payments when the entire supply has been exhausted . In this model, they give a truthful mechanism that is constant competitive with respect to the optimal auction that is restricted to selling all items at a single price, and show a lower bound of @math . Their mechanism is randomized, and is based on random-sampling techniques to achieve truthfulness.
- In a recent paper, Gerschenfeld and the first author @cite , considered the reconstruction problem for , which included the case of proper colorings of the vertices of a random graph. This amounts to understanding the correlation (as measured e.g. through mutual information) between the color of a vertex @math , and the colors of vertices at distance @math from @math . In particular, the problem is said to be unsolvable' if such a correlation decays to @math with @math . We refer to for a precise definition of the reconstruction problem. For a class of models, including the so-called Ising spin glass, the antiferromagnetic Potts model, and proper @math -colorings of a graph, @cite derived a general sufficient condition, under which reconstruction for (sparse) random graphs @math with @math edges is possible if and only if it is possible for a Galton-Watson tree with independent @math degrees for each vertex. Moreover, they also verified that the condition holds for the Ising spin glass and the antiferromagnetic Potts at non-zero temperature, leaving open the case of proper colorings of graphs.
- To achieve the maximimum throughput capacity of a network several optimization problems have to be solved. @cite , they investigate the problem of channel assignement in wireless network. They prove that this problem is NP-Complete and provide a polynomial-time approximation scheme (abbreviated PTAS) for this problem. The network capacity depends also on the number of time slots required to successfully schedule all links. @cite , authors investigate the scheduling problem with SINR constraints, based on the physical SINR models -also called PHY graph model- @cite @cite , and show it to be NP-Complete. In order to prove this result, they give a polynomial time reduction from the well-known subset sum problem. The approximation corresponding problem, called Approx-Subset-Sum is an FPTAS. @cite , authors also invesgate the throughput maximization problem under SINR constraints model and graph-model. They conjecture that the throughput maximization problem is NP-Complete according to the result given in @cite . In this article, we prove the validity of this conjecture and moreover, we provide an additional result that is the approximation throughput maximization problem is APX-Complete. More precisly, we focus on the Remaining Capacity problem ( ) as defined in @cite .
- In a wired network, finding an elementary path between two nodes minimizing over-loaded nodes can be solved by using the distributed and polynomial dijkstra algorithm @cite . In a wireless network, this problem is the Remaining Capacity problem ( ) defined in @cite . @cite @cite , authors show that finding the shortest path (repectively longest path) that avoid over-loaded nodes cannot be solved in a polynomial time. Centralized heuristics are proposed in @cite . Moreover, experimental studies aim at increasing the capacity of the network @cite (i.e. we call network capacity the cumulated data rate flows present in the network) by giving the best routing decisions (distributed routing protocols) that avoids using over-loaded nodes, see @cite @cite @cite @cite @cite @cite . @cite , authors try to decreased the maximum load with curve routing. This routing needs the knowledge of the geographical localization of nodes.
- There has been much work on managing probabilistic, uncertain, incomplete, and or fuzzy data in database systems (see, e.g., @cite @cite @cite @cite @cite @cite @cite ). The work in this area has spanned a range of issues from theoretical development of data models and data languages to practical implementation issues such as indexing techniques; several research efforts are underway to build systems to manage uncertain data (e.g., MYSTIQ @cite , Trio @cite , ORION @cite , MayBMS @cite , PrDB @cite ). The approaches can be differentiated based on whether they support tuple-level uncertainty where existence'' probabilities are attached to the tuples of the database, or attribute-level uncertainty where (possibly continuous) probability distributions are attached to the attributes, or both. The proposed approaches differ further based on whether they consider correlations or not. Most work in probabilistic databases has either assumed independence @cite @cite or has restricted the correlations that can be modeled @cite @cite @cite . More recently, several approaches have been presented that allow representation of arbitrary correlations and querying over correlated databases @cite @cite @cite .
- There has also been work on top-k query processing in probabilistic databases where the ranking is by the result tuple probabilities (i.e., probability and score are identical) @cite . The main challenge in that work is efficient computation of the probabilities, whereas we assume that the probability and score are either given or can be computed easily.
- The aforementioned work has focused mainly on tuple uncertainty and discrete attribute uncertainty. Soliman and Ilyas @cite were the first to consider the problem of handling continuous distributions. Recently, in a followup work @cite , we extended the algorithm for to arbitrary continuous distributions. We were able to obtain exact polynomial time algorithms for some continuous probability distribution classes, and efficient approximation schemes with provable guarantees for arbitrary probability distributions. One important ingredient of those algorithms is an extension of the generating function used in this article.
- Recently, there has also been much work on nearest neighbor-style queries over uncertain datasets @cite @cite @cite @cite . In fact, a nearest neighbor query (or a @math -nearest neighbor query) can be seen as a ranking query where the score of a point is the distance of that point to the given query point. Thus, our new ranking semantics and algorithms can be directly used for nearest neighbor queries over uncertain points with discrete probability distributions.
- There is a tremendous body of work on ranking documents in information retrieval, and learning how to rank documents given user preferences (see Liu @cite for a comprehensive survey). That work has considered aspects such as different ranking models, loss functions, different scoring techniques etc. The techniques developed there tend to be specific to document retrieval (focusing on keywords, terms, and relevance), and usually do not deal with existence uncertainty (although they often do model document relevance as a random variable). Furthermore, our work here primarily focuses on highly efficient algorithms for ranking using a spectrum of different ranking functions. Exploring and understanding the connections between the two research areas is a fruitful direction for further research.
- @cite extend the notion of SFE to NP hard problems for which efficient algorithms must output an approximation to the optimum, unless P=NP. They defined as functional privacy the constraint that two inputs with the same output value (e.g. the size of an optimal vertex cover) must produce the same value under the approximation algorithm. Under this constraint, @cite show that approximating the value of vertex cover to within @math is as hard as computing the value itself, for any constant @math . These hardness results were extended to search problems by @cite , where the constraint is relaxed to only equate those inputs whose sets of optimal solutions are identical. These results were extended and strengthened by @cite @cite .
- Nonetheless, @cite and others show a number of positive approximation results under versions of the functional privacy model. @cite provide positive results in the function privacy setting when the algorithm is permitted to leak few bits (each equivalence class of input need not produce identical output, but must be one of at most @math possible outcomes). Indyk and Woodruff also give some positive results for the approximation of @math distance and a nearest neighbor problem @cite . However, as functional privacy extends SFE, it does not protect sensitive data that can be inferred from the output.
- The confidence and support measures are widely used in discovering approximate functional dependencies @cite @cite and evaluating @cite @cite @cite . The confidence can be interpreted as an estimate of the probability that a randomly drawn pair of tuples agreeing on @math also agree on @math @cite @cite . Scheffer @cite study the trade off between support and confidence for finding association rules @cite , by computing a expected prediction accuracy. In addition, Chiang and Miller @cite also study some other measures such as conviction and @math -test for evaluating dependency rules. When a candidate @math is suggested together with minimum support and confidence, @cite study the discovery of optimal with the minimum pattern tableau size. A concise set of patterns are naturally desirable which may have lower cost during the applications such as violation detection by . On the other hand, Chiang and Miller @cite explore by considering all the possible dependency candidates when @math is not specified. @cite , also study the case when the embedded are not given, and propose three algorithms for different scenarios.
- The concept of matching dependencies ( ) is first proposed in @cite for specifying matching rules for the object identification (see @cite for a survey). The can be regarded as a generalization of , which are based on identical values having matching similarity equal to @math exactly. Thus, can be represented by the syntax of as well. For any two tuples, if their @math values are identical (with similarity threshold @math ), then a @math requires that their @math values are identical too, i.e., a @math . @cite also study the dependencies with matching similarities on attributes @math when given the matched values on @math , which can be treated as a special case of . The reasoning mechanism for deducing from a set of given is studied in @cite . The and their reason techniques can improve both the quality and efficiency of various record matching methods.
- One approach to learning to play games is to generalize reinforcement learning algorithms such as Q-learning @cite . One nice feature of this approach is that it can handle games with state, which is important in distributed systems. In Q-learning, an agent associates a value with each state-action pair. When he chooses action @math in state @math , he updates the value @math based on the reward he received and the best value he can achieve in the resulting state @math ( @math ). When generalizing to multiple agents, @math and @math become vectors of the state and action of every agent and the @math is replaced by a prediction of the behavior of other agents. Different algorithms use different predictions; for example, Nash-Q uses a Nash equilibrium calculation @cite . See @cite for a survey.
- Another approach is , where agents choose a strategy for each round that guarantees that the regret of their choices will be low. Hart and Mas-Colell @cite present such a learning procedure that converges to a @cite given knowledge of what the payoffs of every action would have been in each round. They also provide a variant of their algorithm that requires only information about the agent's actual payoffs @cite . However, to guarantee convergence to within @math of a correlated equilibrium requires @math , still too slow for large systems. Furthermore, the convergence guarantee is that the distribution of play converges to equilibrium; the strategies of individual learners will not converge. Better results can be achieved in restricted settings. For example, @cite showed that in routing games a continuum of no-regret learners will approximate Nash equilibrium in a finite amount of time.
- Foster and Young @cite use a stage-learning procedure that converges to Nash equilibrium for two-player games. Germano and Lugosi @cite showed that it converges for generic @math -player games (games where best replies are unique). Young @cite uses a similar algorithm without explicit stages that also converges for generic @math -player games. Rather than selecting best replies, in these algorithms agents choose new actions randomly when not in equilibrium. Unfortunately, these algorithms involve searching the whole strategy space, so their convergence time is exponential. Another algorithm that uses stages to provide a stable learning environment is the ESRL algorithm for coordinated exploration @cite .
- The past ten years have seen an ever increasing number of research that focuses on the application of game theory and pricing in analyzing network service providers, initiated by the work of Kelly @cite . The majority of research in the wireless setting focuses on resource allocation within the same service provider (see for example @cite , @cite , @cite ) and on the interaction between the users of one provider ( @cite , @cite , @cite ). Perhaps surprisingly, only a few works focus on the competition between providers in a wireless setting. We can distinguish two types of competitions between wireless service providers: competing on behalf of the users (such as @cite and @cite ) and price competition to attract users (such as @cite , @cite and @cite Another related work is @cite which considers two cellular providers competing for users by changing the strength of their pilot signals. ). The latter is the subject of our work.
- Until recently, the heterogeneity of the users was largely ignored. The first work that explicitly takes into account the channel differences for different users is @cite . To our knowledge, our work is the first one to consider the pricing competition of providers for users who are heterogenous in both willingness to pay and the channel quality for arbitrary channel coefficients. Also, most previous work considered nonatomic users, i.e., each user's influence on the network is small and negligible @cite . This assumption may not be realistic in practice, which motivates us to study the atomic user case that involves resource splitting among networks (see Section for details).
- The constraint of quantization of messages, or of states, has been considered in several recent papers @cite @cite @cite @cite @cite @cite @cite @cite . However, existing algorithms in the literature are not able to achieve average consensus with arbitrary precision, with the notable exception of @cite , which uses logarithmic quantizers, together with a coding scheme similar to the one in the present paper. The zooming-in zooming-out algorithm and the main convergence result of this paper appeared, in slightly different version, in @cite and in the thesis @cite . Note that recently @cite a modification of the present algorithm has been proved to converge under a different condition.
- Implicit labeling schemes were first introduced in @cite , where an elegant adjacency labeling schemes of size @math is established on @math -node trees. That paper also notices a relation between adjacency labeling schemes and (see also @cite @cite @cite ). Precisely, it is shown that there exists an adjacency labeling scheme with label size @math for a graph family @math if and only if there exists a universal graph for @math with @math nodes.
- Adjacency labeling schemes on trees were further investigated in an attempt to reduce the constant factor in the label size. In @cite an adjacency labeling scheme using label size of @math is presented; and in @cite the label size was further reduced to @math . This current state of the art bound implies the existence of a universal graph for the family of @math -node trees with @math nodes.
- The problem of community finding in large complex networks has attracted considerable research interest for some time now. Its origins can be traced back to the first studies of the hyperlink structure of the web, e.g. to the observation of @cite that communities emerge spontaneously around authoritative web pages which are identified by means of hub pages. Then, the works by @cite and @cite formally defined and systematically tackled the problem of community detection. In the following, we provide a list of existing methods for community detection classified according to the approach they adopt. A more detailed discussion of existing community detection methods is contained in the survey by @cite .
- @cite consider communities as dense bipartite subgraphs of the web (seen as a directed graph). A natural way to identify dense subgraph structures is by means of graph partition enumeration. In order to drastically reduce the vast number of subgraphs that are possible by complete enumeration, the authors employ a series of heuristic pruning techniques. An extension of this definition led to the notion of @math -dense communities @cite , which can be efficiently discovered based on more sophisticated subgraph enumeration and pruning criteria.
- @cite define communities as subsets of vertices that have more links (undirected) to each other than to the rest of the network nodes. To detect such communities on the web, they integrate a maximum flow strategy with an iterative crawling process. A stricter community definition was considered by @cite and a technique was devised to detect them that was based on both the maximum flow algorithm and an iterative graph partitioning and contraction process.
- According to Girvan and Newman @cite , the community structure of a large network should be revealed by progressively removing edges with high edge betweenness, i.e. by following a divisive approach. Following the same approach but with the use of different measures, namely the edge and the , @cite and @cite , respectively, could uncover the underlying community structure of complex networks. Later, the measure of was defined by Newman and Girvan, as a means to quantify the quality of a network partition into communities @cite . More specifically, modularity reflects the extent to which a given network partition is characterized by higher intra-community density in comparison to the one that would be observed in a random partition of the same network. Building upon this measure, the methods by Newman @cite and @cite describe efficient implementations of community detection by means of agglomerative strategies.
- A combined strategy for community detection is provided by @cite . The authors consider a three-step community detection process: (a) detection of maximal cliques (subgraph enumeration), (b) initial network partition by progressive expansion of the maximal cliques (flooding) and (c) adjustment of the original partition in order to maximize modularity.
- Most of the methods presented above are global, meaning that they need to process the whole network in order to output the identified community structure. Even though some of these methods achieve low complexity (linear to the size of the network), their use is still prohibitive, when there is need for extremely responsive community detection, e.g. in interactive exploration of large networks, which can be only feasible by means of local processing of the network. We could only find two local methods @cite @cite that are suitable for identifying communities within such applications. However, we consider the first of those @cite as unsuitable for graphs of scale-free nature (since the @math -shell would contain the whole graph after just few expansion steps), and the second @cite as not achieving maximum efficiency, since it is not local by design (i.e. there are redundant computation steps when applying the method locally). We consider that our proposed methodology addresses the community detection problem from a local perspective in a more intuitive and efficient way.
- Morse @cite gives a combinatorial definition of the affine stable Grothendieck polynomials @math in terms of affine set-valued tableaux and also proves the Pieri rule for @math . The original @math -Schur functions @math @cite @cite , which arose in the study of Macdonald polynomials, involve a parameter @math . It appears that a @math -analogue @math of @math exists , defined in a similar manner to [Conjecture 9.11] LLMS . The connection of @math and Macdonald theory is explored in @cite .
- As proven in @cite , the NG algorithm obeys a stochastic gradient descent on a function based on the average of the geometric quantization error. As known, this is not true of a SOM @cite , whose learning dynamics does not minimize an objective function of any sort. This property of NG relies on how the units in the network are adapted: at each input, the units in the network are first sorted according to their distance and then adapted by an amount that depends on the ranking.
- In the Grow-When-Required (GWR) algorithm @cite , which is a development of GNG, the error variable associated to each GNG unit is replaced by a firing counter @math that decreases exponentially each time the unit is winner, i.e. closest to the input signal. When @math gets below a certain threshold @math , the unit is deemed and its behavior changes. The adaptation of unit being closest to the input takes place only if the distance is below a certain threshold @math , otherwise a new unit is created. This means that the network continues to grow until the input data sample is completely included in the union of balls of radius @math centered in each unit.
- The complexity class TFNP was introduced by Megiddo and Papadimitriou @cite . The class PLS (for polynomial local search) was introduced by Johnson, Papadimitriou and Yannakakis @cite . In @cite Papadimitriou defined the complexity classes PPA (polynomial parity argument), PPAD (polynomial parity argument in directed graphs), PPP (polynomial pigeon-hole principle) and PPM (polynomial probabilistic method). He proved that computational versions of Brouwer's fixed point theorem and Sperner's lemma are PPAD-complete.
- Daskalakis, Goldberg and Papadimitriou @cite proved that 3-Dimensional Brouwer is PPAD-complete. They reduced 3-dimensional Brouwer to 3-Graphical Nash to prove that 3-Graphical Nash is PPAD-complete. These results together with the reductions of Goldberg and Papadimitriou @cite imply that computing Nash equilibrium in games with 4 players ( 4-Nash ) is PPAD-complete. Chen and Deng @cite , Daskalakis and Papadimitriou @cite independently proved that 3-Nash is PPAD-complete. Chen and Deng @cite proved that 2-Nash is also PPAD-complete. Chen, Deng and Teng @cite showed that approximating Nash equilibrium is also hard. For a list results on the complexity of finding equilibria we refer the reader to a recent book on algorithmic game theory @cite .
- We consider a smoothed learning'' model inspired by Smoothed Analysis, which Spielman and Teng introduced to explain why the simplex method for linear programming (LP) usually runs in polynomial time @cite . Roughly speaking, they show that if each parameter of an LP is perturbed by a small amount, then the simplex method will run in polynomial time with high probability (in fact, the expected run-time will be polynomial). For LP's arising from nature or business (as opposed to reduction from another computational problem), the parameters are measurements or estimates that have some inherent inaccuracy or uncertainty. Hence, the model is reasonable for a large class of interesting LP's.
- A popular approach of tackling the issues related to transient failures of network elements is that of using proactive recovery schemes . These schemes typically work by precomputing alternate paths at the network setup time for the failure scenarios, and then using these alternate paths to re-route the traffic when the failure actually occurs. Also, the information of the failure is suppressed in the hope that the failure is transient and the failed element will recover shortly. The local rerouting based solutions proposed in @cite @cite @cite @cite @cite fall into this category.
- Zhang, et. al. @cite present protocols based on local re-routing for dealing with transient single node failures. They demonstrate via simulations that the recovery paths computed by their algorithm are usually within 15 the theoretically optimal alternate paths.
- Wang and Gao's Backup Route Aware Protocol (BRAP) @cite also uses some precomputed backup routes in order to handle transient single link failures. One problem central to their solution asks for the availability of reverse paths at each node. However, they do not discuss the computation of these reverse paths. As we discuss later, the alternate paths that our algorithm computes qualify as the reverse paths required by the BRAP protocol of @cite .
- Slosiar and Latin @cite studied the single link failure recovery problem and presented an @math time for computing the link-avoiding alternate paths. A faster algorithm, with a running time of @math for this problem was presented in @cite . The local-rerouting based fast recovery protocol of @cite can use these paths to recover from single link failures as well. Both these algorithms, @cite @cite , are centralized algorithms that work using the information of the entire communication graph.
- Hotho @cite adapted PageRank to work on a tripartite graph of users, tags and resources corresponding to a folksonomy. They also developed a form of topic-biasing on the modified PageRank, but the generation of a faceted ranking implies a new computation of the adapted PageRank algorithm on the network for each new facet.
- There has also been some work done on faceted ranking of web pages. For example, the approach of DeLong, Mane and Srivastava @cite involves the construction of a larger multigraph using the hyperlink graph with each vertex corresponding to a pair webpage-concept and each edge to a hyperlink associated with a concept. Subgraph ideas are suggested by them: It might be faster to simply run PageRank on sub-graphs pertaining to each individual concept (assuming there are a small number of concepts).'' Although DeLong @cite obtain good ranking results for single-keyword facets, they do not support multi-keyword queries.
- Query-dependent PageRank calculation was introduced in Richarson and Domingos @cite to extract a weighted probability per keyword for each webpage. These probabilities are summed up to generate a query-dependent result. They also show that this faceted ranking has, for thousands of keywords, computation and storage requirements that are only approximately @math - @math times greater than that of a single query-independent PageRank. As we show in , our facet-dependent ranking algorithms have similar time complexity.
- Scalability issues were also tackled by Jeh and Widom @cite criticizing offline computation of multiple PageRank vectors for each possible query and preferring another more efficient dynamic programming algorithm for online calculation of the faceted rankings based on offline computation of basis vectors. They found that their algorithm scales well with the size of set @math , the biasing page set, and they criticize previous ideas in @cite : [Richarson and Domingos] suggested that importance scores be precomputed offline for every possible text query, but the enormous number of possibilities makes this approach difficult to scale."
- @cite , the author introduced a 1+N protection model in optical mesh networks using network coding over p-cycles. The author suggested a model for protecting @math connections from a set of sources to a set of receivers in a network with @math connections, where one connection might fail. The suggested model can protect against a single link failure in any arbitrary path connecting a source and destination.
- @cite , the author extended the protection model in @cite and provided a GMPLS-based implementation of a link protection strategy that is a hybrid of 1+N and 1:N. It is claimed that the hybrid 1+N link protection provides protection at higher layers and with a speed that is comparable to the speed achieved by the physical layer implementations. In addition, it has less cost and much flexibility.
- Monitoring network information flow using network coding was introduced in @cite @cite . @cite , it was shown how to use network coding techniques to improve network monitoring in overlay networks. Practical aspects of network coding has been shown in @cite .
- Abstraction Schemes: Abstractions have been extensively studied in the context of probabilistic systems. General issues in defining good abstractions as well as specific proposals for families of abstract models are presented in @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . Recently, theorem-prover based algorithms for constructing abstractions of probabilistic systems based on predicates have been presented @cite . Another notion that has been recently proposed is the notion of a magnifying-lens abstraction'' @cite , which can be used to assist in the model checking process, by approximating the measure of the satisfaction of path formulas for sets of concrete states; the method is not an abstraction in the traditional sense in that neither is an abstract model explicitly constructed, nor is the model used for reasoning, one that simulates the concrete model.
- In early work, it is said that the convolution of exponential RVs of different means can be approximated by a Gamma distribution. This needs to be verified! @cite determines the interference in Poisson random networks for @math . Since their interference is averaged over many network realizations, it turns out to be Gaussian. For a given network and a certain MAC scheme, the distribution obtained from a temporal trace will most likely look different (see @cite @cite ), since there will be a few dominant interferers, in particular for high @math . Hence, such Gaussian models have to be used with care for analysis and protocol design. We will characterize the distribution of the cumulated interference for single network realizations and different MAC schemes.
- The study of outage and throughput performance is related to the problem of interference characterization. Important results on the interference in large wireless systems have been derived by @cite @cite @cite @cite @cite . In @cite , outage probabilities for cellular networks are calculated for channels with Rayleigh fading and shadowing while @cite determines outage probabilities to determine the optimum transmission range in a Poisson network. @cite combined the two approaches to determine the optimum transmission range under Rayleigh fading and shadowing. @cite provides a detailed analysis on outage probabilities and routing progress in Poisson networks with ALOHA.
- For our study of @math , @math , and @math networks, we will draw on results from @cite @cite @cite @cite , as discussed in the rest of this section.
- This case is studied in @cite . The characteristic function of the interference is determined to be Note that their notation is adapted to ours. Also, a small mistake in [Eqn. (18)] net:Sousa90 is corrected here.
- @math : Regular fading networks with @math and slotted ALOHA In @cite , the authors derive the distribution of the interference power for one- and two-dimensional Rayleigh fading networks with slotted ALOHA and @math . Closed-form expressions are derived for infinite regular line networks with @math , @math . The Laplace transform of the interference [Eqn. (5)] net:Mathar95wn , can, for @math , be simplified using Euler's product formula for @math to is [Eqn. (8)] net:Mathar95wn
- The Laplace transforms of the interference are particularly convenient for the determination of outage probabilities in Rayleigh fading. As was noted in @cite @cite @cite , the success probability @math can be expressed as the product of the Laplace transforms of the interference and noise:
- In @cite @cite , was calculated for a two-dimensional random network with Rayleigh fading and ALOHA. Ignoring the noise, they obtained (see [Eqn. (3.4)] net:Baccelli06 , [(Eqn. (A.11)] net:Zorzi95 ) with The subscript 2 in @math indicates that this is a constant for the two-dimensional case. Useful values include @math and @math . @math , so @math as @math for any @math . The spatial contention is @math .
- In graph drawing the so-called (2SCM) is an important NP-hard problem that occurs when computing layered graph layouts. Such layouts have been introduced by @cite and are widely used for drawing hierarchical graphs. In 2SCM, vertices of a bipartite graph are to be placed on two parallel lines (called ) such that for each vertex on one line all its adjacent vertices lie on the other line. As in TL the objective is to minimize the number of edge crossings provided that edges are drawn as straight-line segments. In one-sided crossing minimization (1SCM) the order of the vertices on one of the layers is fixed. Even 1SCM is NP-hard @cite . J "unger and Mutzel @cite performed an experimental comparison of exact and heuristic algorithms for both 1SCM and 2SCM. The main findings were that for 1SCM the exact solution can be computed quickly for up to 60 vertices in the free layer, and for 2SCM an iterated barycenter heuristic is the method of choice for instances with more than 15 vertices in each layer.
- The @math model was introduced by H "aggstr "om and Meester @cite . They showed that there was a finite critical value, @math for all @math such that an infinite cluster exists in this model. They proved that the infinite cluster was unique and that there was a value @math such that @math for all @math . Teng and Yao gave an upper bound of 213 for @math @cite .
- : The seminal work @cite provides both upper-bounds and constructive communication schemes to show that the point-to-point capacity of a dense wireless network is @math . In @cite , @cite , @cite , and @cite , the authors derive information theoretic upper-bounds on the network capacity. These works study the capacity scaling of extended networks and obtain upper-bounds on the capacity by using cut-set bounds. In particular, Xie prove that for environments with path-loss exponent @math and constant per node power, the expected transport capacity grows at most linearly in the number of nodes, so that for an extended network with uniformly distributed source-destination pairs, the network capacity is @math @cite .
- While in simple multi-hopping, increasing the transmission power does not improve the capacity scaling, Jovicic in @cite show that the upper-bound on the network transport capacity scales linearly with node power. Similar results are shown in @cite . Their results motivates using the extra degree of freedom provided by the choice of transmission power. We exploit this in the proposed collaborative scheme to increase the network capacity.
- Toumpis in @cite have proposed a gridding of a unit area network and the use of 9-TDMA scheduling to derive the same lower-bound on capacity as @cite in a more straightforward manner. They further extend their model to consider the effect of node mobility and fading on the capacity. Franceschetti in @cite have closed the previous gap between the capacity in and networks. They use percolation theory to show that the sum capacity of their proposed communication scheme scales as @math . None of the above utilizes cooperative transmission.
- There has been much research into node cooperation in the context of single-source, single-destination, and @math relays. For example, Gastpar in @cite show an achievable rate scaling of @math . However, Ozgur in @cite are the first to exploit the linear-scaling result of MIMO communication in a multiple-source network using a distributed MIMO paradigm. For a network, they propose a hierarchical cooperative communication scheme with single-hop distributed MIMO transmission to achieve a network throughput scaling of @math , @math , and single-hop distributed MIMO transmission. They further show that when the same scheme is applied to the more realistic extended network model, it results in a throughput @math , implying that for @math , the scheme outperforms simple multi-hopping.
- Our work is motivated by @cite . However, we propose a multi-hop distributed MIMO scheme specifically designed for extended networks. Furthermore, we show that with a small increase in the transmission power, our scheme performs uniformly better than simple multi-hopping for all values of @math . As far as we are aware, there is no existing work on analyzing the achievable rate of combining cooperative communication and multi-hop forwarding.
- Finally, Aeron and Saligrama in @cite have studied the effect of node cooperation on the capacity of wireless networks with a fixed receiver SNR at all nodes and obtained @math network throughput, through spatially separated MIMO relays which collaborate in their transmissions. However, the assumption of constant receiver SNR at all nodes implies that the transmission power needs to scale as @math . In contrast, in this work we show that with much lower power scaling, the same communication rate can be achieved by using multi-hop distributed MIMO transmissions.
- Another stance at working around the undecidability of dense-time MTL builds upon the fact that the same logic is decidable over discrete time. Hence, a few approaches introduce some notion of discretization, that is partial reduction of the verification problem from dense to discrete time. The present paper goes in this direction by extending previous work on MTL @cite to the case of TA. A different discretization technique, based on the notion of robust satisfiability of MTL specifications, has been introduced in @cite . Other work also deals with notions of robustness in order to guarantee that dense-time TA are implementable with non-ideal architectures @cite . Another well-known notion of discretization is the one based on the concept of @cite ; several authors have applied this quite general notion to the practical verification of descriptive @cite @cite @cite @cite or operational @cite @cite @cite @cite @cite @cite @cite @cite @cite formalisms. See also the related work section of @cite for more references about discretization techniques.
- The research of the single-minded multi-unit combinatorial auction problem, which is closely related to the unsplittable flow problem, yielded similar results. @cite showed that approximating the problem to within a factor of @math is @math -hard, where @math is the minimum multiplicity of an item. Yet, when @math , the integrality gap of the corresponding integer linear program becomes @math . Accordingly, @cite , and Lavi and Swamy @cite devised truthful @math -approximation mechanisms. However, these mechanisms are truthful only in a probabilistic sense and hence, the best known deterministic truthful result for the @math -bounded multi-unit combinatorial auction problem is by @cite , which attains @math -approximation.
- A situation similar in spirit to ours has been examined in the seminal article @cite by Greven and den Hollander, motivated from the point of view of population dynamics. The model treated there is a discrete-time branching model in random environment with drift and corresponds to the case of a bounded i.i.d. potential. In particular, their motivation stems from the discrete-time analogue described at the end of section . The results are formulated by the use of nontrivial variational formulae. While the authors concentrate on the explicit dependence of the results on the drift parameter @math an advantage of our approach is that we may compute the @math -th annealed Lyapunov exponents for all @math and characterise them in a simpler way.
- Scheduling on unrelated machines is one of the most fundamental scheduling Problems. For this NP-hard optimization problem, there is a polynomial time algorithm with approximation ratio of @math @cite . Especially if the number of machines is bounded by some constant, Angel, Bampis and Kononov gave an FPTAS @cite . However there is no corresponding payment strategy to make either of the above allocation algorithms truthful.
- Lavi and Swarmy considered a restricted variant, where each task @math only has two values of running time , and gave a @math -approximation randomized truthful mechanism @cite . They first use the cycle monotonicity in designing mechanisms.
- In @cite , Christodoulou, Koutsoupias and Kov 'a cs considered the fractional version of this problem, in which each task can be split among the machines. For this version, they gave a lower bound of @math and an upper bound of @math . We remark that these two bounds are closed for the case of two machines as in the integral deterministic version. So to explore the exact bound for the randomized version seems very interesting and desirable. We believe that our work in this paper is an important step toward this objective.
- The setting where the nodes are given only their own labels, but not the labels of their neighboring nodes has also been studied quite extensively. Research in this problem has led to the introduction of very interesting combinatorial concepts like selective families . The use of selective families in the design of deterministic protocols for unknown networks was introduced by Chlebus et. al. in @cite . Several recent works exploit this combinatorial tool, specifically the use of probabilistic method, for obtaining good lower and upper bounds for the broadcasting problem @cite , @cite , @cite .
- The problem of broadcasting on directed radio networks has received much attention. The protocol given by @cite requires @math rounds for completion. This upper bound was reduced to @math rounds by a breakthrough result of @cite . In a further breakthrough, @cite , the authors deployed the probabilistic method to bring the upper bound from @math to within logarithmic factors of the best known lower bound @math . In @cite , a further improvement has been made for small diameter networks. They establish an upper bound of @math . In @cite , the authors define and deploy new combinatorial structures to reduce this gap between lower and upper bounds to @math factor i.e., give a broadcasting protocol running in @math rounds.
- Randomized protocols for the broadcasting problem have been studied in @cite , @cite , @cite . In these protocols, the nodes do not know the labels of their neighboring nodes and may in fact have non-unique labels. In @cite , a broadcasting protocol running in @math rounds is given. A lower bound of @math rounds was proved for this problem in @cite . The lower bound of @math rounds for broadcasting protocols also holds for this problem. A broadcasting protocol running in @math , matching the lower bound, was proposed in @cite .
- In the wake up problem, each node in the network either wakes-up spontaneously or is activated by receiving a wake-up signal from another node. Each active node transmits the wake-up signal according to a given protocol. The running time of a wake-up protocol is the number of steps counted from the first spontaneous wake-up, until all nodes become activated. This problem has been studied under different assumptions in @cite , @cite , @cite , @cite . Asynchronous radio broadcasting has been studied in @cite .
- There are some papers on sponsored search that analyze the generalized second-price (GSP) auction, which is the auction currently in use at Google and Yahoo. The equilibria of this auction are characterized and compared with VCG @cite @cite @cite @cite . Here the utility function is the profit-maximizing utility where each bidder attempts to maximize her clicks @math profit per click, and budget constraints are generally not treated.
- @cite consider the problem of budget-constrained bidders for multiple items of a single type, with a utility function that is profit-maximizing, modulo being under the budget (being over the budget gives an unbounded negative utility). They give a truthful mechanism allocating some portion of the items that is revenue-optimal, and prove that in their model, under reasonable assumptions, truthful mechanisms that allocate all the units are impossible. Our work is different both because of the different utility function and the generalization to multiple slots with a scheduling constraint. Using related methods, @cite consider an online setting where an unknown number of copies of an item arrive online, and give a truthful mechanism with a constant competitive ratio guarantee.
- There is some work on algorithms for allocating bidders with budgets to keywords that arrive online, where the bidders place (possibly different) bids on particular keywords @cite @cite . The application of this work is similar to ours, but their concern is purely online optimization; they do not consider the game-theoretic aspects of the allocation. @cite derive a linear program for the offline optimization problem of allocating bidders to queries, and handle multiple positions by using variables for slates'' of bidders. Their LP is related to ours, but again they do not consider game-theoretic aspects of their proposed allocations.
- In our setting one is tempted to apply a Fisher Market model: here @math divisible goods are available to @math buyers with money @math , and @math denotes @math 's utility of receiving @math amount of good @math . It is known @cite @cite @cite that under certain conditions a vector of prices for goods exists such that the market clears , in that there is no surplus of goods, and all the money is spent. Furthermore, this price vector can be found efficiently @cite . The natural way to apply a Fisher model to a slot auction is to regard the slots as commodities and have the utilities be in proportion to the number of clicks. However this becomes problematic because there does not seem to be a way to encode the scheduling constraints in the Fisher model; this constraint could make an apparently market-clearing'' equilibrium infeasible, and indeed plays a central role in our investigations.
- Signals with FRI were initially introduced by Vetterli @cite . The reconstruction schemes hinged on identifying algebraically-independent parameters of the signals, e.g. the weights @math and time locations @math . In the seminal paper on FRI, the sampling kernel for finite signals was chosen to be either the sinc or the Gaussian. An annihilating filter approach led to an elegant algebraic solution via polynomial root finding and least squares. The authors alluded to the noisy case and suggested the use of the Singular Value Decomposition (SVD) for dealing with noisy samples. We will show that, in fact, this method is ill-conditioned because root-finding is itself not at all robust to noise. Thus it is not amenable to practical implementations, for instance on an ADC @.
- Subsequently, Dragotti @cite examined acquisition of the same signals with an eye toward implementability of the sampling kernel. Instead of using the sinc and Gaussian kernels (which do not have compact support), the authors limited the choice of kernels to functions satisfying the Strang--Fix conditions @cite ( e.g. splines and scaling functions), exponential splines @cite and functions with rational Fourier transforms. They combined the moment-sampling and annihilating filter approaches to solve for the parameters. In our work, however, we will continue to use the Gaussian as our sampling kernel. We believe that, even though the Gaussian has infinite support, it can be well approximated by its truncated version. Hence, we can still draw insights from the analysis of using Gaussian filters and the subsequent reconstruction of the signal from its noisy samples @math . More importantly, unlike with previous approaches, the sampling kernel plays no fundamental role in the reconstruction algorithm. We use the Gaussian kernel because of its prominence in earlier work and the intuitiveness of its information spreading properties.
- Maravic and Vetterli @cite and Ridolfi @cite proposed and solved a related problem. Instead of modeling the noise at the , they considered the scenario where @math , the signal in question, is corrupted by additive white noise @math . Clearly, @math does not belong to the class of signals with FRI @. However, in @cite , novel algebraic subspace-based approaches solve the sampling problem in the Laplace domain and these methods achieve some form of optimality. @cite , various algorithms including subspace-based approaches @cite ( ESPRIT and MUSIC ) as well as multidimensional search methods were used and comparisons were made. The authors concluded that, in the noisy signal case, the parameters can be recovered at a rate below that prescribed by the Shannon-Nyquist Theorem but at a factor above the critical rate.
- Chow and Liu @cite considered the problem of estimating Markov random fields whose underlying graphs are trees, and provided an efficient (polynomial-time) algorithm based on the fact that in the tree case maximum-likelihood estimation amounts to the computation of a maximum-weight spanning tree with edge weights equal to pairwise empirical mutual information. Unfortunately, their approach does not generalize to the estimation of Markov random fields whose graphs have cycles. Much work in mathematical biology is devoted to reconstructing tree Markov fields when there are hidden models. For trees, given data that is generated from the model, the tree can be reconstructed efficiently from samples at a subset of the nodes given mild non-degeneracy conditions. See @cite @cite @cite for some of the most recent and tightest results in this setup.
- The most closely related works are @cite and @cite . These can be compared in terms of sampling complexity, running time as well as the generality of the models to which they apply. These are summarized in the Table below. The first line refers to the type of models that the method cover: Does the model allow clique interactions of just edge interactions? The next two lines refer to requirements on the strength of interactions: are they not required to be too weak are only edges with strong interactions returned? are they not required to be too strong? The next line refers to the hardness of verifying if a given model satisfies the conditions of the algorithm (where X denoted that the verification is exponential in the size of the model). The following line refers to the following question: is there a guarantee that the generating model is returned with high probability. The final two lines refers to computational and sampling complexity where @math denotes constants that depend on @math .
- Abbeel, @cite considered the problem of reconstructing graphical models based on factor graphs, and proposed a polynomial time and sample complexity algorithm. However, the goal of their algorithm was not to reconstruct the true structure, but rather to produce a model whose distribution is close in Kullback-Leibler divergence to the true distribution. In applications it is often of interest to reconstruct the true structure which give some insights into the underlying structure of the inferred model.
- Note furthermore that two networks that differ only in the neighborhood of one node will have @math KL distance. Therefore, even in cases where it is promised that the KL distance between the generating distribution and any other distribution defined by another graph is as large as possible, the lower bounds on the KL distance is @math . Plugging this into the bounds in @cite yields a polynomial sampling complexity in the size of the network in order to find the generating network compared to our logarithmic sampling complexity. For other work based on minimizing the KL divergence see the references in @cite .
- The same problem as in the present work (but restricted to the Ising model) was studied by Wainwright, @cite , where an algorithm based on @math -regularization was introduced. The algorithm presented is efficient also for dense graphs with running time @math but is applicable only in very restricted settings. The work only applies to the Ising model and more importantly only models with edge interactions (no larger cliques are allowed). The most important restrictions are the two conditions in the paper (A1 and A2). Condition A1 requires (among other things) that the covariates [spins] do not become overly dependent''. Verifying when the conditions holds seems hard. However, it is easy to see that this condition fails for standard models such as the Ising model on the lattice or on random @math -regular graphs when the model is at low temperatures, i.e. for @math in the case of the two dimensional Ising model and @math for random @math -regular graphs.
- Subsequent to our work being posted on the Arxiv, Santhanam and Wainwright @cite again considered essentially the problem for the Ising model, producing nearly matching lower and upper bounds on the asymptotic sampling complexity. Again their conditions do not apply to the low temperature regime. Another key difference from our work is that they restrict attention to the Ising model, i.e. Markov random fields with pairwise potentials and where each variable takes two values. Our results are not limited to pairwise interactions and apply to the more general setting of MRFs with potentials on larger cliques.
- The problem of determining the scaling of @math was first analyzed by Gupta and Kumar in @cite . They show that, under random placement of nodes in the region, certain models of communication motivated by current technology, and random source-destination pairing, the maximum achievable per-node rate @math can scale at most as @math . Moreover, it was shown that multi-hop communication can achieve essentially the same order of scaling.
- Since @cite , the problem has received a considerable amount of attention. One stream of work @cite @cite @cite @cite @cite @cite @cite has progressively broadened the conditions on the channel model and the communication model, under which multi-hop communication is order optimal. Specifically, with a power loss of @math for signals sent over distance @math , it has been established that under signal attenuation @math and random node placement, the best achievable per-node rate @math for random source-destination pairing scales essentially like @math and that this scaling is achievable with multi-hop communication.
- Another stream of work @cite @cite @cite @cite @cite has proposed progressively refined multi-user cooperative schemes, which have been shown to significantly out-perform multi-hop communication in certain environments. In an exciting recent work, "O zg "u @cite have shown that with nodes placed uniformly at random, and with signal attenuation @math , a cooperative communication scheme can perform significantly better than multi-hop communication. More precisely, they show that for @math , the best achievable per-node rate for random source-destination pairing scales as @math and cooperative communication achieves a per-node rate of @math (here, @math is an arbitrary but fixed constant). That is, cooperative communication is essentially order optimal in the attenuation regime @math .
- The most common scenario studied in this context is a multi-cast one in which the source aims at communicating the same information to a set of receivers (distinct nodes in the same network.) The fundamental theorem of network coding states that this is possible using network coding (i.e., processing at the nodes) if the values of the min-cuts from the source to any of the receivers is at least @math @cite . Moreover, linear network coding suffices @cite . This means that processing at the nodes can be limited to forwarding packets which are linear (over @math ) combinations of incoming packets. Finally, it is not necessary to choose the local encoding functions at the nodes carefully. Random linear combinations are sufficient with probability close to one, provided the cardinality of the field is large enough @cite @cite . For a general introduction into network coding we refer the reader to @cite @cite @cite .
- Upon transmission of @math , a corrupted'' version @math of the codeword is received. Without loss of generality, we assume that @math is brought back into normal form @math by Gaussian elimination. In principle it might be that the received matrix cannot be brought in this form because its first @math columns have rank smaller than @math . However, within the probabilistic model which we discuss in the following, the rank deficiency is small with high probability and can be eliminated by a small perturbation. Following K "o tter and Kschischang @cite , we model the net effect of the transmission- and the processing- noise'' as a low-rank perturbation of @math . More precisely, we assume that where @math is an @math matrix over @math of @math , @math . We call @math the weight of the error, and @math the normalized weight.
- Define the distance of two codewords @math and @math as @math and the minimum distance @math of the code @math as the minimum of the distances between all distinct pairs of codewords. The normalized minimum distance is @math . It is shown in @cite that @math is a true distance metric; in particular it fulfills the triangle inequality. Therefore, given a code @math of minimum distance @math a simple bounded distance decoder can correct all errors of weight @math or less. A bounded distance decoder is an algorithm that, given a received word @math , decodes @math to the unique word within distance @math if such a word exists and declares an error otherwise. Bounded distance decoders are popular since a suitable algebraic structure on the code often ensures that bounded distance decoding can be accomplished with low complexity.
- It was already shown that clustering improves the performance of some EDAs for certain classes of problems. For globally multimodal problems, for instance, clustering has revealed a promising approach @cite . Such problems present several global optima or, in other words, several optimal solutions with the same fitness. Those problems may be very tricky for an evolutionary algorithm to solve, since slow convergence to one of the optima (genetic drift) often occurs. The retarded convergence is explained by the combination of solutions coming from different regions of the search space, which often results in poor solutions. The application of clustering in EDAs when solving globally multimodal problems can be described as the separation of the population in subpopulations (one for each cluster), and the subsequent learning of different probabilistic models for each subpopulation. Breeding generally involves no combination among different subpopulations. Clustering improves the performance of the evolutionary algorithm by such an interbreeding avoidance. However, this approach leads to a smaller exploration of the space, since combination from distant regions could be favorable.
- A similar approach to clustering is the parallelization of EDAs by adopting multiple subpopulations and migration. In @cite , a simple recombination operator called PV-wise uniform crossover is proposed, which is similar to GA uniform crossover. After two parent PVs were selected for combination a new temporary PV is built by randomly selecting, for each gene, from which of the two parents should the binomial proportion be taken from. A new individual is sampled from this PV.
- Another perspective for the application of clustering in EDAs can be found in @cite . The unsupervised estimation of Bayesian network algorithm (UEBNA) uses a model-based approach for clustering, based on the unsupervised learning of Bayesian networks at each generation. An unobserved variable @math is included in the model, which represents the unknown cluster label. The Bayesian network represents a joint probabilistic model for the best individuals, considering a factorization which includes all genes and the cluster random variable @math .
- @PARASPLIT Let @math denote the minimum energy cost of a successful transmission from @math to @math . A topology control algorithm that minimizes energy consumption will remove an edge @math if and only if there exists a path between @math and @math through an intermediate set of nodes @math such that @math . Accomplishing this requires a significant exchange of information between nodes and in such a case, the topology control algorithm is indistinguishable from a routing algorithm. As a result, a number of distributed topology control algorithms have been proposed where nodes rely on lesser exchange of information between neighbors @cite . In this subsection, we focus on the subset of these protocols that can be adapted to work without the exchange of any location-based information between neighboring nodes.
- The Directed Relative Neighborhood Graph (DRNG) protocol @cite removes an edge @math if and only if there exists a path @math such that @math . In the Directed Local Spanning Subgraph (DLSS) protocol @cite , each node creates a local spanning tree from the subgraph induced by itself and its neighbors. A node @math retains the edge @math in the topology-controlled graph if and only if the edge @math exists in the local spanning tree generated at node @math . As proved in @cite , any edge that is removed by DRNG is also removed by DLSS and therefore, DLSS achieves a sparser graph than DRNG. The fewer edges in DLSS often leads to a lower energy consumption, but it can sometimes lead to longer paths and therefore higher energy consumption than DRNG. A generalization of the DRNG protocol is the XTC protocol which was conceived independently and uses the notion of link quality' in making its topology control decision instead of energy costs @cite . When link quality between two nodes is measured by the energy cost of a transmission between them, the XTC protocol reduces to DRNG.
- Among other attempts to accommodate the irregularities of a real wireless environment in topology control are some that allow for uncertainties in whether or not a nearby node is reachable if the distance to that node is above a certain threshold @cite @cite . However, these algorithms assume that each node can know the distances to other nearby nodes, something that cannot be relied upon in a real environments with multipath effects. Some other works have also considered realistic wireless models but they have only presented centralized algorithms @cite .
- We let @math be the largest value of @math such that random @math -SAT instances admit with high probability a solution. It is known @cite that @math . A sharp conjecture on the value of @math has been put forward in @cite on the basis of statistical physics calculations, implying @math , @math , @math for (respectively) @math and @math for large @math @cite .
- Statistical mechanics methods allowed to derive a very precise picture of the solution set @cite @cite @cite . This inspired a new message passing algorithm dubbed @cite . In conjunction with decimation, this algorithm allowed to solve random instances of unprecedentedly large sizes, in difficult regimes of @math and @math .
- In Ref. @cite a simple message passing algorithm, warning propagation (see below), was analyzed for a modified ( planted') ensemble of random formulae. The algorithm was proved to converge and find solutions for large enough density @math (see also @cite @cite ). Both the ensemble and the algorithm are quite different from the ones treated in this paper.
- Further, the definition and analysis of a Maxwell decoder' in @cite @cite , is closely related to the approach in this paper. Let us recall that the Maxwell decoder was a (mostly conceptual) algorithm for implementing maximum likelihood decoding of LDPC codes over the erasure channel. The treatment in @cite @cite applies almost verbatim to a simple constraint satisfaction problem known as XORSAT. The generalization in the present paper is analogous to the one from the erasure to a general binary memoryless symmetric channel.
- Finally, let us mention that BP decimation can be an interesting option in engineering applications, as demonstrated empirically in the case of lossy source coding @cite @cite .
- Previous attempts at studying this problem for sampling uniform colorings yielded weaker results. @cite it is shown that Gibbs sampling rapidly mixes on @math if @math where @math and that a variant of the algorithm rapidly mixes if @math . Indeed the main open problem of @cite is to determine if one can take @math to be a function of @math only.
- Comparing the results presented here to @cite we observe first that there is one sense in which the current results are weaker. @cite the tree access @math can be of order @math while for the results presented here @math has to be of order @math . The results of @cite crucially use the fact that the Ising model is attractive (this is a monotonicity property) and that it is a two spin system which allows using the Weitz tree'' @cite .
- We note that for all @math and all @math the mixing time of Gibbs sampling on @math is with high probability at least @math , see @cite @cite for details. It is an important challenge to find the critical @math for rapid mixing. In particular, the question is if the threshold can be formulated in terms of the coloring model on a branching process tree with @math degree distribution. One would expect rapid mixing for in the uniqueness phase'', but perhaps even beyond it, see @cite @cite @cite .
- Every partial cube may be embedded in a distance-preserving way into an integer lattice @math of some dimension @math . One such labeling simply uses each bit of a bitvector labeling as a coordinate in @math ; however, some graphs may be embeddable into integer lattices of much lower dimension than their isometric dimension. For instance, a path graph can be embedded into @math , and given one-dimensional coordinates that accurately describe the graph distances, despite having an isometric dimension of @math . The of a partial cube is the minimum number @math for which the graph admits a distance-preserving embedding into @math . The lattice dimension, and an embedding of that dimension, may be found in polynomial time using an algorithm based on graph matching @cite , but this algorithm depends on having as input a bitvector labeling and is slower than the algorithm we describe here, so it does not form the basis of an efficient partial cube recognition algorithm.
- The use of Evolutionary Algorithms as weak learners with -in a standard Bagging or Boosting approach has also been investigated. Boosting approaches for GP have been applied for instance to classification @cite or symbolic regression @cite : each run delivers a GP tree minimizing the weighted sum of the training errors, and the weights were computed as in standard Boosting @cite . While such ensembles of GP trees result, as expected, in a much lower variance of the performance, they do not fully exploit the population-based nature of GP, as independent runs are launched to learn successive classifiers.
- Liu @cite proposed a tight coupling between Evolutionary Algorithms and Ensemble Learning. They constructed an ensemble of Neural Networks, using a modified back-propagation algorithm to enforce the diversity of the networks; specifically, the back-propagation aims at both minimizing the training error and maximizing the negative correlation of the current network with respect to the current population. Further, the fitness associated to each network is the sum of the weights of all examples it correctly classifies, where the weight of each example is inversely proportional to the number of classifiers that correctly classify this example. While this approach nicely suggests that ensemble learning is a Multiple Objective Optimization (MOO) problem (minimize the error rate and maximize the diversity), it classically handles the MOO problem as a fixed weighted sum of the objectives.
- The MOO perspective was further investigated by Chandra and Yao in the DIVACE system, a highly sophisticated system for the multi-level evolution of ensemble of classifiers @cite @cite . In @cite , the top-level evolution simultaneously minimizes the error rate (accuracy) and maximizes the negative correlation (diversity). In @cite , the negative correlation-inspired criterion is replaced by a ; the difference concerns the misclassification of examples that are correctly classified by other classifiers. Finally, the ensemble is constructed either by keeping all classifiers in the final population, or by clustering the final population (after their phenotypic distance) and selecting a classifier in each cluster.
- While the MOO perspective nicely captures the interplay of the accuracy and diversity goals within Ensemble Learning, the selection of the classifiers in the genetic pool as done in @cite @cite does not fully exploit the possibilities of evolutionary optimization, in two respects. On the one hand, it only considers the final population that usually involves up to a few hundred classifiers, while learning ensembles commonly involve some thousand classifiers. On the other hand, clustering-based selection proceeds on the basis of the phenotypic distance between classifiers, considering again that all examples are equally important, while the higher stress put on harder examples is considered the source of the better Boosting efficiency @cite .
- Previously, the problem of statistical estimation from compressed data was considered by Zhang and Berger @cite , Ahlswede and Burnashev @cite and Han and Amari @cite from the viewpoint of multiterminal information theory. In these papers, the underlying family of distributions of @math is parametric, i.e., of the form @math , where @math is a subset of @math for some finite @math , and one wishes to estimate the true" parameter @math . The i.i.d. observations @math are drawn from @math , and the input part @math is communicated to the statistician at some rate @math , while the output part @math is communicated at some rate @math . The present work generalizes to the nonparametric setting the case considered by Ahlswede and Burnashev @cite , namely when @math . To the best of the author's knowledge, this paper is the first to consider the problem of nonparametric learning from compressed observations with side information.
- Communications in wireless networks has been progressing from MH to that using cooperative strategies. More research is being directed toward designing codes that are based on information theoretic cooperative coding strategies to harvest the gain in transmission rates predicted by information theory. Examples of codes based on cooperative coding strategies include DF-based Turbo codes @cite @cite and LDPC codes @cite @cite @cite @cite for the single relay channel. It has been mentioned that some of these codes can be extended to the multiple relay channel @cite @cite @cite @cite .
- In the past, link optimization (i.e., maximizing the transmission rate between node pairs) and route optimization were done separately. Routing was optimized after the links between the nodes had been established. Algorithms such as Bellman-Ford [Section 24.1] cormen01 @cite and Dijkstra's algorithm @cite that assign costs to all links were used to find a route with the lowest cost from source to the destination. These ways of separating routing and coding are not optimal for MH or DF as the rates of the links change depending on which route is chosen. Realizing the inter-dependency between links and routes, it has been suggested that links and routes be jointly optimized @cite @cite @cite @cite . This gives rise to cross-layering @cite in the OSI model. However, in these joint routing and coding work, data transmission from the source to destination is still based on MH. Routing algorithms that are optimized for MH might not be suitable for DF.
- In Extremely Opportunistic Routing (ExOR) @cite , a node broadcasts its data to a set of potential relays. Nodes in this set transmit acknowledgments and then selected nodes forward the data. Though ExOR does not have predefined routes, MH is used on the route taken by a packet.
- We focus on the multiple relay channel @cite @cite @cite @cite , which is a single-source single-destination network, as a first step towards understanding general multiple-source multiple-destination networks. We study DF because it is one of the more implementable'' information theoretic coding strategies @cite @cite @cite @cite @cite @cite .
- The first work on human identification dates back to @cite . Since then a lot of other schemes have been proposed in literature @cite , @cite , @cite , @cite , @cite , @cite . Some of them were broken in @cite , @cite . While most of them involve some numerical calculations like @cite and the HB protocol @cite , they can be implemented using some graphical interface employing pictures as memory aids. We can categorized the human identification protocols into two broad categories: Protocols built to be secure against general eavesdropping adversaries and protocols secure against only guessing adversaries'' i.e. Adversaries who do not see the user's input and hence try to guess the secret or impersonate the user without any apriori knowledge. Protocols mentioned so far fall in the first category. They have a drawback, however, that they involve extra computation from the user. As an example, in the HB protocol @cite , the user is required to compute bit-wise binary multiplication for some number of bits in every iteration. This may not seem much but to obtain a higher level of security, the number of computations increase significantly.
- Concerning development of P2P distributed computing systems, there are some frameworks such as: DREAM @cite , which focuses in distributed processing of EAs and uses the P2P network DRM. G2DGA @cite , equivalent to the previous. It centers on distributed genetic algorithms processing by the use of the network G2P2P. JADE (Java Agent Development Framework, available from http: jade.cselt.it ), a P2P system which includes agents as software components.
- The mentioned DRM is an implementation of the newscast protocol @cite . This protocol has served as a guide for the proposed communication mechanism within this work. Newscast is an epidemic approach where every node shares local information with its neighbourhood by selecting a node from it with uniform probability each certain time (refresh rate). Our communication model is inspired by such a protocol. However, our model considers a dynamic refresh rate which depends on the QoS parameters: latency and bandwidth.
- Existing decidability results for MTL involve placing restrictions on the semantics or the syntax of the logic to circumvent the problem of punctuality. Alur and Henzinger @cite showed that the satisfiability and model-checking problems for MTL relative to a discrete-time semantics are EXPSPACE-complete. Alur, Feder, and Henzinger @cite @cite introduced as a fragment of MTL in which the temporal operators may only be constrained by intervals. They showed that the satisfiability and model-checking problems for MITL relative to a dense-time semantics are also EXPSPACE-complete. Wilke @cite considered MTL over a dense-time semantics with , i.e., the semantics is parameterised by a bound @math on the number of events per unit time interval. He showed that the satisfiability problem is decidable in this semantics and that MTL with existential quantification over propositions is equally expressive as Alur-Dill timed automata.
- A class of timed alternating tree automata has been defined by Dickh " o fer and Wilke @cite in the context of model checking a real-time version of Computation Tree Logic, called TCTL @. The language-emptiness problem for these automata is undecidable in general. However, TCTL model checking reduces to a special case of language emptiness, which is shown in @cite to be decidable using Alur and Dill's clock regions construction. In contrast, bounded-dimension clock regions do not suffice in the present paper: we combine clock regions with the notion of well-quasi-orders.
- Another closely related paper is that of Abdulla and Jonsson @cite on networks of one-clock timed processes. This has a similar flavour to the work presented here in that it uses abstractions based on clock regions and also Higman's Lemma. The problems they study are however very different from the ones considered in this paper.
- All the decidability results presented in this paper concern timed alternating automata over finite timed words, including the results that are ostensibly about infinite timed words. In particular, our model-checking procedure for the safety fragment of MTL over infinite timed words depends on the fact that any infinite timed word violating a safety property has a finite , that is, a finite prefix none of whose extensions satisfies the property. Since writing the extended abstract of this paper @cite , we have obtained some positive and negative decidability results about the language emptiness problem for timed alternating automata over infinite words. We discuss these results in the conclusion, .
- The vast majority of research on the decreasing enrollment throughout computer science education repeatedly focuses on two potential "solution" areas: recruitment and retention, which involve getting more students to join and keeping those who have joined @cite @cite @cite . Recruitment entails attracting K-12 students to computer science and often involves outreach efforts which attempt to make computer science look "cool," exciting, useful, and rewarding. In addition, when recruiting underrepresented students in particular, researchers often recommend opening up admissions criteria without lowering standards, welcoming reentry students, and providing opportunities to bridge educational gaps that students might have between their previous education and the entry-level courses at the university @cite @cite .
- As it could be expected, these notions are closely related to quandle extensions and cohomology, which have both been intensively studied in recent years. The subject of rack cohomology originated in the work of R. ,Fenn, C. ,Rourke, and B. ,Sanderson @cite , who constructed a classifying topological space @math for every rack @math . The corresponding quandle (co)homology theory was taken up by J.S. ,Carter and his collaborators, in order to construct knot invariants (see for example @cite @cite @cite ). Quandle coverings were introduced and applied to knot quandles in @cite . They have also appeared in the context of non-abelian extensions, explored by N. ,Andruskiewitsch and M. ,Gra na @cite , where a corresponding non-abelian cohomology theory was proposed. This generalized cohomology, in turn, has been taken up and applied to knot invariants in @cite .
- We can choose randomly, without replacement, @math samples in a sequence of unknown length using a single pass through the data by . Reservoir sampling @cite @cite @cite was introduced by Knuth @cite . All reservoir sampling algorithms begin by appending the first @math samples to an array. In their linear time ( @math ) form, reservoir sampling algorithms sequentially visit every symbol choosing it as a possible sample with probability @math where @math is the number of symbols read so far. The chosen sample is simply appended at the end of the array while an existing sample is flagged as having been removed. The array has an average size of @math samples at the end of the run. In their sublinear form ( @math expected time), the algorithms skip a random number of data points each time. While these algorithms use a single pass, they assume that the number of required samples @math is known a priori, but this is difficult without any knowledge of the data distribution.
- Using suffix arrays @cite @cite and the length of the maximal common prefix between successive prefixes, Nagao and Mori @cite proposed a fast algorithm to compute @math -gram statistics exactly. However, it cannot be considered an online algorithm even if we compute the suffix array in one pass: after constructing the suffix array, one must go through all suffixes at least once more. Their implementation was later improved by Kit and Wilks @cite . Unlike suffix trees @cite , uncompressed suffix arrays do not require several times the storage of the original document and their performance does not depend on the size of the alphabet. Suffix arrays can be constructed in @math time using @math working space @cite . Querying a suffix array for a given @math -gram takes @math time.
- A more sophisticated view-size estimation algorithm used in the context of data warehousing and OLAP @cite @cite is logarithmic probabilistic counting @cite . This approach requires a single pass and almost no memory, but it assumes independent hashing for which no algorithm using limited storage is known @cite . Practical results are sometimes disappointing @cite , possibly because many random hash values need to be computed for each data point. Other variants of this approach include linear probabilistic counting @cite @cite and loglog counting @cite .
- View-size estimation through sampling has been made adaptive by @cite : their strategy is to first attempt to determine whether the distribution is skewed and then use an appropriate statistical estimator. We can also count the marginal frequencies of each attribute value (or symbol in an @math -gram setting) and use them to give estimates as well as (exact) lower and upper bound on the view size @cite . Other researchers make particular assumptions on the distribution of relations @cite @cite @cite @cite .
- related Intrusion detection systems (IDSs) have been deployed as a second defense line behind the intrusion prevention techniques and many research papers have been written on this topics. The conceptual building blocks of our method is the article of Valdes and Skinner in @cite .
- With the deployment of web services, the designers of IDSs recognize more and more the importance of a specific service knowledge such as implementation vulnerabilities and protocol weaknesses. A service specific anomaly detector which checks DNS and HTTP traffic is proposed in @cite . A multi-model approach to the detection of web based attacks has been recently published in @cite .
- Time-indexed reservation is needed when considering advance reservation of bandwidth @cite , which allows requesting bandwidth before actual transfer is ready to happen. For example, a scheduled tele-conference may reserve bandwidth for a specified future time interval. @cite shows that advance reservation causes bandwidth fragmentation in time axis, which may significantly reduce accept probability of requests arriving later. To address the problem, they propose the concept of , which defines advance reservation request with flexible start time and rate.
- There is also a large literature of online job scheduling with deadline, for example, @cite , @cite , @cite . A job monopolizes processor for the time it's being scheduled, which maps exactly to packet level scheduling, while in flow level, we must consider multiple flows share bandwidth concurrently, as represented by @math .
- The first formalisms designed for reasoning about cryptographic protocols are belief logics such as BAN logic @cite , used by the Convince tool @cite with the HOL theorem prover @cite , and its generalizations (GNY @cite , AT @cite , and SVO logic @cite which the C3PO tool @cite employs with the Isabelle theorem prover @cite ). Belief logics are difficult to use since the logical form of a protocol does not correspond to the protocol itself in an obvious way. Almost indistinguishable formulations of the same problem lead to different results. It is also hard to know if a formulation is over constrained or if any important assumptions are missing. BAN logic and its derivatives cannot deal with security flaws resulting from interleaving of protocol steps @cite and cannot express any properties of protocols other than authentication @cite . To overcome these limitations, the knowledge flow formalism has, like other approaches @cite @cite @cite @cite @cite , a concrete operational model of protocol execution. Our model also includes a description of how the honest participants in the protocol behave and a description of how an adversary can interfere with the execution of the protocol.
- Specialized model checkers such as Casper @cite , Mur @math @cite , Brutus @cite , TAPS @cite , and ProVerif @cite have been successfully used to analyze security protocols. Like knowledge flow analysis in Alloy, these tools are based on state space exploration which leads to an exponential complexity. Athena @cite is based on a modification of the strand space model @cite . Even though it reduces the state space explosion problem, it remains exponential. Multiset rewriting @cite in combination with tree automata is used in Timbuk @cite . The relation between multiset rewriting and strand spaces is analyzed in @cite . The relation between multiset rewriting and process algebras @cite @cite is analyzed in @cite .
- Proof building tools such as NRL, based on Prolog @cite , have also been helpful for analyzing security protocols. However, they are not fully automatic and often require extensive user intervention. Model checkers lead to completely automated tools which generate counterexamples if a protocol is flawed. For theorem-proving-based approaches, counterexamples are hard to produce.
- Our formalism derives its simplicity from being just sufficiently expressive to enable modelling of practical cryptographic protocols. In particular, existentials @cite cannot be encoded as knowledge flows; existentials are implicitly modeled in Oscar's initial knowledge. As mentioned in Section ), NP-hardness proofs which use (existential) Horn clause reduction @cite or SAT3 reduction @cite are not applicable to Knowledge Flow Analysis.
- In this section, we relate our definition of -answer sets to several formulations of aggregates proposed in the literature. We begin with a comparison of our unfolding approach with the two most recently proposed semantics for LP with aggregates, i.e., the @cite @cite @cite , the @cite @cite , and the semantics for abstract constraint atoms @cite . We then relate our work to earlier proposals, such as perfect models of aggregate-stratified programs (e.g., @cite ), the fixpoint answer set semantics of aggregate-monotonic programs @cite , and the semantics of programs with weight constraints @cite . Finally, we briefly discuss the relation of -answer sets to other proposals.
- This lemma allows us to prove the following relationship between @math and @math . Proof. The result is a trivial consequence of the fact that @math and @math has the same set of partial stable models as @math @cite . @math
- In this subsection, we investigate the relationship between -answer sets and the notion of answer set defined by in @cite . The notion of answer set proposed in @cite is based on a new notion of reduct, defined as follows. Given a program @math and a set of atoms @math , the reduct of P with respect to S , denoted by @math , is obtained by removing from @math those rules whose body is not satisfied by @math . In other words, [ ^S P = r r ground(P), S body(r) . ] The novelty of this reduct is that it remove aggregate atoms and negation-as-failure literals satisfied by @math .
- The above example shows that our characterization of programs with aggregates differs from the proposal in @cite . Apart from the lack of support for aggregates in the heads of rules, the semantics of @cite might accept answer sets that are not -answer sets. Observe that the two semantical characterizations coincide for large classes of programs (e.g., for programs that have only monotone aggregates).
- A very general semantic characterization of programs with aggregates has been proposed by Marek and Truszczy ' n ski in @cite . The framework offers a model where general aggregates can be employed both in the body and in the head of rules. The authors introduce the notion of abstract constraint atom, @math , where @math is a set of atoms (the domain of the aggregate) and @math is a subset of @math (the solutions of the aggregate). For an abstract constraint atom @math , we will denote @math with @math and @math with @math . In @cite , the focus is only on , i.e., constraints @math , where if @math then all supersets of @math are also in @math .
- [ @math ''] Let us assume @math is a stable model of @math according to @cite . A result in @cite shows that @math where [ ] We will show that @math is a answer set of @math by proving that @math where @math and @math the aggregate-free head reduct of @math with respect to @math (Definition ).
- The proposal of Ferraris @cite applies a novel notion of reduct and answer sets, developed for propositional theories, to the case of aggregates containing arbitrary formulae. The intuition behind the notion of satisfaction of an aggregate relies on translating aggregates to propositional formulae that guarantee that all cases where the aggregate is false are ruled out. In particular, for an aggregate of the form @math , where @math are propositional formulae, @math and @math are real numbers, @math is a function from multisets of real numbers to @math , and @math is a relational operator (e.g., @math , @math ), the transformation leads to the propositional formula: [ ( ( i I ) ( i 1, ,k I ) ) ] The results in @cite show that the new notion of reduct, along with this translation for aggregates, applied to the class of logic programs with aggregates of @cite , captures exactly the class of FLP-answer sets.
- Let us consider the weight constraints employed by Smodels and let us describe a translation method to convert them into our language with aggregates. We will focus on weight constraint that are used in the body of rules (see Sect. for aggregates in the heads of rules). For simplicity, we will also focus on weight constraints with non-negative weights (the generalization can be obtained through algebraic manipulations, as described in @cite ). A weight constraint @math has the form: Note that grounding removes Smodels ' conditional literals. [ L p_1 = w_1, ,p_n = w_n,not :r_1 = v_1, ,not :r_m = v_m U ] where @math are ground atoms, and @math are numeric constants. @math 's and @math 's are called literals of @math . @math denotes the set of literals of @math . The local weight function of a constraint @math , @math , returns the weight of its literals. For example, @math and @math . The weight of a weight constraint @math in a model @math , denoted by @math , is given by [ W(c,S) = p lit(c), : p S w(c)(p) + not :q lit(c), : q S w(c)(not :p). ]
- The following example, used in @cite to show that Smodels -semantics for weight constraints is counter-intuitive in some cases, indicates that the equivalence does not hold when negative literals are allowed in the weight constraint. ] According to the semantics described in @cite , we can observe that, for @math , the reduct @math is @math making it an answer set of @math . For @math , the reduct @math is [ ] thus making @math an answer set of @math .
- We will follow the convention used in @cite of fixing the set of base predicates @math to be equal to the set of EDB predicates, i.e., it contains only predicates which do not occur in the head of rules of @math . This will also mean that @math is fixed and @math is true in every interpretation of the program @math . As such, instead of saying that @math is monotone with respect to @math , we will often say that @math is monotone whenever there is no confusion.
- Another semantic characterization of aggregates that has been adopted by several researchers @cite @cite @cite @cite can be simply described as follows. Given a program @math and an interpretation @math , let @math be the program obtained by: [] removing all the rules with an aggregate atom or a negation-as-failure literal which is false in @math ; and [] removing all the remaining aggregate atoms and negation-as-failure literals. @math is a stable set of @math if @math is the least model of @math . We can prove the following result. Proof: Let @math and let us denote with @math . Let us show that @math .
- A somewhat different direction has been explored in @cite . The semantics proposed in @cite relies on approximation theory, and does not in general coincide with answer set semantics on normal programs. The work in @cite addresses this problem and provides a new semantics for aggregate programs which guarantees minimality of total answer sets, as in our case. We discussed the relationship between the work in @cite and ours in Subsection .
- In correlation based approaches @cite , a region of interest (or patch) is identified in the first image and correlated within a search window in the second image. The location of the best match is then used to compute a displacement vector. When the input image or field is tiled, possibly overlapping, and regions of interest are extracted from each tile location, the result is velocimetry at regular intervals and is most commonly used for Particle Image Velocimetry (PIV). In certain instances it is useful to define interest-points or salient features around which to extract regions of interest. In particular, if the field has many areas with negligible spatial variability, then matches are undefined. As a quality control measure then, matching is restricted only to those regions of interest that have interesting variability, or interest points.
- Here @math is the brightness or intensity scalar field and @math a displacement vector-field. Solutions to the optic flow equation can be formulated using the well-known method by @cite , which can be stated as a solution to the following system of equations:
- BP with other measurement ensembles. Other choices of the measurement matrix @math (instead of @math ) were also investigated by several authors, see e.g. @cite @cite @cite @cite @cite @cite @cite . For instance, it was shown (see e.g. @cite @cite @cite @cite @cite ) that an @math random matrix @math with independent Gaussian distributed entries (with variance @math ) has restricted isometry constant @math with probability at least @math provided Similar estimates are possible for Bernoulli matrices, i.e., random matrices with independent @math entries. Condition ) is slightly better than ). So for certain applications of compressed sensing Gaussian Bernoulli matrices @math might be useful. However, such completely random'' matrices have the disadvantage of not being structured, hence in contrast to the Fourier case no fast algorithms are available for matrix vector multiplication. Moreover, the samples'' @math lack a physical meaning. Of course, this does not matter in encoding decoding problems. However, there are possible applications where the samples result as the output of a physical measurement or an image reconstruction problem @cite and can really be interpreted as samples of a trigonometric polynomial. Clearly, in such a case one must use the Fourier matrix @math .
- Sublinear Algorithms for Sparse Fourier Analysis. Finally, we would like to point out that in the series of papers @cite @cite @cite @cite an approach from @cite has been considered. The algorithms are based on so called isolation and group testing from theoretical computer science. It has been proven that one can recover a sparse trigonometric polynomial @math , @math , from @math randomly chosen samples on @math if These results have been generalized to some extend for multivariate trigonometric polynomials in @cite @cite @cite . Moreover, @cite indicates that @math samples suffice if the random sampling set possesses additional structure.
- The striking point in these algorithms is the fact that the computation time scales sublinear'' as @math in the dimension @math . However, numerical experiments in @cite @cite show even for small sparsities @math a rather large crossover point with the classical FFT which scales as @math . It seems that in practice these algorithms require much more samples than OMP and BP, so that the main concern of these methods is speed rather than a minimal number of samples.
- A possible approach for refuting a formula @math is to find a resolution proof for the unsatisfiability of @math . However, Chvatal and Szemeredi @cite proved that a resolution proof of a random 3CNF formula with linear number of clauses is almost surely of exponential size. A result of a similar flavor for denser formulas was given by Ben-Sasson and Wigderson @cite who showed that a random formula with @math clauses almost surely requires a resolution proof of size @math . These lower bounds imply that finding a resolution proof for a random formula is computationally inefficient.
- Further motivation for studying efficient refutability of random 3CNF formulas is given in @cite . There it is shown that if there is no polynomial time refutation heuristic that works for most 3CNF formulas with @math clauses (where @math is an arbitrarily large constant) then certain combinatorial optimization problems (like minimum graph bisection, the dense @math -subgraph, and others) have no polynomial time approximation schemes. It is an open question whether it is NP-hard to approximate these problems arbitrarily well, though further evidence that these problems are indeed hard to approximate is given in @cite .
- The algorithm considered in the current paper for refuting @math by computing @math was presented in @cite . There is was shown that when @math , almost surely @math . Our current work overcomes a difficulty that prevented the approach of @cite to show that @math , not even for formulas @math with a linear number of clauses. The difficulty was the existence of pairs of clauses that share two variables.
- In a series of papers @cite @cite , have analyzed the performance of standard gossip algorithms. Their fastest standard gossip algorithm for the ensemble of random geometric graphs @math has a @math -averaging time @cite This quantity is computed in section IV.A of @cite but the result is expressed in terms of absolute time units which needs to be multiplied by @math to become clock ticks. @math . For the @math in this paper this averaging time is @math . For @math scaling like @math for any @math , this averaging time scales likes @math . Note that in standard gossip, each gossip round corresponds to communication with only one-hop neighbor and hence costs only one radio transmission which means that the fastest standard gossip algorithm will have a total cost @math radio transmissions for @math . Therefore, our proposed algorithm saves a factor of @math in communication energy by exploiting geographic information.
- Two very recent papers by Moallemi and Van Roy @cite and Mosk-Aoyama and Shah @cite also consider the problem of computing averages in networks. The consensus propagation algorithm of @cite is a modified form of belief propagation that attempts to mitigate the inefficiencies introduced by the random walk'' in gossip algorithms. However, their results, although promising, have only been proven for regular graphs, and it is unclear whether their algorithm will prove efficient for the networks in this paper. In @cite , the authors use an algorithm based on Flajolet and Martin @cite to compute averages and bound the averaging time in terms of a spreading time'' associated with the communication graph. However, they only show the optimality of their algorithm for a graph consisting of a single cycle, so it is currently difficult to speculate how it would perform on a geometric random graph.
- In @cite the authors consider the related problem of computing the average of a network in . They propose a distributed algorithm to solve this problem and show how it can be related to cover times of random walks on graphs.
- Most proposed routing algorithms for self-organizing networks distribute the topology information to all nodes in the network. Thus, following the idea of indirection routing, the i 3 @cite proposes an overlays-based infrastructure that offers a rendezvous based communication abstraction. i 3 decouples the act of sending from the act of receiving: sources send packets to a logical identifier and receives express interest in packets sent to this identifier. i 3 uses a set of servers that store identifiers and map packets with these identifiers to i 3 nodes interested in receiving the packets. This approach combines the generality of IP-layer solutions with the versatility of overlay solutions. Our proposition uses a similar concept of indirect routing, however, it is not based in an overlay infrastructure and is independent of IP-layer.
- Similarly to Tribe, PeerNet @cite is a peer-to-peer based network layer for dynamic and large networks. The address reflects the node's location in the network and is registered with the respective identifier in the distributed node lookup service. In PeerNet, the addresses are organized as leaves of a binary tree -- the address tree. PeerNet routing is a recursive procedure descending through the address tree. Thus, in contrast to Tribe, PeerNet routing disseminates information about the global state of the network, and nodes maintain a routing table that has @math entries, i.e. @math per-node state (where @math is a number of nodes in the network). Because of the address tree organization, a node movement may require the assignment of new addresses to several nodes in PeerNet infrastructure, which implicitly generates many updates in lookup entries.
- Model checking is closely related to database query evaluation. The idea is based on the principle that Kripke structures can be viewed as relational databases @cite . One effective approach for efficiently implementing model checking is based on the translation of temporal formulae into automata and has become an intensive research area @cite @cite @cite . Another approach consists in translating temporal logics to Logic Programming @cite . Logic Programming has been successfully used as an implementation platform for verification systems such as model checkers. Translations of temporal logics such as CTL or @math -calculus into logic programming can be found in @cite @cite @cite . @cite presents the LMC project which uses XSB, a tabled logic programming system that extends Prolog-style SLD resolution with tabled resolution.
- The database query language Datalog has inspired work in @cite , where the language Datalog LITE is introduced. Datalog LITE is a variant of Datalog that uses stratified negation, restricted variable occurrences and a limited form of universal quantification in rule bodies. Datalog LITE is shown to encompass CTL and the alternation-free @math -calculus. Research on model checking in the modal @math -calculus is pursued in @cite where the connection between modal @math -calculus and Datalog is observed. This is used to derive results about the parallel computational complexity of this fragment of modal @math -calculus.
- In previous work @cite we showed that the model checking problem for CTL can be reduced to the query evaluation problem for fragments of Datalog. In more detail, @cite presents a direct and modular translation from the temporal logics CTL, ETL, FCTL (CTL extended with the ability to express fairness) and the modal @math -calculus to Monadic inf-Datalog with built-in predicates. It is called inf-Datalog because the semantics differ from the conventional Datalog least fixed point semantics, in that some recursive rules (corresponding to least fixed points) are allowed to unfold only finitely many times, whereas others (corresponding to greatest fixed points) are allowed to unfold infinitely many times. The work in @cite , which is a preliminary version of some of the results presented here, embeds CTL into a fragment of Datalog @math .
- We know that CTL can be embedded into Transitive Closure logic @cite and into alternation-free @math --calculus @cite . In @cite the authors observe that CTL can be embedded into stratified Datalog. In this paper it is the first time that the exact fragment of stratified Datalog with the same expressive power with CTL has been identified.
- Concerning containment of queries the majority of research refers to CQs. However there are important results concerning also Datalog programs. In @cite it was pointed out that query containment for monadic Datalog is decidable. The work in @cite shows that checking containment of nonrecursive Datalog queries in Datalog queries is decidable in exponential time. In @cite it is shown that containment of Datalog queries in non-recursive Datalog is decidable in triply exponential time, whereas when the non-recursive query is represented as a union of CQs, the complexity is doubly exponential. In @cite @cite authors proved that equivalence of stratified Datalog programs is decidable but only for programs with unary EDB predicates. Our results are the first that encompass also programs that contain binary EDB predicates.
- The basic reference for semantic query optimization is @cite . The most common techniques are: join elimination introduction, predicate elimination and introduction, and detecting an empty answer set. @cite discusses the implementation of predicate introduction and join elimination in an industrial query optimizer. Semantic query optimization techniques for relational queries are studied in @cite in the context of denial and referential constraints, and in @cite in the context of constraint tuple-generating dependencies (a generalization of CGDs and classical relational dependencies). FDs are used for reasoning about sort orders in @cite .
- Viral marketing can be thought of as a diffusion of information about the product and its adoption over the network. Primarily in social sciences there is a long history of the research on the influence of social networks on innovation and product diffusion. However, such studies have been typically limited to small networks and typically a single product or service. For example, Brown and Reingen @cite interviewed the families of students being instructed by three piano teachers, in order to find out the network of referrals. They found that strong ties, those between family or friends, were more likely to be activated for information flow and were also more influential than weak ties @cite between acquaintances. Similar observations were also made by DeBruyn and Lilien in @cite in the context of electronic referrals. They found that characteristics of the social tie influenced recipients behavior but had different effects at different stages of decision making process: tie strength facilitates awareness, perceptual affinity triggers recipients interest, and demographic similarity had a negative influence on each stage of the decision-making process.
- Social networks can be composed by using various information, i.e. geographic similarity, age, similar interests and so on. Yang and Allenby @cite showed that the geographically defined network of consumers is more useful than the demographic network for explaining consumer behavior in purchasing Japanese cars. A recent study by @cite found that adding network information, specifically whether a potential customer was already talking to" an existing customer, was predictive of the chances of adoption of a new phone service option. For the customers linked to a prior customer the adoption rate of was 3--5 times greater than the baseline.
- Factors that influence customers' willingness to actively share the information with others via word of mouth have also been studied. Frenzen and Nakamoto @cite surveyed a group of people and found that the stronger the moral hazard presented by the information, the stronger the ties must be to foster information propagation. Also, the network structure and information characteristics interact when individuals form decisions about transmitting information. Bowman and Narayandas @cite found that self-reported loyal customers were more likely to talk to others about the products when they were dissatisfied, but interestingly not more likely when they were satisfied.
- In the context of the internet word-of-mouth advertising is not restricted to pairwise or small-group interactions between individuals. Rather, customers can share their experiences and opinions regarding a product with everyone. Quantitative marketing techniques have been proposed @cite to describe product information flow online, and the rating of products and merchants has been shown to effect the likelihood of an item being bought @cite @cite . More sophisticated online recommendation systems allow users to rate others' reviews, or directly rate other reviewers to implicitly form a trusted reviewer network that may have very little overlap with a person's actual social circle. Richardson and Domingos @cite used Epinions' trusted reviewer network to construct an algorithm to maximize viral marketing efficiency assuming that individuals' probability of purchasing a product depends on the opinions on the trusted peers in their network. Kempe, Kleinberg and Tardos @cite have followed up on Richardson and Domingos' challenge of maximizing viral information spread by evaluating several algorithms given various models of adoption we discuss next.
- There are numerous other models of influence spread in social networks. One of the first and most influential diffusion models was proposed by Bass @cite . The model of product diffusion predicts the number of people who will adopt an innovation over time. It does not explicitly account for the structure of the social network but it rather assumes that the rate of adoption is a function of the current proportion of the population who have already adopted (purchased a product in our case). The diffusion equation models the cumulative proportion of adopters in the population as a function of the intrinsic adoption rate, and a measure of social contagion. The model describes an S-shaped curve, where adoption is slow at first, takes off exponentially and flattens at the end. It can effectively model word-of-mouth product diffusion at the aggregate level, but not at the level of an individual person, which is one of the topics we explore in this paper.
- Diffusion models that try to model the process of adoption of an idea or a product can generally be divided into two groups: Threshold model @cite where each node in the network has a threshold @math , typically drawn from some probability distribution. We also assign connection weights @math on the edges of the network. A node adopts the behavior if a sum of the connection weights of its neighbors that already adopted the behavior (purchased a product in our case) is greater than the threshold: @math .
- Cascade model @cite where whenever a neighbor @math of node @math adopts, then node @math also adopts with probability @math . In other words, every time a neighbor of @math purchases a product, there is a chance that @math will decide to purchase as well.
- In the independent cascade model, @cite simulated the spread of information on an artificially generated network topology that consisted both of strong ties within groups of spatially proximate nodes and weak ties between the groups. They found that weak ties were important to the rate of information diffusion. Centola and Macy @cite modeled product adoption on small world topologies when a person's chance of adoption is dependent on having more than one contact who had previously adopted. Wu and Huberman @cite modeled opinion formation on different network topologies, and found that if highly connected nodes were seeded with a particular opinion, this would proportionally effect the long term distribution of opinions in the network. Holme and Newman @cite introduced a model where individuals' preferences are shaped by their social networks, but their choices of whom to include in their social network are also influenced by their preferences.
- The problems on general graphs have also received attention. It is known that both and are APX-hard @cite @cite . Using a connection to minimum multicut, several groups @cite @cite @cite presented an @math approximation algorithm for . In fact, it was noted in @cite that the problem is as hard to approximate as minimum multicut (and so this @math factor seems very hard to improve). For the maximization version, algorithms with performance ratio better than @math are known for @cite @cite . The latter work by Swamy @cite shows that a factor @math approximation can also be achieved when the number of clusters is specified (i.e., for k for @math ).
- Another problem that has been considered, let us call it , is that of maximizing correlation, defined to be the difference between the number of agreements and disagreements. A factor @math approximation for on complete graphs is presented in @cite . Recently @cite showed an integrality gap of @math for the underlying semidefinite program used in @cite . They also prove that an approximation of @math can be achieved on general graphs @math , where @math is the Lov ' a sz Theta Function.
- Recently, an unmodulated spin chain has been proposed as a model for short distance quantum communication @cite @cite @cite @cite . In such a scheme, the state to be communicated over the channel is placed on one of the spins of the chain, propagates for a specific amount of time, and is then received at a distant spin of the chain (cf. Fig. ). When viewed as a model for quantum communication, it is generally assumed that a reset of the spin chain occurs after each signal @cite , for example by applying an external magnetic field, resulting in a memoryless channel. However, a continuous operation without reset may lead to higher transmission rates, and corresponds to a quantum channel with memory.
- Apparently, the first model of a quantum channel with memory was introduced by in 2001: they gave an example of a qubit channel with Markovian correlated noise @cite @cite in which entangled input states may increase the transmission rate for classical information. These results have recently been extended to some bosonic Gaussian channels @cite @cite . Such an effect has been demonstrated experimentally for optical fiber channels with fluctuating birefringence, in which consecutive light pulses undergo strongly correlated polarization transformation @cite @cite . (Whether such examples exist in the memoryless setting is still an open question, and presently considered one of the most eminent open problems of quantum information theory, with wide implications for other problems in the field @cite @cite .)
- Subsequently, the study of quantum channels with memory has largely been confined to channels with Markovian correlated noise (cf. @cite @cite and references therein). A Lindbladian approach to memory channels has been taken by @cite @cite . Upper bounds on the classical capacity for a more general class of channels have been given recently by @cite .
- All the memory channels discussed in this Section are causal quantum channels, and thus the Structure Theorem applies. A completely different approach has been taken by Hayashi and Nagaoka @cite , who refrain from imposing any structural assumption on the quantum channels they consider, and apply the information-spectrum method to obtain a coding theorem for the classical product state capacity, following work by Verd ' u and Han @cite on classical channels with memory.
- Most of the current works dealing with home health telecare are focused either on implementing a generic architecture for the integrated medical information system, on improving the daily life of patients using various automatic devices, specific equipment, and basic alarms, or on providing health care services to patients suffering from specific diseases like asthma, diabetes, cardiac, pulmonary, or Alzheimers. Rialle have presented in @cite an overview of projects related to home health telecare. Basic alarms are raised by smart sensors or low layers of a local intelligence unit when a problem occurs at a short temporal scale: either one parameter overpasses a critical value (nocturia, pollakisuria, fall, hypertensive crisis, etc.), or a critical scenario involving the value of possibly more than one parameter is recognized (asthma crisis, etc.). Our focus is on the broadcasting of high level alarms about the persons health status, which concern a larger temporal scale. That issue is solved by first learning the daily living habits of a person to be able to detect later unusual situations. That behavioral profile is built by mining heterogeneous multivariate temporal data collected from sensors installed at home for learning meaningful patterns.
- The techniques used for time-series mining vary according to the application, regarding the characteristics of both the temporal sequences under study and the expected patterns: degree of variability in the values, allowed transformations between instances of a same pattern, possible stretching in time. For instance, Hong have experimented training recurrent neural networks for an unsupervised extraction of multi-temporal sequence patterns @cite . However, this method suffers from noisy data. The use of finite state machines @cite may give out good results, which may however dramatically decrease as the dimensionality increases - that is the number of states. Chiu @cite have implemented in the context of times-series motifs extraction an efficient algorithm based on random projections initially proposed by Buhler and Tompa to find motifs in nucleotide sequences @cite . Although they only deal with one-dimensional time-series and then do not address the issue of heterogeneous multivariate time-series, this algorithm is actually interesting because of the rapid extraction of approximate results and the efficiency even in the presence of noise or don't care'' symbols. However this method, as implemented in @cite , does not allow for stretching in time between motifs instances.
- Following the publication of stide, several research studies have been published with criticisms and suggestions of improvement of stide @cite @cite @cite @cite . Under our proposed framework, they can be interpreted in a logical way to determine their basic foundations.
- Given some deletion kernel @math , it is of interest to consider pairs of partition structures @math such that @math reduces @math to @math , meaning that the following extension of formula d-eq holds: where q(n,x):= n ,: ,x d( ,x)p_0( ) ( 1 x n ) is the unconditional probability that the deletion rule removes a part of size @math from @math distributed according to @math . Pitman @cite showed that if @math is a sequence of partition structures such that size-biased deletion reduces @math to @math for each @math , and @math can be represented in terms of random sampling from @math with @math for each @math , then @math is an @math partition as in param2 for some @math and @math , in which case @math is the @math partition structure. This result and Theorem are two different two-parameter generalisations of Kingman's characterisation of @math partition structures. In the result of @cite , the deletion kernel is still defined by size-biased sampling, and repeated deletions generate a succession of partition structures. Whereas in Theorem the deletion kernel is modified, and repeated deletions generate the same partition structure.
- Update programs are simple and applied to a broader class of abductive programs, and extended abduction is realized by any procedure for computing answer sets of EDPs. In implementation, some restrictions on programs such as function-free and range-restricted conditions would be necessary. Moreover, since extended abduction includes normal abduction as a special case, update programs are also used for computing normal abduction in EDPs. To compute extended abduction, @cite introduced a transaction program which is a set of production rules to compute (anti-)explanations by fixpoint construction. The procedure works correctly in acyclic covered NLPs. Inoue introduces a simple program transformation from extended abduction to normal abduction in general EDPs. Using the transformation, extended abduction is executed via normal abduction.
- In deductive databases, integrity maintenance is often coupled with view updating @cite . Concerning studies which handle integrity maintenance in nonmonotonic logic programs, @cite @cite merge transactions of view updates and integrity maintenance in SLDNF-like top-down procedures. These procedures are sound and @cite is also complete for computing view updates satisfying integrity constraints in locally stratified logic programs. Abductive procedures in @cite @cite @cite also check integrity constraints in the process of computing candidate hypotheses. Compared with these studies, our approach in is based on the computation of answer sets and is applicable to non-stratified, disjunctive, and extended logic programs.
- Buccafurri @math introduce an inheritance program which consists of a set of EDPs ordered by a generality relation. It realizes default reasoning in inheritance hierarchies and is also applied to updating logic programs. According to @cite , inheritance programs are equivalent to update programs of Eiter @math 's, hence the same arguments as the comparison with update programs are applied.
- Decker provides an abductive procedure for computing both user updates and schema updates in normal logic programs. User updates corresponds to view updates, while schema updates consider updating a theory with a rule. The procedure is top-down and works correctly for locally stratified programs. Studies @cite @cite @cite characterize belief update revision based on normal abduction in monotonic propositional theories. These approaches are the so-called interpretation updates'' and compute updates in terms of individual models of a theory. This is in contrast to our theory updates which computes updates directly by a program.
- Inoue characterizes inconsistency resolution in an ELP @math by the abductive program @math . Then, he considers a maximal consistent subset of the hypotheses @math , which is computed using a program transformation from the abductive program to an ELP. We characterized the same problem by the abductive program @math in , but the result is the same as @cite for ELPs. The problem is also characterized by the abductive program @math in @cite . This formulation, however, produces different results in general. For instance, given the inconsistent program @math , @math has the minimal explanation @math which produces the updated program @math . On the other hand, @math has the minimal explanation @math and the result of update is @math . Thus, @math permits the introduction of new facts as well as the deletion of facts to resolve inconsistency. Generally, permitting introduction of sentences increases the number of possible solutions. Nevertheless, this type of inconsistency resolution is also realized by computing consistent U-minimal answer sets of the update program of @math .
- In the usual query model (where one query gives one value of @math ), it is easy to see that @math queries are also necessary. Classical lower bounds have also been shown for more general models (e.g. @cite ).
- Related problems in quantum computing. In collision problem , we are given a 2-1 function @math and have to find @math such that @math . As shown by Brassard, H yer and Tapp @cite , collision problem can be solved in @math quantum steps instead of @math steps classically. @math is also a quantum lower bound @cite @cite .
- If element distinctness can be solved with @math queries, then collision problem can be solved with @math queries. (This connection is credited to Andrew Yao in @cite .) Thus, a quantum algorithm for element distinctness implies a quantum algorithm for collision but not the other way around.
- Quantum walks. There has been considerable amount of research on quantum walks (surveyed in @cite ) and their applications (surveyed in @cite ). Applications of walks @cite mostly fall into two classes. The first class is exponentially faster hitting times @cite @cite @cite . In these applications, quantum walks are used to find a certain vertex exponentially faster than classically. (The first results @cite @cite @cite were quantum walk algorithms that are exponentially faster than classical algorithms based on classical random walks. The most recent result @cite is a quantum walk algorithm which is exponentially faster than any classical algorithm.) Our result is quite different from those, both in the problem that we consider and the way how we solve. While polynomial speedup achieved in our paper is smaller than exponential speedup in @cite , our result has the advantage that we solve a natural problem which has been widely studied in both classical and quantum computing. The second class is quantum walk search algorithms @cite @cite @cite .
- Our algorithm is most closely related to the second class. In this direction, @cite have constructed a counterpart of Grover's search @cite based on quantum walk on the hypercube. Childs and Goldstone @cite @cite and @cite have used quantum walk to produce search algorithms on @math -dimensional lattices ( @math ) which is faster than the naive application of Grover's search. This direction is quite closely related to our work. The algorithms by @cite @cite @cite and current paper solve different problems but all have similar structure.
- Recent developments. After the work described in this paper, the results and ideas from this paper have been used to construct several other quantum algorithms. @cite have used our element distinctness algorithm to give an @math query quantum algorithm for finding triangles in a graph. @cite have used ideas from the current paper to construct a faster algorithm for search on 2-dimensional grid. Childs and Eisenberg @cite have given a different analysis of our algorithm.
- Indexing techniques for relational databases is a large field of study. An overview of indexing of semistructured data, e.g., acyclic digraphs and XML data, may be found in @cite . A particular indexing technique for digraphs based on encoding paths as strings is introduced in @cite . In @cite the authors, of that paper, describe an analysis framework for tree-structured balanced access methods for evaluating performance, based on performance loss metrics" and assumed optimal tree structures. Stabbing and interval queries may be resolved using various trees and structures and recently the relational interval tree has been introduces, see @cite for the RI-tree and further references.
- The problem of securing the routing layer using cryptographically secure messages is addressed by @cite @cite , Papadimitratos and Haas @cite , and @cite . Schemes to handle authentication in ad hoc networks assuming trusted Certificate Authorities have been proposed by Zhou and Haas @cite , and @cite . @cite employ a self-organized PGP-based scheme to authenticate nodes using chains of certificates and transitivity of trust. Stajano and Anderson @cite authenticate users by imprinting,' in analogy to ducklings acknowledging the first moving subject they see as their mother. In OCEAN, we do not attempt to secure the routing layer, although our techniques may be used in conjunction with many secure routing protocols to increase performance and robustness.
- In contrast to securing the routing layer of ad hoc networks, some researchers have also focused on simply detecting and reporting misleading routing misbehavior. Watchdog and Pathrater @cite use observation-based techniques to detect misbehaving nodes and report observed misbehavior back to the source of the traffic. Pathrater manages trust and route selection based on these reports. This allows nodes to choose better paths along which to route their traffic by routing around the misbehaving nodes. However, the scheme does not punish malicious nodes; instead, they are relieved of their forwarding burden.
- CONFIDANT @cite also detects misleading nodes by means of observation and more aggressively informs other nodes of this misbehavior through reports sent around the network. Each node in the network hosts a monitor for observations, reputation records for first-hand and trusted second-hand reports, trust records to control the trust assigned to received warnings, and a path manager used by nodes to adapt their behavior according to reputation information. In more recent work @cite @cite , these researchers find that reputation schemes can be beneficial for fast misbehavior detection, but only when one can deal with false accusations, for which they propose a solution using Bayesian statistics. Our goal is to avoid the machinery for managing these reports and their associated trust issues entirely.
- Researchers have also investigated means of discouraging selfish routing behavior in ad hoc networks, generally through payment schemes @cite , @cite , @cite . These approaches either require the use of tamper-proof hardware modules or central bankers to do the accounting securely, both of which may not be appropriate in some truly ad hoc network scenarios. In the per-hop payment scheme proposed by Buttyan and Hubaux @cite , the payment units are called nuglets and reside in a secure tamper-proof module in each node. They find that given such a module, increased cooperation is beneficial not only for the entire network but also for individual nodes. We rely on much of their work and likewise use a payment scheme. In our simple chipcount'' mechanism, further described in , each node keeps track of the number of packets it has forwarded for its direct neighbors and expects corresponding willingness from those neighbors to carry its traffic. The scheme can result in unfairness to some hosts, but its simplicity and performance may be appropriate in some scenarios.
- We now very briefly describe the methods previous authors have used. They have so far followed two approaches to determine the limiting behaviour of the @math -functions as @math . The translational invariance of this problem means that it has a simple description in momentum space, and the Schr " o dinger approach relies on that fact. We will describe this in more detail in the section below. Beginning with the recursion relation , Nayak and Vishwanath @cite showed that where and @math is the matrix .
- They diagonalize the matrix @math , finding the eigenvalues @math and @math where @math . They then write the @math -functions in terms of the eigenvalues @math and @math and their associated eigenvectors and formally invert the original fourier transform to obtain the closed form integral representations for the wavefunction, They then approximate these, using a combination of the method of stationary phase in one range, and integration by parts in the other. (Note that the Left-Right labelling convention in @cite is the opposite to ours.)
- To ease the design process, various interactive tools have been proposed. For example, in order to add sensors into 3D printed objects, Capricate designs custom-shaped sensors that fit on complex surfaces and automatically wire the underlying sensors @cite . Another design tool, aeroMorph, focuses on simulating bending mechanism that creates shape-changing behaviors with common materials, such as paper and plastics @cite . With the interactive visualization, users receive feedbacks interactively as they work on the design. This tool provides great flexibility for them, bypassing time-consuming traditional validation approaches which require repeated fabrication. Platener, a low-fidelity fabrication tool, shows users an interactive interface with a global slider to define the fidelity-speed trade-off, making it easier to decide on fabrication fidelity @cite . My goal is to develop interactive tools and augment them with efficient physics-based simulations and optimization, providing more accurate interactive feedback on complex problems.
- To design custom-shaped geometries with more complex physical phenomena, offline computational optimization is usually used in the design process. Digital Mechanical Metamaterials proposes to embody mechanical movements into 3D printed objects using a modular method @cite . Each of the modular cells is specially designed such that they can pass digital information when connected together. Acoustruments introduced a passive acoustic-based mechanisms for interactive controls on smartphones @cite . Through carefully designed tube geometries and materials, an expansive dataset of design primitives is generated for easy construction. To reproduce physical haptic interaction during fabrication, HapticPrint explores and builds a library of various patterns to different types of compliance @cite . While this type of research is a promising direction, I would like to investigate how to better combine computational design with intuitive tools for the users.
- Classical Face Alignment Classical face alignment methods, including Active Shape Model (ASM) @cite @cite and Active Appearance Model (AAM) @cite @cite @cite @cite , simulate the image generation process and perform face alignment by minimizing the difference between the model appearance and the input image. These methods can achieve accurate reconstruction results, but require a large number of face models with detailed and accurate point-wise correspondence, as well as high computation cost of parameter fitting. Constrained Local Model (CLM) @cite @cite employs discriminative local texture models to regularize the landmark locations. The CLM algorithm is more robust than the AAM method, which updates the model parameters by minimizing the image reconstruction error. Recently, regression based methods @cite @cite have been proposed to directly estimate landmark locations from the discriminative features around landmarks. Most regression based algorithms do not consider the visibility of facial landmarks under different view angles. As a result, their performance can degrade substantially for input face images with large poses.
- One of the few deployed (nearly) real-time crowd-powered systems, VizWiz @cite allowed blind and low vision users to ask visual questions in natural language when needed. VizWiz used crowd workers to reply to visual and audio content. To date, VizWiz has helped answer over 100,000 questions for thousands of blind people VizWiz: http: www.vizwiz.org . VizWiz is a rare example of a crowd-powered system that has been brought out of the lab. For example, in order to make the system cost effective, latency was higher and fewer redundant answers were solicited per query. However, VizWiz relied less on redundancy in worker responses, and more on allowing end users to assess if the response was plausible given the setting. VizWiz tasks consist of individual, self-contained units of work, rather than a continuous task.
- View @cite , which was built upon the ideas introduced in VizWiz, used a continuous interaction between multiple crowd workers and an end user based on video. View, which aggregates workers answers, has showed that multiple workers answer more quickly, accurately, and completely than individuals. Unfortunately, to date, View has not been deployed in the wild. This is in part because of the cost of scaling this type of continuous interaction, as well as ensuring on-going reliability with minimal ability to automatically monitor interactions. Be My Eyes Be My Eyes: http: www.bemyeyes.org is a deployed application with a similar goal: answer visual questions asked by blind users by streaming video. However, while they draw from a crowd of remote people to answer questions, the interaction is one-on-one, which assumes reliable helpers are available. Be My Eyes relies on volunteers rather than paid crowd workers. However, in more general settings, relying on volunteers is not practical.
- However, all of these intelligent agents are limited in their ability to understand their users. In response, crowd-powered intelligent agents like Chorus @cite use crowdsourcing to make on-going conversational interaction with an intelligent assistant.'' Alternatively, conversational assistants powered by trained human operators such as Magic Magic: http: getmagicnow.com and Facebook M have also emerged in recent years.
- Real-time tracking and correlation filters: Visual tracking methods can rely on either generative ( @cite ) or discriminative ( @cite ) models. Discriminative models are often found to outperform in accuracy by discriminating the target from background. Such trackers can usually run fast using hand-crafted features ( HOG @cite ) and various learning methods of P-N learning @cite , structured SVM @cite , multi-expert entropy minimization @cite , and correlation filter @cite .
- One common reason for the slow speed of the above-mentioned deep trackers is they always conduct a complete feed-forward pass to the last CNN layer. This ignores the fact that the tracking complexity differs across varying conditions. One of our conclusions is that most frames in current video benchmarks are rather easy. For those frames, forwarding to only early layers may suffice. In principle, such an insight can be used to speed up many recent real-time deep trackers, such as GOTURN @cite (165 fps on GPU) and SiamFC @cite (86 fps on GPU), to make them more CPU-friendly with near frame-rate speed.
- Feature selection in tracking: Good features are important to tracking. The initial DCF trackers were limited to a single feature channel, a grayscale image in MOSSE @cite . The DCF framework was later extended to multi-channel features such as HOG @cite @cite , Haar-like features @cite , binary patterns @cite and Color Attributes @cite . Generally, the hand-crafted features are cheap to compute, but they are not discriminative enough to handle complex tracking scenarios. Many recent deep trackers ( @cite @cite ) exploit the semantically robust features from the last CNN layer (fully-connected). However, spatial details of the tracked object are lost in the last layer which is not optimal for visual tracking. Danelljan al @cite found the first convolutional layer is very suitable for tracking. Other works @cite @cite @cite @cite choose to utilize all the hierarchical convolutional features, where early layers can preserve high spatial resolution and deep layers are more discriminative.
- In this paper, we make the best of hand-crafted and deep convolutional features in a cascaded structure, and learn an agent to select a minimum sequence of feature layers for fast tracking purposes. Unlike FCNT @cite that selects features from two pre-defined layers only after a complete forward pass, our selection is sequential and can stop early at any layer with enough confidence.
- Feature cascades: CNNs are a natural cascaded architecture with increasingly abstract feature representations. Contemporary works either improve the cascade's optimality by deep layer supervision @cite , or stack multiple CNNs into a deeper cascade for coarse-to-fine @cite or multi-task @cite predictions. Our work differs in learning a decision policy of using only early feature layers in a cascade, and in combining feature cascades and reinforcement learning @cite to achieve this goal. Our approach bears some similarity to the attentional cascade'' structure @cite which uses a cascade of gradually more complex classifiers. The difference is that attentional cascade aims to use early classifiers to eliminate easy negatives and reduce the burden of complex classifier evaluation, whereas we aim to use these early layers to make a strong decision as early as possible.
- Reinforcement learning for tracking: Reinforcement learning (RL) @cite @cite is capable of learning good policies to take a sequence of actions based on trail and error. It has been successfully applied to vision tasks ( object detection @cite ) by treating them as a decision-making process. For visual tracking, there are two recent works that use RL to temporally attend target regions @cite and select suitable template @cite . Our work is the first one to use RL to learn an early decision policy for speeding up deep tracking.
- Recently, there have been many works related to CNN for computer vision problems. We introduce a few insightful works that are relevant to image processing problems, and all of these pioneer works have demonstrated successful results: image denoising @cite @cite , image artifact removal @cite @cite , super resolution @cite @cite @cite , deblurring @cite @cite , colorization @cite @cite , image inpainting @cite @cite @cite , and image matting @cite @cite . Interestingly, researchers have shed little light on image retargeting based on a deep CNN approach. We sketch the advances of image retargeting works in a categorical manner as follows.
- Approaches for image retargeting can be categorized in a variety of ways, but we roughly classify them into seam carving based and warping based methods. The methods @cite @cite @cite @cite @cite change the aspect ratio of an image by repeatedly removing or insetting seams at unimportant areas. Avida @cite introduce the concept of seam carving and solve it using dynamic programming. Rubinstein @cite later apply seam carving in 3D volume for video retargeting. A representation of multisize media is defined by Shamir @cite to have continuous resizing ability in real time. Han @cite find multiple seams simultaneously with region smoothness and seam shape prior. Frankovich and Wong @cite propose an enhanced seam carving method by incorporating energy gradient information into optimization framework.
- The approaches @cite @cite @cite @cite @cite @cite continuously transform an input image into an image of a target size. In Liu and Gleicher @cite , a non-linear image warping is used to emphasize important parts of an image. Wolf @cite try to reduce distortion by shrinking less important pixels and preserving important regions. A scale-and-stretch warping method is proposed by Wang @cite . Their method iteratively computes optimal scaling factors for local regions and updates a warped image assisted by edge and saliency map. Guo @cite suggest a mesh representation based on image structures to preserve the shape of an input image. An interactive content-aware retargeting is proposed by Jin @cite . They formulate retargeting using a sparse linear system based on a triangular mesh, and then solve it via quadratic optimization.
- We also mention that several recent works extend the standard model of MRA (where @math is fixed) to a scenario where @math assumes discrete heterogeneity @cite @cite @cite , namely that @math is chosen from a finite set of templates @math .
- Last, we mention @cite , where the authors considered the problem of MRFA in the restricted case of @math . A key observation in @cite is that if the distribution of the shifts is uniform, then the bispectrum vanishes entirely. Therefore, it was proposed to employ the fourth-order shift-invariant moment known as the trispectrum @cite to estimate the signal parameters ( @math and @math ), and algorithms were presented for consistently estimating @math and @math as @math . The trispectrum is analogous to the power spectrum and the bispectrum, in the sense that it consists of certain quadruple correlations in the Fourier domain of the signal, which are invariant to cyclic shifts. As in MRA, much focus was put on the sample complexity of the algorithms in the low-SNR regime, since in the high-SNR regime the observations can be accurately aligned using correlations (a fact which is only true for the case @math ).
- Traditional techniques of human pose recovery usually apply pictorial structure models @cite @cite @cite @cite @cite to optimize body part configurations. However, due to the rapid advancements in deep learning, recovering articulated human poses using Convolutional Neural Network (CNN) models has become increasingly popular. For instance, Wei al @cite proposed Convolutional Pose Machines (CPM) to estimate 2D pose keypoints by learning a multi-stage CNN. The authors used a sequential convolutional structure to capitalize on the spatial context and iteratively updated the belief maps. In their method, the receptive field of neurons is carefully designed at each stage to allow the learning of complex and long-range correlations between the body parts. Similarly, Newell al @cite proposed a Stacked Hourglass method to process visual features across different scales. Their results are consolidated to better capture various spatial relationships associated with human body. Nevertheless, the CPM and Stacked Hourglass only work for static frame images and do not account for any geometric consistency between different video frames.
- To estimate 2D human poses in video, Pfister al @cite exploited the temporal context in video by combining information across multiple frames with optical flow. They used the resulting information to align heatmap predictions from the neighbouring frames. As a more recent attempt in modeling temporal information for human pose recovery, Luo al @cite re-modelled CPM as a Recurrent Neural Network to replace the multiple stages of CPM with sequential LSTM cells. The concept of using hand-crafted optical flow or recurrent structure to model temporal information in pose recovery task is beneficial. Nevertheless, both of these methods remain limited to recovering 2D keypoints only.
- It is challenging to extend 2D keypoints recovery methods to recover 3D skeletons, as the latter demands sophisticated solutions. For example, Camillo @cite had to enforce additional constraints on the relative lengths of human limbs and the body joint kinematics to select valid limb configuration. Ramakrishna al @cite proposed an activity-independent technique to recover 3D joint configurations using 2D locations of anatomical landmarks. They also leveraged a large motion capture corpus as a proxy to infer the plausible 3D configurations. A few contributions in this direction have also formulated 3D skeleton recovery as a supervised learning problem. For instance, Pavlakos al @cite proposed a volumetric technique to estimate 3D human poses from a single image. Their volumetric representation converts the 3D coordinate regression problem to a more manageable prediction task in a discretized space. With this representation, they concatenated multiple fully convolutional network components to implement a iterative coarse-to-fine learning process.
- Mehta al @cite enhanced CNN supervision with intermediate heat-maps, and used transfer learning from in-the-wild 2D pose data to improve the generalization to in-the-wild images for 3D pose recovery task. As a typical per-frame pose estimation technique, their method exhibits temporal jitters in video sequences. Another technique, VNect @cite formulates the 3D skeleton recovery problem as a CNN pose regression task that follows an optimization process, termed kinematic skeleton fitting. It is specifically designed to improve temporal stability of the recovered poses. Wang al @cite proposed a two-step technique for 3D pose estimation, named DRPose3D. In the first step, it uses a Pairwise Ranking CNN to extract depth rankings of human joints from images. In the second step, it uses this information to regress the 3D poses. This method depends on a 2D pose estimator for computing the initial joint heat maps. Consequently, it can not be treated as an end-to-end trainable technique.
- Due to its attractive applications, full 3D mesh pose recovery from images is an emerging direction. Many recent methods adopt the parametric human model @cite in this regard. For instance, Alldieck al @cite @cite @cite inferred 3D shape of person with details including hair and cloth using parametric model. Yu al @cite @cite also used depth sensor to reconstruct 3D body shapes in cloth. They used cloth simulation for single view human performance capture. Kanazawa al @cite adopted the parametric human model for 3D mesh pose recovery and predicted its pose and shape parameters from monocular images in an end-to-end manner. Such a learning-based method requires training data with 3D annotations @cite , a requirement fulfilled by very few datasets @cite @cite . The lack of training data has also driven research to exploit generative adversarial networks for 3D pose learning. Kanazawa al @cite used unpaired 3D annotations to create a factorized adversarial prior. Similarly, Yang al @cite learned a discriminator to enforce the 3D pose estimator to generate the plausible poses. However, a major limitation of such methods is that their complexity increases significantly when moving from frame to video modeling.
- The following works feature datasets made of ground-truth noisy image sets. The static scenes approach is necessary to directly compare the level of degradation using a loss function such as the structural similarity index (SSIM) @cite or the mean square error (MSE).
- The Darmstadt Noise Dataset (DND) @cite , containing 50 pairs of noisy-clean images from four cameras, was developed for the purpose of validating denoising algorithms using real data. Synthetic noise is typically used to train and test models, but it had been unclear whether the reported synthetic results translated to real improvements. Pltz and Roth showed that many modern denoising methods do not perform well on real data and that BM3D @cite , which was published in 2006, remains one of the best performing methods @cite . RENOIR @cite is a similar dataset that was published prior to the DND; however, Pltz and Roth noted spatial misalignments that reduced its effectivity. We have additionally found that the light sometimes differs between images in the same scene and that some photographs exhibit significant raw overexposure.
- See-in-the-Dark (SID) @cite is an image noise dataset that is large enough for training and, to our knowledge, was used in the first successful attempt at denoising images using real image noise. This dataset focused on very low-light photography where the camera-generated JPEG appears black. The authors used a U-Net network architecture to create an end-to-end RAW-to-JPEG pipeline that produces realistic colors, improving on standard processing and BM3D denoised images which still suffer from color bias at high ISO. Our work differs from SID in that we aimed to train a general purpose ( blind") denoiser rather than one that handles a specific condition, such as extremely low light images. We chose to work in sRGB space because handling the whole RAW-to-sRGB pipeline removes some information which may otherwise be useful to the author during development. Moreover, one dataset can then be used with different types of color filter arrays.
- The Smartphone Image Denoising Dataset (SIDD) @cite is comprised of 10 scenes * 5 cameras * 4 conditions * 150 images, totalling 30000 images. This dataset aims to address the problem of smartphone image denoising, where the small sensor and aperture size causes noticeable noise even in pictures taken at base ISO. Further processing is thus applied to create ground-truth images out of many images. This method of creating ground-truth images is not entirely relevant for denoising images captured with larger sensors because a single image taken at base ISO on a DSLR-like camera is clean enough to work as ground-truth for training purposes.
- The second group has centered on algorithms to achieve fairness. Along the route of disparate impact, @cite has described algorithms to spot the presence of disparate impact through Support Vector Machine, while @cite applied the notion of disparate impact to design an algorithm that achieves balance in unsupervised clustering algorithms. @cite also introduces the notion of , which we have used in our paper.
- There has been considerable study recently of distributed control and coordination of a set of autonomous robots. Gage @cite @cite has proposed the development of command and control tools for arbitrarily large swarms of microrobots and has proposed coverage paradigms in the context of robot dispersal in an environment. @cite @cite propose the notion of pheromone robotics'' for world-embedded computation. @cite @cite @cite @cite developed multi-robot algorithms, directly inspired by ant behaviors, for searching and covering. @cite @cite and @cite @cite @cite have studied algorithmic aspects of pattern formation in distributed autonomous robotics under various models of robots with minimal capabilities. The related flocking problem, which requires that a set of robots follow a leader while maintaining a formation, has been studied in several recent papers; see, e.g., @cite @cite @cite . Balch @cite has developed Teambots'', a Java-based general-purpose multi-robot simulator.
- There is a vast literature on algorithms for one or several robots to explore unknown environments. The environments can be modeled as graphs (directed or undirected), mazes, or geometric domains, and the robots can have a range of computing powers; see, e.g., @cite @cite @cite @cite @cite . Mitchell @cite includes a survey of many results on exploring and navigating in geometric environments.
- * The and spectral methods If the node attribute space @math is a finite set and no edge label is available, then the reduces to the classical with finite number of blocks. The spectral method and its variants are widely used to recover the underlying clusters under the , see, , @cite @cite @cite @cite @cite . However, the previous analysis relies on the low-rank structure of the edge probability matrix. In contrast, the edge probability matrix under the is not low-rank, and our analysis is based on establishing a correspondence between the spectrum of a compact operator and the spectrum of a weighted adjacency matrix (see Proposition ). Similar connection appears before in the context of data clustering considered in @cite , where a graph is constructed based on observed attributes of nodes and clustering based on the graph Laplacian is analyzed. In contrast our setup does not assume the observation of node attributes. Also in our case the observed graphs could be very sparse, while the graphs considered in @cite are dense.
- * Latent space model If the node attribute space @math is a finite-dimensional Euclidean space and no edge label is present, then the reduces to the latent space model, proposed in ( @cite @cite ). If we further assume the node attribute space @math is the probability simplex endowed with Dirichlet distribution with a parameter @math , and @math is a bilinear function, then the reduces to the mixed membership proposed in @cite , which is a popular model for studying the overlapping community detection problem.
- * Exchangeable random graphs If we ignore the edge labels, the fits exactly into the framework of exchangeable random graphs'' and the edge probability function @math is known as graphon'' (see , @cite and the references therein). It is pointed out in @cite that some known functions can be used to approximate the graphon, but no analysis is presented. Our spectral algorithm approximates the graphon using the eigenfunctions and The exchangeable random graph models with constant average node degrees has been studied in @cite , but the focus there is on the phase transition for the emergence of the giant connected component.
- * Phase transition if @math There is an emerging line of works @cite @cite @cite @cite @cite @cite that try to identify the sharp phase transition threshold for positively correlated clustering in the regime with a bounded average node degree. All previous rigorous results focus on the two communities case, while @cite gives detailed predictions about the phase transition thresholds in the more general case with multiple communities. Here with multiple communities we identify a threshold below which positively correlated clustering is impossible. However, our threshold is not sharp.
- The current mainstream of cross-modal retrieval methods is to learn an intermediate common space for the features of different modalities, then the cross-modal similarity can be directly measured in one common space. Figure illustrates the main framework of such common space learning methods. As indicated in @cite , we mainly introduce three categories of existing methods as follows, namely traditional statistical correlation analysis methods, cross-modal graph regularization methods and DNN-based methods.
- Hybrid TMs share some similarities to our work, since they all feature multiple execution paths. The first hybrid TM algorithms allowed HTM and STM transactions to run concurrently @cite @cite . Hybrid NOrec @cite and Reduced hardware NOrec @cite are hybrid TMs that both use global locks on the fallback path, eliminating any concurrency. We discuss two additional hybrid TMs, Phased TM @cite (PhTM) and Invyswell @cite , in more detail.
- Invyswell is closest to our three path approach. At a high level, it features an HTM middle path and STM slow path that can run concurrently (sometimes), and an HTM fast path that can run concurrently with the middle path (sometimes) but not the slow path, and two global locking fallback paths (that prevent any concurrency). Invyswell is more complicated than our approach, and has numerous restrictions on when transactions can run concurrently. Our three path methodology does not have these restrictions. The HTM fast path also uses an optimization called lazy subscription. It has been shown that lazy subscription can cause opacity to be violated, which can lead to data corruption or program crashes @cite .
- Different options for concurrency have recently begun to be explored in the context of TLE. Refined TLE @cite and Amalgamated TLE @cite both improve the concurrency of TLE when a process is on the fallback path by allowing HTM transactions to run concurrently with a on the fallback path. Both of these approaches still serialize processes on the fallback path. They also use locks, so they cannot be used to produce lock-free data structures.
- Timnat, Herlihy and Petrank @cite proposed using a strong synchronization primitive called ( @math -CAS) to obtain fast HTM algorithms. They showed how to take an algorithm implemented using @math -CAS and produce a two-path implementation that allows concurrency between the fast and fallback paths. One of their approaches used a lock-free implementation of @math -CAS on the fallback path, and an HTM-based implementation of @math -CAS on the fast path. They also experimented with two-path implementations that do not allow concurrency between paths, and found that allowing concurrency between the fast path and fallback path introduced significiteant overhead. Makreshanski, Levandoski and Stutsman @cite also independently proposed using HTM-based @math -CAS in the context of databases.
- In recent years, significant efforts have been made towards learning embeddings of KBs. TransE @cite , the pioneer of translation-based methods, interprets a relationship vector as the translation from the head entity vector to its tail entity vector. In other words, if a relationship triple @math holds, @math is expected. TransE has shown its great capability of modeling 1-to-1 relations and achieved promising results for KB completion. To further improve TransE, later work including TransH @cite and TransR @cite was proposed. Additionally, there exist a few non-translation-based approaches to KB embedding @cite @cite @cite .
- Besides, several studies take advantage of knowledge in KBs to improve embeddings. Krompa @cite added type constraints to KB embedding models and enhanced their performance on link prediction. KR-EAR @cite embeds attributes additionally by modeling attribute correlations and obtains good results on predicting entities, relationships and attributes. But it only learns attribute embeddings in a single KB, which hinders its application to cross-lingual cases. Besides, KR-EAR focuses on the attributes whose values are from a small set of entries, e.g. values of gender" are Female, Male . It may fail to model attributes whose values are very sparse and heterogeneous, e.g. name", label" and coordinate". RDF2Vec @cite uses local information of KB structures to generate sequences of entities and employs language modeling approaches to learn entity embeddings for machine learning tasks. For cross-lingual tasks, @cite extends NTNKBC @cite for cross-lingual KB completion. @cite uses a neural network approach that translates English KBs into Chinese to expand Chinese KBs.
- Existing work on cross-lingual KB alignment generally falls into two categories: cross-lingual ontology matching and cross-lingual entity alignment. For cross-lingual ontology matching, Fu @cite @cite presented a generic framework, which utilizes machine translation tools to translate labels to the same language and uses monolingual ontology matching methods to find mappings. Spohr @cite leveraged translation-based label similarities and ontology structures as features for learning cross-lingual mapping functions by machine learning techniques (e.g. SVM). In all these works, machine translation is an integral component.
- Edge detection has evolved from category-agnostic to category-aware semantic level towards more complete and coherent scene understanding. Category-agnostic edge detection aims to detect object boundaries in a simple binary classification manner, in which each pixel is classified as edge or non-edge without distinguishing specific categories. Recently, deep methods @cite @cite @cite @cite employ holistically nested topology to solve this task such as HED @cite . Category-aware semantic edge detection @cite is an extension of category-agnostic edge detection. Inspired by HED, CASENet @cite shares the same set of low-level hierarchical features and then fuses them with semantic channels at the top convolution layer. Several following works @cite @cite @cite @cite continuously lift semantic edge detection performance. In existing SED setting, predicted boundaries of categories are semantic-level and in this work, we propose a new panoptic edge detection task aiming to form a complete and coherent scene understanding by predicting instance-level boundaries for . We also design a multi-branch network PEN to solve this challenging task.
- Our work is also related to panoptic segmentation @cite , which unifies semantic segmentation and instance segmentation. Panoptic segmentation also generates a coherent scene parsing but from the perspective of segmentation. Panoptic FPN @cite tackles panoptic segmentation by extending Mask R-CNN @cite with a semantic segmentation branch. We choose Panoptic FPN with some minor modifications (denoted as ) as a baseline model. The comparison experiments between with the proposed PEN model (in Section ) demonstrate that directly utilising a framework designed for panoptic segmentation will not work for panoptic edge detection due to the intrinsic differences between these two tasks.
- The -test-and-set spin lock @cite makes challenging threads continuously check the lock variable until it is released and, only in this case, they try to acquire it via RMW instructions. This allows threads to (read the actual value of the lock variable) in cache without disturbing others, thus generating cache memory traffic only when strictly needed.
- The authors of @cite introduce a simple back-off time before attempting to re-acquire the lock. Anyhow, such a strategy requires some variables to be set up, such as the maximum and minimum back off time, that cannot be universal across any hardware architecture and or workload @cite .
- Spin locks might lead to starvation since there is no assurance that a given thread wins the challenge eventually. The authors of @cite introduce the queued spin lock to resolve this issue. It is a linked list where the first connected node is owned by the thread holding the lock, while others are inserted in a FIFO order by threads trying to access the critical section. Such threads spin on a boolean variable encapsulated in their individual nodes. This guarantees that each spinning thread repeatedly reads a memory cell different from other threads and a releasing thread updates a cacheline owned by a unique CPU-core, which significantly reduce the pressure on the cache management firmware.
- This work builds on the original article on ST @cite that stands out from the literature as one of the few works that explicitly addresses increasing the robustness a neural network against generic input perturbations. We extend their work by considering train distorsions beyond Gaussian noise and a symmetric stability objective to increase the method's performance for transformative distorsions.
- There is a large body of works on related methods in the context of semi-supervised learning. The most relevant works for the present context can be subsumed as consistency smoothness enforcing methods. The common idea in all cases is to impose a consistency constraint @cite to enforce similar classification behavior for the original and perturbed input. On the labeled subset of the data both the original loss and the consistency loss can be evaluated, whereas on the unlabeled subset only the consistency loss is imposed. The focus in these works lies on incorporating information from the unlabeled subset to increase the model performance but none of them considered the aspect of robustness with respect to input distorsions. The main difference between the different methods lies in the way how the consistency constraint is implemented @cite @cite @cite @cite @cite @cite @cite .
- In the domain of DA, @cite recently presented a first theoretical investigation of the robustness properties of DA. On the practical side, there have been proposals for more elaborate implementations of DA that try to circumvent the need for dataset-specific hyperparameter searches by appropriate meta search algorithms @cite @cite , which focusing, however, on model performance rather than robustness. In fact, it would represent an interesting line of research to investigate also these techniques from the robustness point of view to see in detail how a far a DA strategy tailored for robustness can get. Mixup @cite can be seen as an extension to DA in the sense that not only on perturbed input samples but rather on convex combinations of input samples and the corresponding labels are used during training. Recent extensions such as @cite @cite @cite incorporate also a consistency constraint for stabilization.
- Generative Adversarial Networks: Generative Adversarial Networks @cite is a powerful implicit generative model to produce a model distribution that mimics a given target distribution, and it has been applied to many fields, such as low-level image processing tasks (image in-painting @cite @cite , image super-resolution @cite @cite @cite ), high-level semantic or style transfer @cite @cite @cite @cite , video prediction and generation @cite @cite and even classical computer vision problem (Object detection @cite ).
- The core idea for most of the method based on machine learning is to use large training set labeled with specific eye angle and head pose information to learn the warping field. The warping field is used to relocate eye pixels in the original image, thus realizing gaze redirection. @cite proposed a novel deep model based on the convolutional network as a predictor, which helps them to achieve a high-quality result. We present a novel method for gaze correction. With an encode-decode architecture as a generator and adversarial process, we can learn from the face image to fill in the missing regions with new contents representing corrected eye gaze. Compared with those learning-based methods, our training dataset is unpaired and collected from the website, where the dataset is not labeled with specific eye angle and head pose information, thus, is easy to collect and use. Furthermore, these models are hard to achieve high-quality results for gaze correction in the wild images.
- Automatic hashtag segmentation can improve the performance of many applications besides sentiment analysis, such as text classification @cite , named entity linking @cite and modeling user interests for recommendations @cite . It can also help in collecting data of higher volume and quality by providing a more nuanced interpretation of its content, as shown for emotion analysis @cite , sarcasm and irony detection @cite @cite . Better semantic analysis of hashtags can also potentially be applied to hashtag annotation @cite , to improve distant supervision labels in training classifiers for tasks such as sarcasm @cite , sentiment @cite , emotions @cite ; and, more generally, as labels for pre-training representations of words @cite , sentences @cite , and images @cite .
- 1.3 Red and blue blue words are negative and positive entries in the Twitter sentiment lexicon @cite , respectively.
- First of all, note that our work implicitly relies on previous work on termination for LD-derivations @cite @cite , since we reduce the problem of termination of a program with declarations to the classical problem of termination for LD-derivations.
