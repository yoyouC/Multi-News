- Within the MAS community, some work @cite has focused on how artificial AI-based learning agents would fare in communities of similar agents. For example, @cite and @cite show how agents can learn the capabilities of others via repeated interactions, but these agents do not learn to predict what actions other might take. Most of the work in MAS also fails to recognize the possible gains from using explicit agent models to predict agent actions. @cite is an exception and gives another approach for using nested agent models. However, they do not go so far as to try to quantify the advantages of their nested models or show how these could be learned via observations. We believe that our research will bring to the foreground some of the common observations seen in these research areas and help to clarify the implications and utility of learning and using nested agent models.
- Grasping action is the most basic component of any interaction and it is composed of three major components @cite . The first one is related to the process of approaching the arm and hand to the target object, considering the overall body movement. The second component focuses on the hand and body pre-shaping before the grasping action. Finally, the last component fits the hand to the geometry of the object by closing each of the fingers until contact is established.
- Grasping data-driven approaches have existed since a long time ago @cite . These methods are based on large databases of predefined hand poses selected using user criteria or based on grasp taxonomies (i.e. final grasp poses when an object was successfully grasped) which provide us the ability to discriminate between different grasp types.
- The selection process is also constrained by the hand high degree of freedom (DOF). In order to deal with dimensionality and redundancy many researchers have used techniques such as principal component analysis (PCA) @cite @cite . For the same purpose, @cite studied the correlations between hand DOFs aiming to simplify hand models reducing DOF number. The results suggest to simplify hand models by reducing DOFs from 50 to 15 for both hands in conjunction without loosing relevant features.
- In order to achieve realistic object interactions, physical simulations on the objects should also be considered @cite @cite @cite . Moreover, hand and finger movement trajectories need to be both, kinematically and dynamically valid @cite . @cite simulate hand interaction, such as two hands grasping each other in the handshake gesture. @cite simulate grasping an object, drop it on a specific spot on the palm and let it roll on the hand palm. A limitation of this approach is that information about the object must be known in advance, which disable robot to interact with unknown objects. By using an initial grasp pose and a desired object trajectory, the algorithm proposed by Liu @cite can generate physically-based hand manipulation poses varying the contact points with the object, grasping forces and also joint configurations. This approach works well for complex manipulations such as twist-opening a bottle. Ye and Liu @cite reconstruct a realistic hand motion and grasping generating feasible contact point trajectories. Selection of valid motions is defined as a randomized depth-first tree traversal, where nodes are recursively expanded if they are kinematically and dynamically feasible. Otherwise, backtracking is performed in order to explore other possibilities.
- Graph interpolation can be viewed as an extension of tree adjunction to parse graphs. And, indeed, TAGs @cite , by introducing a 2-dimensional formalism into computational linguistics, have made a decisive step towards designing a syntactic theory that is both computationally tractable and linguistically realistic. In this respect, it is an obligatory reference for any syntactic theory intent on satisfying these criteria.
- In Lexical Functional Grammars @cite , grammatical functions are loosely coupled with phrase structure, which seems to be just the opposite of what is done in a GIG, in which functional edges are part of the phrase structure. Nonetheless, these two approaches share the concern of bringing out a functional structure, even if much of what enters into an f-structure (i.e. a functional structure) in LFG is to be addressed by the semantic component ---a topic for further research--- in GIG.
- To our knowledge, lexical databases have been used only once in TC. Hearst @cite adapted a disambiguation algorithm by Yarowsky using WordNet to recognize category occurrences. Categories are made of WordNet terms, which is not the general case of standard or user-defined categories. It is a hard task to adapt WordNet subsets to pre-existing categories, especially when they are domain dependent. Hearst's approach shows promising results confirmed by the fact that our WordNet -based approach performs at least equally to a simple training approach.
- Lexical databases have been employed recently in word sense disambiguation. For example, Agirre and Rigau @cite make use of a semantic distance that takes into account structural factors in WordNet for achieving good results for this task. Additionally, Resnik @cite combines the use of WordNet and a text collection for a definition of a distance for disambiguating noun groupings. Although the text collection is not a training collection (in the sense of a collection of manually labelled texts for a pre-defined text processing task), his approach can be regarded as the most similar to ours in the disambiguation task. Finally, Ng and Lee @cite make use of several sources of information inside a training collection (neighborhood, part of speech, morfological form, etc.) to get good results in disambiguating unrestricted text.
- Word--sense disambiguation has more commonly been cast as a problem in supervised learning (e.g., @cite , @cite , @cite , @cite , @cite , @cite , @cite , @cite , @cite ). However, all of these methods require that manually sense tagged text be available to train the algorithm. For most domains such text is not available and is expensive to create. It seems more reasonable to assume that such text will not usually be available and attempt to pursue unsupervised approaches that rely only on the features in a text that can be automatically identified.
- A more recent bootstrapping approach is described in @cite . This algorithm requires a small number of training examples to serve as a seed. There are a variety of options discussed for automatically selecting seeds; one is to identify collocations that uniquely distinguish between senses. For plant , the collocations manufacturing plant and living plant make such a distinction. Based on 106 examples of manufacturing plant and 82 examples of living plant this algorithm is able to distinguish between two senses of plant for 7,350 examples with 97 percent accuracy. Experiments with 11 other words using collocation seeds result in an average accuracy of 96 percent.
- While @cite does not discuss distinguishing more than 2 senses of a word, there is no immediate reason to doubt that the one sense per collocation'' rule @cite would still hold for a larger number of senses. In future work we will evaluate using the one sense per collocation'' rule to seed our various methods. This may help in dealing with very skewed distributions of senses since we currently select collocations based simply on frequency.
- Clustering has most often been applied in natural language processing as a method for inducing syntactic or semantically related groupings of words (e.g., @cite , @cite , @cite , @cite , @cite , @cite ).
- An early application of clustering to word--sense disambiguation is described in @cite . There words are represented in terms of the co-occurrence statistics of four letter sequences. This representation uses 97 features to characterize a word, where each feature is a linear combination of letter four-grams formulated by a singular value decomposition of a 5000 by 5000 matrix of letter four-gram co-occurrence frequencies. The weight associated with each feature reflects all usages of the word in the sample. A context vector is formed for each occurrence of an ambiguous word by summing the vectors of the contextual words (the number of contextual words considered in the sum is unspecified). The set of context vectors for the word to be disambiguated are then clustered, and the clusters are manually sense tagged.
- The features used in this work are complex and difficult to interpret and it isn't clear that this complexity is required. @cite compares his method to @cite and shows that for four words the former performs significantly better in distinguishing between two senses.
- The literature on corpus-based determination of word similarity has recently been growing by leaps and bounds, and is too extensive to discuss in detail here (for a review, see @cite ), but most approaches to the problem share a common assumption: semantically similar words have similar distributional behavior in a corpus. Using this assumption, it is common to treat the words that co-occur near a word as constituting features, and to compute word similarity in terms of how similar their feature sets are. As in information retrieval, the feature'' representation of a word often takes the form of a vector, with the similarity computation amounting to a computation of distance in a highly multidimensional space. Given a distance measure, it is not uncommon to derive word classes by hierarchical clustering. A difficulty with most distributional methods, however, is how the measure of similarity (or distance) is to be interpreted. Although word classes resulting from distributional clustering are often described as semantic,'' they often capture syntactic, pragmatic, or stylistic factors as well.
- Statistical analysis of NLP data has often been limited to the application of standard models, such as n-gram (Markov chain) models and the Naive Bayes model. While n-grams perform well in part--of--speech tagging and speech processing, they require a fixed interdependency structure that is inappropriate for the broad class of contextual features used in word--sense disambiguation. However, the Naive Bayes classifier has been found to perform well for word--sense disambiguation both here and in a variety of other works (e.g., @cite , @cite , @cite , and @cite ).
- In order to utilize models with more complicated interactions among feature variables, @cite introduce the use of sequential model selection and decomposable models for word--sense disambiguation. They recommended a model selection procedure using BSS and the exact conditional test in combination with a test for model predictive power. In their procedure, the exact conditional test was used to guide the generation of new models and the test of model predictive power was used to select the final model from among those generated during the search.
- Alternative probabilistic approaches have involved using a single contextual feature to perform disambiguation (e.g., @cite , @cite , and @cite present techniques for identifying the optimal feature to use in disambiguation). Maximum Entropy models have been used to express the interactions among multiple feature variables (e.g., @cite ), but within this framework no systematic study of interactions has been proposed. Decision tree induction has been applied to word-sense disambiguation (e.g. @cite and @cite ) but, while it is a type of model selection, the models are not parametric.
- In computational linguistics, on the other hand, positive imperatives have been extensively investigated, both from the point of view of interpretation @cite @cite @cite @cite and generation @cite @cite @cite @cite . Little work, however, has been directed at negative imperatives. (for exceptions see the work of in interpretation and of in generation).
- Recently, VDSH @cite proposed to use a VAE to learn the latent representations of documents and then use a separate stage to cast the continuous representations into binary codes. While fairly successful, this generative hashing model requires a two-stage training. NASH @cite proposed to substitute the Gaussian prior in VDSH with a Bernoulli prior to tackle this problem, by using a straight-through estimator @cite to estimate the gradient of neural network involving the binary variables. This model can be trained in an end-to-end manner. Our models differ from VDSH and NASH in that mixture priors are employed to yield better hashing codes, whereas only the simplest priors are used in both VDSH and NASH.
- Most classical image denoising methods belong to this category, through designing a MAP model with a fidelity loss term and a regularization one delivering the pre-known image prior. Along this line, total variation denoising @cite , anisotropic diffusion @cite and wavelet coring @cite use the statistical regularities of images to remove the image noise. Later, the nonlocal similarity prior, meaning many small patches in a non-local image area possess similar configurations, was widely used in image denoising. Typical ones include CBM3D @cite and non-local means @cite . Some dictionary learning methods @cite @cite @cite and Field-of-Experts (FoE) @cite , also revealing certain prior knowledge of image patches, had also been attempted for the task. Several other approaches focusing on the fidelity term, which are mainly determined by the noise assumption on data. E.g., Mulitscale @cite assumed the noise of each patch and its similar patches in the same image to be correlated Gaussian distribution, and LR-MoG @cite , DP-GMM @cite and DDPT @cite fitted the image noise by using Mixture of Gaussian (MoG) as an approximator for noises.
- Instead of pre-setting image prior, deep learning methods directly learn a denoiser (formed as a deep neural network) from noisy to clean ones on a large collection of noisy-clean image pairs. Jain and Seung @cite firstly adopted a five layer convolution neural network (CNN) for the task. Then some auto-encoder based methods @cite @cite were applied. Meantime, @cite achieved the comparable performance with BM3D using plain multi-layer perceptron (MLP). @cite further proposed the denoising convolution network (DnCNN) and achieved state-of-the-art performance on Gaussian denoising tasks. @cite proposed a deep fully convolution encoding-decoding network with symmetric skip connection. In order to boost the flexibility against spatial variant noise, FFDNet @cite was proposed by pre-evaluating the noise level and inputting it to the network together with the noisy image. @cite and @cite both attempted to simulate the generation process of the images in camera.
- Text Embedding There has been various methods to embed textual information into vector representations for NLP tasks. The classical method for embedding textual information could be one-hot vector, term frequency inverse document frequency (TF-IDF), etc. Due to the high-dimension and sparsity problems in here, @cite proposed a novel neural network based skip-gram model to learn distributed word embeddings via word co-occurrences in a local window of textual content. To exploit the internal structure of text, convolutional neural networks (CNNs) @cite @cite is applied to obtain latent features of local textual content. Then, by following a pooling layer, fixed-length representations are generated. To have the embeddings better reflect the correlations among texts, soft attention mechanisms @cite @cite is proposed to calculate the relative importances of words in a sentence by evaluating their relevances to the content of comparing sentences. Alternatively, gating mechanism is applied to strengthen the relevant textual information, while weakening the irrelevant one by controlling the information-flow path of a network in @cite @cite .
- Modern neural networks that provide good performance tend to be large and overparameterised, fuelled by observations that larger @cite @cite @cite networks tend to be easier to train. This in turn drives numerous efforts to reduce model size using techniques such as weight pruning and quantisation @cite @cite @cite .
- Early works like @cite and @cite explored pruning by computing the Hessian of the loss with respect to the parameters in order to assess the saliency of each parameter. Other works involving saliency computation include @cite and @cite where sensitivity of the loss with respect to neurons and weights are used respectively. On the other hand, works such as @cite @cite directly induce network sparsity by incorporating sparsity-enforcing penalty terms into the loss function.
- Most of the recent works in network pruning focused on vision-centric classification tasks using Convolutional Neural Networks (CNNs) and occasionally RNNs. Techniques proposed include magnitude-based pruning @cite @cite @cite and variational pruning @cite @cite @cite . Among these, magnitude-based weight pruning have become popular due to their effectiveness and simplicity. Most notably, @cite employed a combination of pruning, quantization and Huffman encoding resulting in massive reductions in model size without affecting accuracy. While unstructured sparse connectivity provides reduction in storage size, it requires sparse General Matrix-Matrix Multiply (GEMM) libraries such as cuSPARSE and SPBLAS in order to achieve accelerated inference. Motivated by existing hardware architectures optimised for dense linear algebra, many works propose techniques to prune and induce sparsity in a structured way in which entire filters are removed @cite @cite @cite .
- [label= *)] Simple and fast. Our approach enables easy pruning of the RNN decoder equipped with visual attention, whereby the best number of weights to prune in each layer is automatically determined. Compared to works such as @cite @cite , our approach is simpler with a single hyperparameter versus @math - @math hyperparameters. Our method also does not rely on reinforcement learning techniques such as in the work of @cite . Moreover, our method applies pruning to all the weights in the RNN decoder and does not require special considerations to exclude pruning from certain weight classes. Lastly our method completes pruning in a single-shot process rather than requiring iterative train-and-prune process as in @cite @cite @cite @cite . Good performance-to-sparsity ratio enabling extreme sparsity. Our approach achieves good performance across sparsity levels from @math l_2 @math l_1 @math l_0$ regulariser are used to encourage network sparsity. Their work also only focuses on image classification using CNNs.
- While there are other works on compressing RNNs, most of the methods proposed either comes with structural constraints or are complementary to model pruning in principle. Examples include using low-rank matrix factorisations @cite @cite , product quantisation on embeddings @cite , factorising word predictions into multiple time steps @cite @cite @cite , and grouping RNNs @cite . Lastly, another closely related work by @cite also incorporated model pruning into image captioning. However we note three notable differences: 1) their work is focused on proposing a new LSTM cell structure named the ; 2) their work utilises the grow-and-prune (GP) method @cite which necessitates compute and time expensive iterative pruning; and 3) the compression figures stated are calculated based on the size of the LSTM cells instead of the entire decoder.
- BERT @cite is a pre-trained transformer network @cite , which set for various NLP tasks new state-of-the-art results, including question answering, sentence classification, and sentence-pair regression. The input for BERT for sentence-pair regression consists of the two sentences, separated by a special [SEP] token. Multi-head attention over 12 (base-model) or 24 layers (large-model) is applied and the output is passed to a simple regression function to derive the final label. Using this setup, BERT set a new state-of-the-art performance on the Semantic Textual Semilarity (STS) benchmark @cite . RoBERTa @cite showed, that the performance of BERT can further improved by small adaptations to the pre-training process. We also tested XLNet @cite , but it led in general to worse results than BERT.
- Pooling methods are requisite either in two-stream networks @cite @cite or in other feature fusion models. @cite simply uses average pooling and outperforms others. @cite proposes bilinear pooling to model local parts of object: two feature representations are learned separately and then multiplied using the outer product to obtain the holistic representation. @cite combines two-stream network with a compact bilinear representation @cite . @cite defines a general kernel-based pooling framework which captures higher-order interactions of features. However, most existing bilinear pooling models are capable to combine only two features, and none of their variants could cope with more than two features, which is needed in video action recognition.
- Recently, lightweight neural networks including SqeezeNet @cite , Xception @cite , ShuffleNet @cite , ShuffleNetV2 @cite , MobileNet @cite , and MobileNetV2 @cite have been proposed to run on mobile devices with the parameters and computation reduced significantly. Since we focus on mobile video action recognition, all these lightweight models could be use as backbone.
- Another important result is following the Bennett's inequality. Corollary 5 in @cite shows that: where @math is the sample variance. It is notable that @math is equivalent (with a constant scaling) to the empirical variance @math . Similarly, the above uniform estimate can be extended to infinite loss classes using different complexity measures .
- An intuitive approach to considering the variance-based regularization is to include the first two terms on the right hand side into the objective, which is the formulation proposed in @cite , i.e., sample variance penalty (SVP): An excess risk bound of @math may be achieved by solving the SVP. However, @cite does not consider solution methods for solving the above variance-regularized empirical risk minimization problem.
- Recently, @cite proposed a min-max formulation based on distributionally robust optimization for variance-based regularization as following: where @math is a hyper-parameter, @math , @math , and @math is called the @math -divergence based on @math . The above problem is convex-concave when the loss function @math is convex in terms of @math . It is was shown in that the above min-max formulation is equivalent to the problem ) with a proper value of @math with high probability under the assumption that the number of training examples @math is sufficiently large (see Theorem 1 and Theorem 2 in @cite ).
- To solve the above min-max formulation, @cite proposed stochastic primal-dual algorithms based on the stochastic mirror prox methods proposed in for addressing convex-concave problems. When the loss function @math is non-convex (e.g., the hypothesis class is defined by deep neural networks), the resulting min-max problem is non-convex in terms of @math and but is concave in terms of @math . Recently, @cite proposed new stochastic algorithms for solving the non-convex concave min-max problem when the objective function is weakly convex with respect to the minimization variable given the maximization variable. They proved the convergence to a nearly stationary point of the minimization objective function. However, the stochastic algorithms proposed in are not scalable due to updating and maintaining of the dual variable @math .
- Finding feasible control constraints that can be translated to a set of state constraints has been of particular interest both in the controls and machine learning communities. Early work includes the study of artificial potential functions in the context of obstacle avoidance, and the construction of so-called navigation functions was studied in @cite . Alternatively, if there exists a control Lyapunov function @cite , one can stabilize the agent while keeping it inside a level set of the function. Control Lyapunov functions can be learned through demonstrations @cite , for example, and Lyapunov stability was also used in the safe reinforcement learning (see @cite @cite @cite for example). As inverse optimality @cite dictates that finding a stabilizing policy is equivalent to finding an optimal policy in terms of some cost function, these approaches can also be viewed as optimization-based techniques.
- On the other hand, control barrier functions (CBFs) @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite were proposed to guarantee that an agent remains in a certain region of the state space (i.e., forward invariance @cite ) by using a locally accurate model of the agent dynamics (i.e., a model that accurately predicts a time derivative of the state at the current state and control input). When the system is linearizable and has a high relative degree, an exponential control barrier function @cite was proposed and was applied to control of quadrotors @cite . When a Lyapunov function is available, the work @cite proposed a sum-of-squares approach to compute a valid barrier function. The idea of constraints-driven controls is in stark contrast to solving the task-specific problem that basically aims at singling out one optimal trajectory. However, although there exist converse theorems for safety and barrier functions which claim that a forward invariant set has a barrier function under certain conditions @cite @cite @cite , finding such a set without assuming stability of the system is difficult in general (see @cite for the conditions that a candidate barrier function can be a valid one).
- Moreover, our work is also related to safe reinforcement learning, such as Lyapunov-based safe learning (cf. @cite @cite ) and constrained Markov decision processes (CMDPs) (cf. @cite @cite ). The former is based on the fact that sublevel sets of a control Lyapunov function are forward invariant, and considers stability as safety. The latter is aimed at selecting an optimal policy that satisfies constraints. Note these approaches are designed for one specific task. Our work, on the other hand, does not require stability, and can consider an arbitrarily shaped set of safe states.
- Besides, transfer learning (cf. @cite ) aims at learning a new task by utilizing the knowledge already acquired via learning other tasks, and is sometimes referred to as "lifelong learning" @cite and "learning to learn" @cite . In transfer learning for reinforcement learning contexts, we first learn a set of source tasks , and speed up learning of a target task (see @cite for example). When the source tasks and the target task have hierarchical structures, it is often called hierarchical reinforcement learning (e.g., @cite @cite @cite ). Other examples include meta-learning (e.g., @cite ) that considers so-called the task distribution. Our work can also be used as a transfer learning technique that uses a set of good enough policies as useful information shared among other tasks.
- Humans are capable of perceiving 3D environment and inferring ego-motion in a short time, but it is hard for an agent to be equipped with similar capabilities. VO SLAM has been considered as a multi-view geometric problem for decades. It is traditionally solved by minimizing photometric @cite or geometric @cite reprojection errors and works well in regular environments, but fails in challenging conditions like dynamic objects and abrupt motions. In light of these limitations, VO has been studied with learning techniques in recent years and many approaches with promising performance have been proposed.
- Supervised methods formulate VO as a supervised learning problem and many methods with good results have been proposed. DeMoN @cite jointly estimates pose and depth in an end-to-end manner. Inspired by the practice of parallel tracking and mapping in classic VO SLAM, DeepTAM @cite utilizes two networks for pose and depth estimation. DeepVO @cite treats VO as a sequence-to-sequence learning problem by estimating poses recurrently. The limitation of supervised learning is that it requires a large amount of labeled data. The acquisition of ground truth often requires expensive equipment or highly manual labeling, and some gathered data are inaccurate. Depth obtained by LIDAR is sparse, and the output depth of Kinect contains a lot of noise. Furthermore, some ground truth is unable to obtain ( optical flow). Previous works have tried to address these problems with synthetic datasets @cite , but there is always a gap between synthetic and real-world data.
- Self-supervised methods In order to alleviate the reliance on ground truth, recently many self-supervised methods have been proposed for VO. The key to self-supervised learning is to find the internal correlations and constraints in the training data. SfMLearner @cite leverages the geometric correlation of depth and pose to learn both of them in a coupled way, with a learned mask to mask out regions that don't meet static scene assumption. As the first self-supervised approach for VO, SfMLearner couples depth and pose estimations with image warping, which becomes the problem of minimizing photometric loss. Inherited from this idea, many self-supervised VO have been proposed, including modifications on loss functions @cite @cite , network architectures @cite @cite @cite @cite @cite , predicted contents @cite , and combination with classic VO SLAM @cite @cite . For example, GeoNet @cite extends the framework to jointly estimate optical flow with forward-backward consistency to infer unstable regions and achieves state-of-the-art performance among self-supervised VO methods.
- Despite its feasibility, self-supervised VO still underperforms supervised ones. Apart from the effectiveness of direct supervision, a key reason is that they focus mainly on geometric properties @cite but pay little attention to the sequential nature of the problem. In these methods, only a few frames (no more than 5) are processed in the network, while previous estimations are discarded and the current estimation is made from scratch. Instead, the performance can be enhanced by taking geometric relations of sequential observations into account.
- In recent years, neural information retrieval and neural question answering research has developed several effective ways to improve ranking accuracy. Interaction-based neural rankers match query and document pair using attention-based deep model; representation-based neural rankers output sentence representations and using cosine distance to score the sentence pairs. There are many effective representation-based model include DSSM @cite , CLSM @cite and LSTM-RNN @cite and many effective interaction-based model include DRMM @cite Match-SRNN @cite and BERT @cite . Our deep model belongs to the representation-based models which could output the final semantic representation vector for each sentence.
- Sentence embeddings is an important topic in this research area. Skip-Thought @cite input one sentence to predict its previous and next sentence. InferSent @cite outperforms Skip-Thought. @cite is the methods that use unsupervised word vectors @cite to construct the sentence vectors which is a strong baseline. Universal Sentence Encoder @cite present two models for producing sentence embeddings that demonstrate good transfer to a number of other of other NLP tasks.
- Wolfgang @cite proposes random competition, in which the computations compete using the randomness in search algorithm. Although he analyzes speedups based on the variance of the measured execution times, there is no mention of CV.
- Without enough attention to the degree of the variance of the execution time among processors, using naively wastes computing resources. To overcome this problem, Cledat @cite @cite @cite proposes the methods called and . The CV of WalkSAT, one of the application they adopted for evaluation, is less than one and the speedup is worse than a linear speedup. Meanwhile, the CV of another application, namely, MSL motion planning is greater than one and a superlinear speedup is achieved. These results are consistent with our result. Therefore, it is proper to claim that our results reinforce and extend their work.
- Cybersecurity becomes a critical issue due to the large-scale deployment of smart devices and their integration with information and communication techologies (ICTs) @cite @cite . Hence, security risk management is an important task which has been investigated in different research fields, such as communications and infrastructures @cite @cite , cloud computing @cite and IoT @cite . The interconnections between nodes and devices make the risk management a challenge problem as the cyber risk can propogate and escalate into systemic risk @cite , and hence the interdependent security risk analysis is necessary @cite . Managing systemic risk is nontrivial as demonstrated in financial systems @cite , critical infrastructures @cite , and communication networks @cite . In a network with a small number of agents, graph-theoretic methods have been widely adopted to model the strategic interactions and risk interdependencies between agents @cite @cite . When the number of nodes becomes large, @cite has proposed a mean-field game approach where a representative agent captures the system dynamics. Different from @cite @cite in minimizing the static systemic risk at equilibrium, we focus in this paper on a mechanism design problem that can reduce the systemic risks by understanding the system dynamics.
- Natural language generation techniques have been widely popular for synthesizing unique pieces of textual content. NLG techniques proposed by @cite @cite rely on templates pre-constructed for specific purposes. The fake email generation system in @cite uses a set of manually constructed rules to pre-define the structure of the fake emails. Recent advancements in deep learning networks have paved the pathway for generating creative as well as objective textual content with the right amount of text data for training. RNN-based language models have been widely used to generate a wide range of genres like poetry @cite @cite , fake reviews @cite , tweets @cite , geographical information @cite and many more.
- The system used for synthesizing emails in this work is somewhat aligned along the lines of the methodology described in @cite @cite . However, our proposed system has no manual labor involved and with some level of post processing has been shown to deceive an automated supervised classification system.
- In this paper, we focus primarily on generation of fake emails specifically engineered for phishing and scamming victims. Additionally, we also look at some state-of-the-art phishing email detection systems. Researchers in @cite extract a large number of text body, URL and HTML features from emails, which are then fed into supervised (SVMs, Neural Networks) as well as unsupervised (K-Means clustering) algorithms for the final verdict on the email nature. The system proposed in @cite extracts 25 stylistic and structural features from emails, which are given to a supervised SVM for analysis of email nature. Newer techniques for phishing email detection based on textual content analysis have been proposed in @cite @cite @cite @cite . Masquerade attacks are generated by the system proposed in @cite , which tunes the generated emails based on legitimate content and style of a famous personality. Moreover, this technique can be exploited by phishers for launching email masquerade attacks, therefore making such a system extremely dangerous.
- The proliferation of Low Power Wide Area Networks (LPWAN), such as Sigfox and LoRaWAN, has brought a new domain of application of the fingerprinting methods. A recent study @cite has experimentally verified the intuitive assumption that fingerprinting methods outperform, in terms of accuracy, proximity or ranging positioning methods, in a Sigfox setting.
- Researchers study the TDMA optimization in neuron-based MC, which employs neurons to communicate and built in-body sensor-actuator networks (IBSANs) @cite . They use an evolutionary multi-objective optimization algorithm to design the TDMA schedule. The resource allocation in MC has already studied for two transmitter nodes in @cite where the authors propose a game-theoretic framework and study Bit Error Rate (BER) of such a system. In addition, the investigation of the channel capacity for multiple-access channels, which employs the principles of natural ligand-receptor is studied in @cite . Furthermore, the researchers have found a high capacity in Single-input Single-output (SISO) and Multi-input Single-output (MISO)-based MC system @cite . The investigation of more than two transmitter nodes in multiple access channel in existing works has not been considered yet. In addition, TDMA in Molecular Communication via Diffusion (MCvD) system has not been studied in the existing works on MC. The optimization of symbol durations and the number of released molecules by each transmitter node is also not considered in the existing works.
- Recent years, great efforts have been made on extracting relational fact from unstructured raw texts to build large structural knowledge bases. A relational fact is often represented as a triplet which consists of two entities (subject and object) and semantic relation between them. Early works @cite @cite @cite mainly focused on the task of relation classification which assumes the entity pair are identified beforehand. This limits their practical application since they neglect the extraction of entities. To extract both entities and their relation, existing methods can be divided into two categories : the pipelined framework, which first uses sequence labeling models to extract entities, and then uses relation classification models to identify the relation between each entity pair; and the joint approach, which combines the entity model and the relation model through different strategies, such as constraints or parameters sharing. * 2mm
- The work of @cite introduced ShuffleWatcher", a MapReduce scheduler that reduces throughput and job completion time. The scheme replicates Map tasks and delays or elongates a job's communication time depending on the network load. Their technique also judiciously assigns Reduce tasks to workers based on the Map assignment. Other related work on this topic has been published in @cite which considers a model of MapReduce executed on a multi-core machine and proposes a topology-aware architecture to expedite data shuffling. The authors of @cite present an algorithm that finds the optimal placement and jointly optimizes Map and Shuffle time.
- The recent work of @cite introduces a scheme to handle the case when each Reduce function is computed by @math workers by utilizing a hybercube structure which controls the allocation of Map and Reduce tasks. Their work is motivated by distributed applications that require multiple rounds of Map and Reduce computations, where the Reduce results of the previous round serve as the inputs to the Map functions of the next one.
- Another approach that re-examines the computation - communication tradeoff from an alternate viewpoint has been investigated in @cite . In this case, the assumption is that a server does not need to process all locally available files and storage constraints do not necessarily imply computation constraints. A lower bound on the was derived along with a heuristic scheme to achieve it in some cases.
- In @cite , the authors propose a scheme which gives each server access to a random subset of the input files and not all Reduce functions depend on the entire data set. This fact changes the policy according to which we decide which server computes which function.
- As discussed above both @cite and @cite require a certain problem dimension to be very large. In particular, @cite considers a single job and requires it to be split into a number of tasks that grows exponentially in the problem parameters. On the other hand @cite considers functions that can be aggregated but requires the number of jobs being processed simultaneously to grow exponentially. Our work builds on the initial work in @cite and @cite and makes the following contributions.
- Scene text is regarded as a special type of object, several methods @cite @cite @cite @cite @cite @cite are based on Faster R-CNN @cite , SSD @cite and DenseBox @cite , which generates text bounding boxes by regressing coordinates of boxes directly. TextBoxes @cite and RRD @cite adopt SSD as a base detector and adjust the anchor ratios and convolution kernel size to handle variation of aspect ratios of text instances. @cite and EAST @cite perform direct regression to determine vertex coordinates of quadrilateral text boundaries in a per-pixel manner without using anchors and proposals, and conduct the Non-Max Suppression (NMS) to get the final detection results. RRPN @cite generates inclined proposals with text orientation angle information and propose Rotation Region-of-Interest (RRoI) pooling layer to detect arbitrary-oriented text. Limited by the receptive field of CNNs and the relatively simple representations like rectangle bounding box or quadrangle adopted to describe text, detection-based methods may fall short when dealing with more challenging text instances, such as extremely long text and arbitrarily-shaped text.
- Instance segmentation is a challenging task, which involves both segmentation and classification tasks. The most recent and successful two-stage representative is Mask R-CNN @cite , which achieves amazing results on public benchmarks, but requires relatively long execution time due to the per-proposal computation and its deep stem network. Other frameworks rely mostly on pixel-features generated by a single FCN forward pass, and employ post-processing like graphical models, template matching, or pixel embedding to cluster pixels belonging to the same instance. More specifically, Non-local Networks @cite utilizes a self-attention @cite mechanism to enable a pixel-feature to perceive features from all the other positions, while the CCNet @cite harvests the contextual information from all pixels more efficiently by stacking two criss-cross attention modules, which augments the feature representation a lot. In post-processing step, @cite present a pixel affinity scheme and cluster pixels into instances with a simple yet effective graph merge algorithm. Instance-Cut @cite and the work of @cite predict object boundaries intentionally to facilitate the separation of object instances.
- This paper continues the trend towards rectifying the substantial discrepancy'' @cite between early cyber insurance models and informal claims about the insurance market. Early research considered factors relevant to the viability of a market. Interdependent security occurs when the risk depends on the actions of others'' @cite @cite . Optimists argued that insurers could coordinate the resulting collective action problem @cite @cite , leading to a net social welfare gain and a viable market. Skeptics instead focused on the high correlation in failure of information systems'' @cite @cite @cite , citing it as a major impediment to the supply of cyber insurance. Recent empirical work @cite analyzing 180 cyber insurance filings shows that the cyber insurance market is viable.
- The timing of the insurer's intervention plays is an important strategic aspect. Ex-ante interventions for the insurer include risk assessments and security investments before the policy term begins. @cite investigated an insurer who could assess security levels perfectly or not at all, concluding that the latter cannot support a functioning market. @cite showed that ex-ante assessments in combination with discounts for adopting security controls can lead to an increase in social welfare. A more recent model introduces stochastic uncertainty about the policyholder's security level @cite .
- The literature on economic theory of insurance fraud has developed two main approaches: and @cite . The costly state falsification approach assesses the client's behaviour towards a claim. We consider the costly state verification approach, which focuses on the insurer identifying fraudulent claims. The insurer can verify the claims via auditing but has to bear a verification cost. The optimal claim handling usually involves random auditing @cite .
- Our contribution to the literature is the first theoretical consideration of post-incident claims management. Our model captures the trade-off between the incentive to exaggerate security posture to receive a premium discount and the possibility of punishment for non-compliance with the reported security policies. We consider misrepresenting security posture a strategic choice for the insured and allow the insurer to respond by auditing claims. Not allowing the insurer to do so leads to market collapse @cite .
- As far as the CSA is concerned, this component can be easily built from the BWT using small space as it is formed (in its simplest design) by just a BWT with rank select functionality enhanced with a suffix array sampling, see also @cite .
- We are aware of only one work building the LCP array in small space from the BWT: @cite show how to build the LCP array in @math time and @math bits of working space on top of the input BWT and the output. Other works @cite @cite show how to build the LCP array directly from the text in @math time and @math bits of space (compact).
- K "a rkk "a @cite show that the PLCP bitvector can be built in @math time using @math bits of working space on top of the text, the suffix array, and the output PLCP. Kasai at al.'s lemma also stands at the basis of a more space-efficient algorithm from V " a lim " a @cite , which computes the PLCP from a CSA in @math time using constant working space on top of the CSA and the output. Belazzougui @cite recently presented an algorithm for building the PLCP bitvector from the text in optimal @math time and compact space ( @math bits).
- The remaining component required to build a compressed suffix tree (in the version described by Sadakane @cite ) is the suffix tree topology, represented either in BPS @cite (balanced parentheses) or DFUDS @cite (depth first unary degree sequence), using @math bits. As far as the BPS representation is concerned, @cite show how to build it from a CSA in @math time and compact space for any constant @math . Belazzougui @cite improves this running time to the optimal @math , still working within compact space. V " a lim " a @cite describe a linear-time algorithm that improves the space to @math bits on top of the LCP array (which however needs to be represented in plain form), while @cite show how to build the DFUDS representation of the suffix tree topology in @math time using @math bits of working space on top of a structure supporting access to LCP array values in @math time.
- In this paper, we give new space-time trade-offs that allow building the CST's components in smaller working space (and in some cases even faster) with respect to the existing solutions. We start by combining 's algorithm @cite with the suffix-tree enumeration procedure of Belazzougui @cite to obtain an algorithm that enumerates (i) all pairs @math , and (ii) all suffix tree intervals in @math time using just @math bits of working space on top of the input BWT. We use this procedure to obtain algorithms that build (working space is on top of the input BWT and the output):
- Also contribution ) improves the state-of-the-art, due to @cite @cite . In those papers, the authors show how to merge the BWTs of two texts @math and obtain the BWT of the collection @math in @math time and @math bits of working space for any @math [Thm. 7] belazzougui2016linear . When @math , this running time is the same as our result ), but the working space is much higher on small alphabets.
- In real-world applications like search engines and recommendation systems, systems provide ranked lists tailored to users and their queries @cite @cite @cite . In some cases, mapping those preferences into an ordinal variable leads to better user experience. Such tasks require the use of regression and multi-class classification methods @cite .
- Regression problems are known to suffer from under-predicting rare instances @cite . Approaches proposed to correct fitting models consider prior correction that introduces terms capturing a fraction of rare events in the observations and weighting the data to compensate for differences @cite @cite . Hsu and Sabato @cite proposed a methodology for linear regression with possibly heavy-tailed responses. They split data into multiple pieces, repeat the estimation process several times, and select the estimators based on their performance. They analytically prove that their method can perform reasonably well on heavy-tailed datasets. Quantile regression related approaches are proposed as well. Wang @cite proposed estimating the intermediate conditional quantiles using conventional quantile regression and extrapolating these estimates to capture the behavior at the tail of the distribution. Robust Regression for Asymmetric Tails (RRAT) @cite was proposed to address the problem of asymmetric noise distribution by using conditional quantile estimators. Zhang and Zhou @cite considered linear regression with heavy-tail distributions and showed that using @math loss with truncated minimization can have advantages over @math loss. Like all truncated based approaches, their method requires prior knowledge of distributional properties. None of these regression techniques can capture non-linear decision boundaries.
- In literature, efficient methodologies were proposed to learn pairwise relations more efficiently than comparing all @math pairs exhaustively. Qian proposed using two-step hashing framework to retrieve relevant instance and nominate pairs whose ranking is uncertain @cite . Similar approaches to efficiently search similar pairs and approximately learning pairwise distance are proposed in the literature for information retrieval and image search @cite @cite @cite .
- Recently, neural networks trained with a ranking loss considering image pairs @cite , triplets @cite , quadruplets @cite or beyond @cite , have been considered for metric learning @cite @cite and for a broad range of search tasks such as face person identification @cite @cite @cite @cite or instance retrieval @cite @cite . These learning-to-rank approaches have been generalised to two or more modalities. Standard examples include building a joint embedding for images and text @cite @cite , videos and audio @cite and, more related to our work, for videos and action labels @cite , videos and text @cite @cite @cite or some of those combined @cite @cite @cite .
- Representing text Early works in image-to-text cross-modal retrieval @cite @cite @cite used TF-IDF as a weighted bag-of-words model for text representations (either from a word embedding model or one-hot vectors) in order to aggregate variable length text captions into a single fixed sized representation. With the advent of neural networks, works shifted to use RNNs, Gated Recurrent Units (GRU) or Long Short-Term Memory (LSTM) units to extract textual features @cite or to use these models within the embedding network @cite @cite @cite @cite @cite for both modalities.
- Hahn al @cite use two LSTMs to directly project videos into the Word2Vec embedding space. This method is evaluated on higher-level activities, showing that such a visual embedding aligns well with the learned space of Word2Vec to perform zero-shot recognition of these coarser-grained classes. Miech al @cite found that using NetVLAD @cite results in an increase in accuracy over GRUs or LSTMs for aggregation of both visual and text features. A follow up on this work @cite learns a mixture of experts embedding from multiple modalities such as appearance, motion, audio or face features. It learns a single output embedding which is the weighted similarity between the different implicit visual-text embeddings. Recently, Miech al @cite propose the HowTo100M dataset: A large dataset collected automatically using generated captions from youtube of how to tasks'. They find that fine-tuning on these weakly-paired video clips allows for state-of-the-art performance on a number of different datasets.
- Fine-grained action recognition Recently, several large-scale datasets have been published for the task of fine-grained action recognition @cite @cite @cite @cite @cite . These generally focus on a closed vocabulary of class labels describing short and or specific actions.
- Rohrbach al @cite investigate hand and pose estimation techniques for fine-grained activity recognition. By compositing separate actions, and treating them as attributes, they can predict unseen activities via novel combinations of seen actions. Mahdisoltani al @cite train for four different tasks, including both coarse and fine grain action recognition. They conclude that training on fine-grain labels allows for better learning of features for coarse-grain tasks.
- There are several studies that investigate the structure of mass loss in V1309 Scorpii through computer simulation. One approach to modeling this system is smoothed-particle hydrodynamics (SPH). Notable SPH applications include StarSmasher @cite @cite (a fork of StarCrash @cite ) and an unpublished code developed by a collaboration of researchers from Princeton University, Columbia University, and Osaka University @cite @cite . An alternative approach is to use the finite volume method to simulate mass transfer. Examples of such applications include Athena @cite @cite and its rewrite named Athena++ @cite @cite @cite . Lastly, Enzo @cite is a project that implements finite volume hydrodynamics along with a collisionless N-body module that can be used to simulate binary systems where one component is taken to be a point mass. With the exception of SPH codes using direct summation for gravity, is unique among three-dimensional self-gravitating hydrodynamics codes in that it simultaneously conserves both linear and angular momentum to machine precision. SPH codes using direct summation for gravity are limited to only a few thousand particles, making the better choice for high resolution simulations.
- Adaptive multithreading systems such as HPX expose concurrency by using user-level threads. Some other notable solutions that take such an approach are Uintah @cite , Chapel @cite , Charm++ @cite , Kokkos @cite , Legion @cite , and PaRSEC @cite . Note that we only refer to distributed memory capable solutions, since we focus here on large distributed simulations. Different task-based parallel programming models, e.g. Cilk Plus, OpenMP, Intel TBB, Qthreads, StarPU, GASPI, Chapel, Charm++, and HPX, are compared in @cite . Our requirements (distributed, task-based, asynchronous) are met by few, out of which HPX has the highest technology readiness level according to this review. It is furthermore the only one with a future-proof C++ standard conforming API and allows us to support the libfabric networking library without changing application code. For more details, see Sec. .
- There are several particle-based FMM implementations utilizing task-based programming available. The approach described in @cite uses the Quark runtime environment @cite , the implementation in @cite @cite uses StarPu @cite , whilst @cite uses OpenMP @cite , and @cite compares Cilk @cite , HPX-5, and OpenMP tasks @cite . Our choice of HPX for the task-based runtime system is motivated by the same findings as the above mentioned review and the need to implement specialized kernels for energy conservation that require coupling between different parts of the solver.
- . Adversarial training schemes have been extensively employed in the literature to impose structural consistencies for semantic segmentation @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . @cite incorporate a discriminator network trained to distinguish the real labels and network-produced predictions. Involving the segmenter in a minimax game with the discriminator motivates the network to bridge the gap between the two distributions and consequently having higher-level consistencies in predicted labels.
- @cite also discuss the value-based discrimination issue, which they attempt to alleviate by feeding the discriminator with a Cartesian product of the prediction maps and the input image channels. However, their followed strategy resulted in no improvements as reported. This can be attributed to remaining value-based evidence based on values distribution granularity. For instance, a very tiny response to a first-layer edge detector, in this case, can already signify a fake data sample.
- is the technique of injecting adverserial examples and the corresponding gold standard labels into the training set @cite @cite @cite . The motivation of this methodology is that the network will learn the adverserial perturbations introduced by the attacker. The problem with adverserial training is that it doubles the training time of the classifier as new examples need to be generated. Moreover, as shown by , adversarial training needs all types of adverserial examples produced by all known attacks, as the training process is non-adaptive @cite . Our method can be paired with any of these types of defenses.
- @cite proposes a novel estimator for estimating the link probability matrix @math of an undirected network by neighborhood smoothing (NBS). The essential idea consists of the following: Given an adjacent matrix @math , the link probability @math between node @math and @math is estimated by where @math is a certain set of neighboring nodes of node @math , which consists of the nodes that exhibit similar connection patterns as node @math . With a well-designed neighborhood adaptive to the network structure, the smoothing achieves an accurate estimation for @math . NBS in @cite estimates @math with a single adjacency matrix @math . For a dynamic network, a sequence of adjacency matrices @math is available, which provides extra information of the network. By aggregating information from repeated observations across time, in Section , we propose a modified NBS by carefully shrinking the neighborhood size, which yields a better convergence rate in estimating the link probability matrix @math and thus an improved rate in change-point detection.
- Another related area of research is anomaly detection in dynamic networks, where the task is to detect short abrupt deviation of the network behavior from its norm. This is not the focus of our paper and we refer the readers to @cite for a comprehensive survey.
- Inflating convolutional layers to 3D for video tasks was first explored in @cite , in which the authors chose to optimise an architecture for the video task, rather than adapt one from an image problem. Both @cite and @cite have adapted large image classification models (Inception and ResNet respectively) to activity recognition tasks, such as @cite @cite . Aside from the added dimensionality, these architectures are much the same as in image tasks, and intuitively find similar success in the spatio-temporal domain as they do in the spatial domain, achieving state-of-the-art performance. These models are as complex and black-box in nature as their 2D counterparts and as such the motivation to explain them also translates.
- A variety of approaches have been attempted for explaining decisions made by deep neural networks. For example, in @cite the authors propose feature visualisation for CNNs, in which the input images are optimised to maximally activate each filter in the CNN convolutional layers, following work in @cite on non-convolutional models. Local explanations, in the sense that they are local to a single input, explain the inputs contribution to the model decision using feature attribution; these have found much success in explaining deep image processing models. These methods in some way approximate the contribution to the models decision, most commonly in a supervised task, to its input variables, pixels or features at a higher level. This has been implemented in a number of ways, for example, through use of probability gradients @cite , global average pooling @cite and its generalisation to networks with hidden layers in @cite , or through local relevance based around a decision-neutral root point @cite @cite @cite . These works are all considered in that they use information from the model internal parameters, i.e., its weights and activations, in generating an explanation.
- Layer-wise relevance propagation (LRP) rules, as defined in @cite , have found moderate success in explaining image recognition tasks. Multiple implementations and improvements have been made to these rules, with marginal winning probability (MWP) @cite , to our knowledge being the first implementation of the rules. Deep Taylor decomposition, an implementation of LRP by the original authors themselves has become very popular, and as a result of its input-domain agnosticism, has been applied to other domains outside of image recognition, including activity recognition @cite . It is for these reasons we choose the deep Taylor method as the exemplar technique for our proposed method
- In addition to MWP, the authors in @cite also show that removing relevance for the dual of the signal improves the focus of the explanation. This contrastive MWP (cMWP) effectively removes relevance to all classes, by explaining all other outputs at the second logits layer, leaving only relevance contributing to the chosen output neuron. Our method is similar to cMWP, in that we make use of subtraction of separate LRP signals to remove unwanted relevance. However, we backpropagate both signals through the network fully before subtracting. Where the cMWP method removes relevance towards all classes from the explanation, our method removes relevance towards spatially salient features in the frame, such as edges and background objects.
- Work on explainability methods outside of image tasks is still developing. Papers such as @cite use feature visualisation techniques to provide insight into the models they have trained, but to our knowledge @cite is still one of the only instances of an LRP based method applied to a video task. In this work, the difference between frames in relevance is highlighted by flattening the explanation block and plotting the overall relevance, which shows frames at certain points in an activity are more relevant overall. Saliency tubes, as proposed in @cite , adapts the CAM technique of @cite @cite to localise salient motion in video frames. This method is the most similar to ours in that it highlights motion in 3D CNNs.
- In addition to the VAE-based methods for control over music generation processes mentioned above, a number of other studies have applied deep learning methods to address the problem of music generation in general, as reviewed in @cite . Drum track generation has been tackled using recurrent architectures @cite @cite , Restricted Boltzmann Machines @cite , and Generative Adversarial Networks (GANs) @cite . Approaches to the generation process may rely on sampling from some latent representation of the material to be generated @cite @cite , possibly in an incremental fashion @cite , or conditioning on user-provided information (such as a style label @cite , unary @cite , or structural @cite constraints). @cite demonstrates style transfer for audio. GANs are used in @cite @cite , where the output of the generation process is determined by providing some (time-varying) noise, in combination with conditioning on existing material. Similar to our study, @cite uses a GAE to model between musical material in an autoregressive prediction task. To our knowledge this is the first use of GAEs for conditional music generation.
- Recently, a number of approaches have been proposed to adapt a DNN model to the continual learning setting, from an adaptive model architecture perspective such as adding columns or neurons for new tasks @cite @cite @cite ; model parameter adjustment or regularization techniques like, imposing restrictions on parameter updates @cite @cite @cite @cite ; memory revisit techniques which ensure model updates towards the optimal directions @cite @cite @cite ; Bayesian approaches to model continuously acquired information @cite @cite @cite ; or on broader domains with approaches targeted at different setups or goals such as few-shot learning or transfer learning @cite @cite .
- In order to demonstrate our idea in comparison with the state-of-the-art techniques, we briefly discuss the following three popular approaches to continual learning: I) : It constrains or regularizes the model parameters by adding additional terms in the loss function that prevent the model from deviating significantly from the parameters important to earlier tasks. Typical algorithms include elastic weight consolidation (EWC) @cite and continual learning through synaptic intelligence (SI) @cite . II) : It revises the model structure successively after each task in order to provide more memory and additional free parameters in the model for new task input. Recent examples in this direction are progressive neural networks @cite and dynamically expanding networks @cite . III) : It stores data samples from previous tasks in a separate memory buffer and retrains the new model based on both the new task input and the memory buffer. Popular algorithms here are gradient episodic memory (GEM) @cite , incremental classifier and representation learning (iCaRL) @cite .
- @PARASPLIT Note that our dataset is very different from other popular large-scale 3D datasets, such as NYU v2 @cite , SUN RGB-D @cite , 2D-3D-S @cite @cite , ScanNet @cite , and Matterport3D @cite , in which the ground truth 3D information is stored in the format of point clouds or meshes. These datasets lack ground truth annotations of semi-global or global structures. While it is theoretically possible to extract 3D structure by applying structure detection algorithms to the point clouds or meshes ( , extracting planes from ScanNet as did in @cite ), the detection results are often noisy and even contain errors. In addition, for some types of structure like wireframes and room layouts, how to reliably detect them from raw sensor data remains an active research topic in computer vision.
- In recent years, synthetic datasets have played an important role in successful training of deep neural networks. Notable examples for indoor scene understanding include SUNCG @cite , SceneNet RGB-D @cite , and InteriorNet @cite . These datasets exceed real datasets in terms of scene diversity and frame numbers. But just like their real counterparts, these datasets lack ground truth structure annotations. Another issue with some synthetic datasets is the degree of realism in both the 3D models and the 2D renderings. @cite shows that physically-based rendering could boost the performance of various indoor scene understanding tasks. To ensure the quality of our dataset, we make use of 3D room models created by professional designers and the state-of-the-art industrial rendering engines in this work.
- Room layout estimation. Room layout estimation aims to reconstruct the enclosing structure of the indoor scene, consisting of walls, floor, and ceiling. Existing public datasets ( , PanoContext @cite and LayoutNet @cite ) assume a simple cuboid-shape layout. PanoContext @cite collects about 500 panoramas from the SUN360 dataset @cite , LayoutNet @cite extends the layout annotations to include panoramas from 2D-3D-S @cite . Recently, Realtor360 @cite collects 2,500 indoor panoramas from SUN360 @cite and a real-estate database, and provides annotation of a more general Manhattan layout. We note that all room layout in these real datasets is manually labeled by the human. Since the room structure may be occluded by furniture and other objects, the ground truth'' inferred by humans may be not consistent with the actual layout. In our dataset, all ground truth 3D annotations are automatically extracted from the original house design files.
- The available approaches can handle the control of the flying robot when it does not engage with an interaction. However, the challenges associated with the aerodynamic interaction require the system to be more responsive, adaptive and resilient @cite @cite @cite @cite . This operation also brings system and environment based constraints including the level of the interaction. The available approaches that consider the constraints leverage individual multi-models for generic interaction problems which bring additional complexity @cite . Moreover, nominal optimization-based approaches are considered in the UAV control for the interaction tasks, wherein the system lacks the ability to take external forces, changing parameters and unmodeled dynamics into account @cite @cite @cite .
- Research found that especially non-technical end-users are more likely to express their opinions on social networks, such as Twitter @cite . Several studies have identified Twitter as an important source for crowd-based requirements engineering and software evolution @cite @cite @cite . Similar to app reviews, tweets contain important information, such as feature requests or bug reports. By performing a survey with software engineering practitioners and researchers @cite underlined the need for automatic analysis techniques to, e.g., summarize, classify, and prioritize tweets. The authors highlight that a manual analysis of the tweets is unfeasible due to its quantity, unstructured nature, and varying quality. @cite found that tweets provide additional requirements-related information. Compared to app reviews, by mining tweets the authors extracted @math 22 authors have used tweets to crowdsource app features @cite , to support release decisions @cite , to categorize and summarize technical information included in tweets @cite , or to rank the reported issues @cite . These studies enforce the relevance of our approach.
- Smart cyber-physical systems (SCPS) have become an important part of the IT landscape. Often these systems include IoT devices that allow effective and easy acquisition of data in areas such as healthcare, smart cities, smart vehicles, and smart homes @cite . Data mining and analysis are among the primary goals of collecting data from SCPS. The infrastructural extensions of SCPSs have contributed to the exponential growth in the number of IoT sensors, but security is often overlooked, and the devices become a source of privacy leak. The security and privacy concerns of big data and data streams are not entirely new, but require constant attention due to technological advancements of the environments and the devices used @cite . Confidentiality, authentication, and authorization are just a few of the concerns @cite @cite @cite . Many studies have raised the importance of privacy and security of SCPS due to their heavy use of personally identifiable information (PII) @cite . Controlling access via authentication @cite , attribute-based encryption @cite , temporal and location-based access control @cite and employing constraint-based protocols @cite are some examples of improving privacy of SCPS.
- Among these existing works, PTGAN @cite and SPGAN @cite transfer source images into target-domain style by CycleGAN and then use translated images to train a model. However, due to unable to guarantee the identity of generated images, these style transfer learning methods can not result in satisfactory performance. Another line of unsupervised cross-domain person Re-ID works @cite @cite @cite @cite combine other auxiliary information as an assistant task to improve the model generalization. For instance, TFusion @cite integrates spatio-temporal patterns to improve the Re-ID precision, while EANet @cite uses pose segmentation. TJ-AIDL @cite learns an attribute-semantic and identity discriminative feature representation space simultaneously, which can be transferred to any new target domain for re-id tasks. Similar as the difficulty of supervised learning, these domain adaptation approaches suffer from the requirement of collecting attribute annotations.
- Beyond the above methods, some approaches @cite @cite @cite focus on estimating pseudo identity labels on the target domain so as to learn deep models in a supervised manner. Usually, clustering methods are used in the feature space to generate a series of clusters which are used to update networks with an embedding loss ( , triplet loss @cite or contrastive loss) @cite @cite or classification loss ( , softmax cross-entropy loss) @cite . Whereas, embedding loss functions suffer from the limitation of sub-optimal results and slow convergence, while classification loss extremely depends on the quality of pseudo labels. While the work in @cite introduces a simple domain adaptation framework which also use both triplet loss and softmax cross-entropy loss jointly, it aims at solving one-shot leaning problem.
- ReVal https: github.com rohitguptacs ReVal @cite is also a metric using sentence embeddings. ReVal trains sentence embeddings from labeled data in WMT Metrics Shared Task and semantic similarity estimation tasks, but can not achieve sufficient performance because it uses only small data. RUSE trains only regression models from labeled data using sentence embeddings pre-trained on large data such as Quick Thought @cite .
- As shown in Figure , RUSE encodes an MT hypothesis and an reference translation by a sentence encoder, respectively. Then, following InferSent @cite , a features are extracted by combining sentence embeddings of the two sentences, and the evaluation score is estimated by the regression model based on multi-layer perceptron (MLP).
- User data and their economics have long been an interesting topic and attracted a considerable body of research @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . In particular, in @cite , Acquisti al discuss the value of privacy after defining two concepts (i) : the monetary amount users are willing to pay to protect their privacy, and (ii) : the compensation that users are willing to accept for their privacy loss. In two user-studies @cite @cite authors measure how much users value their own offline and online personal data, and consequently how much they would sell them to advertisers. @cite , authors propose transactional'' privacy to allow users to decide what personal information can be released and receive compensation from selling them.
- Bashir al in @cite , study the diffusion of user tracking caused by RTB-based programmatic ad-auctions. Results of their study show that under specific assumptions, no less than 52 tracking companies can observe at least 91 an attempt to shed light upon Facebook's ad ecosystem, Andreou al in @cite investigate the level of transparency provided by the mechanisms Why am I seeing this?'' and Ad Preferences Page. The authors built a browser extension to collect Facebook ads and information extracted from these two mechanisms before performing their own ad campaigns and target users that used their browser extension. They show that ad explanations are often incomplete and misleading. @cite , the authors aim to enhance the transparency in ad ecosystem with regards to information sharing, by developing a content agnostic methodology to detect client- and server- side flows of information between ad exchanges and leveraging retargeted ads. By using crawled data, the authors collected 35.4k ad impressions and identified 4 different kinds of information sharing behavior between ad exchanges.
- On a high level, our work relates to the use of abstraction in creating effective visual representations, , the use of . Viola and Isenberg @cite describe this concept as a process, which removes detail when transitioning from a lower-level to a higher-level representation, yet which preserves the overall concept. While they attribute the removed detail to natural variation, noise, etc.'' in the investigated multi-scale representation we actually deal with a different data scenario: DNA assemblies at different levels of scale. We thus technically do not deal with a concept-preserving transformation'' @cite , but with a process in which the underlying representational concept (or parts of it) can change. Nonetheless, their view of abstraction as an interactive process that allows viewers to relate one representation (at one scale) to another one (at a different scale) is essential to our work.
- Also important from Viola and Isenberg's discussion @cite is their concept of , which are traversed in scale space. We also connect the DNA representations at different scales, facilitating a smooth transition between them. In creating this axis of abstraction, we focus primarily on changes of Viola and Isenberg's geometric axis, but without a geometric interpolation of different representations. Instead, we use visual embedding of one scale in another one.
- We investigate multi-scale representations of the DNA, which relates to work in bio-molecular visualization. Several surveys have summarized work in this field @cite @cite @cite @cite , so below we only point out selected approaches. In addition, a large body of work by professional illustrators on mesoscale cell depiction inspired us such as visualizing the human chromosome down to the detail of individual parts of the molecule @cite .
- In general, as one navigates through large-scale 3D scenes, the underlying subject matter is intrinsically complex and requires appropriate interaction to aid intellection @cite . The inspection of individual parts is challenging, in particular if the viewer is too far away to appreciate its visual details. Yet large, detailed datasets or procedural approaches are essential to create believable representations. To generate not only efficient but visualizations, we thus need to remove detail in Viola and Isenberg's @cite visual abstraction sense. This allows us to render at interactive rates as well as to see the intended structures, which would otherwise be hidden due to cluttered views. Consequently, even most single-scale small-scale representations use some type of multi-scale approach and with it introduce abstraction. Generally we can distinguish three fundamental techniques: multi-scale representations by leaving out detail of a single data source, multi-scale techniques that actively represent preserved features at different scales, and multi-scale approaches that can also transit between representations of different scales. We discuss approaches for these three categories next.
- Learning to learn or meta-learning has a long history @cite @cite @cite . With the recent successes of applying meta-learning on few-shot classification @cite @cite and reinforcement learning @cite @cite , it has regained attention. The pioneering work @cite designs an off-line learned optimizer using gradient decent and shows promising performance compared with traditional optimization methods. However, it does not generalize well for large numbers of descent step. To mitigate this problem, @cite proposes several training techniques, including parameters scaling and combination with convex functions to coordinate the learning process of the optimizer. @cite also addresses this issue by designing a hierarchical RNN architecture with dynamically adapted input and output scaling. In contrast to other works that output an increment for each parameter update, which is prone to overfitting due to different gradient scales, we instead associate an adaptive learning rate produced by a recurrent neural network with the computed gradient for fast convergence of the model update.
- In order to excavate the complexity of each instance, several works are proposed for assigning different parts of the designed network to different input data dynamically. For example, @cite @cite @cite utilized attention and gate layers to evaluate each channel and discard some of them with subtle importances during the inference phrase. @cite @cite @cite utilized a gate cell to discard some layers in pre-trained deep neural networks for efficient inference. @cite @cite @cite @cite @cite further proposed the branch selection operation to allow the learned neural networks to change themselves according to different input data. @cite @cite @cite applied the dynamic strategy on the activations of feature maps in neural networks.
- In this section, we briefly present state-of-the-art methods in multi-modal and multi-label @cite fields. As for modality extraction in multi-modal learning, it is closely related to feature extraction @cite . Therefore, we briefly review some related work on these two aspects in this section.
- Multi-label learning is a fundamental problem in machine leaning with a wide range of applications. In multi-label learning, each instance is associated with multiple interdependent labels. Binary Relevance (BR) @cite algorithm is the most simple and efficient solution of multi-label algorithms. However, the effectiveness of the resulting approaches might be suboptimal due to the ignorance of label correlations. To tackle this problem, Classifier Chains (CC) @cite was proposed as a high-order approach to consider correlations between labels. It is obviously that the performance of CC is seriously affected by the training order of labels. To account for the effect of ordering, Ensembles of Classifiers Chains (ECC) @cite is an ensemble framework of CC, which can be built with @math random permutation instead of inducing one classifier chain. Entropy Chain Classifier (ETCC) @cite extends CC by calculating the contribution between two labels using information entropy theory while Latent Dirichlet Allocation Multi-Label (LDAML) @cite exploiting global correlations among labels. LDAML mainly solve the problem of large portion of single label instance in some special multi-label datasets. Due to high dimensionality of data , dimensionality reduction @cite or feature extraction should be taken into consideration.
- In this paper, taking both multi-label learning and feature extraction into consideration, we propose MCC model with an end-to-end approach @cite for MMML problem, which is inspired by adaptive decision methods. Different from previous feature selection or dimensionality reduction methods, MCC extracts different modalities for different instances and different labels. Consequently, when presented with an unseen instance, we would extract the most informative and cost-effective modalities for it. Empirical study shows the efficiency and effectiveness of MCC, which can achieve better classification performance with less average modalities.
- Our work can be regarded as a problem of associating first-person and third-person cameras, which has been studied by many researchers. For example, @cite identify a first-person camera wearer in a third-person video by incorporating spatial and temporal information from the videos of both cameras. In @cite , information from first- and third-person cameras, together with laser range data, is fused to improve depth perception and 3D reconstruction. @cite predict gaze behavior in social scenes using both first- and third-person cameras. In @cite , first- and third-person cameras are synchronized, followed by associating subjects between their videos. In @cite , a first-person video is combined to multiple third-person videos for more reliable action recognition. The third-person cameras in these methods usually bear horizontal views or views with certain slope angle. Differently, in this paper the third-person camera is mounted on a drone and produces top-view images, making cross-view appearance matching a very difficult problem.
- As mentioned above, cross-view subject association can be treated as a person re-id problem, which has been widely studied in recent years. Most existing re-id methods can be grouped into two classes: similarity learning and representation learning. The former focuses on learning the similarity metric, e.g., the invariant feature learning based models @cite @cite @cite , classical metric learning models @cite @cite @cite , and deep metric learning models @cite @cite . The latter focuses on feature learning, including low-level visual features such as color, shape, and texture @cite @cite , and more recent CNN deep features @cite @cite . These methods assume that all the data are taken from horizontal views, with similar or different horizontal view angles, and almost all of these methods are based on appearance matching. In this paper, we attempt to re-identify subjects across top and horizontal views, where appearance matching is not an appropriate choice.
- ZSL can recognize new objects using attributes as the intermediate semantic representation. Some researchers adopt the probability-prediction strategy to transfer information. @cite proposed a popular baseline, i.e. direct attribute prediction (DAP). DAP learns probabilistic attribute classifiers using the seen data and infers the label of the unseen data by combining the results of pre-trained classifiers. Most recent works adopt the label-embedding strategy that directly learns a mapping function from the input features space to the semantic embedding space. One line of works is to learn linear compatibility functions. For example, @cite presented an attribute label embedding (ALE) model which learns a compatibility function combined with ranking loss. Romera- @cite proposed an approach that models the relationships among features, attributes and classes as a two linear layers network. Another direction is to learn nonlinear compatibility functions. @cite presented a nonlinear embedding model that augments bilinear compatibility model by incorporating latent variables. @cite proposed a first general kronecker product kernel-based learning model for ZSL tasks. In addition to the classification task, @cite proposed an attribute network for zero-shot hashing retrieval task.
- Attributes, as popular semantic representation of visual objects, can be the appearance, a part or a property of objects @cite . For example, object has the attribute and , object has the attribute . Attributes are widely used to transfer information to recognize new objects in ZSL tasks @cite @cite . Using attributes as the semantic representation, data of different categories locates in different boxes bounded by the attributes as shown in Fig. . Since the attribute representation of the seen classes and the unseen class are different, the boxes with respect to the seen data and the unseen data are disjoint.
- Deep generative models aim to estimate the joint distribution @math of samples and labels, by learning the class prior probability @math and the class-conditional density @math separately. Generative model can be extended to a conditional generative model if the generator is conditioned on some extra information, such as attributes in the proposed method. Mirza and Osindero @cite introduced a conditional version of generative adversarial nets, i.e. CGAN, which can be constructed by simply feeding the data label. CGAN is conditioned on both the generator and discriminator and can generate samples conditioned on class labels. Conditional Variational Autoencoder (CVAE) @cite , as an extension of Variational Autoencoder, is a deep conditional generative model for structured output prediction using Gaussian latent variables. We modify CVAE with the attribute representation to generate out-of-the-box data for the attribute selection.
- Some recent works based on Fully Convolution Networks (FCNs) @cite have achieved promising results on public benchmarks @cite , @cite . We then review the latest deep-learning-based methods from lightweight-oriented and accuracy-oriented aspects for scene parsing tasks.
- Early works @cite @cite apply a 3D Morphable Model and search for dense point correspondence to complete the invisible face region. @cite proposes a high fidelity pose and expression normalization approach based on 3DMM. @cite formulate the frontalization as a low rank optimization problem. @cite formulate the frontalization as a recurrent object rotation problem. @cite propose a concatenate network structure to rotate faces with image-level reconstruction constraint. @cite proposes using the identity perception feature to reconstruct normalized faces. Recently, GAN-based generative models @cite @cite @cite @cite @cite @cite have achieved high visual quality and preserve identity with large extent. Our method aligns in the GAN-based methods but works on 3D UV position and texture other than the 2D images.
- Recent image-conditioned models have shown promising results for numerous image-to-image translation tasks such as maps @math satellite images, sketches @math photos, labels @math images @cite @cite @cite , future frame prediction @cite , superresolution @cite , and inpainting @cite . Moreover, images can be stylized by disentangling the style and the content @cite @cite or by encoding styles into a stylebank (set of convolution filters) @cite . Models @cite @cite for rendering a person's appearance onto a given pose have shown to be effective for person re-identification. Label-conditioned models @cite @cite have also been explored for generating images for specific categories.
- Proposed by @cite , knowledge distillation is designed for transferring knowledge from a teacher classifier to a student classifier. The teacher classifier normally would have more privileged information @cite compared with the student classifier. The privileged information includes two aspects. The first aspect is referred to as the learning power, namely the size of the neural networks. A student classifier could have a more compact network structure compared with the teacher classifier, and by distilling knowledge from the teacher classifier to student classifier, the student classifier would have similar or even better classification performance than the teacher network. Relevant applications include network compression @cite and network training acceleration @cite . The second aspect is the learning resources, namely the amount of input data. The teacher classifier could have more learning resources and see more data that the student cannot see. Compared with the first aspect, this aspect is relatively unexplored and is the focus of our work.
- Many techniques have been recently proposed for solving continuous learning problems in computer vision @cite @cite and robotics @cite in both discriminative and generative settings.
- For discriminative settings, Shmelkov al @cite employ a distillation loss that measures the discrepancy between the output of the old and new network for distilling knowledge learnt by the old network. In addition, Castro al @cite propose to use a few exemplar images from previous tasks and perform knowledge distillation using new features from previous classification layers followed by a modified activation layer. For generative settings, continual learning has been primarily achieved using memory replay based methods. Replay was first proposed by @cite , where the images for previous tasks are generated and combined together with the data for the new task to form a joint dataset, and a new model is trained on the joint dataset. A similar idea is also adopted by @cite for label-conditioned image generation. Approaches based on elastic weight consolidation @cite have also been explored for the task of label-conditioned image generation @cite , but they have limited capability to remember previous categories and generate high quality images.
- Detection is a fundamental task in computer vision. In conventional methods, hand crafted features, e.g., HOG @cite and SIFT @cite , are used for detection either with a sliding-window strategy which holds a dense set of candidates, e.g., DPM @cite or with a region proposal method which keeps a sparse set of candidates, e.g., Selective Search @cite . Recently, since deep neural networks have shown the dominating performance in classification tasks @cite , the features obtained from neural networks are leveraged for detection tasks.
- One-stage detectors are also developed for efficiency @cite @cite @cite . Since there is no region proposal phase to sample background candidates, one-stage detectors can suffer from the imbalance issue both between classes and in the background distribution. To alleviate the challenge, SSD @cite adopts hard example mining, which only keeps the hard background candidates for training. Recently, RetinaNet @cite is proposed to address the problem by focal loss. Unlike SSD, it keeps all background candidates but re-weights them such that the hard example will be assigned with a large weight. Focal loss improves the performance of detection explicitly, but the imbalance problem in detection is still not explored sufficiently. In this work, we develop the distributional ranking loss that ranks the distributions of foreground and background. It can alleviate the imbalance issue and capture the data distribution better with a data dependent mechanism.
- Within the context of NSynth @cite , a new high-quality dataset of one shot instrumental notes was presented, largely surpassing the size of the previous datasets, containing @math musical notes with unique pitch, timbre and envelope. The sounds were collected from @math instruments from commercial sample libraries and are annotated based on their source (acoustic, electronic or synthetic), instrument family and sonic qualities. The instrument families used in the annotation are bass, brass, flute, guitar, keyboard, mallet, organ, reed, string, synth lead and vocal. The dataset is available online https: magenta.tensorflow.org datasets nsynth and provides a good basis for training and evaluating one shot instrumental sound classifiers. This dataset is already split in training, validation and test set, where the instruments present in the training set do not overlap with the ones present in validation and test sets. However, to the best of our knowledge, no methods for instrument classification have so far been evaluated on this dataset.
- The detection of solar modules in an EL image is an object detection task. Traditionally, feature-based methods have been applied to solve the task of object detection. Especially, Haar wavelets have proven to be successful @cite . For an the efficient computation, Viola and Jones @cite made use of integral images, previously known as summed area tables @cite . Integral images are also an essential part of our method.
- In the last years, convolutional neural networks (CNNs) have achieved superior performance in many computer vision tasks. For example, single-stage detectors like YOLO @cite yield good detection performance with a tolerable computational cost. Multi-stage object detectors, such as R-CNN @cite , achieve even better results but come with an increased computational cost. In contrast to CNN-based approaches, the proposed method does not require any training data and is computationally very efficient.
- There are not many preliminary works on the automated detection of solar modules. al Vetter @cite proposed an object detection pipeline that consists of several stacked filters followed by a Hough transform to detect solar modules in noisy infrared thermography measurements. Recently, al Deitsch @cite proposed a processing pipeline for solar modules that jointly detects the modules in an EL image, estimates the configuration ( the number of rows and columns of cells), estimates the lens distortion and performs segmentation into rectified cell images. Their approach consists of a preprocessing step, where a multiscale vesselness filter @cite is used to extract ridges (separating lines between cells) and bus bars. Then, parabolic curves are fitted onto the result to obtain a parametric model of the module. Finally, the distortion is estimated and module corners are extracted. Since this is, to the best of our knowledge, the only method that automatically detects solar modules and cell crossing points in EL images, we use this as a reference method to assess the performance of our approach.
- A recent work proposed reconstruction of dynamic fluids @cite for static cameras. Another work used RGB-D cameras to obtain reconstruction of non-rigid surfaces @cite . Pioneering research in general dynamic scene reconstruction from multiple handheld wide-baseline cameras @cite @cite exploited prior reconstruction of the background scene to allow dynamic foreground segmentation and reconstruction. Recent work @cite estimates shape of dynamic objects from handheld cameras exploiting GANs. However these approaches either work for static indoor scenes or exploit strong prior assumptions such as silhouette information, known background or scene structure. Also all these approaches give per frame reconstruction leading to temporally incoherent geometries. Our aim is to perform temporally coherent dense reconstruction of unknown dynamic non-rigid scenes automatically without strong priors or limitations on scene structure.
- Many of the existing multi-view reconstruction approaches rely on a two-stage sequential pipeline where foreground or background segmentation is initially performed independently with respect to each camera, and then used as input to obtain visual hull for multi-view reconstruction. The problem with this approach is that the errors introduced at the segmentation stage cannot be recovered and are propagated to the reconstruction stage reducing the final reconstruction quality. Segmentation from multiple wide-baseline views has been proposed by exploiting appearance similarity @cite @cite @cite . These approaches assume static backgrounds and different colour distributions for the foreground and background @cite @cite which limits applicability for general scenes.
- Joint segmentation and reconstruction methods incorporate estimation of segmentation or matting with reconstruction to provide a combined solution. Joint refinement avoids the propagation of errors between the two stages thereby making the solution more robust. Also, cues from segmentation and reconstruction can be combined efficiently to achieve more accurate results. The first multi-view joint estimation system was proposed by @cite which used iterative gradient descent to perform an energy minimization. A number of approaches were introduced for joint formulation in static scenes and one recent work used training data to classify the segments @cite . The focus shifted to joint segmentation and reconstruction for rigid objects in indoor and outdoor environments. These approaches used a variety of techniques such as patch-based refinement @cite @cite and fixating cameras on the object of interest @cite for reconstructing rigid objects in the scene. However, these are either limited to static scenes @cite @cite or process each frame independently thereby failing to enforce temporal consistency @cite @cite .
- An approach based on optical flow and graph cuts was shown to work well for non-rigid objects in indoor settings but requires known background segmentation to obtain silhouettes and is computationally expensive @cite . Practical application of temporally coherent joint estimation requires approaches that work on non-rigid objects for general scenes in uncontrolled environments. A quantitative evaluation of techniques for multi-view reconstruction was presented in @cite . These methods are able to produce high quality results, but rely on good initializations and strong prior assumptions with known and controlled (static) scene backgrounds.
- Temporally coherent 4D reconstruction refers to aligning the 3D surfaces of non-rigid objects over time for a dynamic sequence. This is achieved by estimating point-to-point correspondences for the 3D surfaces to obtain 4D temporally coherent reconstruction. 4D models allows to create efficient representation for practical applications in film, broadcast and immersive content production such as virtual, augmented and mixed reality. The majority of existing approaches for reconstruction of dynamic scenes from multi-view videos process each time frame independently due to the difficulty of simultaneously estimating temporal correspondence for non-rigid objects. Independent per-frame reconstruction can result in errors due to the inherent visual ambiguity caused by occlusion and similar object appearance for general scenes. Recent research has shown that exploiting temporal information can improve reconstruction accuracy as well as achieving temporal coherence @cite .
- 3D scene flow estimates frame to frame correspondence whereas 4D temporal coherence estimates correspondence across the complete sequence to obtain a single surface model. Methods to estimate 3D scene flow have been reported in the literature @cite for autonomous vehicles. However this approach is limited to narrow baseline cameras. Other scene flow approaches are dependent on 2D optical flow @cite @cite and they require an accurate estimate for most of the pixels which fails in the case of large motion. However, 3D scene flow methods align two frames independently and do not produce temporally coherent 4D models.
- Research investigating spatio-temporal reconstruction across multiple frames was proposed by @cite @cite @cite exploiting the temporal information from the previous frames using optical flow. An approach for recovering space-time consistent depth maps from multiple video sequences captured by stationary, synchronized and calibrated cameras for depth based free viewpoint video rendering was proposed by @cite . However these methods require accurate initialisation, fixed and calibrated cameras and are limited to simple scenes. Other approaches to temporally coherent reconstruction @cite either requires a large number of closely spaced cameras or bi-layer segmentation @cite @cite as a constraint for reconstruction. Recent approaches for spatio-temporal reconstruction of multi-view data either work on indoor studio data @cite .
- In the field of image segmentation, approaches have been proposed to provide temporally consistent monocular video segmentation @cite @cite @cite @cite . Hierarchical segmentation based on graphs was proposed in @cite , directed acyclic graph were used to propose an object followed by segmentation @cite . Optical flow is used to identify and consistently segment objects @cite @cite . Recently a number of approaches have been proposed for multi-view foreground object segmentation by exploiting appearance similarity spatially across views @cite @cite @cite @cite . An approach for space-time multi-view segmentation was proposed by @cite . However, multi-view approaches assume a static background and different colour distributions for the foreground and background which limits applicability for general scenes and non-rigid objects.
- To address this issue we introduce a novel method for spatio-temporal multi-view segmentation of dynamic scenes using shape constraints. Single image segmentation techniques using shape constraints provide good results for complex scene segmentation @cite (convex and concave shapes), but require manual interaction. The proposed approach performs automatic multi-view video segmentation by initializing the foreground object model using spatio-temporal information from wide-baseline feature correspondence followed by a multi-layer optimization framework. Geodesic star convexity previously used in single view segmentation @cite is applied to constraint the segmentation in each view. Our multi-view formulation naturally enforces coherent segmentation between views and also resolves ambiguities such as the similarity of background and foreground in isolated views.
- One area of related work is on data-independent locality sensitive hashing (LSH) @cite and data-dependent locality preserving hashing (LPH) @cite @cite . LSH hashes input vectors so that similar vectors have the same hash value with high probability. There are many algorithms in the family of LSH. One of the most common LSH methods is the random projection method called SimHash @cite , which uses a random hyperplane to hash input vectors.
- Sentiment analysis is a task in NLP that aims to predict the sentiment of a sentence @cite . The task can range from a binary classification task where the aim is to predict whether a document is positive or negative to a fine-grained task with multiple classes. In sentiment analysis, state-of-the-art results have been achieved using neural network architectures such as convolutional neural networks @cite and recurrent neural networks @cite . Variants of RNNs; LSTMs and GRUs, have also been used to great success @cite .
- The attention mechanism was first proposed for the task of machine translation @cite . Attention allows a network to 'focus' on one part of the sentence at a time. This is done through keeping another vector which contains information on the impact of individual words. Attention has also been used in other tasks within NLP area such as document classification @cite , sentiment analysis @cite and teaching machines to read @cite
- Encoder-decoder networks @cite @cite are often used in neural machine translation to translate a sequence from one language to another. These networks use RNNs or other types of neural networks to encode the information in the sentence and the another network to decode this sequence to the target language. Since RNNs do not perform well on longer sequences, the LSTM @cite unit is often used for their memory component. Gated Recurrent Units @cite are simpler variants on the LSTM, as they do not have an output gate.
- Transforming the sentiment of sentences has not been systematically attempted, however there are some previous pieces of research into this particular topic. @cite propose a method where a sentence or phrase with the target attribute, in this case sentiment, is extracted and either inserted in the new sentence or completely replacing the previous sentence. Their approach finds phrases based on how often they appear in text with a certain attribute and not in text with the other attribute. However, this approach can not take phrases into account that by themselves are not necessarily strongly leaning towards one sentiment, but still essential to the sentiment of the sentence.
- There are approaches which try to prioritize different examples to train on as the learning process goes on such as @cite and @cite . Although these techniques involve selecting examples to train on, they do not seek to identify redundant subsets of the data, but rather to sample the full dataset in a way that speeds up convergence.
- An early mention of trying to reduce the training dataset size can be seen in @cite . Their proposed algorithm splits the training dataset into many smaller training sets and iteratively removes these smaller sets until the generalization performance falls below an acceptable threshold. However, the algorithm relies on creating many small sets out of the given training set, rendering it impractical for modern usage.
- @cite pose the problem of subset selection as a constrained sub-modular maximization problem and use it to propose an active learning algorithm. The proposed techniques are used by @cite in the context of image recognition tasks. These drawback however, is that when used with deep-neural networks, simple uncertainty based strategies out-perform the mentioned algorithm.
- Another example of trying to identify a smaller, more informative set can be seen in @cite . Using their own definition of value of a training example, they demonstrate that prioritizing training over examples of high training value can result in improved performance for object detection tasks. The authors suggest that their definition of training value encourages prototypicality and thus results is better learning.
- Most recently @cite attempts to find redundancies in image recognition datasets by analyzing gradient magnitudes as a measure of importance. They prioritize examples with high gradient magnitude according to a pre-trained classifier. Their method fails to find redundancies in and datasets.
- Adaptive learning rates in online and stochastic optimization date back at least to @cite and were popularized in @cite @cite , the former of which introduced the well-known AdaGrad algorithm. Several variants of AdaGrad have now been proposed in the optimization and machine learning literature (see and the references therein), the most notable of which is the Adam algorithm @cite . All of these methods require (at least) linear space for maintaining various per-parameter statistics along their execution.
- Theoretically, recovering 3D shape from single-view images is an ill-posed problem. To address this issue, many attempts have been made, such as ShapeFromX @cite @cite , where X may represent silhouettes @cite , shading @cite , and texture @cite . However, these methods are barely applicable to use in the real-world scenarios, because all of them require strong presumptions and abundant expertise in natural images @cite .
- With the success of generative adversarial networks (GANs) @cite and variational autoencoders (VAEs) @cite , 3D-VAE-GAN @cite adopts GAN and VAE to generate 3D objects by taking a single-view image as input. However, 3D-VAE-GAN requires class labels for reconstruction. MarrNet @cite reconstructs 3D objects by estimating depth, surface normals, and silhouettes of 2D images, which is challenging and usually leads to severe distortion @cite . OGN @cite and O-CNN @cite use octree to represent higher resolution volumetric 3D objects with a limited memory budget. However, OGN representations are complex and consume more computational resources due to the complexity of octree representations. PSGN @cite and 3D-LMNet @cite generate point clouds from single-view images. However, the points have a large degree of freedom in the point cloud representation because of the limited connections between points. Consequently, these methods cannot recover 3D volumes accurately @cite .
- SfM @cite and SLAM @cite methods are successful in handling many scenarios. These methods match features among images and estimate the camera pose for each image. However, the matching process becomes difficult when multiple viewpoints are separated by a large margin. Besides, scanning all surfaces of an object before reconstruction is sometimes impossible, which leads to incomplete 3D shapes with occluded or hollowed-out areas @cite .
- Powered by large-scale datasets of 3D CAD models (e.g., ShapeNet @cite ), deep-learning-based methods have been proposed for 3D reconstruction. Both 3D-R2N2 @cite and LSM @cite use RNNs to infer 3D shape from single or multiple input images and achieve impressive results. However, RNNs are time-consuming and permutation-variant, which produce inconsistent reconstruction results. 3DensiNet @cite uses max pooling to aggregate the features from multiple images. However, max pooling only extracts maximum values from features, which may ignore other valuable features that are useful for 3D reconstruction.
- The problem of detecting communities in dynamic networks has attracted a lot of attention in recent years, with various approaches tackling different aspects of the problem, see @cite for a recent survey. Most of these methods consider that the studied dynamic networks are represented as sequences of snapshots, with each snapshot being a well formed graph with meaningful community structure, see for instance @cite @cite . Some other methods work with interval graphs, and update the community structure at each network change, e.g., @cite @cite . However, all those methods are not adapted to deal with link streams, for which the network is usually not well formed at any given time. Using them on such a network would require to first aggregate the links of the stream by choosing an arbitrarily temporal scale (aggregation window).
- Our work is also related to research conducted on change point detection considering community structures. In these approaches, given a sequence of snapshots, one wants to detect the periods during which the network organization and or the community structure remains stable. In @cite , the authors proposed the first change-point detection method for evolving networks that uses generative network models and statistical hypothesis testing. @cite proposed a hierarchical change point detection method to detect both inter-community(local change) and intra-community(global change) evolution. A recent work by @cite used graph distance measures and hierarchical clustering to identify sequences of system state dynamics. From those methods, our proposal keeps the principle of stable periods delimited by change points, and the idea of detecting changes at local and global scales. But our method differs in two directions: @math we are searching for stable individual communities instead of stable graph periods, and @math we search for stable structures at multiple levels of temporal granularity.
- Adversarial learning @cite @cite @cite @cite has been incorporated into the training procedure of re-ID systems in many previous works. In these works, generative adversarial networks (GAN) @cite typically acts as a data augmentation strategy by generating photo-realistic person images to enhance the training set. For example, Zheng al @cite applied GAN to generate unlabeled images and assigned a uniform label distribution during training. Wei al @cite proposed Person Transfer Generative Adversarial Network (PTGAN) to bridge the gap between different datasets. Moreover, Ge al @cite propose Feature Distilling Generative Adversarial Network to learn identity-related and pose-unrelated representations. @cite , binary codes are learned for efficient pedestrian matching via the proposed Adversarial Binary Coding.
- However, to the best of our knowledge, no prior work has rigorously considered the robustness of re-ID systems towards adversarial attacks, which have received wide attention in the context of classification-based tasks, including image classification @cite , object detection @cite and semantic segmentation @cite . As these vision tasks aims to sort an into a , they are therefore special cases of the broader classification problem. On such systems, it has been demonstrated that adding carefully generated human-imperceptible perturbations to an input image can easily cause the network to misclassify the perturbed image with high confidence. These tampered images are known as adversarial examples. Great efforts have been devoted to the generation of adversarial examples @cite @cite @cite . In contrast, our work focuses on adversarial attacks on metric learning systems, which analyze the relationship between two .
- Other approaches have involved encouraging the correct adjacencies of various object classes, whether they were learned from the data as in @cite or provided as a prior as in @cite . Such methods allow the introduction of this simple topological feature into a loss function when performing image segmentation but cannot be easily generalised to any other kinds of higher-order feature such as the presence of holes, handles or voids.
- The recent work of @cite introduced a topological regulariser for classification problems by considering the stability of connected components of the classification boundary and can be extended to higher-order topological features. It also provided a differentiable loss function which can be incorporated in the training of a neural network. This approach differs from ours in that firstly, it imposes topological constraints on the shape of the classification boundary in the feature space of inputs to the network, rather than topological constraints in the space of the pixels in the image, and secondly it aims only to reduce overall topological complexity. Our approach aims to fit the desired absence or presence of certain features and so complex features can be penalised or rewarded, as is appropriate for the task at hand.
- Persistent homology has previously been applied to the problem of semantic segmentation, such as in @cite @cite @cite . The important distinction between our method and these previous works is that they apply PH to the input image to extract features, which are then used as inputs to some other algorithm for training. Such approaches can capture complex features of the input images but require those topological features to be directly extractable from the raw image data. Our approach instead processes the image with a CNN and it is the output of the CNN, representing the pixelwise likelihood of the structure we want to segment, which has PH applied to it.
- : Using deep learning to solve geometric model fitting has received growing considerations. The dense approaches start from raw image pairs to estimate models such as homography @cite or non-rigid transformation @cite . @cite proposed to estimate the camera pose directly from image sequences.
- In contrast to the preceding works, DSAC @cite learns to extract from sparse feature correspondences some geometric models in a manner akin to RANSAC. The ability to learn representations from sparse points was also developed recently @cite @cite . This ability was exploited by @cite to fit camera motion (essential matrix) from noisy correspondences. Despite the promising results, none of the existing works have considered generic model fitting and, more importantly, fitting data of multiple models and even multiple types. In this work, we formulate the generic multi-model multi-type fitting problem as one of learning good representations for clustering.
- Query Expansion has rich literature in the area of Information Retrieval (IR). In the era of 1960s, @cite was the first researcher who applied QE for literature indexing and searching in a mechanized library system. In 1971, Rocchio @cite brought QE to spotlight through relevance feedback method'' and its characterization in a vector space model. This method is still used in its original and modified forms in automatic query expansion (AQE). Rocchio's work was further extended and applied in techniques such as collection-based term co-occurrence @cite @cite , cluster-based information retrieval @cite @cite , comparative analysis of term distribution @cite @cite @cite and automatic text processing @cite @cite @cite .
- Another popular approach is the use of Wikipedia articles, titles and hyper-links (in-link and out-link) @cite @cite . We have already mentioned the importance of Wikipedia as an ideal knowledge source for QE. Recently, quite a few research works have used it for QE (e.g., @cite @cite @cite @cite @cite ). Article @cite attempts to enrich initial queries using semantic annotations in Wikipedia articles combined with phrase-disambiguation. Their experiments show better results in comparison to the relevance based language model.
- For SGD-based methods, Online Kernel Hashing (OKH) @cite is the first attempt to learn hash functions via an online passive-aggressive strategy @cite , which updates hash functions to retain important information while embracing information from new pairwise input. Adaptive Hashing (AdaptHash) @cite adopts a hinge loss to decide which hash function to be updated. Similar to OKH, labels of pairwise similarity are needed for AdaptHash. Inspired by Error Correcting Output Codes (ECOCs) @cite , Online Supervised Hashing (OSH) @cite adopts a more general two-step hash learning framework, where each class is firstly deployed with a vector from ECOCs, and then an convex function is further exploited to replace the @math loss. In @cite , an OH with Mutual Information (MIHash) is developed which targets at optimizing the mutual information between neighbors and non-neighbors.
- Motivated by the idea of data sketching'' @cite , skech-based methods provide a good alternative for unsupervised online binary coding, via which a large dataset is summarized by a much smaller data batch. Leng proposed the Online Sketching Hashing (SketchHash) @cite , which adopts an efficient variant of SVD decomposition to learn hash functions. More recently, Subsampled Randomized Hadamard Transform (SRHT) is adopted in FasteR Online Sketching Hashing (FROSH) @cite to accelerate the training process of SketchHash.
- However, existing sketch-based algorithms are based on unsupervised learning, and their retrieval performance is mostly unsatisfactory without fully utilizing label information. Although most SGD-based algorithms aim to preserve the label information via online hash function learning, the relaxation process is adopted to update the hash functions, which contradicts with the recent advances in offline hashing where discrete optimizations are adopted directly, such as Discrete Graph Hashing @cite and Discrete Supervised Hashing @cite . In this paper, we are the first to investigate OH with discrete optimizations, which have shown superior performance compared with the quantization-based schemes.
- GANs @cite @cite @cite @cite are proven effective in generating photo-realistic images in recent developments of neural networks. Because of the adversarial training approach, it is difficult for GANs to map inputs to latent vectors. Although some approaches @cite @cite are proposed to address this problem, it still remains open and requires further investigation. Compared to GANs, VAEs @cite @cite are generative models which can easily map an input to its corresponding latent vector. This advantage enables VAEs to be either used as data compressors or employed in application scenarios where manipulation of the latent space is required @cite @cite . Compared with AEs @cite , VAEs encode inputs to Gaussian distributions instead of deterministic latent vectors, and thus enable them to generate examples. On one hand, Gaussian distributions do not form a vector space. Naively treating them as vectors will ignore its geometric properties. On the other hand, most machine learning models including neural networks are designed to work with vector outputs. To incorporate the geometric properties of Gaussian distributions, the type of space of Gaussian distributions needs to be identified first; then corresponding techniques from geometric theories will be adopted to design the neural networks.
- Geometric theories have been applied to analyze image feature space. In @cite , covariance matrices are used as image feature representations for object detection. Because covariance matrices are symmetric positive definite (SPD) matrices, which form a Riemannian manifold, a corresponding boosting algorithm is designed for SPD inputs. In @cite , Gaussian distributions are used to model image features and the input space is analyzed using Lie group theory.
- In these methods the user provides local hints, as for instance color scribbles, which are then propagated to the whole image. They were initiated with the work of Levin al @cite . They assume that spatial neighboring pixels having similar intensities should have similar colors. They formalize this premise optimizing a quadratic cost function constrained to the values given by the scribbles. Several improvements were proposed. Huang al @cite improve the bleeding artifact using edge information of the grayscale image. Yatziv al @cite propose a luminance-weighted chrominance blending to relax the dependency of the position of the scribbles. Then, Luan al @cite use the input scribbles to segment the grayscale image and thus better propagate the colors. This class of methods suffer from requiring large amounts of user inputs in particular when dealing with complex textures. Moreover, choosing the correct color palette is not an easy task.
- @cite , a supervised learning method is proposed through a linear parametric model and a variational autoencoder which is computed by quadratic regression on a large dataset of color images. These approaches are improved by the use of CNNs and large-scale datasets. For instance, Iizuka al @cite extract local and global features to predict the colorization. The network is trained jointly for classification and colorization in a labeled dataset.
- Zhang al @cite learn the color distribution of every pixel and infer the colorization from the learnt distribution. The network is trained with a multinomial cross entropy loss with rebalanced rare classes allowing for rare colors to appear in the colorized image. In a similar spirit, Larsson al @cite train a deep CNN to learn per-pixel color histograms. They use a VGG network in order to interpret the semantic composition of the scene as well as the localization of objects and then predict the color histograms of every pixel based on this interpretation. They train the network with the Kullback-Leibler divergence. Again, the colorization is inferred from the color histrograms.
- Other CNN based approaches are combined with user interactions. For instance, Zhang al @cite propose to train a deep network given the grayscale version and a set of sparse user inputs. This allows the user to have more than one plausible solution. Also, He al @cite propose an exemplar-based colorization method using a deep learning approach. The colorization network jointly learns faithful local colorization to a meaningful reference and plausible color prediction when a reliable reference is unavailable.
- Some methods use GANs to colorize grayscale images. Isola al @cite propose to use conditional GANs to map an input image to an output image using a U-Net based generator. They train their network by combining the @math -loss with an adapted GAN loss. An extension is proposed by Nazeri al @cite generalizing the procedure to high resolution images, speeding up and stabilizing the training. Cao al @cite also use conditional GANs but, to obtain diverse possible colorizations, they sample several times the input noise, which is incorporated in multiple layers in the proposed network architecture, which consists of a fully convolutional non-stride network. Their choice of the LSUN bedroom dataset helps their method to learn the diversity of bedroom colors. Notice, that none of these GANs based methods use additional information such as classification.
- In general, a rainy image can be formed as the composition of a clean background image layer and a rain layer. On one hand, linear summation is usually adopted as the composition model @cite @cite @cite . Then, image deraining can be formulated by incorporating with proper regularizers on both background image and rain layer, and solved by specific optimization algorithms. Among these methods, Gaussian mixture model (GMM) @cite , sparse representation @cite , and low rank representation @cite have been adopted for modeling background image or a rain layers. Based on linear summation model, optimization-based methods have been also extended for video deraining @cite @cite @cite @cite @cite . On the other hand, screen blend model @cite is assumed to be more realistic for the composition of rainy image, based on which Luo al @cite use the discriminative dictionary learning to separate rain streaks by enforcing the two layers share fewest dictionary atoms. However, the real composition generally is more complicated and the regularizers are still insufficient in characterizing background and rain layers, making optimization-based methods remain limited in deraining performance.
- When applied deep network to single image deraining, one natural solution is to learn a direct mapping to predict clean background image @math from rainy image @math . However, it is suggested that plain fully convolutional networks (FCN) are ineffective in learning the direct mapping @cite @cite . Instead, Fu al @cite @cite apply a low-pass filter to decompose @math into a base layer @math and a detail layer @math . By assuming @math , FCNs are then deployed to predict @math from @math . In contrast, Li al @cite adopt the residual learning formulation to predict rain layer @math from @math . More complicated learning formulations, such as joint detection and removal of rain streaks @cite , and joint rain density estimation and deraining @cite , are also suggested. And adversarial loss is also introduced to enhance the texture details of deraining result @cite @cite . In this work, we show that the improvement of deraining networks actually eases the difficulty of learning, and it is also feasible to train PRN and PReNet to learn either direct or residual mapping.
- For the architecture of deraining network, Fu al first adopt a shallow CNN @cite and then a deeper ResNet @cite . In @cite , a multi-task CNN architecture is designed for joint detection and removal of rain streaks, in which contextualized dilated convolution and recurrent structure are adopted to handle multi-scale and heavy rain steaks. Subsequently, Zhang al @cite propose a density aware multi-stream densely connected CNN for joint estimating rain density and removing rain streaks. @cite , attentive-recurrent network is developed for single image raindrop removal. Most recently, Li al @cite recurrently utilize dilated CNN and squeeze-and-excitation blocks to remove heavy rain streaks. In comparison to these deeper and complex networks, our work incorporates ResNet, recurrent layer and multi-stage recursion to constitute a better, simpler and more efficient deraining network.
- Graph classification is an important problem with many practical applications. Data like social networks, chemical compounds, brain networks can be represented as graphs naturally and they can have applications such as community detection @cite , anti-cancer activity identification @cite @cite and Alzheimer's patients diagnosis @cite @cite respectively. Traditionally, researchers mine the subgraphs by DFS or BFS @cite @cite , and use them as the features. With the rapid development of deep learning (DL), many works are done based on DL methods. GAM builds the model by RNN with self-attention mechanism @cite . DCNN extend CNN to general graph-structured data by introducing a diffusion-convolution operation @cite .
- Earlier works on metric learning are based on @cite . In that study, two identical neural networks extract the features of two arbitrary images. Next, these features are compared by a metric which is based on a radial function The distance between any two members in the feature space is defined as the cosine of the angle between them @cite . . While their loss function forces the samples in the same class to be closer to each other in the sense of the selected distance function, the samples in the different classes are forced to be mapped far from each other. The cost function of such a network is given below @cite where @math represents the operation of @math , and @math are distances in between samples.
- Another approach is to utilize the hierarchical class labels of the training samples @cite . In that method, samples with similar fine labels have the same coarse label, i.e. a sample has more than one label. The cost function is modified by considering both the coarse and fine labels. For this purpose, each quadruplet sample is constructed as follows: (1) Reference sample (anchor sample), @math , (2) Positive positive sample, @math , (3) Positive negative sample, @math , (4) Negative sample, @math . Similar to the triplet selection, the quadruplets are selected such that three constraints should be taken into account. First, both the coarse and fine classes of @math and @math should be the same. Second, although the coarse class of @math is the same as the coarse class of @math , the fine classes are different. Finally, the coarse class of @math and @math should be different.
- Moreover, the loss function for the quadruplets is similar to the triplet based methods @cite . On the other hand, in @cite , the use of the global loss has been proposed, while the quadruplet samples are selected randomly (Note that these quadruplets hold the constraints). The global loss penalizes the network in case of the mean and variance of the distances between the samples in a quadruplet are not appropriate, as given in In , @math , @math , and @math , @math as defined in @cite . , where @math and @math are the margins, similar to .
- In @cite , the hierarchical labels of the training samples are utilized. It should be noted that a model has difficulty in convergence when the samples are selected randomly since the most informative pairs are not effectively considered. Here, we propose two methods for sample selection to address this issue.
- The basic variant of this problem has been first considered in @cite under the name Representatives Selection Problem, where we are allowed to select one item from each set of alternatives. In order to alleviate the effects of cost uncertainty on decision making, the min-max and min-max regret criteria @cite @cite have been proposed to assess the solution quality. The problem formulations using these criteria belong to the class of robust optimization problems @cite . Such approach appears to be more suitable for large scale design projects than an alternative stochastic optimization approach @cite , when: 1) decision makers do not have sufficient historical data for estimating probability distributions; 2) there is a high factor of risk involved in one-shot decisions, and a precautionary approach is preferred. The robust approach to discrete optimization problems has been applied in many areas of industrial engineering, such as: scheduling and sequencing @cite @cite @cite , network optimization @cite @cite , assignment @cite @cite , and others @cite .
- Note that deterministic version of Representatives Selection Problem is easily solvable in polynomial time. For interval uncertainty representation of cost parameters the problem can still be solved in polynomial time, both in case of minimizing the maximum regret and the relative regret @cite . However, in case of discrete set of scenarios, the problem becomes NP-hard even for 2 scenarios, and strongly NP-hard when the number of scenarios @math is a part of the input. In @cite authors prove that strong NP-hardness holds also when sets of eligible items are bounded. In @cite an @math -approximation algorithm for this variant was given.
- A recent work developed a framework called Production Assessment 4.0, which aims to support enterprises developing Industry 4.0 use cases. For doing so, they made use of the design thinking approach. After elaborating on the framework and its processes, a section about its evaluation is presented. Production Assessment 4.0 was evaluated in several consulting projects with enterprises. However, no details about, e.g., their data characteristics or their state of Industry 4.0 adoption progress are given @cite .
- With respect to Industry 4.0, there are many existing definitions and views published. An overview of selected perceptions of Industry 4.0 is presented in @cite . Moreover, it also states that there is no generally accepted definition for the term Industry 4.0.
- Another work presents design principles for Industry 4.0 that are derived through text analysis and literature studies @cite . Thereby, it is aimed to help both, the scientific community and practitioners with this result. In total, four design principles were identified, namely technical assistance, interconnection, decentralized decisions, and information transparency.
- Self-supervised learning with surrogate supervision is a relatively new trend in computer vision, with promising schemes appearing only in recent years. Consequently, the literature on the effectiveness of surrogate supervision in medical imaging is meager. @cite proposed longitudinal relationships between medical images as the surrogate task to pre-train model weights. To generate surrogate supervision, they assign a label of 1 if two longitudinal studies belong to the same patient and 0 otherwise. @cite used noise removal in small image patches as the surrogate task, wherein the surrogate supervision was created by mapping the patches with user-injected noise to the original clean image patches. @cite used image colorization as the surrogate task, wherein color colonoscopy images are converted to gray-scale and then recovered using a conditional Generative Adversarial Network (GAN).
- There are several studies for solving relation classification task. Early methods used handcrafted features through a series of NLP tools or manually designing kernels @cite . These approaches use high-level lexical and syntactic features obtained from NLP tools and manually designing kernels, but the classification models relying on such features suffer from propagation of implicit error of the tools.
- On the other hands, deep neural networks have shown outperform previous models using handcraft features. Especially, many researches tried to solve the problem based on end-to-end models using only raw sentences and pre-trained word representations learned by Skip-gram and Continuous Bag-of-Words @cite @cite @cite . employed a deep convolutional neural network (CNN) for extracting lexical and sentence level features @cite . Dos proposed model for learning vector of each relation class using ranking loss to reduce the impact of artificial classes @cite . Zhang and Wang used bidirectional recurrent neural network (RNN) to learn long-term dependency between entity pairs @cite . Furthermore, proposed bidirectional LSTM network (BLSTM) utilizing position of words, POS tags, named entity information, dependency parse @cite . This model resolved vanishing gradient problem appeared in RNNs by using BLSTM.
- Although the most natural application of Deep Learning algorithms to medical diagnosis is automated medical image diagnosis @cite , the usage of Physiological Time Series (PTS) and Electronic Medical Record (EMR) data, is a more general source of data on which machine learning models can be trained. EMRs are very attractive as a potential data source since their use is widespread, which makes them abundant and accessible electronically. However, there are certain challenges associated with their secondary use in Machine Learning @cite . Despite this, several works have reported the successful use of EMRs and PTS to train Machine Learning Deep Learning based models for diagnosis.
- A Parkinson's database comprising MRI and DaT Scan data from 78 subjects, 55 patients with Parkinson's and 23 non patients, has been recently released @cite ; it includes, in total 41528 MRI data (31147 from patients and 10381 from non patients) and 925 DaT scans (595 and 330 respectively). Our developments next are based on this database.
- Recent advances in deep neural networks @cite , @cite , @cite , @cite have been explored in @cite , where convolutional (CNN) and convolutional-recurrent (CNN-RNN) neural networks were developed and trained to classify the information in the above Parkinson's database in two categories, i.e., patients and non patients, based on either MRI inputs, or DaT Scan inputs, or together MRI and DaT Scan inputs.
- The developed networks included: transfer learning of the ResNet-50 network @cite as far as the convolutional part of the networks was concerned, with retraining of the fully connected network layers; adding on top of this and training a recurrent network using Gated Recurrent Units (GRU) @cite in an end-to-end manner.
- Passive walking robots @cite @cite fall in the dynamic locomotion category too. These studies shed light on the important aspects of biped locomotion, but do not provide direct application for feedback control related to our methods. On the other hand, the progress made in actuated planar biped locomotion is impressive. @cite @cite show biped robots running and their capability to recover from disturbances on irregular terrains. However, there is an obvious gap between supported (or constrained) locomotion and unsupported walking. @cite shows unsupported single leg hopping, which is a remarkable accomplishment. Besides the strong contribution in dynamic locomotion of that work, the study omitted several important aspects of unsupported biped locomotion such as body posture control, continuous interaction of the stance leg through the ground contact phases, and disturbances from the other limbs' motion, which are a focus of our paper.
- Region-CNN family @cite @cite @cite @cite @cite considers object detection as two sequential problems: first propose a (large) set of bounding box candidates, crop them, and use an image classification module to classify the cropped region or region feature. R-CNN @cite uses selective search @cite to generate region proposals and feeds them to an ImageNet classification network. SPP @cite and Fast RCNN @cite first feed an image through a convolutional network and crop an intermediate feature map to reduce computation. Faster RCNN @cite further replaces region proposals @cite with a Region Proposal Network. The detection-by-classification idea is intuitive and keeps the best performance so far @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite .
- One-stage object detectors @cite @cite @cite @cite @cite @cite @cite do not have a region cropping module. They can be considered as region or anchor proposal networks and directly assign a class label to each positive anchor. SSD @cite @cite uses different scale anchors in different network layers. YOLOv2 @cite learns category-specific anchor shape priors. RetinaNet @cite proposes a focal loss to balance the training contribution between positive and negative anchors. RefineDet @cite learns to early reject negative anchors. Well-designed single-stage object detectors achieve very close performance with two-stage ones at higher efficiency.
- As a bottom-up object detection method, our idea of grouping center and extreme points is related to Deformable Part Model @cite . Our center point detector functions similarly with the root filter in DPM @cite , and our four extreme points can be considered as a universal part decomposition for all categories. Instead of learning the part configuration, our predicted center and four extreme points have a geometry structure. And we use a state-of-the-art keypoint detection network instead of low-level image filters for part detection.
- Determining which keypoints are from the same person is an important component in bottom-up multi-person pose estimation. There are multiple solutions: Newell al @cite proposes to learn an associative feature for each keypoint, which is trained using an embedding loss. Cao al @cite learns an affinity field which resembles the edge between connected keypoints. Papandreous al @cite learns the displacement to the parent joint on the human skeleton tree, as a 2-d feature for each keypoint. Nie al @cite also learn a feature as the offset with respect to the object center.
- Prevalent keypoint detection methods work on well-defined semantic keypoints, e.g., human joints. StarMap @cite mixes all types of keypoints using a single heatmap for general keypoint detection. Our extreme and center points are a kind of such general implicit keypoints, but with more explicit geometry property.
- In the recent work of Hayashi @cite , an encoder-decoder approach was presented, where the first sentence was reformulated to a headline. Our Encoder-Decoder baseline (see section ) follows their setup.
- Many interesting theoretical results have been developed on the loss surface of neural networks @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . There is also a whole line of researches studying convergence of learning algorithms in training neural networks and others studying generalization properties, which is however beyond the scope of this paper.
- One of the most related studies is the one by Xia et al @cite . They investigated what developers search for on the Web, and found that developers search for explanations of unknown terminology, explanations for exceptions error messages (e.g., HTTP 404), reusable code snippets, solutions to common programming bugs, and suitable third-party libraries services. Furthermore, they found that searching for solutions to performance bugs, solutions to multi-threading bugs, public datasets to test newly developed algorithms or systems, reusable code snippets, best industrial practices, database optimization solutions, solutions to security bugs, and solutions to software configuration bugs are the most difficult search tasks that developers consider.
- Many researchers have made use of code comments in their work. @cite automatically identify bugs by analyzing inconsistencies between code and comments. Ratol and Robillard @cite used code comments to assist refactoring activities. Wong et al @cite used code comments to map source code and Stack Overflow content. German et al @cite developed the ninka tool that automatically identifies a software license in code comments. Goldman and Miller @cite developed the tool CodeTrail, that demonstrates how the developer's use of web resources can be improved by connecting the Eclipse integrated development environment (IDE) and the Firefox web browser.
- Self-admitted technical debt is a commenting activity that has been well-studied in recent years @cite . @cite and @cite studied the removal of self-admitted technical debt based on the modification of comments. Our finding of referencing bug reports for self-admitted technical debt could be another opportunity to study development activities around technical debt.
- There are also studies which analyze link sharing occurring in other software artifacts. Gomez et al @cite investigated link sharing on Stack Overflow to gain insights into how software developers discover and disseminate innovations. Rath et al @cite investigated links to issue tracking systems in commit comments. They reported that developers often do not provide external links to issues. They evaluated several methods to automatically recover links by searching issues related to a given commit. Alqahtani et al @cite proposed a tool to automatically link dependent components in a system to online resources for analyzing their vulnerabilities. Chen et al @cite proposed a tool to link problematic source code to relevant Stack Overflow questions using similarity of source code fragments.
- Traceability links between source code and documents is another related research topic. Scanniello et al @cite reported that developers can understand source code effectively if they can refer to design models including source code element names. Their observation has been obtained through a controlled experiment of program comprehension tasks with UML models produced in a requirements engineering phase and a design phase. Antoniol et al @cite proposed a method to identify links between source files and design documents because developers may update source file names without updating related documents. Their method uses similarity of attribute names of a class to identify its original class definition in design documents. Rahimi et al @cite proposed a rule-based method to update links between source files and requirements documents. Their method recognizes a change scenario from semantic differences of source code and then updates links according to a rule corresponding to the change scenario. Those methods would be effective to automatically update traceability links. Similar tool support for external source referencing is a future direction of our research.
- Finally, in 2006, researchers investigated the use of neural networks to predict television ad effectiveness @cite . They achieved an accuracy of 99
- Recently, a series of papers by Young @cite , Zhang @cite @cite , @cite , @cite @cite , @cite @cite and @cite initiated a rigorous analysis of stochastic processes induced by Schelling's model. In these processes either two randomly chosen unhappy agents of different type swap positions @cite @cite @cite or a randomly chosen agent changes her type with a certain probability @cite @cite @cite @cite @cite . It is worth noticing that both types of processes are closely related but not identical to Schelling's original model where discontent agents move to different positions until they become content with their current location. The focus of the above mentioned works is on investigating the expected size of the obtained homogeneous regions, but it is also shown that the stochastic processes starting from a uniform random agent placement converge with high probability to a stable placement. The convergence time was considered by Mobius & Rosenblat @cite who observe that the Markov chain analyzed in @cite @cite @cite has a very high mixing time. @cite show in the two-dimensional grid case a dichotomy in mixing times for high @math and very low @math values.
- Very recently, @cite studied a variant of the model by @cite , where the agents are partitioned into stubborn and strategic agents. The former agents do not move and the latter agents try to maximize the fraction of same-type agents in their neighborhood by jumping to a suitable empty location. This corresponds to a variant of the JSG with @math . They show that equilibria are not guaranteed to exist and that deciding equilibrium existence or the existence of an agent placement with certain social welfare is NP-hard. This relates to our hardness results for computing socially optimal states. They also prove that the price of anarchy and the price of stability can be unbounded.
- The first insertion-only streaming algorithm for the @math -median clustering problem was presented in 2000 by Guha, Mishra, Motwani, and O'Callaghan @cite . Their algorithm uses @math space for a @math approximation, for some @math . Subsequently, Charikar al , @cite present an @math -approximation algorithm for @math -means clustering that uses @math space. Their algorithm uses a number of phases, each corresponding to a different guess for the value of the cost of optimal solution. The guesses are then used in the online facility location ( ) algorithm of @cite , which provides a set of centers whose number and cost allows the algorithm to reject or accept the guess. This technique is now one of the standard approaches for handling @math -service problems. Braverman al , @cite improve the space usage of this technique to @math . @cite and @cite develop algorithms for @math -means clustering on sliding windows, in which expired data should not be included in determining the cost of a solution.
- Another line of approach for @math -service problems is the construction of coresets, in particular when the data points lie in the Euclidean space. Har-Peled and Mazumdar @cite give an insertion-only streaming algorithm for @math -medians and @math -means that provides a @math -approximation, using space @math , where @math is the dimension of the space. Similarly, Chen @cite introduced an algorithm using @math space, with the same approximation guarantees.
- Cohen and Strauss @cite study problems in time-decaying data streams in 2003. There are a number of results @cite @cite @cite @cite in this line of work, but the most prominent time-decay model is the sliding window model. Datar al , @cite introduced the exponential histogram as a framework in the sliding window for estimating statistics such as count, sum of positive integers, average, and @math norms. This initiated an active line of research, including improvements to count and sum @cite , frequent itemsets @cite @cite , frequency counts and quantiles @cite @cite , rarity and similarity @cite , variance and @math -medians @cite and other geometric and numerical linear algebra problems @cite @cite @cite .
- Generative Adversarial Networks (GANs) and Convolutional Neural Networks (CNNs) have been extensively used in the process of generating new images given a specific database as a reference. The modeling of new images can be learned from the probability distribution of any set of images @cite . This process can be perceived in the literature in applications such as the generation of new images @cite , the transfer of styles from one set of images to another @cite , the modeling of new images combining features in the discriminative space @cite , among others.
- @cite presented a neural algorithm for style transfer based on the extraction of image style through convolutional layers. The authors showed that the deeper the convolutional layers, the more the content of the image and the artistic style could be separated -- and, as a result, more the artistic style could be extracted from the input image. Similar to this, the higher layers of the CNN can generate more robust, sharp and detailed artistic styles images @cite . @cite brought to light optimization to the neural algorithm proposed by @cite , where the neural feed-forward network was trained with perceptual loss, instead of a per-pixel loss. Such an optimization had similar qualitative results in regards to the artistic style transfer, with three orders of magnitude faster @cite . The optimization proposed by @cite also showed that instance normalization could be applied to the CNN with improved results over batch normalization, in training and testing time.
- Let us recall that Semantic GP uses the information in the target behavior, i.e., @math , to guide the search. Notably, Krawiec @cite affirmed that aware semantic methods make search algorithms better informed. For example, Nguyen @cite proposed Fitness Sharing, a technique that promotes dispersion and diversity of individuals. Their proposal consisted of calculating the individual fitness as @math , where @math is approximately equal to the number of individuals that behave similarly to individual @math .
- Some crossover and mutation operators have been developed with the use of semantics. Beadle and Johnson @cite proposed a crossover operator that measures the semantic equivalence between parents and offsprings; and rejects the offspring that is semantically equivalent to its parents. Quang Uy @cite proposed a semantic crossover and mutation. The crossover operator searches for a crossover point in each parent in such way that subtrees were semantically similar, and the mutation operator allows the replacement of an individual subtree only if the new subtree is semantically similar. Hara @cite proposed the Semantic Control Crossover that uses the semantics to combine individuals where a global search was performed in the first generations and a local search in the last ones. Graff used subtrees semantics and partial derivatives to proposed crossover @cite @cite and mutation @cite operators.
- Moraglio @cite @cite proposed Geometric Semantic Genetic Programming (GSGP). Their work called the attention of the GP scientific community because the crossover operator produces an offspring that stands in the segment joining the parents' semantics. Therefore, offspring fitness cannot be worse than the worst fitness of the parents. Given two parents @math and @math , the crossover operator generates an offspring as @math , where @math is a real value between @math and @math . This property transforms the fitness landscape into a cone. Unfortunately, the offspring is always bigger than the sum of the size of its parents; this makes the operator unusable in practice. Later, some operators appear intending to improve Moraglio's GSGP. For example, Approximately Geometric Semantic Crossover (SX) @cite , Deterministic Geometric Semantic Crossover @cite , Locally Geometric Crossover (LGX) @cite @cite and Approximated Geometric Crossover (AGX) @cite , Semantic Crossover and Mutation based on projections @cite @cite and Subtree Semantic Geometric Crossover (SSGX) @cite .
- Pawlak @cite proposed the Random Desired Operator (RDO). It propagates the target semantics to calculate the desired semantics in the node selected as mutation point. This desired behavior is used to search in a procedures library for the most similar subtree. Finally, it swaps the mutated node with the subtree. RDO was extended by Szubert @cite introducing the Forward Propagation Mutation (FPM) which uses a combination of forward and back-propagation to find a combination of unitary and binary functions that is the most similar to the desired behavior.
- Chen @cite proposed the Angle-Driven Selection (ADS) where the first parent is selected using fitness and the second is with an angle-distance defined as @math . One of our selection heuristics is similar to ADS; however, there are significant differences, the first parent is randomly selected whereas the second parent is selected using an equivalent similarity with the difference that the target behavior is not considered in our approach.
- Loveard and Ciesielski @cite proposed different techniques for representing classification problems in GP; one of them assign the class based on a range, there were as many intervals as classes. Muni @cite proposed to evolve a tree for each class following an equivalent strategy of one-vs-all approach. Jaben and Baig @cite developed a two-stage method, the first one evolves a classifier for each class, and the second phases combine these classifiers.
- Ingalalli @cite introduced a GP framework called Multi-dimensional Multi-class Genetic Programming (M2GP). The main idea is to transform the original space into another one using functions evolved with GP, then, a centroid is calculated for each class, and the vectors are assigned to the class which corresponds to the nearest centroid using the Mahalanobis distance. M2GP takes as argument the dimension of the transform space; this parameter is evolved in M3GP @cite by including specialized search operators that can increase or decrease the number of feature dimensions produced by each tree. They extended M3GP and proposed M4GP @cite that uses a stack-based representation in addition to new selection methods, namely lexicase selection, and age-fitness Pareto survival.
- Naredo @cite use NS for evolving genetic programming classifiers based on M3GP where the difference is the procedure to compute the fitness. Each GP individual is represented as a binary vector whose length is the training set size and each vector element is set to 1 if the classifier assigns the class label correctly and 0 otherwise. Then, they use this binary vectors to measure the sparseness among individuals, and the more the sparseness the higher the fitness value. Their results show that all their NS variants achieve competitive results relative to the traditional objective-based.
- Auto machine learning consists of obtaining automatically a classifier (regressor) that includes the steps of preprocessing, feature selection, classifier selection, and hyperparameters tuning. Feurer @cite developed a robust automated machine learning (AutoML) technique using Bayesian optimization methods. It is based on scikit-learn @cite , using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods; giving rise to a structured hypothesis space with 110 hyperparameters. Olson @cite proposed the use of GP to develop a powerful algorithm that automatically constructs and optimizes machine learning pipelines through a Tree-based Pipeline Optimization Tool (TPOT). On classification, the objective consists of maximizing accuracy score performing a searching of the combinations of 14 preprocessors, five feature selectors, and 11 classifiers; all these techniques implemented on scikit-learn @cite .
- @cite extend a controlled virtual environment called ANUBIS to collect sample's execution trace. An 8-tuple is constructed as the representation, which consists of the system call's name, corresponding objects such as files, and dependencies between these system calls and objects.
- @cite introduce a feature representation called Malware Instruction Set (MIST). MIST uses several levels of features to represent a system call. The first level represents the category and name of API calls. The following levels are specified manually for each API call to represent their arguments. Therefore, the feature from the same level but for different APIs could have different semantics. The inconsistency imposes challenges to learn patterns using machine learning models. Qiao, @cite extend the MIST and propose a representation called Byte-based Behavior Instruction Set (BBIS). They claim that only the first level of MIST is efficient and thus BBIS only uses the category and name of API calls. Besides, they propose an algorithm (CARL) to process consecutively repeated API calls.
- @cite propose a feature representation associating the API call sequence with their arguments. It assigns each argument to bind with its API call to form a new sequence, However, this approach leads to an extremely long feature vector and might lose the pattern of API call sequence. @cite propose another two feature representations. These representations consist of first 200 API calls as well as its argument". However, this argument" only indicates whether this API call is connected with the later one and it might not maintain sufficient information from arguments.
- David and Netanyahu @cite treat the sandbox report as an entire text string, and then split all strings by any special character. They count the frequency of each string and keep only top 20,000 frequent ones by using a 20,000-bit vector to represent it. Their model is a deep belief network (DBN) which consists of eight layers, from 20,000-sized vectors to 30-sized vectors. They append a softmax layer after the top layer. Cross-entropy loss is used to train the model, which attains 98.6 @cite propose a two-stage approach, a feature learning stage and a classification stage. At the first stage, they use the Recurrent Neural Networks (RNNs) to predict the next possible API call based on the previous API call sequence. For the classification stage, they freeze the RNNs, and feed the outputs into a max-pooling layer aggregate the features for classification. They attain 71.71 @cite propose an approach which combines convolution neural network (CNN) with RNNs. Their approach stacks two CNN layers, and each CNN layer uses a 3-sized filter to simulate the 3-grams approach. A Long short-term memory (LSTM) with a 100-sized hidden vector is appended to handle the time-series sequence.
- The previous 3 papers only use the API call sequence but ignore the arguments. Huang and Jack @cite uses a feature representation consisting of three parts, the presence of unpacked code fragments in the arguments, the combination of the API call name with one of its arguments (selected manually), and the 3-gram of API call sequence. This feature representation with 50,000 features which is reduced to 4,000 by a random projection. They claim for the first time the deep learning model (i.e., RNN) outperforms a shallow architecture proposed by @cite . @cite also use the API call sequence and the arguments. Their feature representation consisting of a one-hot vector from API call name and top N frequent n-grams of the argument strings. The model uses several stacked LSTMs and shows a better performance than @cite . They also claim more LSTMs cannot increase the performance.
- This paper follows a recent body of literature on data-driven optimization under uncertainty in operations research and management science. Much of this work has focused on the paradigm of distributionally robust optimization, in which the optimal solution is that which performs best in expectation over a worst-case probability distribution from an ambiguity set. Motivated by probabilistic guarantees, distributionally robust optimization has found particular applicability in data-driven settings in which the ambiguity set is constructed using historical data, such as @cite @cite @cite @cite . In particular, the final steps in our convergence result () draw heavily from similar techniques from @cite and @cite . In contrast to previous work, this paper develops a new measure concentration result for the weighted empirical distribution () which enables machine learning and covariates to be incorporated into sample robust optimization and Wasserstein-based distributionally robust optimization for the first time.
- Several recent papers have focused on tractable approximations of two- and multi-stage and robust optimization. Many approaches are based around policy approximation schemes, including lifted linear decision rules , @math -adaptivity , and finite adaptability . Alternative approaches include tractable approximations of copositive formulations . Closest related to the approximation scheme in this paper are @cite and @cite , which address two-stage problems via overlapping decision rules. @cite propose a modeling approach that leads to novel approximations of various distributionally robust applications, including two-stage distributionally robust optimization using Wasserstein ambiguity sets and expectations of piecewise convex objective functions in single-stage problems. Independently, @cite investigate a of two-stage sample robust optimization by optimizing a separate linear decision rule for each uncertainty set and prove that this approximation gap converges to zero as the amount of data goes to infinity. In of this paper, we show how to extend similar techniques to dynamic problems with many stages for the first time.
- As discussed previously, the methodology in this paper also follows recent work on incorporating covariates in optimization under uncertainty using local predictive methods (such as @math -nearest neighbor regression, kernel regression, and random forests). In particular, the asymptotic optimality justification of @cite in single-stage settings relies on the strong universal consistency for local predictive models (, @cite ). Our proof of asymptotic optimality instead relies on convergence guarantees rooted in distributionally robust optimization. The reason we use a different approach is that the arguments for the convergence for local predictive models from @cite require finite dimensional decision variables. In contrast, the convergence guarantees in this paper apply for dynamic optimization over general spaces of policies.
- As reasoned above, normalized standard bit mutation offers an elegant way to interpolate between deterministic mutation strengths and regular standard bit mutation, thus showing that Randomized Local Search (RLS) variants with their deterministic search radii and the (1+1) EA with mutation rate @math are essentially just different instantiations of the same meta-algorithm. Similar results also extend to population-based @math EAs. Note that normalized standard bit mutation also allows other degrees of randomization, thereby offering a wide range for further experimentation. In this context we note that for the special case of standard RLS (i.e., the greedy (1+1) hill climber that flips in each iteration exactly one uniformly chosen bit) a similar meta-model allowing to interpolate between the (1+1) EA and RLS is the (1+1) EA @math introduced in @cite @cite . This model, however, is much less flexible, and does not allow, for example, deterministic search radii greater than one.
- Our work falls under the category of real-parameter black-box global optimization @cite . Traditional approaches for black-box optimization like covariance matrix adaptation evolution strategy (CMA-ES) @cite , Nelder-Mead @cite , and Particle Swarm Optimization (PSO) @cite hand-design rules using heuristics (e.g. using nature-inspired genetic algorithms) to decide the next query point(s) given the observations made so far. Another category of approaches for global optimization of black-box functions include Bayesian optimization techniques @cite @cite @cite . These approaches use observations (query and response) made thus far to approximate the black-box function via a surrogate (meta-) model, e.g. using a Gaussian Process @cite , and then use this model to construct an acquisition function to decide the next query point. The acquisition function updates needed at each step are known to be costly @cite .
- Recent work on Physics-guided deep learning @cite @cite incorporates domain knowledge in the learning process via additional loss terms. Such approaches can be useful in our setting if the optimizer network is to be trained from scratch for a given application. However, the purpose of building a generic optimizer that can be transferred to new applications requires incorporating domain constraints in a posterior manner during inference time when the optimizer is suggesting query points. This is not only useful to adapt the same optimizer to a new application but also useful in another practical scenario of adapting to a new set of domain constraints for a given application. ThermalNet @cite uses a deep Q-network as an optimizer and uses an LSTM predictor for combustion optimization of a boiler in a power plant but does not handle domain constraints. Similar to our approach, ChemOpt @cite uses an RNN based optimizer for chemical reaction optimization but does not address aspects related to handling an unknown range for the function being optimized and incorporating domain constraints.
- dRRT* is a MAPF algorithm designed for continuous spaces @cite . It is a sample-based technique that is asymptotically complete and optimal. is optimal and complete, and is designed to run over a discrete graph. ORCA @cite @cite and ALAN @cite are also MAPF algorithms designed for continuous space. They are fast and distributed, but do not provide optimality or completeness guarantees.
- Zhang al @cite proposed a single image-based method that involved multi-column network to extract features at different scales. By utilizing filters with receptive fields of different sizes, the features learned by each column CNN are adaptive to variations in people head size due to perspective effect or image resolution. Onoro-Rubio and L 'o pez-Sastre in @cite addressed the scale issue by proposing a scale aware counting model called Hydra CNN to estimate the object density maps. Sam al @cite trained a Switching-CNN network to automatically choose the most optimal regressor among several independent regressors for a particular input patch. More recently, Sindagi and Patel @cite proposed Contextual Pyramid CNN (CP-CNN), where they demonstrated significant improvements by fusing local and global context through classification networks.
- The authors of @cite introduced the CRD framework which inspired this work. The main theorem (Theorem 2.2) of @cite is an analog of our Theorem and provides a similar bound the approximation error for recovery via IHT. First note that the statement of the Theorem 2.2 of @cite is missing the required hypothesis @math . This hypothesis appears in Lemma 3.6 of @cite , which is used to prove Theorem 2.2, but it appears to have been accidentally dropped from the statement of Theorem 2.2. We note that, by making the constants explicit, the proof of Lemma 3.6 of @cite gives the same restricted isometry property that we do in Theorem . Therefore, the guarantees we obtain for IHT are essentially the same as in @cite . The main difference is that, to derive recovery guarantees for IHT from the restricted isometry property, we utilize Theorem below (which is a modified version of Theorem 6.18 of @cite ) while the authors of @cite utilize Theorem 3.4 in @cite (which is taken from @cite ).
- Other works that provide guarantees include @cite and @cite where the authors frame the problem as one of regularizing the Lipschitz constant of a network and provide a lower bound on the norm of the perturbation required to change the classifier decision. The authors of @cite use robust optimization to perturb the training data and provide a training procedure that updates parameters based on worst case perturbations. A similar approach to @cite is @cite in which the authors use robust optimization to provide lower bounds on the norm of adversarial perturbations on the training data. In @cite , the authors use techniques from Differential Privacy @cite in order to augment the training procedure of the classifier to improve robustness to adversarial inputs. Another approach using randomization is @cite in which the authors add i.i.d Gaussian noise to the input and provide guarantees of maintaining classifier predictions as long as the @math -norm of the attack vector is bounded by a function that depends on the output of the classifier.
- Most defenses against adversarial inputs do not come with theoretical guarantees. Instead, a large body of research has focused on finding practical ways to improve robustness to adversarial inputs by either augmenting the training data @cite , using adversarial inputs from various networks @cite , or by reducing the dimensionality of the input @cite . For instance, @cite use robust optimization to make the network robust to worst case adversarial perturbations on the training data. However, the effectiveness of their approach is determined by the amount and quality of training data available and its similarity to the distribution of the test data. An approach similar to ours but without any theoretical guarantees is @cite . In this work, the authors use Generative Adversarial Networks (GANs) to estimate the distribution of the training data and during inference, use a GAN to reconstruct an input that is most similar to a given test input and is not adversarial.
- A number of works have been published on the stability, feasibility, and performance of linear tube MPC @cite @cite @cite . While this is an effective strategy to achieve robustness, decoupling the nominal MPC problem and controller design is suboptimal. Rakovi @cite showed that the region of attraction can be enlarged by parameterizing the problem with the open-loop trajectory and tube size. The authors presented the homothetic tube MPC (HTMPC) algorithm that treated the state and control tubes as homothetic copies of a fixed cross-section shape, enabling the problem to be parameterized by the tubes centers (i.e., open-loop trajectory) and a cross-section scaling factor. The work was extended to tubes with varying shapes, known as elastic tube MPC (ETMPC), but at the expense of computational complexity @cite . Both HTMPC and ETMPC possess strong theoretical properties and have the potential to significantly improve performance but a nonlinear extension has yet to be developed.
- Works with SHA-1 implementation on other hardware platforms can be found in @cite and @cite in which comparisons between Graphics Processing Units (GPUs) and CPUs were performed. The GPUs NVIDIA Tesla M2050 with @math CUDA cores and AMD FirePro V7800 with @math stream processors could achieve throughput peaks of up to @math Gbps.
- We begin code constructions existence results. present an explicit construction of MSR codes with small sub-packetization @math when the code rate @math is at most @math @cite . @cite show the existence of high rate MSR codes when the sub-packetization approaches infinity. Motivated by this result, the problem of designing high-rate MSR codes with finite sub-packetization level is explored in @cite @cite @cite @cite @cite @cite @cite @cite @cite and references therein. In particular, @cite show the existence of MSR codes with the sub-packetization level @math . Such a result with similar sub-packetization levels for repair of only @math systematic nodes was obtained earlier in @cite @cite . In order to ensure the MDS property, these results relied on huge fields and randomized construction of the parity check matrices.
- In two fascinating (independent) works, Ye and Barg @cite and Sasidharan, Vajha, and Kumar @cite give a fully explicit construction of MSR codes over small fields with sub-packetization level @math . These constructions also have the so-called or property, which means that the helper nodes do not have to perform any linear combinations on their data, and can simply transfer a suitable subset of @math coordinates of the vector in @math that they store. Thus the number of symbols at a node equals the number of symbols it transmits over the network to aid the repair (recall that the repair-bandwidth measures the latter amount).
- In summary, while there are several constructions of high rate MSR codes, they all incur large sub-packetization, which is undesirable as briefly explained earlier. This has been partially explained by lower bounds on @math in a few previous works. For the special case of optimal-access MSR codes, a lower bound of @math was shown in @cite , and this was improved (when all-node repair is desired) to @math recently @cite . Together with the above-mentioned constructions, we thus have matching upper and lower bounds on @math for the optimal-access case. This help-by-transfer setting is primarily combinatorial in nature, which is exploited heavily in these lower bounds.
- However, lower bounds for general MSR codes, that allow helper nodes to transmit linear combinations of their comments, are harder to obtain. Such a lower bound must rule out a much broader range of possible repair schemes, and must work in an inherently linear-algebraic rather than combinatorial setting. Note that the simple example presented above also used linear combinations in repairing one of the nodes. An MSR code construction with sub-packetization @math , which beats the above lower bound for optimal-access codes and thus shows a separation between these models, was given in @cite .
- Turning to known lower bounds on @math , a weak bound of @math was shown via a combinatorial argument in @cite . Using an elegant linear independence and partitioning argument, the following bound is proven in @cite : (This was slightly improved in @cite , but the improvement is tiny for the case when @math which is our focus.) The bound implies a lower bound on sub-packetization of @math . Even for the case @math , it was not known if one can achieve sub-packetization smaller than @math . Our Theorem now rules out this possibility. We conjecture that our bound can be improved to @math which will show that the construction in @cite is exactly tight.
- A slight relaxation MSR codes called @math -MSR codes where the helper nodes are allowed to transmit a factor @math more than the cutset bound, i.e., @math symbols, were put forth in @cite . They showed that one can construct @math -MSR codes with sub-packetization @math , and roughly logarithmic sub-packetization is also necessary.
- Regenerating and MSR codes have close connections to communication-efficient secret sharing schemes, which were studied and developed in @cite . In this context, the sub-packetization corresponds to the size of the shares that the parties must hold.
- In recent years, Convolutional Neural Networks(CNN) have demonstrated great efficacy on computer vision tasks such as classification @cite , localization @cite , and SRGAN @cite . Together with Recurrent Neural Networks(RNN), it has motivated the development of custom silicon for Deep Learning(DL). For example, GPU Tensor Core @cite , TPU @cite and Graphcore @cite .
- There has also been work to optimize DL on programmable logic. Notably, Song @cite proposed software-hardware co-design. While silicon implementations must customize for a range of DL applications, an FPGA can customize to a single DL application. This enables application specific customization of precision, sparsity and network structure. However, this is not the limit of FPGA customization. FPGAs can be further customized to a specific instance of a DL application by implementing post training parameters as constants. We call this a Compiled CNN or RNN.
- There has also been some research on intrusion detection and anomaly detection systems for IoT. A whitelist-based intrusion detection system for IoT devices (Heimdall) has been presented in @cite . Heimdall is based on dynamic profile learning and is designed to work on routers acting as gateways for IoT devices. The authors in @cite propose an intrusion detection model for IoT backbone networks leveraging two-layer dimension reduction and two-tier classification techniques to detect U2R (User-to-Root) and R2L (Remote-to-Local) attacks. In a recently published paper @cite , deep-autoencoders based anomaly detection has been used to detect attacks launched from IoT botnets. The method consists of extraction of statistical features from behavioral snapshots of normal IoT device traffic captures, training of a deep learning-based autoencoder (for each IoT device) on the extracted features and comparison of the reconstruction error for traffic observations with a threshold for normal-anomalous classification. The proposed detection method was evaluated on Mirai and BASHLITE botnets formed using commercial IoT devices.
- Fourth, we do not extract CnC communication features and use them to identify bot-CnC communications as done in @cite @cite @cite . This is because we aim to detect bots infected by Mirai-like IoT malware, towards which much simpler features can be used as discussed in Section . Fifth, unlike @cite , we aim to detect IoT bots much before the actual attack, during the scanning phase itself as explained in Section . Finally, most of the above cited works use quantifiers such as detection rate and false positive rates to evaluate the performance of their proposed botnet detection solutions. Instead, we use a quantity called average detection delay (defined in Section ) for the performance evaluation of our proposed bot detection solution since the features used by our solution eliminate the possibility of inaccurate detections or false positives. To the best of our knowledge, there are no existing papers on detecting IoT bots compromised by Mirai or its variants which exhibit port-based SYN scanning behavior.
- As the result of NNs' popularity, little attention has been paid to other AI approaches, such as symbolism, evolutionarism, or Bayesian statistics @cite . More recently, though, new studies emerge that show successful results in applying alternative approaches to AI tasks. For example, Denis G Willson at el. show that their evolutionary algorithm can outperform the deep neural network approach in playing Atari games @cite . More studies are targeting General AI as, for instance, the CYC project @cite . Some researchers advocate that the combination of different techniques into one powerful AI system is the way to go @cite .
- Recent works have made substantial progress in imposing diversity constraint on the latent space of a generative model. In particular, Liu et. al. @cite proposes the normalized diversification technique that effectively solves the problem of mode collapsing. Building on top of their prior work, we use a similar technique to learn an accurate encoding of action and state spaces in physical manipulation tasks. To our knowledge, our model is the first to use normalized diversification in these applications.
- In this paper we adopt Refinenet @cite as the baseline. The main difference between Refinenet and Unet @cite lies in the unique block "Refine Block". The Refine Block is a unique feature fusion block, which can be divided into three parts. (1) Residual Convolution Unit (RCU). This is a convolutional module based on the residual connection design. Compared with the original Resnet @cite , the BN layer is removed, and the parameter amount is reduced to be used as a feature extractor. (2) Multi-size fusion. Our task is semantic segmentation, with the output and the input in the same size, the blocks except for Block No.4 being dual input, and the two input in different scales. Thus multi-size fusion is applied for upsampleing and feature fusion. (3) Chain residual pooling(CRP). The module efficiently fuses features through convolution pooling operations of different window sizes. Through this chained pooling operation, the receptive field is expanded. At the same time, multi-scale information is merged through short jump connections, which let gradient go to directly from one module to another.
- Methods requiring labels generally use less training data as they can be more data efficient due to the better training signal that can obtained from labeled data. Examples include: InferSent which uses labelled entailment pairs, GenSen utilizing supervision from multiple tasks, and ParaNMT with paraphrase sentence pairs or conversational responses @cite .
- T he fifth generation of wireless cellular networks (5G) provides a wide range of services black with various requirements that should be guaranteed in the network @cite @cite . In the traditional network, dedicated and specific hardware equipment are required. Therefore, in order to provide a new service in these networks, it is necessary for each operator to purchase the hardware resources and install it on the network @cite .
- A dynamic service function chain deployment is proposed in @cite in which the authors consider a trade-off between resource consumption and operational overhead. In @cite , NF placement in the network is studied. Moreover, its impact on network performance with the aim of minimizing the cost of having virtual machines (VMs) In this paper, VM, node, and server have the same meaning. and the cost of steering traffic into servers are investigated. Service function chain (SFC) placement in the cloud-based network with the aim of minimizing end-to-end (E2E) latency of SFCs and enhancing QoS is investigated in @cite . An automated decentralized method for online placement and optimization of VMs in NFV-based network is proposed in 8501940 . In @cite , VNF embedding with the aim of minimizing physical machine and taking into consideration users' SFC requests and factors such as basic resource consumption and time-varying workload is studied.
- In @cite , an online scheduling and embedding algorithm by considering the capacity of available buffers and the processing time of each VNF is proposed for NFV. The authors propose a set of greedy algorithms and tabu search algorithm for mapping and scheduling. Moreover, cost, revenue, and acceptance ratio of these algorithms are compared together. VNF placement in a network with several mobile virtual network operators (MVNOs) are investigated in @cite in which the slice scheduling mechanism is introduced in order to isolate the traffic flow of MVNOs. In this paper, the goal is optimizing VNF placement based on the available radio resources. black Joint VNF placement and admission control (AC) are studied in @cite . In this paper, the aim is to maximize networks provider revenue in terms of bandwidth and capacity. black The authors in @cite , propose an RA algorithm which integrates placement and scheduling of VNF together. In @cite , a VNF scheduling problem is investigated and joint VNF scheduling and traffic steering is formulated as a mixed integer linear program. In the optimization problem, both the processing latency of VNFs and service chain transmission latency at virtual links are considered.
- Traditional core NLP research typically focuses on English newswire datasets such as the Penn Treebank @cite . In recent years, with the increasing usage of social media platforms, several NLP techniques and datasets for processing social media text have been proposed. For example, build a Twitter part-of-speech tagger based on 1,827 manually annotated tweets. annotated 800 tweets, and performed an empirical study for part-of-speech tagging and chunking on a new Twitter dataset. They also investigated the task of Twitter Named Entity Recognition, utilizing a dataset of 2,400 annotated tweets. annotated 929 tweets, and built the first dependency parser for tweets, whereas built the Chinese counterpart based on 1,000 annotated Weibo posts. To the best of our knowledge, question answering and reading comprehension over short and noisy social media data are rarely studied in NLP, and our annotated dataset is also an order of magnitude large than the above public social-media datasets.
- As shown in Theorem , the LRR gets the MSDR if and only if the subspaces are independent, that is, @math . This condition implies that each subspace does not intersect with the sum of other subspaces, or equivalently, @math , which is much stricter than that given in Theorem . It is proven by @cite that the iPursuit can separate two subspaces ( @math ) with a high probability. This is one of the special cases shown in Corollary . The condition for LRSSC is similar with that of SSC in the same form and is stricter. We omit the comparison the condition with that of LRSSC, but a detailed comparison with SSC is given below.
- For the SSC, @cite showed that if the samples are uniformly distributed in a union of subspaces @math and for the basis matrices @math of @math , @math with where @math is a give parameter, then SSC could give a block-diagonal solution partitioned as the ideal subspace segmentation with a probability approximately equal to one, depending on @math , @math , @math , and @math . We remark this claim does not imply a connected solution as we have explained in the early discussion or mentioned by @cite .
- The Two-view approach is derived merely from the relative camera poses from multiple views, called relative-pose constraints, without any additional assumptions of the scene. The epipolar constraint is such a constraint between two views @cite .
- Random Voting (RV) @cite , which is considered as the leading geometric method for motion segmentation partly because of its robustness to noise, has shown particularly successful results with a low computational cost. The algorithm, based on epipolar geometry, is an iterative process of randomized feature selection between two frames, estimating a fundamental matrix from the selected features and vote scores for the rest of the remaining features to be associated with a certain motion model. Since the method uses random initialization, it never loses any information even when the selected features do not represent a motion model. However, this approach only works well when the independent moving object is big enough, such that it consists of enough features to properly estimate the object's motion. In addition, objects in the scene need to be in a certain size so that the background object features ratio is not too high in order for the object's features to be selected in the randomized features selection. Finally, its accuracy rate results can vary due to the random initialization.
- The Multiview approach utilizes the trajectory of the feature points. PAC @cite and SSC @cite methods have quite accurate results in multiple motion cases in a sequence and are also robust to noise. However, those algorithms are extremely slow. Latent low-rank representation-based method (LatLRR) @cite is faster and more accurate, but this method becomes degraded in extremely noisy environments. The ICLM-based approach @cite is very fast, but has lower accuracy than other state-of-the-art approaches. In addition, while Multiview approaches are more accurate than Two-view approaches, they do not have good performance when there are only a few frames.
- Online Learning, or Online Convex Optimization, is an active research domain. In this section, we only summarize works which are directly related to ours. We refer the reader to comprehensive books @cite @cite and references therein for a more complete overview. The first no-regret algorithm has been given by . Subsequently, and gave improved algorithms with regret @math where @math is the size of the action space. However, these algorithms have running-time @math which is exponential in the size of the input for many applications, in particular for combinatorial optimization problems. An intriguing question is whether there exists a no-regret online algorithm with running-time polynomial in @math . proved that no such algorithm exists in general settings without any assumption on the structure. Designing online polynomial-time algorithms with approximation and vanishing regret guarantees for combinatorial optimization problems is a major research agenda.
- In their breakthrough paper, presented the first efficient online algorithm, called (FTPL), for linear objective functions. The strategy consists of adding perturbation to the cumulative gain (payoff) of each action and then selecting the action with the highest perturbed gain. This strategy has been generalized and successfully applied to several settings @cite @cite @cite @cite . Specifically, FTPL and its generalized versions have been used to design efficient online no-regret algorithms with oracles beyond linear settings: to submodular settings @cite and non-convex settings @cite . However, all these approaches require best-response oracles, and as we show in this paper, for several problems such best-response oracles require exponential time computation.
- Another direction is to design online learning algorithms using (offline polynomial-time) approximation algorithms as oracles. provided an algorithm which is inspired by Zinkevich's algorithm @cite (gradient descent): at every step, the algorithm updates the current solution in the direction of the gradient and project back to the feasible set using an approximation algorithm. They showed that given an @math -approximation algorithm for a optimization problem, after @math prediction rounds (time steps) the online algorithm achieves an @math -regret bound of @math using @math calls to the approximation algorithm per round in average. Later on, gave an algorithm with @math -regret bound of @math using only @math calls to the approximation algorithm per round in average. These algorithms rely crucially on the linearity of the objective functions and it remains an interesting open question to design algorithms for online non-linear optimization problems.
- In computer science, with the constant networking and middleware development, scheduling in distributed processing systems is one of the topics which has gained attention in the last two decades. Casavant and Kuhl @cite present a taxonomy of scheduling in general purpose distributed systems. The classification presented by the authors include local and global, static and dynamic, distributed and non-distributed, cooperative and non-cooperative scheduling, as well as some approaches to solve the problem, such as optimal and sub-optimal, heuristic, and approximate. This presented classification is complete in some sense, and it is still valid nowadays. However the current state of distributed systems indeed demands the addition of new branches in this taxonomy.
- Kwok and Ahmad @cite survey static scheduling algorithms for allocating tasks connected as directed task graphs (DAGs) into multiprocessors. The authors presented a simplified taxonomy for approaches to the problem, as well as the description and classification of @math scheduling algorithms. The DAG scheduling algorithms for multiprocessors have been adapted for scheduling in distributed systems, incorporating intrinsic characteristics of such systems for an enhanced performance. Therefore, Kwok and Ahmad presented static scheduling algorithms for multiprocessors, which are also applicable to distributed systems, and their classification. In this paper we review extensions of those algorithms as well as the their classification by including heterogeneous systems, dynamic scheduling algorithms, scheduling algorithms in modern distributed environments, and new scheduling techniques.
- In the last decades, after Kwok and Ahmad's work, other surveys and taxonomies for solutions to the scheduling problem for parallel systems have been developed. Most of these works focus on heterogeneous distributed systems @cite , which Ahmad and Kwok considered as one of the most challenging directions to follow @cite . Job scheduling strategies for grid computing are evaluated by in @cite . The authors present common scheduling structures, such as centralized, decentralized, and hierarchical. Within each scheduling structure, they present and evaluate @math processor selection strategies and three scheduling algorithms, namely (FCFS), , and . After this work, many dynamic scheduling strategies were developed to tackle with the grid dynamicity.
- As claim in their scheduling review @cite , parallel job scheduling reviews are needed in a regular basis. The purpose of their short review was to introduce clusters and grids into the parallel job scheduling literature. Indeed, the authors present an introduction to job scheduling in grids, highlighting differences between a parallel computer and the grid. They point out cross-domain load balancing and co-allocations as two main concerns when scheduling in grids. In our work, we introduce a classification of schedulers in distributed systems that comprises a more extensive view of grid computing algorithms. Moreover, we highlight new requirements for the cloud computing emergent paradigm as well as its differences to grid computing.
- @cite present a taxonomy in the scheduling problem for workflows considering multiple criteria optimization in grid computing environments. The authors separate the multi-criteria scheduling taxonomy in @math , namely , and , each facet describing the problem from a different point of view. These facets are expanded to classify existing works in a smaller granularity, pointing out where current research can be expanded and the work in each facet.
- We highlight two conclusions achieved by the authors in @cite which are touched by contributions given by our survey: (i) grid workflow scheduling problem is still not fully addressed by existing work''; and (ii) there are almost no workflow scheduling approaches which are based on an adaptive cost model for criteria''. As a contribution to (i), in this survey we expand the general distributed system scheduling to comprise it. As a contribution to (ii), we include scheduling taxonomies for utility grids and cloud computing environments.
- Recently, in independent work, @cite proposed using the Mahalanobis kernel in a similar way for heritability esitmation with GWAS data. 's paper primarily focuses on empirical analysis, using both simulated and real datasets to illsutrate advantages of the Mahalanobis kernel. The present work contains more precise mathematical and statistical justification for much of the work in @cite , and introduces statistical principles (e.g. @math -heritability in Section ) that can be extended to other targeted application areas and genetics (like partitioning heritability).
- Appropriate similarities between samples can improve the performances of the retrieval system. During the past decade, several well-known distance metric learning methods are proposed for various fields @cite @cite @cite @cite @cite @cite , such as ITML @cite , LMNN @cite , SVMs @cite , PCA @cite , LDA @cite , etc. These algorithms have been used for many computer vision and computer graphic tasks, such as classification, retrieval, correspondence, etc. These algorithms solve the problem that most features lie in a complex high-dimensional spaces where Euclidean metric is ineffective. However, most distance metric learning methods fail to integrate compatible and complementary information from multiple features to construct a distance metric. In order to explore more useful information for various applications, many researchers invest many methods to combine multi-view setting to distance metric learning algorithm. Kan @cite proposed a multi-view discriminant analysis as an extension of LDA, which has achieved excellent performances facing with multi-view features. Wu @cite proposed an online multi-modal distance metric learning which has been successfully applied in image retrieval.
- The interdisciplinary research of multimedia and sociology has been studied for many years @cite @cite . Popular topics include social networks discovery @cite , key actors detection @cite , group activity recognition @cite , and so on. In recent years, social recognition from images has attracted attention from researchers @cite @cite @cite @cite . For example, Zhang proposed to learn social relation traits from face images by CNNs @cite . Sun proposed a social relation dataset based on the social domain theory @cite and exploited CNNs to recognize social relations from a set of attributes @cite . Li proposed to an attention-based dual-glance model for social relation recognition, in which the first glance extracted features from persons and the second glance focused on contextual cues @cite . Wang proposed to model persons and objects in an image as a graph and perform relation reasoning by a Gated Graph Neural Network @cite . However, they only considered the co-existence of persons and objects in a scene but neglected global information and interactions among persons and objects that are important knowledge for social relation recognition. Therefore, we propose a Multi-Granularity Reasoning framework to explore complementary cues for social relation recognition.
- Data association is the process of dividing a set of instances into different groups, such that to maximize the global cross-group similarities while maintaining one-to-one association constraint. This fundamental technique exists in various domains that involve correspondence matching @cite , such as person re-identification @cite @cite @cite , keypoint matching @cite , 3D reconstruction @cite , action recognition @cite , and T-by-D based MOT @cite .
- Following the Generative Adversarial Networks proposed by @cite , deep learning models have been advancing for generative tasks for inpainting @cite , translation @cite , and editing @cite . GAN architecture can be simplified as the "game" between the generator network and the discriminator network. Generator adapts its parameters to create realistic images that mimic the distribution of the real data, and discriminator adapts its parameters to correctly differentiate real and fake images created by the generator. Inherently, all generative approaches suffer from the control over generation. In the context of GANs, this problem is mostly explored by Variational Autoencoders (VAE) and Conditional GANs to control the generation by putting constraints in the latent space @cite @cite . In addition to improving the control over GANs, other approaches improved the training efficiency, accuracy, and realism of GANs by deep convolutions @cite , Wasserstein distances @cite , least square @cite , and progressive growing @cite . All these advancements in the generative power, realism, and efficiency of GANs resulted in the development of "deep fake"s.
- In par with the increasing number of inauthentic facial images and videos, methods for detecting authenticity of such content have also been proposed. Those are mostly based on finding inconsistencies in images, such as detecting distortions @cite , finding compression artifacts @cite , and assessing image quality @cite . However, for synthetic images in our context, the noise and distortions are harder to detect due to the non-linearity and complexity of the learning process @cite . It is possible to investigate the color and noise distributions of specific networks @cite @cite , or training CNNs for synthetic images @cite @cite , but catching synthetic images and videos in the wild is still an open research topic. The last approach we would like to introduce in this domain, which can be considered as the most similar to ours as the only fake detector on videos, exploits blinks to detect inauthentic facial videos @cite as another biological signal.
- The Winograd convolution algorithm was first used to accelerate convnets by @cite . The authors derived several small fixed-size algorithms over the field of rationals based on the minimal filtering algorithm proposed by Winograd @cite , which achieve arithmetic complexity reductions ranging from @math x to @math x for the popular filter sizes.
- Joint embedding of image and text models have been increasingly popular in applications including image captioning @cite @cite @cite , question answering @cite , and information retrieval @cite @cite @cite . DeVise @cite is the first method to generate visual-semantic embeddings that linearly transform a visual embedding from a pre-trained deep neural network into the embedding space of textual representation. The method begins with a pre-trained language model, then optimizes the visual-semantic model with a combination of dot-product similarity and hinge rank loss as the loss function. After DeVise, several visual semantic models have been developed by optimizing bi-directional pairwise ranking loss @cite @cite and maximum mean discrepancy loss @cite . Maximizing CCA (Canonical Correlation Analysis) @cite is also a common way to acquire cross-modal representation. @cite address the problem of matching images and text in a joint latent space learned with deep canonical correlation analysis. @cite develop a canonical correlation analysis layer and then apply pairwise ranking loss to learn a common representation of image and text for information retrieval tasks. However, most image-text multi-modal studies focus on matching image and text. Few methods study the problem of unsupervised clustering of image-text pairs.
- addressed a related problem where they aim to cluster images by integrating the multimodal feature generation with the Locality Linear Coding (LLC) and co-occurrence association network, multimodal feature fusion with CCA, and accelerated hierarchical k-means clustering @cite . However, the text data they handled are tags instead of longer, noisy, and unreliable free-text descriptions as we do in MultiDEC. proposed EZLearn @cite , a co-training framework which takes image-text data and an ontology to classify images using labels from the ontology. This model requires prior knowledge of the data in order to derive an ontology; this prior knowledge is not always available, and can significantly bias the results toward the clusters implied by the ontology.
- Other papers involving usage of weights and discrete Morse theory include @cite , where weights are applied to different colors in the Red-Green-Blue (RGB) encoding. Discrete Morse theory is then used in combination with persistent homology for data analysis. In @cite , discrete Morse theory is used to extract the extremal structure of scalar and vector fields on 2D manifolds embedded in @math . Weights @math are assigned to the edges of the cell graph, followed by computing the sequence of maximum weight matchings. An algorithmic pipeline computes a hierarchy of extremal structures, where the hierarchy is defined by an importance measure and enables the user to select an appropriate level of detail.
- An early example of linear-time solvable special case for diameter computation is the class of interval graphs @cite . For every interval graph @math and for any integer @math , if we first compute an interval representation for @math in linear-time @cite then we can compute by dynamic programming, for every vertex @math , the contiguous segment of all the vertices at a distance @math from @math in @math . It takes almost linear-time and it implies a straightforward quasi linear-time algorithm for diameter computation. More efficient algorithms for diameter computation on interval graphs and related graph classes were proposed in @cite . Nevertheless we will show in what follows that interval orderings are a powerful tool for diameter computation on more general geometric graph classes.
- None of the approaches above consider the use of network coding @cite , and all are evaluated in single-path scenarios. Given the benefits that network coding brings to multipath communications in NDN @cite @cite @cite @cite , some approaches have been proposed to improve the benefits of caching in network coding enabled NDN architectures @cite @cite @cite . @cite and @cite propose optimal solutions to the problem of efficiently caching in network coding enabled NDN. However, both approaches need a central entity that is aware of the network topology and the Interests, which does not scale well with the number of network nodes. @cite is an eviction policy in which routers, before evicting a Data packet, apply network coding to the Data packet by means of combining it with other Data packets with the same name prefix that will remain in the cache. Due to the increased Data packet diversity in the network, the cache-hit rate is improved. However, in Interest aggregation and Interest pipelining are problematic, limiting the benefits that network coding brings to the NDN architecture.
- A typical domain adaptation @cite @cite @cite @cite problem consists of two domains: a well-labeled source domain and an unlabeled target domain. The two domains generally have the same label space but different data distributions @cite . Domain adaptation aims to mitigate the gap between the two data distributions, so that the knowledge, e.g., features and parameters, learned from the source domain can be transfered to the target domain. Recently, domain adaptation has been successfully applied to many real-world applications, such as image recognition @cite @cite , multimedia analysis @cite @cite and recommender systems @cite @cite .
- Since domain adaptation aims to mitigate the distribution gap between the source domain and the target domain, it is vital to find a metric which can measure the data distribution divergence. Maximum mean discrepancy (MMD) @cite is widely considered as a favorable criteria in previous work. For instance, deep adaptation networks (DAN) @cite generalizes deep convolutional neural networks to the domain adaptation scenario. In DAN, the general (task-invariant) layers are shared by the two domains and the task-specific layers are adapted by multi-kernel MMD. Furthermore, joint adaptation networks (JAN) @cite extends DAN by aligning the joint distributions of multiple domain-specific layers across domains based on a joint maximum mean discrepancy (JMMD) criterion.
- Recently, generative adversarial networks (GAN) @cite has been introduced into domain adaptation. Compared with the distribution alignment methods, adversarial domain adaptation models @cite @cite are able to generate domain invariant features under the supervision of a discriminator. For instance, adversarial discriminative domain adaptation (ADDA) @cite combines discriminative analysis, untied weight sharing and a GAN loss under a generalized framework. Coupled generative adversarial networks (CoGAN) @cite minimize the domain shifts by simultaneously training two GANs to handle the source domain and the target domain.
- The need for outdoor labelled point clouds has been addressed by a range of researchers. , @cite released the Paris-rue-Madame MLS dataset containing 20M points ( @math and reflectance), , @cite the iQmulus dataset containing 300M points ( @math , time, reflectance and number of echoes) and , @cite the Paris-Lille-3D containing 143.1M points ( @math , scanner @math , gps time, reflectance). However, many caveats exist within these datasets. For example, Paris-rue-Madame, whilst large enough for traditional machine learning algorithms (i.e. Support Vector Machines, Random Forest), does not meet the scale for a modern DNN, which number of parameters can easily exceed 10x the number of points available. The iQmulus is more suited in terms of size however due to a 2D semi-manual data labelling approach, many mislabelled ground truth points exist.
- Design of fault-tolerant bipartite graphs has potential applications in the design of flexible processes, where there are @math different request types and @math servers that should process them (see, for example, the work of @cite for a review of the topic).
- Nevertheless, the relation of our model with the process flexibility literature is still remote, since the process flexibility community has so far focused on systems where a server can process different kinds of compatible requests within the same time period. Moreover, only recently unbalanced systems, i.e., with @math , have started to be considered (see, @cite and @cite for examples).
- A related line of research is not to design the smallest fault tolerant graphs (in terms of any metric, like the ones given above), but to analyze the level of fault tolerance assured by prescribed topologies @cite @cite . This topic is of particular interest for algorithm design in high performance computing. Supercomputers are comprised of many processing nodes (with some local memory) that use an interconnection network to communicate during the execution of distributed algorithms. An algorithm delegates computational tasks to different nodes, and uses some logical topology for its message passing. This logical topology has to be somehow embedded in the interconnection network provided by the supercomputer. So it is of practical interest to study if the message passing topologies most common in algorithm design (like cycles, and trees of certain types) can still be embedded in interconnection topologies provided by supercomputers (often similar to hypercubes) when the system presents some faults @cite .
- A problem that is closely related to ours was presented by Perarnau and Petridis @cite . The authors studied the existence of perfect matchings in induced balanced subgraphs of random biregular bipartite graphs.
- While many works have studied scheduling for single-server queues, most of them focused on the extreme cases where job size is either completely unknown ( algorithms) or known perfectly. When the job size distribution is skewed---meaning that resources are occupied most of the time by a minority of large jobs---algorithms such as Least Attained Service (LAS) @cite or multi-level queues @cite @cite can still perform well by prioritizing new jobs. LAS is evaluated experimentally in this work; for more details about it, see .
- were the first to consider estimation errors for size-based scheduling, and observed that existing algorithms perform well only when job sizes were rather accurately estimated. Further work @cite @cite has shown that most problems happen when the job size distribution is skewed and large jobs' sizes are under-estimated: in that case, these jobs eventually reach a very high priority and are not preempted when smaller jobs arrive, clogging the system. PSBS @cite () and MCSS @cite () are proposals that perform better on estimated job sizes; in both are evaluated and compared to SPT.
- In the literature, some systems use job size estimation to drive scheduling. For batch computation systems, a part of jobs is run to estimate running time @cite @cite ; web servers use file size to estimate serving time @cite . More elaborate approaches predict the size of database queries @cite , MapReduce jobs @cite @cite @cite , deep learning training @cite and the length of call-center calls @cite : approaches such as these can be used to inform size-based schedulers.
- The traditional approach to compute distances between sequences (or time series, or trajectories) is to perform Dynamic Time Warping (DTW) @cite which was introduced in 1978. Since then, several improvements of the algorithm have been published, notably a fast version by Salvador @cite . DTW is considered one of the best metric to use for sequence classification @cite combined with @math -nearest neighbors. Recently, Abid @cite proposed a neural network architecture to learn the parameters of a warping distance accordingly to the euclidean distances in a projection space. However, DTW, as other shaped-based distances @cite , is only able to retrieve local similarities when time series have a relatively small length and are just shifted or not well aligned.
- Devising efficient representations of tweets, i.e., features, for performing clustering has been studied extensively. Most frequently used features for representing the text in tweets as numerical vectors are (BoWs) and (tf-idf) features @cite @cite @cite @cite @cite . Both of these feature extraction methods are based on word occurrence counts and eventually, result in a sparse (most elements being zero) document-term matrix. Proposed algorithms for clustering tweets into topics include variants of hierarchical, density-based and centroid-based clustering methods; k-means algorithm being the most frequently used one @cite @cite @cite .
- Numerous works on topic modeling of tweets are available as well. Topic models are generative models, relying on the idea that a given tweet is a mixture of topics, where a topic is a probability distribution over words @cite . Even though the objective in topic modeling is slightly different than that of pure clustering, representing each tweet as a topic vector is essentially a way of dimensionality reduction or feature extraction and can further be followed by a clustering algorithm. Proposed topic modeling methods include conventional approaches or variants of them such as Latent Dirichlet Allocation (LDA) @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite and Non-negative Matrix Factorization (NMF) @cite @cite . Note that topic models such as LDA are based on the notion that words belonging to a topic are more likely to appear in the same document and do not assume a distance metric between discovered topics.
- In contrary to abovementioned feature extraction methods which are not specific to representation of tweets but rather generic in natural language processing, various works propose custom feature extraction methods for certain health-related information retrieval tasks from Twitter. For instance, engineered sentiment analysis features to discover latent infectious diseases from Twitter @cite . In order to track public health condition trends from Twitter, specific features are proposed by Parker at al. employing Wikipedia article index, i.e., treating the retrieval of medically-related Wikipedia articles as an indicator of a health-related condition @cite . Custom user similarity features calculated from tweets were also proposed for building a framework for recommending health-related topics @cite .
- Metrics for evaluating the performance of clustering algorithms varies depending on whether the ground truth topic categories are available or not. If so, frequently used metrics are and . In the case of absence of ground truth labels, one has to use internal clustering criterions such as Calinski-Harabasz (CH) score @cite and Davies-Bouldin index @cite . provides an extensive comparative study of cluster validity indices @cite .
- Most of the currently available computational tools for protein-protein interactions are focusing on protein pairs and a comprehensible overview was published by Huang @cite . Some of the existing approaches, such as ArDock @cite , already combine the computational method with a basic visual representation of the predictions. There are even some solutions, such as DockingShop @cite , which are enabling the user to interactively design an initial configuration for a protein docking prediction process through a molecular graphics interface.
- One of the first tools designed primarily for multi-body docking was CombDock @cite . The algorithm works on a principle of hierarchical construction of the complex from smaller subunits and a greedy selection of the best-ranking subunits. The combinatorial step is followed by the reduction of solutions based on RMSD and a scoring function. Multi--LZerD @cite uses a genetic algorithm to generate complexes from initial pairwise docks and applies an energy minimization structure refinement procedure for the ranking of the solutions. @cite proposed an ant colony optimization approach to solve the combinatorial problem. DockStar @cite formulates the task of detecting the spatial conformation of a protein complex as an Integer Linear Program. Unlike other methods, it also integrates experimental data from mass spectrometry into the scoring of the solutions. Another tool reusing pairwise docks in combination with experimental data is PRISM-EM @cite . It uses density maps from cryo-electron microscopy for guiding the placement of subunits.
- Although these schematic representations are conveying the information about a single configuration, they do not support the comparison and interactive filtering of entire ensembles of configurations. This issue is addressed in the CoCoMaps @cite and COZOID @cite tools. Both tools come with linked visualizations, aiding the users in analyzing and comparing interactions between protein pairs. CoCoMaps and its successor CONS-COCOMAPS @cite enable to measure and visualize the consensus in multiple docking solutions and display the conservation of residue contacts using intermolecular contact maps. The COZOID tool uses a set of linked views for the interactive exploration of large ensembles of protein pairs, supporting a visual drilldown approach for narrowing down the set of possibly relevant configurations. The main limitation of these approaches is that they are operating only on protein pairs (i.e., single ) and cannot be directly applied to multi-body complexes. The multiscale aspect in molecular visualization can be explored on different granularity levels, as shown in the recent survey of @cite .
- In our case, we were not only concerned with designing proper visual representations of the individual hierarchy levels of large ensembles of multi-body complexes, but also with how to interactively explore and filter these ensembles to support the identification of biochemically most relevant instances. @cite focus on the problem of interactive visual steering of hierarchical simulation ensembles. In their substantially different application case, they also deal with linking representations on different levels of detail as well as with the challenge that the ensemble can grow during the exploration process.
- MSER @cite and SWT @cite are classical text component extraction methods. In the era of deep learning, CTPN @cite extracts horizontal text components with fixed-size width using a modified Faster R-CNN framework. Horizontal text lines are easily generated, since CTPN adjusted the Faster R-CNN @cite framework to output dense text components. SegLink @cite proposed a kind of oriented text component (i.e. segment) and a component-pair connection structure (i.e. link). A link indicates which two segments should be connected. Naturally, SegLink dealt better with multi-oriented texts than CTPN. PixelLink @cite provided an instance segmentation based solution that detects text pixels and their linkage with neighbor pixels. Positive pixels with positive links are grouped as the collection of connected components. Besides, Markov Clustering Network @cite regarded detected text pixels as nodes and associated them with computed attractors by a designed markov clustering networks. The above mentioned methods provided inspiring ideas on text detection. However, the regions between characters are sometimes in-discriminative with background in some cases, especially in text lines where distances between characters are large.
- Recently, quite a few works @cite @cite @cite @cite @cite @cite @cite @cite have put emphasis on adjusting some popular object detection frameworks including Faster R-CNN @cite , SSD @cite and Densebox @cite to detect word boundary. In contrast to general objects, texts appearing in the real-world have larger varieties of aspect ratios and orientations. @cite and @cite directly added more anchor boxes of large aspect ratio to cover texts of wider range. @cite and @cite added the angle property to the bounding box to deal with the problem of multiple orientations, while EAST @cite and @cite provided a looser representation namely quadrangle. These methods seem to easily achieve high performance on benchmarks with word-level annotations, but not on non-Latin scripts or curved text with polygon-level annotations.
- The goal of metric learning or embedding methods @cite @cite @cite is to learn a function that measures how similar two samples are. There are many successful applications of metric learning @cite @cite @cite @cite , such as ranking, image retrieval, face verification, speaker verification and so on. By far, applications of metric learning on document analysis or text reading were limited to the problem of word spotting and verification @cite @cite @cite . In this work, we verify the effectiveness of deep metric learning in text detection task. Based on character candidates, we provide an end-to-end trainable network that can output the character bounding boxes and their embedding vectors simultaneously. Text regions could be easily detected by grouping characters which embedding distances are small.
- A considerable contribution to this area is http: wiki.librec.net doku.php , a Java-based library that, so far, comprises around 70 resource recommendation algorithms and evaluation modules @cite . Another Java-based, open-source framework is http: ranksys.org , which focuses on the evaluation of ranking problems and supports the investigation of novelty as well as diversity for academic research @cite , which is reflected in its design (e.g., data input interfaces work with a triple of user, item and features).
- Other examples of open-source recommender software are http: www.mymedialite.net , an item recommender library that focuses on rating and ranking predictions in collaborative filtering approaches @cite , https: github.com irecsys CARSKit , a recommendation library specifically designed for context-aware recommendations, and http: www.libfm.org tagrec.html , a software component that implements Tensor Factorization models for personalized tag recommendations in C++ @cite .
- Using exact but non-rigorous methods from statistical physics, @cite @cite determines the critical values for @math and @math at which it becomes information-theoretically possible to reconstruct the membership into clusters better than chance. Rigorous results on this model are given in @cite where bounds on the critical values are obtained. The precises thresholds were then determined in @cite . Our analysis builds on the techniques derived in this last reference with two main modifications: additional work is required to compute the classification accuracy (as opposed to the mean squared error) and to incorporate the side information.
- To the best of our knowledge, there are much fewer theoretical works dealing with a semi-supervised setting. @cite studies a mixture model where the estimation problem is essentially reduced to the one of estimating the mixing parameter and shows that the information content of unlabeled examples decreases as classes overlap. More closely related to our work, @cite provides the first information theoretic tight analysis for inference of latent community structure given a dense graph along with high dimensional node covariates, correlated with the same latent communities. @cite studies a class of graph-oriented semi-supervised learning algorithms in the limit of large and numerous data similar to our setting.
- In contrast, there are a number of practical works and proposed algorithms for semi-supervised learning based on transductive models @cite , graph-based method @cite or generative modeling @cite , see the surveys @cite and @cite . SSL methods based on training a neural network by adding an additional loss term to ensure consistency regularization are presented in @cite , @cite , @cite . We refer in particular to the recent work @cite for an overview of these SSL methods (currently the state-of-the-art for SSL on image classification datasets). The algorithm MixMAtch introduced in @cite obtains impressive results on all standard image benchmarks. Given these recent improvements, natural questions arise: what is the best possible achievable performance? to what extend can we generalize those improvement to other domains? We believe that our work is a first step in a theoretical understanding of these questions.
- Point cloud feature learning for 3D object detection. There are generally three ways of learning features from point cloud for 3D detection. @cite @cite @cite @cite @cite projected point cloud to bird-view map and utilized 2D CNN for feature extraction. @cite @cite @cite conducted PointNet @cite @cite to learn the point cloud features directly from raw point cloud. @cite proposed VoxelNet and @cite applied sparse convolution to speed up the VoxelNet for feature learning. Inspired by VoxelNet, we designed a UNet-like @cite backbone network by using sparse convolution and deconvolution to extract discriminative point features for predicting intra-object part locations and 3D object detection.
- Much empirical research focuses on either rating stars treating the scores as numerical data or explainable text generation offering the features of the POIs, and those studies adopt different models, features and evaluation methods @cite @cite @cite @cite @cite @cite . This project is motivated by these empirical studies on rating stars prediction, and we proposed an integrated model which can predict rating stars and generate explainable opinion-aspect pairs for users.
- 0.05in Collaborative Filtering is commonly used in recommender system @cite @cite @cite @cite --- exploiting similarity among the preference of users to generate recommendations. Research is done using users historical data to predict ratings, and deliver the results of recommendation to users @cite @cite . The traditional methods in rating prediction are the user-based model, and matrix factorization(MF). According to @cite , they introduced the user-based method which predicts the rating scores based on the other users who have similar preferences. Regarding matrix factorization, mentioned the applications of two commonly used methods: Singular Value Decomposition(SVD) and Non-negative Matrix Factorization(NMF) @cite . Additionally, many researchers utilized neural-based collaborative filtering model for rating prediction @cite @cite . However, there are many limitations for the traditional algorithms such as scalability problems, and lacking bias terms. As for neural collaborative filtering, it performs better on rating prediction, but there is no description of generating explainable text in empirical works.
- A considerable body of work is dedicated to automatic photo enhancement. However, it traditionally only focused on a specific subproblem, such as super-resolution, denoising, deblurring, or colorization. All of these subproblems are tackled simultaneously when we generate plausible high-quality photos from low-end ones. Furthermore, these older works commonly train with artifacts that have been artificially applied to the target image dataset. Recreating and simulating all the flaws in one camera given a picture from another is close to impossible, therefore in order to achieve real-world photo enhancement we use the photos simultaneously captured by a capture rig from Ignatov @cite . Despite their limitations, the related works contain many useful ideas, which we briefly review in this section.
- is the task of increasing the resolution of an image, which is usually trained with down-scaled versions of the target image as inputs. Many prior works have been dedicated to doing this using CNNs of progressively larger and more complex nature @cite @cite @cite @cite @cite @cite . Initially, a simple pixel-wise mean squared error (MSE) loss was often used to guarantee high fidelity of the reconstructed images, but this often led to blurry results due to uncertainty in pixel intensity space. Recent works @cite aim at perceptual quality and employ losses based on VGG layers @cite , and generative adversarial networks (GANs) @cite @cite , which seem to be well suited to generating plausible-looking, realistic high-frequency details.
- In , the aim is to hallucinate color for each pixel, given only its luminosity. It is trained on images with their color artificially removed. Isola @cite achieve state of the art performance using a GAN to solve the more general problem of image-to-image translation.
- aim to remove optical distortions from photos that have been taken out of focus, while the camera was moving, or of faraway geographical or astronomical features. The neural models employed are CNNs, typically trained on images with artificially added blur or haze, using a MSE loss function @cite @cite @cite @cite @cite . Recently, datasets with both hazy and haze-free images were introduced @cite and solutions such as the one of Ki @cite were proposed, which use a GAN, in addition to L1 and perceptual losses. Similar techniques are effective for as well @cite @cite @cite @cite .
- The use of GANs has progressed towards the development of general purpose image-to-image translation. Isola @cite propose a conditional GAN architecture for paired data, where the discriminator is conditioned on the input image. Zhu @cite relax this requirement, introducing the cycle consistency loss which allows the GAN to train on unpaired data. These two approaches work on many surprising datasets, however, the image quality is too low for our purpose of photo-realistic image enhancement. This is why Ignatov introduce paired @cite and unpaired @cite GAN architectures that are specially designed for this purpose.
- The DPED dataset @cite consists of photos taken simultaneously by three different cell phone cameras, as well as a Canon 70D DSLR camera. In addition, these photographs are aligned and cut into 100x100 pixel patches, and compared such that patches that differ too much are rejected. In this work, only the iPhone 3GS data is considered. This results in 160k pairs of images.
- Compared with the serial TDMA scheme, concurrent transmission scheduling can significantly increase the system throughput, and thus has been extensively studied @cite , @cite - @cite . Cai @cite proposed a scheduling algorithm based on exclusive region to support concurrent transmissions. To maximize the number of flows scheduled in the network so that the QoS requirement of each flow is satisfied, Qiao . @cite proposed a flip-based scheduling algorithm. In @cite , Zhu proposed a Maximum QoS aware Independent Set (MQIS) based scheduling algorithm for mmWave backhaul networks to maximize the number of flows with their QoS requirements satisfied. In MQIS, the concurrent transmission and the QoS aware priority are exploited to achieve more successfully scheduled flows and higher network throughput. In @cite , based on Stackelberg game, Li proposed a distributed transmission power control solution for the concurrent transmission scheduling between interference D2D links to further enhance the network throughput. Niu @cite proposed an energy efficient scheduling scheme for the mmWave backhaul network, which exploits concurrent transmissions to achieve higher energy efficiency. However, all the above scheduling algorithms assume the devices are HD.
- Recently, the development of SI cancelation technology has made FD communication possible. Jain @cite proposed the signal inversion and adaptive cancelation. Combining signal inversion cancelation with digital cancelation can reduce SI by up to 73dB. Everett @cite showed the BS could exploit directional diversity by using directional antennas to achieve additional passive suppression of the SI. Besides, Miura @cite proposed a novel node architecture introducing directional antennas into FD wireless technology. Rajagopal @cite proved enabling backhaul transmission on one panel while simultaneously receiving backhaul on an adjacent panel is attainable for next generation backhaul designs. In @cite , Xiao showed the configuration with separate Tx Rx antenna arrays appeared more flexible in SI suppression, and proposed the beamforming cancelation in FD mmWave communication.
- Considering the potential of the FD communication in increasing network performance, Feng @cite proposed a design framework for 5G mmWave backhaul, which combined FD transmissions and hybrid beamforming with routing and scheduling schemes. However, the scheduling solution in @cite was for the system with sufficient TS resources and aimed at accomplishing all of the transmissions with the minimum time. Thus, there was no special consideration for the QoS requirements of flows in limited time. Therefore, for mmWave backhaul networks with limited TS resources, a more QoS-favorable FD scheduling algorithm is needed.
- In the case of infinite horizon (e.g. the plain @math arrangement of the spherical scatterers of diameter less than the lattice spacing) the free flight distribution of a particle flying in a uniformly sampled random direction has a heavy tail which causes a different type of long time behaviour of the particle displacement. The arguments of @cite indicated that in the two-dimensional case super-diffusive scaling of order @math is expected. A central limit theorem with this anomalous scaling was proved with full rigour in @cite , for the Lorentz-particle displacement in the @math -dimensional periodic case with infinite horizon. The periodic infinite horizon case in dimensions @math remains open.
- In @cite and @cite it is proved that in the Boltzmann-Grad limit the trajectory of the Lorentz particle in any compact time interval @math with @math fixed, converges weakly to a non-Markovian flight process which has, however, a complete description in terms of a Markov chain of the successive collision impact parameters and, conditionally on this random sequence, independent flight lengths. (For a full description in these terms see @cite .) As a second limit, an invariance principle is proved in @cite for this non-Markovian random flight process, with superdiffusive scaling @math . Note that in this case the second limit doesn't just drop out from Donsker's theorem as it did in the random scatterer setting. The results of @cite are valid in @math while those of @cite and @cite in arbitrary dimension.
- Named Entity Recognition (NER), for which the goal is to discover mention-boundaries in addition to typing, often using a small set of mutually exclusive types, has a considerable amount of work @cite @cite @cite @cite @cite .
- There is a handful of works aiming to pave the road towards zero-shot typing by addressing ways to extract cheap signals, often to help the supervised algorithms: e.g., by generating gazetteers @cite , or using the anchor texts in Wikipedia @cite @cite . Ren2016AFETAF project labels in high-dimensional space and use label correlations to suppress noise and better model their relations. In our work, we choose not to use the supervised-learning paradigm and instead merely rely on a general entity linking corpus and the signals in Wikipedia. Prior work has already shown the importance of Wikipedia information for NER. use a cross-lingual to facilitate cross-lingual NER. However, they do not explicitly address the case where the target entity does not exist in Wikipedia.
- The resource provisioning from cloud computing infrastructures using Spot Instances or similar mechanisms has been addressed profusely in the scientific literature in the last years @cite . However, the vast majority of this work has been done from the users' perspective when using and consuming Spot Instances @cite and few works tackle the problem from the resource provider standpoint.
- Due to the unpredictable nature of the Spot Instances, there are several research papers that try to improve the task completion time ---making the task resilient against termination--- and reduce the costs for the user. @cite propose a probabilistic model to obtain the bid prices so that the costs and performance and reliability can be improved. In @cite @cite @cite @cite the task checkpointing is addressed so as to minimize costs and improve the whole completion time.
- Related with the previous works, have studied the usage of Spot Instances to deploy reliable virtual clusters @cite @cite , managing the allocated instances on behalf of the users. They focus on the execution of compute intensive tasks on top of a pool of Spot Instances, in order to find the most effective way to minimize both the execution time of a given workload and the price of the allocated resources. Similarly, in @cite the autors develop a workflow scheduling scheme that reduces the completion time using Spot Instances.
- Regarding Big Data analysis, several authors have studied how the usage of Spot Instances could be used to execute MapReduce workloads reducing the monetary costs, such as in @cite @cite . The usage of Spot Instances for opportunistic computing is another usage that has awaken a lot of interest, especially regarding the design of an optimal bidding algorithm that would reduce the costs for the users @cite @cite . There are already existing applications such as the vCluster framework @cite that can consume resources from heterogeneous cloud infrastructures in a fashion that could take advantage of the lower price that the Spot Instances should provide.
- @cite delivered an implementation of preemptible instances for the Nimbus toolkit in order to utilize those instances for backfilling of idle resources, focusing on HTC fault-tolerant tasks. However, they did not focus on offering this functionality to the end-users, but rather to the operators of the infrastructure, as a way to maximize their resource utilization. In this work, it was the responsibility of the provider to configure the backfill tasks that were to be executed on the idle resources.
- Nadjaran have developed a Spot Instances as a Service (SIPaaS) framework, a set of web services that makes possible to run a Spot market on top of an OpenStack cloud @cite . However, even if this framework aims to deliver preemptible instances on OpenStack cloud, it is designed to utilize normal resources to provide this functionality. SIPaaS utilizes normal resources to create the Spot market that is provided to the users by means of a thin layer on top of a given OpenStack, providing a different API to interact with the resources. From the CMF point of view, all resources are of the same type, being SIPaaS the responsible of handling them, in different ways. In contrast, our work leverages two different kind of instances at the CMF level, performing different scheduling strategies depending on which kind of resource it is being requested. SIPaaS also delivers a price market similar to the Amazon EC2 Spot Instances market, therefore they also provide the Ex-CORE auction algorithm @cite in order to govern the price fluctuations.
- have proposed @cite a capacity planning method combined with an admission service for IaaS cloud providers offering different service classes. This method allows providers to tackle the challenge of estimating the minimum capacity required to deliver an agreed Service Level Objective (SLO) across all the defined service classes. In the aforementioned paper lean on their previous work @cite @cite , where they proposed a way to reclaim unused cloud resources to offer a new class. This class, in contrast with the preemptible instances described here, still offer a SLO to the users, being the work on focused on the reduction of the changes that the SLO is violated due to an instance reclamation because of a capacity shortage.
- Generally speaking, existing Cloud Management Frameworks (CMFs) do not implement full-fledged queuing mechanism as other computing models do (like the Grid or traditional batch systems). Clouds are normally more focused on the rapid scaling of the resources rather than in batch processing, where systems are governed by queuing systems @cite . The default scheduling strategies in the current CMFs are mostly based on the immediate allocation or resources following a fist-come, first-served basis. The cloud schedulers provision them when requested, or they are not provisioned at all (except in some CMFs that implement a FIFO queuing mechanism) @cite .
- However, some users require for a queuing system ---or some more advanced features like advance reservations--- for running virtual machines. In those cases, there are some external services such as Haizea @cite for OpenNebula or Blazar https: launchpad.net blazar for OpenStack. Those systems lay between the CMF and the users, intercepting their requests and interacting with the cloud system on their behalf, implementing the required functionality.
- Besides simplistic scheduling policies like first-fit or random chance node selection @cite , current CMF implement a scheduling algorithm that is based on a rank selection of hosts, as we will explain in what follows:
- * Image importance and object saliency. The problem of deciding which components in an image are important has been studied intensively. The main approaches involved identifying characteristics of objects and images that could contribute to importance, and use labeled data for predicting object importance. Elazary and Itti @cite considered the order of object naming in the LabelMe dataset @cite as a measure of the interest of an object and compare that to salient locations predicted by computational models of bottom-up attention. The elegant work of Spain and Perona @cite examined which factors can predict the order in which objects will be mentioned given an image. @cite characterized factors related to semantics, to composition and to the likelihood of attribute-object, and investigated how these affected the measures of importance. @cite focused on predicting entry-level classes using a supervised approach. These studies also make it clear that the object saliency is strongly correlated with its perceived importance @cite @cite .
- Rather, we look at all the feature maps and remove the maps that are dynamically determined not to be participating in the classification. Rhu @cite recently described a compressing DMA engine (cDMA) that improved virtualized DNNs (vDNN) performance by 32 technique prunes by channel rather than elements. This benefits instruction set processors, particularly signal processors, because data can be easily loaded into the processor using sliding windows.
- Previous works have investigated various kinds of security issues in App markets. Chen @cite and Rahman @cite analyzed malware dissemination in Google Play. Zhu @cite and Chen @cite studied the suspicious Apps involved in search ranking in iOS App Store. @cite delved crowdsourced spam reviews in both Google Play and iOS App Store. @cite gave a longitudinal analysis of Apps in Google Play and provided suggestions on detecting search ranking fraud. According to @cite , Google Play does not eliminate all fake downloads. Moreover, few previous works have investigated this problem either. It is mainly due to the lack of the ground truth of fraud activities. The data crawled from the front end, which has limited information, also hinders previous work for a comprehensive study on the download fraud. In this work, with server-side data and device vendor information as the ground truth, we could take a holistic approach to probe the download fraud in App market.
- The outcome of download fraud is similar to click fraud, which is a type of fraud that occurs in pay-per-click online advertising @cite . Click fraudsters usually inject fake clicks to target URLs using click bots and steal money from advertisers. To detect click fraud, @cite employed peer-to-peer measurements, command-and-control telemetry, and contemporaneous click data to analyze click fraud on botnets. @cite devised various temporal and statistical patterns to detect click fraud in online advertising. @cite leveraged behavior features and click patterns to detect spam URL sharing. The download fraud we investigated in this paper is more complicated than click fraud (i.e., mixed with human and bot activities). Inspired by the click fraud detection works mentioned above, we propose to model the download fraud activities in a multiview and feature-based perspective.
- For the black markets investigation, several works @cite @cite @cite @cite have probed the crowdsourcing websites and devised machine learning approaches to detect crowdturfing campaigns and crowd workers. Other works like @cite inspected the transactions over trading App reviews and @cite investigated the crowd fraud in Internet advertisement. However, seldom previous work has studied the black markets targeting download fraud. Like @cite @cite @cite , we launch a honeypot App in App market to acquire reliable ground truth of download fraud activities. Moreover, we infiltrate into the black market and reap useful information from fraudsters to help our analysis.
- The idea of GAN @cite is to generate realistic samples through the adversarial game between generator @math and discriminator @math . GAN becomes popular owing to its ability to achieve unsupervised learning. However, GAN also encounters many problems such as instability and model collapsing. Hence later methods @cite @cite @cite try to improve GAN in both the aspects of implementation and theory. DCGAN @cite provides a new framework that is more stable and easier to train. WGAN @cite suggests to use Wasserstein distance to measure the loss. WGAN-GP @cite further improves the way of the Lipschitz constraint being enforced, by replacing weight clipping with gradient penalty.
- To reduce the burden of @math , Denton al @cite use a pyramid structure and Karras al @cite consider a progressive training methodology. Both of them divide the task into smaller sequential steps. In our case, we alleviate the burden of @math by incorporating some well-defined image processing operations into the network model, , converting background color into grayscale to simulate the visual effect of color selectivo , or blurring the background to create the Bokeh effect.
- Computer vision problems may benefit from GAN by including an adversarial loss into, say, a typical CNN model. Many intricate tasks have been shown to gain further improvements after adding adversarial loss, such as shadow detection @cite , saliency detection @cite , and semantic segmentation @cite . However, those training methodologies require paired images (with ground-truth) and hence lack the advantage of unsupervised learning. For the applications of modifying photo styles, some methods @cite @cite @cite can successfully achieve image-to-image style transfer using unpaired data, but their results are limited to subjective evaluation. Moreover, those style-transfer methods cannot be directly applied to the task of unsupervised segmentation.
- Most of the existing segmentation methods that are based on deep neural networks (DNNs) to treat the segmentation problem as a pixel-level classification problem @cite @cite @cite . The impressive performance relies on a large number of high-quality annotations. Unfortunately, collecting high-quality annotations at a large scale is another challenging task since it is exceedingly labor-intensive. As a result, existing datasets just provide limited-class and limited-annotation data for training DNNs. DNN-based segmentation methods thus can only be applied to a limited subset of category-dependent segmentation tasks.
- To reduce the dependency of detailed annotations and to simplify the way of acquiring a sufficient number of training data, a possible solution is to train DNNs in a semi-supervised manner @cite @cite or a weakly-supervised manner @cite @cite @cite with a small number of pixel-level annotations. In contrast, our model is trained without explicit ground-truth annotations.
- Existing GAN-based segmentation methods @cite @cite improve their segmentation performance using mainly the adversarial mechanism of GANs. The ground-truth annotations are needed in their training process for constructing the adversarial loss, and therefore they are GAN-based but not unsupervised'' from the perspective of application and problem definition.
- For natural image and video analysis problems, a number of pretext tasks have been explored, including prediction of image rotation @cite , relative position @cite , colorisation @cite and image impainting @cite etc. In medical imaging domain, self-supervised learning has also been explored but to a less extent. proposed a pretext task for subject identification @cite . A Siamese network was trained to classify whether two spinal MR images came from the same subject or not. The pretrained features were used to initialise a disease grade classification network. defined re-colourisation of surgical videos as a pretext task and used the pretrained features to initialise a surgical instrument segmentation network @cite . used rotation prediction as a pretext task and the self-learnt features were transferred to lung lobe segmentation and nodule detection tasks @cite . Different from previous works in the medical imaging domain, we propose a novel pretext task, which is to predict anatomical positions. In particular, we leverage the rich information encoded in the cardiac MR scan view planes and DICOM headers to define the anatomical positions for the task.
- The approaches proposed in Mudflow @cite and Chabada @cite are closely related to ours. Mudflow @cite is a tool for malware detection based on sensitive information flow. Similar to our approach, they rely on static taint analysis to detect flows of sensitive data towards potential leaks. Then, these flows are used to train a @math SVM one-class classifier and later classify new apps. While we also use static analysis, the main difference is that we consider the dominant topic inferred from app description as an important feature for the classification. Our empirical evaluation shows that dominant topics are fundamental to achieve a higher in anomaly detection. Moreover, our approach not only focuses on detecting malware, but also focuses on vulnerable and defective apps. Lastly, while Mudflow applies intra-component static analysis, we use inter-component analysis for covering flows across components.
- Chabada @cite is a tool to find apps whose descriptions differ from their implementations. While we apply similar techniques in terms of natural language processing of apps descriptions, the goals differ. The goal of their approach is to find anomalous apps among the apps in the wild based on the inconsistencies between the advertised apps descriptions and their actual behaviors. By contrast, our approach specifically targets at identifying anomalies in the flow of sensitive information in the app code. More specifically, Chabada only identifies calls to sensitive APIs to characterize benign and anomalous apps. By contrast, we consider data flows from sensitive data sources to sensitive sinks.
- Information leak in mobile apps is a widespread security problem. Many approaches that deal with this security problem are related to ours. Information flow in mobile apps is analysed either statically @cite , @cite , @cite , @cite , @cite , @cite , @cite or dynamically @cite , to detect disclosure of sensible information. Tainted sources are system calls that access private data (e.g., global position, contacts entries), while sinks are all the possible ways that make data leave the system (e.g., network transmissions). An issue is detected when privileged information could potentially leave the app through one of the sinks. In the following, we discuss some of these approaches and then explain the major differences.
- Other closely related work is about detecting permission re-delegation vulnerabilities in apps. @cite presented the permission re-delegation problems, and their approach detects them whenever there exists a path from a public entry point to a privileged API call. @cite and @cite also detect permission re-delegation vulnerabilities. However, as acknowledged by and , their approaches cannot differentiate between legitimate and illegitimate permission re-delegation behaviors.
- @cite proposed Appsealer, a runtime patch to mitigate permission re-delegation problem. They perform static data flow analysis to determine sensitive data flows from sources to sinks and apply a patch before the invocations of privileged APIs such that the app alerts the user of potential permission re-delegation attacks and requests the user's authorization to continue. This is an alternative way of distinguishing behaviors and abnormal ones by relying on the user. @cite also proposed a similar approach but they extended the Android framework to track ICC vulnerabilities instead of patching the app. Instead of relying on the user, who might not be aware of security implications, we resort to a model that reflects normal information flow behaviors to detect anomalies in the flow of sensitive information.
- OpenAI published an interesting robotics application of one-shot learning, described in @cite . Their algorithm, named , uses a meta-learning framework for training robotic systems in performing certain tasks based only on a couple of demonstrations. Also, the large amount of data required by Deep Reinforcement Learning systems has been approached in @cite through the introduction of their algorithm.
- One of the most influential work on GOL is the research on (GAN) @cite . The major difference between GOL and the adversarial nets is that, within GOL, the generation of synthetic information is performed based on the generalization functions which generate new data from a one-shot object, thus making GOL a pure one-shot learning framework.
- One of the most popular deep learning methods is the social LSTM @cite model which represents the pedestrians in the local neighbourhood using LSTMs and then generates their future trajectory by systematically pooling the relavant information. This removes the need for handcrafted features and learns the required feature vectors automatically through the encoded trajectory representation. This architecture is further augmented in @cite where the authors propose a more efficient method to embed the local neighbourhood information via a soft and hardwired attention framework. They demonstrate the importance of fully capturing the context information, which includes the short-term history of the pedestrian of interest as well as their neighbours.
- Although mixture GANs seem to further get rid of problems like mode collapse, they exhibit two drawbacks. Firstly, mixing weights ( mode distribution) @math in mixture GANs, for example in MGAN and MixGAN @cite @cite , are fixed. Such a fixed mixing scheme limits the flexibility of model distribution @math , probably leading to an undesired large divergence between real and model distributions. Besides, @math sometimes is predefined, such as in @cite , which constraints the model capacity, and thus causes mode dropping. Secondly, some mixture GANs @cite plausibly encourage mode diversity of generated samples, resulting in intra-class mode dropping.
- We discuss related work on suboptimal minima of the loss surface. In addition, we refer the reader to the overview article @cite for a discussion on the non-convexity in neural network training.
- It is known that learning the parameters of neural networks is, in general, a hard problem. Blum and Rivest @cite prove NP-completeness for a specific neural network. It has also been shown that local minima and other critical points exist in the loss function of neural network training (see e.g. @cite @cite @cite @cite @cite @cite ). The understanding of these critical points has led to significant improvements in neural network training. This includes weight initialization techniques (e.g. @cite ), improved backpropagation algorithms to avoid saturation effects in neurons @cite , entirely new activation functions, or the use of second order information @cite @cite .
- That suboptimal local minima must become rather degenerate if the neural network becomes sufficiently large was observed for networks with one hidden layer in @cite and @cite . Recently, Nguyen and Hein @cite generalized this result to deeper networks containing an extremely wide hidden layer. Our contribution can be considered as a continuation of this work.
- To gain better insight into theoretical aspects, some papers consider linear networks, where the activation function is the identity. The classic result by Baldi and Hornik @cite shows that linear two-layer neural networks have a unique global minimum and all other critical values are saddle points. Kawaguchi, @cite , Lu and Kawaguchi @cite and @cite discuss generalizations of @cite to deep linear networks.
- Finally, worth mentioning is the study of Liao and Poggio @cite who use polynomial approximations to argue, by relying on Bezout's theorem, that the loss function should have many local minima with zero empirical loss. Also relevant is the observation by @cite showing that, if the global minimum is not of zero loss, then a perfect predictor may have a larger loss in training than one producing worse classification results.
- In general, the safety of DNNs is commonly recognized as a huge challenge @cite . There are more and more attempts towards the certification, verification, or explainability of DNNs, of which we provide now a short overview. None of them however (and, as far as we know, no other work either) addresses the traceability of DNNs.
- There has been attempts to apply principles of software engineering (or even of engineering in general) to NNs @cite . It particularly attempts to address the lack of reproducibility in the development of NNs. Even though the terminology and techniques have changed a lot since 2004, the identified problems are still relevant. Still, the solutions of the paper do not answer the need for traceability and seem to hardly match nowadays' practice.
- The discrepancy between the recommendations of the ISO 26262 and the methods actually used in practice was analyzed in @cite . This cannot be directly used for traceability but is indirectly a very useful source of information.
- There has been attempts to set grounds for a rigorous science'' of machine learning @cite , again very useful, but not related to traceability. Finally, many safety-related problems have been identified for AI, especially for reinforcement learning @cite . The identified challenges are relevant and the paper proposes a few attempts of solutions. Most of them are a source of inspiration to identify sources of problem and to analyze whether those sources can be tackled with traceability (even though it turns out not to be really the case for solutions identified in the present paper).
- There is a mature literature on successful applications of bandits in web content optimization (e.g., @cite , @cite , @cite , @cite , @cite ). This paper belongs to a sub-stream of this work that has focused on using bandits for controlled experiments on the web. The closest papers to our work are the complementary papers by @cite , @cite and @cite who propose using bandit experiments to evaluate creatives for targeted advertising, without focusing explicitly on the problem addressed here of comparing target audiences.
- There are two major approaches in the literature to address domain adaption. The approach for a group of methods is based on preprocessing the target domain data points. The target data is mapped from the target domain to the source domain such that the target data structure is preserved in the source @cite . Another common approach is to map data from both domains to a latent domain invariant space @cite . Early methods within the second approach learn a linear subspace as the invariant space @cite @cite where the target domain data points distribute similar to the source domain data points. A linear subspace is not suitable for capturing complex distributions. For this reason, recently deep neural networks have been used to model the intermediate space as the output of the network. The network is trained such that the source and the target domain distributions in its output possess minimal discrepancy. Training procedure can be done both by adversarial learning @cite or directly minimizing the distance between the two distributions @cite .
- Several important UDA methods use adversarial learning. @cite pioneered and developed an effective method to match two distributions indirectly by using adversarial learning. @cite and @cite use the Generative Adversarial Networks (GAN) structure @cite to tackle domain adaptation. The idea is to train two competing (i.e., adversarial) deep neural networks to match the source and the target distributions. A generator network maps data points from both domains to the domain-invariant space and a binary discriminator network is trained to classify the data points, with each domain considered as a class, based on the representations of the target and the source data points. The generator network is trained such that eventually the discriminator cannot distinguish between the two domains, i.e. classification rate becomes $50
- As Wasserstein distance is finding more applications in deep learning, efficient computation of Wasserstein distance has become an active area of research. The reason is that Wasserstein distance is defined in form of a linear programming optimization and solving this optimization problem is computationally expensive for high-dimensional data. Although computationally efficient variations and approximations of the Wasserstein distance have been recently proposed @cite @cite @cite , these variations still require an additional optimization in each iteration of the stochastic gradient descent (SGD) steps to match distributions. @cite used a regularized version of the optimal transport for domain adaptation. @cite used a dual stochastic gradient algorithm for solving the regularized optimal transport problem. Alternatively, we propose to address the above challenges using Sliced Wasserstein Distance (SWD). Definition of SWD is motivated by the fact that in contrast to higher dimensions, the Wasserstein distance for one-dimensional distributions has a closed form solution which can be computed efficiently. This fact is used to approximate Wasserstein distance by SWD, which is a computationally efficient approximation and has recently drawn interest from the machine learning and computer vision communities @cite @cite @cite @cite @cite .
- The first -- most popular -- category includes works classifying algorithms by the techniques they employ to divide the graph into groups of nodes, i.e. by their . Examples in this category are @cite , @cite , @cite , @cite , @cite , @cite ; @cite -- focusing on multilayer networks; and @cite -- whose attention narrows down to genetic algorithms. Here, we are agnostic about how an algorithm works, as we are focused on figuring out which algorithm returns similar partitions to which other. This is influenced by how they work, but even algorithms based on the philosophy of modularity maximization might end up in different categories.
- The second category includes works classifying community discovery algorithms by the of community they are searching for in the network. Notable definition-based review works are @cite , @cite , @cite , @cite , and @cite , the latter three focusing on directed, overlapping, and evolving networks. This is the closest category to ours, as we are also interested in building an ontology of community discovery algorithms. However, the works in this category employ a top-down approach. They take the stated -- theoretical -- definition of community of a paper and use it to classify it. Here, we have a data-driven approach: we classify algorithms not by their stated definition, but by their practical results.
- The third category -- gaining popularity recently -- includes works classifying community discovery algorithms by giving them a specific task and ranking them in how well they in that task. Such tasks can be maximizing modularity or the normalized mutual information of the communities they recover versus some other metadata we have about the nodes. In this category, we can find papers such as @cite , @cite , @cite , @cite , @cite , @cite , @cite ; and, specifically for overlapping community discovery, @cite . In line with this approach, we also use standardized tests and benchmarks. However, we have no interest in which algorithm performs best'' -- whatever the definition of best'' is -- rather in what works similarly. We have a small ranking discussion, but we use it to criticize the notion of a best'' community discovery algorithm rather than taking the results at face value.
- * Visual speech synthesis Over the last two decades, there has been extensive study dedicated towards creating realistic animations for speech @cite in 2D or 3D. 2D has the advantage that video cutouts of the mouth area can be used and combined leading to realistic visualizations @cite @cite . 3D approaches are much more versatile, as viewpoints and illumination can be changed at will @cite . Given that our goal is to produce 2D animation based on audio, instead of formulating entire mapping as an end-to-end optimization task, we are inherently interested in the intermediate representations. Recent advances in this line were more or less focused on synthesizing only the parts of the face (around the mouth) and borrowing the rest of the subject from existing video footage @cite @cite @cite .
- * Human pose estimation Human pose estimation is a general problem in computer vision to detect human figures in images and video. Recent deep-learning based algorithmic advances enable not only the detection and localization of major body points, but detailed surface-based human body representation (e.g.,OpenPose @cite and DensePose @cite ). We use a pre-trained DensePose estimator to create body figure RGB images from video frames.
- * Image-to-Image translation Several recent frameworks have used generative adversarial networks (GANs) @cite to learn a parametric translation function between input and output images @cite . Similar ideas have been applied to various tasks, such as generating photographs from sketches, or even video applications for both paired and unpaired cases @cite @cite @cite @cite . However, none of these techniques are fool-proof, and some amount of limitations often remain.
- Negative mining methods train a classifier based on the strongly labelled negative training data. For each instance in a positive bag, based on the inter-class information, NegMin @cite compute their similarities with all of the negative instances, and select the instance that has minimum max-similarity as of interest. CRANE @cite selects negative instances to vote against an unknown instance by specifying some similarity threshold, and improves the robustness of labelling noise among negative instances. @cite also make use of the similarity information as a pre-processing heuristic for a bag-level classification. They select instances with least similarity to the negative bags and use them to initialize cluster centers, which are then used to create the bag level feature descriptors of @cite . Moreover, Jiang @cite trains a one-class SVM based on negative instances, then ranks the saliency according to the distances to the decision boundary.
- Besides using the inter-class information, key instance detection can be accomplished by searching similar patterns among diverse positive bags. The most classical framework is diverse density (DD) @cite . It defines a conditional probability with similarity, and uses the noisy-or model to define a diverse density to select instances with high similarities to diverse positive bags and low similarities to negative bags. DD has been widely used as a basis for many methods including EM-DD @cite , GEM-DD @cite and DD-SVM @cite . However, DD is sensitive to labelling noise. Evidence confidence @cite is proposed to seek the mode on observed instances rather than in a continuous space to facilitate the computation and alleviate the sensitivity. @cite exploit similarity among class-specific features to decide prototypes, which are used in a voting-based mechanism to select instances with a high diverse occurrence.
- Since we derive a KDE interpretation for our voting scheme, we also make a literature review on this subject. KDE possesses the advantages of nonparametric method for unsupervised density estimation. @cite propose a supervised KDE to make use of labels, and extend the mean shift @cite to a supervised version to seek modes. In order to make full use of unlabelled data, @cite @cite propose a semi-supervised KDE to estimate class-specific density based on a little fraction of labelled data. SSKDE is later extended to a manifold structure @cite .
- Shallow learning methods have been outperformed by deep convolutional neural networks (DCNN) significantly on the visual recognition tasks resulting from their powerful feature representation @cite . One approach to boosting the performance of shallow methods is using deep features from pre-trained DCNN models. R-CNN @cite combined SVM with DCNN features to boost the object detection performance. DCNN features have also been incorporated into weakly supervised visual recognition tasks. @cite concatenate multiple convolutional outputs and max-pool them to represent the super-pixel features. Observing that a region probably belongs to an object if many channels of the hidden-layer activation fire simultaneously, @cite select the object regions using aggregation map, then max-pool the concatenation of multiple-layer activations to represent the image. Similarly, based on the findings that the hidden-layer activations of a pre-trained object recognition network usually fire up on objects rather than background, @cite leverage these masks for weakly supervised semantic segmentation.
- Generating class agnostic region proposals has been investigated in computer vision for more than a decade. Initial methods include multi-scale combinatorial grouping @cite , constrained parametric min-cuts @cite , selective search @cite . These methods generate region proposals which obtain high recall for objects in a category agnostic fashion. They were also very successful in the pre-deep learning era and obtained state-of-the-art performance even with a bag-of-words model @cite . Using region proposals based on selective search @cite , R-CNN @cite was the first deep learning based detector. Unsupervised region proposals were also used in later detectors like Fast-RCNN @cite but since the Faster-RCNN detector @cite generated region proposals using a convolutional neural network, it has become the de-facto algorithm for generating region proposals.
- To improve RPN, several modifications have been proposed. State-of-the-art detectors can also detect objects in a single step. Detectors like SSH @cite , SSD @cite , RetinaNet @cite , MS-CNN @cite generate multi-scale feature maps to classify and regress anchors placed on these feature-maps. These single-shot detectors are closely related to the region proposal network as they have specific filters to detect objects of different sizes and aspect ratios but also combine feature-maps from multiple layers of the deep neural network. No further refinement is performed after the initial offsets generated by the network are applied. Another class of detectors are iterative, like G-CNN @cite , Cascade-RCNN @cite , LocNet @cite , FPN @cite , RFCN-3000 @cite , Faster-RCNN @cite . These detectors refine a pre-defined set of anchor-boxes in multiple stages and have more layers to further improve classification and localization of regressed anchors. One should note that even in these networks, the first stage comprises of the region proposal network which eliminates the major chunk of background regions. FA-RPN is closer to this line of work but, in contrast, it supports iterative refinement of region proposals during inference.
- Topic-based solutions perform content-based analysis to extract the relevant topics on a user's browsing history and the ads he receives. Then, using different heuristics and statistical means, targeted ads are identified as those having topics that share some semantic overlap with the user's browsing history. Topic-based detection could, in principle, be applied to real users, as we have done for evaluation purposes in . Existing work, however, has only used it in conjunction with artificially constructed , , robots that browse the web imitating very specific (single-topic) demographic groups @cite @cite , or to emulate real-users offline using click-streams @cite .
- The only topic-based solution meant to be used by real users is MyAdchoice @cite , which has been implemented in the form of a browser extension. This extension is available only under request, and based on the information reported in the paper, it has been only used in a beta-testing phase by few tens of friends and colleagues. Independently of the specific pros and cons of individual solutions, topic-based detection presents some common limitations. The most important being that it can only detect direct interest-based targeted advertising. It is unable to detect other forms of targeting based on demographic or geographic parameters, as well as indirect targeting (see for definitions).
- Correlation-based solutions treat the online advertising ecosystem as a blackbox and apply machine learning and statistical methods to detect correlations between the browsing behavior and other characteristics of a user (OS, device type, location, ) and the ads he sees. For instance, XRay @cite and Sunlight @cite create for each persona several . Each shadow account performs a subset of the actions performed by the original persona. By analyzing the common actions performed by shadow accounts receiving the same reaction from the ecosystem ( , the same ad), the authors can infer the cause of a targeting event. AdFisher @cite uses similar concepts to find discrimination practices, for instance, in the ads shown to men vs. women. As with topic-based detection, this technique presents important challenges related to scalability and practical implementation. Moreover, they are not suitable for real-time targeting detection. With the exception of @cite , no previous work has been implemented as a tool for end-users. Most of them, including @cite , rely on content-based analysis, thereby suffering from scalability issues and inability to detect indirect targeting.
- Another type of method has been proposed to alleviate the cost of the comparison function and to shift the burden to the audio features extraction function -- which can be done offline and stored. The general principle is to encode each audio track as a single scalar or vector -- its embedding -- and to reduce the similarity computation to a simple Euclidean distance between embeddings. Originally, embeddings were for instance computed as a single hash encoding a succession of pitch landmarks @cite , or as a vector obtained by PCA dimensionality reduction of a chromagram's 2D-DFT @cite or with locality-sensitive hashing of melodic excerpts @cite .
- The principle is to learn a mapping between the input space and a latent manifold where a simple distance measure (such as Euclidean distance) should approximate the neighborhood relationships in the input space. There is however a trivial solution to the problem, where the function ends up mapping all the examples to the same point. Contrastive Loss was introduced to circumvent this problem, aiming at simultaneously similar pairs together and dissimilar pairs apart @cite .
- However, when the amount of labels becomes larger, the number of dissimilar pairs becomes quickly intractable. It was moreover observed in practice that once the network has become reasonably good, negative pairs become relatively easy to discern, which stalls the training of the discriminative model. is the strategy of training the model only with hard pairs, i.e. positive (resp. negative) pairs with large (resp. small) distances @cite . Further improvement was introduced with the triplet loss, which is used to train a model to map each sample to an embedding that is closer to all of its positive counterparts than it is to all of its negative counterparts @cite . Formally, for all triplets @math , @math , @math where @math is an anchor, and @math or @math is one of its positive or negative example, respectively, the loss to minimize is expressed as @math , where @math is a margin and @math and @math are the distances between each anchor @math and @math or @math , respectively.
- In a recent work @cite , we suggested in an analogy with image processing that dominant melody extraction can be seen as a type of image segmentation, where contours of the melody have to be isolated from the surrounding background. We have thus proposed for dominant melody estimation an adaptation of U-Net @cite -- a model originally designed for medical image segmentation -- which slightly improves over @cite .
- Speed perturbation @cite is a data augmentation technique which creates warped time signals in addition to the original speech signals. Given an audio signal of length @math and a warping factor @math , speed perturbation creates a new signal with duration @math by resampling the original signal with a sampling rate of @math , where @math is the sampling rate of the original signal. Speed perturbation shifts the speech spectrum and also results in change in number of frames as the duration of the resulting signal is different @cite .
- This category includes studies about the general structure of RDF graphs at instance, schema, and metadata levels. @cite present the status of RDF datasets in the LOD Cloud in terms of size, linking, vocabulary usage, and metadata. LODStats @cite and the large-scale approach DistLODStats @cite report on statistics about RDF datasets on the web, including number of triples, RDF terms, and properties per entity, and usage of vocabularies across datasets. Loupe @cite is an online tool that reports on the usage of classes and properties in RDF datasets. Fern ' @cite define measures to describe the relatedness between nodes and edges using subject-object, subject-predicate, and predicate-object ratios. @cite study the distribution of RDF terms, classes, instances, and datatypes to measure the quality of public RDF data. In summary, the study of RDF-specific properties of publicly available RDF datasets have been extensively covered and is currently supported by online services and tools such as LODStats and Loupe. Therefore, in addition to these works, we focus on analyzing graph invariants in RDF datasets.
- In the area of structural network analysis, it is common to study the distribution of certain graph measures in order to characterize a graph. RDF datasets have also been subject to these studies. The study by @cite reveals that the power-law distribution is prevalent across graph invariants in RDF graphs obtained from 1.7 million documents. Also, the small-world phenomenon, known from experiments on social networks were studied within the Semantic Web @cite . More recently, Fern ' @cite have studied the structural features of real-world RDF data. Fern ' also propose measures in terms of in- and -out degrees for subjects, objects, and predicates and analyze the structure of @math RDF graphs from different knowledge domains. Most of these works focus on studying different in- and out-degree distributions and are limited to a rather small collection of RDF datasets. Moreover, the work by @cite analyze further relevant graph invariants in RDF graphs including @math index and reciprocity. The work by applied graph-based metrics on synthetic RDF datasets. Complementary to these works, we present an study on @math RDF datasets from the LOD Cloud and analyze their structure based on the average degree, @math -index, and powerlaw exponent.
- Our work is partially inspired by SGF @cite and GFK @cite , which have shown that the intermediate domains between source and target domains are useful for addressing the domain adaptation problem. They represented each domain as a subspace, and then connected them on Grassmannian manifold to model intermediate domains. Different from them, we model the intermediate domains by directly translate images on pixel level. This allows us to easily improve the existing deep domain adaptation models by using the translated images as training data. Moreover, our model can also be applied to image-level domain generalization by generating mixed-style images.
- Well-known methods for comparing graphs using distance measures include combinatorial (e.g., graph edit distance @cite ) and spectral (e.g., eigenvalue decomposition @cite ) approaches. Graph edit distance minimizes the cost of transforming one graph to another via a set of elementary operators such as node edge insertions deletions, while spectral approaches optimize objective functions based on properties of the graph spectra.
- Recently, several distances for comparing metric graphs have been proposed based on ideas from computational topology. In the case of a special type of metric graph called a Reeb graph, these distances include: the functional distortion distance @cite , the combinatorial edit distance @cite , the interleaving distance @cite , and its variant in the setting of merge trees @cite . In particular, the functional distortion distance can be considered as a variation of the Gromov-Hausdorff distance between two metric spaces @cite . The interleaving distance is defined via algebraic topology and utilizes the equivalence between Reeb graphs and cosheaves @cite . For metric graphs in general, both the persistence distortion distance @cite and the intrinsic distance @cite take into consideration the structure of metric graphs, independent of their geometric embeddings, by treating them as continuous metric spaces. In @cite , Oudot and Solomon point out that since compact geodesic spaces can be approximated by finite metric graphs in the Gromov--Hausdorff sense @cite (see also the recent work of M 'emoli and Okutan @cite ), one can study potentially complicated length spaces by studying the persistence distortion of a sequence of approximating graphs.
- In the context of comparing the relative discriminative capabilities of these distances, Bauer, Ge, and Wang @cite show that the functional distortion distance between two Reeb graphs is bounded from below by the bottleneck distance between the persistence diagrams of the Reeb graphs. Bauer, Munch, and Wang @cite establish a strong equivalence between the functional distortion distance and the interleaving distance on the space of all Reeb graphs, which implies the two distances are within a constant factor of one another. Carri ere and Oudot @cite consider the intrinsic versions of the aforementioned distances and prove that they are all globally equivalent. They also establish a lower bound for the bottleneck distance in terms of a constant multiple of the functional distortion distance. In @cite , Dey, Shi, and Wang show that the persistence distortion distance is stable with respect to changes to input metric graphs as measured by the Gromov-Hausdorff distance. In other words, the persistence distortion distance is bounded above by a constant factor of the Gromov-Hausdorff distance. Furthermore, the intrinsic distance is also bounded from above by the Gromov-Hausdorff distance for general metric spaces @cite .
- Another line of work that is conceptually close to our method copes with intrinsic motivation which is used to drive the agent's exploration. Examples of such works include empowerment @cite @cite , count-based exploration @cite @cite @cite @cite , information gain about agent's dynamics @cite and forward-inverse dynamics models @cite . While our method uses an information-theoretic objective that is similar to these approaches, it is used to learn a variety of skills that can be directly used for model-based planning, which is in contrast to learning a better exploration policy for a single skill. We provide a discussion on the connection between empowerment and DADS in Appendix .
- Methods that do not model road-agent interactions are regarded as sub-optimal or as less accurate than methods that model the interactions between road-agents in the scene @cite . Examples of methods that explicitly model road-agent interaction include techniques based on social forces @cite @cite , velocity obstacles @cite , LTA @cite , etc. Many of these models were designed to account for interactions between pedestrians in a crowds (i.e. homogeneous interactions) and to improve the prediction accuracy @cite . Techniques based on velocity obstacles have been extended using kinematic constraints to model the interactions between heterogeneous road-agents @cite . Our learning approach does not use any explicit pairwise motion model. Rather, we model the heterogeneous interactions between road-agents implicitly.
- Two typical methods of learning from demonstration, are inverse reinforcement learning (IRL) and imitation learning (IL). Inverse reinforcement learning was introduced in ng2000algorithms . Its goal is to infer the underlying reward function given the optimal demonstration behavior. Further IRL algorithm includes Bayesian IRL @cite @cite , Maximum Entropy IRL @cite @cite , Repeated IRL @cite , etc. But IRL can be intractable when problem scale is large. Earlier imitation learning indicates behavior cloning, which could fail when agent encounters untrained states. Later representative IL algorithm includes Data Aggregation (DAgger) @cite , Generative Adversarial Imitation Learning (GAIL) , etc. However, their work focuses on imitating optimal demonstration, regarding mediocre and failed demonstration unusable. They also never consider exploration problem after imitating.
- Conditional computation has been well studied in computer vision. Cascaded classifiers @cite shorten computation by identifying easy negatives and have recently been adapted to deep learning @cite @cite . More directly, @cite and @cite both propose a cascading architecture which computes features at multiple scales and allows for dynamic evaluation, where at inference time the user can trade off speed for accuracy. Similarly, @cite adds intermediate classifiers and returns a label once the network reaches a specified confidence. @cite @cite both use the state of the network to adaptively decrease the number of computational steps during inference. @cite uses an intermediate state sequence and a halting unit to limit the number of blocks that can be executed in an RNN; @cite learns an image dependent stopping condition for each ResNet block that conditionally bypasses the rest of the layers in the block. @cite trains a large number of small networks, called Experts, and then uses gates to select a sparse combination of the experts for a given input.
- During train time, the activation rate, a hyperparameter set by the user, determines how frequently each individual gate should be open and the target loss is added to the classification loss, where activation loss is L2 of the difference between the target rate and current rate. Our work differs from @cite several ways. We reformulate the loss to a per-batch loss and consider both data-dependent and data-independent layer bypass. Data-independent per-batch loss results in the network trying to remove enough layers to reach the target rate while retaining accuracy (similar to pruning techniques), while data-dependent per-batch loss has more flexibility to utilize layers.
- Another approach to decreasing the computation time is network pruning. The earliest works attempted to determine the importance of specific weights @cite @cite or hidden units @cite and remove those which are unimportant or redundant. Weight-based pruning on CNNs follows the same fundamental approach; @cite prunes weights with small magnitude and @cite incorporates these into a pipeline which also includes quantization and Huffman coding. Numerous techniques prune at the channel level, whether through heuristics @cite @cite or approximations to importance @cite @cite @cite . @cite prunes at the filter level using statistics from the following layer. @cite applies binary mask variables to a layer's weight tensors, sorts the weights during train time, and then sends the lowest to zero.
- @cite is the most related to our data-independent bypass. They add a sparsity regularization and then modifies stochastic Accelerated Proximal Gradient to prune the network in an end-to-end fashion. Our work differs from @cite by using GS to integrate the sparsity constraint into an additive loss which can be trained by any optimization technique; we use unmodified stochastic gradient descent with momentum (SGD), the typical technique for training classification. Recently, @cite suggests that the main benefits of pruning come primarily from the identified architecture.
- Our work is also related to regularization techniques such as Dropout @cite and Stochastic Depth @cite . Both techniques try to induce redundancy through stochastically removing parts of the network during training time. Dropout ignores individual units and Stochastic Depth (as described above) skips entire layers. Both provide evidence that the increased redundancy improves helps to prevent overfitting. These techniques can be seen as applying stochastic gates to units or layers, respectively, where the gate probabilities are hyperparameters.
- Existing LTR approaches suffer from several limitations. Although, in practical applications, only the top (say N) items in the ranked list are of interest, and the lower-ranked ratings in the list are less reliable, most existing LTR methods are optimized on the ranks of the entire lists, which, could potentially reduce the ranking quality of the top-ranked items. Furthermore, the computational complexity of straightforward approaches to optimizing ranking measures (e.g., DCG @cite , MRR @cite , AUC @cite or MAP @cite ), scale quadratically with @math (the average number of observed items across all users), which renders such methods impractical in large-scale real-world settings.
- Furthermore, Miniproxy @cite can be used to accelerate TCP's connection establishment. This approach places a proxy between the client and the web server, which doubles the number of required TCP handshakes. Miniproxy can provide a faster TCP connection establishment in case of a favorable network topology and significant RTTs between client and web server. The QUIC protocol includes computationally expensive cryptographic handshakes causing a significant delay compared to TCP's handshake @cite . Therefore, this approach seems less feasible when QUIC is used.
- The ASAP @cite protocol piggybacks the first transport packet within the client's DNS query and the DNS server forwards it to the web server after resolving the IP address. However, this approach requires the DNS server to spoof the clients IP address which leads to a violation of the Best Current Practice RFC 2827 @cite . Furthermore, a deployment of ASAP requires significant infrastructural changes to the Internet because it uses a custom transport protocol.
- Further possible performance improvements can be achieved by sending replicated DNS queries to several DNS resolvers and occasionally receiving a faster response @cite . Another DNS-based mechanism aiming to reduce latency uses Server Push @cite where the resolver provides speculative DNS responses prior to the client's query. In total, these approaches tradeoff a higher system utilization versus a possibly reduced latency.
- The idea of extracting entity pairs by discovering textual patterns dates back to early work on bootstrapping for relation extraction with the DIPRE system @cite . This system was designed to find co-occurrences of seed entity pairs of a known relationship type inside unlabelled text, then extract simple patterns (exact string matches) from these occurrences and use them to discover new entity pairs. introduced a pattern evaluation methodology based on the precision of a pattern on the set of entity pairs which had already been discovered; they also used the dot product between word vectors instead of an exact string match to allow for slight variations in text. Later work @cite @cite @cite has proposed more sophisticated pattern extraction methods (based on dependency graphs or kernel methods on word vectors) and different pattern evaluation frameworks (document relevance scores).
- A well known body of work, OpenIE @cite @cite @cite @cite aims to extract patterns between entity mentions in sentences, thereby discovering new surface forms which can be clustered @cite @cite in order to reveal new meaningful relationship types. In the biomedical domain, Percha and Altman attempt something similar by extracting and clustering dependency patterns between pairs of biomedical entities (e.g. chemical-gene, chemical-disease, gene-disease). Our work differs from these approaches in that we extract pairs for a pre-specified relationship type (either from scratch or by augmenting existing data written with specific guidelines), which is not guaranteed to correspond to a cluster of discovered surface forms.
- In unconstrained environments, to deal with nonrigid transformations and other noises, involving global information instead of pixel-wise local information for designing a robust similarity is a key cue. Histogram matching (HM) @cite @cite @cite , which mainly measure the similarity between two color histograms, is not restricted by geometric transformation. However, it is usually not a good choice when background clutter and occlusions appear within the windows. Earth mover's distance (EMD) @cite is proposed to measure the similarity between two probability distributions. Furthermore, a more robust approach @cite is proposed by using spatial-appearance representation to measure the EMD. Tone mapping similarity measure @cite is proposed for handling noise, which is approximated by a piece-wise constant linear function. Asymmetric correlation @cite is proposed to deal with both the noise and illumination changes. Other measures focus on improving the robustness against noise as proposed in M-estimator @cite @cite and Hamming-based distance @cite @cite . We refer the interested readers to a comprehensive survey @cite .
- An eye-catching family of similarity measures in recent years is to explore a global statistic property over the two point sets. Bi-directional similarity (BDS) @cite proposes that two point sets are considered similar if all points of one set are contained in the other, and vice versa. Best-buddies-similarity (BBS) @cite @cite counts the two-side NNs as a similarity statistic. Deformable diversity similarity (DDIS) @cite measures the diversity of feature matches between the two sets and is reported to outperform BBS by revealing the deformation'' of the NN field. Despite the robustness of BBS and DDIS against the transformations within the search windows, scaling and rotation on the whole search windows have not been considered. In this paper, we propose a scaling and rotation independent similarity measure which leads to a significant improvement and allows multi-scale template matching in unconstrained environments.
- Due to the revolutionary success (convolutional) neural networks have had on computer vision problems over the last decade, researchers have extended the fields of applications of neural networks significantly. A particularly interesting concept is to learn the solution of complex, possibly nonconvex, optimization problems. Different lines of research have considered directly learning the optimizer itself, e.g. modelled as a recurrent neural network @cite , or rolling out optimization algorithms and learning the incremental steps, e.g. in the form of parameterized proximal operators in @cite . Further hybrid approaches include optimization problems in the networks' architecture, e.g. @cite , or combining optimizers with networks that have been trained individually @cite @cite . The recent work of @cite trains a network to predict descent directions to a given energy in order to give provable convergence results on the learned optimizer.
- The most related prior work is the 3D face reconstruction network from Tewari al @cite . They aimed at finding a semantic code vector from a given facial image such that feeding this code vector into a rending engine yields an image similar to the input image itself. While this problem had been addressed using optimization algorithms a long time ago @cite (also known under the name of analysis-by-synthesis approaches), the Tewari al @cite replaced the optimizer with a neural network and kept the original cost function to train the network in an unsupervised way. The resulting structure resembles an AE in which the decoder fixed to the forward model and was therefore coined model-based AE. As we will discuss in the next section, the idea of model-based AEs generalizes far beyond 3D face reconstruction and can be used to boost the THz parameter identification problem significantly.
- Finally, a recent work has exploited deep learning techniques in Terahertz imaging in @cite , but the considered application of super-resolving the THz amplitude image @math by training a convolutional neural network on synthetically blurred images is not directly related to our proposed approach.
- The earliest approaches to regression-based face alignment trained a cascade of regressors to detect face landmarks @cite @cite @cite @cite @cite . More recently, deep convolutional neural networks (CNNs) have been used for both 2D and 3D facial landmark detection from 2D images @cite @cite . These methods are generally classified into coordinate regression models @cite @cite @cite @cite , where a direct mapping is learned between the image and the landmark coordinates, and heatmap regression models @cite @cite @cite , where prediction heatmaps are learned for each landmark. Heatmap-based architectures are generally derived from stacked hourglass @cite @cite @cite @cite or convolutional pose machine @cite architectures used for human body pose estimation. Pixel coordinates can be obtained from the heatmaps by applying the argmax operation; however, @cite @cite use soft-argmax to achieve end-to-end differentiability. A more comprehensive overview of face alignment methods can be found in @cite .
- Neural networks have been used for various other face image analysis tasks such as gender determination @cite and face detection @cite . More recently, deep CNNs have been used to improve face detection results especially in uncontrolled environments and with more extreme poses @cite @cite . Additionally, CNNs have been employed for face segmentation @cite @cite @cite , facial pose and reflectance acquisition @cite @cite , and face recognition @cite @cite .
- Using deep networks such as VGG-16 @cite for losses has been shown to be effective for training other deep networks for tasks such as style transfer and super-resolution @cite . Such techniques have also been used for image generation @cite and face swapping @cite . Furthermore, deep networks have been used in energies for traditional optimization problems for style transfer @cite , texture synthesis @cite , and image generation @cite @cite . While @cite @cite use the L-BFGS @cite method to minimize the optimization problem, @cite @cite use gradient descent methods @cite .
- Generalization has been cast as avoiding overfitting to a particular training environment, implying that sampling from diverse environments is necessary for generalization @cite @cite . Other work has focused on generalization as improved performance in off-policy states, a framework much closer to standard approaches in supervised learning. Techniques such as adding stochasticity to the policy @cite , having the agent take random steps, no-ops, steps from human play @cite , or probabilistically repeating the agent's previous action @cite , all force the agent to transition to off-policy states.
- Transfer learning (TL) aims to transfer knowledge from one domain task to improve performance on the another @cite . The most widely used TL technique for deep networks is fine-tuning @cite @cite @cite . Instead of training a target network from scratch, its weights are initialised by a pre-trained model from another task such as ImageNet @cite classification. While fine-tuning reduces label requirement compared to learning the target problem from scratch, it is prone to over-fitting if target labels are very few @cite . Therefore, it is ineffective for very sparsely supervised DLSTL, and not applicable to unsupervised DLSTL. Moreover, vanilla TL does not exploit available unlabelled samples for the target problem (i.e. semi-supervised TL). The most related method to ours is @cite which does exploit both unlabelled and few labelled data, i.e., semi-supervised DLSTL. However like other TL methods, it does not generalise to the unsupervised DLSTL setting where no target annotations are available.
- Entropy loss for unlabelled data is another widely used SSL regulariser @cite @cite . It is applied at the classification layer in problems where the unlabelled and labelled data share the same label-space -- and reflects the inductive bias that a classification boundary should not cut through the dense unlabelled data regions. Its typical use is on softmax classifier outputs where it encourages a classifier to pick a single label. In contrast we use entropy-loss to solve DLSTL problems by applying it element-wise on our intermediate CFS layer in order to weakly align domains by encouraging them to share a near-binary representation.
- In view of this problem, representation learning and clustering are performed simultaneously in recent works such as [ @cite ], [ @cite ], and [ @cite ]. However, these works do not sufficiently address the imbalanced-data problem. DEC [ @cite ] exhibits some degrees of robustness to imbalanced datasets, but work is needed to further improve its robustness. Several methods based on generative models have also been proposed [ @cite ], [ @cite ]. VaDE [ @cite ] models a data generation procedure based on the variational autoencoder, where the data distribution in the latent space is modeled by GMM, the representations are sampled and then mapped into the space via the DNN. This approach is novel and can work well in some cases. However, because the class distribution is unknown in imbalanced dataset, it is difficult to learn a good generative model, which may lead to low versatility and robustness.
- The message passing interface @cite defines a standard for high level networking primitives to send and receive data between local and remote processes, typically used for HPC applications.
- @cite is a network stack designed for next generation systems for applications with an highly multi-threaded environment. It provides three independent layers: UCS is a service layer with different cross platform utilities, such as atomic operations, thread safety, memory management and data structures. The transport layer UCT abstracts different hardware architectures and their low-level APIs, and provides an API to implement communication primitives. UCP implements high level protocols such as MPI or PGAS programming models by using UCT.
- @cite is a distributed key-value storage optimized for low latency data access using InfiniBand with messaging verbs. Multiple transports are implemented for network communication, e.g. using reliable and unreliable connections with InfiniBand and Ethernet with unreliable connections. @cite implements a key-value and graph storage using a shared memory architecture with RDMA. It performs well with a throughput of 167 million key-value lookups and 31 us latency using 20 machines. @cite also implements a key-value storage using RDMA for get operations and messaging verbs for put operations. @cite implements a key-value storage with a focus on NUMA architectures. It maps each CPU core to a partition of data and communicates using a request-response approach using unreliable connections. @cite borrows the design of MICA and implements networking using RDMA writes for the request to the server and messaging verbs for the response back to the client.
- In a framework based on the simulation paradigm, introduced the notion of deniable authentication @cite , followed by the work of Di on the formalization of deniable key exchange @cite . Both works rely on the formalism of zero-knowledge (ZK) proofs, with definitions formalized in terms of a simulator that can produce a simulated view that is indistinguishable from the real one. In a subsequent work, Di Raimondo and Gennaro gave a formal definition of forward deniability @cite , requiring that indistinguishability remain intact even when a (corrupted) party reveals real coins after a session. Among other things, they showed that statistical ZK protocols are forward deniable.
- Pass @cite formally defines the notion of deniable zero-knowledge and presents positive and negative results in the common reference string and random oracle model. In @cite , establish a link between deniability and ideal authentication and further model a situation in which deniability should hold even when a corrupted party colludes with the adversary during the execution of a protocol. They show an impossibility result in the PKI model if adaptive corruptions are allowed. Cremers and Feltz introduced another variant for key exchange referred to as peer and time deniability @cite , while also capturing perfect forward secrecy. More recently, Unger and Goldberg studied deniable authenticated key exchange (DAKE) in the context of secure messaging @cite .
- To the best of our knowledge, the only work related to deniability in QKE is a single paper by Beaver @cite , in which the author suggests a negative result arguing that existing QKE schemes are not necessarily deniable.
- The study of line segment detection has a very long history since 1980s @cite . The early pioneers tried to detect line segments based upon the edge map estimation. Then, the perception grouping approaches based on the are proposed. Both of these methods concentrate on the hand-crafted low-level features for the detection, which have become a limitation. Recently, the line segment detection and its related problem edge detection have been studied under the perspective of deep learning, which dramatically improved the detection performance and brings us of great practical importance for real applications.
- In a long range of time, the hand-crafted low-level features (especially for image gradients) are heavily used for line segment detection. These approaches can be divided into edge map based approaches @cite @cite @cite @cite @cite @cite and perception grouping approaches @cite @cite @cite . The edge map based approaches treat the visual features as a discriminated feature for edge map estimation and subsequently applying the Hough transform @cite to globally search line configurations and then cutting them by using thresholds. In contrast to the edge map based approaches, the grouping methods directly use the image gradients as local geometry cues to group pixels into line segment candidates and filter out the false positives @cite @cite .
- Last decade has seen a great deal of literature on tool path planning for free-form surfaces, such as iso-parametric method @cite @cite @cite , iso-planar method @cite @cite @cite , iso-scallop method @cite @cite @cite @cite @cite @cite @cite , iso-phote method @cite and C-space method @cite , to name a few. Surveys of much more work about tool path planning research can be found in @cite @cite . Since we aim at optimal tool paths with respect to iso-scallop and smoothness, we put special interest in the iso-scallop method, which means the height of the points at the scallop curves remains as high as a given value so that the tool path has no unnecessary cutting. Conventionally, constant scallop height is obtained by varying the offset magnitude along each path. A mathematical method for generating iso-scallop tool paths following such strategy was first proposed by @cite . Afterwards, methods to improve the computing efficiency @cite @cite and accuracy @cite @cite @cite were proposed. In 2007, Kim @cite reformulated the iso-scallop tool path as geodesic parallel curves on the design surface by defining a new Riemannian metric.
- There also exist some efforts to generate smooth tool paths without considering the overlapping between neighbor machining strips (i.e., the iso-scallop condition). Generally, such methods are based on the Laplacian. For example, Bieterman and Sandstrom @cite proposed a Laplacian based contour parallel tool path generation method by selecting the level sets of a harmonic function defined over a pocket as the tool path. But how to choose the level sets for it still remains an open problem, namely there is no formula for path interval calculation so far. Similarly, Chuang and Yang @cite combined the Laplacian method and iso-parametric method to generate tool paths for pockets with complex topology, i.e., complex boundaries and islands. However, the smoothness of the tool path cannot be guaranteed through Laplacian energy as small Laplacian value does not necessarily mean small curvature of the level set curves. And solving a Laplace equation over a surface can only generate a unique and uncontrollable scalar function (scaling has no impact on the shape of tool paths). Another drawback of the Laplacian based approach is the severe overlapping between machining strips of neighbor paths, especially for paths near the boundary, which results in too much redundant machining.
- In (NAS), the goal is to devise algorithms that automatically optimize the neural architecture for a given problem. Several NAS papers have recently proposed a number of approaches. In @cite , a reinforcement learning algorithm was used to optimize the architecture of a neural network. In @cite , a genetic algorithm is used to optimize the structure of two types of blocks'' (a combination of neural network layers and building components) that have been used for constructing architectures. The number of blocks comprising the full architecture was manually optimized. It was observed that the optimal number of blocks is mostly dependent on the size of the training set. More efficient optimization techniques were proposed in @cite @cite @cite @cite . In all these works, the architecture search algorithms were focused on optimizing the structure of one (or two) blocks that were manually connected together to span the full architecture. The algorithm proposed in @cite optimizes both the block structure and the number of blocks simultaneously.
- When considering NAS for fully-connected networks, @cite proposed an algorithm that iteratively adds neurons to an existing layer or to initiate a new layer. Their algorithm iteratively optimizes the width and depth of a network. For a comprehensive survey on NAS techniques, see @cite . To the best of our knowledge, no work has been done on architecture searches for active learning.
- -map approaches The second category of localization techniques leverages existing maps, in order to solve the localization problem. In particular, two classes of approaches have been presented in the literature: geometry-based and projection-based methods. Caselitz al @cite proposed a geometry-based method that solves the visual localization problem by comparing a set of 3D points, the point cloud reconstructed from a sequence of images and the existing map. Wolcott al @cite , instead, developed a projection-based method that uses meshes built from intensity data associated to the 3D points of the maps, projected into an image plane, to perform a comparison with the camera image using the (NMI) measure. Neubert al @cite proposed to use the similarity between depth images generated by synthetic views and the camera image as a score function for a particle filter, in order to localize the camera in indoor scenes.
- The work presented in this paper has been inspired by Schneider al @cite , which used 3D scans from a LIDAR and RGB images as the input of a novel CNN, RegNet. Their goal was to provide a CNN-based method for calibrating the extrinsic parameters of a camera a LIDAR sensor. Taking inspiration from that work, in this paper we propose a novel approach that has the advantages of both the categories described above. Differently from the aforementioned literature contribution, which exploits the data gathered from a synchronized single activation of a 3D LIDAR and a camera image, the inputs of our approach are a complete 3D LIDAR map of the environment, together with a single image and a rough initial guess of the camera pose. Eventually, the output consists of an accurate 6-DoF camera pose localization. It is worth to notice that having a single LIDAR scan taken at the same time as the image imply that the observed scene is exactly the same. In our case, instead, the 3D map usually depicts a different configuration, road users are not present, making the matching more challenging.
- Shrouf and Miragliotta @cite report on different approaches for energy management enabled by Internet of Things (IoT) technologies. Based on literature, expert interviews, and reports of manufactures, they summarize different IoT architectures for power monitoring and present a general abstraction of them. The resulting architecture primarily focuses on network interconnections and integration of other systems. As in our approach, it respects real-time data processing and the challenge of integrating data of different sensors and data formats. However, data is only processed in a cloud or local server infrastructure and does not follow fog computing paradigms. The architecture represents a general approach and is therefore too abstract to offer a reference implementation.
- We did not find any monitoring approaches for production environments designed in a microservices architecture. However, microservice-based approaches exist for other applications of the Internet of Things, such as @cite and @cite . As we propose in our architecture, these approaches intend to deploy microservices decentralized for flexibility and extendibility. Moreover, they use an asynchronous messaging bus for the exchange of sensor data as in our approach. Both approaches do not focus on scalability and, therefore, do not evaluate this.
- @cite used a convolutional neural network (CNN) architecture based on the architecture of Kim @cite to classify the title of the products. The first layer uses random word embedding. In addition they used a VGG network for image classification @cite . While they experimented with both early and late fusion, only the late fusion resulted in an improvement in accuracy. The image and text classifiers were trained separately to achieve maximal performance individually before being combined by a policy network. The policy network which achieved the highest accuracy is a neural network with 2 fully connected layers and takes in the top-3 class probabilities from the image and text CNNs as input. Their dataset contained 1.2 million images and 2890 possible shelves. On average, each product falls in 3 shelves. Their model is considered accurate when the network correctly outputs one of the three shelves.
- The dataset contained 96,806 products belonging to 193 different classes. Note that each product was assigned to one class. Hence @math berg applied a softmax function in the final layer before outputting the class probabilities. Similar to @cite , both late and early fusion were explored, and late fusion yielded better results. Both heuristic policies and network policies were explored. Heuristic policies refer to some static rule; as an example, the mean of the probabilities from different modals. Network policies refer to training a neural network that takes the output probabilities from different networks and produces a new probability vector.
- The difference between the various surrogate approaches lies into the model space assumption and the acquisition function. Sequential Model-based Algorithm Configuration ( ) @cite @cite is based on Random Forest, so are frameworks based on such as @cite @cite or @cite . @cite uses a Tree-structured Parzen Estimator (TPE) while @cite is based on Gaussian processes (GP).
- An acquisition function is used to determined the next configuration to be sampled. Most of those functions are based on Bayesian optimization @cite @cite . One popular strategy is to select @math such that it maximizes the expected improvement @cite .
- As an alternative to Bayesian optimization, @cite proposes to use Monte-Carlo Tree Search to iteratively explore a tree-structured search space while pruning the less promising configurations.
- In @cite , the authors uses a genetic algorithm to sample a subset of representative input vectors in order to speed-up the model training while increasing the model performances. Genetic algorithms are also used to search for the whole pipeline as in @cite or @cite .
- The intrinsic difficulty of building a machine learning pipeline lies in the nature of the search space: the objective is non-separable i.e., the marginal performance of an operator @math depends on all the operators in all the paths leading to @math from the source, within the configuration space of a specific operator @math , there might be some dependencies between the hyperparameters (e.g. for Neural Networks, the coefficients @math , @math and @math make sense only for Adam solver @cite ). Therefore, building a machine learning pipeline is a mix between selecting a proper sequence of operations and, for each operation, selecting the proper configuration in a structured and conditional space. On the contrary, most systems handle the problem by aggregating the whole search space, losing the sequential aspect of it. A notable exception is @cite , inspired by , that explores the search space in terms of actions on operators (insertion, deletion, etc.).
- To the best of our knowledge, the only approach that uses a non-predetermined sequence of operators is @cite , but it is not possible to add additional constraints.
- @cite used a more classical approach to classify Twitter users as being at risk of depression or not. They first manually crafted features that describe users' online behavior and characterize their speech. The measures were computed daily, so a user is represented as the time series of the features. Then, the training and predictions were done by a svm with PCA for dimensionality reduction.
- More similarly to our approach, @cite used Hierarchical Attention Networks @cite to represent user-generated documents. Sentence representations are learned using a rnn with an attention mechanism and are then used to learn the document's representation using the same network architecture. The computation of the attention weights they use is different from ours as it is non-parametric. Their equivalent of equation would be This means that the rnn learn the attention weights along with the representation of the sequences themselves. This attention function has been introduced in @cite under that name of .
- The @cite is a simpler version of the that we used, that only takes into account the target hidden state. It is stated as such :
- The function introduced in @cite , has been improved in @cite . use a concatenation layer to combine the information of the hidden state and the context vector.
- was developed as part of Neural Turing Machines @cite , where the attention is focused on inputs that are similar to the values in memory.
- In the IoT healthcare and BAN domain, all of the existing solutions do sensing and communication in the digital domain (using ADCs DACs Microprocessors), which needs more power than analog sensing and communication. For example, Yang's group @cite developed a miniaturized node that incorporates wireless communication, on-board processing, nine-axis motion tracking, and other sensors @cite . It also developed an e-AR sensor @cite , a small device to be worn behind the ear that captures information about the balance of the wearer such as gait, posture, skelet al joint shock-wave transmission, and activity of the individual. Yuce's group developed techniques based on Ultra Wide Band (UWB) wireless technology to reduce the power consumption of body-worn sensors @cite @cite . Another important example is the activity recognition of the user using various body sensors viz., accelerometer and gyroscope data. The data from the sensors is digitally processed and stored into a wrist-band device that syncs the data with the mobile phone using Bluetooth technology @cite . Unlike these approaches, we adopt an entirely different approach based on analog sensing and communication that does not use any power-hungry ADCs (see Table ).
- Shannon mapping has been applied in a number of applications such as Software-Defined Radio (SDR) systems @cite , optical digital communications @cite , Compressive Sensing (CS) @cite , and digital video transmissions @cite . All these applications use power-hungry ADCs and other digital components making such implementations unsuitable for healthcare and other low-power IoT solutions. Some works have studied the N:1 spiral-type mapping @cite . The advantage of considering rectangular-type Shannon mapping is that there are existing low-power, all-analog circuit realizations for rectangular-type mapping (our previous work @cite ). Using this approach, sensors can be designed using all-analog circuits that can compress multiple signals into one signal, thereby consuming less power. The signals from multiple sensors are multiplexed at different frequency locations in an interleaved pattern. Similar pattern has been studied for topics of pilot placement @cite @cite .
- This paper is the first work to propose this structure for multiplexing data of AJSCC sensors. Table compares the power numbers of our circuit @cite with state-of-the-art wireless sensor motes (all of which are digital). We can notice that @math is possible with our circuit, which is essential in low-power applications. The existing circuit realizations of spiral-type mapping also are all based on digital circuits and systems @cite . In this scenario, it is worth noting that the Hybrid Digital Analog (HDA) coding can also perform signal compression @cite @cite . However, the digital part still needs digitization of the signals. Contrary to all these approaches, we propose signal compression and encoding in the analog domain with no need for ADCs DACs Microprocessors. To show the feasibility of our vision (analog compression and encoding), we previously developed a novel circuit to compress two signals @cite and verified its applicability to two pathological signals (molecular biomarkers and physiological signal) @cite . In this paper, we extend the theory for N-dimensional signal compression and propose novel multiplexing techniques that address the above mentioned challenges of scalability and power in the context of healthcare IoT as one of the key applications.
- On the side of shape features, they can be extracted based on botanical characteristics @cite @cite . These features may include: Aspect Ratio, Rectangularity, Convex Area, Ratio, Convex Perimeter Ratio, Sphericity, Circularity, Eccentricity, Form Factor, etc. @cite discussed some other features applied on leave shapes and introduced two new multiscale triangle representations. There are also a lot of other work done with more in-depth design aiming for general shapes than just leaves. @cite defines inner distance of shape contours to build shape descriptors. @cite develops the visual descriptor called CENTRIST (CENsus TRansform hISTogram) for scene recognitions, it get good performance when applied to leave images. Authors of @cite uses the transformation form shape contours to 1 dimensional time series and present the method of shapelet for shape recognition. @cite describes a hierarchical representation for two dimensional objects that captures shape information at multiple levels of resolution for matching deformable shapes. Features coming from different method can be stacked together, these bagged features can usually help provide better performance as discussed in @cite .
- Compared with methods mentioned above which tackles the difficulty in classification by designing complicated hand crafted deep features, convolutional neural networks (CNN) @cite can take simple features as input and automatically abstracts useful features through its early convolutional blocks for later classification tasks @cite . In this way, the difficulty is transferred into heavy computation where modern hardware now can provide sufficient support. It is more straightforward if we apply a CNN directly on leave images combining feature extraction task and classification task together, but this will make a model of unnecessary large size with a lot of parameters and they usually require a lot of data and time to be trained well with more risk of overfitting the data at hand. The key idea of this paper is to take the advantage of convolutional architecture, but apply it on the extracted single 1d CCDC feature to reduce the computational cost.
- The need for the design of new initial cell-search phase in mmWave communication has brought great research interest in this field recently. In IEEE 802.11ad standard, a coarse-grained sector matching is followed by a second beam training stage that provides a further refinement of the beamforming vectors @cite @cite . The authors in @cite proposed a similar hierarchical design with multi-resolution codebook based on ideas from compressive sensing. Reference @cite provided a framework to evaluate the performance of mmWave IA using 3GPP new radio (NR) scenario configurations. In @cite , the authors analyzed various design options for IA given different scanning and signaling procedures. Specifically, the synchronization and cell-search consists of a sending a series of directional pilots to enable a joint time-frequency-spatial synchronization to occur jointly. @cite and @cite investigated initial cell-search based on context information which uses external localization service to get positioning information. In @cite , the performance of these schemes are summarized in terms of cell detection failure probability and latency. In contrast to the aforementioned link-level studies, @cite and @cite provide a system-level analysis of IA protocols in terms of cell-search latency under different user equipment (UE) status.
- Methods for identifying relevant groups is an active area of research; a great deal of work is on graph-based data such as social or information networks. Community detection is then based on choosing an objective function that captures the intuition of a community as a set of nodes with better internal connectivity than external connectivity @cite . This is a rich area of research and briefly summarizing, there is work across spectral algorithms @cite , measures of centrality @cite and matrix factorization @cite .
- Neural representation has been implemented for social media short messages albeit in different ways than we propose @cite . Gaussian Restricted Boltzmann Machines (RBM) have been used for modeling user's posts within a social network to identify their topics of interest, and finally construct communities @cite . However, a parametric approach was used in which it was necessary to specify the number of clusters communities. In addition to not requiring such initialization, this approach is conceptually different than our proposed work in that it directly maps individuals to communities (instead of mapping the content of their posts, which may better capture heterogeneous community memberships). Further, content-based density approachefs as proposed, versus parametric ones could potentially learn a more organic number of communities. Given this gap, and the fact that content-based community detection (opposed to graph-based) may be more pertinent in health-related communities, here we explore content-based clustering of health communities.
- Approximation algorithms offer rigorous mathematical tools and fold a protein structure within polynomial time. However, it may lead to a weak approximation ratio, resulting in a structure far from the optimal solution. Hart and Istrail proposed an approximation algorithm with ratio 3 8 of the optimal score for the 3D cubic lattice structure @cite . An improved approximation algorithm with 2 5 performance guarantees was further developed by the same authors @cite . For the 2D square lattice, an approximation algorithm @cite can achieve the approximation ratio of 1 3.
- Heuristic algorithms cannot guarantee the optimal solution, but they usually obtain an approximation solution in a reasonable time frame. Beutler and Dill introduced a Core-directed chain Growth method (CG) using a heuristic bias function to help assemble a hydrophobic core @cite . Ant colony optimization based algorithms were developed by Shmygelska @cite and Thalheim @cite . proposed a new Monte Carlo method, fragment regrowth via energy-guided sequential sampling @cite . Other techniques, such as simulated annealing @cite , quantum annealing @cite , genetic algorithms @cite and reinforcement learning @cite , were also applied to the HP model with limited success and scalability.
- In the parametric optimization setting, @cite develop an optimization model that encodes KKT optimality conditions for imputing objective function coefficients of a convex optimization problem. @cite focus on the same problem under the assumption of noisy measurements, developing a bilevel problem and two algorithms which are shown to maintain statistical consistency. Saez-Gallego and Morales @cite address the case of learning @math and @math jointly in a parametric setting where the @math vector is assumed to be an affine function of a regressor. The general case of learning the weights of a parametric linear optimization problem where @math , @math and @math are functions of @math (Figure (iii)) has not been addressed in the literature.
- Recent work in machine learning @cite @cite @cite views inverse optimization through the lens of online learning, where new observations appear over time rather than as one batch. Our approach may be applicable in online settings, but we focus on generality in the batch setting and do not investigate real-time cases.
- Methodologically, our unrolling strategy is similar to @cite who directly optimize the hyperparameters of a neural network training procedure with gradient descent. Conceptually, the closest papers to our work are by Amos and Kolter @cite and Donti, Amos and Kolter @cite . However, these papers are written independently of the inverse optimization literature. Amos and Kolter @cite present the OptNet framework, which integrates a quadratic optimization layer in a deep neural network. The gradients for updating the coefficients of the optimization problem are derived through implicit differentiation. This approach involves taking matrix differentials of the KKT conditions for the optimization problem in question, while our strategy is based on allowing a deep learning framework to unroll an existing optimization procedure. Their method has efficiency advantages, while our unrolling approach is easily applicable, including to processes for which the KKT conditions may not hold or are difficult to implicitly differentiate. We include a more in-depth discussion in .
- @cite @cite proposed to extract deep features to divide the problem space and use simple probabilistic distribution at leaf nodes. These models enabled traditional decision regression trees with deep representation learning ability. Leaf node update rules were proposed based on convex optimization techniques, and they out-performed deep models without divide-and-conquer strategy. However, since the last layer of a deep model was used to divide the problem space, every path in the tree needs to be computed. Even when a branch of computation contributes little to the final prediction, it stills need evaluation because each splitting node requires the full forward-pass of the deep neural network. A model structure where each splitting node is separately evaluated was used @cite for depth estimation, but a general framework was missing and the effect of computation path pruning was not investigated.
- Pixel-difference feature is a special type of hand-crafted feature where only several pixels from an input are considered during its evaluation. They are thus efficient to compute and succeeded in computer vision tasks such as face detection @cite , face alignment @cite @cite @cite @cite @cite , pose estimation @cite @cite and body part classification @cite . These features were also naturally incorporated into decision regression trees to divide the input feature space. A counterpart of sparse feature extraction process in CNNs is sparse convolution where the few non-zero entries in the convolution kernel determine the feature extraction process. To obtain a sparse convolution kernel, sparse decomposition @cite and pruning @cite techniques were proposed to sparsify a pre-trained dense CNN. @cite proposed an alternative where random sparse kernel was initialized before the training process. While they focus on speeding up CNNs, there have not been study on using these sparse convolutional layers in problem space dividing process, as traditional pixel-difference feature was used in decision trees.
- Popular kernel-based tests include FaST-LMM-Set @cite @cite , the sequence kernel association test (SKAT, @cite ) and optimal SKAT (SKAT-O, @cite ), which are based on weighted linear kernels @math @cite @cite @cite , or a linear combination of weighted linear and collapsing kernels @cite . Newer approaches @cite derive further data-adaptive combinations of linear, quadratic, IBS, and collapsing kernels. However, all these kernels provide suboptimal performance for the analysis of very rare genetic variants. Here, linear and quadratic kernels yield uninformative similarity measures (i.e., diagonal kernel matrices for singletons, which are variants with only one observed copy of the minor allele) and collapsing kernels often yield unspecific signals and aggregate noise.
- Instructional video understanding has received much attention recently. @cite and @cite both leverage the natural language annotation of the videos to learn the instructional procedure in videos. @cite , however, propose to learn the temporal boundaries of different steps in a supervised manner without the aid of textual information. Dense captioning is also posed on instructional videos in @cite , which aims at localizing temporal events from a video, and describing them with natural language sentences. Visual-linguistic ambiguities can be a common problem in instructional videos with narratives. @cite focus on such ambiguities caused by the changing in visual appearance and referring expression, and aim to resolve references with no supervision. @cite perform visual grounding task in instructional videos, also coping with visual-linguistic ambiguities. Yet, none of these works have tackled the QA problem on instructional videos, despite the uniqueness for instructional videos to perform reasoning.
- People are gaining interests in video question answering (VideoQA) in recent years. Most of the current VideoQA tasks are focusing on direct facts in short videos @cite @cite @cite @cite @cite . They all automatically generate QA pairs using a state-of-the-art question generation algorithm proposed in @cite . However, such auto-generation mechanism often generates QA pairs with poor quality and low diversity, though grammatically correct. Worse still, auto-generated QA pairs cannot involve reasoning. From the reasoning point of view, MovieQA @cite use human annotated QA pairs on movies to evaluate automatic story comprehension. SVQA @cite , following the step of @cite , extend the CLEVR dataset to the video version. Yet, it still focuses on short-term relations, and does not fit natural settings.
- VQA is an AI complete task that requires high-level multimodal reasoning both in visual and language domains. The recent literature in VQA mostly focus on the optimal mechanisms to fuse multimodal cues. A simple fusion approach was used by Lu al @cite that progressively combines multimodal features using concatenations and sum-pooling operations. Xu al @cite proposed a recurrent neural network to generate intelligent image captions by considering the previously predicted words and targeted visual content. Bilinear models provide an effective way to model complex interactions, but impose restrictions due to computational intractability for high-dimensional inputs. Efficient versions of bilinear pooling were used in @cite @cite to learn the second-order interactions between visual and language features. To further speed-up the computations, Ben-younes al @cite introduced a Tucker fusion scheme that first projects the individual modalities to low dimensions and subsequently learns full bilinear relationships. Recently, Farazi al @cite fused complementary object level features alongside image level descriptors to achieve superior performance.
- Although most VQA approaches only work with the given training set, some efforts explore the use of supplementary information to help the VQA system. Generally, such methods employ external knowledge sources (both textual and visual) to augment the training set. For example, Teney al in @cite @cite used web searches to find related images which were used for answer prediction. Language based external knowledge bases were used by Wang al @cite and Wu al @cite to provide logical reasons for each answer choice and to answer a more diverse set of questions. More recently, Teney al @cite proposed a meta-learning approach that learns to use an externally supplied support set comprising of example questions-answers. In contrast to these approaches, we do not use any external data, rather learn an attention function that learns to use similar examples from the training set to provide better inference-time predictions. Patro al @cite proposed a differential attention mechanism that uses an exemplar from the training set to generates human-like attention maps, however does not consider a transferable attention function that can reason about new visual semantic concepts.
- An approach was suggested by @cite using an image processing inspiration for surface de-noising. It removes noise by applying a Weiner filter which approximates the components of the surface as a statistical distribution. There are two problems with this algorithm in our context. First, it needs to be decided this algorithm should be applied, unnecessary smoothing might remove features that describe the underlying geometry, although there is some attempt to apply a surface based anisotropic diffusion to preserve edges. In addition, the formula used requires the user to both know and supply the noise denoted by the variance @math @cite . It may not be possible to determine the noise of the data as it is an unstructured point cloud.
- Yumer and Kara suggest a NN regression method of surface fitting and hole stitching. The flexibility achieved by an adaptive neural network topology differs from previous attempts as the ideal topology of the network obtained (the hyperaparmeters) are not fixed @cite , meaning the network can be tailored to each point cloud automatically. This method is good for removing noise as the underlying geometry of the point data and not random noise is represented in the final surface.
- In a slightly different problem, where a NN is used to reconstruct the shape of a 3D object from its shading in a 2D @cite . show from experiment that quantitative improvement does not necessarily lead to quantitative improvement. This is something to consider when using a 'black box' function like a neural network, especially where there could be some information loss. In this regard we must ensure that the final model is representative of the ground truth and not only rely on an error measure. It is suggested that more research must be done for 3D surface quality metrics @cite . Visual quality will be assessed in the method presented here alongside quantitative results in the absence of quality metrics.
- SMDP generalizes MDP by considering the random sojourn time at each state, and is widely applied to machine maintenance @cite , resource allocation @cite , and cybersecurity @cite . However, as far as we know, it is the first time that the SMDP is applied to determine the optimal attacker engagement policy and to quantify the trade-off between the investigation reward and the risks.
- In learning classifier with web data, previous works focus on handling the label noise in three directions, removing label noise @cite @cite @cite @cite @cite @cite @cite , building noise-robust model @cite @cite @cite @cite @cite @cite , and curriculum learning @cite @cite .
- For label noise removal, some approaches address the label noise issue as outlier detection in an unsupervised manner. Xia al @cite removes outlier images by using the reconstruction errors of an autoencoder. CleanNet @cite used a fraction of manually-verified data to transfer the knowledge of label noise to other categories. For noise-robust model designing, Patrini al @cite proposed to train DNN models with a loss correction framework, which is insensitive to class-dependent label noise. Sukhbaatar al @cite developed an extra label flip layer which is enabled to match the noisy label distribution and absorb the noise. For curriculum learning @cite , MentorNet @cite designed an additional network to weight training examples and enforce the model to focus more on clean samples. CurriculumNet @cite measured the distribution density of images in their feature space and ranked them for model training.
- More recently, memory networks have been employed for one-shot learning @cite @cite , few-shot learning @cite , and semi-supervised learning @cite . Specifically, Kaiser al @cite designed a memory augmented network that could do life-long one-short learning. By adding an abstracting memory module, CMN @cite encoded videos into fixed-size features via a multi-saliency embedding algorithm. MA-DNN @cite leveraged the assimilation-accommodation interaction in memory networks for semi-supervised learning.
- The recent successes of deep learning in image classification @cite and object recognition @cite have encouraged the application of neural networks in place recognition. In early work, the pre-trained AlexNet @cite network is used to produce a feature vector out of the Conv3 layer @cite @cite . Rather than simply using a pre-trained network, NetVLAD learns visual place recognition end-to-end. In NetVLAD, triplet loss is used to find the optimal VLAD encoding to match scenes across both viewpoint and condition variations @cite . LoST uses the semantic CNN RefineNet @cite to select salient keypoints within the width by height dimensions of a convolutional tensor @cite . In a related work, these keypoints have been found by observing the activations out of a late convolutional layer @cite . The aforementioned examples involve improving a pre-trained neural network for place recognition, either by re-training, or selecting the most useful components out of the network activations.
- Formal methods have long been used to analyze network protocols @cite @cite @cite @cite @cite , often with a focus on security. However, even if the formal analysis of a network protocol has successfully proven a property, be it related to correctness or security, it is by no means guaranteed that this property will also hold for an implementation of said protocol.
- Programs have also been analyzed with formal methods, such as symex , to test for obvious problems like memory safety and assertion violations @cite @cite and for less easily checked properties, such as liveness violations @cite and authentication bypass flaws in firmware binaries @cite . One of the main problems encountered when formally analyzing real-world code is the penchant of the state-space to grow infeasibly large---a problem also known as . Many different approaches to tame the state explosion problem inherent in symex have been proposed in the past: state merging @cite , targeted search strategies @cite and pruning of provably equivalent paths @cite , to name a few.
- As the the state explosion problem grows exponentially with the number of programs considered at the same time, approaches explicitly targeting distributed programs have been developed. For example, KleeNet @cite @cite exploits the independence of networked programs by delaying codependent path forks until messages are received at each node that require the fork to be actualized.
- Testing protocols and programs independently is -- however worthwhile -- not enough. To this end, approaches have been designed that test implementations for protocol compliance using many different testing and verification methodologies, ranging from fuzzing @cite @cite over symex @cite to model checking @cite @cite . Validating that a given implementation fulfills a specification or standard does, however, require a formalized representation to be available, which effectively constitutes another implementation of the specification.
- One way to circumvent this chicken-egg problem is to exploit the fact that any relevant standard will have multiple implementations, which enables the substitution of compliance testing with that of interoperability testing. While it is possible that neither implementation is technically compliant with the standard, it becomes more and more improbable that the standard is captured incorrectly by many different people in exactly the same manner. Due to the inherent state-explosion problem of interoperability testing (multiple different, or even all possible programs are considered at once), multiple approaches to specialized @cite and general @cite @cite interoperability testing have been proposed in the past.
- Our work can be considered an approach to @cite @cite , in which a target function is approximated by a surrogate that is cheaper to compute but introduces inaccuracy. In computer vision and image processing, some level of inaccuracy is often tolerable, due to the limits of human perception and the lack of a clearly delineated correct'' answer @cite . Approximation can be introduced at the hardware level, such as by using approximate adder circuits (e.g. @cite ), or at the software level by restructuring the algorithm.
- Our work can also be seen as an application of the semi-supervised learning paradigm @cite , where the learner is given both labeled and unlabeled training data. We take a bootstrapping or self-supervised'' approach @cite @cite , using elements of the processing pipeline as surrogate models to label the unlabeled examples. The imputed labels will contain errors, and thus techniques for learning from noisy labels @cite @cite are also relevant. Several works have shown that neural networks can be trained successfully based on noisy labels (e.g. @cite @cite @cite @cite ).
- Object detection is a classical computer vision problem @cite @cite @cite @cite . Modern work can be split broadly into two general approaches: Single stage detectors @cite @cite @cite @cite @cite are usually very fast, while multi-stage detectors @cite @cite @cite @cite perform a coarse proposal step followed by a fine-grained classification, and are usually more accurate. Most state-of-the-art systems are based on Faster R-CNN @cite , a two-step object detector that generates proposals, for each of which it crops features out of the last feature map of a backbone. Feature Pyramid Networks @cite are a popular extension that uses feature maps at multiple spatial resolutions to increase scale invariance.
- Visual search has a long history in perceptual psychology (reviewed, , by @cite ), although typically with simple visual patterns, while search for arbitrary objects in real scenes has been addressed only recently @cite @cite , and often using a natural language cue @cite .
- Few-Shot learning has seen great progress over the last years. A classic approach is based on metric learning using Siamese neural networks @cite @cite @cite , which -- due to its simplicity -- is also the approach we use. The metric learning approach has seen a number of improvements in recent years @cite @cite @cite @cite @cite . Other approaches are based on generative models @cite @cite , ideas from information retrieval @cite or employ meta learning @cite @cite @cite @cite @cite @cite @cite @cite @cite .
- There is related, but not directly comparable work on few-shot object detection. Some work focuses on settings with few (more than one) annotated training images per category @cite @cite , while others tackle the zero-shot setting based on only a textual description of the reference @cite @cite . Most closely related to our work is concurrent work based on Siamese networks for one-shot detection on an Omniglot-based dataset and for audio data @cite as well as work on fine-grained bird classification and localization in ImageNet images @cite , which tend to have only one or few instances per image. In contrast, we work on potentially cluttered real-world images.
- Until recently, there have been many attempts to recognize and translate sign language using deep learning (DL). @cite have introduced and evaluated several architectures for CNNs to predict the 3D joint locations of a hand given a depth map. @cite have developed a sign language recognition system that is robust in different video backgrounds by extracting signers using boundary and prior shape information. Then, the feature vector is constructed from the segmented signer and used as input to artificial neural network. An end-to-end sequence modelling using CNN-BLSTM architecture usually used for gesture recognition was proposed for large vocabulary sign language recognition with RWTH-PHOENIC-Weather 2014 @cite .
- At the same time, one of the most interesting breakthroughs in neural machine translation or even in the entire DL was introduced under the name of sequence-to-sequence (seq2seq)' @cite . The seq2seq model relies on a common framework called an encoder-decoder model with RNN cells such as LSTMs or GRUs. The seq2seq model proved its effectiveness in many sequence generation tasks by achieving almost the human-level performance @cite . Despite its effectiveness, the seq2seq model still has some drawbacks such as the input sequences of varying lengths being represented in fixed-size vectors and the vanishing gradient due to the long-term dependency between distant parts.
- @cite formalized a sign language translation based on the pre-existing framework of Neural Machine Translation (NMT) with word and spatial embeddings for target sequences and sign videos, respectively. The extracted non-linear frame from a sign video is converted into the spatial representation through @math CNN, and then it is tokenized. The sequence-to-sequence (seq2seq) based deep learning methods learns how to translate the spatio-temproal representation of signs into the spoken or written language. Recently, researchers developed a simple sign language recognition system based on bidirectional GRUs which just classifies a given sign language video into one of the classes that are predetermined @cite
- Various probability divergences have been proposed in the literature, such as Euclidean least-squares (when data ordering is known) @cite @cite @cite , Kullbeck-Liebler (KL) @cite , maximum mean discrepancy (MMD) @cite @cite @cite @cite , and the Wasserstein distance @cite , where trade-offs are often statistical (e.g., consistency, sample complexity) versus computational. Alignment problems are ill-posed since the space of @math is large, so structure is often necessary to constrain @math based on geometric assumptions. Compact manifolds like the Grassmann or Stiefel @cite @cite are primary choices when little information is present, as they preserve isometry. Non-isometric transformations, though richer, demand much more structure (e.g., manifold or graph structure) @cite @cite @cite @cite @cite .
- Principal components analysis (PCA), one of the most popular methods in data science, assumes a model where the top- @math principal components of a dataset provide the optimal rank- @math approximation under an Euclidean loss. This has been extended to robust (sparse errors) settings @cite , and multi- (union of) subspaces settings where data can be partitioned into disjoint subsets where each subset of data is locally low-rank @cite . Transfer learning methods based on subspace alignment @cite @cite @cite work well with zero-mean unimodal datasets, but struggle on more complicated modalities (e.g., Gaussian mixtures or union of subspaces) due to a mixing of covariances. Related to our work, @cite performs multi-subspace alignment by greedily assigning correspondences between subspaces using chordal distances; this however neglects sign ambiguities in principal directions since subspaces inadequately describe a distribution's shape.
- . Real-time databases have been studied since 1980s, and the key goal is to enable as many real-time transactions as possible to meet their respective time constraints @cite . Real-time databases are more concern with timeliness, not system speed @cite , due to a basis hypothesis that catastrophic consequences do not happen in the real world if a transaction is finished within the deadline. Hence, many of works focus on scheduling @cite @cite and transaction processing @cite @cite . Storing and processing all the data under periodic time constraint can avoid data loss and ensure temporal data consistency maybe enough for traditional real-time databases, but the transient feature of scientific event requires that the online query latency should be as low as possible. Only in this way, we can exploit the value of scientific data. Periodic survey cycle and the unpredictability of scientific event propose the new challenge for online big scientific data analysis.
- Various aspects of network parameter configuration have been studied in the literature, such as pilot power configuration, spectrum, handoff threshold, etc. Traditional approaches derive analytical relationship between network configuration and its performance based on communication theory, such as @cite @cite @cite @cite . Such approaches are often prohibitively complex, involve various approximations, and require a significant amount of input information (such as the number of users, the location of each user, etc.).
- Recently, learning-based methods are proposed @cite @cite @cite @cite . In @cite , the authors propose a tailored form of reinforcement learning to adaptively select the optimal antenna configuration in a time-varying environment. In @cite , the authors use Q-learning with compact state representation for traffic offloading. In @cite , the authors design a generalized global bandit algorithm to control the transmit power in the cellular coverage optimization problem. In all these papers, BS similarities are not considered, and thus require more exploration. In @cite , the authors study the pilot power configuration problem and design a Gibbs-sampling-based online learning algorithm so as to maximize the throughput of users. In comparison, they make the assumption that all BSs are equal while we allow different BSs to learn different mappings.
- Contextual bandit @cite is an extension of classic multi-armed bandit (MAB) problem @cite . One type of algorithm is the UCB-type such as Lin-UCB @cite , Kernel-UCB @cite , in which they assume the reward is a function of the context and trade off between the exploitation and exploration based on upper confident bound of the estimation @cite . The contextual bandit is also widely used in many application areas, such as news article recommendation @cite , clinical trials @cite .
- Isomorphism-based Subgraph Matching. In the labelled case, @cite used the spanning tree of the query graph to filter infeasible results. @cite observed the importance of matching order. In @cite , the authors proposed to utilize the symmetry properties in the data graph to compress the results. @cite proposed based on the core-forest-leaves'' matching order, and obtained performance gain by postponing the notorious cartesian product.
- The unlabelled case is also known as subgraph listing enumeration, and due to the gigantic (intermediate) results, people have been either seeking scalable algorithms in parallel, or devising techniques to compress the results. Other than the algorithms studied in this paper ( algorithms ), proposed the external-memory-based parallel algorithm DualSim @cite , which maintains the data graph in blocks on the disk, and matches the query graph by swapping in out blocks of data to improve I O efficiency.
- Incremental Subgraph Matching. Computing subgraph matching in a continuous context has recently drawn a lot of attentions. @cite proposed incremental algorithm that identifies a portion of the data graph affected by the update regarding the query. The authors in @cite used the join scheme as algorithms ( edge-at-a-time ). The algorithm maintained a left-deep join tree for the query, with each vertex maintaining a partial query and the corresponding partial results. Then one can compute the incremental answers of each partial query in response to the update, and utilizes the join tree to re-construct the results. Graphflow @cite solved incremental subgraph matching using join, in the sense that the incremental query can be transformed into @math independent joins, where @math is the number of query edges. Then they used the worst-case-optimal join algorithm to solve these joins in parallel. Most recently, proposed TurboFlux that maintains data-centric index for incremental queries, which achieves good tradeoff between performance and storage.
- Query Languages and Systems. As the increasing demand of subgraph matching in graph analysis, people start to investigate easy-use and highly expressive subgraph matching language. Neo4j introduced @cite , and now people are working on standardizing the semantics of subgraph matching based on Cypher @cite . Gradoop @cite is a system based on Apache Hadoop that translates a Cypher query into a MapReduce job. proposed based on relational semantics for graph processing, in which they leveraged worst-case optimal join algorithm to solve subgraph matching. Arabesque @cite was designed to solve graph mining (continuously computing frequent subgraphs) at scale, while it can be configured for single subgraph query.
- The scene text proposal idea is mainly inspired by the success of object proposal in many object detection systems. It has advantage in locating more possible text regions to offer higher detection recall. It's often evaluated according to the recall rate as well as the number of needed proposals - typically the smaller the better at a similar recall level @cite . False-positive scene text proposals are usually eliminated by either a text nontext classifier @cite @cite or a scene text recognition model @cite @cite in end-to-end scene text reading systems.
- Different scene text proposal approaches have been explored. One widely adopted approach combines generic object proposal techniques with text-specific features for scene text proposal generation. For example, EdgeBoxes @cite is combined with two text-specific features for scene text proposal generation @cite . In another work @cite , EdgeBoxes is combined with the Aggregate Channel Feature (ACF) and AdaBoost classifiers to search for text regions. In @cite , Selective Search @cite is combined with Maximally Stable Extremal Regions (MSER) to extract texture features for dendrogram grouping. A text-specific symmetry feature is explored in @cite to search for text line proposals directly, where false text line proposals are removed by training a CNN classifier. Deep features have also been used for scene text proposal due to its superior performance in recent years. For example, inception layers are built on top of the last convolution layer of the VGG16 for generating text proposal candidates in @cite . The Region Proposal Network (RPN) and Faster R-CNN structure are adopted for scene text proposal generation in @cite @cite .
- Most existing scene text proposal techniques have various limitations. For example, the EdgeBoxes based technique @cite is efficient but often generate a large number of false-positive proposals. The hand-crafted text-specific features rely heavily on object boundaries which are sensitive to image noise and degradation @cite . Techniques using heuristic rules and parameters @cite do not adapt well across datasets. The deep learning based technique @cite produces a small number of proposals but the recall rate becomes unstable when the Intersection over Union (IoU) threshold increases. As a comparison, our proposed proposal technique does not leverage heuristic parameters and obtains a high recall rate with a small number of false-positive proposals.
- A large number of scene text detection techniques have been reported in the literature. Sliding window has been widely used to search for texts in scene images @cite @cite @cite . However, it usually has a low efficiency because it adopts an exhaustive search process by using multiple windows of different sizes and aspect ratios. Region based techniques have been proposed to overcome the low efficiency constraint. For example, the Maximal Stable External Regions (MSRE) has been widely used @cite @cite @cite @cite for scene text detection. In addition, various hand-craft text-specific features have also been extensively investigated such as Stroke Width Transform (SWT) @cite , Stroke Feature Transform (SFT) @cite , text edge specific features @cite , Stroke End Keypoints (SEK), Stroke Bend Keypoints (SBK) @cite , and deep features based regions @cite @cite @cite . Different post-processing schemes have also been designed to remove false positives, e.g heuristic rules based classifier @cite @cite @cite @cite , graph processing @cite @cite , support vector regression @cite , convolutional K-mean @cite , distance metric learning @cite , AdaBoost @cite @cite , random forest @cite @cite , convolution neural network @cite @cite , etc.
- With the advance of convolutional neural network (CNN), different CNN models have been exploited for the scene text detection tasks. For example, the DeepText makes use of convolutional layers for deep features extraction and inception layers for bounding boxes predictions @cite . The TextBoxes @cite adopts the Single Shot Multiboxex Detector (SSD) @cite to deal with multi-scale texts in scenes. Quadrilateral anchor boxes have also been proposed for detecting tighter scene text boxes @cite . In addition, direct regression solution has also been proposed @cite to remove the hand-crafted anchor boxes. Different CNN based detection and learning schemes have also been explored. For example, some work adopts a bottom-up approach that first detection characters and then group them to words or text lines @cite @cite @cite . Some system instead defines a text boundary class for pixel-level scene text detection @cite @cite . In addition, weakly supervised and semi-supervised learning approach @cite has also been studied to address the image annotation constraint @cite .
- Quite a number of CNN based end-to-end scene text reading systems have been reported in recent years. In @cite @cite , a CNN based character recognition model is developed where word information is extracted from text saliency map using sliding windows. The same framework has been implemented in @cite , where a more robust end-to-end scene text reading system is developed by training a model handling three functions including text and non-text classification, case-insensitive characters recognition, and case-sensitive characters recognition. In @cite , an advanced end-to-end scene text reading system is designed where the Single Shot Multiboxes Detector (SSD) is employed for scene text detection and a transcription model proposed in @cite is adopted for recognition. End-to-end trainable scene text reading system has also been proposed which can concurrently produce texts location and text transcription @cite
- Our developed end-to-end scene text reading system adopts a similar framework as presented in @cite @cite that exploits proposals and existing scene text recognition models. One unique feature is that it uses only around one-fifth of the number of proposals that prior proposal based end-to-end systems use thanks to our proposed pooling based proposal technique and gradient histogram based proposal ranking.
- The work @cite provides an overview of deterministic clustering based on neural networks, and @cite proposes a biologically inspired network for online clustering. Our work differs from previous approaches in its use of neural networks to explicitly approximate fully Bayesian inference in a probabilistic generative clustering model. Similar amortized approaches to Bayesian inference have been explored in Bayesian networks @cite , sequential Monte Carlo @cite , probabilistic programming @cite @cite and particle tracking @cite . The representation of a set via a sum (or mean) of encoding vectors was also used in @cite @cite @cite @cite .
- Another approach to let the user extend the software framework is shown by some molecular dynamics packages. They use high level languages like Python or their own embedded domain specific language (DSL) to describe the simulation. In order to achieve good performance on various hardware architectures they use just-in-time compilation to generate user specific executables for various architectures. However, to support MPI or CUDA additional wrapper libraries like pyMPI and PyCUDA are needed. Many packages using this technique claim that this can be done with almost no loss in performance compared to native C++ code. Packages that provide such capabilities with a varying degree of just-in-time compilation are for example @cite , @cite , @cite and @cite .
- Data sanitization ( knowledge hiding) aims at concealing patterns modeling confidential knowledge by limiting their frequency, so that they are not easily mined from the data. Existing methods are applied to: (I) a of set-valued data (transactions) @cite or spatiotemporal data (trajectories) @cite ; (II) a of sequences @cite @cite ; or (III) a sequence @cite @cite @cite . Yet, none of these methods follows our CSD setting: Methods in category I are not applicable to string data, and those in categories II and III do not have guarantees on privacy-related constraints @cite or on utility-related properties @cite @cite @cite @cite . Specifically, unlike our approach, @cite cannot guarantee that all sensitive patterns are concealed (constraint C1 ), while @cite @cite @cite @cite do not guarantee the satisfaction of utility properties ( @math and P2 ).
- Anonymization aims to prevent the disclosure of individuals' identity and or information that individuals are not willing to be associated with @cite @cite . Anonymization works such as @cite @cite @cite are thus not alternatives to our work (see the appendix).
- One of the first fruitful connections between parameterized complexity and approximability was observed independently by Bazgan @cite and Cesati and Trevisan @cite : They showed that EPTASs, i.e., @math -approximation algorithms with @math time, imply fixed-parameter tractability for the decision version. Thus, proofs for 1 -hardness of the decision version became a strong tool for ruling out improvements of PTASs, with running time @math , to EPTASs. More recently, @cite improved this approach by directly proving 1 -hardness of obtaining a @math -approximation, thus bypassing the requirement of a 1 -hard decision version (see also @cite ).
- The systematic study of parameterized approximation as a field was initiated independently by three separate publications @cite @cite @cite . A very good introduction to the area including key definitions as well as a survey of earlier results that fit into the picture was given by Marx @cite . In particular, Marx also defined a so-called that, given input @math will run for @math time and return (say, for a maximization problem) a solution of value at least @math if the optimum is at least @math . As mentioned earlier, Marx pointed out that a standard FPT-approximation scheme that finds a solution of value at least @math in time @math if @math is not interesting to study: By setting @math we can decide the decision problem @math ?'' in FPT time. Thus, such a scheme is not helpful if the decision problem is 1 -hard and therefore unlikely to have an FPT-algorithm. Nevertheless, PASs can be useful in this case, as they imply standard FPT-approximation algorithms with ratio @math for each fixed @math despite 1 -hardness.
- A central goal of parameterized approximation is to settle the status of problems like Dominating Set or Clique , which are hard to approximate and also parameterized intractable. Recently, Chen and Lin @cite made important progress by showing that Dominating Set admits no constant-factor approximation with running time @math unless @math . Generally, for problems without exact FPT-algorithms, the goal is to find out whether one can beat inapproximability bounds by allowing FPT-time in some parameter; see e.g. @cite @cite @cite @cite @cite @cite @cite @cite @cite ).
- For the special case of where all input objects are squares a PTAS is known @cite but there can be no EPTAS @cite . Recently, @cite found polynomial-time algorithms for and with approximation ratio smaller than @math (also for the weighted case). For the special case that all input objects are squares there is a PTAS @cite and even an EPTAS @cite .
- Previous works on WSI used context vectors and attributes @cite , pretrained classification systems @cite , and alignment of parallel corpus @cite . In the most recent shared task on WSI @cite , top models used lexical substitution method () @cite and Hierarchical Dirichlet Process trained with additional instances () @cite .
- Latent variable models such as LDA @cite are used to induce the word sense of a target word after rigorous preprocessing and feature extraction (, ) @cite . More recent models introduced a latent variable for the sense of a word, with the assumption that a sense has multiple concepts (, ) @cite and that topics and senses should be inferred jointly () @cite . In this paper, we also use a separate sense latent variable, however we show boost in performance by representing it with more versatility and by incorporating the use of target-neighbor pairs. HC was also extended to a nonparametric model () @cite in order to automatically set the number of senses of a word, providing flexibility to the sense granularity @cite @cite @cite . In our experiments, we show that the sense granularity induced from nonparametric models are incorrect making the models less effective.
- In the unsupervised author name disambiguation (UAND) domain, LDA-based models have also been used @cite to employ text features for the task, while non-text features such as co-authors, publication venue, year, and citations are found to be stronger features @cite . In this paper, we study on how to improve the performance of text features for UAND using latent variable models, which can later be combined with non-text features in the future work.
- Detecting 2D bounding boxes in images is a widely studied problem and recent approaches are able to excel even on the most formidable datasets @cite @cite @cite . Existing methods may broadly be divided into two main categories: detectors such as YOLO @cite , SSD @cite and RetinaNet @cite which predict object bounding boxes directly and detectors such as Faster R-CNN @cite and FPN @cite which add an intermediate region proposal stage. To date the vast majority of 3D object detection methods have adopted the latter philosophy, in part due to the difficulty in mapping from fixed-sized regions in 3D space to variable-sized regions in the image space. We overcome this limitation via our OFT transform, allowing us to take advantage of the purported speed and accuracy benefits @cite of a single-stage architecture.
- Obtaining 3D bounding boxes from images, meanwhile, is a much more challenging problem on account of the absence of absolute depth information. Many approaches start from 2D bounding boxes extracted using standard detectors described above, upon which they either directly regress 3D pose parameters for each region @cite @cite @cite @cite or fit 3D templates to the image @cite @cite @cite @cite . Perhaps most closely related to our work is Mono3D @cite which densely spans the 3D space with 3D bounding box proposals and then scores each using a variety of image-based features. Other works which explore the idea of dense 3D proposals in the world space are 3DOP @cite and Pham and Jeon @cite , which rely on explicit estimates of depth using stereo geometry. A major limitation of all the above works is that each region proposal or bounding box is treated independently, precluding any joint reasoning about the 3D configuration of the scene. Our method performs a similar feature aggregation step to @cite , but applies a secondary convolutional network to the resulting proposals whilst retaining their spatial configuration.
- Integral images have been fundamentally associated with object detection ever since their introduction in the seminal work of Viola and Jones @cite . They have formed an important component in many contemporary 3D object detection approaches including AVOD @cite , MV3D @cite , Mono3D @cite and 3DOP @cite . In all of these cases however, integral images do not backpropagate gradients or form part of a fully end-to-end deep learning architecture. To our knowledge, the only prior work to do so is that of Kasagi al @cite , which combines a convolutional layer and an average pooling layer to reduce computational cost.
- Recent work shows that maximum likelihood training could be sub-optimal due to the different conditions between training and test modes @cite @cite . In order to address the exposure bias and the loss which does not operate at the sequence level, Ranzato:2016:ICLR employ the REINFORCE algorithm @cite to decide whether or not tokens from a sampled prediction could contribute to a high task-specific score ( BLEU). bahdanau2016actor use the actor-critic method from reinforcement learning to directly optimize a task-specific score.
- Recently, adversarial learning @cite has been successfully applied to neural machine translation @cite @cite @cite . In the adversarial framework, NMT models generally serve as the generator which defines the policy to generate the target sentence y given the source sentence x . A discriminator tries to distinguish the translation result @math from the human-generated one @math , given the source sentence @math .
- Inadequate translation problem is a commonly-cited weakness of NMT models @cite . A number of recent efforts have explored ways to alleviate this problem. For example, tu2016modeling and Mi:2016:EMNLP employ coverage vector as a lexical-level indicator to indicate whether a source word is translated or not. Zheng:2018:TACL and Meng:2018:IJCAI move one step further and directly model translated and untranslated source contents by operating on the attention context vector. He:2017:NIPS use a prediction network to estimate the future cost of translating the uncovered source words. Our approach is complementary to theirs since they model the adequacy learning at the word-level inside the generator (i.e., NMT models), while we model it at the sequence-level outside the generator. We take the representative coverage mechanism @cite as another stronger baseline model for its simplicity and efficiency, and experimental results show that our model can further improve performance.
- '' denotes discriminator and O '' denotes orientator. MRT '' indicates minimum risk training @cite , and D @math '' indicates adversarial training with a CNN-based discriminator @cite . # Para.'' denotes the number of parameters, and Speed'' denotes the training speed (words second). @math '' and @math '' indicate statistically significant difference ( @math and @math respectively) from the corresponding baseline.
- On image processing tasks, cropping images is common approach of data augmentation @cite . In this paper, we propose to slice fragments from parallel utterances according to text alignment and use them as training samples. This technique could make use of more alignment information within the parallel utterances and is expected to reduce overfitting of the built seq2seq model.
- In object classification, Incremental learning (IL) is the process of increasing the breadth of an object classifier, by training it to recognize new classes, while retaining its knowledge of the classes on which it has been trained originally. In the past couple of years, there has been considerable research efforts in this field @cite @cite . Moreover, there exist several subsets of this research problem which impose different constraints in terms of data storage and evaluation. We can divide existing methods based on their constraints:
- In this problem, a model trained to perform object classification on specific classes of a dataset is incrementally trained to classify new unseen classes in the same dataset. Most of the existing work exploring this problem use single-headed evaluation. This makes the CI problem more difficult than the TI problem because the model can confuse the new class with a base class in the CI problem. iCaRL @cite belongs to this category. In iCaRL @cite , propose a technique to jointly learn feature representation and classifiers. They also introduce a strategy to select exemplars which is used in combination with the distillation loss to prevent catastrophic forgetting. In addition, a new baseline: LwF-MC is introduced in @cite , which is a class incremental version of LwF @cite . LwF-MC uses the distillation loss to preserve the knowledge of base classes along with a classification loss, without storing the data of base classes and is evaluated using single-headed evaluation. Another work aiming to solve the CI problem is @cite , which evaluates using both single-headed and multi-headed evaluations and highlight their difference. @cite introduce metrics to quantify forgetting and intransigence, and also propose an algorithm: Riemannian walk to incrementally learn classes.
- Several experiments have been proposed to use a small percentage of the data of base classes while training the classifier to learn new classes. iCaRL @cite uses the exemplars of base classes, while incrementally learning new classes. Similarly, @cite also use a fraction of the data of base classes. @cite also show that this is especially useful for alleviating intransigence, which is a problem faced in single-headed evaluation. However, storing data for base classes increases memory requirement at each incremental step, which is not feasible when the memory budget is limited.
- Several TI methods described earlier (such as @cite @cite ) do not use the information about base classes while training the classifier to learn new classes incrementally. To the best of our knowledge, LwF-MC @cite is the only CI method which does not use base class data but uses single-headed evaluation.
- We intend to explore the CI problem by proposing to constrain the attention maps of the teacher and student models to be equivalent (in addition to their prediction vectors), to improve the information preserving capability of LwF-MC @cite . In LwF-MC and our proposed method LwM, storing teacher models trained in previous incremental steps is not allowed since it would not be feasible to accumulate models from all the previous steps when the memory budget is limited.
- Besides graph models, recurrent neural networks (RNN)-based tracking also plays an important role in recent years @cite @cite @cite @cite @cite @cite . One advantage of RNN-based tracking is the ability of online prediction. However, along with the propagation of RNN block, the relation between two faraway detections becomes very weak. Without direct connections, the performance of RNN-based methods degrades in the long run and sometimes can be easily affected by unreliable detections.
- Features are very important in the tracking-by-detection framework. There are two types of features that are used in common, i.e., appearance features and temporal features. For appearance features, many works adopt CNN-based features from Re-ID tasks @cite @cite @cite . However, histogram-based features, like color histograms, HOG, and LBP, are still powerful if no training data is provided @cite . As for temporal features, the location, size, and motion of bounding boxes are commonly used. Given the appearance features and temporal features, the tracker can fuse them together using human defined weights @cite @cite @cite . Although @cite @cite propose RNN-based networks to combine features together, it is still empirical and difficult to determine the weight of each feature.
- Another category of tracking is based on end-to-end frameworks @cite @cite @cite , where we input raw video sequences and output object trajectory. In other words, the detection and tracking are trained jointly in a single-stage network. One major advantage of this framework is that the errors will not be accumulated from detection to tracking. The temporal information across frames can help improve the detection performance, while reliable detections can also feedback reliable tracking. However, such a framework requires a lot of training data. Without enough training data, overfitting becomes a severe problem. Unlike detection based training, tracking annotations for video sequences are usually hard to get, which becomes the major limitation of the end-to-end tracking framework.
- pmlr-v70-hoffman17a proposed another hybrid method based on VI and HMC without auxiliary approximation. The idea is to use a Monte Carlo estimation of the marginal likelihood by averaging over samples from HMC chains, that are initialized by variational distribution. In a very similar framework is proposed using Metropolis-adjusted Langevin dynamics. This idea is very similar to contrastive divergence in @cite . The main disadvantage of this methods is that the HMC parameters are manually pretuned. Especially, As mentioned by , No-U-turn Sampler (NUTS), an adaptive HMC, is not appliable due to engineering difficulties. @cite pointed out that HMC is very sensitive to the choice of Leapfrog step size and number of leaps.
- DeVise @cite uses a language model trained on Wikipedia text combined with a visual model to improve classification and enable zero-shot learning on the ImageNet dataset @cite @cite . The visual model is pre-trained without semantic aid and then fine-tuned to maximize a similarity measure between prediction and label in a semantic embedding, thereby improving performance.
- Rodner al show a method that trains Gaussian processes from few examples in @cite . It uses knowledge transfer between related classes to enable few-shot learning with reasonable accuracy. The WordNet hierarchy @cite supplies the relationships between concepts that are ultimately used to steer the knowledge transfer. When compared to individual GP learning of classes, knowledge transfer improves accuracy.
- Content-based image retrieval is an area that profits significantly from semantic information, especially when such systems are judged by a human ranking baseline. Vogel and Schiele propose an image representation in @cite that describes an image's semantics locally that is then used to rank images by semantic similarity the query. An approach specifically using taxonomic information is presented by Barz al in @cite , where an embedding space is constructed such that the dot product of label pairs matches a semantic similarity measure. They show that this label representation improves both image retrieval as well as image classification.
- The relationship between visual similarity and semantic similarity has been subject of previous investigation. In @cite , Deselaers and Ferrari consider a semantic similarity measure by Jiang and Conrath (see sec:semsiminf and @cite ) as well as category histograms, in conjunction with the ImageNet dataset. They propose a novel distance function based on semantic as well as visual similarity to use in a nearest neighbor setting that outperforms purely visual distance functions. The authors also show a positive correlation between visual and semantic similarity for their choice of similarity measures on the ImageNet dataset. Their selections of Jiang-Conrath distance and the GIST feature descriptor are also evaluated in our work, where we add several other methods to compare.
- Attention has been recently brought to the communicational aspects of multiagent systems. Recent research has explored cooperative @cite @cite @cite and semi-cooperative scenarios such as negotiation games @cite . The emergence of communication in adversarial scenarios has been explored less extensively.
- Generative adversarial networks (GANs) @cite have resulted in a wide range of interesting adversarial applications. However, the extension to sequence to sequence models (Seq2Seq) @cite has been difficult. Combining GAN with Seq2Seq models is challenging because discrete samples drawn from categorical distributions hinder backpropagation. In addition, alternatives on how to perform reward imputation to partial sequences @cite have been proposed.
- With respect to backpropagating errors, reparametrization @cite has been used multiple times to allow for backpropagation through stochastic nodes. In particular, Gumbel-Softmax @cite has allowed categorical distributions @cite in stochastic computational graphs. This technique has been shown as an alternative to reinforcement learning @cite within the scope of cooperative referential games. More recently, similar ideas resulted in SeqGAN @cite being proposed. Further incremental improvements have been published, such as applying actor-critic models @cite or combining with proximal policy optimization (PPO) @cite in order to improve learning performance.
- Twitter for emergency applications has been studied by several researchers, e.g., @cite @cite @cite @cite @cite . @cite , researchers concluded that Twitter was not yet ready for first responders. However, it was helpful for civilians. These were the early days of Twitter, as we find from @cite that individuals immediately posted specific information helpful to early recognition and characterization of emergency events'' in the case of the Boston marathon bombing. @cite , researchers found that tangible, useful information was found in the early period before storm system Sandy and it got buried in emotional tweets as the storm actually hit. However, we think more studies are needed on this issue, since the tweets collected were rather small, approximately 27,000, using just the hashtag #sandy. A bilingual analysis of tweets obtained over 84 days overlapping the Tohoku earthquake showed, among other results, the correlation between Twitter data and earthquake events @cite . A survey of this literature can be found in @cite .
- Researchers have examined the question of whether Twitter can replace newswire for breaking news @cite . They studied a period of 77 days in 2011 during which 27 events occurred. The biggest disasters in this event-set are: an airplane crash with 43 deaths, and a magnitude 5.8 earthquake in Virginia that caused infrastructural damage. None of these disasters, bad as they are, rise to the level of the Nepal Earthquake(s) of 2015 in which almost 10,000 lives were lost. They collected a large dataset of tweets and news articles, but then eliminated a large collection of tweets based on clustering. More elimination of tweets led to only 97 linked tweet-news article pairs, which is a small dataset.
- In @cite , a framework for connecting news articles to Twitter conversations is proposed using Local cosine similarity, global cosine similarity, local frequency of the hashtag and global frequency of the hashtag as the classification features extracted for each article-hashtag pair. The task of linking tweets with related news articles is studied in another paper to construct user profiles @cite . The authors proposed two sets of strategies to find relevant news articles to each tweet in this paper. In addition to URL-based strategies, which is similar to the idea used in @cite , they also proposed several content-based strategies that include computing the similarity between hashtag-based, entity-based and bag-of-word-based representations of tweets and news articles to discover the relation between them. In addition to user modeling, the tweet-news linking task has been employed in document summarization @cite , sentiment analysis @cite and event extraction @cite
- @cite , researchers proposed two methods that leverage tweets for ranking sentences in news articles for summarization: a voting method based on tweet hit counts of sentences, and a random walk on a heterogeneous graph (HGRW) consisting of tweets and news article sentences as nodes and the edge weights are defined by weighted idf-modified-cosine scores. The best ROUGE-1 F-score @cite is achieved by a version of HGRW that outputs both sentences from news articles and tweets in the summary, where the summary consists of top four sentences tweets as highlights of the article.
- Tweet summarization has also been studied, e.g., see @cite and references cited therein. Our problem is a little different, we consider tweets that are linked and found relevant (or partially relevant) to news articles from the perspective of summaries of those news articles. We then evaluate them to get an idea of how much content of the articles is captured by these tweets.
- There are a wide variety of hierarchical reinforcement learning approaches. One of the most widely applied HRL framework is the framework (). An option can be thought of as an action that extends over multiple timesteps thus providing the notion of temporal abstraction or subroutines in an MDP. Each option has its own policy (which is followed if the option is selected) and the termination condition (to stop the execution of that option). Many strategies are proposed for discovering options using task-specific hierarchies, such as pre-defined sub-goals , hand-designed features , or diversity-promoting priors . These approaches do not generalize well to new tasks. @cite proposed an approach to learn options in an end-to-end manner by parameterizing the intra-option policy as well as the policy and termination condition for all the options. Eigen-options use the eigenvalues of the Laplacian (for the transition graph induced by the MDP) to derive an intrinsic reward for discovering options as well as learning an intra-option policy.
- Video object detection tracking is a task in ILSVRC 2017 @cite , where the winning entries are optimized for accuracy rather than speed. @cite adopts flow aggregation @cite to improve the detection accuracy. @cite combines flow-based @cite and object tracking-based @cite tubelet generation @cite . THU-CAS @cite considers flow-based tracking @cite , object tracking @cite and data association @cite .
- Nevertheless, these methods combine multiple cues (e.g., flow aggregation in detection, and flow-based and object tracking-based tubelet generation) which are complementary but time-consuming. Moreover, they apply global post-processing such as seq-NMS @cite and tubelet NMS @cite which greatly improve the accuracy but are not suitable for a realtime and low latency scenario.
- Approaches to video object detection have been developed rapidly since the introduction of the ImageNet VID dataset @cite . @cite @cite propose a framework that consists of per-frame proposal generation, bounding box tracking and tubelet re-scoring. @cite proposes to detect frames sparsely and propagates features with optical flow. @cite proposes to aggregate features in nearby frames along the motion path to improve the feature quality. Futhermore, @cite proposes a high-performance approach by considering feature aggregation, partial feature updating and adaptive keyframe scheduling based on optical flow. Besides, @cite proposes to learn detection and tracking using a single network with a multi-task objective. @cite proposes to propagate the sparsely detected results through a space-time lattice. All the methods above focus on the accuracy of each individual frame. They either do not associate the presence of an object in different frames as a tracklet, or associate after performing object detection on each frame, which is time-consuming.
- Multiple object tracking (MOT) focuses on data association: finding the set of trajectories that best explains the given detections @cite . Existing approaches to MOT fall into two categories: batch and online mode. Batch mode approaches pose data association as a global optimization problem, which can be a min-cost max-flow problem @cite @cite , a continuous energy minimization problem @cite or a graph cut problem @cite @cite . Contrarily, online mode approaches are only allowed to solve the data association problem with the present and past frames. @cite formulates data association as a Markov decision process. @cite @cite employs recurrent neural networks (RNNs) for feature representation and data association.
- State-of-the-art MOT approaches aim to improve the data association performance given publicly-available detections since the introduction of the MOT challenge @cite . However, we focus on the sequential decision problem of detection or tracking. Although the widely-used Hungarian algorithm is adopted for simplicity and fairness in the experiments, we believe the incorporation of existing MOT approaches can further enhance the accuracy.
- Researchers have proposed approaches to adaptive keyframe scheduling beyond regular frame skipping in video analytics. @cite proposes to estimate the quality of optical flow, which relies on the time-consuming flow network. @cite proposes an to consider the size and motion of small objects, which is hand-crafted and more importantly, it is a detect-then-schedule paradigm but cannot determine to detect or track. @cite @cite learn to predict the discrepancy between the segmentation map of the current frame and the keyframe, which are only applicable to segmentation tasks.
- An infrared sensor array is a device composed of a small number of discrete infrared sensors. It represents the spatial distribution of temperature as a low-resolution image. Unlike colour cameras, infrared sensor arrays only capture the shape of the human body, therefore making individual identification harder. Additionally, the low spatial resolution also makes identification of individuals difficult. As this is more comfortable for users, it becomes more acceptable for installation in residential environments. Such infrared sensor arrays can be applied in many scenarios. A 4x4 sensor array has been used to recognise hand motion directions @cite , although the extremely low resolution of this sensor renders it unsuitable for more complex visual tasks. A 8x8 pixel sensor array has been successfully used to detect, count and track people indoors @cite . Human movements has also be inferred by using the subject's location and moving trajectory using a 16x16 sensor array @cite . Most recently, a multi-sensor system has been designed for human movement detection and activity recognition @cite , which our method will be compared against.
- The visual trace of human activity in video forms a spatio-temporal pattern. Here the salient features are well-developed for images captured by conventional visible-light RGB cameras @cite . However, the majority of well developed features, such as histogram of oriented gradients or optical flow, are not appropriate and applicable for very low resolution images such as those captured in this study, i.e. for 8x8 pixel resolution images.
- Several features have been investigated specifically for low resolution infrared sensors, most notably @cite . Here, connected component analysis was used to evaluate the number of individuals present in the scene, which subsequently led to motion tracking of the individuals; however this method was sensitive to background noise. A thermo-spatial sensitive histogram feature approach was able to reduce the noise from background pixels @cite . Although counting and tracking of individuals is a non-trivial task, here we are concerned with the activity of each individual. Intuitively, this would appear to require finer detail, and this poses a difficult task given the low spatial resolution of the image.
- A large amount of research is underway in the development of a smart sensing system to detect falls in home environments. However, the use of thermal infrared arrays for fall detection has to date not been widely investigated. Although a real-time system to recognise fall and non-fall events has been presented in @cite , their study overlooks the complexity of non-fall actions, where some actions, such as sitting down and inactivity, can be confused with falling @cite . Taking this into consideration, various non-fall activities are specifically incorporated in our dataset, including those most likely to be confused with falling.
- There is a long history of parameter estimation in network formation processes @cite @cite . Probably the most studied formation model is preferential attachment @cite , which relates the likelihood of new nodes connecting to a node @math to @math 's degree. More references on PA, since we claim it is the most studied? The original justification of the model was based on the degree distribution of the resulting graph qualitatively matching those of several early large-scale empirical networks. Shortly after, a number of authors proposed non-parametric methods to measure preferential attachment by counting what the degrees of the nodes getting edges are and normalizing them by the relative likelihood of those degrees @cite @cite @cite . There are several generalizations of preferential attachment, where node @math is chosen proportional to some function @math of the degree @math of node @math (here, @math is standard preferential attachment). TODO: Need references if claiming there are several generalizations Much of this research has focused on the case when @math , i.e., attachment is preferential to some (latent) power @math of the degree @cite .
- Recently, @cite proposed a principled method for estimating the attachment kernel. Their proposal corresponds precisely to maximum likelihood estimation of the attachment function as a conditional logit model with a parameter for every degree. Then, they find the corresponding @math using least squares, instead of doing with so maximum likelihood also. The same authors also proposed an extension of their work that includes node fitness @cite . Other recent work takes a maximum likelihood approach to estimating a mixture probability @math between uniform attachment and preferential attachment @cite . With their focus on characterizing individual formation models, these prior works fall short of the full potential of discrete choice modeling. For example, they don't consider the arbitrary combinations Can we take this thought one step further? Why do they fall short?
- There is also a connection with the literature on link prediction in social networks @cite . A network formation model implicitly makes claims about what edges are most likely to form next, and so can be evaluated by the same metrics as link prediction algorithms @cite . Features like distance, common neighbors and degree have been shown to be predictive of link formation in multiple contexts @cite @cite . However, in that literature, the main focus of interest is usually predictive accuracy, rather than a robust understanding of the drivers of formation. While we use predictive accuracy as a measure of goodness of fit, we are more concerned with interpretability of the model and estimates, which is one of the advantages of the conditional logit model.
- A related line of research studies the so-called stochastic actor-oriented model @cite @cite . This model combines multiple formation dynamics in a multinomial logit functional form, and develops connections between network formation and Markov chains in the space of graphs. However, they are impractical to estimate, especially for larger data sets. should increase distancing here? Need to return to the distancing here. Why is it impractical for large datasets?
- Estimating the parameters that drive edge formation is different from identifying the factors that could have lead to the observed graph. There is vast literature on pursuing the latter question by estimating a logit model with maximum likelihood, called exponential random graph models (ERGMs) @cite @cite @cite . However, these models do not consider individual edge events, are hard to estimate, and have known pathologies @cite @cite . Should check out TERGMs again, temporal ERGMs, and put that work in it's place here. I think Carter Butts is in that literature.
- Attempts to exploit preference strength information have led to various approaches for modeling, eliciting, measuring, and aggregating people's preference intensities in a variety of fields, including Likert scales, semantic differential scales, sliders, constant sum paired comparisons, graded pair comparisons, response times, willingness to pay, vote buying, and many others (see @cite @cite @cite for summaries). In our work we specifically consider only a small amount of coarse information about preference strengths, since obtaining detailed information is extremely difficult. Intuitively, any rule used to aggregate preference strengths must ask under what circumstances an apathetic majority' should win over a more passionate minority @cite , and we provide a partial answer to this question when the objective is to minimize distortion.
- Self-supervised learning is similar in flavor to unsupervised learning, where the goal is to learn visual representations from large-scale unlabeled images or videos without using any human annotations. Self-supervised representations are learned by first defining a pretext task, an objective function, for the model to solve and then producing proxy labels to guide the pretext task based solely on the visual information present in unlabeled data. The simplest self-supervised task is minimizing reconstruction error in autoencoders @cite to create low-dimensional feature representations, where the proxy labels are the values of the image pixels. More sophisticated self-supervised tasks such as image inpainting @cite , colorizing grayscale images @cite @cite , and predicting image rotations @cite have shown impressive results for unsupervised visual feature learning. The key to utilizing self-supervision for SSL is to learn useful features from unlabeled data through the pretext task that can be transferred and adapted to downstream supervised applications where labeled training data is scarce.
- Models belonging to the self-ensembling class, such as Pseudo-Ensembles @cite , Ladder networks @cite , @math model @cite and Mean Teacher @cite , utilize the output predictions on unlabeled data as proxy labels for SSL. This class of methods considers the model as a stochastic prediction function, in which different model configurations, such as dropout @cite and data augmentation, along with varying levels of noise in the input data can produce drastically different output predictions. The unsupervised objective of self-ensemble models is to minimize the mean squared error of multiple model outputs under random perturbations and data augmentation for the same training examples. The motivation behind this approach is to further regularize the model through the principle that perturbations in the input data and or data augmentation techniques should not significantly change the output of the model @cite . Self-ensembling approaches are robust to random perturbations and geometric transformations, and are currently among the state of the art in SSL on several benchmark image classification datasets.
- Rather than relying on the model to randomly perturb the input data by way of dropout or data augmentation, @cite proposed the concept of adversarial training to approximate the perturbations in the direction that would most significantly alter the output of the model. While adversarial training requires access to ground truth labels to perform adversarial perturbations, the Virtual Adversarial Training (VAT) mechanism proposed by @cite @cite can be applied to unlabeled data and is thus suitable for SSL under the consistency regularization principle. Adversarial training is closely related to generative adversarial networks (GANs) @cite , which have been proposed for semi-supervised learning with promising results @cite @cite @cite . Most recently, the self-supervised GANs with auxiliary rotation loss @cite have been shown to synthesize high-fidelity, diverse natural images at high resolution using only a fraction of the available labels.
- Knowledge graph embedding learning has been an active research area with applications directly in knowledge base completion (i.e. link prediction) and relation extractions. TransE @cite started this line of work by projecting both entities and relations into the same embedding vector space, with translational constraint of @math . Later works enhanced KG embedding models such as TransH @cite , TransR @cite , and TransD @cite introduced new representations of relational translation and thus increased model complexity. These models were categorized as translational distance models @cite or additive models, while DistMult @cite and ComplEx @cite are multiplicative models @cite , due to the multiplicative score functions used for computing entity-relation-entity triplet likelihood.
- The most recent KG embedding models are ConvE @cite and ConvKB @cite . ConvE was the first model using 2D convolutions over embeddings of different embedding dimensions, with the hope of extracting more feature interactions. ConvKB replaced 2D convolutions in ConvE with 1D convolutions, which constrains the convolutions to be the same embedding dimensions and keeps the translational property of TransE. ConvKB can be considered as a special case of Conv-TransE that only uses filters with width equal to @math . Although ConvKB was shown to be better than ConvE , the results on two datasets (FB15k-237 and WN18RR) were not consistent, so we leave these results out of our comparison table. The other major difference of ConvE and ConvKB is on the loss functions used in the models. ConvE used the cross-entropy loss that could be sped up with 1-N scoring in the decoder, while ConvKB used a hinge loss that was computed from positive examples and sampled negative examples. We take the decoder from ConvE because we can easily integrate the encoder of GCN and the decoder of ConvE into an end-to-end training framework, while ConvKB is not suitable for our approach.
- GCNs were first proposed in @cite where graph convolutional operations were defined in the Fourier domain. The eigendecomposition of the graph Laplacian caused intense computation. Later, smooth parametric spectral filters @cite @cite were introduced to achieve localization in the spatial domain and improve computational efficiency. Recently, @cite simplified these spectral methods by a first-order approximation with the Chebyshev polynomials. The spatial graph convolution approaches @cite define convolutions directly on graph, which sum up node features over all spatial neighbors using adjacency matrix.
- GCN models were mostly criticized for its huge memory requirement to scale to massive graphs. However, @cite developed a data efficient GCN algorithm called PinSage, which combined efficient random walks and graph convolutions to generate embeddings of nodes that incorporated both graph structure as well as node features. The experiments on Pinterest data were the largest application of deep graph embeddings to date with 3 billion nodes and 18 billion edges @cite . This success paves the way for a new generation of web-scale recommender systems based on GCNs. Therefore we believe that our proposed model could take advantage of huge graph structures and high computational efficiency of Conv-TransE .
- Generative Adversarial Networks (GANs) @cite have proven to be a powerful tool in many Computer Vision disciplines, such as image generation @cite , style transfer @cite , or super-resolution @cite . In the context of image to image translation @cite @cite , GANs are composed of a generator that aims to reproduce the target domain, and a discriminator that tells whether the output of the generator is close to the target distribution or not. Both are learnt simultaneously using the minimax strategy. Since the introduction of GANs, many improvements on adversarial learning have been proposed, including the Least-Squares GAN @cite , the Wasserstein GAN @cite @cite , the Geometric GAN @cite , or Spectral Normalisation @cite @cite @cite , however there is no consensus as to which exhibits a systematic improvement.
- Most of the methods mentioned above apply a self-consistency loss to preserve identity. As shown in Fig. , this loss limits existing methods to one-to-one mappings, and render generated images that are unable to be used as the basis for generating further images. These methods might leave a neutral face to a given expression @cite , or a non-frontal face to a frontal one @cite . In either case, the network is not required to perform more than one forward pass from a given image. Thus, a self-consistency loss is applied to either preserve appearance or identity. While this yields impressive results, it causes a mismatch between the input and target distributions, when a desired property would be to actually make them match. As we shall see, our proposed GANnotation does have this property, and thanks to that .
- There are some improvements based on revised Gaussian Kernel functions in order to get better similarity measurements. @cite proposes a symmetrised SNE, @cite enable t-SNE to accommodate various heavy-tailed embedding similarity functions; and @cite propose an algorithm based on similarity triplets of the form A is more similar to B than to C'' to model the local structure of the data more effectively.
- To reduce the runtime of t-SNE, @cite explores tree-based indexing schemes and uses the Barnes-Hut approximation to reduce the time complexity to @math . This gives a trade-off between speed and mapping quality. To further reduce the time complexity to @math , @cite utilise a fast Fourier transform to dramatically reduce the time of computing the gradient during each iteration. The method uses vantage-point trees and approximate nearest neighbours in dissimilarity calculation with rigorous bounds on the approximation error.
- There are some works focusing on analysing the heuristics methods for solving non-convex optimisation problems for the embedding . Recently, @cite theoretically analyse this optimisation and provide a framework to make clusterable data visually identifiable in the 2-dimensional embedding space. These works are not related to similarity measurements; therefore not directly relevant to work reported here.
- Recently, a large number of improvements of the original LSM-tree @cite have been proposed. Chen and Carey @cite survey these improvements, range from improving write performance @cite @cite @cite @cite @cite , reducing the buffer cache misses due to merges @cite @cite , supporting automatic design tuning of LSM-trees @cite @cite , to optimizing LSM-based secondary indexes @cite @cite . However, all of these efforts focus on the throughput of LSM-trees, while performance variances and write stalls are largely ignored.
- Performance stability has long been recognized as a critical performance metric. The TPC-C benchmark @cite measures not only absolute throughput, but also specifies the acceptable upper bounds for the percentile latencies of the transactions. @cite applied VProfiler @cite to identify major sources of variance in database transactions and proposed a variance-aware transaction scheduling algorithm. @cite proposed techniques to optimize parameterized queries while balancing the average and variance of query cost. To reduce the variance of query processing, most existing proposals have either emphasized the use of table scans @cite @cite @cite or stuck to worst-case query plans @cite @cite . Cao @cite conducted an experimental study of the performance variance of modern storage stacks; they found that variance is common in storage stacks and heavily depends on configurations and workloads. Dean and Barroso @cite discussed several engineering techniques to reduce performance variance in large-scale distributed systems at Google. Different from these efforts, in this work we focus on evaluating and minimizing the performance variances of LSM-trees due to their inherent out-of-place update design.
- There are several works in the literature on detecting PC-based botnets using their CnC (Command-and-control) server communication features. Bothunter @cite builds a based on which three bot-specific sensors are constructed and correlation is performed between inbound intrusion scan alarms and the infection dialog model to generate a consolidated report. Spatio-temporal similarities between bots in a botnet in terms of bot-CnC coordinated activities are captured from network traffic and leveraged towards botnet detection in a local area network in Botsniffer @cite . In BotMiner @cite , the authors have proposed a botnet detection system which clusters similar CnC communication traffic and similar malicious activity traffic, and uses cross cluster correlation to detect bots in a monitored network.
- There has also been some research on intrusion detection and anomaly detection systems for IoT. A whitelist-based intrusion detection system for IoT devices (Heimdall) has been presented in @cite . The authors in @cite propose an intrusion detection model for IoT backbone networks leveraging two-layer dimension reduction and two-tier classification techniques to detect U2R (User-to-Root) and R2L (Remote-to-Local) attacks.
- Of late, there has been an interest in IoT botnet and attack detection in the research community resulting in a number of papers addressing these problems. In @cite , deep-autoencoders based anomaly detection has been used to detect attacks launched from IoT botnets. A few works have focused on building normal communication profiles for IoT devices which are not expected to deviate much over a long period of time. DEFT @cite has used ML algorithms at SDN controllers and access gateways to build normal device traffic fingerprints while @cite proposes a tool to automatically generate MUD (Manufacturer Usage Description) profiles for a number of consumer IoT devices. In DIoT @cite , the authors have proposed a method to classify typically used IoT devices into various device types and build their normal traffic profiles so that a deviation from those profiles is flagged as anomalous traffic.
- Our work addresses a few important gaps in the literature when it comes to distinguishing between legitimate and botnet IoT traffic. First, the works on detecting botnets using their CnC communication features @cite @cite @cite @cite are designed for PC-based botnets rather than IoT botnets which are the focus of our work. Second, we do not aim to detect botnets (networks of bots) but instead, network activity generated by individual bots. IoT botnets tend to consist of hundreds of thousands to millions of devices spread over vast geographies, hence, it is impractical to detect a whole network of IoT bots. Therefore, we do not require computationally expensive clustering algorithms as used in @cite @cite .
- Third, unlike @cite @cite , we aim to detect IoT malware activity much before the actual attack, during the scanning infection phase. Finally, instead of fingerprinting the normal traffic of IoT devices @cite @cite and using those fingerprints towards anomaly detection, we detect the malware-induced scanning packet traffic generated by infected IoT devices. This is because the former approach suffers from limitations such as possibility of misclassification of an infected device as a legitimate device type, testing against only simple malware e.g. Mirai which may result in failure to detect other, more sophisticated malware, etc. The latter approach is not free from limitations as well, since it is not resilient against new undiscovered malware whose scanning traffic features have not been updated in the database. We advocate for a combined approach consisting of both IoT device fingerprinting anomaly detection and IoT malware scanning traffic detection.
- In literature, posteriors for Bayesian Neural Network models obtained by Hamiltonian Monte Carlo (HMC) @cite are frequently used as ground truth. However, HMC scales poorly on high dimensional parameter space and large datasets @cite @cite . Mini-batched versions of HMC, such as Stochastic Gradient Langevin Dynamics (SGLD) @cite and Stochastic Gadient HMC @cite , have been introduced to address the issue of scalability. However, these methods still suffer from lower mixing rate and are not theoretically guaranteed to converge to the true posterior when model assumptions are not met (e.g. when the true model of the gradient noise is not well-estimated).
- As a result, much effort has been spent on variational methods. Mean Field Variational Bayes for BNNs were in introduced in @cite , the gradient computation of which was later improved in Bayes by Backprop (BBB) . However, the fully factorized Gaussian variational family used in BBB is unable to capture correlation amongst the parameters in the posterior. In contrast, Matrix Gaussian Posteriors (MVG) @cite , Multiplicative Normalizing Flows (MNF) @cite , and Bayes by Hypernet (BBH) @cite are explicitly designed to capture posterior correlation by imposing structured approximation families; works like Black Box @math -Divergence @cite and Probabilistic Backpropagation (PBP) use a richer family of divergence measures, encouraging approximate posteriors to capture important properties of true posterior distributions.
- Finally, Dropout @cite and ensemble methods @cite @cite @cite by-pass the difficulties of performing Bayesian inference and obtain predictive uncertainty estimates through implicitly or explicitly training multiple models on the same data.
- Our work is related to Zipf's law and the distributions of word length and sentence length. Power laws have been observed to appear in numerous natural and man-made systems @cite , we here concern them in language.
- According to the review by , first demonstrated that the word length in a corpus empirically and theoretically follows a variant of Poisson distributions. The word length of a natural corpus has been observed to follow the variants of Poisson distributions in more than 32 languages @cite .
- From an algorithmic point of view, clustering algorithms can be classified into bottom-up algorithms and top-down algorithms @cite . As our algorithm follows a bottom-up strategy, we briefly discuss the relevant algorithms of this class to highlight our contributions.
- Another relevant topic is co-clustering (a.k.a bi-clustering or pattern-based clustering) @cite . Co-clustering can be considered as a more general class of clustering high dimensional data by simultaneously clustering rows (points) and columns (dimensions). The main point that differentiates co-clustering from subspace clustering lies in the approach to the problem, and the homogeneous methodology to find clusters in both axis-parallel and arbitrarily oriented subspaces @cite . In this paper, we also compare the performance of our algorithm on gene expression data with a range of co-clustering algorithms, including SWCC @cite , BBAC-S @cite , ITCC @cite , FFCFW @cite , and HICC @cite .
- There is a growing body of recent works on multi-view learning algorithms, e.g., @cite @cite @cite , that attempt to integrate information across the multiple views to optimize the predictive performance of the classifier (see @cite @cite ). Some multi-view learning methods seek to maximize the agreement between views using regularization @cite @cite whereas others seek to optimally selecting subsets of features from different views for each prediction task @cite @cite However, these methods were not designed for network embedding. Most of the existing multi-view learning algorithms are either not directly applicable to multi-view networks or are not designed to cope with high degrees of data sparsity, a key challenge in modeling real-world multi-view networks.
- Network embedding methods aim to produce information preserving low-dimensional embeddings of nodes in large networks. State-of-the-art network embedding methods include Deepwalk @cite , LINE @cite and node2vec @cite are limited to single view networks, i.e, networks with a single type of links. However, most real-world networks are comprised of multiple types of nodes and links @cite @cite @cite wherein each type of link induces a view. Hence, there is a growing interest in network embedding methods for multi-view networks @cite @cite @cite @cite . Some multi-view network embedding methods use canonical correlation analysis (CCA) @cite @cite @cite to integrate information from multiple views. Others construct multi-view embeddings by integrating embeddings obtained from the individual views. Examples include MVWE @cite which uses a weighted voting mechanism to combine information from multiple views; MVE2vec @cite which attempts to balance the preservation of unique information provided by specific views against information that is shared by multiple views; and DMNE @cite which uses a co-regularized cost function to combine information from different views. MVWE, MVE2vec, and DMNE use deep neural network models at their core. Specifically, MVWE and MVE2vec are based on a skip-gram model and DMNE is based on an AutoEncoder.
- In contrast to the existing multi-view network embedding methods, MVNE exploits a recently discovered connection between network adjacency matrix factorization and network embedding @cite to utilize GFC @cite , a graph factorization method, to perform single view network embedding. MVNE extends the resulting single view network embedding algorithm to the multi-view setting. Inspired by @cite , MVNE uses a novel objective function that maximizes the agreement between views while combining information derived from the local as well as the global structure of the underlying multi-view networks. Like DMNE @cite , MVNE uses a co-regularized objective function to maximize the agreement in the embedding space and to control the embedding dimension. Unlike DMNE which requires on computationally expensive training of a deep neural network, MVNE is considerably more efficient and hence scalable to large networks.
- For subspace-based methods. @cite employed a Principal Component Analysis (PCA) based global appearance model to hallucinate LR faces and a local non-parametric model to enhance the details. @cite used multiple local exemplar patches sampled from aligned HR facial images to hallucinate LR faces. @cite resolved to sparse representation on local face patches. These subspace-based methods require precisely aligned reference HR and LR facial images with the same pose and facial expression.
- . Recently, deep convolutional neural networks (DCNNs) achieve remarkable progresses in a variety of face analysis tasks, such as face recognition @cite @cite @cite , face detection @cite @cite , facial attribute recognition @cite @cite @cite @cite . @cite proposed a bichannel CNN to hallucinate blurry facial images in the wild. For un-aligned faces, @cite proposed to jointly learn face hallucination and facial dense spatial correspondence field estimation. The approach of @cite is a GAN-based method to generate realistic facial images. These works ignore the identity information recovery that is important for recognizability and hallucination quality. @cite and @cite relied on perceptual loss function closer to perceptual similarity to recover visually more convincing HR images for general image SR. In this paper we modified the perceptual loss to facilitate identity hypersphere space and propose a novel training approach to overcome the challenging while using the loss.
- The issue of deadline-constrained traffic scheduling has been investigated by several works including @cite @cite @cite @cite . For example, in @cite , the authors study the problem of dynamic channel allocation in a single user multi-channel system with service costs and deadline-constrained traffic. They propose online algorithms to enable the controller to learn the optimal policy based on Thompson sampling for multi-armed bandit problems. The MDP framework and reinforcement learning approaches for downlink packet scheduling are considered in @cite @cite @cite @cite @cite @cite . In @cite , the authors propose an MDP for deadline-constrained packet scheduling problem and use dynamic programming to find the optimal scheduling policies. The authors do not consider QoS constraints in the scheduling problem.
- Most risk-sensitive approaches consist in analyzing higher order statistics than the average metric such as the variance of the reward @cite @cite @cite @cite . For instance, a risk-sensitive reinforcement learning is studied in @cite in millimeter-wave communications to optimize both the bandwidth and transmit power. The authors consider a utility (data rate) that incorporates both the average and the variance to capture the tail distribution of the rate, useful for the reliability requirement of URLLC traffic. The authors do not exploit frequency diversity.
- In this work, we consider an alternative approach to the risk which consists in minimizing the risk state visitation probability. In fact, due to the stochastic nature of the problem (time-varying channel and random arrival traffic in our context), giving a low reward to an undesirable or a risk-state may be insufficient to minimize the probability of visiting such state @cite . Therefore, in addition to the maximization of the total expected reward, we propose to consider a second criterion which consists in minimizing the probability of visiting risk states where a risk state here is related to the violation of QoS requirements.
- KG representation learning has been widely studied in recent years @cite . One of the most famous translational methods is TransE @cite , which models a triple @math as @math . TransE works well for one-to-one relationships, but fails to model more complex relationships like one-to-many and many-to-many. TransR @cite tries to solve this problem by involving a relation-specific matrix @math to project @math by @math . PTransE @cite leverages path information to learn inferences among relations. For example, if there exist two triples @math , which form a path in KG, and another triple @math holds simultaneously, PTransE models the path information by learning @math , where @math denotes the operator used to merge @math . KG completion is the most prevalent task for KG representation learning, and there also exist some non-translation methods that are particularly tailored for KG completion @cite @cite .
- DeepWalk @cite is one of the most well-known models in the network representation learning area. It uses uniform random walks to sample paths in a network, and applies Skip-Gram @cite to model the generated paths. Skip-Gram learns the embedding of a node by maximizing the probabilities of its neighbors, which captures the information among the nodes. node2vec @cite proposes biased random walks to refine the process of sampling paths from a network. It smoothly controls the node selection strategy to make the random walks explore neighbors in a breadth-first-search as well as a depth-first-search fashion. In this paper, the proposed EA-specific random walk sampling is inspired by node2vec, but concentrates on generating long and cross-KG paths.
- Traditionally, people have attempted to solve audio source separation through matrix-factorization algorithms. Independent Component Analysis (ICA) @cite and Non-negative Matrix Factorization (NMF) @cite are two common techniques used for source separation.
- Wave-U-Net model @cite is an adaptation of the U-Net @cite , a convolutional encoder-decoder network developed for image segmentation. The U-Net approach has been adapted already for singing voice separation in @cite , however this model applies 2D convolutions and works with spectrograms. Instead of doing a 2D convolution, Wave-U-Net performs series of 1D convolutions, downsampling and upsampling with skip connections on a raw waveform signal. The input to this network is a single channel audio mix, and the desired output is the separated @math channels of individual audio sources, where @math is the number of sources present in the audio mix. An interesting aspect of the Wave-U-Net is that it avoids implicit zero paddings in the downsampling layers, and it performs linear interpolation as opposed to de-convolution. This means that our dimension size is not preserved, and our output results will actually become a lot shorter compared to our input. However, by doing this we can better preserve temporal continuity and avoid audio artifacts in the results.
- Traditional NCR algorithms usually suffer from sparsity of the dataset as the similarity calculation can be hard when there is not enough information about users available. Graph based methods try to solve this problem by modeling the data as a graph in order to estimate the distances more accurately when the data is sparse. These methods first construct a graph to represent data and then make recommendations by analyzing the graph. In @cite different types of nodes and a multi-layer structure have been used to make context-aware recommendation through a random walk in the graph. SibRank @cite uses a signed bipartite preference network for representing the data and analyzes it using a signed version of Personalized PageRank to capture users' similarities. Among more recent approaches, GRank @cite is an state of the art method which uses personalized PageRank over a tripartite preference network to directly infer the total ranking of items. GRank may use unreliable paths that are inconsistent with the general idea of similarity in neighborhood collaborative ranking. ReGRank @cite ranks items based on reliable recommendation paths that are in harmony with the semantics behind different approaches in neighborhood collaborative ranking.
- It is necessary to enhance the robustness of machine translation since the ASR system carries misrecognized transcriptions over into the downstream MT system in the SLT scenario. Prior work attempted to induce noise by considering the realistic ASR outputs as the source corpora used for training MT systems @cite @cite . Although the problem of error propagation could be alleviated by the promising end-to-end speech translation models @cite @cite . Unfortunately, there are few training data in the form of speech paired with text translations. In contrast, our approach utilizes the large-scale written parallel corpora. Recently, sperber2017neural adapted the NMT model to noise outputs from ASR, where they introduced artificially corrupted inputs during the training process and only achieved minor improvements on noisy input but harmed the translation quality on clean text. However, our approach not only significantly enhances the robustness of NMT on noisy test sets, but also improves the generalization performance.
- Our approach is motivated by the work of NMT incorporated with linguistic input features @cite . Chinese linguistic features, such as radicals and Pinyin, have been demonstrated effective to Chinese-sourced NMT @cite @cite and Chinese ASR @cite . We also incorporate Pinyin as an additional input feature in the robust NMT model, aiming at improving the robustness of NMT further.
- Regularization in RL has been considered via several different perspectives. One line of investigation focuses on regularizing the features learned on the state space . In particular backward bootstrapping method's can be seen as regularizing in feature space based on temporal proximity @cite @cite @cite . These approaches assume that nearby states in the state space have similar value. Other works focus on regularizing the changes in policy directly. Those approaches are often based on entropy methods . Explicit regularization in the temporal space has received much less attention. Temporal regularization in some sense may be seen as a backward'' multi-step method . The closest work to ours is possibly , where they define natural value approximator by projecting the previous states estimates by adjusting for the reward and @math . Their formulation, while sharing similarity in motivation, leads to different theory and algorithm. Convergence properties and bias induced by this class of methods were also not analyzed in .
- There are many previous works @cite considering the traffic condition as a time series and predicting for different segments separately through time series analysis, like Auto-Regressive Moving Average (ARMA) based algorithms (ARIMA, SARIMA). Additionally, some research @cite @cite uses the methods of statistical learning such as Bayesian Network (BN), SVR and GBDT, and adds extra information to assist the training. @cite compares those methods and shows their similar performances. In these approaches, the strong spatiotemporal couplings, which exist in metropolitan circumstance particularly, lead to the dilemma of choices between the computation complexity and the sufficiency of input information.
- The improved methods that we propose are also closely related to variational drop -out @cite as discussed below. We give a new interpretation to variational dropout and apply it in combination with normalization techniques.
- VQA has improving dramatically recently @cite @cite @cite . We briefly introduce typical VQA methods, and recommend the surveys @cite @cite for a more details. Depending on the attention usage, VQA methods can be roughly divided into three groups: (i) non-attention methods, (ii) visual attention methods and (iii) visual-text co-attention methods. Non-attention methods include multimodal compact bilinear networks @cite , relational networks @cite , and Deeper LSTM Question+Image @cite . They usually produce answers through a general network architecture with attention implicitly embedded in the model. Visual attention methods, on the contrary, utilize image-question pairs to attend to the discriminative image regions to predict the answer. For example, stacked attention networks @cite , ABC-CNN @cite and dynamic memory networks @cite all explicitly compute the visual attention by combining top-down question contexts and bottom-up image cues. Visual-text co-attention methods such as hierarchical co-attention networks @cite , dual attention networks @cite and compositional attention networks @cite all build explicit attention on both images and questions. Tough results are encouraging, these methods don't address zero-shot transfer problem.
- Zero-shot learning was early proposed by @cite and soon become an interesting research problem in the cross-modal domain spanning natural language processing and computer vision @cite , where no finite set of samples can cover the diversity of the real world and all datasets naturally follow a heavy-tail distribution with new classes appearing frequently after the training @cite @cite . Usually, zero-shot learning requires to transfer knowledge from other sources such as attributes @cite , word embedding @cite or the relationship to other categories @cite in order to predict the novel class labels. In our dataset, novel words are embedded inside one module (i.e. questions) and we test the model's zero-shot generalization ability to the other module (i.e. answers).
- There are many VQA datasets, such as @cite @cite for compositionality, zero-shot VQA dataset @cite @cite using extra resources, and more on the surveys @cite @cite . Different from them, our dataset focus on transfer between input and output for natural human learning.
- Sheyner @cite present a formal analysis of attacks on a network along with cost-benefit analysis and security measures to defend against the network attacks. In @cite , Chowdhary provide a polynomial time method for attack graph construction and network reconfiguration using a parallel computing approach, making it possible to leverage information for strategic reason of attacks in large-scale systems.
- In the context of cloud systems, Peng discusses a risk-aware MTD strategy @cite where they model the attack surface as a non-decreasing probability density function and then estimate the risk of migrating a VM to a replacement node using probabilistic inference. Kampanakis @cite highlight obfuscation as a possible MTD strategy in order to deal with attacks like OS fingerprinting and network reconnaissance in the SDN environment. Furthermore, they highlight that the trade-off between such random mutations, which may disrupt any active services, require analysis of cost-benefits.
- Learning transformation-equivariant representations can trace back to the seminal work on training capsule nets @cite @cite @cite . The transformation equivariance is characterized by the various directions of capsules, while the confidence of belonging to a particular class is captured by their lengths.
- Many efforts have been made in literature @cite @cite @cite on extending the conventional translation-equivariant convolutions to cover more transformations. Among them are group equivariant convolutions (G-convolution) @cite that have been developed to equivary to more types of transformations. The idea of group equivariance has also been introduced to the capsule nets @cite by ensuring the equivariance of output pose vectors to a group of transformations with a generic routing mechanism. However, the group equivariant convolution is restricted to discrete transformations, which limits its ability to learn the representations equivariant to generic continuous transformations.
- Auto-Encoders and GANs. Unsupervised auto-encoders have been extensively studied in literature @cite @cite @cite . Existing auto-encoders are trained by reconstructing input data from the outputs of encoders. A large category of auto-encoder variants have been proposed. Among them is the Variational Auto-Encoder (VAE) @cite that maximizes the lower-bound of the data likelihood to train a pair of probabilistic encoder and decoder, while beta-VAE seeks to disentangle representations by introducing an adjustable hyperparameter on the capacity of latent channel to balance between the independence constraint and the reconstruction accuracy @cite . Denoising auto-encoders @cite attempt to reconstruct noise-corrupted data to learn robust representations, while contrastive Auto-Encoders @cite encourage to learn representations invariant to small perturbations on data. Along this direction, @cite propose capsule networks to explore transformation equivariance by minimizing the discrepancy between the reconstructed and target data.
- On the other hand, Generative Adversarial Nets (GANs) have also been used to train unsupervised representations. Unlike the auto-encoders, the GANs @cite and their variants @cite @cite @cite @cite generate data from the noises drawn from a simple distribution, with a discriminator trained adversarially to distinguish between real and fake data. The sampled noises can be viewed as the representation of generated data over a manifold, and one can train an encoder by inverting the generator to find the generating noise. This can be implemented by jointly training a pair of mutually inverse generator and encoder @cite @cite . There also exist better generalizable GANs in producing unseen data based on the Lipschitz assumption on the real data distribution @cite @cite , which can give rise to more powerful representations of data out of training examples @cite @cite @cite . Compared with the Auto-Encoders, GANs do not rely on learning one-to-one reconstruction of data; instead, they aim to generate the entire distribution of data.
- Self-Supervisory Signals. There exist many other unsupervised learning methods using different types of self-supervised signals to train deep networks. Mehdi and Favaro @cite propose to solve Jigsaw puzzles to train a convolutional neural network. @cite train the network by inferring the relative positions between sampled patches from an image as self-supervised information. Instead, @cite count features that satisfy equivalence relations between downsampled and tiled images. @cite propose to train RotNets by predicting a discrete set of image rotations, but they are unable to handle generic continuous transformations and their compositions. @cite create a set of surrogate classes by applying various transformations to individual images. However, the resultant features could over-discriminate visually similar images as they always belong to different surrogate classes. Unsupervised features have also been learned from videos by estimating the self-motion of moving objects between consecutive frames @cite .
- In addition, there exist a large number of semi-supervised models in literature. Here, we particularly mention three state-of-the-art methods that will be compared in experiments. Temporal ensembling @cite and mean teachers @cite both use an ensemble of teachers to supervise the training of a student model. Temporal ensembling uses the exponential moving average of predictions made by past models on unlabeled data as targets to train the student model. Instead, mean teachers update the student model with the exponential moving average of the weights of past models. On the contrary, the Virtual Adversarial Training (VAT) @cite seeks to minimizes the change of predictions on unlabeled examples when their output values are adversarially altered. This could result in a robust model that prefers smooth predictions over unlabeled data.
- The SAT also differs from transformation-based data augmentation in which the transformed samples and their labels are used directly as additional training examples @cite . First, in the semi-supervised learning, unlabeled examples cannot be directly augmented to form training examples due to their missing labels. Moreover, data augmentation needs to preserve the labels on augmented images, and this prevents us from applying the transformations that could severely distort the images (e.g., shearing, rotations with arbitrary angles, and projective transformations) or invalidate the associated labels (e.g., vertically flipping 6" to 9"). In contrast, the SAT avoids using the labels of transformed images to supervisedly train the classifier directly; instead it attempts to encode the visual structures of images equivariant to various transformations without access to their labels. This leads to a label-blind TER regularizer to explore the unlabeled examples for the semi-supervised problem.
- In a working paper, @cite use data from the Colombian nonprofit organization to investigate the killings of social leaders. The authors hypothesise that social leaders were increasingly killed by armed groups excluded from the peace process that wanted to consolidate their power, especially in areas where they took over FARC's illegal activities. In the dataset, @cite also find that the category of social leaders targeted the most since the beginning of the ceasefire are local community council leaders.
- published a study of a stencil kernel on multiple architectures in 2009 @cite . It is based on the same modeling principles but does not provide a unified process and presentation reusable for other kernels.
- Most previous works on accelerating CNNs can be roughly divided into three categories, namely, @cite @cite , @cite @cite , and . -based approaches aim to remove the unnecessary connections of the neural network @cite @cite . Essentially, always results in unstructured models, which makes it hard to deploy the existing efficient BLAS library, while not only reduces the storage usage on devices but also decreases computation cost to accelerate the inference. We could roughly divide the filter pruning methods into two categories by whether the training data is utilized to determine the pruned filters, that is, and filter pruning. Data independent method is more efficient than data dependent method as the ultimating of training data is computation consuming.
- Many recent works @cite @cite @cite @cite @cite @cite @cite focus on pruning fine-grained weight of filters. For example, @cite proposes an iterative method to discard the small weights whose values are below the predefined threshold. @cite formulates pruning as an optimization problem of finding the weights that minimize the loss while satisfying a pruning cost condition.
- Some filter pruning approaches @cite @cite @cite @cite @cite @cite @cite @cite are data dependent, which means the training data is utilized to determine the pruned filters. @cite adopts the statistics information from the next layer to guide the filter selections. @cite aims to obtain a decomposition by minimizing the reconstruction error of training set sample activations. @cite proposes an inherently data driven method which use Principal Component Analysis (PCA) to specify the proportion of the energy that should be preserved. @cite applies subspace clustering to feature maps to eliminate the redundancy in convolutional filters.
- Concurrently with our work, some data independent filter pruning strategies @cite @cite @cite @cite have been explored. @cite utilizes an @math -norm criterion to prune unimportant filters. @cite proposes to select filters with an @math -norm criterion and prune those selected filters in a soft manner. @cite proposes to prune models by enforcing sparsity on the scaling parameter of batch normalization layers. @cite uses spectral clustering on filters to select unimportant ones.
- To the best of our knowledge, only one previous work reconsiders the smaller-norm-less-important criterion @cite . We would like to highlight our advantages compared to this approach as below: (1) @cite pays more attention to enforce sparsity on the scaling parameter in the batch normalization operator, which is not friendly to the structure without batch normalization. On the contrary, our approach is not limited by this constraint. (2) After pruning channels selected, @cite need fine-tuning to reduce the performance degradation. However, our method combines the pruning operation with normal training procedure, thus extra fine-tuning is not necessary. (3) Calculation of the gradient of scaling factor is needed for @cite , thus lots of computation cost are inevitable, whereas our approach could accelerate the neural network without calculating of the gradient of scaling factor.
- Some other works @cite @cite @cite @cite @cite sharing some common ideas with ours, that is, find the filters that have similar function so that the filters could be pruned, the differences exist in @cite (1) We focus on acceleration of inference of neural network, while @cite concentrate on the emergence of duplicate filters over training iterations; (2) We use the geometric median to select filters to prune, while @cite applies cosine similarity to analyze the similarity of filters; (3) We have demonstrated our approach on large-scale datasets with sophisticated ResNet, while @cite only conducted experiments on small-scale CIFAR-10 with AlexNet, @cite only conducted experiments on small-scale CIFAR-10 and CIFAR-100 with AlexNet
- Footstep planning for humanoid robot has been studied extensively @cite @cite @cite @cite @cite @cite @cite . In these works, the planner plans a footstep sequence to avoid obstacles on the ground and remain inside the specified contact regions on a flat or piecewise-flat ground. To increase the likelihood of success, they incorporate an approximation of robot balance and kinematic reachability into the contact transition model, and do not explicitly perform balance check online. There are also works addressing contact planning in unstructured environment using both palm and foot contacts @cite @cite @cite @cite @cite . However, these approaches assumes quasi-static motions, and drops solutions involving dynamic motions.
- Approaches to synthesize dynamically feasible multi-contact motions have also been extensively studied @cite @cite @cite @cite @cite . However, it is not trivial to include planning of contact poses in these approaches because contacts planning in general involves discrete or non-convex constraints for the contact poses. @cite addresses the non-convexity by decomposing the environment into a set of convex regions and approximating the rotation using piecewise affine functions. The problem is then formulated as a mixed integer convex program and solved to global optimality. Although @cite only uses foot contact, and does not consider dynamics, it points a direction to include contact planning in an optimization problem.
- Extensions of @cite for dynamic planning of a contact sequences are proposed in @cite @cite , which extend @cite with the selection of contact timings or hand contacts respectively. More recent works @cite @cite use the same concept to plan gait sequences for quadruped robots and produce dynamically robust motions. However, mixed-integer approaches scale poorly against the number of integer decision variables. For instance, their applicability is limited to online contact generation in environments with few convex terrain regions, and short planning horizons.
- @cite proposes a kinodynamic sampling-based contact planner to plan kinodynamically feasible contact sequences. They use a simplified robot model to dynamically plan smooth center of mass (CoM) trajectories based on convex optimization and then search for kinematically feasible contact poses around it. It shows a unified planning framework to consider dynamics and kinematics constraints, but it suffers from long planning time. @cite proposes an efficient dynamic feasibility check by conservatively reformulating the problem as a linear program. While the check guarantees to reject dynamically infeasible motions, they do not address dynamical robustness in the stability check. @cite learns quadratic dynamics objective of humanoid walking motion, and apply this learned model to select steps in a search-based footstep planner. However, their dynamics model assumes flat contact, and does not consider palm contacts, which limits the applicability of the approach.
- The developments of these noise injection techniques specific to the architectures are not unique to convolutional networks. In fact, similar to convolutional networks, recurrent networks require their own noise injection methods. Currently, Variational Dropout @cite and ZoneOut @cite are two of the most commonly used methods to inject noise to recurrent connections.
- Our method is inspired by Cutout @cite , a data augmentation method where parts of the input examples are zeroed out. DropBlock generalizes Cutout by applying Cutout at every feature map in a convolutional networks. In our experiments, having a fixed zero-out ratio for DropBlock during training is not as robust as having an increasing schedule for the ratio during training. In other words, it's better to set the DropBlock ratio to be small initially during training, and linearly increase it over time during training. This scheduling scheme is related to ScheduledDropPath @cite .
- However, compared to the CNN, the optical flow calculation is computationally expensive. It is thus the major speed bottleneck of the current two-stream approaches. There have been recent attempts to better model the temporal information. @cite pre-trained a deep 3D CNN network on a large-scale dataset, and use it as a general spatiotemporal feature extractor. The features generalize well to several tasks but are inferior to two-stream approaches. @cite reduced the dimension of each frame clip using a CNN and aggregated frame-level information using Long Short Term Memory (LSTM) networks. @cite proposed to reduce the size of each frame and use longer clips (e.g., 60 vs 16 frames) as inputs. They managed to gain significant accuracy improvements compared to shorter clips with the same spatial size. @cite experimented with sparse sampling and jointly trained on the sparsely sampled frames clips. In this way, they incorporate more temporal information while preserving the spatial resolution. Recent approaches @cite @cite have evolved to end-to-end learning and are currently the best at incorporating global temporal information. However, none of them handle multirate video analysis effectively.
- To handle multirate videos, there are two widely adopted approaches. One is to train multiple models, each of them corresponding to a different fixed frame-rate. This is similar to using image pyramids to handle the multi-scale problem in image analysis. The other is to generate sliding windows of different lengths for each video (a.k.a, temporal jittering), with the hope of capturing temporal invariance. However, neither of these approaches is exhaustive, and they are both computationally intensive. @cite is the most similar work to ours since they deal with motion speed variance. However, our work differs in several aspects. First, we aim to explicitly learn the transitions between frames while @cite uses past and future neighboring video clips as the temporal context, and reconstruct the two temporal transitions. Their objective is considerably harder to optimize, which may lead to sub-optimal solutions. Second, our random skipping strategy is easy to implement without computational overhead whereas the image reconstruction of @cite will lead to significant computational burden. Third, their proposed multirate gated recurrent unit only works in RNNs, while our strategy is generally applicable.
- The author of @cite describes a problem setting not unlike the one presented in this paper, i.e., the combination of maintaining diversity and searching in a changing environment. The issue of premature convergence is tackled by integrating a certain amount of random search into the genetic algorithm by performing hyper-mutation. This has since become standard procedure and is included in all genetic algorithms presented in this paper, which aims to further improve the resilience of the search process.
- The preparation for unexpected or previously wrongly modeled change is an important issue for the practical application of machine learning in industry @cite . From an engineer's point of view, the diversity of the population of plans can be regarded as a typical with the cost of the plan representing the functional requirement. Applying NFR engineering processes to self-adaptive systems is still a new idea and a clear canon of relevant NFRs for these new challenges has not yet been found @cite @cite .
- A trustworthy and privacy-preserving cloud may be addressed by the use of cryptographic techniques such as fully homomorphic encryption (FHE) @cite . However, it is still inefficient for most computations @cite . Similarly in verifiable computing @cite , it was designed to enable result correctness verification but has not shown support for general purpose cloud computing yet.
- is a slime mold that apparently is able to solve shortest path problems. Nakagaki, Yamada, and T ' o th @cite report about the following experiment; see Figure . They built a maze, covered it by pieces of Physarum (the slime can be cut into pieces which will reunite if brought into vicinity), and then fed the slime with oatmeal at two locations. After a few hours the slime retracted to a path following the shortest path in the maze connecting the food sources. The authors report that they repeated the experiment with different mazes; in all experiments, Physarum retracted to the shortest path.
- The paper @cite proposes a mathematical model for the behavior of the slime and argues extensively that the model is adequate. Physarum is modeled as an electrical network with time varying resistors. We have a simple graph @math with two distinguished nodes modeling the food sources. Each edge @math has a positive length @math and a positive capacity @math ; @math is fixed, but @math is a function of time. The resistance @math of @math is @math . In the electrical network defined by these resistances, a current of value 1 is forced from one of the distinguished nodes to the other. For an (arbitrarily oriented) edge @math , let @math be the resulting current over @math . Then, the capacity of @math evolves according to the differential equation where @math is the derivative of @math with respect to time.
- Nakagaki et.-al. @cite pointed out that different edge may react with different speed to the differences between flow and capacity. For example, Physarum prefers darkness over bright light and hence edges in a bright environment react differently than edges in darkness. This let to the non-uniform dynamics where @math is an indicator for the reactivity of an edge.
- Automated web service composition aims to loosely couple web services to fulfill a service request, without strictly obeying a pre-given abstract workflow. Instead, composition workflows are gradually built up while its component services are selected. Existing works in fully automated web service composition can be categorized into two approaches --- direct approaches and indirect approaches @cite . The direct approaches represent composition solutions explicitly in the representation that displays actual execution flows of composite services, while the indirect approaches often represent composite services implicitly as permutations, which require a decoding process to build up actual execution workflows.
- In the second category, service composition solutions are represented as permutations, which are then decoded into solutions represented as DAGs @cite @cite @cite . PSO is utilized to find an optimized queue of services (i.e., a permutation), which can be decoded into a corresponding DAG-based composite service @cite . @cite extends @cite to jointly optimize QoSM and QoS, where a weighted DAG is decoded, where edge weights correspond to matchmaking quality between services. These two PSO-based approaches rely on PSO to determine the weights of particle's position (that corresponding with a service) to form an ordered service queue. Optimizing QoSM and QoS simultaneously is more challenging than optimizing QoS only because the searching space has significantly increased, and it demands more effective and efficient searching techniques. Apart from that, it has been suggested that utilizing the indirect representation often contributes to a higher performance, compared to direct representation @cite . It is due to that the search space is not unwittingly restricted by unconstrained random initialization of solutions and operators.
- Memetic algorithms have drawn growing attention from researchers in recent years and achieved significant successes in many applications @cite . By introducing local search, the performance of EC techniques can be improved. In the domain of service composition, to overcome the prematurity and proneness of GP, Tabu search is combined with GP to solve QoS-aware data-intensive web service composition @cite . @cite proposed an indirect memetic approach for QoS-aware web service composition, where a domain-dependent crossover operator is proposed to produce candidate solutions. Besides that, an exhaustive local search is applied to composite solutions represented as permutations. However, the produced neighbors are likely to be decoded into the same composite solution. Therefore, the effectiveness of this local search operator demands further improvement.
- Recently, EDA has been used as a technique to tackle permutation-based optimization problems @cite . In particular, a distribution model is learned iteratively for each population. Subsequently, new offsprings are generated based on the learned model. Moreover, domain-dependent local search operators are often introduced to enhance the performances of EDA. For example, a probability matrix that is related to the job priority permutation of a solution is learned in EDA-based flow-shop scheduling problem, and different job-based local search operators were proposed to enhance the exploitation ability of EDA @cite . An Edge Histogram Matrix is applied to uncertain capacitated arc routing problems and is leaned from solutions represented by a set of routes @cite . To make local improvements, different move operators, such as single insertion and swap, are also proposed.
- The use of EDA has only been investigated for semi-automated web service composition @cite @cite @cite . However, we recently proposed an EDA-based approach for fully automated web service composition, where candidate solutions are represented as permutations over a given service repository. The success of the proposed method strongly depends on the distribution model and the way of learning the distribution model. We employ Node Histogram Matrix (NHM) to learn the distribution of promising solutions in one population, Node Histogram-Based Sampling Algorithm (NHBSA) @cite is empoloyed to produce candidate solutions. Although we started an initial study for fully automated service composition, it remains an opportunity to improve its performance further. EDA is good at global exploration, and local search operators are motivated to be introduced in EDA to enhance its capability in exploitation.
- From a text retrieval perspective, question answering embodies the task of finding the relevant piece of text containing the answer and subsequently extracting the answer @cite . This view led to open-domain QA, which encompasses the majority of today's QA systems. In recent years, QA began incorporating machine learning, with the IBM Watson being one of the most famous systems @cite . The primary approach behind Watson is extensive data, statistical and machine learning analysis. Several other neural network approaches have also been explored. used neural networks to answer quiz bowl type questions, where given a description the task is to identify the subject being discussed @cite . extended simple RNN models with an attention mechanism to enable transitive reasoning and made steps towards reasoning-based QA @cite . Malinowski proposed a model using both CNN and LSTM to incorporate image recognition and QA @cite .
- Answer sentence selection is a QA task which involves selecting the sentence that is the most likely to contain the answer. Early approaches were predominantly syntactical, using the idea that the question and answer sentence should relate to each other loosely through syntactical transformations. proposed a generative model that transforms the answers to the questions @cite . Wang and Manning introduced a probabilistic model that models tree-edit operations on dependency parse trees, making use of sophisticated linguistics features @cite . Other similar models include using dynamic programming to find the optimal tree edit sequences @cite . The main drawback of these approaches is that they require too much feature engineering and were difficult to adapt to new domains. Only recently, researchers started applying neural network models. used CNN models for answer sentence selection on the TREC benchmark @cite . also proposed several CNN models for answer sentence selection task. Wang and Nyberg constructed a joint-vector based on both the question and the answer using an LSTM model @cite .
- Many of the proposals for time series clustering are based on the combination of a distance measure and a clustering algorithm. First, we will analyse the most important distance measures proposed for time series comparison, and then we will introduce the clustering methods that can be applied based on them Further information about time series clustering can be found in @cite or @cite .
- So far, GAN has been widely used in the field of computer vision and image processing, and has achieved many amazing results. @cite @cite @cite @cite . Pix2pixHD @cite is the state-of-the-art for image-to-image translation.
- Visual Question answering has developed dramatically in the last few years @cite @cite @cite . Practically all current works are based on casting the problem into a multi class classification problem, where image features, retrieved by a Convolutional Neural Network, are fused with question features (mostly extracted by Recurrent Neural Network) and used to predict one of the common training set answers, mostly short and succinct answers. These methods have the advantage of not requiring to incorporate a complicated parsing and understanding process and may present decent results when trained and tested on current existing datasets, yet they lack some important human characteristics like using a compositional process, utilizing existing and meaningful sub processes. Using meaningful sub processes allows humans to focus on different aspects and scopes according to the specific task, utilize existing abilities and modularly integrate novel ones, understand limitation and provide elaborations including suggestions of alternatives.
- Incorporating the question information is largely addressed by seeking mechanisms for image-language features fusion. A large focus in this line of works was in simplifying bilinear pulling (which is based on the outer product of the two feature vectors) by reducing dimensionality of the features @cite or a low rank factorization @cite @cite .
- In order to extract image information that is more informative to the question and avoid the noise of irrelevant image areas, many works incorporated attention mechanisms. During the attention stage image areas, that are considered more relevant, are multiplied by higher weights and contribute more to answering the question. Attention may be stacked for multiple stages @cite with the motivation of refining it for complicated questions. Extracting relevant areas was also performed by integrating regions of detected objects related to question words @cite . The attention concept was also extended to include both image features and the question representation @cite , where both attention types effect each other. Additional attention mechanisms utilize CRF @cite , consider all word-region interactions @cite , incorporate correlations between image, question and candidate answer @cite and combine grid based and object detection based regions @cite @cite .
- Combining results of meaningful tasks (other than using pre-trained networks as visual features) such as object detection was in the focus of several additional works. One such work uses object and attribute recognition tasks for proposed regions and combines them with corresponding representations from question and candidate answer @cite . The use of visual concepts (object class and attributes) of attended regions and comparing them to extracted concepts from the question was proposed as well @cite . In another work concatenating pairs of vectors representing two detected objects and their properties with the encoded question was used to allow relation reasoning @cite . Objects and relations between them was utilized in a work that used graph representation for both the image (synthetic images) and the question @cite . For the image graph objects were the nodes and edges were the spatial relations between them and for the question graph words were the nodes and their dependencies were the edges. Representations were merged in an attention-like mechanism to fuse the features and predict the answer.
- Stochastic multi-armed bandits is a prevalent framework for sequential decision-making. Early work on stochastic MAB problems @cite @cite @cite tended to be more focused on asymptotic guarantees, whereas more recent work @cite @cite has been directed towards a non-asymptotic analysis in which regret can be bounded over a fixed time horizons @math . Two of the best-known and well-studied techniques are known as the UCB algorithm that follows the OFU principle @cite and the explore then exploit algorithm @cite @cite . Recently, the Bayesian setting accompanied by the (TS) technique has also been thoroughly analyzed due to the ease of implementation and favorable empirical results @cite .
- A special case of linear bandits is combinatorial bandits where the action set is constrained to subset of @math In combinatorial stochastic bandits, it is often assumed that the reward loss vector is observed at all the coordinates sampled by the action taken. This is the so-called semi-bandit feedback setting @cite . The authors of @cite initiated the study of combinatorial stochastic bandits under semi-bandit feedback and a network-structured action set; while @cite studied the general action set case. The authors of @cite further characterized tight upper and lower bounds for this problem. Assuming the noise is independent across different coordinates, the authors of @cite improved upon the results obtained in @cite . For the bandit feedback case, the authors of @cite gives algorithms that require brute-force search over the action space with instance dependent regret @math . For adversarial combinatorial bandits, the authors of @cite presented the efficient and optimal algorithm for the semi-bandit feedback case while the authors of @cite described an optimal algorithm for the bandit feedback case, but its computational complexity scales linearly with the number of actions.
- Besides, most of the previous methods require the presence of specific resources for training, e.g. translation of the parallel data generated by existing MT system(s) @cite @cite . propose approaches to use an SMT model to provide word and phrase recommendations for an attention-based NMT, where the two systems are deeply coupled. propose to train context-aware translation models by the aids of large document discourse-level data. In contrast, our training procedure is more general and simpler, which only uses word sampling from the original parallel data and requires no external resources. To leverage outside information, such as words, for a generation task, propose to use lexical constraints on decoding process to utilize correct external word translations. propose to use copying mechanism in the single-turn dialogue task, inspiring us for the basic framework. Compared to their attempts, our approach provides more robust solutions to discriminate noises.
- Another related topic is semi-supervised learning for the text classification. A simple but efficient method is to use pre-trained modules, e.g., initializing the word embedding or bottom layers with pre-trained parameters. Although word embedding technique has been wildly used in NLP models, e.g., Glove @cite and ELMo @cite , other pretraining-based method is model-dependent. The ELMo @cite replaces the embedding layer with the pre-trained BILSTM to capture the contextual representation. This method is complementary with the proposed method. The combination with our method may yield better performance than either of them alone, but that investigation is beyond the scope of this paper. However, other methods, e.g., the Transformer LM @cite , proposed a unified semi-supervised framework to handle various tasks. This constraint prevents advanced supervised models from the semi-supervised learning.
- Bag-of-tasks on clouds are widely used not only for scientific applications but also for many commercial applications. In @cite , Facebook reports that the jobs running on their own internal data centers are mostly independent tasks. Many works propose then scheduling the execution of independent tasks both on homogeneous and heterogeneous cloud environments @cite . In the former, the performance and pricing of all available VMs are the same. In this case, authors usually consider either reserved VMs @cite or on-demand VMs @cite . For instance, @cite study scheduling of applications on on-demand VMs distributed across different datacenters, focusing on the trade-offs between performance and cost while @cite provides a solution that satisfies job deadlines while minimizing monetary cost. The proposed heuristics use both on-demand and reserved VMs. Works on heterogeneous cloud consider different types of VMs. For instance, in @cite the authors present a heuristic algorithm for executing a bag-of-tasks applications taking into account either budget or deadline constraints. In @cite , present an extensive survey and taxonomy of existing research in scheduling of bag-of-task applications on clouds.
- Some works take into account Amazon spot VMs instance features. In @cite , use hybrid instances, including both on-demand instances for high priority tasks and backup, and spot instances for normal computational tasks. Authors of @cite propose to switch to on-demand resources when there is no spot instance available to ensure the desired performance. Using both on-demand and spot VM instances, SpotCheck @cite provides the illusion of an IaaS platform that offers always-available on-demand VMs for a cost near that of spot VMs. Also claiming performance of on-demand VMs, but at a cost near that of the spot market, the authors in @cite present the SpotOn batch service computing, that uses fault-tolerance mechanism to mitigate the impact of spot revocations. To our knowledge no work studies the impact of the new hibernation feature of spot instances on scheduling algorithms.
- Edge-based methods rely on classification of image gradients @cite @cite @cite . The problem is that during the integration of gradients, a single misclassified edge will result in errors in a wide area of recovered reflectance @cite . Our shading orders can capture much more information than the gradients, since the objects of the measurements are no longer limited to adjacent pixels. The non-local shading orders can reduce the adverse influence of misclassified edges. Further, the long range relations define the large-scale structure directly, which can avoid the accumulation of error when integrating local measurements. Some Markov random field models @cite @cite @cite and dense Conditional random field models @cite also consider the relation between distant pixels. However, their non-local smooth terms are only applicable to pixels with the same reflectance or shading. This is of particular importance for pixels whose reflectance and shading are both different. Imposing shading smoothness on these pixels ( @cite ) may cause large errors.
- Different constraints often result in quite different shading orders. How to fuse them remains an open problem. Edge-based methods classify each edge into a reflectance edge or a shading edge. Accordingly, the shading order between the two sides of the edge can be decided. In particular, Retinex classified the edges by the magnitude of gradients @cite . This classification method is risky. Some shadow edges are quite strong, while the reflectance edges between similar colors are relatively weak. Extensions of Retinex introduced several new features, including texture similarity @cite , classifiers over local features @cite or patches @cite @cite , correlation between the mean luminance and luminance amplitude @cite , and image sequences under different illumination directions @cite @cite . These features improved the accuracy of classification, but none of them are robust enough to handle all kinds of scenes. CSF faces a similar problem of selecting the optimal pairwise order from several estimates. The difference is that CSF incorporates consistency between the pairwise orders and global order into the selection criteria, which can rectify the inconsistent selections made by noisy image features.
- Ranking elements from their pairwise comparisons has been extensively studied in many fields @cite @cite @cite . Angular Embedding @cite adopts a cosine error function, which is proven to be more robust to outliers than the traditional @math or @math errors used by Least Squares Embedding @cite . Angular Synchronization (AS) also uses the angular space @cite , but it does not consider the confidences of pairwise measures.
- Many recent methods address more intrinsic components other than shading and reflectance, including specular reflection @cite , shape and illumination @cite , coarse-scale and detailed shading @cite , direct and indirect irradiance @cite @cite , illuminant color and sensor characteristics @cite , and texture @cite . These detailed decompositions give a more comprehensive analysis of the scene, but they also make the problem much more complex. Recently, new constraints have been formed based on the geometric information of RGB-Depth images @cite @cite @cite @cite . We use the depth map to render shadow maps that can determine the positions of shading edges. More recently, the intrinsic video techniques have extended the research to videos @cite @cite @cite .
- Bottleneck distance is closely related to the bipartite matching problem, which can be solved by the classic maximum flow technique of Hopcroft and Karp @cite . The current best exact algorithm for planar bipartite matching is due to Efrat @cite and runs in @math time for point sets of size @math .
- Earlier seminal work by Hefferman and Schira @cite considered approximation algorithms for the more general problem in which one of the point sets is mapped by an isometry (translated, rotated, and possibly reflected) prior to being matched. In the case of just computing the bottleneck distance, their methods provide a @math time algorithm to test if @math , where the answer must be correct if @math . A key idea in @cite is to check for bottleneck matchings using a maximum flow computation in graph that arises from snap-rounding'' the point sets to their nearest point in grid. Our approach uses a similar idea in which the maximum flow instance is a planar graph (not true for @cite ), so a recent improved algorithm for multi-source, multi-sink maximum flow due to Borradaile @cite that runs in @math time can be leveraged.
- Bottleneck distance arises naturally in the comparison of persistence diagrams in topological data analysis @cite . @cite consider the related problem of building a database of persistence diagrams that permits approximate querying in @math time ( @math is the number of point sets stored in @math ). Their approach is also based on representing point sets by snap-rounding each point to neighboring grid points at each level in a multilevel grid. All combinations of snap-roundings are considered and the resulting grid point distributions are stored in a database. Binary search is used on the hashed values to query the database and resulting matches are shown to provide a @math -approximation to the nearest point set in the database. They also observe that the approximation ratio can be improved to @math , if more snap-roundings are done ( @math per point). Rather than using a hashing scheme, it is possible to use a trie data structure (described in ) to achieve @math time queries.
- Approximation results are known for general bipartite matching in metric spaces; in @cite the authors show that for any @math , there is an algorithm that computes a @math -approximate matching, where @math in @math time. A variation on minimum-distance bottleneck matching, with the additional constraint that the matched edges cannot cross, was recently shown to be NP-hard to approximate within a factor of less than @math @cite .
- Liu @cite proposed a universal network for counting people in a crowd with varying density and scale. in this study the proposed network is composed of two components: a detection network (DNet) and an encoder-decoder estimation network (ENet). The input first run through DNet to detect and count individuals who can be segmented clearly. Then, ENet is utilized to estimate the density maps of the remaining areas, where the numbers of individuals cannot be detected. Modified version of Xception used as an encoder for feature extraction and a combination of dilated convolution and transposed convolution used as decoder. Authors attempted to address the variations in crowd density with two literally isolated deep networks which significantly slows down the process lacks novelty.
- In another study, Mehta @cite proposed independent decoding reinforcement branch as a binary classifier which helps the network converge much earlier and also enables the network to estimate density maps with high Structural Similarity Index (SSIM). A joint loss strategy, the binary cross entropy (BCE) loss and mean squared error (MSE) loss used to train the network in an end to end fashion. They have used variation of the U-net model to generate the density maps. The proposed model shows notable improvements in recreation of the crowd density maps over the existing models.
- A study by Oh @cite attempt to address the uncertainty estimation in the domain of crowd counting. This study proposed a scalable neural network framework with quantification of decomposed uncertainty using a bootstrap ensemble. The proposed method incorporates both epistemic uncertainty and aleatoric uncertainty in a neural network for crowd counting. The proposed uncertainty quantification method provides additional auxiliary insight to the crowd counting model. The proposed technique attempt to address the uncertainty issue in crowd counting. However the use of unsupervised calibration method to re-calibrate the predictions of the pre-trained network is questionable.
- In another study Olmschenk @cite attempt to address the inefficiency of the existing crowd density map labeling scheme for training deep neural networks. This study proposes a labeling scheme based on inverse k-nearest neighbor ( @math ) maps which does not explicitly represents the crowd density. Authors claim a single @math map provides information similar to the commonly practiced accumulation of many density maps with different Gaussian spreads.
- A study by Idrees @cite stems from the observation that crowd counting, density map estimation and localization are very interrelated and can be decomposed with respect to each other through composition loss, which can then be used to train a neural network. This study
- Recently, image generative modeling has gained a lot of attention from both scientific communities and fashion industry. Generative Adversarial Networks (GANs) @cite are the most popular generative models for the tasks of image synthesis and image modification. There have been some works @cite @cite that exploit GAN in conditional setting. In @cite @cite , generative models are developed conditioning upon class labels. Text @cite @cite and images @cite @cite @cite @cite @cite have also been used as conditions to build image generative models.
- Image Synthesis in Fashion. These techniques have also been applied @cite @cite @cite to exploit image generative models in fashion technology. An image based virtual try-on network has been proposed in @cite where the generative model transfers a desired clothing item onto the corresponding region of a person using a coarse-to-fine strategy. A novel approach is presented in @cite for generating new clothing on a wearer through generative adversarial learning by utilizing textual information. In @cite , a conditional U-Net has been used to generate image guided by shape information, and conditioned on the output of a variational autoencoder for appearance. In @cite , the authors present a generative model conditioned upon pose to manipulate a person in an image to an arbitrary pose. @cite study similar task with us, instead they transfer poses to target person from video in a frame-by-frame manner.
- Even though we aim at solving similar problem as @cite , our work differs from @cite in terms of architectural choices both in generator and discriminator. Unlike most of the image generative approaches, we exploit multiple images of a fashion item as input which are usually available on e-commerce shopping platforms.
- Techniques for imitation learning differ in the way they approach the problem. Two popular approaches to imitation learning have been behavioral cloning @cite and inverse reinforcement learning (IRL) @cite @cite . Behavioral cloning views the imitation learning problem as a supervised learning problem that attempts to learn a direct mapping from states to actions. On the other hand, inverse reinforcement learning works to find a cost function under which the expert demonstrator is optimal. One approach of this type is guided cost learning @cite which builds on maximum entropy IRL @cite and guided policy search algorithm @cite and achieves impressive results on physical robots. Later, in ho2016generative , ho2016generative used generative adversarial networks to imitate policies when both states and actions are available using a technique called generative adversarial imitation learning (GAIL) @cite . One imitator network attempts to imitate the policy while another attempts to discriminate between the imitation and provided demonstration data @cite . Several follow-up works have improved upon this approach on different aspects @cite @cite and recently, there has been efforts to address sample efficiency of this algorithm by proposing approaches for unbiasing rewards and deriving an off-policy formulation of adversarial imitation learning algorithms @cite .
- On the other hand, in reinforcement learning policy learning through environment-provided reward functions only direct policy search in a large state-action space requires numerous samples and often can fall into poor local optima. Guided policy search (GPS) is a method to improve the sample efficiency of direct policy search and guide learning in a large space away from poor local optima @cite . The basis of GPS is to use trajectory optimization to focus policy learning on high-reward actions.
- In guided policy search under unknown dynamics, time-varying linear Gaussian models of the dynamics for a small set of specific tasks are first trained to fit a small set of sample data through LQR @cite . These Gaussian controllers are then sampled to generate samples to optimize a general policy for a model with thousands of parameters that would typically require much more training data. Specifically, samples in regions of trajectories that have been found to lead to higher reward are generated, guiding the policy learning.
- Different NLI models apply dropout at different layers in general NLI architecture. NLI models proposed by @cite and @cite apply dropout to each feed-forward layer in the network whereas others have applied dropout only to the final classifier layer @cite . @cite apply dropout only to the input and output of sentence encoding layers.The models proposed by @cite and @cite applied dropout to the output of embedding layer and to the input and output of classifier layer. @cite and @cite use dropout but they do not elaborate on the location.
- Dropout rates are also crucial for the NLI models @cite . Even the models which apply dropout at the same locations vary dropout rates.
- Previous research on dropout for RNNs on the applications such as neural language models @cite , handwriting recognition @cite and machine translation @cite have established that recurrent connection dropout should not be applied to RNNs as it affects the long term dependencies in sequential data.
- @cite studied dropout at different places with respect to the LSTM units in the network proposed by @cite for handwriting recognition. The results show that significant performance difference is observed when dropout is applied to distinct places. They concluded that applying dropout only after recurrent layers (as applied by @cite ) or between every feed-forward layer (as done by @cite ) does not always yield good results. @cite , investigated the effect of applying dropout in LSTMs. They randomly switch off the outputs of various gates of LSTM, achieving an optimal word error rate when dropout is applied to output, forget and input gates of the LSTM.
- Evaluations in previous research were conducted on datasets with fewer samples. We evaluate the RNN model on a large, SNLI dataset (570,000 data sample) as well as on a smaller SciTail dataset (27,000 data samples). Furthermore, previous studies concentrate only on the location of dropout in the network with fixed dropout rate.We further investigate the effect of varying dropout rates. We focus on the application of widely used conventional dropout @cite to non-recurrent connection in RNNs.
- Applying machine learning techniques has proven very effective in optical flow estimation problem @cite @cite @cite which is closely related to finding pixel correspondence task. Recently proposed methods, PWC-Net @cite and FlowNet2 @cite , utilize a correlation layer to predict image similarities in some neighborhood around the center pixel in a coarse-to-fine manner. While such a spatially constrained correlation layer leads to state-of-the-art results in optical flow, it performs poorly for very strong geometric transformations that we consider in this work. Rocco al @cite proposed a CNN-based approach for determining correspondences between two images and applying it to instance-level and category-level tasks. In contrast to optical flow methods @cite @cite , it comprises a matching layer calculating the correlation between target and reference feature maps without any spatial constraint. The method casts finding pixel correspondences task as a regression problem and consisting of two independent Siamese CNNs trained separately and directly predicting affine and TPS geometric transformations parametrizing 6-element and 18-element vectors. On the contrary, we propose a more general approach handling more diverse transformations and operating in an end-to-end fashion.
- Most state-of-the-art approaches for inferring road maps from aerial imagery apply convolutional neural networks (CNNs) to segment the imagery for road'' and non-road'' pixels, and then post-process the segmentation output to extract a road network graph. develop a cascaded CNN architecture with two jointly trained components, where the first component detects pixels on the road, and the second focuses on pixels close to the road centerline @cite . They then threshold and thin the centerline segmentation output to extract a graph.
- propose improving the segmentation output by using a conditional generative adversarial network @cite . They train the segmentation CNN not only to output the ground truth labels (with a mean-squared-error loss), but also to fool a discriminator CNN that is trained to distinguish between the ground truth labels and the segmentation CNN outputs.
- DeepRoadMapper adds an additional post-processing step to infer missing connections in the initial extracted road network @cite . Candidate missing connections are generated by performing a shortest path search on a graph defined by the segmentation probabilities. Then, a separate CNN is trained to identify correct missing connections.
- Rather than segmenting the imagery, RoadTracer @cite and IDL @cite employ an iterative graph construction (IGC) approach that extracts roads via a series of steps in a search process. On each step, a CNN is queried to determine what direction to move in the search, and a road segment is added to a partial road network graph in that direction. Although IGC methods improve accuracy, they are an order of magnitude slower in execution time than segmentation approaches and thus not suitable in interactive settings.
- The growth in both the number of IoT devices and IoT applications in a wide array of domains has brought about new requirements to the IoT ecosystem which include location awareness, geo-distribution of processing nodes and low latency in device-cloud communication. This has led to a burden on the traditional resources for IoT including networking, storage and processing resources. Consequently, considerable amount of literature has been published on the use of Single Board Computers (SBCs) like Raspberry Pi as an intermediate processing layer @cite @cite and an enabler for various IoT applications @cite @cite . Moreover, the suitability of networking resource virtualization for IoT including Software Defined Networks (SDNs) and Network Virtualization among others @cite @cite have also been discussed in existing work. Our study on the existing literature focuses on virtualization of storage and processing resources at the OS-level in the form of containerization. We study two aspects of containerization in the IoT context, first, the existing studies on the suitability and performance of containerization on resource-constrained devices and second, the application of containerization to different use-cases of IoT.
- Previous studies have focused on the tradeoffs in applying hypervisor based virtualization and lightweight containerization to edge devices. The authors of @cite illustrate the advantages of containerization over hypervisor based hardware virtualization in terms of size of the resources, flexibility and portability. The authors state that hypervisor based virtualization is more suited for Infrastructure-as-a-Service on the cloud than containerization, which offers a portable runtime, easier deployability on multiple servers and interconnectivity among containers. These advantages, on the other hand, make containerization more suitable for the edge layer in a Platform-as-a-Service scenario @cite . Pahl et. al @cite leverages the resources of Raspberry Pi devices to further build a cluster of containers running on multiple devices in the PaaS context. The cluster is designed to perform computationally intensive tasks including cluster and data management, overcoming the resource-constrained nature of each device.
- Several articles in existing literature present applications of IoT based on containerization in different use cases. The authors of @cite demonstrate a distributed and layered architecture for Industrial IoT based applications with deployment of docker containers on end devices, the gateway and the cloud. Chesov et. al @cite present a multi-tier approach to containerize different functionalities in the smart cities context like data aggregation, business analytics and user interaction with data and deploy the containers on the cloud. Kovatsch et. al simplify programmability of IoT applications by exposing scripts and configurations using a RESTful CoAP interface from a deployed container @cite .
- Early approaches typically involve extracting multiple local descriptors (usually hand-crafted), and combining them into a fixed length image-level representation for fast matching. Local descriptors may be scale-invariant and centered on image feature points, such as for SIFT @cite , or extracted on regular, dense grids, possibly at multiple scales independently of the image content @cite . An impressive number of local descriptors have been developed over years, each claiming superiority, making it difficult to select the best one for the job - an attempt at comparative study can be found in @cite . It should be noted that descriptor dimension (for hand-crafted features) is typically between 32 and 192, which is an order or two less than the number of deep features available for each image region.
- Virtually all aggregation schemes rely on clustering in feature space, with varying degree of sophistication: Bag-of-Words (BOW) @cite , Vector of Locally Aggregated Descriptors (VLAD) @cite , Fisher Vector (FV) @cite , and Robust Visual Descriptor (RVD) @cite . BOW is effectively a fixed length histogram with descriptors assigned to the closest visual word; VLAD additionally encodes the positions of local descriptors within each voronoi region by computing their residuals; the Fisher Vector (FV) aggregates local descriptors using the Fisher Kernel framework (second order statistics), and RVD combines rank-based multi-assignment with robust accumulation to reduce the impact of outliers.
- All of the aforementioned approaches use fixed pre-trained CNNs. However, these CNNs were trained for the purpose of image classification (e.g. 1000 classes of ImageNet), in a fashion blind to the aggregation method, and hence likely to perform sub-optimally in the task of image retrieval. To tackle this, @cite , proposed to fine-tune MAC representation using the Flickr Landmarks dataset @cite . More precisely, the MAC layer is added to the last convolutional layer of VGG or ResNet. The resultant network is then trained with a siamese architecture @cite , minimizing the contrastive loss. In @cite , the MAC layer is replaced by trainable Generalized-Mean (GEM) pooling layer which significantly boosts retrieval accuracy. In @cite , trained a siamese architecture with ranking loss to enhance the RMAC representation. The recent NetVLAD @cite consists of a standard CNN followed by a Vector of Locally Aggregated Descriptors (VLAD) layer that aggregates the last convolutional features into a fixed dimensional signature and its parameters are trainable via back-propagation. @cite proposed SIAM-FV: an end-to-end architecture which aggregates deep descriptors using Fisher Vector Pooling.
- The use of deep learning models has been motivated by the increased availability of datasets and computational resources and resulted in significant performance improvements. The methods based on CNNs and RNNs have established the new state-of-the-art performance on the SED task, thanks to the capabilities to learn the non-linear relationship between time-frequency features of the audio signal and a target vector representing sound events. In @cite , the authors show how local'' patterns can be learned by a CNN and can be exploited to improve the performance of detection and classification of non-speech acoustic events occurring in conversation scenes, in particular compared to a FNN-based system which processes multiple resolution spectrograms in parallel.
- The combination of the CNN structure with recurrent units has increased the detection performance by taking advantage of the characteristics of each architecture. This is the case of convolutional recurrent neural networks (CRNNs) @cite , which provided state-of-the-art performance especially in the case of polyphonic SED. CRNNs consolidate the CNN property of local shift invariance with the capability to model short and long term temporal dependencies provided by the RNN layers. This architecture has been also employed in almost all of the most performing algorithms proposed in the recent editions of research challenges such as the IEEE Audio and Acoustic Signal Processing (AASP) Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE) @cite . On the other hand, if the datasets are not sufficiently large, problems such as overfitting can be encountered with these models, which typically are composed of a considerable number of free-parameters (i.e., more than 1M).
- The authors of @cite show that CapsNets outperform state-of-the-art approaches based on CNNs for digit recognition in the MNIST dataset case study. They designed the CapsNet to learn how to assign the suited partial information to the entities that the neural network has to predict in the final classification. This property should overcome the limitations of solutions such as max-pooling, currently employed in CNNs to provide local translation invariance, but often reported to cause an excessive information loss. Theoretically, the introduction of the dynamic routing can supply invariances for any property captured by a capsule, allowing also to adequately train the model without requiring extensive data augmentation or dedicated domain adaptation procedures.
- The early work in the domain of barcode detection from 2D images was motivated by the wide spread of mobile phones with cameras. @cite proposes a method for finding 2D barcodes via corner detection and 1D barcodes through spiral scanning. @cite introduces another method for 1D barcode detection based on decoding. Both approaches however require certain guidance from the user.
- In more recent papers authors pay more attention on developing solutions which can be done automatically with less user guidance. @cite finds regions with high difference between x and y derivatives, @cite calculates oriented histograms to find patches with dominant direction, @cite relies on morphology operations to detect both 1D and 2D barcodes, reporting high accuracy on their own data. The work of S " o r " o s @cite is notable as they compare their own algorithm with other works mentioned in this paragraph. They demonstrate that their approach is superior on the same dataset WWU Muenster Barcode Database (Muenster). Their algorithm is based on the idea that 1D barcodes have many edges, 2D barcodes have many corners, while text areas have both many edges and many corners.
- The work of Cresot , 2015 @cite is a solid baseline for 1D barcode detection. They evaluated their approach on Muenster and on extended ArTe-Lab 1D Medium barcode database (Artelab) provided by Zamberletti @cite outperforming him on both datasets. The solution in @cite seems to outperform @cite despite it is hard to compare as they were evaluated on different datasets using slightly different metrics. Cresot's algorithm detects dark bars of barcodes using Maximal Stable Extremal Regions (MSER) followed by finding imaginary perpendicular to bars center line in Hough space. In 2016 Cresot came with a new paper @cite improving previous results using a new variant of Line Segment Detector instead of MSER, which they called Parallel Segment Detector. @cite proposes another bars detection method for 1D barcode detection, which is reported to be absolutely precise in real-time applications.
- The problem of speaker naming in movies has been explored by the computer vision and the speech communities. In the computer vision community, the speaker naming problem is usually considered as a face person naming problem, in which names are assigned to their corresponding faces on the screen @cite @cite @cite @cite @cite . On the other hand, the speech community considered the problem as a speaker identification problem, which focuses on recognizing and clustering speakers rather than naming them @cite @cite . In this work, we aim to solve the problem of speaker naming in movies, in which we label each segment of the subtitles with its corresponding speaker name whether the speaker's face appeared on in the video or not.
- In @cite @cite , the authors proposed a weakly supervised model depending on subtitles and a character list. They extracted textual cues from the dialog: first, second, and third person references, such as I'm Jack'', Hey, Jack!'', and Jack left''. Using a character list from IMDB, they mapped these references onto true names using minimum edit distance, and then they ascribed the references to face tracks. Other work removed the dependency on a true character list by determining all names through coreference resolution. However, this work also depended on the availability of scripts @cite . In our model, we removed the dependency on both the true cast list and the script, which makes it easier to apply our model to other movies and TV shows.
- On the other hand, talking faces have been used to improve speaker recognition and diarization in TV shows @cite @cite @cite . In the case of @cite , they modeled the problem of speaker naming as facial recognition to identify speakers in news broadcasts. This work leveraged optical character recognition to read the broadcasters' names that were displayed on screen, requiring the faces to already be annotated.
- Nesterov's technique has also been used to accelerate non-gradient based methods. In @cite it was used to accelerate ADMM, and @cite used it to accelerate an approximate Newton method.
- Nesterov's accelerated gradient method is known to exhibit oscillatory behavior on convex problems. An interesting discussion on this is provided in @cite which formulates an ODE as the continuous time analogue of Nesterov's method. Such oscillatory behavior happens when the method approaches convergence, and can be alleviated by restarting the algorithm using the current iterate as the initial solution, usually resetting the sequence of momentum weights to its initial state close to 0. In @cite an explanation is provided of why resetting the momentum weight to a small value is effective using the ODE formulation of Nesterov's accelerated gradient descent. In @cite the use of adaptive restarting was explored for convex problems, and @cite explored the use of adaptive restarting and adaptive momentum weight for nonlinear systems of equations resulting from finite element approximation of PDEs. Our work is the first study of a general Nesterov-accelerated ALS scheme.
- Several ALS-specific nonlinear acceleration techniques have been developed recently as discussed in the introduction @cite @cite @cite . These algorithms often have complex forms and incur significant computational overhead. Our Nesterov-ALS scheme is simple and straightforward to implement, and only incurs a small amount of computational overhead.
- Compared with SMT, neural machine translation (NMT) has shown to produce state-of-the-art results @cite @cite . The central approach of NMT is an encoder-decoder architecture implemented by recurrent neural networks, which can represent the input sequence as a vector, and then decode that vector into an output sequence. Therefore, NMT models were used for text simplification task, and achieved good results @cite @cite @cite . The main limitation of the aforementioned NMT models for text simplification depended on the parallel ordinary-simplified sentence pairs. Because ordinary-simplified sentence pairs are expensive and time-consuming to build, the available largest data is EW-SEW that only have 296,402 sentence pairs. The dataset is insufficiency for NMT model if we want to NMT model can obtain the best parameters. Considering simplified data plays an important role in boosting fluency for phrase-based text simplification, and we investigate the use of simplified data for text simplification. We are the first to show that we can effectively adapt neural translation models for text simplifiation with simplified corpora.
- TML is quite related to transfer subspace learning (TSL) @cite @cite or transfer feature learning (TFL) @cite . An early work on TSL is presented in @cite that finds a low-dimensional latent space, where the distribution difference between the source and target domain is minimized. This algorithm is conducted in a transductive manner and not convenient to derive a representation for new samples. This issue is tackled by @cite , where a generic regularization framework is proposed for TSL based on Bregman divergence @cite . A low-rank TSL (LTSL) framework is proposed in @cite @cite , where the subspace is found by reconstructing the projected target data using the projected source data under the low-rank representation @cite @cite theme. The main advantage of the framework is that only relevant source data are utilized to find the subspace and noisy information can be filtered out. That is, it can avoid negative transfer. The framework is further extended in @cite to help recover missing modality in the target domain and improved in @cite by exploiting both low-rank and sparse structures on the reconstruction matrix.
- TFL is very similar to TSL and a representative method is presented in @cite , where the typical MMD is modified to take both the marginal and class-conditional distributions into consideration. More recent works on TFL are built upon the powerful deep feature learning. For example, considering that the features in deep neural networks are usually general in the first layers and task-specific in higher layers, @cite propose the deep adaptation networks (DAN), which frozes the general layers in convolutional neural networks (CNN) @cite and only conduct adaption in the task-specific layers. Besides, multi-kernel MMD (MK-MMD) @cite is employed to improve kernel selection in MMD. In DAN, only the marginal distribution difference between the source and target domains is exploited. This is improved by the joint adaptation networks (JAN) @cite , which is able to reduce the joint distribution divergence using a proposed joint MMD (JMMD). The JMMD can involve both the input features and output labels in domain adaptation. The constrained deep TSL @cite method can also exploit the joint distribution and the target domain knowledge is incorporated gradually during a progressive transfer procedure.
- There are several methods out there describing hardware accelerators which exploit feature map sparsity to reduce computation: Cnvlutin @cite , SCNN @cite , Cambricon-X @cite , NullHop @cite , Eyeriss @cite , EIE @cite . Their focus is on power gating or skipping some of the operations and memory accesses. While this automatically entails defining a scheme to feed the data into the system, minimizing the bandwidth was not the primary objective of any of them. They all use one of three methods: Zero-RLE (used in SCNN): A simple run-length encoding for the zero values, i.e. a single prefix bit followed by the number of zero-values or the non-zero value. Zero-free neuron array format (ZFNAf) (used in Cnvlutin): Similarly to the widely-used compressed sparse row (CSR) format, non-zero elements are encoded with an offset and their value. Compressed column storage (CCS) format (e.g. used in EIE): Similar to ZFNAf, but the offsets are stored in relative form, thus requiring less bits to store them. Few bits are sufficient, and in case they are all exhausted, a zero-value can be encoded as if it was non-zero.
- Most compression methods are focusing on minimizing the model size. Most of them are very complex (area) to implement in hardware and need large dictionaries. One such method, deep compression @cite , combines pruning, trained clustering-based quantization, and Huffman coding. Most of these steps involved cannot be applied to the intermediate feature map, which change for every inference as opposed to the weights which are static and can be optimized off-line. Furthermore, applying Huffman coding---while being optimal---implies storing a large dictionary (typically several MB). Similar issues arise when using Lempel-Ziv-Welch (LZW) coding @cite @cite as present in e.g. the ZIP compression scheme, where the dictionary is encoded in the compressed data stream. This makes it unsuitable for a lightweight and energy-efficient VLSI implementation @cite @cite .
- The most directly comparable approach, cDMA @cite , describes a hardware-friendly compression scheme to reduce the data size of intermediate feature maps. Their target application differs in that their main goal is to allow faster offloading of the feature maps from GPU to CPU memory through the PCIe bandwidth bottleneck during training, thereby enabling larger batch sizes and deeper and wider networks without sacrificing performance. They propose to use , which takes a block of 32 activation values, and generates a 32-bit mask where only the bits to the non-zero values are set. The non-zero values are transferred after the mask. This provides the main advantage over Zero-RLE that the resulting data volume is independent of how the values of the feature maps are serialized while also providing small compression ratio advantages. Note that this is a special case of Zero-RLE with a maximum zero burst length of 1.
- For this work, we build on a method known in the area of texture compression for GPUs, @cite , fuse it with sparsity-focused compression methods, and evaluate the resulting compression algorithm on intermediate feature maps to show compression ratios of 4.4 and 2.8 for 8 ,bit AlexNet and SqueezeNet, respectively.
- As mentioned before, we do not motivate new definitions for moral responsibility here, but draw on HK, which, in turn, is based upon @cite and the work on causality in @cite . Their framework is also related to the intentions model in @cite which considers predictions about the moral permissibility of actions via influence diagrams, though there is no emphasis on learning or tractability. In fact, the use of tractable architectures for decision-making itself is recent (see, e.g. @cite @cite ). The authors in @cite learn PSDDs over preference rankings (as opposed to decision-making scenarios more generally), though their approach does not take account of different preferences in different contexts.
- An important part of learning a model of moral decision-making is in learning a utility function. This is often referred to as (IRL) @cite or @cite . Our current implementation considers a simple approach for learning utilities (similar to @cite ), but more involved paradigms such as those above could indeed have been used.
- Our contributions here are related to the body of work surrounding MIT's Moral Machine website @cite . For example, @cite build on the theory of @cite by developing a computational model of moral decision-making whose predictions they test against Moral Machine data. Their focus is on learning abstract moral principles via hierarchical Bayesian inference, and although our framework can be used to these ends, it is also flexible with respect to different contexts, and allows constraints on learnt models. @cite develop a method of aggregating the preferences of all participants (again, a secondary feature of our system) in order to make a given decision. However, due to the large numbers of such preference orderings, tractability issues arise and so sampling must be used.
- Many variants of DBSCAN have been attempted to overcome the weakness of detecting clusters with varied densities. OPTICS @cite draws a reachability'' plot based on the @math -nearest neighbour distance. In the @math -axis of the plot, adjacent points follow close to each other such that point @math is the closest to @math in terms of the reachability distance'' The reachability-distance'' of object @math to object @math is the greater one between the core distance'' of @math and the distance between @math and @math . The core distance'' of @math is the minimum @math that makes @math a core'' object (the distance to its @math -nearest neighbour, @math ). . The reachability distance for each point is shown in @math -axis. Since clusters centre normally has a higher density or lower reachability distance than the cluster boundaries, each cluster is visible as a valley'' in this plot. Then a hierarchical method can be used to extract different clusters. The overall clustering performance depends on the hierarchical method employed on the reachability plot.
- The fusion of deep learning and sampling is not new. @cite proposed A-NICE-MC, where the proposal distribution in MCMC is, instead of domain-agnostic, adversarially trained using neural networks. Stein GAN also proposes to train a neural network to draw samples from given target distributions for probabilistic inference. Their method is by iteratively adjusting the weights according to the SVGD updates. From a GAN perspective, in each iteration of Stein GAN, the discriminator is performing a two sample test between the currently generated samples and the one-step updated samples by SVGD. This method generalized SVGD to training neural networks and it is minimizing the Kullback-Leibler divergence between the sampling distribution and the target inside a RKHS.
- As for the hardness of the problem, Dasgupta @cite showed that it is NP-hard for @math -means clustering in high dimensional space even if @math ; @cite proved that there is no PTAS for @math -means clustering if both @math and @math are large, unless @math . Guruswami and Indyk @cite showed that it is NP-hard to obtain any PTAS for @math -median clustering if @math is not a constant and @math is @math .
- Besides the traditional clustering models, Balcan considered the problem of finding the clustering with small difference from the unknown ground truth @cite @cite .
- Early story and plot generation systems relied on symbolic planning @cite @cite @cite @cite @cite @cite or case-based reasoning @cite @cite . These techniques only generated stories for predetermined, well-defined domains, conflating the robustness of manually-engineered knowledge with algorithm suitability. Regardless, symbolic planners in particular are able to provide long-term causal coherence. Early machine learning story generation techniques include textual case-based reasoning trained on blogs @cite and probabilistic graphical models learned from crowdsourced example stories @cite .
- Reinforcement learning (RL) addresses some of the issues of preserving coherence for text generation when sampling from a neural language model. Additionally, it provides the ability to specify a goal. Reinforcement learning @cite is a technique that is used to solve a Markov decision process (MDP). An MDP is a tuple @math where @math is the set of possible world states, @math is the set of possible actions, @math is a transition function @math , @math is a reward function @math , and @math is a discount factor @math . The result of reinforcement learning is a policy @math , which defines which actions should be taken in each state in order to maximize the expected future reward. The policy gradient learning approach to reinforcement learning directly optimizes the parameters of a policy model, which is represented as a neural network. One model-free policy gradient approach, REINFORCE @cite , learns a policy by sampling from the current policy and backpropagating any reward received through the weights of the policy model.
- RRT* @cite extends RRTs to guarantee asymptotic optimality by incrementally rewiring the RRT graph connections such that the shortest path is asymptotically guaranteed @cite . However, to determine an @math -near optimal path in @math dimensions, roughly @math samples are required, which makes RRT* no better than grid search methods @cite . Likewise, experiments in @cite @cite also confirmed that RRT* exhibits slow convergence rates to optimal path solution in higher-dimensional spaces. The following sections discusses various existing biased adaptive sampling methods to speed up the convergence rate of SMPs to compute optimal near-optimal path solution.
- @cite proposed the Informed-RRT* algorithm which takes an initial solution from RRT* algorithm to define an ellipsoidal region from which new samples are drawn to minimize the initial solution for a given cost function. Although Informed-RRT* demonstrated enhanced convergence towards an optimal solution, this method suffers in situations where finding an initial path solution takes most of the computation time. To address this limitation, proposed Batch Informed Trees (BIT*) @cite . BIT* is an incremental graph search technique where an ellipsoidal subset, containing configurations to update the graph, is incrementally enlarged. BIT* is shown empirically to outperform prior methods such as RRT* and Informed-RRT*. However, confining a graph search to ellipsoidal region slows down the performance of an algorithm in maze-like scenarios especially where the start and goal configurations are very close to each other, but the path among them traverses a complicated maze stretching waypoints far away from the goal. Furthermore, such a method would not translate to non-stationary environments or unseen environments.
- Many approaches exist that use learning to improve classical SMPs computationally. A recent method called a Lightning Framework @cite stored paths into a lookup table and used a learned heuristic to write new paths as well as to read and repair old paths. Another similar framework by @cite is an experience-based strategy to cache experiences in a graph instead of individual trajectories. Although these approaches exhibit superior performance in higher-dimensional spaces when compared to conventional planning methods, lookup tables are memory inefficient and incapable of generalizing well to new planning problems. @cite proposed a reinforcement learning-based method to bias samples in discretized workspaces. However, reinforcement learning-based approaches are known for their slow convergence as they require a large number of interactive experiences.
- It aims to classify and locate objects in an image. Generally, the methods of object detection can be divided into two main classes: two-stage methods and one-stage methods. Two-stage methods firstly extract some candidate object proposals from an image and then classify these candidate proposals into the specific object categories. R-CNN @cite and its variants (e.g., Fast RCNN @cite and Faster RCNN @cite ) are the most representative frameworks among the two-stage methods. Based on R-CNN series, researchers have done many improvements @cite @cite @cite . To accelerate detection speed, Dai @cite proposed R-FCN which uses position-sensitive feature maps for proposal classification and bounding box regression. To output multi-scale feature maps with strong semantics, Lin @cite proposed feature pyramid network (FPN) based on skip-layer connection and top-down pathway. Recently, Cai @cite trained a sequence of object detectors with increasing IoU thresholds to improve detection quality.
- One-stage methods directly predict object class and bounding box in a single network. YOLO @cite and SSD @cite are two of the earliest proposed one-stage methods. After that, many variants are proposed @cite @cite @cite @cite . DSSD @cite and RON @cite use the encoder-decoder network to add context information for multi-scale object detection. To train object detector from scratch, DSOD @cite uses dense layer-wise connections on SSD for deep supervision. Instead of using in-network feature maps of different resolutions for multi-scale object detection, STDN @cite uses scale-transferrable module to generate different high-resolution feature maps from last feature map. To solve class imbalance in the training stage, RetinaNet @cite introduces focal loss to downweight the contribution of easy samples.
- It aims to predict the semantic label of each pixel in an image, which has achieved significant progress based on fully convolutional networks (i.e., FCN @cite ). Generally, the methods of semantic segmentation can be also divided into two main classes: encoder-decoder methods and spatial pyramid methods. Encoder-decoder methods contain two subnetworks: an encoder subnetwork and a decoder subnetwork. The encoder subnetwork extracts strong semantic features and reduces spatial resolution of feature maps, which is usually based on the classical CNN models (e.g., VGG @cite , ResNet @cite , DenseNet @cite ) pre-trained on ImageNet @cite . The decoder subnetwork gradually upsamples the feature maps of encoder subnetwork. DeconvNet @cite and SegNet @cite use max-pooling indices of the encoder subnetwork to upsample the feature maps. To extract context information, some methods @cite @cite @cite adopt skip-layer connection to combine the feature maps from the encoder and decoder subnetworks.
- Spatial pyramid methods adopt the idea of spatial pyramid pooling @cite to extract multi-scale information from the last output feature maps. Chen @cite @cite @cite @cite proposed to use multiple convolutional layers of different atrous rates in parallel (called ASPP) to extract multi-scale features. Instead of using convolutional layers of different atrous rates, Zhao @cite proposed pyramid pooling module (called PSPnet), which downsamples and upsamples the feature maps in parallel. Yang @cite proposed to use dense connection to cover object scale range densely.
- It aims to simultaneously detect objects and predict pixel semantic labels by a single network. Recently, researchers have done some attempts. Yao @cite proposed to use the graphical model to holistic scene understanding. Teichmann @cite proposed to join object detection and semantic segmentation by sharing the encoder subnetwork. Kokkinos @cite also proposed to integrate multiple computer vision tasks together. Mao @cite found that joint semantic segmentation and pedestrian detection can help improve performance of pedestrian detection. The similar conclusion is also demonstrated by SDS-RCNN @cite . Meanwhile, joint instance semantic segmentation and object detection is also proposed @cite . Recently, Dvornik @cite proposed a real-time framework (called BlitzNet) for joint object detection and semantic segmentation. It is based on the encoder-decoder network, where each layer of the decoder is used to detect objects of different scales and multi-scale fused layer is used for semantic segmentation.
- Procedural content generation via Machine Learning @cite is a relatively new field, focused on generating content through machine learning methods. The majority of PCGML approaches represent black box methods, without any prior approach focused on explainability or co-creativity. We note some discussion in the survey paper on potential collaborative approaches. Summerville explored adapting levels to players, but no work to our knowledge looks at adapting models to individual designers.
- Super Mario Bros. (SMB) represents a common area of research into PCGML @cite @cite @cite @cite . Beyond explainability, our approach differs from prior SMB PCGML approaches in terms of representation quality and the size of generated content. We focus on the generation of individual level sections instead of entire levels in order to better afford collaborative level building @cite . Second, prior approaches have abstracted away the possible level components into higher order groups. For example, treating all enemy types as equivalent and ignoring decorative elements. We make use of a rich representation of all possible level components and an ordering that allows our approach to place decorative elements appropriately.
- Design patterns represent a well-researched approach to game design @cite . In theory, game design patterns describe general solutions to game design problems that occur across many different games. Game Design patterns have been used as heuristics in evolutionary PCG systems including in the domain of Super Mario Bros. @cite . Researchers tend to derive game design patterns through either rigorous, cross-domain analysis @cite or based upon their subjective interpretation of game structure. We embrace this subjectivity in our work by having designers create a language of game design patterns unique to themselves with which to interact with a PCGML system.
- A series of studies exist for covariate shift assuming that a regression function is an analytical function @cite @cite @cite , such as kernel ridge regression @cite . In our problem, however, we assume that the functional relation of the regression model @math is only given as a nonanalytical function: a simulation. The difference between kernel ridge regression and the proposed method with formulation is presented in .
- Kernel mean embedding is a framework to map distributions into a reproducing kernel Hilbert space (RKHS) @math as a feature space @cite . Figure shows a schematic illustration of the kernel mean embedding. In this section, we briefly review three applications of kernel mean embedding: kernel ABC, kernel sum rule, and kernel herding. The detailed formulations of these methods are described in with the proposed method.
- As ASP has been applied to more and more problems, the importance of ASP software development tools has been realized by the community. Some integrated development environment (IDE) tools, e.g., APE @cite , ASPIDE @cite , iGROM @cite and SeaLion @cite have previously been developed. They provide a graphical user interface for users to carry out a sequence of tasks from editing an ASP program to debugging that program, easing the use of ASP significantly. However, the target audience of these tools is experienced software developers. Compared with the existing environments, our environment is online, self contained (i.e., fully independent of the users' local computers) and provides a very simple interface, focusing on teaching only. The interface is operable by any person who is able to use a typical web site and traverse a file system.
- As for online systems, in addition to IDP mentioned earlier, there are several others. Both DLV and Clingo offer online environments ( http: asptut.gibbi.com and http: potassco.sourceforge.net clingo.html respectively) which provide an editor and a window to show the direct output of the execution of DLV and Clingo command, but provide no other functionalities. We also noted SWISH http: lpsdemo.interprolog.com which offers an online environment for Prolog and a more recent computer language Logic-based Production Systems @cite . A recent online system LoIDE @cite allows a user to edit ASP programs and find answer sets of the programs. LoIDE allows a programmer to highlight names in answer sets.
- Compared to the CF-based approach, the hybrid model relies on only two sources of information to mitigate the sparsity problem. Based on the how tightly the interaction data and auxiliary information are integrated, the hybrid model can be divided into two subcategories: loose coupled and tightly coupled methods @cite . The loosely coupled method combines the output from separate collaborative and content-based systems into a final recommendation by a linear combination @cite or voting scheme @cite . The tightly coupled method takes the processed auxiliary information as a feature of the collaborative method @cite . However, they all assume that the features are the good representation which is usually not the case. Collaborative topic regression (CTR) @cite is a method that explicitly integrates the latent Dirichlet allocation @cite (LDA) and PMF for two source information with promising result. However, the representation ability is limited to the topic model.
- There is also another line of research that only utilizes one single learner with only auxiliary information such as review text as input for rating regression @cite @cite , DeepCoNN @cite that models users and items using review text for rating prediction problems have shown promising result. Although they utilize word-embedding technique @cite and Convolutional neural network @cite (CNN) to learn good representation for text data, compared to the hybrid model, it only uses one single learner to learn user item representation only with the auxiliary information as input, so it can not capture the implicit relationship between users stored in interaction data well.
- Recently, there are two common approaches that have become popular which use neural networks to learn latent representations, Encoder-Decoders and Generative Adversarial Networks (GAN) @cite . Encoder-Decoders, such as an Autoencoder @cite , compress data by encoding the inputs into a latent vector which is then uncompressed by the decoder. The Autoencoder is trained by minimizes the difference between the input and the output of the decoder. GANs take the opposite approach and use a generator, similar to an encoder, then uses a discriminator to maximize the authenticity of the generated data. Where Encoder-Decoders learn the latent representations directly, GANs learn to construct data from random latent representations.
- For offline and online handwriting conversion, it has traditionally been done using classical feature-based methods @cite but there has been some recent work using neural networks. @cite used a CNN and RNN-based Encoder-Decoder network for handwriting trajectory recovery. Attempts were also made using neural networks to identify graph features @cite and for sequential stroke prediction using regression CNNs @cite .
- @cite introduced RNN into language modeling to handle arbitrary-length sequences in computing conditional probability @math . They demonstrated that the RNN language model outperformed the Kneser-Ney smoothed 5-gram language model @cite , which is a sophisticated @math -gram language model.
- @cite drastically improved the performance of language modeling by applying LSTM and the dropout technique @cite . @cite applied dropout to all the connections except for recurrent connections but @cite proposed variational inference based dropout to regularize recurrent connections. @cite demonstrated that the standard LSTM can achieve superior performance by selecting appropriate hyperparameters. Finally, @cite introduced DropConnect @cite and averaged SGD @cite into the LSTM language model and achieved state-of-the-art perplexities on PTB and WT2. For WT103, @cite found that QRNN @cite , which is a faster architecture than LSTM, achieved the best perplexity. Our experimental results show that the proposed char @math -MS-vec improved the performance of these state-of-the-art language models.
- @cite introduced character information into RNN language models. They applied CNN to character embeddings for word embedding construction. Their proposed method achieved perplexity competitive with the basic LSTM language model @cite even though its parameter size is small. @cite also applied CNN to construct word embeddings from character embeddings. They indicated that CNN also positively affected the LSTM language model in a huge corpus. @cite proposed a method concatenating character embeddings with a word embedding to use character information. In contrast to these methods, we used character @math -gram embeddings to construct word embeddings. To compare the proposed method to these methods, we combined the CNN proposed by @cite with the state-of-the-art LSTM language model (AWD-LSTM) @cite . Our experimental results indicate that the proposed method outperformed the method using character embeddings (charCNN in Table ).
- Some previous studies focused on boosting the performance of language models during testing @cite @cite . For example, @cite proposed dynamic evaluation that updates model parameters based on the given correct sequence during evaluation. Although these methods might further improve our proposed language model, we omitted these methods since it is unreasonable to obtain correct outputs in applications such as machine translation.
- Previous studies proposed various methods to construct word embeddings. @cite applied Recursive Neural Networks to construct word embeddings from morphemic embeddings. @cite applied bidirectional LSTMs to character embeddings for word embedding construction. On the other hand, @cite and @cite focused on character @math -gram. They demonstrated that the sum of character @math -gram embeddings outperformed ordinary word embeddings. In addition, @cite found that the sum of character @math -gram embeddings also outperformed word embeddings constructed from character embeddings with CNN and LSTM.
- As an encoder, previous studies argued that additive composition, which computes the (weighted) sum of embeddings, is a suitable method theoretically @cite and empirically @cite @cite . In this paper, we used multi-dimensional self-attention to construct word embeddings because it can be interpreted as an element-wise weighted sum. Through experiments, we indicated that multi-dimensional self-attention is superior to the summation and standard self-attention as an encoder.
- Meta-learning originates in the concept of learning to learn @cite @cite @cite . Recently, there has a been a wide interest in finding ways to improve learning speeds and generalization to new tasks through meta-learning. The main directions of the research in this area can be divided into learning representations that can be easily adapted to new tasks @cite , learning unsupervised rules that can be transferred between tasks @cite @cite , learning optimizer policies that transform policy updates with respect to known loss or reward functions @cite @cite @cite @cite , or learning loss reward landscapes @cite @cite .
- Our framework falls into the category of learning loss landscapes; similar to @cite , we aim at learning a separate optimization procedure that can be applied to various optimizee models. However, in contrast to @cite and @cite , our framework does not require a specific recurrent architecture of the optimizer and can operate without an explicit external loss or reward function during test time. Furthermore, as our learned loss functions are independent of the models to be optimized, they can be easily transferred to other optimizee models, in contrast to @cite , where the learned representation can not be separated from the original model of the optimizee.
- A range of recent works demonstrate advantages of meta-learning for improving exploration strategies in RL settings, especially in the presence of sparse rewards. @cite , an agent is trained to mimic expert demonstrations while only having access to a sparse reward signal during test time. @cite and @cite , a structured latent exploration space is learned from prior experience, which enables fast exploration in novel tasks. @cite proposes a method for automatically learning potential-based reward shaping by learning the Q-function parameters during the meta-training phase, such that at meta-test time the Q-function can adapt quickly to new tasks. In our work, we also demonstrate that we can significantly improve the RL sample efficiency by training our meta-loss to optimize an actor policy, even when providing only limited or no reward information to the learned loss function at test time.
- Closest to our method are the works on @cite , @cite and @cite . In contrast to using an evolutionary approach as in @cite , we design a differentiable framework and describe a way to optimize the loss function with gradient descent in both supervised and reinforcement learning settings. In @cite , instead of learning a differentiable loss function directly, a teacher network is trained to predict parameters of a manually designed loss function, whereas each new loss function class requires a new teacher network design and training. Our method does not require manual design of the loss function parameterization as our loss functions are learned entirely from data. Finally, in @cite a is learned to provide a value function conditioned on a task, used to train an actor policy. Although training a meta-critic in the supervised setting reduces to learning a loss function similar to our work, in the reinforcement learning setting we show that it is possible to use learned loss functions to optimize policies directly with gradient descent.
- Many topic models such as LDA @cite treat documents as independent mixtures, yet this approach fails to model how comments interact with one another throughout a larger discourse if such connections exist in the data. Other work has considered modeling hierarchy in topics @cite . These models form hierarchical representations of topics themselves, but still treat documents as independent. While this approach can succeed in learning topics of various granularities, it does not explicitly track how topics interact in the context of a nested conversation.
- Some approaches such as Pairwise-Link-LDA and Link-PSLA-LDA @cite attempt to model interactions among documents in an arbitrary graph, albeit with important drawbacks. The former models every possible pairwise link between comments, and the latter models links as a bipartite graph, limiting its ability to scale to large tree-structured threads. Similar work on Topic-Link LDA @cite models link probabilities conditioned on both topic similarity and an authorship model, yet this approach is poorly suited to high volume, semi-anonymous online domains. Other studies have leveraged reply-structures on Reddit in the context of predicting persuasion , but DDTM differs in its generative, unsupervised approach.
- DDTM's emission potentials are similar to those of Replicated Softmax @cite , an undirected model based on a Restricted Boltzmann Machine. Unlike LDA-style models, RS does not assign a topic to each word, but instead builds a distributed representation. In this setting, a single word can be likely under two different topics, both of which are present, and lend probability mass to that word. LDA-style models by contrast would require the topics to compete for the word.
- Planning for systems with high-dimensional motion flexibility quickly reaches its limits for larger environments since the search space grows exponentially. Similar to multiresolution planning, several approaches utilize multiple representations with different planning dimensionalities to decrease planning complexity. @cite generate an initial plan in a low-dimensional search space and replan in the high-dimensional search space by only considering those states that are part of the low-dimensional plan. @cite plan a path in a low-dimensional search space and only switch to high-dimensional planning in those areas where low-dimensional planning cannot find a solution. Similarly, @cite plan in 2D and switch to high-dimensional planning in the robot vicinity and at key points. As described for multiresolution planning, planning with multiple robot configuration dimensionalities might lead to wrong or bad plans, since a low-dimensional robot representation might assess challenging situations wrongly.
- To achieve further planning acceleration, it is an obvious idea to combine multiresolution and multidimensional planning. However, only few works, such as by @cite address this. Different planning dimensionalities and resolutions are applied by using different sets of motion primitives. A fine resolution is only considered close to the start and goal pose and close to obstacles. A high planning dimensionality is considered for states which will be reached within a given time interval. This allows the planner to provide detailed plans close to the robot while planning times stay feasible. The drawbacks of both, multiresolutional and multidimensional planning also apply to this work.
- Thus, several alternative methods have arisen in the last years , aiming to provide a fast sensory positioning of a set of products by assessors who are not necessarily trained. Skipping the need to train the panellists allows to elude the need of waiting a long time before obtaining results, as well as the need of agreeing on particular attributes, which may become difficult when working with experts like wine professionals or chefs . Introduced by @cite @cite , Projective Mapping asks the assessors to position the presented samples on a two-dimensional space, usually a blank sheet of paper as tablecloth, following their own criteria: The more similar they perceive two samples, the closer they should position them, and vice versa . In those seminal works, the data were analysed by generalized procrustes analysis (GPA) and principal component analysis (PCA) , using the RV coefficient to compare the method with conventional profiling.
- Projective Mapping has been successfully used with many different kinds of products, among which the application to wine stands out . Other examples of beverages analysed by these methods are beers , citrus juices , drinking waters high alcohol products , hot beverages , lemon iced teas , powdered juices , or smoothies . The book by @cite details more products to which consumer based descriptive methodologies have been applied.
- Studies such as @cite show that bag-of-n-grams model is usually considered deficient in dealing with data sparsity and poor generalization. In particular, Le shows that CBOW and bigram models perform poorly in encoding paragraph information, and bigram representation generally outperforms unigram. This induces the question whether this result can be generalized to sentence representation and whether a bigger n leads to even better performance. However, a more detailed and systematic analysis has not been done on the properties of bag-of-n-grams embeddings.
- Although almost a consensus among deep learning community, we found interesting theoretical and experimental results from @cite . explore so-called The Lottery Ticket Hypothesis" and realized that the success of many large network can be attributed to the large number of layers and parameters that made possible for successful sub-networks to appear, which are usually discovered via pruning.
- In particular, we need to assume that the base network that we adapt for transfer learning purpose is a high quality network in terms of its convolution channels. We assume the features learned from training well-design networks like ResNet and Inception using ImageNet dataset are generally transferable. This assumption is confirmed by the work What makes ImageNet good for transfer learning?'' from @cite . ImageNet is confirmed to be a good dataset to produce general convolution feature extractors.
- Data-driven robotic grasp detection for novel object has been investigated extensively @cite . Saxena proposed a machine learning based method to rank the best graspable location for all candidate image patches from different locations @cite . Jiang proposed a 5D robotic grasp representation and further improved the work of Saxena by proposing a machine learning method to rank the best graspable image patch whose representation includes orientation and gripper distance among all candidates @cite . The work of Jiang achieved the prediction accuracy of 60.5 time of 50 sec (50,000 ms) per image.
- Redmon proposed a deep learning regressor based robotic grasp detection method based on the AlexNet @cite that that yielded 84.4 with fast computation time (76 ms per image) @cite . When performing robotic grasp regression and object classification together, image-wise prediction accuracy of 85.5 Kumra also proposed a real-time regression based grasp detection method using ResNet @cite especially for multimodal information (RGB-D). Their method yielded up to 89.2 with fast computation time (103 ms per image) @cite .
- The surveillance problem is related to the art gallery problem in computational geometry, where the task is to determine the minimum set of guards who can together observe a polygonal gallery. Vertex guards must be stationed at the vertices of the polygon, while point guards can be anywhere in the interior. For simply-connected polygonal scenes, Chv 'atal showed that @math vertex guards, where @math is the number of vertices, are sometimes necessary and always sufficient @cite . For polygonal scenes with @math holes, @math point guards are sufficient @cite @cite . However, determining the optimal set of observers is NP-complete @cite @cite @cite .
- propose an alternating minimization scheme for optimizing the visibility of @math observers @cite . use a system of differential equations to optimize the location and orientation of @math sensors to maximize surveillance @cite . Both works assume the number of sensors is given.
- For the exploration problem, a class of approaches pick new vantage points along shadow boundaries (aka frontiers), the boundary between free and occluded regions @cite . propose a frontier-based approach for 2D polygonal environments which requires @math views, where @math is the number of reflex angles @cite . For general 2D environments, @cite @cite @cite use high order ENO interpolation to estimate curvature, which is then used to determine how far past the horizon to step. However, it is not necessarily optimal to pick only points along the shadow boundary, e.g. when the map is a star-shaped polygon @cite .
- There has been some attempts to incorporate deep learning into the exploration problem, but they are myopic and focus on navigation rather than exploration. The approach of @cite terminates when there is no occlusion within view of the agent, even if the global map is still incomplete. Tai and Liu @cite @cite @cite train agents to learn obstacle avoidance.
- While graph structures are central tools for various learning tasks ( semi-supervised learning in @cite @cite ), how to design efficient graph convolution networks has become a popular research topic. Graph convolutional approaches are often categorized into spectral and non-spectral classes @cite . The spectral approach first proposed by @cite defines the convolution operation in Fourier domain. Later, @cite enables localized filtering by applying efficient spectral filters, and @cite employs Chebyshev expansion of the graph Laplacian to avoid the eigendecomposition. Recently, GCN is proposed in @cite to simplify previous methods with first-order expansion and re-parameterization trick. Non-spectral approaches define convolution on graph by using the spatial connections directly. For instance, @cite learns a weight matrix for each node degree, the work by @cite defines multiple-hop neighborhoods by using the powers series of a transition matrix, and other authors @cite extracted normalized neighborhoods that contain a fixed number of nodes.
- A recent line of research is to generalize convolutions by making use of the patch operation @cite and self-attention @cite . As opposed to GCNs, these methods implicitly assign different importance weights to nodes of a same neighborhood, thus enabling a leap in model capacity. Particularly, Monti al @cite presents mixture model CNNs to build CNN architectures on graphs using the patch operation, while the graph attention networks @cite compute the hidden representations of each node on graph by attending over its neighbors following a self-attention strategy.
- More recently, two kinds of sampling-based methods including GraphSAGE @cite and FastGCN @cite were developed for fast representation learning on graphs. To be specific, GraphSAGE computes node representations by sampling neighborhoods of each node and then performing a specific aggregator for information fusion. The FastGCN model interprets graph convolutions as integral transforms of embedding functions and samples the nodes in each layer independently. While our method is closely related to these methods, we develop a different sampling strategy in this paper. Compared to GraphSAGE that is node-wise, our method is based on layer-wise sampling as all neighborhoods are sampled as altogether, and thus can allow neighborhood sharing as illustrated in Figure . In contrast to FastGCN that constructs each layer independently, our model is capable of capturing the between-layer connections as the lower layer is sampled conditionally on the top one. We detail the comparisons in . Another related work is the control-variate-based method by @cite . However, the sampling process of this method is node-wise, and the historical activations of nodes are required.
- , is based on a recent observation that deep generative models immerse random Riemannian manifolds @cite . This implies a change in the way distances are measured in the latent space, which reveals a clustering structure. Unfortunately, practical algorithms for actually computing such distances are missing, and it is the main focus of the present paper. With such an algorithm in hand, clustering can be performed with high accuracy in the latent space of an off-the-shelf VAE.
- Depth from vision is one of the problems studied with neural network, and has been addressed with a wide range of training solution. Some datasets @cite @cite allow a neural network to learn end-to-end depth or disparity @cite @cite @cite . Reprojection error has also been used for unsupervised training for depth from a single image @cite @cite or for disparity between two frames of a stereo rig @cite @cite .
- For depth from more complex movement from a monocular camera, current state of the art methods tend to use motion, and especially structure from motion, and most algorithm do not rely on deep learning @cite @cite @cite . Prior knowledge w.r.t. scene is used to infer a sparse depth map with its density usually growing over time. These techniques also called SLAM are typically used with unstructured movement (translation and rotation with varying magnitudes), produce very sparse point-cloud based 3D maps and require heavy calculation to keep track of the scene structure and align newly detected 3D points to the existing ones.
- Tracking-by-detection is becoming the most popular strategy for multi-object tracking. @cite associated tracklets with detection in different ways according to their confidence values. Sanchez- @cite exploited multiple detectors to improve tracking performance. They collected outputs from multiple detectors, during the so called over-detection process. Combining results from multiple detectors can improve the tracking performance but is not efficient for real-time applications. In contrast, our tracking framework needs only one detector and generates candidates from existing tracks. @cite used a binary classifier and single object tracker for online multi-object tracking. They shared the feature maps for classification but still had a high computation complicity.
- UniformLink @cite builds a sentence graph on a set of similar documents, where a sentence's score is computed based on both with-in document score and cross-document score. URank @cite uses a unified graph-based framework to study both single-document and multi-document summarizations.
- @math @cite , as well as @math @cite , use a bipartite graph to represent a document and a different algorithm, Hyperlink-Induced Topic Search (HITS) @cite , is used to score sentences. They both treat the summarization problem as an ILP problem, which maximizes the sentence importance, non-redundancy, and coherence at the same time. However, since ILP is an NP-hard problem, obtaining an exact solution to an ILP problem is intractable.
- Submodularity optimization @cite and Latent Semantic Analysis @cite are two other widely used unsupervised techniques for extractive summarizations.
- Deep learning methods, able to learn sentence or document representations automatically, have recently been used to score sentences. For example, R2N2 @cite uses a recursive neural network for both word level and sentence level scoring, followed by an ILP optimization strategy for selecting sentences. CNN-W2V @cite is another example, which modifies a convolutional-neural-network (CNN) model of sentence classification @cite to rank sentences. SummaRuNNer @cite , on the other hand, treats summarizations as sequence classifications and uses a two-layer bi-directional recurrent neural network (RNN) model to extract sentences, where the first layer RNN is for words and the second layer is for sentences. Unlike unsupervised methods, the state-of-the-art deep learning approaches require a larger dataset and a significantly longer time to train a model, yet with a much lower ROUGE-1 score when evaluating on DUC dataset.
- Our work relates to the field of model pretraining in NLP and computer vision fields @cite . In the NLP community, works on model pretraining can be divided into unstructured text-based and structured knowledge-based ones. Both word embedding learning algorithms @cite and contextual embedding learning algorithms @cite @cite belong to the text-based direction. Compared with these methods, which aim to learn a representation for a continuous sequence of words, our goal is to model the concept relatedness with graph structure in the knowledge base. Previous works on knowledge-based pretraining are typically validated on knowledge base completion or link prediction task @cite @cite . Our work belongs to the second line. We pre-train models from the commonsense knowledge base and apply the approach to the question answering task. We believe that combining both structured knowledge graphs and unstructured texts to do model pretraining is very attractive, and we leave this for future work.
- Initially most behavior planners were handcrafted state machines, made up by a variety of modules to handle different tasks of driving. During the DARPA Urban Challenge Boss (CMU) for example used five different modules to conduct on road driving. The responsibilities of the modules ranged from lane selection, merge planning to distance keeping @cite . Other participants such as Odin (Virginia Tech) or Talos (MIT) developed very similar behavior generators @cite @cite .
- Due to the limitations of state machines, current research has expanded on the initial efforts by creating more complex and formal models: A mixture of POMDP, stochastic non-linear MPC and domain knowledge can be used to generate lane change decisions in a variety of traffic scenarios @cite . Capturing the mutual dependency of maneuver decisions between different agents, planning can be conducted with foresight @cite @cite . While @cite plans only the next maneuver focusing on the reduction of collision probabilities between all traffic participants, @cite explicitly addresses longer planning horizons and the replanning capabilities of others.
- In recent years, deep reinforcement learning (DRL) has been successfully used to learn policies for various challenges. used DRL in conjunction with supervised learning on human game data to train the policy networks of their program AlphaGo @cite ; @cite @cite present an overview of RL and DRL respectively. In @cite and @cite their agents achieve superhuman performance in their respective domains solely using a self-play reinforcement learning algorithm which utilizes Monte Carlo Tree Search (MCTS) to accelerate the training. proposed their deep Q-network (DQN) @cite @cite which was able to learn policies for a plethora of different Atari 2600 games and reach or surpass human level of performance. The DQN approach offers a high generalizability and versatility in tasks with high dimensional state spaces and has been extended in various work @cite @cite @cite . Especially actor-critic approaches have shown huge success in learning complex policies and are also able to learn behavior policies in domains with a continuous action space @cite @cite .
- In the domain of autonomous driving, DRL has been used to directly control the movements of simulated vehicles to solve tasks like lane-keeping @cite @cite . In these approaches, the agents receive sensor input from the respective simulation environments and are trained to determine a suitable steering angle to keep the vehicle within its driving lane. Thereby, the focus lies mostly on low-level control.
- Since it can be problematic to model multi-agent scenarios as a Markov Decision Process (MDP) due to the unpredictable behavior of other agents, one possibility is to decompose the problem into learning a cost function for driving trajectories @cite . To make learning faster and more data efficient, expert knowledge can be incorporated by restricting certain actions during the training process @cite . Additionally, there is the option to handle task and motion planning by learning low-level controls for lateral as well as longitudinal maneuvers from a predefined set and a high-level maneuver policy @cite .
- Other approaches are not explicitly cooperative, however they do capture the interdependencies of actions as they evaluate the threat resulting from different maneuver combinations, and hence predict the future motions of vehicles @cite and are able to generate proactive cooperative driving actions @cite .
- Furthermore, off-line calculated maneuver templates can be used to devise cooperative plans. First, the maneuver template needs to match a given traffic scene with specific initial constraints, then the maneuver described in the template is checked for feasibility. If multiple templates are feasible given a specific traffic scene the maneuver template with the lowest cost specifies the cooperative maneuver @cite .
- Our work joins a growing number of works that harness steady-state parametrizations. Such results include criteria for when such parametrizations exist @cite @cite and methods for using them to determine whether a network is multistationary @cite @cite @cite @cite . Going further, steady-state parametrizations can also be used to find a witness to multistationarity or even the precise parameter regions that yield multistationarity @cite @cite . In this work, we use a steady-state parametrization in a novel way: to study oscillations via Hopf bifurcations. (Our approach is similar in spirit to using Clarke's convex parameters together with a Hopf-bifurcation criterion @cite @cite @cite @cite ).
- As mentioned earlier, there has been much interest in the dynamics of phosphorylation systems @cite . The mixed-mechanism network fits into the related literature as follows. The mixed network is a dual-site network situated between two extremes: the fully processive dual-site network -- in which the phosphorylation and dephosphorylation mechanisms are both processive -- and the fully distributive dual-site network. One might therefore expect the dynamics of the mixed-mechanism network to straddle those of the two networks. This is indeed the case. As summarized in Table , and reviewed in @cite , fully processive networks are globally convergent to a unique steady state @cite @cite @cite , while mixed-mechanism networks admit oscillations but not bistability @cite , and fully distributive networks admit bistability @cite (and the question of oscillations is open @cite ).
- Finally, we revisit Suwanmajo and Krishnan's claim mentioned earlier that the mixed-mechanism network is among the simplest enzymatic mechanisms with oscillations. In support of this claim, Tung proved that the simpler system obtained from the mixed-mechanism network by taking its (two-dimensional) Michaelis-Menten approximation, is not oscillatory @cite . Moreover, Rao showed that this approximation is globally convergent to a unique steady state @cite . The validity of the Michaelis-Menten approximation for phosphorylation systems has been called into question @cite , and what we know about the mixed-mechanism system concurs: this system is oscillatory, but its Michaelis-Menten approximation is not.
- (ER), first introduced in @cite , is a crucial component to stabilize convergence in off-policy deep RL networks, as demonstrated by the successful Deep Q-Network @cite algorithm. ER dissects episodes into experience quadruples of the form @math , where the elements represent current state, action, next state and reward respectively. The experiences are stored in a collective database termed as , from where they undergo uniform random sampling to form minibatches for training the RL networks. In addition to allowing for temporal independence in the training set, this approach enables the reuse of past experiences, as any single experience may be sampled multiple times, and hence increases sample-efficiency.
- Subsequent research aimed to find more effective sampling strategies and compositions of the replay buffer. Prioritized Experience Replay (PER) @cite achieves higher sample efficiency by selecting experiences from the replay buffer according to a frequency distribution prioritizing the importance of each individual transition. @cite investigates the effect of size of the replay buffer on performance and proposes Combined Experience Replay (CER) to alleviate the liability of a large replay buffer. Hindsight Experience Replay (HER) @cite , described in greater detail in the following section, strategically augments the replay buffer by reformulating unsuccessful episodes as successful transitions accomplishing a different goal. The resulting balance in successful and unsuccessful experiences in the replay buffer overcomes the disadvantage of sparse rewards.
- Hindsight bias was first documented by Fischhoff @cite , referring to the inflation in people's predicted likelihood of the true outcome of an event, after the outcome is known. This phenomenon, also termed as creeping determinism", is one of the most pervasive cognitive biases and routinely affects judgments in multiple domains, including medical diagnoses @cite , criminal justice @cite and financial systems @cite .
- Parallel methods to improve sample-efficiency of deep RL include learning hierarchical abstractions @cite @cite , reducing variance in policy gradients @cite , model-based algorithms @cite and effective exploration @cite @cite .
- A few papers have worked with the charCNN and analyzed its performance. @cite show that the hidden states of the charCNN can classify morphology better than standard seq2seq models at the word level. @cite show that charCNNs are sensitive to various types of noise. Our analysis complements this line of work, supporting the conclusion that the charCNN is learning morphology better than standard seq2seq models and explores additionally how its strengths interact with BPE.
- Soon after the release of the first widely successful deep learning models for image classification, perturbation schemes were proposed for understanding them @cite or for exposing some of their shortcomings through the generation of adversarial perturbations that confuse the models into misclassifying @cite . These two areas of study using perturbation methods have seen considerable progress over the years.
- Following the seminal work of Szegedy et. al. @cite on adversarial perturbations, the linear nature of convolutional layers (before activation functions) was exploited for the rapid generation of adversarial examples @cite . The existence of (untainted) real-world images that also cause deep learning models to misclassify has also been reported @cite . These and other findings triggered a wave of introspection resulting on a series of comprehensive studies such as an evaluation of the robustness of neural networks @cite @cite , model generalization assessments @cite @cite , and a proof of the existence of universal adversarial perturbations that can be applied to any image @cite . An impressive consequence of the latter result may be found in the work of Baluja and Fisher @cite who were able to train adversarial models on a full dataset of images with the explicit goal of confusing a target model into believing that all of the samples belong to an arbitrarily chosen class.
- The only previous work that we are aware of regarding perturbations of video samples for the study of deep neural network models was recently published by Wei et. al. @cite . In this study, video pixels are modified to reduce the class score assigned to the original video while attempting to keep the perturbed video as close as possible to the original one. Modifications to the original video are confined to a few video frames by the use in the temporal direction of a norm penalty during the optimization and by the introduction of a mask that explicitly prevents modifications to some frames. The authors were able to produce sparse adversarial perturbations that considerably reduced the original score assigned by the model.
- Due to the large number of parameters present in deep neural networks, generally a very large number of training samples is necessary to prevent overfitting. This fact was recognized early on in the development of modern convnet architectures @cite and prompted the development of data augmentation techniques. Early image data augmentation was achieved by shifting, rotating, cropping, and flipping the images @cite @cite . More recently, other heuristics have been used for data augmentation such as color casting, vignetting and lens distortion @cite leading to the training of more robust image classification models. In general, any transformation that does not change the class of a sample can be exploited for data augmentation. Thus, adversarial image perturbations generated by a learned perturbative mask @cite @cite @cite or through the training of generative adversarial networks (GANs) @cite have been proposed for data augmentation. GANs have been successfully applied to the augmentation of biomedical images @cite field in which the number of available samples is limited.
- Domain adaption aims to generate a shared representation for distinct domains @cite . @cite use multi-task and transfer learning to act in a new domain by using previous knowledge. The multi-task learning method employed, called "Actor-Mimic", used model compression techniques to train one multi-task network from several expert networks. The multi-task network is treated as a Deep Q network (DQN) pre-trained on some tasks in the source domain. The DQN which used this multi-task pre-training learns a target task significantly faster than a DQN starting from a random initialization, effectively demonstrating that the source task representations generalize to the target task. @cite adapt the DQN algorithm to competitively learn two similar Atari games. The authors prove that a DQN trained to solve a particular specific task does not perform well on unforeseen tasks, however, their competitively learning technique does when a DQN agent is trained simultaneously.
- Consensus @cite should guarantee of the system, , and (consistency) of data stored in the ledger. Liveness means that the system should never stop and should be able to recover from errors. Security means that non-faulty peers should not accept false data. Consistency means that all non-faulty peers should maintain or converge to the same global ordering and state. Various consensus algorithms have been proposed for different situations. Consensus algorithms are often discussed in terms of some weak synchrony assumption, and, as shown by @cite , no distributed consensus algorithm can give a deterministic solution in a fully asynchronous network.
- Zyzzyva @cite extended PBFT, avoiding the expensive three-phase commit protocol, utilizing fast track and actively involving the client into the consensus process. In the best case scenario, the client sends the request to the leader, who is broadcasts it to all other replicas, and the client waits for replies from all the peers. If the client receives 3 @math +1 replies, it commits the transaction(s). In the case when the client receives between 2 @math +1 to 3 @math messages, a regular consensus algorithm is used. The client is a major player, who is responsible for checking the integrity of replicas.
- @cite argued that current weak synchronous consensus algorithms rely heavily on timing assumptions. They showed that an adversary can attack PBFT, causing it to either stop making any progress for consensus, or significantly slowing the protocol.
- Traditional approaches in activity recognition deals were performed on untrimmed videos. Thus, being able to localize the action in the video stream is important. The candidate temporal or spatio-temporal regions are referred as action proposals. Action proposals don't provide information about the class of an action but provides localize regions which are most likely to contain an action as mentioned in this extensive review @cite paper . Action proposals (or non-action shots) prevent going through an exhaustive search space and helps detect temporal or spatio-temporal locations of actions.
- Paper @cite describes that the models trained on Kinetics dataset serves as a better pre-trained model as compared with the ImageNet model. The incorporate temporal information using 3D models and long temporal information using temporal segment networks. Further, their approach also explored multi-modal cues such as audio cues and body pose estimation. Paper @cite mentions
- In @cite optical flow features between consecutive frames are used to model temporal aspects of activities depicted in the videos. It was shown by @cite optical flow features capture crucial information for the task of activity classification. @cite use the features obtained in the fc6 layer activations in the CNN pre-trained on the UCF101 video dataset to represent the activity for the task of activity classification. Deep convolutional networks can also be employed to capture the complementary information contained in still frames and motion between frames. In order to incorporate spatial and temporal features @cite propose a two-stream ConvNet architecture and demonstrate that a ConvNet trained on multi-frame dense optical flow is able to achieve very good performance despite having limited training data. They also we show that applying multitask learning to two different action classification datasets can be used to increase the amount of training data and improve the performance on both. UCF101 dataset and Thumos dataset built from web videos have become benchmarks for video classification.
- @cite provide an overview of a platform for distributed urban noise measurement, which is part of an ongoing German research project called . A wireless distributed network of audio sensors based on quad-core ARM BCM2837 SoC was employed to receive urban noise signals, pre-process the obtained audio data and send it to a central unit for data storage and performing higher-level audio processing. A final stage of web application was used for visualization and administration of both processed and unprocessed audio data. The authors in @cite used Ameba RTL 8195AM and Ameba 8170AF as IoT platforms to implement a distributed sensing system for visualization of the noise pollution. In @cite , two hardware alternatives, Raspberry Pi platform and Tmote-Invent nodes, were evaluated in terms of their cost and feasibility for analyzing urban noise and measuring the psycho-acoustic metrics according to the Zwicker's annoyance model. In contrast to related work, our approach is not concerned with measuring the noise level in dB using IoT, but with determining the type of noise (for instance, a jackhammer or gun shot).
- Mobile user's capacity is greatly augmented in the era of MEC. As a result, mobile service provisioning is expected to have further improved quality of experience (QoE) @cite . To this end, various mobile edge architectures have been proposed. Tandom proposed to deploy edge resources within radio access networks. They characterized the relationship between latency and caching size, as well as latency and fronthaul capacity, from an information-theoretic perspective @cite . Yang introduced an edge resource provisioning architecture based on cloud radio access network (C-RAN), and devised a cloud-edge interoperation scheme via software defined networking techniques @cite . Tong designed a hierarchical edge architecture, aiming at making efficient use of edge resources when serving the peak loads from mobile users @cite . As the 5G wireless network is expected to incorporate diverse access technologies, in this paper, we consider edge caching in the context of heterogeneous networks. Potential EN deployment can be capacity-augmented base stations, WiFi access points and other devices with excess resources.
- Unfortunately, often times content popularity profile can not only be unknown , but also time-varying. This is because user's interests change constantly, and meanwhile new contents are being created @cite . As a result, learning-based caching algorithms should be designed in an online fashion, i.e., requiring no training phase, and adaptive to popularity fluctuations. To this end, Roy proposed to predict video popularity by utilizing knowledge from the social streams @cite . M "uller introduced context-aware proactive caching @cite . By constructing context space based on user information, they proposed an online algorithm that first learns context-specific user demands, and then updates cached contents accordingly. Other information has also been used for context differentiation, such as content features and system states @cite . The prediction accuracy of those solutions is highly dependent on the information used for context differentiation. To content service providers, however, user information is extremely sensitive and often unavailable. In addition, it is also impossible for them to get detailed system or network information when making caching decisions.
- In this paper, we exploit locational features for context differentiation. Locational information can be easily obtained, for example, users attached to different ENs are naturally divided into geographical groups. Based on which we investigate the location-aware caching problem with unknown and time-varying content popularity profile. By modeling user demand as linear combination of location features and content attributes, our previous work has addressed the content popularity prediction problem with the assumption that the model noise is zero-mean @cite . As an extension, this paper additionally considers the practical scenario, where noise structure is unknown . Specifically, a robust prediction algorithm is proposed with detailed theoretical caching performance analysis. The proposed algorithm is robust and practical as it guarantees prediction accuracy regardless of the noise statistics. Additionally, numerical analysis and comparison on the root causes of estimation errors of both algorithms are presented. Much extensive experiments are conducted to validate the performance of the proposed algorithms.
- It is worth noting that, in the mobile context, fetching content from the edge cache significantly reduces the delay, compared with that from conventional content distribution network (CDN). Moreover, existing content pushing strategies in CDN do not consider the fine-grained popularity differentiation in neighbouring Wi-Fi APs and cellular base stations @cite . With the consideration of location awareness, this paper further models and predicts the dynamics of content popularity, which is constantly varying with time.
- Most related to our work are the papers introducing the corpora that we have analyzed, Euses @cite and Enron @cite . While the paper introducing EUSES describes statistics on charts too the paper on Enron does not. Another spreadsheet corpus is FUSE @cite . This corpus consists of almost 250,000 spreadsheets that were extracted from a public web archive with over 26 billion pages. We preferred the Enron corpus over the FUSE corpus because the Enron spreadsheets were used in an industrial setting.
- Miranda laws @cite @cite @cite and Wadler's views @cite @cite are seminal work. These proposals provide methods to decompose data with multiple representations by explicitly declaring transformations between each representation. These are the earliest studies that allow users to customize the execution process of pattern matching. However, the pattern-matching systems in these proposals treat neither multiple pattern matching results nor non-linear patterns. Also, these studies demand a canonical form for each representation.
- Active patterns @cite @cite provides a method to decompose non-free data. In active patterns, users define a for each pattern to specify how to decompose non-free data. For example, insert for multisets is defined as a match function in @cite . An example of pattern matching against graphs using matching function is also shown in @cite . One limitation of active patterns is that it does not support backtracking in the pattern matching process. In active patterns, the values bound to pattern variables are fixed in order from the left to right of a pattern. Therefore, we cannot write non-linear patterns that requires backtracking such as a pattern that matches with a collection (like sets or multisets) that contains two identical elements. (The pattern matching fails if we unfortunately pick an element that appears more than twice at the first choice.)
- First-class patterns @cite is a sophisticated system that treats patterns as first-class objects. The essence of this study is a that defines how to decompose data with each data constructor. First-class patterns can deal with pattern matching that generates multiple results. To generate multiple results, a pattern function returns a list. A critical limitation of this proposal is that first-class patterns do not support non-linear pattern matching.
- Functional logic programming @cite is an approach towards this integration. It allows both of non-linear patterns and multiple pattern-matching results. The key difference between the functional logic programming and our approach is in the method for defining pattern-matching algorithms. In functional logic programming, we describe the pattern-matching algorithm for each pattern in the logic-programming style. A function that describes such an algorithm is called a . A pattern constructor takes decomposed values as its arguments and returns the target data. On the other hand, in our proposal, pattern constructors are defined in the functional-programming style: pattern constructors take a target datum as an argument and returns the decomposed values. This enables direct description of algorithms.
- In this work, we use Learning from Demonstration framework to train an agent to learn the task of driving using high-dimensional observations in form of images. Learning from Demonstration along with deep function approximators have been used to tackle a lot of problems in robotics like indoor mobile robot navigation , quad-rotor control in forest trials , robot-arm manipulation among others. The closest to our work are the works of @cite who show autonomous lane following using a single trained network, @cite who demonstrate autonomous driving in CARLA using an additional conditional input from a high-level planner, @cite who compare various contemporary networks for autonomous driving tasks and @cite who demonstrate multi-task and multi-modal behavior for autonomous driving.
- Multi-task learning (MTL) research shows the joint training of auxiliary related side-tasks along with the main task enhances the training performance . MTL in neural networks has been successfully demonstrated in many tasks previously including text-to-speech conversion , natural language processing , speech processing and computer vision . In the field of sequential decision making, @cite demonstrate MTL for 3D game playing, @cite and @cite demonstrate MTL in 3D maze navigation task whereas @cite utilize the MTL framework for autonomous driving. Instead of employing future control outputs as auxiliary tasks as shown by @cite , in this work we employ action and visual abstractions to guide the driving behavior.
- Supervised learning of visual affordances for autonomous driving was introduced by @cite , though they use the predicted affordances to plan using a set of fixed rules whereas our network uses visual affordances as auxiliary tasks for the main task of driving. Action primitives can be inferred as sub-policies for the desired task. Learning hierarchical policies via demonstration is an active area of research and research in developmental psychology has also found evidence of hierarchical task decomposition during imitation in young children . Our work decomposes the main task of driving into sub-policies which are used as auxiliary supervision to derive the final control commands.
- Our definition of a scaffold task includes stand-alone methods for estimating word embeddings @cite @cite @cite . After training word embeddings, the tasks implied by models like the skip-gram or ELMo's language model become irrelevant to the downstream use of the embeddings. A noteworthy difference is that, rather than pre-training, a scaffold is integrated directly into the training of @math through a multitask objective.
- Neural architectures have often yielded performance gains when trained for multiple tasks together @cite @cite @cite @cite . In particular, performance of semantic role labeling tasks improves when done jointly with other semantic tasks @cite @cite @cite . Contemporaneously with this work, proposed a multitask learning setting for universal syntactic dependencies and UCCA semantics @cite . Syntactic scaffolds focus on a primary semantic task, treating syntax as an auxillary, eventually forgettable prediction task.
- There is much recent interest in applying artificial intelligence and machine learning techniques to a variety of software engineering tasks; see @cite for a comprehensive review. The two papers closest to our work are @cite , which attempts to identify vulnerabilities at the function level, and @cite . which attempts to identify vulnerabilities at the line level.
- Our work, in contrast, is more modest: we attempt to find the minimal code complexity necessary to break the state of the art AI system @cite . In doing so, we find that a memory network can learn how to identify buffer overflows in synthetic code, but the it appears as though the memory network needs a nearly exhaustive amount of training data in order to do so.
- There are other works that employ various types of turn-taking interaction to learn models for language grounding. Some of these use a restricted vocabulary @cite @cite , or additional knowledge of predicates (for example that red'' is a color) @cite . Others do not use active learning @cite @cite @cite @cite , or do not learn a policy that guides the interaction @cite @cite @cite .
- Also related to our work is the use of reinforcement learning in dialog tasks, such as slot-filling and recommendation @cite @cite , understanding natural language instructions or commands @cite @cite , and open domain conversation @cite @cite . These typically do not use active learning. In our task, the policy needs to trade-off model improvement against task completion.
- We aim to address this issue by transferring features learned by a classification problem to survival analysis without losing performance. Moreover, we propose a method to combine radiomics and learned CNN features that enforce the CNN to learn features that are both discriminative and not covered by the radiomics feature set. All methods are evaluated on a publicly available dataset of computed tomography (CT) images of non-small-cell lung cancer (NSCLC) patients and corresponding survival labels. We show that our method can outperform the previous state-of-the-art presented in @cite .
- Back-translation (BT) is an alternative to leverage monolingual data. BT is simple and easy to apply as it does not require modification to the MT training algorithms. It requires training a target-to-source system in order to generate additional synthetic parallel data from the monolingual target data. This data complements human bitext to train the desired source-to-target system. BT has been applied earlier to phrase-base systems . For these systems, BT has also been successful in leveraging monolingual data for domain adaptation @cite @cite . Recently, BT has been shown beneficial for NMT . It has been found to be particularly useful when parallel data is scarce .
- Conversation dialog has been accumulating in the online communities making the data-driven dialog model possible @cite . In literature, query-response pairs are modeled by seq2seq model with attention mechanism @cite @cite @cite . And the essence of the neural response generation model are designed by maximizing the likelihood of target response given source query in the training procedure. As various response are reasonable to reply to a query, the information included in the query is limited to constrain the model inference, which makes the NRG models prefer universal replies with minimum risk @cite @cite .
- To address this issue, various works are conducted on bringing more information to influence these models. propose to utilize keywords to constrain the topic of responses, and incorporates the replies with topic information. By contrary, some researcher believed in diverse responses are just buried by the greedy beam-search rules @cite , so they focus on involving more punishment or randomness in the inference stages. To illustrate, constrain the search space using mutual information with the query and randomly chose candidate words from top beams to consist short phrases. These existing works mainly focus on the generation strategies during inference, in contrast, the model's architecture and loss function have rarely been explored.
- In addition to the work mentioned in the Introduction, Lakkaraju et. al., ( lakkaraju2017selective ) showed that analyses of recidivism based on human decisions are further complicated by the selective labels'' problem, where observability of outcomes are affected by judges' release decisions. Other work studied how humans perceive different features as fair or not @cite .
- Fully-supervised approaches based on neural networks have achieved impressive results on fine-grained sentiment classification @cite @cite . More recently, (MIL) models have been proposed that use freely available review ratings to train segment-level predictors. and train sentence-level predictors under a MIL objective, while our previous work introduced , a hierarchical model that is trained end-to-end on document labels and produces polarity-based opinion summaries of single reviews. Here, we use to predict the sentiment polarity of individual opinions.
- A few extractive neural models have been recently applied to generic multi-document summarization. train a recursive neural network using a ranking objective to identify salient sentences, while follow-up work @cite employs a multi-task objective to improve sentence extraction, an idea we adapted to our task. propose a graph convolution network to represent sentence relations and estimate sentence salience. Our summarization method is tailored to the opinion extraction task, it identifies aspect-specific and salient units, while minimizing the redundancy of the final summary with a greedy selection algorithm @cite @cite . Redundancy is also addressed in who propose a graph-based framework for abstractive summarization. introduce an encoder-decoder neural method for extractive opinion summarization. Their approach requires direct supervision via gold-standard extractive summaries for training, in contrast to our weakly supervised formulation.
- " @cite . In the past decades, there has been a trend of increasing the context that an LM can condition on. N-gram models @cite assume that each symbol depends on the previous @math symbols. Feed forward neural network based LMs @cite are not count based but they inherit the restrictive assumption. To model longer-term dependencies, RNNLMs @cite are proposed. RNNLMs often achieve smaller PPLs than the N-gram counterparts @cite @cite @cite . This paper focuses on RNNLM-type architectures.
- While these works all adopt PPL as the metric to optimize, sometimes one may optimize a task-specific objective. For example, and propose discriminative LMs to improve speech recognition. The common methodology therein is to fit a probabilistic model, e.g., conditional random field @cite , to the space of text candidates, and maximize the probability at the desired candidate. The problem is often solved by perceptron algorithm. However, these methods all rely on ad-hoc choice of features, e.g., counts of @math -grams where @math varies in a small range (e.g.,1 to 3). Moreover, it is also not clear how these methods would take advantage of an existing language model (trained on large unsupervised corpus). Nevertheless, the same methodology can be extended to RNNLMs, thus avoiding the aforementioned limitations. For example, train an RNNLM by favoring sentences with high BLEU scores and integrate it into a phrase-based MT decoder.
- If we cast the problem of picking the best text sequence as a ranking problem, the aforementioned works can be considered as pointwise" learning-to-rank approaches @cite . In contrast, the proposed method is a pairwise" approach @cite , as it learns a neural language model by comparison between pairs of sentences. Earlier works in this fashion may date back to @cite , which improves a semantic parser. Learning by pairwise comparison" is also seen in several MT literatures. For example, propose to train a phrase-based MT system by minimizing a pairwise ranking loss. optimize the beam search process in a Neural Machine Translation (NMT) system. They enforce the score of a reference to be higher than that of its decoded k-th candidate by at least a unit margin.
- There is a wide variety of approaches to sentence-level representations that can be used in natural language inference. @cite and @cite explore RNN and LSTM architectures, @cite convolutional neural networks and @cite GRUs, to name a few. The basic idea behind these approaches is to encode the premise and hypothesis sentences separately and then combine those using a neural network classifier.
- @cite explore multiple different sentence embedding architectures ranging from LSTM, BiLSTM and intra-attention to convolution neural networks and the performance of these architectures on NLI tasks. They show that, out of these models, BiLSTM with max pooling achieves the strongest results not only in NLI but also in many other NLP tasks requiring sentence level meaning representations. They also show that their model trained on NLI data achieves strong performance on various transfer learning tasks.
- Supervised learning with deep neural networks in the framework of encoder-decoder models has become the state-of-the-art methods for approaching NLP problems @cite . Recent text generation models use a wide variety of GANs such as gradient policy based sequence generation framework @cite and an actor-critic conditional GAN to fill missing text conditioned on surrounding text @cite for performing natural language generation tasks. Other architectures such as those proposed in @cite with RNN and variational autoencoder generator with CNN discriminator and in @cite with leaky discriminator to guide generator through high-level extracted features have also shown great results.
- Our approach is inspired by the recent success of applying deep neural networks to a variety of domains, including image recognition @cite , machine translation @cite , and speech recognition @cite . These techniques are solely focused on prediction, and our method is similar, in that we are focused on training deep networks for the purposes of query prediction. Yet, we differ in that prediction is not the only goal, rather we want to perform learning in a manner that provides the user a fast and low-memory-cost way to visually explore data. The query prediction task at hand can be viewed as a means to realize these goals.
- Our approach to training neural networks for data exploration can be viewed as a form of making deep networks more interpretable. In the visual analytics (VA) literature there has been much recent work devoted to the interpretability of deep networks, namely with respect to the training process, the learned features of the network, the input domain, and the network's output space. @cite visualize convolutional neural networks (CNNs) by clustering activations within each layer given a set of inputs, and visualize prominent flows between clustered activations. Other VA approaches to interpreting CNNs have considered visualizing correlations between classes during the training process @cite , and visualizing per-layer convolution filters and their relationships, in order to understand filters that are important to training @cite . Visualizing and understanding recurrent neural networks (RNNs) has also received much attention, through understanding the training process @cite , as well as understanding hidden states dynamics @cite . All of these approaches seek to provide interpretability for deep neural networks that were never designed to be interpretable. In contrast, our approach into the network, such that the user can take advantage of different aspects of the learned network to help their exploration.
- In this context, our method for learning features of aggregation queries can be viewed as a form of unsupervised learning, where treating query prediction as pretext, the features that we learn along the way can be used for other purposes -- in our case exploratory purposes. This is similar to recent techniques in computer vision that learn features using different forms of self supervision, for instance learning to predict spatial context @cite @cite , temporal context @cite , and perhaps more pertinent to our work, learning to count visual primitives in a scene @cite . These techniques solve certain types of relevant visual tasks that do not require human supervision, but then extract the learned features for supervised learning. Our approach is similar: our training data does not require human intervention, since it is built from existing data cubes techniques, yet the features that we learn from this task can be used to help with visual data exploration.
- We also note that there is some very recent work that seeks to combine databases with neural networks. @cite make the connection between indexing, such as b-trees or hashes, and models, and show that such indexing schemes can be learned using neural networks. Mitzenmacher @cite consider similar learning techniques for Bloom filters. These methods are concerned with using neural networks to speed up computation and minimize memory storage. Although we demonstrate that our method can attain these benefits, the primary focus of our method is in using a neural network as an integral component to visual exploration, i.e. is not trying to predict queries that a database can answer.
- There has been some preceding work in academia related to using functional programming languages for tackling machine learning tasks. About a decade ago, Allison explored using Haskell for defining various machine learning and statistical learning models @cite . Yet, that work was of a theoretical nature. Going back even further, Yu and Clack presented a system for polymorphic genetic programming in Haskell @cite . Likewise, this was from a theoretical perspective. More recently, Sher @cite did extensive work on modeling evolutionary computations. A central part of his contribution is an ANN implemented in Erlang. However, his fairly complex system could only have been used as the starting point of our work with substantial modifications. One key difference is that individual nodes of the ANN are modeled as independent processes, and so are sensors and actuators. A related ANN implementation in Erlang is yanni , The corresponding code repository is located at https: bitbucket.org nato yanni (accessed on August 6, 2018). which follows Sher's approach of using message passing, albeit only between layers.
- Counterfactual reasoning and estimation systems @cite @cite @cite answer questions like "how would the system performance had been changed if the modification had been applied to the system during log collection?". The idea for these systems is to use online event logs to train models and predict KPIs for new modifications.
- Our focus in this paper is to propose efficient counterfactual estimator for practical tuning problems within the context of Sponsored Search System. In particular, we target on tuning scenarios where @math testing and existing observational approaches can not work efficiently. We split the causal graph for the sponsored search problem into input and system layer and apply different methodologies to predict outcomes. While machine learning methods gives very good results for predicting user click behaviors @cite @cite @cite , we used log replay based to estimate the outcome of the sponsored search system. The reader could think of applying similar approaches for policy optimization problem when training data is very noisy due to frequent system changes or when it is not practical to create online experiment with the proposed modification.
- There is previous work regarding rating the academic paper @cite , while this work is about rating the news comments, which is different from them.
- Remote control may be accomplished through the use of a wide variety of interfaces @cite . These have changed considerably with the advent of Human-Centered Design (HCD) and have evolved from joystick-and-button black boxes to force-feedback devices @cite , smart gloves @cite , body-suits @cite , verbal instructions @cite , and vision-based analysis of natural body motions @cite @cite @cite @cite . While attractive, an immediate complication in using these methods is obtaining and learning to use the required equipment. In addition, mixed results from immersive teleoperation have been reported, resulting in efforts to reduce the cognitive load of the operators @cite . In @cite a clear trade-off was detected: greater control and situational awareness came at the cost of possible cognitive overload and impaired performance.
- On the other hand, there has been a lot of effort in developing methods for reducing the cognitive load on the user in 2D-GUIs. In @cite , simple constraining of motions was effective in offloading some of the cognitive effort. The familiarity and widespread use of smartphones and tablets has also brought attention to touch-based control. In @cite , a simple touch-based interface was aimed at reducing operator fatigue.
- Interfaces based on Monitor, Mouse, and Keyboard are a simple and cheap alternative that has been applied to a wide variety of robotics applications like robot navigation @cite , grasping @cite @cite @cite , or object manipulation @cite @cite .
- Work on human perception points to the fact that we process 3D objects as arrangements of 2D views @cite @cite . In addition, several tasks involving shape understanding only need 2D views, like recognition and detection @cite .
- Display type may play a part on the ability to perform different tasks. In the work done by St @cite a distinction is made between scene understanding and specific tasks involving the precise judgment of relative position. They compared pure 2D interface without projective effects to 3D displays: 2D views of 3D scenes with perspective effects. They reason that, while 3D displays seem compelling, integrated and natural, they can cause ambiguity or distortion by the nature of their presentation in a 2D display. In particular, for tasks requiring the precise relative position of two objects, 2D views proved superior.
- Informative can be obtained from users in several ways. One possibility is to add tags and qualifiers to a dataset after it has been acquired. This can range from intensive human labeling in post-production to methods that require minimal markings. Full image segments are drawn by humans and used as ground truth in @cite , whereas in @cite , the segmentation algorithm needs only an initial seed point provided by a human. In 3D environments, human-segmented data is often used for training and validation @cite . Simple hints have also been used in these environments to seed automatic segmentation algorithms @cite . Annotations can also be used for tracking objects in 3D from an initial seed labeling @cite . In @cite , an annotated dataset is constructed for robot navigation.
- Learning from Demonstration (LfD) offers an alternative to the active addition of annotations in post-processing. In LfD, the parameters of actuation are implicitly refined through the observation of examples provided by humans. This happens because under this strategy, perception-action systems are built specifically to follow human actions and replicate them within their own circumstances (like different kinematic structures or safety constraints). See @cite for a thorough review.
- A third approach is , where an operator indicates appropriate motions by physically guiding the robot while it records the event @cite . Alternatively, the robot may be guided virtually, aided with a variety of feedback options @cite @cite .
- In one sense, annotation can be obtained without human supervision by automatically processing the data and extracting useful intelligence from the collected information. Metadata can be analyzed as a form of annotation for the payload it describes. Often, the input data itself comes from data sources rich with explicit or implicit human annotation. In @cite , automatic classification methods are trained using existing human-interaction datasets that have been previously grouped by activity. While effective in its own right, this type of unsupervised learning needs to be validated before being put to used in any context where safety, preference, or informed judgment is required. One problem with unintentional annotation is that it is unstructured and noisy by nature, making the extraction of usable intelligence a considerably more difficult proposition @cite . When possible, it is preferable to use annotations that are directly aimed at enhancing the understanding of a situation or task that one intends to refine.
- Annotation of Perceptual Data There are different mediums for annotation that the robot may provide or use as input for operation. When using kinesthetic learning, the system focuses on the recording of changes in its joint space with respect to a given objective @cite . While useful, it is usually more powerful to also have a way of capturing the surrounding environment. A common approach is to use visual perception of some kind. This is due to the enormous amount of relevant information that may be extracted from this approach. It may capture color, changes in spatial and temporal relations, and importantly, it's a perceptual stream that we as humans generally favor to analyze our environments. Annotation of images has a long tradition in robotics. For traditional image stills or streams of images (2D), large databases have been constructed for segmentation @cite and object recognition @cite @cite . More recently, stereo vision and depth sensors have extended the perceptual stream to 3D. In this realm, several annotated databases have been constructed @cite @cite . The volumetric and color information has been used for object recognition @cite , tracking @cite , or shape reconstruction @cite .
- Off-the-shelf sensors like the Microsoft Kinect give easy access to these input streams, and have made the 3D scanning of environments and objects ubiquitous. Several algorithms have been developed and refined to allow the integration of depth sensing onto unified models of the scenes they capture @cite . The Kinect Fusion algorithm @cite , and its many variants @cite , allow the tracking of a scene and integration of captured depth-frames into a coherent volumetric representation. Some variants have even accomplished deformation and part-tracking @cite . One important aspect of the models that these approaches create is their sensitivity to the sensor and the context. The Kinect has a resolution of about 2 mm and has trouble capturing thin surfaces, transparent or shiny objects and requires careful scanning or large amounts of computation for dealing with noise. While various techniques exist to deal with these problems, when these appear in conjunction to context-dependent constraints, human annotations become highly valuable. For an in-depth review of automatic and semi-automatic methods of reconstruction from point clouds, see @cite .
- Annotations that help solve particular instances of problems can be gathered into a knowledge-base that can, in turn, be studied to gain insights into the solving of more general situations. One way to gather a large amount of usable annotations is to take advantage of the growing infrastructure for tasks, like Amazon's Mechanical Turk @cite .
- One important aspect of GUI design is to consider the user and the objectives. Several studies have looked at what humans can and can't do in these robotics tasks, what interaction exists between an individual's abilities, and what the interface can do @cite @cite . In addition, studies involving tasks in 3D space usually consider human natural abilities or previous experience in similar environments @cite . Indeed, studies suggest that spatial reasoning ability not only varies between individuals, but that different tasks might require different abilities within the realm of spatial reasoning @cite @cite . One common precept in HCD is to avoid visual clutter in order to lower cognitive load. Studies have found that novice users get better results on interfaces that hide extraneous features @cite , and where they only focus on the high-level tasks while the robot controls the low level details @cite .
- Object reconstruction refers to the recovery of a real object's structure in digital form from some stream of perceptual data. Significant progress has been made in automatic reconstruction. One of the general methods consists of using multiple 2D views of an object to obtain its shape @cite @cite . While this approach allows the use of monocular cameras, depth-annotated images have become easily accessible and allow for accurate pose estimation and refinement of models @cite . The work of @cite used the Microsoft Kinect sensor to integrate depth-augmented images into a single volumetric model of the scene. This approach was very successful and has been used and refined extensively @cite @cite @cite .
- The main problem with automatic reconstruction is its sensitivity to input resolution and any noise or artifacts the sensor might cause. A well known problem has to do with the natural difficulties in dealing with thin structures, noisy scans, or shiny or transparent surfaces @cite . Several algorithms attempt to combat these problems by imposing correction mechanisms to preserve shape or inferred structure @cite @cite @cite .
- A third option is semi-automatic reconstruction. Here, the idea is to use human cues to seed or refine automatic methods. These can work on 2D images, like the work of @cite , where simple human marks on 2D images may result in the creation of 3D objects. The work of @cite integrates simple annotations on 2D images with 3D object models extracted from a repository to extract complex objects from 2D scenes. Another approach is to work directly with 3D scans of a scene. This can either be on the depth-based point clouds or the reconstructed mesh. In @cite , simple cues help snap planar primitives on top of point cloud sections in order to build simple architectural models. In the work of @cite , annotations are made on reconstructed 3D models of the whole environment to generate segmented 3D scenes. In the work of @cite , topology-aware reconstruction is first attempted automatically, and then iteratively corrected with human input. A refined interface showing a similar approach is shown in the work by @cite .
- One important distinction between methods is the form these annotations take. In @cite @cite simple lines and sketches are interpreted as instructions and combined with the section of the images they are used on. The results are either 3D models of the 2D-segmented shapes, or a segmented 3D space, where each part of the scene may be labeled with the object categories. Another typical objective is the creation of new object models from existing ones. One approach is to deform them into new shapes. In the work of @cite @cite , cuboid proxies are used to guide shape deformations.
- The idea of using shape proxies for future actions over the base shape is, in a sense, a method of annotation. In the work of @cite , a slice-based proxy was developed for representing 3D meshes. These were placed by humans and later used as training input for an automatic planar-proxy creation algorithm. The potential applications range from printing simplified 3D objects to the annotation and recovery of 3D shapes. In a follow-up work, @cite used these planar proxies, in conjunction with crowdsourcing to study user abilities regarding estimation of surface normals in commonly occurring 3D object models.
- In the work of @cite , a is extracted from 3D object meshes and later used to deform them by performing simple modifications of the underlying proxy. This approach highlights the power of simplified representations for object analysis and modification. The work of @cite shows the use of these simple sweep-based proxies to fit 2D sketches and transform them to 3D objects.
- @cite , developed an annotation scheme that uses simple gestures and the structure of object point clouds to infer a set of underlying part proxies that can be modified to reconstruct the shape. This gesture-based reconstruction approach can help deal with incomplete scans and was designed specifically to offer the best trade-off between user-effort and shape quality. The inference of underlying shape is carried out automatically, with some parameter tweaking done by the operator. One disadvantage is that this automatic reconstruction has trouble dealing with multiple small holes or with topologies that contain cavities, like mugs or bowls. A similar approach has been used to segment meshes into generalized cylinders @cite . Two typical topology-related problems are objects missing cavities, like a mug without a cavity @cite or unexpected topologies like mugs with various holes @cite .
- Some studies avoid using object models and use the perceptual input directly to extract grasp affordances @cite . In @cite , local surface features are used to propose grasps. Some techniques use a multitude of heuristics to generate grasp hypotheses from the sensed point clouds of the scene @cite . These approaches are, in a way, model-less and allow the grasping of objects by their current appearance. An alternative approach uses object models to store and seed the inference of grasps. In the work of @cite @cite , , develop a technique called where low-dimensional grasp subspaces are computed that match the object shape. These in turn may be used for seeding interactive grasping approaches. Some approaches are denominated hybrid, since they use appearance features or 3D-model detection depending on the situation @cite . While automatic grasping is a promising field, context and functional requirements might place additional conditions on the choosing fo a specific grasp.
- A slightly less independent'' procedure is to match and apply grasping templates to the sensed input @cite . For graping based on existing knowledge-bases, one option is to detect known objects followed by executing a specific grasp linked to that particular instance or that category of objects @cite @cite . In @cite , shape primitives are linked to the target object and used to seed appropriate contact-level grasping.
- Databases of grasping examples may be used as a starting point for automatically generating a grasp hypotheses @cite . While these approaches are not fully automatic, they already contain important semantic information that might be germane to a multitude of context-dependent grasping tasks. In @cite @cite , shape and objective constraints are used to specify appropriate grasps.
- Humans are excellent at quickly arriving at functional grasps, even for novel objects. We have the advantage of context and experience to help guide our choice. Whether for direct grasp suggestions, or for creating labeled model data, human annotations have been widely used. Human action may be observed and used under the LfD strategy and then processed to generate robot grasps @cite . Alternatively, human input can serve to label grasping databases or to interactively choose grasps @cite . In summary, a range of automatic and interactive methods exist that can use either raw perception and automation or existing models to obtain grasps. For a survey on data-driven grasp synthesis, see @cite .
- Significant effort has been placed on obtaining a taxonomy of types of grasps and to judge their physical qualities @cite @cite @cite . These studies draw inspiration from early analysis of human grasping @cite or robot hands @cite . One widely used approach for judging grasp quality is that of the metric developed by @cite . It uses the grasp contact points to construct a convex hull over the set of wrenches that can be applied with the given grasp. This is called the Grasp Wrench Space (GWS). Two measurements that can be used are the volume of the computed convex hull and the radius of the maximum sphere contained in it, which we call the epsilon-distance, or @math -dist. These indicate the robustness of a grasp and the versatility of the wrenches applicable for the given grasp. One popular tool used in the literature to evaluate grasp quality and generate grasp candidates is GraspIt! @cite . It uses the GWS volume and @math metrics to rank grasps that have been generated using a variety of grasp-planning techniques including the above mentioned .
- While several methods abound that use existing 3D mesh objects or that detect them from 2D-views or point clouds, less effort has been paid to linking grasps to simpler shape representations or proxies like the ones obtained in @cite @cite . As mentioned previously, these proxies are compact versions of objects, and can be used to represent categories as well as instances, and help indicate shape, as well as topology. Linking grasps to these might work as a sort of look-up table for grasps.
- Grasping and manipulation are closely related. For some grasps finding the final contacts of the gripper is insufficient. In several tasks, the approach to accomplish the grasp, as well as the task-dependent manipulation that will be performed affect the generated grasp hypotheses. Some studies have looked at shape category and the physical context to determine manipulation constraints @cite , while others have used human experience to seed approach vectors @cite . In @cite , a grasp planning algorithm is used in conjunction to a set of task-dependent semantic constraints to choose grasps, which in turn may inform the approach vector. These semantic constraints are extracted from example grasps and then used to construct a semantic affordance map which directly relates the object class (obtained from object depth information) to the approach vectors and different task-appropriate grasps. This provides evidence that relating object instance or category models to manipulation annotations is of great utility.
- One popular tool for motion planning is Moveit! @cite . This tool can be used within the larger Robot Operating System (ROS) to perform automatic motion planning with collision avoidance. This may also be used in conjunction with a manipulation interface that humans can use to indicate motions. In the work by , @cite , this interface was used to analyze manipulation strategies for indicating grasps and grasp approach methodologies. They compared continuous teleoperation against three other strategies with varying degrees of autonomy. They showed than a combination of manual annotation and automatic processing resulted in the greatest success rate. This is supported by the work done by Hertkorn @cite , where the importance of shared workload is thoroughly analyzed in the context of grasping and manipulation.
- We based our object modeling usability study on similar studies involving interactive shape reconstruction from point clouds @cite @cite @cite . These constitute the state-of-the-art on this type of interactive approaches @cite @cite . In these studies, user interactions with their software are logged and timed, and later analyzed for precision errors. In @cite @cite , an expert user illustrates the possibilities of their interface, while in @cite , five users are tested. Similarly to these studies, we recorded action choices and timing and evaluated shape quality. We also chose to include an expert user to show the possibilities of the interface if enough time is devoted to learning it.
- In the area of object handling, the work by Hertkorn @cite has one study with a similar objective (evaluating the effect a particular grasping assistance technique). In this study, @math participants were were asked to complete @math grasp trials each. Basic shapes were chosen to attempt to eliminate the effects of shape complexity on the interface's effectiveness in assisted grasping. Similarly, we chose to use simple and or familiar basic shapes.
- @cite uses a database of preplanned grasps, as well as an online planner to help a user find appropriate grasps. Time and success rate was computed for five subjects attempting three tasks. While five subjects is a small number, results indicate trends and allow the research to hone in on the appropriate refinements for a more in-depth evaluation.
- The work by @cite is designed to compare different interaction strategies for attaining valid grasps using the PR2 robot. In this study, @math participants were asked to grasp as many objects as possible (between @math and @math ) in three rounds of trials. Participants were shown a tutorial before the start of the trial.
- In the work of @cite , several tasks and trials were tested using crowdsourcing. Here, the annotation tasks were simple in order to allow testing the approach, and later integrated with the ultimate objective of completing pick-and-place tasks. They found that in many cases a single average worker'' produced poor results, but that by cleaning and averaging @math of them, high accuracy was obtained.
- The work by @cite uses a simple protocol that involves input and output surveys, a tutorial, and a challenge section. While they were focused on evaluating different methods of directing robot motion and attention, we used the same protocol structure for our own annotation tests.
- @cite lay the groundwork for by analyzing the relevance of GermaNet synsets to specific domains. @cite build upon this, showing that WSD performance can be improved by performing on a corpus of the same domain as the testing data. @cite present an method based on a published thesaurus, which they use to induce a coarse-grained sense inventory. This separates their method from other related work, which typically uses WordNet as the sense inventory for WSD and .
- @cite present a method for based on a thesaurus constructed from a parsed corpus. This thesaurus is used to induce a word similarity function, which they use to assess the prevalence of each sense of a given target word. They perform both intrinsic and extrinsic evaluations; we compare to their reported results to the extent their experimental setup allows. This method was subsequently applied to a WSD shared task @cite , and to the identification of infrequent word senses @cite . An extended analysis of the method was presented by @cite . @cite adapt this method to Japanese using only the glosses of words, excluding the use of semantic networks such as WordNet.
- @cite present the first method based on automatically learned vector word embeddings. They test their method on English and a private Hindi dataset. This is the most recent work we are aware of which considers the exact same task as we do, in the same setting; given its recency relative to other such works, we consider this to be the state-of-the-art for . We re-implement this method, and compare to it directly in our experimental evaluation.
- As part of one of our methods, we induce vector representations not only of words, but also of individual senses. Our method differs from recent prior work on constructing embeddings using sense information @cite @cite @cite @cite ; instead, we build upon the methods of @cite and @cite . The resulting vectors are easier to create with fewer resources, more interpretable, and easier to extend and compare to other types of vectors such as those we construct in Sections and . We are particularly interested in maintaining the ability to perform semantic comparisons across different languages, as demonstrated by prior work @cite , motivating our decision to work with vectors known to have this property.
- Our methods leverage cross-lingual information and contextually related words. These concepts have previously been used to improve WSD -- @cite @cite @cite @cite , and others -- but our usage of these concepts for is novel.
- Many methods for discovering Markov blankets and causal networks rely on conditional independence tests @cite @cite ---which, by conditioning on the empty set, can also be used to measure association. For discrete data, the state of the art commonly relies on either the @math test or conditional mutual information (CMI) @cite @cite @cite . Both measures, however, have drawbacks. The @math test is biased to independence when only limited data is available, whereas CMI has the opposite problem. In particular, it holds that @math for any @math , even if @math . For both @math and CMI it is necessary to define a significance threshold or cut-off, which can be arbitrary. Our proposed method based on algorithmic independence overcomes these limitations.
- The discovery of Markov blankets is important in two regards. First, it represents the optimal set of variables for feature selection @cite and second, for investigating the local structure around a target it is much faster than discovering the whole Bayesian network @cite . The idea of first discovering the neighbourhood of a node, instead of the full Bayesian network got more common with the grow and shrink (GS) algorithm @cite . It consists of two sub routines. First, it discovers the potential parents and children in a bottom up approach. Then, it finds the spouses based on the parents and children from the previous step.
- To the best of our knowledge, there exists no algorithm that directly discovers the directed, causal Markov blanket, i.e. that can tell apart parents, children and spouses given only the Markov blanket of a target. In , we first discover the Markov blanket, and then orient the edges. To discover the blanket, we build upon and extend the state of the art @cite and @cite algorithms. Both follow the general structure of the GS algorithm, with employing fast symmetry correction to exclude children of children. Zhu and Yang @cite proposed to speed up by pre-filtering based on mutual information, whereas @cite discover Markov blankets based on conditional mutual information. Unlike our approach, these approaches require the user to set a cut-off value and a scaling parameter @math . For an in depth overview of related algorithms, we refer to the following articles @cite @cite .
- Multi-Task Learning (MTL) is a learning paradigm in machine learning and its target is to leverage useful information contained in multiple related tasks to help improve the generalization performance of all the tasks. Consider the wave propagation problem, we can formulize it as finding the distribution @math , where @math denotes the state of all nodes at time @math . The assumption is that the state of each node at time @math is independent of other nodes at time @math given the states of its neighbors at time @math . So the distribution can be decomposed into @math (assume we have @math nodes in total) conditional distributions. Rather than using @math neural networks to approximate those distributions, MTL uses one neural network with @math outputs. The first few hidden layers are shared among nodes, while the following layers are node-specific. @cite shows that such setting greatly reduces the risk of overfitting. This makes sense intuitively: The more factors we are learning simultaneously, the more our model has to find a representation that captures all of the factors and the less is the chance of overfitting. We implement a MTL model as the opponent of our distribution-based penalization model.
- Finally, we refer readers to several recent papers for other algorithms for weakly convex problems @cite @cite . For example, Drusvyatskiy and Paquette @cite studied a subclass of weakly convex problems whose objective consists of a composition of a convex function and a smooth map, and proposed a prox-linear method that could enjoy a lower iteration complexity than @math by smoothing the objective of each subproblem. Davis and Drusvyatskiy @cite studied a more general algorithm that successively minimizes a proximal regularized stochastic model of the objective function. When the objective function is smooth and has a finite form, variance-reduction based methods are also studied @cite @cite @cite @cite @cite , which have provable faster convergence than in terms of @math . However, in all of these studies the convergence is provided on an impractical solution, which is either a solution that gives the minimum value of the (proximal) subgradient's norm @cite or on a uniformly sampled solution from all iterations @cite @cite @cite @cite .
- Anomaly and novelty detection. Also known as out-of-distribution detection, it aims at identifying inputs that are completely different from or unknown to the original data distribution used for training @cite . @cite , they perform novelty detection by learning a distance in an embedding. It proposes a Kernel Null Foley-Sammon transform that aims at projecting all the samples of each in-distribution class into a single point in a certain space. Consequently, novelty detection can be performed by thresholding the distance of a test sample to the nearest of the collapsed class representations. However, they employ handcrafted features, thus optimizing only the transform parameters and not the representation, like in the presently dominating paradigm of deep learning.
- Metric Learning. Several computer vision tasks such as retrieval, matching, verification, even multi-class classification, share the need of being able to measure the similarity between pairs of images. Deriving such a measure from data samples is known as metric learning @cite . Two often cited seminal works on this subject through neural networks are @cite @cite , where the Siamese architecture was proposed for this purpose. Differently from classification networks, the goal is to learn rather than a representation amenable for classification, one for measuring how similar two instances are in terms of the Euclidean distance. Another popular architecture is triplet networks @cite . For both of them many authors have realized that mining the samples of the training set in order to find out or challenging pairs or triplets is important in order to converge faster or to better minima @cite @cite @cite . Like them, we have also resorted to a mining strategy in order to obtain good results in the task of out-of-distribution detection.
- 6D pose estimation using only RGB information has been widely studied @cite @cite @cite @cite . Since this work concentrates on using point cloud inputs, which contain depth information, we mainly review works that also consider depth information. We also review how depth information can be represented.
- Voting-based methods attempt to infer the pose of an object by accumulating evidence from local or global features of image patches. One example is the Latent-Class Hough Forest @cite @cite which adapts the template feature from @cite for generating training data. During inference stage, a random set of patches is sampled from the input image. The patches are used in Hough voting to obtain pose hypotheses for verification.
- 3D object coordinates and object instance probabilities are learned using a Decision Forest in @cite . The 6D pose estimation is then formulated as an energy optimization problem which compares synthetic images rendered with the estimated pose with observed depth values. 3D object coordinates are also used in @cite @cite . However, those approaches tend to be very computationally intensive due to generation and verification of hypotheses @cite .
- Most recent approaches rely on convolutional neural networks (CNNs). @cite , the work in @cite is extended by adding a CNN to describe the posterior density of an object pose. A combination of using a CNN for object segmentation and geometry-based pose estimation is proposed in @cite . PoseCNN @cite uses a similar two-stage network, in which the first stage extracts feature maps from RGB input and the second stage uses the generated maps for object segmentation, 3D translation estimation and 3D rotation regression in quaternion format. Depth data and ICP are used for pose refinement. @cite propose a three-stage, instance-aware approach for 6D object pose estimation. An instance segmentation network is first applied, followed by an encoder-decoder network which estimates the 3D object coordinates for each segment. The 6D pose is recovered with a geometric pose optimization step similar to @cite . The approaches @cite @cite @cite do not directly use CNN to predict the pose. Instead, they provide segmentation and other intermediate information, which are used to infer the object pose.
- Depth information in deep learning systems can be represented with, e.g., voxel grids @cite @cite , truncated signed distance functions (TSDF) @cite , or point clouds @cite . Voxel grids are simple to generate and use. Because of their regular grid structure, voxel grids can be directly used as inputs to 3D CNNs. However, voxel grids are inefficient since they also have to explicitly represent empty space. They also suffer from discretization artifacts. TSDF tries to alleviate these problems by storing the shortest distance to the surface represented in each voxel. This allows a more faithful representation of the 3D information. In comparison to other depth data representations, a point cloud has a simple representation without redundancy, yet contains rich geometric information. Recently, PointNet @cite has allowed to use raw point clouds directly as an input of a CNN.
- Due to the high level of inaccuracies present in feature extraction and matching, such algorithms must compute the fundamental matrix or homography in a RANSAC loop. While feature-based methods have achieved accurate results, they remain computationally wasteful due to their reliance on RANSAC for robust estimation of such parameters. Several examples of such systems that use indirect methods are ORB-SLAM, ORB-SLAM2 @cite @cite and Parallel Tracking and Mapping (PTAM) @cite . Alternatively, direct methods directly use the sensor inputs, such as image intensities, to optimize an error function to determine relative camera pose.
- There have been many iterations of direct dense methods such as direct dense VO in @cite , RGB-D SLAM @cite , and LSD-SLAM @cite . Even using dense methods, these systems achieve real-time performance on modern CPUs due to the highly efficient nature of these types of algorithms. More recent advances highlight the fact that the information contained in image intensities are highly redundant, and attempt to minimize the photometric error only over sparse random points in the image in order to increase efficiency and thus speed @cite . Another direct method that has been used with success is the iterative closest point (ICP) algorithm, which is used in systems such as @cite @cite . These systems minimize the difference between point alignment in contrast to image intensities.
- The extension of direct methods using edge pixels is a logical direction, yet to the best of our knowledge no work has solely used edge pixels in a direct method minimizing the photometric error. @cite the authors reduce a Euclidean geometric error using the distance transform on edges which does not utilize all information available in the scene. @cite the authors minimize a joint error function combining photometric error over all pixels along with geometric error over edge pixels. Minimizing a joint error function always suffers from the decision on how best to weight each function, and the weighting can have significant effect on the final converged solution. @cite , the authors threshold by gradients, which does not guarantee edges due to noise. They additionally select texture-less regions as well.
- Any system that extracts edges must choose between several edge extraction algorithms. The most prominent are Canny edges @cite , followed by edges extracted from Laplacian of Gaussian (LoG) filters which are efficiently implemented using Difference of Gaussians (DoG). Another type of edge that is not as popular but is very simple are Sobel edges. More recently, there has been research involving the learning of edge features. In @cite the authors utilize structured forests, and in @cite the authors utilize deep learning. Instead of selecting one, we test various edge extraction algorithms with our system select the optimal edge extraction algorithm. Note that @cite requires the use of a GPU and is far from real-time, so we do not consider this method.
- @cite propose an approach that is based on Bayesian networks for automatic generation of residential building layouts in the context of computer graphics applications (such as, computer games). Authors define a cost function that aims at avoiding layout anomalies, such as, ill-formed rooms or incompatibilities between floors.
- @cite surveyed various methods (such as, Simulated Annealing) for decentralized scheduling in Grid computing environments. Grid scheduling involves mapping of a collection of tasks to resources with the aim of minimizing the total execution time of all considered tasks. Compared to a game theory scheduling method, the average scheduling time per task of Simulated Annealing was lower.
- @cite @cite use Simulated Annealing for optimization of DNA sequence analysis on heterogeneous computing systems that comprise a host with multi-core processors and one or more many-core devices. The optimization procedure aims at determining the number of threads, thread affinities, and DNA sequence fractions for host and device, such that the overall execution time of DNA sequence analysis is minimized.
- Drexl and Nikulin @cite use Pareto Simulated Annealing for solving the gate assignment problem in the context of an airport. Various aspects of airport operation are considered, such as, the total passenger walking distance, open flights, connection time, and gate assignment preferences.
- @cite propose to use Simulated Annealing for solving the hybrid vehicle routing problem. The aim is to minimize the total travel cost for hybrid vehicles that use fuel and electricity while considering the time limit, electric capacity, fuel capacity, locations of fuel stations and electricity charging stations.
- Some review articles focused on specific aspects of VLC such as VLC channel modeling methods @cite , noise optical sources and noise mitigation mechanisms @cite , VLC-based positioning techniques for indoor and outdoor applications @cite , and the pertinent issues associated with the outdoor usage of VLC in vehicular communication @cite . They generally identified emerging challenges and proposed future research directions.
- This paper provides, in Section II, an overview of VLC technology, defines and discusses the objectives and constraints that must be taken into account when optimizing VLC networks. Special emphasis is placed on channel capacity derivations, and the unique properties of VLC. We also discuss the variables, parameters, and constraints having an impact on the performance of VLC networks. All optimization techniques are reviewed in Section III, including power and resource allocation, users-to-APs association, cell formation, and AP cooperation used for mitigating the disadvantages of VLC networks to improve performance. This important topic was previously investigated by Li @cite . However, their study was focused on the difference between user-centric and network-centric cell formations, and the interference reduction techniques, whereas in this paper, we place our attention on the techniques, used in RF VLC and in VLC standalone networks, that are aimed at alleviating the limitations in VLC networks. In other words, we show how to formulate optimization problems, what are the techniques used for solving these optimization problems, how the different objectives, limitations, constraints are evaluated, and how added RF APs can remove stand-alone VLC network limitations.
- Finally, Sciancalepore propose a different authorization framework for the IoT @cite , also based on OAuth 2.0 and other standard protocols. In particular, it provides access control through an intermediary gateway acting as mediator between IoT networks and non-constrained Internet segments. However, unlike the ACE framework, @cite displays a considerably higher level of complexity and requires the intermediary gateway to be fully trusted.
- The game @cite has been a testbed for various vision-and-language tasks over the past years, including object retrieval @cite @cite @cite @cite @cite @cite , semantic image segmentation @cite @cite , and generating referring descriptions @cite @cite @cite . To tackle object retrieval, @cite @cite @cite extract additional visual features such as relative object locations and @cite @cite use reinforcement learning to iteratively train the object retrieval and description generation models. Closer to our work, @cite @cite use the full image and the object crop to locate the correct object. While some previous work relies on task-specific modules @cite @cite , our approach is general and can be easily extended to other vision-and-language tasks.
- The game @cite can be seen as a dialogue version of the game, one which additionally draws on visual question answering ability. @cite @cite @cite make headway on the dialogue generation task via reinforcement learning. However, these approaches are bottlenecked by the accuracy of Oracle and Guesser models, despite existing modeling advances @cite @cite ; accurate Oracle and Guesser models are crucial for providing a meaningful learning signal for dialogue generation models, so we believe the architecture will facilitate high quality dialogue generation as well.
- A special case of Feature-wise Linear Modulation was first successfully applied to image style transfer @cite , whose approach modulates image features according to some image style (i.e. @, cubism or impressionism). @cite extended this approach to vision-and-language tasks, injecting FiLM-like layers along the entire visual pipeline of a pre-trained ResNet. @cite demonstrates that a convolutional network with FiLM layers achieves strong performance on CLEVR @cite , a task that focuses on answering reasoning-oriented, multi-step questions about synthetic images. Subsequent work has demonstrated that FiLM and variants thereof are effective for video object segmentation where the conditioning input is the first image's segmentation (instead of language) @cite and language-guided image segmentation @cite . Even more broadly, @cite overviews the strength of FiLM-related methods across machine learning domains, ranging from reinforcement learning to generative modeling to domain adaptation.
- One task that is similar to live comment generation is image caption generation, which is an area that has been studied for a long time. tried to generate descriptions of an image by retrieving from a big sentence pool. proposed to generate descriptions based on the parsing result of the image with a simple language model. These systems are often applied in a pipeline fashion, and the generated description is not creative. More recent work is to use stepwise merging network to improve the performance @cite .
- We cast this problem as a natural language generation problem, and we are also inspired by the recent related work of natural language generation models with the text inputs @cite @cite @cite @cite .
- One of the main heuristic tools in Proof Planning is Rippling @cite . Rippling gives a direction to the rewriting process. The idea is based on the observation that throughout the process, on each side of the equality being proved, there is an unchanging part (skeleton) and a changing part (wave-front). In principle the proof is guided towards rewrite rules that move the wave-front upwards in the syntax tree of the term. Rippling has proved to be a powerful heuristic for inductive proofs. However, there is still a possibility that the proof will block, thus requiring a patch'' or critic''. Critics include lemma speculations and generalizations, some of which can be produced automatically in the modern Proof Planning systems.
- Perhaps it is precisely because music is so often perceived as a profoundly human endeavour that there has also been, in parallel, an ongoing fascination with automating its creation. This fascination long predates notions such as the Turing test (ostensibly for discriminating automation of the most human behaviour), and has spawned a range of efforts: from attempts at the formalization of unambiguously strict rules of composition to incorporation of complete random chance into scores and performances. The use of rules exemplifies the algorithmic (and largely deterministic) approach to music generation, one that is interesting and outside the scope of the current work; for background on this we refer the reader, for example, to the text by Nierhaus @cite . Our present work, on the other hand, lies in a part of the spectrum that incorporates probability and sampling.
- Two centuries later, as the foundations of AI were being set, the notion of automatically understanding (and therefore generating) music was among the earliest applications to capture the imagination of researchers, with papers on computational approaches to perception, interpretation and generation of music by Simon, Longuet-Higgins and others @cite @cite @cite @cite @cite . Since then, many interesting efforts were made @cite @cite @cite @cite @cite @cite , and it is clear that in recent years both interest and progress in score generation has continued to advance, e.g. @cite , Boulanger- @cite , @cite , @cite , @cite , Sturm @cite , to name only a few. @cite provide a survey of generative music models that involve machine learning. @cite provide a comprehensive survey and satisfying taxonomy of music generation systems. McDonald @cite gives an overview highlighting some key examples of such work.
- @cite , observe that many previous performance rendering systems often consist of many heuristic rules and tend to be complex. It makes [it] difficult to generate and select the useful rules, or perform the optimization of parameters in the rules.'' They thus present a method that uses Gaussian Processes to achieve this, where some parameters can be learned. In their ostensibly simpler system, for each single note, three outputs and corresponding thirteen input features are defined, and three functions each of which returns one of three outputs and receive the thirteen input features, are independently learned''. However, some of these features, too, depend on certain information, e.g. they compute the differences between successive pitches, and this only works in compositions where the voice leading is absolutely clear; in the majority of classical piano repertoire, this is not the case. In Laminae @cite , systematize a set of context-dependent models, building a decision tree which allows rendering a performance by combining contextual information.
- Moulieras and Pachet @cite use a maximum entropy model to generate expressive music, but their focus is again monophonic plus simple harmonic information. They also explicitly assume that musical expression consists in local texture, rather than long-range correlations''. While this is fairly reasonable at this point, and indeed it is hard to say how much long-range correlation is captured by our model, we wished to choose a model which, at least in principle, allowed the possibility of modeling long-range correlation: ultimately, we believe that these correlations are of fundamental importance. Malik and Ek @cite use a neural network to learn to predict the dynamic levels of individual notes while assuming quantized and steady timing.
- In the past, a lot of related researches about augmented reality have been presented. Before IAR, some novel styles of reality have been proposed. For example, Lifton al @cite proposed the dual reality'' system to make the virtual world and the physical world be corresponding to each other. Roo al @cite proposed the one reality'' system, which contained a 6-level mixture of virtual and real contents ranging from purely physical to purely virtual world. But they all describe the mixed reality from the perspective of humans, ignoring the view from the virtual world.
- To make the virtual world intelligent, Taylor al @cite discussed the possibility of making a virtual world evolve by itself. The evolution of the virtual world took advantage of the principle of biological evolution in the physical world. Though the self-learning is not simple, there are still many learning frameworks that can be used to obtain the self-learning ability, such as evolutionary computation @cite , reinforcement learning @cite and deep learning @cite .
- In @cite @cite , the secret is one bit. In @cite , secrets of length equal to a fraction of @math (number of players) is considered. This time binary secret sharing with adaptive and non-adaptive adversaries similar to the model we consider in this work is defined. However the paper considers only a privacy threshold @math , and reconstruction is from the full share set ( @math always). Their goal is to achieve large secrets @math over binary shares with large privacy parameter @math , which is also similar to ours. They have an additional goal of keeping the computational complexity of the reconstruction algorithm within AC @math , which we do not consider in this work. Their large privacy parameter @math is with a @math much smaller than @math , which means that the relative threshold gap @math can not be arbitrarily small.
- The notion of secrecy in wiretap codes has evolved over years. More recently the notion of semantic security for wiretap model has been introduced @cite , which allows arbitrary message distribution and is shown to be equivalent to negligible leakage with respect to statistical distance. There remains one last distinction between semantically secure wiretap model and secret sharing with fixed share size. That is the nature of the main and wiretapper channels are typically stochastic (e.g., the erasure channel with random i.i.d. erasures), whereas for secret sharing a worst-case guarantee for the erasure patterns is required. Namely, in secret sharing, reconstruction with overwhelming probability is required for every choice of @math or more shares, and privacy of the secret is required for every (adaptive or non-adaptive) choice of the @math shares observed by the adversary.
- To the best of our knowledge, our work is the first to examine the impact of numeric representations on the accuracy-efficiency trade-offs on large-scale, deployed DNNs with over half a million neurons (GoogLeNet, VGG, AlexNet), whereas prior work has only reported results on much smaller networks such as CIFARNET and LeNet-5 @cite @cite @cite @cite @cite @cite . Many of these works focused on fixed-point computation due to the fixed-point representation working well on small-scale neural networks. We find very different conclusions when considering production-ready DNNs.
- Generally, Domain Adaptation involves two different types of datasets, one from a source domain and the other from a target domain. The source domain typically contains a sufficient amount of annotated data such that a model can be efficiently built, while there is often little or no labeled data in the target domain. Domain adaptation for NLG have been less studied despite its important role in developing multi-domain SDS. proposed a SPoT-based generator to address domain adaptation problems. Subsequently, a system focused on tailoring user preferences @cite , and controlling user perceptions of linguistic style @cite . Moreover, a phrase-based statistical generator @cite using graphical models and active learning, and a multi-domain procedure @cite via data counterfeiting and discriminative training.
- Neural variational framework for generative models of text have been studied longitudinally. proposed a recurrent latent variable model VRNN for sequential data by integrating latent random variables into hidden state of a RNN model. A hierarchical multi scale recurrent neural networks was proposed to learn both hierarchical and temporal representation @cite . introduced a variational neural machine translation that incorporated a continuous latent variable to model underlying semantics of sentence pairs. presented a variational autoencoder for unsupervised generative language model.
- Adversarial adaptation methods have shown promising improvement in many machine learning applications despite the presence of domain shift or dataset bias, which reduce the difference between the training and test domain distributions, and thus improve generalization performance. proposed an improved unsupervised domain adaptation method to learn a discriminative mapping of target images to the source feature space by fooling a domain discriminator that tries to differentiate the encoded target images from source examples. We borrowed the idea of @cite , where a domain-adversarial neural network are proposed to learn features that are discriminative for the main learning task on the source domain, and indiscriminate with respect to the shift between domains.
- The attention mechanisms @cite @cite are originally proposed for solving language-related tasks @cite . Xu @cite introduce an attention mechanism for image captioning, which shows that the attention maps could be adaptively generated for predicting captioning words. Based on @cite , Yang @cite propose to stack multiple attention layers so that each layer can focus on different regions adaptively. In @cite , a co-attention mechanism is proposed. The model generates question attention and spatial attention masks so that salient words and regions could be jointly selected for more effective feature fusion. Similarly, Lu @cite employ a co-attention mechanism to simultaneously learn free-form and detection-based image regions related to the input question. In MCB @cite , MLB @cite , and MUTAN @cite , attention mechanisms are adopted to partially recover the spatial information from the input image. Question-guided attention methods @cite @cite are proposed to generate attention maps from the question.
- Network parameters could be dynamically predicted across different modalities. Our approach is mostly related to methods in this direction. In @cite , language are used to predict parameters of a fully-connected (FC) layer for learning visual features. However, the predicted fully-connected layer cannot capture spatial information of the image. To avoid introducing too many parameters, they predict only a small portion of parameters using a hashing function. However, this strategy introduces redundancy because the FC parameters only contain a small amount of training parameters. In @cite , language is used to modulate the mean and variance parameters of the Batch Normalization layers in the visual CNN. However, learning the interactions between two modalities by predicting the BN parameters has limited learning capacity. We conduct comparisons with @cite and @cite . Our proposed method shows favorable performance. We notice that @cite use language-guided convolution for object tracking. However, they predict all the parameters which is difficult to train.
- Recent research found that the combination of depth-wise convolution and channel shuffle with group convolution could reduce the number of parameters in CNN without hindering the final performance. Motivated by Xception @cite , ResNeXt @cite , and ShuffleNet @cite , we decompose the visual CNN kernels into several groups. By shuffling parameters among different groups, our model can reduce the number of predicted parameters and improve the answering accuracy simultaneously. Note that for existing CNN methods with group convolution, the convolutional parameters are solely learned via back-propagation. In contrast, our QGHC consists of question-dependent kernels that are predicted based on language features and question-independent kernels that are freely updated.
- Multi-label Classification Multi-label classification is relevant in many application domains, where each data instance can be assigned into multiple classes. Many multi-label learning works developed in the literature have centered on exploiting the correlation interdependency information between the multiple labels, including the max-margin learning methods with pairwise ranking loss @cite , weighted approximate pairwise ranking loss (WARP) @cite , and calibrated separation ranking loss (CSRL) @cite . Moreover, incomplete labels are frequently encountered in many multi-label applications due to noise or crowd-sourcing, where only a subset of true labels are provided on some training instances. Multi-label learning methods with missing labels have largely depended on observed label correlations to overcome the label incompleteness of the training data @cite @cite @cite . These methods however assumed that all the labels are at least observed on a subset of training data and they cannot handle the more challenging zero-shot learning setting where some labels are completely missing from the training instances.
- The most used data augmentation method for TSC is the slicing window technique, originally introduced for deep CNNs in @cite . The method was originally inspired by the image cropping technique for data augmentation in computer vision tasks @cite . This data transformation technique can, to a certain degree, guarantee that the cropped image still holds the same information as the original image. On the other hand, for time series data, one cannot make sure that the discriminative information has not been lost when a certain region of the time series is cropped. Nevertheless, this method was used in several TSC problems, such as in @cite where it improved the Support Vector Machines accuracy for classifying electroencephalographic time series. @cite , this slicing window technique was also adopted to improve the CNNs' mortgage delinquency prediction using customers' historical transactional data. In addition to the slicing window technique, jittering, scaling, warping and permutation were proposed in @cite as generic time series data augmentation approaches. The authors in @cite proposed a novel data augmentation method specific to wearable sensor time series data that rotates the trajectory of a person's arm around an axis (e.g. @math axis).
- @cite , the authors proposed to extend the slicing window technique with a warping window that generates synthetic time series by warping the data through time. This method was used to improve the classification of their deep CNN for TSC, which was also shown to significantly decrease the accuracy of a NN-DTW classifier when compared to our adopted data augmentation algorithm @cite . We should note that the use of a window slicing technique means that the model should classify each subsequence alone and then finally classify the whole time series using a majority voting approach. Alternatively, our method does not crop time series into shorter subsequences which enables the network to learn discriminative properties from the whole time series in an end-to-end manner.
- Polar codes @cite @cite have been selected for the fifth generation (5G) wireless standard. With state-of-the-art code construction techniques @cite @cite @cite and SC-List (SCL) decoding algorithm @cite @cite @cite @cite @cite @cite @cite @cite , Polar codes demonstrate competitive performance over LDPC and Turbo codes in terms of block error rate (BLER). Beyond 5G, ultra-low decoding latency emerges as a key requirement for applications such as autonomous driving and virtual reality. The latency of practical Polar decoders, e.g., an SC-list decoder with list size @math , is relatively long due to the serial processing nature.
- Continuous efforts @cite @cite @cite @cite @cite @cite @cite have been made to significantly reduce decoding latency. Among them, we are particularly interested in hardware implementations, which are dominant in real-world products, due to better power- and area-efficiency. According to our cross-validation, three approaches are shown to be cost-effective, yet incur no or negligible performance loss compared to the original SCL decoder, as summarized below: Pruning on the SC decoding tree @cite (parallelizing constituent code blocks with mult-bit decision) Rate-0 (R0), repetition (Rep) nodes @cite @cite . General (Gen) nodes comprised of consecutive bits @cite @cite . Reduce the number of path splitting Rate-1 (R1), single parity-check (SPC) nodes @cite . Do not split upon the most reliable (good) bits @cite @cite . Reduce the latency of list pruning Adopt bitonic sort @cite for efficient pruning. Quick list pruning @cite .
- Transfer learning enables the use of different domains, tasks, and distributions for training and testing @cite . @cite reviewed the current state-of-the-art transfer learning approaches in BCI. @cite transferred general features via a convolutional network across subjects and experiments. @cite applied kernel principle analysis and transductive parameter transfer to identify the relationships between classifier parameter vectors across subjects. @cite evaluated the transferability between subjects by calculating distance and transferred knowledge in comparable feature spaces to improve accuracy. @cite proposed an approach which can simultaneously transfer knowledge across domains and tasks. @cite and @cite attempted the learning of transferable features by embedding task-specific layers in a reproducing kernel Hilbert space where the mean embeddings of different domain distributions can be explicitly matched.
- Crowd counting is active an area of research with works tackling the three aspects of the problem: counting-by-regression @cite , @cite , @cite , @cite , @cite , density map estimation @cite , @cite , @cite , @cite , @cite and localization @cite , @cite .
- Earlier regression-based approaches mapped global image features or a combination of local patch features to obtain counts @cite , @cite , @cite , @cite . Since these methods only produce counts, they cannot be used for density map estimation or localization. The features were hand-crafted and in some cases multiple features were used @cite , @cite to handle low resolution, perspective distortion and severe occlusion. On the other hand, CNNs inherently learn multiple feature maps automatically, and therefore are now being extensively used for crowd counting and density map estimation.
- For localization in crowded scenes, Rodriguez al @cite use density map as a regularizer during the detection. They optimize an objective function that prefers density map generated on detected locations to be similar to predicted density map @cite . This results in both better precision and recall. The density map is generated by placing a Gaussian kernel at the location of each detection. Zheng al @cite first obtain density map using sliding window over the image through @cite , and then use integer programming to localize objects on the density maps. Similarly, in the domain of medical imaging, Sirinukunwattana al @cite introduced spatially-constrained CNNs for detection and classification of cancer nuclei. In this paper, we present results and analysis for simultaneous crowd counting, density map estimation, and localization using Composition Loss on the proposed UCF-QNRF dataset.
- Deep Learning for Collaborative Filtering. To take advantage of the expressiveness of DNNs, there are many recent efforts focused on developing deep learning models for collaborative filtering @cite @cite @cite @cite @cite @cite . Early work on DNNs focused on explicit feedback settings @cite @cite @cite , such as rating predictions. Recent research gradually recognized the importance of implicit feedback @cite @cite @cite , where the user's preference is not explicitly presented @cite . This setting is more practical but challenging, and is the focus of our work. Our method is closely related to three papers, on VAEs @cite , collaborative denoising autoencoder (CDAE) @cite and neural collaborative filtering (NCF) @cite . CDAE and NCF may suffer from scalability issues: the model size grows linearly with both the number of users as well as items. The VAE @cite alleviates this problem via amortized inference. Our work builds on top of the VAE, and improves it by optimizing to the ranking-based metric.
- Learned Metrics in Vision & Languages. Recent research in computer vision and natural language processing has generated excellent results, by using learned metrics instead of hand-crafted metrics. Among the rich literature of generating realistic images via generative adversarial networks (GANs) @cite @cite @cite @cite , our work is most similar to @cite , where the VAE objective @cite @cite is augmented with the learned representations in the GAN discriminator @cite to better measure image similarities. For language generation, the discrepancy between word-level MLE training and sequence-level semantic evaluation has been alleviated with GANs or RL techniques @cite @cite @cite @cite @cite . The RL approach directly optimizes the metric used at test time, and has shown improvement on various applications, including dialogue @cite , image captioning @cite and translations @cite . Despite the significant successes in vision and language analysis, there has been little if any research reported for directly learning the metrics with deep neural networks for collaborative filtering. Our work fills the gap, and we hope it inspires more research in this direction.
- Learning to Rank (L2R). The idea of L2R has existed for two decades in the information-retrieval community. The goal is to directly optimize against ranking-based evaluation metrics @cite @cite . Previous work on L2R employs objective relaxations @cite . Some techniques can be extended to recommendation settings @cite @cite @cite @cite @cite . Many L2R methods in recommendation are essentially trained by optimizing a classification function, such as the popular pairwise L2R method BPR @cite and WARP @cite described Section 2.1. One limitation is that they are computationally expensive when the number of items is large. To accelerate these approaches, cheap approximations are made in each training step, which results in degraded performance. In contrast, the proposed RaCT is efficient and scalable. In fact, the traditional L2R methods can be integrated into our actor-critic framework, yielding improved performance as shown in our experiments.
- Adler and Monteiro @cite studied the parametric analysis of LO problems using the concept of optimal partition. Another treatment of sensitivity analysis for LO based on the optimal partition approach was given by @cite and Greenberg @cite . @cite @cite extended the optimal partition approach to linearly constrained quadratic optimization (LCQO) with perturbation in the right hand side vector and showed that the optimal value function is convex and piecewise quadratic. There have been further studies on optimal partition and parametric analysis of conic optimization problems. In contrast to LO, the optimal partition of SDO is defined as a 3-tuple of mutually orthogonal subspaces of @math , see . Goldfarb and Scheinberg @cite considered a parametric SDO problem, where the objective is perturbed along a fixed direction. They derived auxiliary problems to compute the directional derivatives of the optimal value function and the so-called invariancy set of the optimal partition. Yildirim @cite extended the concept of the optimal partition and the auxiliary problems in @cite for linear conic optimization problems.
- Constrained Maximum Likelihood Linear Regression (CMLLR) @cite @cite , also known as feature-space MLLR (fMLLR), is a widely used speaker adaptation technique for hidden Markov model (HMM)-based speech processing systems in which a speaker-dependent affine transformation is applied to source acoustic features to explain target data better. In the case of automatic speech recognition (ASR), the transformation acts as a method of normalization, whereas in the case of speech synthesis, the transformation purpose is to diverge the acoustic output to each target speaker @cite . The fMLLR method can be described using the following equation: where @math is the source acoustic features, @math represents approximated acoustic features of the target speaker @math , @math is a full linear matrix and @math is the bias vector. @math and @math are transformation parameters specific to each speaker.
- Next we explain the existing DNN-based speaker adaptation methods, that is, speaker-dependent layers and speaker-dependent input code using similar notations to the above fMLLR. For the speaker-dependent layers @cite @cite approach, the weight matrices and bias vectors of specific layers are fine-tuned using adaptation data, therefore we can rewrite Equation as: where @math and @math are now specific to a target speaker @math and @math also represents an adapted hidden layer . The method has the advantage of modeling both a full matrix @math and the bias vector @math , which usually yield favorable result when the adaptation data is sufficient @cite @cite . However when the amount of adaptation data is limited, the result is unstable as number of parameters estimated is very large @cite . This is also the reason that this method typically involves reducing the number of parameters estimated @cite @cite @cite in order to retain the adaptation performance.
- Learning Hidden Unit Contribution (LHUC) @cite is an adaptation method that transforms outputs of the activation function using a speaker-dependent diagonal transformation matrix, which significantly reduces the number of parameters: where @math is a diagonal matrix for speaker @math , @math is an operation to extract diagonal elements of a @math matrix as a @math vector, and @math is an element-wise multiplication of vectors. In LHUC, since we apply the transformation after the activation function of the current layer, we may write the LHUC operation at the next hidden layer as follows: From these equations, we see that a speaker-specific weight matrix @math is factorized as @math .
- Eliminating redundant literals in clauses (see e.g. @cite @cite @cite @cite @cite @cite @cite @cite @cite ) before and during the search is crucial for the performance of CDCL SAT solvers for several reasons: (i) shorter clauses need less memory; (ii) shorter clauses are easier to become unit, and thus increase the power of unit propagation; and (iii) shorter clauses can lead to shorter learnt clauses.
- Clustering-based approaches, on the other hand, leverage traditional clustering techniques such as @math -means for superpixel segmentation. Widely-used algorithms in this category include SLIC @cite , LSC @cite , and Manifold-SLIC @cite . These methods mainly do @math -means clustering but differ in their feature representation. While the SLIC @cite represents each pixel as a @math -dimensional positional and color features ( @math features), LSC @cite method projects these @math -dimensional features on to a @math -dimensional space and performs clustering in the projected space. Manifold-SLIC @cite , on the other hand, uses a @math -dimensional manifold feature space for superpixel clustering. While these clustering algorithms require iterative updates, a non-iterative clustering scheme for superpixel segmentation is proposed in the SNIC method @cite . The proposed approach is also a clustering-based approach. However, unlike existing techniques, we leverage deep networks to learn features for superpixel clustering via an end-to-end training framework.
- Inspired by the success of deep learning for supervised tasks, several methods investigate the use of deep networks for unsupervised data clustering. Recently, Greff al @cite propose the neural expectation maximization framework where they model the posterior distribution of cluster labels using deep networks and unroll the iterative steps in the EM procedure for end-to-end training. In another work @cite , the Ladder network @cite is used to model a hierarchical latent variable model for clustering. Hershey al @cite propose a deep learning-based clustering framework for separating and segmenting audio signals. Xie al @cite propose a deep embedded clustering framework, for simultaneously learning feature representations and cluster assignments. In a recent survey paper, Aljalbout al @cite give a taxonomy of deep learning based clustering methods. In this paper, we also propose a deep learning-based clustering algorithm. Different from the prior work, our algorithm is tailored for the superpixel segmentation task where we use image-specific constraints. Moreover, our framework can easily incorporate other vision objective functions for learning task-specific superpixel representations.
- Traditional approaches to image-based gaze estimation are typically categorized as or . Feature-based approaches reduce an eye image down to a set of features based on hand-crafted rules @cite @cite @cite @cite and then feed these features into simple, often linear machine learning models to regress the final gaze estimate. Model-based methods instead attempt to fit a known 3D model to the eye image @cite @cite @cite @cite by minimizing a suitable energy.
- Early works in appearance-based methods were restricted to laboratory settings with fixed head pose @cite @cite . These initial constraints have become progressively relaxed, notably by the introduction of new datasets collected in everyday settings @cite @cite or in simulated environments @cite @cite @cite . The increasing scale and complexity of training data has given rise to a wide variety of learning-based methods including variations of linear regression @cite @cite @cite , random forests @cite , @math -nearest neighbours @cite @cite , and CNNs @cite @cite @cite @cite @cite @cite . CNNs have proven to be more robust to visual appearance variations, and are capable of person-independent gaze estimation when provided with sufficient scale and diversity of training data. Person-independent gaze estimation can be performed without a user calibration step, and can directly be applied to areas such as visual attention analysis on unmodified devices @cite , interaction on public displays @cite , and identification of gaze targets @cite , albeit at the cost of increased need for training data and computational cost.
- Several CNN architectures have been proposed for person-independent gaze estimation in unconstrained settings, mostly differing in terms of possible input data modalities. Zhang al @cite @cite adapt the LeNet-5 and VGG-16 architectures such that head pose angles (pitch and yaw) are concatenated to the first fully-connected layers. Despite its simplicity this approach yields the current best gaze estimation error of @math when evaluating for the within-dataset cross-person case on MPIIGaze with single eye image and head pose input. In @cite separate convolutional streams are used for left right eye images, a face image, and a @math grid indicating the location and scale of the detected face in the image frame. Their experiments demonstrate that this approach yields improvements compared to @cite . In @cite a single face image is used as input and so-called spatial-weights are learned. These emphasize important features based on the input image, yielding considerable improvements in gaze estimation accuracy.
- It has been shown @cite @cite that by applying a loss function on intermediate outputs of a network, better performance can be yielded in different tasks. This technique was introduced to address the vanishing gradients problem during the training of deeper networks. In addition, such intermediate supervision allows for the network to quickly learn an estimate for the final output then learn to refine the predicted features - simplifying the mappings which need to be learned at every layer. Subsequent works have adopted intermediate supervision @cite @cite to good effect for human pose estimation, by replicating the final output loss.
- Most similar to our contribution is the work in @cite where facial landmark localization performance is improved by applying an auxiliary emotion classification loss. A key aspect to note is that their network is sequential, that is, the emotion recognition network takes only facial landmarks as input. The detected facial landmarks thus act as a manually defined representation for emotion classification, and creates a bottleneck in the full data flow. It is shown experimentally that applying such an auxiliary loss (for a different task) yields improvement over state-of-the-art results on the AFLW dataset. In our work, we learn to regress an intermediate and minimal representation for gaze direction, forming a bottleneck before the main task of regressing two angle values. Thus, an important distinction to @cite is that while we employ an auxiliary loss term, it directly contributes to the task of gaze direction estimation. Furthermore, the auxiliary loss is applied as an intermediate task. We detail this further in Sec. .
- Recent work in multi-person human pose estimation @cite learns to estimate joint location heatmaps alongside so-called part affinity fields''. When combined, the two outputs then enable the detection of multiple peoples' joints with reduced ambiguity in terms of which person a joint belongs to. In addition, at the end of every image scale, the architecture concatenates feature maps from each separate stream such that information can flow between the part confidence'' and part affinity'' maps. Thus, they operate on the image representation space, taking advantage of the strengths of convolutional neural networks. Our work is similar in spirit in that it introduces a novel image-based representation.
- Deep learning techniques have improved the accuracy of various vision systems @cite @cite @cite @cite . Especially, a lot of generative problems @cite @cite @cite @cite have been solved by GANs @cite . However, traditional frameworks fail to handle complicated tasks, e.g., to generate fine-grained images or videos with large motion changes. Recent approaches @cite @cite @cite prove that coarse-to-fine strategy can handle these cases. Our model also employs this strategy for video generation.
- @cite proposed an algorithm to generate video in two stages, but there are important differences between their work and ours. First, @cite is proposed for time-lapse videos while we can generate general videos. Second, we make use of structure conditions to guide the generation in the first stage but @cite models this stage with 3D convolutional networks. Finally, we can make long-term predictions while @cite only generates videos with fixed length.
- Video Generation. Recent methods @cite @cite @cite @cite solve image-to-video generation problem by training transformation networks that translate the input image into each future frame separately, together with a generator predicting the structure sequence which conditions the future frames. However, due to the absence of pixel-level temporal knowledge during the training process, motion artifacts can be observed from the results of these methods.
- Other approaches explore learning temporal visual features from video with spatiotemporal networks. @cite showed how 3D convolutional networks could be applied to human action recognition. @cite employed spatiotemporal 3D convolutions to model features encoded in videos. @cite built a model to generate scene dynamics with 3D generative adversarial networks. Our method differs from the two-stream model of @cite in two aspects. First, our residual motion map disentangles motion from the input: the generated frame is conditioned on the current and future motion structures. Second, we can control object motions in future frames efficiently by using structure conditions. Therefore, our method can be applied to motion manipulation problems.
- Goyal @cite present a strategy to train ResNet-50 @cite in one hour using the ImageNet dataset. They employed a distributed synchronous Stochastic Gradient Descent (SGD) approach with up to 256 GPUs. They used a large minibatch---8192 examples---and reached an accuracy similar to a much smaller minibatch of 256 examples.
- Tensorflow has also parallel execution capabilities built-in @cite . In this framework, the programmer specifies a set of operations to be placed in the available devices. These devices can be processors and accelerators (GPUs and TPUs) that are distributed across a cluster of computers. A common strategy is to split the data and processing units into parameter servers and workers nodes. The parameter servers are responsible for holding the network weights and other parameters, while the workers are responsible for computing the forward and backward passes of the model.
- The Tensorflow framework has grown in great popularity. However, its parallelization capabilities have been criticized for its unnecessary complexity. One alternative was proposed by Sergeev and Balso @cite . They used a simplified strategy based on MPI to parallelize and run distributed SGD models. A key feature of the approach is to combine small messages so that it better uses the network.
- In medical imaging, the current state-of-art for classification and segmentation of 3D exams are 3D deep learning models @cite . However, the widely known computational burden of such models due to 3D convolutions, in many times hinders the development of real-time systems for aiding in the diagnosis of 3D exams. This creates opportunities for methods that aim at providing time-efficient support to process 3D models in parallel.
- ResNet @cite has become popular by virtue of its assistance in deep model training. Numerous works based thereon improve performance by expanding its structure @cite @cite @cite @cite or use its explanation of ordinary differential equations to explore its reversible form @cite @cite . Because ResNet is internally diverse without operations such as the "drop-path" @cite and has been proven to be structurally redundant @cite , destructive approaches may promote its efficiency and enrich the structural representation by means of policy learning @cite @cite or a dynamic exit strategy @cite @cite .
- A parallel line of research has deemed that intermediate feature maps should be modeled repeatedly @cite . This compact architecture enables intermediate features to be refined and expanded, thereby enhancing the representation ability with concentrated parameter sizes. @cite proposed a more compact model by circulating the dense block. Furthermore, dual-path networks (DPNs) @cite combine the advantages of ResNet and DenseNet, and cause the residual units to perform extra modeling of the relationship between the identity and densely connected flow. A trend of compact architectures is to expand the mission of the network subassemblies while refining the intermediate features. Based on the SE block @cite , our proposed CMPE-SE design also refines the intermediate features and develops the role of the SE unit. The difference is that our model focuses on self-controlling of components in ResNet, rather than simple feature reuse. Moreover, the re-imaging of channel signals presents a novel modeling view of intermediate features.
- Attention is widely applied in the modeling process of CNNs @cite and is typically used to re-weight the image spatial signals @cite @cite @cite @cite , including multi-scale @cite @cite and multi-shape @cite features. As a tool for biasing the allocation of resources @cite , attention is also used to regulate the internal CNN features of @cite @cite . Unlike channel switching, combination @cite @cite or using reinforcement learning to reorganize the network paths @cite , channel-wise attention, typically such as @cite , provides an end-to-end training solution for re-weighting the intermediate channel features. Moreover, certain models combine spatial and channel-wise attention @cite @cite @cite , and their modeling scope is still limited in total attentional elements. In contrast, our proposed CMPE-SE block considers the additional related factors (identity mappings) apart from the objects of attention (residual mappings). Furthermore, we test the effects of various convolutional filters in channel-wise attention with channel signal inner-imaging, which can mine the spatial channel-wise relations.
- Traditional NER systems use hand-crafted features, gazetteers and other external resources to perform well @cite . obtain state-of-the-art results by relying on heavily hand-crafted features, which are expensive to develop and maintain. Recently, many studies have outperformed traditional NER systems by applying neural network architectures. For instance, use a bidirectional LSTM-CRF architecture. They obtain a state-of-the-art performance without relying on hand-crafted features. , who achieved the first place on WNUT-2016 shared task, use a BLSTM neural network to leverage orthographic features. We use a similar approach but we employ CNN and BLSTM in parallel instead of forwarding the CNN output to the BLSTM. Nevertheless, our main contribution resides on Multi-Task Learning (MTL) and a combination of POS tags and gazetteers representation to feed the network. Recently, MTL has gained significant attention. Researchers have tried to correlate the success of MTL with label entropy, regularizers, training data size, and other aspects @cite @cite . For instance, use a multi-task network for different NLP tasks and show that the multi-task setting improves generality among shared tasks. In this paper, we take advantage of the multi-task setting by adding a more general secondary task, NE segmentation, along with the primary NE categorization task.
- There have been plenty of works using deep learning to do image steganalysis and got excellent performance. @cite proposed a CNN-based steganalysis model GNCNN, the model introduced the hand-crafted KV filter to extract residual noise and used the gaussian activation function to get more useful features. The performance of the GNCNN is inferior to the state-of-the-art hand-crafted feature set spatial rich model (SRM) @cite slightly. Based on GNCNN, @cite presented Batch Normalization @cite in to prevent the network falling into the local minima. XuNet was equipped with Tanh, @math convolution, global average pooling, and got comparable performance to SRM @cite . @cite put forward YeNet which surpassed SRM and its several variants. YeNet used 30 hand-crafted filters from SRM to prepropose images, applied well-designed activation function named TLU and selection-channel module to strengthen features from rich texture region where is more suitable for hiding information. @cite proposed a JPEG steganalysis model with less parameters than XuNet and got better performance than XuNet. These works have applied deep learning to steganalysis successfully, but there is still space for improvement.
- Baluja @cite designed a CNN model to conceal a color secret image into a color cover image yielding state-of-art performance. @cite proposed another encoder-decoder based model to finish the same steganography task (their secret images are gray images). This is a novel steganography method which gets rid of hand-crafted algorithms. It can learn how to merge the cover image and the secret image together automatically. But stego images generated by their models are distorted in color. As shown in Fig. , Atique's stego images are yellowing when compared with the corresponding cover images. And their stego images are easily recognized by well trained CNN-based steganalyzer @cite because of the large capacity. Inspired by works of Baluja and Atique, we improve each shortcoming and get .
- Pixel-level semantic image segmentation has received considerable attention over the past few years. Most recently published techniques are based on fully convolutional networks (FCN) @cite , possibly combined with fully connected CRF @cite @cite @cite @cite . The general architecture of a FCN includes a sequence of convolution and downsampling layers ( encoder ), which extract multi--scale features, followed by a sequence of deconvolution layers ( decoder ), which produce a high--resolution segmentation (or prediction''). Skip layers are often added, providing shortcut links from an encoder layer to its corresponding decoder layer. The role of skip layers is to provide well-localized information to a decoder layer, in addition to the semantic-rich but poorly resolved information from the prior decoder layer. In this way, skip layers enable good pixel-level localization while facilitating gradient flow during training. Similar architectures have been used in various applications such as text segmentation @cite @cite @cite @cite , edge detection @cite , face segmentation @cite , and scene parsing @cite . Although these algorithms could be used for the foreground segmentation component of a content removal system, unavoidable inaccuracies are liable to dramatically decrease the quality of the recovered background region.
- Image inpainting @cite has a long history. The goal of inpainting is to fill in a missing region with realistic image content. A variety of inpainting methods have been proposed, including those based on prior image statistics @cite @cite , and those based on CNNs @cite @cite . More recently, outstanding results have been demonstrated with the use of GANs to inpaint even large missing areas @cite @cite @cite @cite . While appropriate for certain domains ( face inpainting), methods in this category often suffer from serious limitations, including the requirement that the missing region have fixed size and shape. All of the inpainting methods mentioned above assume that the corrupted region mask is known (typically as provided by the user). This limits their scope of application, as in most cases, this mask is unavailable. This shortcoming is addressed by blind inpainting algorithms @cite @cite , which do not need access to the foreground mask. However, prior blind inpainting work has been demonstrated only for very simple cases, with constant-valued foreground occupying a small area of the image.
- Single image SR is one of the most relevant inverse problems in the field of generative image processing tasks @cite @cite . Since the initial work by @cite which applied small convolutional neural networks to the task of single image SR, several better neural network architectures have been proposed that have achieved a significantly higher PSNR across various datasets @cite @cite @cite @cite @cite @cite @cite . Generally, advances in network architectures for image detection tasks have also helped in SR, e.g. adding residual connections @cite enables the use of much deeper networks and speeds up training @cite . We refer the reader to Agustsson and Timofte @cite for a survey of the state of the art in single image SR.
- Since maximizing for PSNR leads to generally blurry images @cite , another line of research has investigated alternative loss functions. @cite and Alexey and Brox @cite replace the mean squared error (MSE) in the image space with an MSE measurement in feature space of large pre-trained image recognition networks. @cite extend this idea by adding an adversarial loss and @cite combine perceptual, adversarial and texture synthesis loss terms to produce sharper images with hallucinated details. Although these methods produce detailed images, they typically contain small artifacts that are visible upon close inspection. While such artifacts are bearable in images, they lead to flickering in super-resolved videos. For this reason, applying these perceptual loss functions to the problem of video SR is more involved.
- Amongst classical video SR methods, @cite have achieved notable image quality using Bayesian optimization methods, but the computational complexity of the approach prohibits use in real-time applications. Neural network based approaches include @cite who use a bidirectional recurrent architecture with comparably shallow networks without explicit motion compensation. More recently, neural network based methods operate on a sliding window of input frames. The main idea of @cite is to align and warp neighboring frames to the current frame before all images are fed into a SR network which combines details from all frames into a single image. Inspired by this idea, @cite take a similar approach but employ a flow estimation network for the frame alignment. Similarly, @cite use a sliding window approach but they combine the frame alignment and SR steps. @cite also propose a method which operates on a stack of video frames. They estimate the motion in the frames and subsequently map them into high-resolution space before another SR network combines the information from all frames. @cite operate on varying numbers of frames at the same time to generate different high-resolution images and then condense the results into a single image in a final step.
- For generative video processing methods, temporal consistency of the output is crucial. Since most recent methods operate on a sliding window @cite @cite @cite @cite , it is hard to optimize the networks to produce temporally consistent results as no information of the previously super-resolved frame is directly included in the next step. To accommodate for this, @cite use a frame-recurrent approach where the estimated high-resolution frame of the previous step is fed into the network for the following step. This encourages more temporally consistent results, however the authors do not explicitly employ a loss term for the temporal consistency of the output.
- To the best of our knowledge, VSR methods have so far been restricted to MSE optimization methods and recent advancements in perceptual image quality in single image SR have not yet been successfully transferred to VSR. A possible explanation is that perceptual losses lead to sharper images which makes temporal inconsistencies significantly more evident in the results, leading to unpleasing flickering in the high-resolution videos @cite .
- The style transfer community has faced similar problems in their transition from single-image to video processing. Single-image style-transfer networks might produce very distant images for adjacent frames @cite , creating very strong transitions from frame to frame. Several recent works have overcome this problem by including a temporal-consistency loss that ensures that the stylized consecutive frames are similar to each other when warped with the optical flow of the scene @cite @cite @cite .
- Adversarial training constitutes the current foundation of state-of-the-arts for defending against adversarial attacks. It is first developed in @cite where both clean images and adversarial images are used for training. Kannan al @cite propose to improve robustness further by encouraging the logits from the pairs of clean images and adversarial counterparts to be similar. Instead of using both clean and adversarial images for training, Madry al @cite formulate adversarial training as a min-max optimization and train models exclusively on adversarial images. However, as these works mainly focus on demonstrating the effectiveness of their proposed mechanisms, a fair and detailed diagnosis of adversarial training strategies remains as a missing piece. In this work, we provide a detailed diagnosis which reveals two intriguing properties of training adversarial defenders.
- Most of works applied this approach improve the efficiency of CNNs via weight pruning @cite @cite @cite and quantization @cite . These approaches are effective because deep neural networks often have a substantial number of redundant weights that can be pruned or quantized without sacrificing much accuracy. For convolutional neural networks, different pruning techniques may lead to different levels of granularity. Fine-grained pruning, e.g., independent weight pruning @cite , generally achieves a high degree of sparsity. However, it requires storing a large number of indices, and relies on special hardware software accelerators. In contrast, coarse-grained pruning methods such as filter-level pruning @cite achieve a lower degree of sparsity, but the resulting networks are much more regular, which facilitates efficient implementations. These approaches are simple and intuitive, however, iterative optimization strategy is commonly utilized in these approaches, which slows down the training procedure.
- Scale-transformed weights were proposed in @cite , where it was observed to improve performance over the normal baseline CNN, on MNIST-Scale. On the same dataset (with a 10k, 2k and 50k split), better performance was observed in @cite , where in addition to forwarding the maximum filter response to a range of scales, the actual scale at which the response was obtained was also forwarded. In both works, weight scaling was only indirectly emulated, by rather scaling the input and the resizing back the convolution response to a fix size for max-pooling across scales.
- @cite @cite @cite represent the pioneer works that employed CNNs for semantic segmentation. FCN @cite laid the foundations for modern architectures where CNNs are employed in a fully-convolutional way. Authors used a pre-trained encoder together with a simple decoder module that takes advantage of skip-connections from lower layers to exploit high-resolution feature maps. They obtained a significant improvement both in terms of accuracy and efficiency. DeepLab @cite made use of Dilated Convolutions @cite to increase the receptive field of inner layers without increasing the overall number of parameters. After the introduction of Residual Networks (Resnets) @cite most methods employed a very deep Resnet as encoder DeepLabv2 @cite Resnet38 @cite FRRN @cite , pushing forward the performance boundary on semantic segmentation task. PSPNet @cite and DeepLabv3 @cite introduced context layers in order to expand the theoretical receptive field of inner layers. All these methods attain high accuracy on different benchmarks but at high computational costs.
- Empirical studies on larger corpora of real-world programs started in the early 2000s and revealed that surprisingly, antipatterns are prevalent @cite . This was first discovered for circular dependencies @cite , and later confirmed to apply to other antipatterns as well @cite . Antipatterns can be detected by means of static analysis before a system is deployed. The main issue here is the use of dynamic programming language features that create dependencies that may not be visible when the static analysis models are built. This area is generally under-researched, and we must assume that the models used only under-approximate the behaviour of the actual program. In particular, dependency graphs may not contain all edges showing actual program dependencies.
- There exist many approaches to visualise software evolution @cite . Most visualisations want to provide an improved understanding of the development activities by visualising structural changes, e.g. by using added and removed lines as metrics @cite @cite @cite @cite @cite @cite @cite or by providing highly aggregated information @cite @cite @cite . Our use case requires the visualisation of the structural evolution of the system and the antipattern instances at the same time. We are not aware of any evolution visualisation that supports this. There exist evolution visualisations of call graphs @cite @cite @cite . However, they do not provide any structural information.
- Previous work on classical survival analysis has demonstrated the advantages of deep learning over statistical methods @cite @cite @cite . Cox proportional hazards model @cite is the baseline statistical model for survival analysis, but is limited since the dependent risk function is the product of a linear covariate function and a time dependent function, which is insufficient for modeling complex non-linear medical data. @cite replaced the linear covariate function with a feed-forward neural network as input for the Cox PH model and demonstrated improved performance. The current literature addresses competing risks based on statistical methods (the Fine Gray model @cite ), classical machine learning (Random Survival Forest @cite @cite ), multi-task learning @cite ) etc., with limited success. These existing competing risk models are challenged by computational scalability issues for datasets with many patients and multiple covariates. To address this challenge, we propose a deep learning architecture for survival analysis with competing risks to optimize the time-dependent discrimination index. This is not trivial and will be elaborated in the next section.
- Instead, the optimisations applied to our networks are very much motivated by MobileNets @cite . Most notably, we utilise to significantly reduce the computational complexity of our segmentation networks. Such convolutions split a regular convolution into a filter and a combination step: first a separate 2D filter is applied to each input channel, after which a 1x1 convolution is applied to combine the results of these features. This can be seen as a factorisation of a full convolution that reduces the computational cost by a factor of @math , where @math is the number of output features and @math the kernel size. Not all convolutions can be factorised like this, so separable convolutions have less expressive power, but the results of the original MobileNets and those reported here show they can still perform at high accuracy.
- @cite is a more advanced open-source application developed by computer scientists in collaboration with journalists to support investigative journalism. The application supports import of PDF, MS Office and HTML documents, document clustering based on topic similarity, a simplistic location entity detection, full-text search, and document tagging. Since this tool is already mature and has successfully been used in a number of published news stories, we adapted some of its most useful features such as document tagging and a keyword-in-context (KWIC) view for search hits. Furthermore, in we concentrate on intuitive and visually pleasing approaches to display extracted contents for a fast exploration of collections.
- The @cite @cite @cite system is a third tool that supports investigative analysis of textual documents. Jigsaw focuses on the extraction of entities (using multiple NER tools for English) and their correlation with metadata. It provides visualization of entities as lists and document contents as a (tree-structured) word graph. instead visualizes coherence of such information as network graphs.
- Recent successes in visual recognition tasks, including object classification, detection, and segmentation, can be attributed to exploration of different CNN designs @cite @cite @cite @cite @cite @cite . To make these network designs more efficient, they have been extended with efficient and sparse forms of convolutions, such as depth-wise and group convolutions @cite @cite @cite @cite @cite @cite . In this paper, we introduce dimension-wise convolutions that generalize depth-wise convolutions to all dimensions of the input tensor.
- Recently, neural search methods, including reinforcement learning and genetic algorithms, have been proposed to automatically construct network architectures @cite @cite @cite @cite @cite @cite . These methods search over a huge network space (e.g. MNASNet @cite searches over 8K different design choices) using a dictionary of pre-defined search space parameters, including different types of convolutional layers and kernel sizes, to identify a network structure, usually non-homogeneous, that satisfies optimization constraints, such as inference time. Recent search-based methods @cite @cite @cite use MobileNetv2 @cite as a basic search block for automatic network design. Since the proposed unit delivers better performance than MobileNetv2 (see Section ), we believe that neural architecture search with our proposed unit would enable finding a better network design.
- These approaches include network quantization @cite @cite @cite @cite @cite @cite @cite , compression @cite @cite @cite @cite @cite @cite , and distillation @cite @cite @cite @cite @cite . Network quantization-based approaches approximate convolution operations with fewer bits instead of using 32-bit full precision floating points. This improves speed at inference and also, reduces memory required for storing network weights. Network compression-based approaches improve the efficiency of a network by removing redundant weights and connections. Unlike network quantization and compression, distillation-based approaches improve the accuracy of (usually shallow) networks by supervising the training with large pre-trained networks.
- Multiple Instance Learning (MIL) provides a suitable way for formulating and solving WSOD. In specific, if an image is annotated with a specific class, at least one proposal instance from the image is positive for this class; and no proposal instance is positive for unlabeled classes. Previous works on applying MIL to WSOD can be roughly categorized into two-step @cite @cite @cite @cite and end-to-end @cite @cite @cite @cite @cite @cite based approaches.
- first extract proposal representation leveraging hand-crafted features or pre-trained CNN models and employ MIL to select the best object candidate for learning the object detector. For instance, Wang al @cite presented a latent semantic clustering approach to select the most discriminative cluster for each category. Cibis al @cite learned a multi-fold MIL detector by re-labeling proposals and re-training the object classifier iteratively. Li al @cite first trained a multi-label classification network on entire images and then selected class-specific proposal candidates using a mask-out strategy, followed by MIL for learning a Fast R-CNN detector. Recently, Jie al @cite took a similar strategy as Li al @cite and proposed a more robust self-taught approach to learn a detector by harvesting more accurate supportive proposals in an online manner. However, splitting the WSOD into two steps results in a non-convex optimization problem, making such approaches trapped in local optima.
- Beyond the above mentioned related works, some fully-supervised object detection approaches @cite @cite @cite @cite also exploit contextual information of bounding boxes for benefiting object detection. Both Chen al @cite and Li al @cite leveraged information of enlarged contextual proposals to enhance the accuracy of the classifier. Zhu al @cite proposed to use a pool of segments obtained in the bottom-up manner to obtain better detection boxes. Our TS 2 C is totally different from these works in terms of both motivation and methodology. In particular, our motivation is to employ surrounding segmentation context to suppress these false positive objects parts. In addition, our approach can be easily embedded into any WSOD framework to make a further performance improvement.
- Several works have studied the end-to-end discrete representation learning model with different incorporated structures in the bottleneck stages. @cite and @cite introduce scalar quantization in the latent space and optimize jointly the entire model for rate-distortion performance over a database of training images. @cite proposes a compression model by performing vector quantization on the network activations. The model uses a continuous relaxation of vector quantization which is annealed over time to obtain a hard clustering. @cite and @cite introduce the Gumbel-Softmax gradient estimator for non-differentiable discrete distributions. The Gumbel-Softmax estimator determines the gradient of discrete distributions by sampling from a differentiable Gumbel-Softmax distribution which can be smoothly annealed into a categorical distribution.
- The interest for measuring the diameter and intima-media thickness (IMT) of major vessels has stemmed from its importance as biomarker of hypertension damage and atherosclerosis in adults. Typically, the IMT is assessed on the carotid artery by identifying its lumen and the different layers of its wall on high resolution US images. The improvements provided by the design of semi-automatic and automatic methods based mainly on the image intensity profile, distribution and gradients analysis, and more recently on active contours. For a comprehensive review of these classical methods we refer the reader to @cite and @cite . In the prenatal setting, the lower image quality, due to the need of imaging deeper in the mother's womb and by the movement of the fetus, makes the measurement of a IMT biomarker, although measured on the abdominal aorta, challenging.
- However, the exploitation of temporal redundancy on US sequences was shown to be a solution for improving overall detection results of the fet al heart @cite , where the use of a CNN coupled with a recurrent neural network (RNN) is strategic. Other works, propose similar approach in order to detect the presence of standard planes from prenatal US data using CNN with Long-Short Term Memory (LSTM) @cite .
- Face editing or manipulation has been widely studied in the field of computer vision and graphics, including face morphing @cite , expression edits @cite @cite , age progression @cite , facial reenactment @cite @cite @cite . However, these models are designed for a particular task, thus rely heavily on domain knowledge and certain assumptions. For example, @cite assumes neutral and frontal faces to begin with while @cite assumes the availability of target videos with variation in both poses and expressions. Our model differs from them as it is a data-driven approach that does not require domain knowledge, designed to handle general face manipulations.
- Our work can be categorized into image translation with generative adversarial networks @cite @cite @cite @cite @cite @cite @cite , whose goal is to learn a mapping @math that induces an indistinguishable distribution to target domain @math , through adversarial training between a pair of generator @math and discriminator @math . For example, @cite takes image as a condition for general image-to-image translation trained on paired samples. Later, Zhu et.al @cite extends @cite by introducing cycle-consistency loss to obviate the need of matched training pairs. In addition, it alleviates many-to-one mapping during training generative adversarial networks also known as mode collapse. Inspired by this, we integrate this loss into our model for identity preservation between different domains.
- Another seminal work that inspired our design is StarGAN @cite , where target facial attributes are encoded into a one-hot vector. In StarGAN, each attribute is treated as a different domain and an auxiliary classifier used to distinguish these attributes is essential for supervising the training process. Different from StarGAN, our goal is to perform continuous edits in the pixel space that cannot be enumerated with discrete labels. This implicitly implies a smooth and continuous latent space where each point in this space encodes meaningful axis of variation in the data. We treat different style modalities as domains in this paper and use two words interchangeably. In this sense, applications like beautification de-beautification, aging younger, with beard without beard can also be included into our general framework. We compare our approach against CycleGAN @cite and StarGAN @cite in and illustrate in more details about our design in .
- We are aware of works that use pose as condition in the task of person re-identification for person image generation @cite @cite @cite @cite . For example @cite concatenates one-hot pose feature maps in a channel-wise fashion to control pose generation similar to @cite , where keypoints and segmentation mask of birds are used to manipulate locations and poses of birds. To synthesize more plausible human poses, Siarohin et.al @cite develop deformable skip connections and compute a set of affine transformations to approximate joint deformations. These works share some similarity with ours as both facial landmark and human skeleton can be seen as a form of pose representation. However, all those works deal with manipulation in the original domain and does not preserve identity. Moreover, generated results in those works are low-resolution whereas our model can successfully generate 512x512 resolution with photo-realistic quality.
- Neural style transfer was first proposed by @cite . The idea is to preserve content from the original image and mimic style'' from the reference image. We adopt Gram matrix in our model to enforce texture consistency and replace L-BFGS iteration with back propagation for end-to-end training. Also, considering the lack of groundtruth data of many face manipulation tasks, we apply a fast neural style transfer algorithm @cite to generate pseudo targets for multi-modality manipulations. Note that our model is easily extensible to any desired target domains with current design unchanged.
- Providing low level information (such as the coarse masks in our method) in the form of an embedding layer is explored in previous works @cite @cite . In @cite , the noisy labels are jointly embedded with the visual features extracted from an Inception-V3 @cite ConvNet. We explore an analogous approach, which concatenates the coarse masks with the convolution blocks at different network locations in the scarce data semantic segmentation domain.
- The connection between back-propagation and optimal control of dynamical systems is known since the earlier works on control and deep learning @cite @cite @cite . Recently, the dynamical systems approach to deep learning was proposed in @cite and explored in the direction of training algorithms based on the PMP and the method of successive approximations @cite @cite . In another vein, there are also studies on the continuum limit of neural networks @cite @cite and on designing network architectures for deep learning @cite @cite @cite @cite based on dynamical systems and differential equations. Instead of analysis of algorithms or architectures, the present paper focuses on the mathematical aspects of the control formulation itself, and develops a mean-field theory that characterize the optimality conditions and value functions using both PDE (HJB) and ODE (PMP) approaches. The over-arching goal is to develop the mathematical foundations of the optimal control formulation of deep learning.
- Spatial transformer Network (STN) @cite is a sub-differentiable sampling-based module, which is designed to spatially transform its input map to an output map that corresponds to a subregion of the input map and can be hence regarded as an effective region selection mechanism. It is convenient to incorporate a spatial transformer layer to the convolutional neural network and train it with the standard back-propagation algorithm. A parameter matrix is used to determine the location of the subregion, as well as its resize scale and the rotation angle. Recently, the spatial transformer has been applied to various computer vision tasks, e.g., multi-label image recognition @cite and saliency detection @cite . To the best of our knowledge, our work is the first in successfully using multiple iterations of STN within a LSTM framework for crowd counting.
- Although syntaxes for type annotations in dynamic languages go back at least as far as Lisp, the first attempts at adding a comprehensive static type system to a dynamically typed language involved Smalltalk, with the first practical system being Bracha's Strongtalk. Strongtalk (independently replicated for Ruby) provided a powerful and flexible static type system, where crucially, the system was (also known as pluggable @cite ). Programmers could run the static checker over their Smalltalk code (or not); either way the type annotations had no impact whatsoever of the semantics of the underlying Smalltalk program.
- The diversity of semantics and language designs incorporating gradual typing has been captured recently via surveys incorporating formal models of different design options. Chung et al. present an object-oriented model covering optional semantics (erasure), transient semantics, concrete semantics (from Thorn @cite ), and behavioural semantics (from Typed Racket), and give a series of programs to clarify the semantics of a particular language. Greenman et al. take a more functional approach, again modelling erasure, transient ( first order''), and behavioural ( higher order'') semantics @cite , and also present performance information based on Typed Racket. Wilson et al. take a rather different approach, employing questionnaires to investigate the semantics programmers expect of a gradual typing system @cite .
- Most recently, Kuhlenschmidt @cite employ an ahead of time ( traditional, static) compiler for a custom language called Grift and demonstrate good performance for code where more than half of the program is annotated with types, and reasonable performance for code without type annotations.
- Perhaps the closest to our approach are Vitousek @cite (incl. ) and Richards @cite . Vitousek describe dynamically checking transient types for Reticulated Python (termed tag-type'' soundness by Greenman and Migeed @cite ). As with our work, Vitousek 's transient checks inspect only the top-level'' type of an object. Reticulated Python undertakes these transient type checks at different places to Moth. Moth explicitly checks type anotations, while Reticulated Python implicitly checks whenever values flow from dynamic to static types. We refrain from a direct performance comparison since Reticulated Python is an interpreter without just-in-time compilation and thus performance tradeoffs are different.
- Richards @cite take a similar implementation approach to our work, demonstrating that key mechanisms such as object shapes used by a VM to optimize dynamic languages can be used to eliminate most of the overhead of dynamic type checks. Unlike our work, Richards implement monotonic'' gradual typing with blame, rather than the simpler transient checks, and do so on top of an adapted Higgs VM. The Higgs VM implements a baseline just-in-time compiler based on basic-block versioning. In contrast, our implementation of dynamic checks is built on top of the Truffle framework for the Graal VM, and reaches performance approaching that of V8 (cf. sec:baseline-perf ). The performance difference is of relevance here since any small constant factors introduced into a VM with a lower baseline performance can remain hidden, while they stand out more prominently on a faster baseline.
- Overall, it is unclear whether our results confirm the ones reported by Richards @cite , because our system is simpler. It does not introduce the polymorphism issues caused by accumulating cast information on object shapes, which could be important for performance. Considering that Richards report ca. 4 the classic Richards benchmark, while we see , further work seems necessary to understand the performance implications of their approach for a highly optimizing just-in-time compiler.
- Consequently, numerous works focus on providing SFC in SDNs. An SFC taxonomy that considers architecture and performance dimensions as the basis for the subsequent state-of-the-art analysis is introduced in @cite .
- Recently, the authors in @cite presented a scheduling and routing solution in SDN NFV time-triggered flows. In detail, they approximate the optimal solution over a corresponding static scheduling problem and solve it using ILP. As in our approach, hard constraints on the overall execution times are considered by @cite . However, we point out that, unlike our approach: (i) the focus of @cite is on the traffic routing and scheduling between SDN-enabled switches per time-flow, so that the resulting flow scheduler does not support, by design, failure and fault tolerance per link and switch of data time-flow; (ii) the joint flow and computing rate mapping afforded in @cite is, by design, static; (iii) the scheduler in @cite does not perform real-time reconfiguration rerouting, real-time traffic hosted by the serving controller; (iv) the work in @cite does not consider SFC and rerouting; and (v) the scheduler in @cite does not enforce per-flow QoS guarantees on the limited time minimum energy and or the minimum side-effect. Although the aforementioned solutions are interesting, however, none of them considers the problem of service chaining with respect to the energy consumption of the VMs.
- The available literature ranges from the joint problem of fault-aware distributing and routing the traffic flows content in SDNs NFVs infrastructure @cite @cite to the problem of fault detection and recovery solutions in SDNs NFVs @cite @cite . In detail, in @cite the authors analyze the fault tolerance over SDN. They present a discussion about fault tolerance and failure happening in the OpenFlow (OF) protocol that is applied in SDNs. Specifically, they propose a link node failure detection and failure recovery method in the data plane that can be controlled through the controller. However, they do not present any discussion about the application plane side-effect and do not cover the SFC fault-awareness.
- In @cite , they present a controller-based fault recovery solution that covers path-failure detection and preconfigured backup paths. However, we point out that, unlike our approach: (i) the focus of @cite is on presenting the network configuration in order to manage the traffic flows, which is not an effective solution, by design, in real scenarios; and (ii) the presented fault prevention method in @cite does not support the SFC over the SDN-enabled switches. The authors in @cite propose a solution to quickly detect link failures in order to increase the fault tolerance by combining the flow retrieval which is achieved through analyzing the protection switching times and using a fast protection method. Interestingly, this paper supports the fault minimization over the links and addresses the end-to-end fault tolerance method per flow, but not radically. Overall, the contribution does not afford, by design, jointly the QoSs in the node and link of SDN and does not support the SFC fault minimization, both of which are adopted in this paper.
- Besides, authors in @cite present NFV-FD, a fault-tolerant unreliable failure detector that is adapted based on information (it includes communication links states and the flow characteristics) obtained from an SDN. The paper presents flavor of novelties, but it fails to address the SFC traffic flows. Moreover, our solution utilizes a network equipment fault-aware technique that spreads out the fault tolerance process all over the components running in the SDN. In @cite , authors applied novel rule-based programming language presented in @cite to talk between the controller and the data plane to manage the adopted in-network fast-fail over mechanisms of incoming traffic flows in FatTire programs. Although this method is an interesting step toward to the fault-aware SDN traffic flow policy management, it suffers from fault recovery and fault prevention that matter in our solution.
- Numerous works address the switch energy efficiency and energy-aware routing strategies in SDNs NFVs @cite @cite @cite @cite @cite . In detail, the authors in @cite present a network-wide energy-aware routing method using OF maximizing aggregate network utilization and optimized load balancing in SDN. Their practical solution has problem with scalability and does not even support the FRFP SFC aspects that this paper also targets.
- Focusing on Fog computing appliances in SDNs NFVs, there are limited works that target the routing in fog-supported SDNs NFVs, such as @cite @cite @cite @cite . In particular, the authors in @cite address Fog computing over SDNs structures that preserve safety and non-safety services and are validated across two use cases: Data streaming and lane-change assistance services. The authors do not present any discussion about the SFC fault probability minimization, or failure recovery prevention. In another work, in @cite the authors push the Fog Node to remain in edge to manage the on-demand location-based applications flows received from mobile users engaged in SDNs NFVs and analyze the possible routing in such network. Unlike our method, it suffers from a lack of service chain management, fault-awareness, failure prevention and SDN-enabled switch energy minimization.
- Moreover, recently in another work @cite , the authors address the resource allocation and total energy minimization over the Fog Nodes by proposing a novel QoS-aware distributed and scalable scheduler. Although based on the authors claim that it can be applied in real-time services, it fails to address the chain of services when it faces fault and failure in such a dynamic network. Interestingly, our architecture, FRFP, can cover all the benefits of this method by covering all the limitations addressed. The most recent method similar to our current work is our previous work @cite on SFC management in SDNs NFVs. We present energy-aware resource reallocation SFC algorithms for SDNs. We allocate VNF to a set of flows and find several optimal and near-optimal solutions to optimize such network. Compared to our contribution, the paper @cite has several limitations: (i) the presented routing algorithms do not exploit the capability of routing all flows simultaneously, i.e., it is impossible to reroute a flow considering the possible routes of other flows; and, (ii), we did not adopt the fog nodes to support fault probability minimization and failure recovery prevention.
- In this section, we discuss other error recovery approaches used by top-down parsers with backtracking, focusing on PEGs. Error handling in top-down parsing based on CFGs is a well-studied subject. Grune and Jacobs @cite presents an overview of several error handling techniques used in this context.
- Swierstra and Duponcheel @cite shows an implementation of parser combinators for error recovery, but is restricted to LL(1) grammars. The recovery strategy is based on a noskip set, computed by taking the FIRST set of every symbol in the tails of the pending rules in the parser stack. Associated with each token in this set is a sequence of symbols (including non-terminals) that would have to be inserted to reach that point in the parse, taken from the tails of the pending rules. Tokens are then skipped until reaching a token in this set, and the parser then takes actions as if it found the sequence of inserted symbols for this token.
- Our work relates to (but constrasts with) those that perform interpretability analysis for trained networks. In posthoc analysis, one interprets a trained network by fitting explanations to how the network performs classification. There are two general approaches to understanding networks posthoc: one is class-specific activation maximization @cite @cite @cite @cite @cite @cite @cite , and the other is input-specific posthoc visualization such as deconvolution @cite and gradient-based saliency visualization @cite @cite @cite @cite . All of these posthoc visualization methods do not explain the reasoning process of how a network makes its decisions. In contrast, our network has a built-in case-based reasoning process, and the explanations generated by our network are actually used during classification and are not created posthoc.
- Our work relates closely to works that build interpretability into deep neural networks. Attention mechanisms that identify the most relevant parts of an input for various tasks have been integrated into neural networks: various methods have been proposed to jointly train networks with integrated class-specific attention maps @cite @cite . There are also works that not only identify the important parts but also make use of them directly for classification: these works usually single'' out the important parts and use only these parts in the downstream reasoning process. They either use heavy supervision to locate the most relevant parts for classification (e.g. @cite @cite @cite ), or rely on an auxiliary (pre-trained) network to extract image patches for unsupervised identification of important parts (e.g. @cite @cite ), or propose a number of candidate parts using selective search-based region proposal network @cite @cite @cite @cite or Monte Carlo sampling @cite . However, none of these works learn prototypical cases for comparison and prediction as we do in our work.
- Some of the initial efforts on compositionality prediction were undertaken by , who use LSA to calculate the similarity between a phrase and its components, whereas extend this idea with collocation features (e.g., phrase frequency, point-wise mutual information). Researchers also tried to identify non-compositionality in verb-noun phrases using syntax @cite and selectional preferences @cite . Attempts to examine the possibility to derive the semantics of a compound or multiword expression from its parts have been researched extensively . define a compositionality score and use different vector operations to estimate the semantic distance between a phrase and its individual components. Some of the investigations are made for compositionality detection using representation learning of word embeddings . also show that distributional similarity over multiple languages can help in improving the quality of compositionality prediction.
- Multisensory perception has been widely studied in the literature and enables the robot to combine joint information with other sensors such as images and tactile cues. Bayesian estimation has been proved to achieve robust and accurate model based robot arm tracking @cite even under occlusion @cite . Furthermore, integrated visuomotor processes enabled humanoid robots to learn object representations through manipulation without any prior knowledge about them @cite , learn motor representations for robust reaching @cite @cite and even visuotactile motor representations for reaching and avoidance behaviors @cite .
- Active inference (under the free-energy principle) includes action as a classical spinal reflex arc pathway triggered by perception prediction errors and has been mainly studied in theoretical or simulated conditions. Friston presented in @cite a theoretical motor model with two degrees of freedom as an extension of the dynamic expectation maximization algorithm. It was recently studied in robot control of a simulated PR2 robot @cite , one degree of freedom simulated vehicle @cite and two degrees of freedom simulated robot arm @cite .
- A first model of the free-energy optimization in a real robot was performed in @cite working as an approximate Bayesian filter estimation, where the robot was able to perceive its arm location fusing visual, proprioceptive and tactile information. However, authors left out the action. In this work, we took one step further and modelled and applied active inference to the iCub robot for dual arm reaching with active head tracking. For reproducibility, the code is publicly available tobereleased . While the arms goal is to minimize the prediction error between the goal (object) and the end-effector visual location, the head goal is to maintain the object centered in the field of view to provide wider and more accurate reaching capabilities.
- Saliency detection One of the first methods for computational saliency was proposed by @cite . Their model based on the feature integration theory of @cite and the work of @cite decomposes the input image into low level feature maps including color, intensity and orientation. These maps are subsequently merged together using linear filtering and center surround structures to form a final saliency map. Their seminal work initiated much research in biologically inspired saliency models as well as more mathematical models for computational saliency . The central surround allows to measure contrast with the context, however it is confined to predefined shapes; normally the circle shape of the Gaussian filters or rectangle shapes in the work of @cite . In this paper we will propose a method for arbitrary shaped contexts.
- Local and global approaches for visual saliency can be classified in the category of bottom-up approaches. Local approaches compute local center-surround contrast and rarity of a region over its neighborhoods. @cite derive a bottom-up visual saliency based on center surround difference through multiscale image features. @cite propose a binary saliency estimation method by training a CRF to combine a set of local, regional, and global features. @cite propose the GBVS method which is a bottom-up saliency approach that consists of two steps: the generation of feature channels as in Itti's approach, and their normalization using a graph based approach. A saliency model that computes local descriptors from a given image in order to measure the similarity of a pixel to its neighborhoods was proposed by @cite . @cite propose a AWS method which is based on the decorrelation and the distinctiveness of local responses.
- Another class of features for saliency are based on global context or rarity; the saliency of a feature is based on its rarity with respect to the whole image. @cite consider the difference of patches with all other patches in the image to compute global saliency. @cite compute saliency by considering the reconstruction error which is left after reconstructing a patch from other patches (other patches can be from the same image or from the whole dataset). @cite compute the rarity of a feature by comparing the contrast between a 15 pixel border around the image and the object proposal histogram. Other than these methods we propose a method to compute the saliency with respect to the direct context of the object. Finally, to compute saliency @cite combined local and global objectness cues with a set of candidates location.
- Deep convolutional neural networks have revolutionized computer vision over the last few years. This has recently led to several papers on deep learning for saliency detection . Both @cite and @cite consider parallel networks which evaluate the image at various scales. @cite use two networks to describe local and global saliency. @cite combine a local and global model to compute saliency. The main challenge for saliency detection with deep networks is the amount of training data which is not always available. This is solved in by training on the largest available saliency dataset, namely MSRA-B , and testing on the other datasets (both also use pretrained network weights trained on the 1M Imagenet dataset). Like these method, we will use a pretrained network for the extraction of features for saliency detection.
- Among the first object proposal methods the work of @cite , named the Constrained Parametric Min-Cuts (CPMC) method, uses graph cuts with different random seeds to obtain multiple binary foreground and background segments. @cite proposes to measure the objectness of an image window, where they rank randomly sampled image windows based on their likelihood of containing the object by using multiple cues among which edges density, multiscale saliency, superpixels straddling and color contrast. @cite proposed an object proposal method similar to the CPMC method by generating multiple foreground and background segmentations. A very fast method for object proposals was proposed by @cite , which generates box proposals at 300 images per second.
- An extensive comparison of object proposal methods was performed by @cite . Among the best evaluated object proposal methods (which generate object segmentation) are the selective search , the geodesic object proposals and the multiscale combinatorial grouping method . Selective search proposes a set of segments based on hierarchical segmentations of the image where the underlying distance measures and color spaces are varied to yield a large variety of segmentations. @cite , propose the geodesic object proposals method, which applies a geodesic distance transfer to compute object proposals. Finally, Multiscale Combinatorial Grouping is based on a bottom-up hierarchical image segmentation. Object candidates are generated by a grouping procedure which is based on edge strength.
- Krengel and Sucheston gave the first tight @math -single item prophet inequality @cite @cite . The connection between multiple-choice prophet inequalities and mechanism design was recognized in @cite ; they proved a prophet inequality for uniform matroids. This bound was later improved by Alaei @cite using the , which is an OCRS in disguise. @cite further developed the connection between prophet inequalities and mechanism design, and showed how to be @math -prophet inequality for general matroids in a variant where the algorithm may choose the element order. Yan @cite improved this result to @math -competitive using the for submodular functions, first studied in @cite @cite . @cite adapted correlation gaps to a polytope to design CRSs. Improved correlation gaps were presented in @cite @cite . The matroid prophet inequality was first explicitly formulated in @cite . @cite gave an alternate proof, and extended to Bernoulli submodular functions, using OCRSs. Finally, information theoretic @math -prophet inequalities are also known for general downward-closed constraints @cite @cite .
- For structured sequence generation, Markov chains together with pre-defined repetition structure schemes were employed in @cite , where specific methods for handling transitions between repeating segments were proposed; in @cite , where an approach to a controlled creation of variations was introduced; in @cite , where chords were generated, obeying a pre-defined repetition structure. In @cite , a convolutional restricted Boltzmann machine was employed, and different structural properties were imposed using differentiable soft-constraints and gradient descent optimization. A constrained variable neighborhood search to generate polyphonic music obeying a tension profile and the repetition structure from a template piece was proposed in @cite . In @cite , Markov chains and evolutionary algorithms were used to generate repetition structure for Electronic Dance Music.
- Current path detection methods rely considerably on work developed for ill-structured unpaved rural roads. The typical solution in this case is to segment the road region from its surroundings by considering a set of pixels whose the probability of belonging to the road surface is above a given threshold calculated through models learned off-line @cite @cite or on-line in a self-supervised way @cite @cite . Frequently, a simplified known model of the road (e.g., trapezoidal) is fit to the segmented images. The Region Growing technique is an alternative to the model fitting process for less structured roads @cite @cite @cite . By enforcing a global shape constraint, the model-based approach enables the substitution of the road non-road pixel classification process by an unsupervised clustering mechanism @cite .
- The models referred in the previous paragraph are the basis of most work on path detection. An example is the use of knowledge about the color distributions of both the path and their surroundings for segmentation @cite . The main direction of the path can also be learned off-line in a supervised way @cite . The use of off-line learned models has the limitation that the robot is only able to perceive environments that have been covered by the training set. Robustness can be increased if these models are substituted by models learned on-line @cite @cite . In contrast to the road domain, the definition of the reference regions from which it is possible to supervise the learning process is challenging. With varying width and orientation, it is difficult to assure that the robot is on the path, and from that, which regions of the input image can be used as reference. Moreover, paths and its surroundings often exhibit the same height, which hampers a straightforward use of depth information to determine a path reference patch.
- The use of a global shape constraint (e.g., triangular) to avoid the learning process has also been tested in the path detection domain @cite . This is done by over-segmenting the image, creating sets of segments, and then scoring them against the global shape constraint. Accurate image over-segmentation is a computationally demanding task and usually requires clear edges segmenting the object from the background. Moreover, global shape constraints limit the type of paths that can be detected.
- To overcome the previous work's limitations, @cite proposed the use of local appearance contrast for path detection. Later, @cite exploited the related concept of visual saliency to detect paths. Visual saliency was exploited with a swarm-based solution, inspired by the social insects metaphor @cite , capable of interpreting the input images without the cost of explicitly over-segmenting the input image and the brittleness of depending on accurate appearance and shape models. As mentioned before, this method has shown to not exhibit sufficient robustness in the presence of strong distractors in the environment. This paper proposes an extension to the this previous method, based on 3-D data acquired from a visual SLAM technique, to reduce the chances of the method to be mistakenly attracted by distractors in the environment.
- The use of three-dimensional information to improve the targeting methods for detection of trails was initially explored by @cite @cite . More specifically, @cite used a binocular vision system composed of two omni-directional cameras and a laser scanner to map the vehicle's surrounding area. Thus, this previous method also presents itself as a solid solution for trail segmentation but requires the use of a laser scanner, which should be avoided if size, weight, and energy constraints found in small UAV are to be met. @cite also proposed a system based on binocular vision for detection and tracking of paths in a forest environment. In this paper we offer a solution that is based on monocular vision and, thus, is more easily integrated in current commercial UAV solutions. Moreover, the ability to solve the trail detection problem with a single camera is key for robots equipped with binocular systems when one of the camera fails or when stereo calibration data becomes deprecated.
- The scheduling of counters to build a compressor depends naturally on the selection of available modules. It is the backing technology that defines which counters can be implemented most efficiently. A discussion of the choices for ASICs was composed by Verma and Ienne @cite . FPGA-targeted counters have been most prominently proposed by Parandeh- @cite @cite @cite as well as Kumm and Zipf @cite @cite . As this paper focuses on the construction of compressors within a modern Xilinx FPGA fabric, it will heavily build on the work of these latter two groups.
- A heuristic for constructing compressors for Altera devices was proposed by Parandeh- in 2008 @cite . They used a single-pass heuristic selecting the most efficient from a selection of parallel counter that would fit into the work still to do by the compression step starting from the least-significant and proceeding to the most-significant bit position. The compression goal was a matrix of, at most, three rows. This relaxed goal definition exploits the fact that ternary adders map well onto modern FPGA architectures. It also has the tremendous benefit that half adders can be avoided altogether. Half-adders only have a reshaping function and do not reduce the number of bits in the matrix. As shown in figHAmust , they must be used to reshape an almost done two-row matrix in parallel so that it can accommodate just one more carry efficiently. This pressure disappears with a goal of three rows.
- In their follow-up work @cite , Parandeh- start considering mapping counters to the broader structural context of an Altera Adaptive Logic Module (ALM) rather than assuming an indifferent pool of lookup tables (LUTs). This enables them to exploit the carry-chain links between adjacent LUT stages for fast and yet more capable counters. Finally @cite , they tie individual counters together by merging the carry output of one module with the carry input of another into one LUT stage. While this, indeed, reduces LUT usage, it also creates unwieldy structures that severely limit the mobility of individual counters during the logic placement, which complicates the routing optimization to be performed by the tools. Last but not least, this work also looks into a generalization for Xilinx architectures.
- Most of the action recognition methodologies models the action using full-body motion based features, which only works well for the specific class of action recognition problem where action set is relatively small such as in @cite , @cite , @cite . These approach do not look useful when it comes to their application on real everyday actions. Research in the area of human action recognition has been mainly focused on full-body motions that can be characterized by movement and change of posture like walking, waving, etc.
- In many action recognition approaches @cite , @cite , @cite , human motion information have been used. The problem of action recognition has been dealt using motion trajectories with the use of depth cameras like Kinect. These approaches (e.g. see @cite ) are typically considered to be more robust to generate human pose information which can be used for the purpose of action recognition. However, Kinect body pose recognition is not accurate when there are human-object interactions due to occlusions. Motion dynamics based action recognition still cannot capture the representation for the subtle object manipulations. Another interesting aspect is the variations in goal of the task with similar motion dynamics.
- Hand gesture recognition is more closer to the problem of object manipulation action recognition. Hand gesture recognition has also been addressed using depth data generated from Kinect in @cite and @cite . But these techniques mainly target sign language gestures and not the human hand-object interactions. @cite treats an action sequence as a 4D shape and propose random occupancy pattern (ROP) features, extracted from random sampling of 4D subvolumes with different sizes and at different locations. In gesture depth sequences, the semantics of the gestures are mainly understood by the large movement of the hand. These approaches use cropped portion of hand using some hand detection approach, to determine these large hand movements to model different gestures. But, these clear motion information are not easily perceivable in the case object manipulation tasks.
- At this point, we note here that the above mentioned works involve processing low-level information (e.g. feature extraction from videos images), whereas our goal in this work is to convey the importance of grasp and motion-constraints information at the higher semantic level (e.g. types of grasps and motion-constraints). Such high level attributes for manipulating actions, are indeed available @cite , @cite , @cite as a part of the Yale human grasping dataset that we are considering in this work.
- To the best of our knowledge, apart from @cite , @cite and @cite , there has been no work using grasp information for action recognition. @cite semantically group action intentions using grasp based information into three coarse and somewhat abstract classes: Force-oriented, Skill-oriented, and Casual actions. They use hand grasps recognized through convolutional neural network to understand the class of action, each image belong to. @cite develop a grammatical formalism for parsing and interpreting action sequences. Their basic idea is to divide actions into sub-actions of when the object is grasped and released, or if there is change in the grasp type during the course of an action motion. This grammatical formalism provides a syntax and semantics of action, over which basic tools for understanding of actions can be developed. @cite considers the problem of grasp classification on Yale human grasping dataset, again based on the coarsely defined task attributes such as force (interaction and weight), motion-constraints on objects and functional class (use and hold), whereas we propose a solution to task or manipulation action classification based on the grasp information, motion-constraints, and object class.
- The important aspects of our work include: a) A compact representation of the grasp and motion-constraints using some popular and some contemporary schemes. b) Demonstrating the usefulness of information from coarse-grained and fine-grained grasp attributes as well as motion-constraints for fine-grained and coarse-grained action recognition. c) A differential experimental analysis involving subsets of grasp and motion-constraints features, to provide more insights on the usefulness of grasp information alone, motion-constraints information alone, and grasp and motion-constraints based information together for intended classification problem. d) Comparisons between Instance and Sequence level modeling of object manipulation actions using fine-grained grasp information. e) An extensive experimental evaluation using different contemporary multi-class and binary classifiers (with a multi-class voting strategy), which also serves as a useful comparative study of popular classifiers for the manipulation action recognition problem. This analysis also helps to demonstrate that different classification frameworks, largely arrive at a consensus with respect to our hypothesis about using grasp and motion-constraints for fine-grained action classification. We demonstrate our results on a large Yale Human Grasping dataset @cite which involves various tasks on different objects.
- Two content marketing platforms and millions of headlines were studied to find features that contribute to increasing users engagement and change of unsubscribed readers into subscribers. This study suggested that clickbait techniques may increase the users engagement temporarily @cite .
- An interesting model was proposed by Zhou for Clickbait Challenge 2017 @cite . He employed automatic approach to find clickbait in the tweet stream. Self-attentive neural network was employed for the first time in this article to examine each tweets probability of click baiting.
- Another successful method @cite , which was proposed in Clickbait Challenge 2017, used ensemble of Linear SVM models. They showed that how the clickbait can be detected using a small ensemble of linear models. Since the competitors were allowed to use external data sources, they were used in their research in order to find the pattern of non-clickbait headlines and expand the size of their training set.
- In @cite , authors developed linguistically-infused network model for the Clickbait Challenge 2017 that is able to learn strength of clickbait content from not only the texts of the tweets but also the passage of the articles and the linked images. They believed using the passage of the articles and the linked images can lead to a substantial boost in the models performance. They trained two neural network architectures which are Long Short-Term Memory (LSTM) @cite and Convolutional Neural Network (CNN). Their text sequence sub-network was constructed using embedding layer and two 1-dimensional convolution layers followed by a max-pooling layer. They initialize their embedding layer with pre-trained Glove embeddings @cite using 200-dimensional embeddings.
- In @cite , another model was proposed using neural networks for the Clickbait Challenge 2017. In the text processing phase, they used whitespace tokenizer with lower casing and without using any domain specific processing such as Unicode normalization or any lexical text normalization. Then all the tokens were converted to the word embeddings which were then fed into LSTM units. The embedding vectors were initialized randomly. They employed batch normalization to normalize inputs to reduce internal covariate shift. Also, the risk of over-fitting was reduced through using dropout between individual neural network layers. At the end, individual networks are fused by concatenating the dense output layers of the individual networks which then were fed into a fully connected neural network.
- A machine learning based clickbait detection system was designed in @cite . They extracted six novel features for clickbait detection and they showed in their results that these novel features are the most effective ones for detecting clickbait news headlines. Totally, they extracted 331 features but to prevent overfitting, they just kept 180 features among them. They used all the fields in the dataset such as titles, passages, and key words in their model for extracting these features.
- Distributed representations of code were first suggested by @cite , followed by several works leveraging embeddings to apply NLP techniques to programming languages @cite @cite .
- Learned representations of code are commonly used for two types of tasks: uncovering program semantics or optimizing programs. For the former task, code embeddings have been used to perform function or variable naming @cite @cite , clone detection @cite , code completion @cite @cite , summarization @cite , and algorithm classification @cite . As for program optimization, research has been conducted on automatic feature generation for code @cite @cite ; and @cite notably leverage embeddings of OpenCL code to predict optimal device mapping and thread coarsening factors. Their work differs from ours in that the method is restricted to the OpenCL language, and that they process programs in a sequential order, which does not capture complex code structures. Furthermore, the state-of-the-art in automatic tuning for program optimization @cite uses surrogate performance models and active learning, and does not take code semantics into account.
- Previous works that use code embeddings do not evaluate the quality of the trained space on its own merit, but rather through the performance of subsequent (downstream) tasks. One exception is @cite , who present empirical evidence of vector similarities for similar method names. To the best of our knowledge, we are the first to quantify the quality of a code embedding space itself in the form of clustering, syntactic analogies, semantic analogies, and categorical distance tests.
- Deep learning has achieved far more accurate results than traditional methods on image processing. There are several works on lane makings and parking slots detection using deep learning on front sight images. J Kim and M Lee @cite presented a robust lane detection method based on the combined convolutional neural network with random sample consensus algorithm; S @cite proposed a unified end-to-end trainable multi-task network that jointly handles lane and road marking detection and recognition guided by a vanishing point; G @cite proposed a decentralized and efficient solution for visual parking slots occupancy detection based on a deep convolutional neural network.
- Traditional image processing methods on panoramic images and deep learning methods on front sight images both achieve excellent results, but few work is aimed to use deep leaning on panoramic images. This is because the accuracy of deep learning model is largely related to datasets, and there are few public dataset of panoramic images which can be used to train a model. The first public panoramic dataset for lane markings and parking slots is panoramic surround view (PSV) dataset, released by Yan @cite . And this dataset is specially used for semantic segmentation, which is labeled pixel by pixel, and each pixel has its corresponding class. In order to combine the advantages of panoramic images and deep learning, we use the semantic segmentation method with convolutional network to segment the area and classify the class of lane makings and parking slots on the PSV dataset.
- In @cite , they achieved the segmentation of lane markings and parking slots using semantic segmentation method on PSV dataset, and proposed a VH-stage module special for linear structures. But the size of their model is too large to meet the requirement of using in embedded and mobile platform. In this paper, we put forward a smaller size model, but with higher accuracy. And the two main improved methods we proposed are proved to be significant.
- * -0.2cm In the context of sub-6GHz systems, @cite have studied a distributed antenna system providing both data communication and positioning functionalities. The authors assumed that the UEs know the positions of the BSs and attempt to estimate their own positions based on the received signals. @cite have shown that localization using mm-wave frequencies is efficient in terms of accuracy, even in the presence of a limited number of anchor nodes. In fact, mm-wave beam-forming allows for accurate localization and orientation of UEs with respect to the BSs @cite . @cite have studied a location-aided initial access strategy for mm-wave networks, in which the information of UE locations enables to speed up the channel estimation and beam-forming procedures. @cite have studied the trade-off between communication rate and positioning quality in a single user mm-wave link. Similarly @cite have studied the beamforming optimization and spectral power allocation based on theoretical localization bounds.
- The downlink communication performance in random wireless networks is typically characterized by signal to interference and noise ratio (SINR) coverage probability and rate coverage probability, using stochastic geometry @cite . For this, the positions of the BSs are modeled using homogeneous Poisson point process (PPP) @cite or using repulsive point processes @cite . Recently, Ghatak et.al. @cite investigated a more realistic scenario, where mm-wave BSs are deployed along the roads of a city. We use this model in this paper, and accordingly we study a one dimensional setting where the BSs and the served users are assumed to be on the same street.
- A large number of prior works have implemented policy gradient algorithms with entropy regularization @cite @cite @cite @cite , which boost exploration by greedily maximizing policy entropy at each time step. In contrast to such greedy procedure, maximum entropy objective considers entropy over the entire policy trajectories @cite @cite @cite . Though entropy regularization is simpler to implement in practice, @cite @cite argues in favor of maximum entropy objective by showing that trained policies can be robust to noise, which is desirable for real life robotics tasks; and multi-modal, a potentially desired property for exploration and fine-tuning for downstream tasks. However, their training procedure is fairly complex, which consists of training a soft Q function by fixed point iteration and a neural sampler by Stein variational gradient @cite . We argue that properties as robustness and multi-modality are attainable through simple entropy regularized policy gradient algorithms combined with expressive policy representations.
- Prior works have studied the property of maximum entropy objective @cite @cite , entropy regularization @cite and their connections with variants of operators @cite . It is commonly believed that entropy regularization greedily maximizes local policy entropy and does not account for how a policy update impacts future states. In Section 4, we show that entropy regularized policy gradient update maximizes a lower bound of maximum entropy objective, given constraints on the differences between consecutive policy iterates. This partially justifies why simple entropy regularization combined with expressive policy classes can achieve competitive empirical performance in practice.
- There is a number of prior works that discuss different policy architectures. The most common policy for continuous control is unimodal Gaussian @cite @cite @cite . @cite discusses mixtures of Gaussian, which can represent multi-modal policies but it is necessary to specify the number of modes in advance. @cite also represents a policy using implicit model, but the policy is trained to sample from the soft Q function instead of being trained directly. Recently, we find @cite also uses Normalizing Flows to represent policies, but their focus is learning an hierarchy and involves layers of pre-training. Contrary to early works, we propose to represent flexible policies using implicit models Normalizing Flows and efficient algorithms to train the policy end-to-end.
- Implicit models have been extensively studied in probabilistic inference and generative modeling @cite @cite @cite @cite . Implicit models define distributions by transforming source noise via a forward pass of neural networks, which in general sacrifice tractable probability density for more expressive representation. Normalizing Flows are a special case of implicit models @cite @cite @cite , where transformations from source noise to output are invertible and allow for maximum likelihood inference. Borrowing inspirations from prior works, we introduce implicit models into policy representation and empirically show that such rich policy class entails multi-modal behavior during training. In @cite , GAN @cite is used as an optimal density estimator for likelihood free inference. In our work, we apply similar idea to compute entropy regularization when policy density is not available.
- Various audio input representations have been used for the first step of the pipeline, such as filtered logarithmic magnitude and phase spectrum @cite @cite . The former can be subdivided by the filterbank type -- Bark scale bands @cite , Mel scale bands @cite @cite or constant-Q bands @cite @cite .
- : The state-of-the-art performance in the MIREX Audio Onset Detection is defined by deep learning-based methods. @cite proposed using recurrent neural networks (RNNs) with LSTM units to predict the input frames binarily as onset or non-onset. Schl " u ter and B " o ck @cite used the similar idea but replaced RNNs by convolutional neural networks (CNNs) and achieved the best performance in the MIREX Audio Onset Detection task. @cite used convolutional-recurrent neural networks (CRNNs) to detect drum onset and produced a better score than CNNs on several percussion datasets.
- The last step of the pipeline -- onset selection can be done by peak-picking @cite or hidden Markov model (HMM) inference @cite @cite if the musical score is available.
- LSB (Least Significant Bit)-based methods @cite are the most commonly used image domain steganography methods which hide information at the pixel level. Most LSB methods aim at altering parts of the cover image to such an extent that human visual system can barely notice. These methods are motivated by the fact that the visual part of most figures is dominated by the highest bits of each pixel, and the LSB bits (the underlined part of one pixel as shown in Figure ) are statistically similar to randomly generated data, and therefore, hiding information via altering LSB cannot change the visual result apparently.
- Since the least significant bits of the image data should look like random data, there are major two schemes in distributing the hiding data. The first kind of methods is to put in the hiding message sequentially after encrypting or compressing to achieve the randomness. The second kind of methods is scattering the hiding data by adopting a mutually acknowledged random seed by which generates the actual hiding sequence @cite .
- However, designing and tuning handcrafted patterns are highly technical and might be effective for only some tasks. On the contrary, convolutional neural networks have the advantage of automatically creating patterns for specific tasks through back-propagation @cite on its own, and even further, high-level features can be easily learned through combinations of convolution operations @cite @cite @cite .
- Our method is inspired by traditional autoencoder neural networks @cite , which was originally trained to generate an output image the same as input image in appearance. It is usually made up of two neural networks, one encoding network (h = f(x) ) and one decoding network (d = g(h) ), restricted under (d = x ), who finally can learn the conditional probability distribution of (p(h|x) ) and (p(x|h) ) correspondently. The autoencoder architecture has shown the ability to extract salient features in from images seen through shrinking hidden layer ( (h ))'s dimension, which has been applied to various fields, i.e., denoising @cite , dimension reduction @cite , image generation @cite , etc.
- Recently there are some works on applying neural networks for steganography. El-Emam @cite and Saleema @cite work on using neural networks to refine the embedded image generated via traditional steganography methods, i.e., LSB method. Volkhonskiy's @cite and Shi's @cite work focus on generating secure cover images for traditional steganography methods to apply image steganography. Baluja @cite is working on the same field as StegNet. However, the hidden image is slightly visible on residual images of the generated embedded images. Moreover, his architecture uses three networks which requires much more GPU memory and takes more time to embed.
- The growing interest in IoT and edge fog computing has given rise to several . @cite extends the prior work on CloudSim @cite to simulate the behavior of applications over fog devices, sensors and actuators that are connected by a network topology. Users define the compute, network and energy profiles of fog devices, and the properties and distributions of tuples from sensors. DAG-based applications with tasks consuming compute capacity and bandwidth can be defined by the user, and its execution over the fog network is simulated using an extensible resource manager. The goal is to evaluate different scheduling strategies synthetically. We similarly let devices, network and sensors to be defined, but actually instantiate the first two -- only the sensor stream is simulated. This allows users to evaluate real applications and schedulers.
- @cite offers similar capabilities, but also introduces mobility models for the edge into the mix. They simulate network characteristics like transmission delay for LAN and WAN, and also task failures due to mobility for a single use-case. , despite its name, simulates the execution of Map Reduce and stream processing tasks on top of a cloud data center, and uses CloudSim as the base simulation engine. While IoT motivates the synthetic application workloads for their big data platform simulation, they do not actually simulate an IoT deployment.
- Other have proposed IoT data stream and application workloads for evaluating big data platforms, particularly stream processing ones. Here, the sensor data is simulated at large-scales while maintaining realistic distributions @cite @cite . These can be used in place of the synthetic sensor streams that we provide. Our prior work has proposed stream and stream processing application workloads for IoT domains @cite . These can potentially use for evaluating execution on edge and fog, besides just cloud resources.
- A lot of recent work in collaborative filtering leverages reinforcement learning techniques. One approach by @cite is to treat recommender systems as a (MDP) and use RL techniques to solve it. @cite also formalize the problem as a MDP and learn the connection between the time sequence of user ratings using Q-learning. @cite formulate the problem as a gridworld game using a a bi-clustering technique to reduce the action and state space substantially. To address the cold-start problem using RL techniques, @cite cast it as a contextual bandit problem. They propose a new method based on the popular LinUCB algorithm @cite for contextual-bandit problems.
- In the past, @cite combined content and collaborative data under a single probabilistic framework. @cite proposed a pairwise predictive system to build feature-based regression models, where user demographic information, item content features, and other information about users and items are utilized to address cold-start problems. The model uses a user's ratings as targets and uses a pair of vectors @math and @math , to predict the rating @math on the item @math given by the user @math , where @math is an index over users and @math is an index over items.
- One successful method for addressing cold-start is a preliminary interview process to learn user's interests for good recommendation. @cite propose building a decision tree for the initial interview process by bootstrapping. They construct a decision tree for the initial interview with each node being an interview question, enabling the recommender to query a user adaptively according to his her prior responses.
- In @cite , the authors build on the previous work and propose . In this, latent profiles are associated for each node of the decision tree and hence the user profile is the function of all possible answers to the interview. The novelty is an iterative optimization algorithm that alternates between decision tree construction and latent profiles' generation. This helps to learn the best decision tree to ask questions and generate good embeddings simultaneously. The profile @math is tied to user @math s responses in the form of a function; thus the name functional matrix factorization (fMF). Given an answer set @math , the user profile is generated by @math . The goal is to learn T and @math (item profiles) from the observed ratings set.
- The most relevant works to ours in technique are @cite @cite @cite , all of which considered the Bayesian classification model for statistical parity or equalized odds. They either reduce their problem to unconstrained optimization problem by the Lagrangian principle or can be alternately expressed in that form. Our work uses similar techniques but in comparison can handle a wider class of fairness metrics.
- Another approach is to propose other fairness metrics as a proxy of statistical parity or equalized odds, e.g., @cite @cite @cite . @cite @cite proposed a covariance-type constraint for statistical parity and equalized odds. The third approach post-processes a baseline classifier by shifting the decision boundary (can be different for different groups), e.g., @cite @cite @cite @cite @cite @cite . These approaches modify the constrained classification problem.
- @cite @cite also provide a general framework to handle multiple fairness constraints. Quadrianto and Sharmanska @cite encode fairness constraints as a distance between the distributions for different values of a single binary sensitive attribute, and then use the privileged learning framework to optimize loss with respect to fairness constraints. While this results in an interesting heuristic, they do not provide theoretical guarantees for their approach. @cite give a method to compute a nearly optimal fair classifier with respect to linear fairness constraints, like demographic parity or equalized odds, by the Lagrangian method. However their framework does not support constraints on predictive parity (see Remark ).
- Another line of research is to pre-process on the training data and achieve an unbiased dataset for learning, e.g., @cite @cite @cite @cite @cite @cite . This approach is quite different from ours since we focus on learning classifiers and investigating the accuracy-fairness tradeoff from the feeding dataset.
- Beyond group fairness, recent works also proposed other fairness definitions concerned in classification. @cite and @cite discussed a notion of that similar individuals should be treated similarly. @cite defined based on the concepts of fair division and envy-freeness in economics. Moreover, Grgi 'c -Hla c @cite @cite discussed that investigates which input features are fair to use in the decision process and how including or excluding the features would affect outcomes. Finally, Chouldechova @cite and @cite investigated the inherent tradeoff between equalized odds and predictive parity (called well-calibrated in their papers).
- To analyze the performance of the mmWave networks, theoretical channel models have been proposed to describe the new propagation characteristics in mmWave bands @cite @cite @cite @cite @cite @cite . In @cite @cite , the authors proposed a LOS ball model to approximate the irregular LOS region, which was shown to be flexible yet accurate enough to capture the features of the blockage effects in mmWave bands by the field measurements @cite . The LOS ball model was further extended to the two-ball-based blockage model in @cite and the multiple-ball-based blockage model in @cite to account for the three different states of each link, i.e., LOS, NLOS and outage. Based on the established theoretical channel models, the network-wide system performance of the mmWave networks was investigated, e.g., in @cite @cite @cite .
- The stochastic geometry framework has been widely adopted to analyze the network-wide performance of cellular networks, including the works in @cite @cite @cite @cite @cite . In @cite @cite , the authors modeled the random locations of the BSs as a homogeneous poisson point process (HPPP). And based on this model, the outage probability of a typical user was derived in single tier cellular network @cite and in multiple tiers heterogeneous networks (HetNets) @cite @cite @cite . Compared with the traditional grid model, using HPPP to model the locations of BSs and mobile users provides analytical tractability while guarantees satisfying accuracy of the analytical results @cite .
- Based on the stochastic geometry model, BS cooperation in conventional microwave networks has already been extensively studied in the past few years @cite @cite @cite @cite @cite @cite @cite @cite .
- There have been several applications of predefined systemic nets to textual prediction problems. For example, Whitelaw @cite show improvement in sentiment analysis using the Appraisal Net mentioned above. Argamon show how to predict personality type from authored text, again using systemic functional ideas @cite . Herke-Couchman and Patrick derive interpersonal distance from systemic network attributes @cite .
- Kappagoda @cite shows that word-function tags can be added to words using conditional random fields, in the same kind of general way that parsers add part-of-speech tags to words. These word-function tags provide hints of the systemic-functional role that words carry. This is limited because there is no hierarchy. Nevertheless, he is able to show that the process of labelling can be partially automated and that the resulting tags aid in understanding documents.
- : Most existing person Re-ID models are supervised, and based on either invariant feature learning @cite , metric learning @cite or deep learning @cite . However, in the practical deployment of Re-ID algorithms in large-scale camera networks, it is usually costly and unpractical to label the massive online surveillance videos to support supervised learning as mentioned in @cite .
- : In order to improve the effectiveness of the Re-ID algorithms towards large-scale unlabeled datasets, some unsupervised Re-ID methods @cite are proposed to learn cross-view identity-specific information from unlabeled datasets. However, due to the lack of the knowledge about identity labels, these unsupervised approaches usually yield much weaker performance compared to supervised learning approaches.
- The notion of abstraction of an MPL system has been first discussed in @cite . The procedure starts by transforming the MPL system characterised by @math into a PWA (piece-wise affine) model [Algorithm 2] Dieky1 , and then considering the partitions associated to the obtained PWA [Algorithm 6] Dieky1 . The abstract states associated to the partitions are represented by DBMs. The transitions are then generated using one-step forward-reachability analysis @cite : first, the image of each abstract state w.r.t. the MPL system is computed; then, each image is intersected with partitions associated to other abstract states; finally, transition relations are defined for each non-empty intersection. This procedure is summarised in [Algorithm 7] Dieky1 .
- The computation of image and of inverse image of a DBM is described in @cite . These computations are used to perform forward and backward reachability analysis, respectively. The worst-case complexity of both procedures is @math , where @math is the number of variables in @math excluding @math . A more detailed explanation about image and inverse image computation of a DBM is in Section 3.3.
- proposed a seminal neural architecture for sequence labeling. It captures word sequence information with a one-layer CNN based on pretrained word embeddings and handcrafted neural features, followed with a CRF output layer. extended this model by integrating character-level CNN features. built a deeper dilated CNN architecture to capture larger local features. was the first to exploit LSTM for sequence labeling. built a BiLSTM-CRF structure, which has been extended by adding character-level LSTM @cite @cite , GRU @cite , and CNN @cite @cite features. proposed a neural reranking model to improve NER models. These models achieve state-of-the-art results in the literature.
- compared several word-based LSTM models for several sequence labeling tasks, reporting the score distributions over multiple runs rather than single value. They investigated the influence of various hyperparameters and configurations. Our work is similar in comparing different neural architectures under unified settings, but differs in four main aspects: 1) Their experiments are based on a BiLSTM with handcrafted word features, while our experiments are based on end-to-end neural models without human knowledge. 2) Their system gives relatively low performances on standard benchmarks Based on their detailed experiment report @cite , the F1-scores on CoNLL 2003 NER task are generally less than 90
- The previous work related to image patch matching can be categorized under three categories: approaches using image intensities, approaches using hand engineered features and approaches using deep learned features. start with using pixel based distance to identify patch correspondence, where @math distance was used to compare pixel values of two patch images. The next stage of this category image patch matching has used normalized correlation between patches to identify their correspondence @cite @cite . Calculating image descriptors such as Scale-invariant feature transform (SIFT) @cite and DAISY @cite and estimating the descriptor distance is the main concept behind the hand engineered feature based methods .
- In the most recent literature, there are two main approaches which have been motivated by the recent advances in neural networks and deep learning @cite @cite . The aim of these are to generate more robust descriptors which can overcome the drawbacks of hand crafted features such that the descriptors and the matching algorithms are not vulnerable to challenging factors in the patches such as illumination changes , occlusions and shadings.
- In the approach suggested by Zagoruyko and Komodakis @cite they have evaluated three main neural network architectures for the patch matching. The architectures they have suggested are 1) Siamese, 2) Pseudo-siamese and 3) 2-channel. The siamese and pseudo-siamese architectures contain two branches and as the input each branch takes one of the two patches to be compared. The output of these two branches are analogous to the feature descriptors in the traditional approaches and the branches are merged at the top to make the comparison. In contrast to the siamese architecture where the weights of the two branches are shared in the pseudo siamese architecture the weights are uncoupled. In 2-channel architecture two patches are considered as a 2 channel image where there is no explicit separation on feature descriptor generation and matching. The evaluations on these architectures have been carried out on UBC dataset @cite . Their evaluations conclude that when the convolution layer was divided into small kernels of size @math , 2 channel architecture performs the best.
- Matchnet @cite is a convolution neural network (CNN) based approach where the architecture consists of two sub components as feature network - to extract the features of the patches and metric network - to model the similarity between the patches. To train the networks they have used cross entropy error. Their evaluations are also based on the standard UBC dataset @cite .
- Coefficients of an encoding mechanism for a set of images such that the accuracy of reconstruction and the sparseness of coefficient are maximized, can possess statistical properties of natural visual scenes @cite @cite . have successfully been used in computer vision applications as well as in signal processing applications in general. While some applications of this concept are image classification @cite , image reconstruction @cite and audio analysis @cite @cite , this concept has not been adopted for patchmatching processes. In the previous approaches of image patch matching visual features have majorly been used. In our approach we use coefficient resulted from over-complete sparse coding of patches (Figure , Figure ) as the feature representation and to estimate the patch similarity we use a neural network while tolerating non linear dependencies in input images.
- Surrogate loss functions for the @math - @math loss have been widely of interest to the machine learning and statistics communities @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . Recently, there has been renewed interest in alternative losses for classification @cite @cite @cite @cite @cite other than the oft-used log-loss. During the inception of the field, convex losses were widely considered optimal @cite @cite @cite @cite . However, more recent works propose the use of non-convex losses as a means to moderate the behavior of an algorithm @cite @cite @cite @cite . In particular, motivated by superior robustness and classification accuracy of non-convex losses, Mei @cite studied the empirical landscape of such functions. Along these lines, Hazan @cite introduce SLQC as a means to study the unimodality of quasi-convex functions and their optimization characteristics. For SLQC functions, they introduce Normalized Gradient Descent algorithm and prove its convergence guarantees. Using their methodology, we consider @math -loss under logistic regression for binary classification and derive intuition about the operating characteristics of @math -loss.
- Our work is similar to @cite , wherein Nguyen present a tunable sigmoid loss which can be made arbitrarily close to the @math - @math loss. In essence, their loss moves from a smooth to non-smooth loss. Our loss is always smooth and moves from convex to quasi-convex. We find that in the setting of deep neural networks, some quasi-convexity of the @math -loss smooths the empirical landscape. In particular, we provide strong experimental evidence for a narrow range of @math to be used in practice (a limit on the amount of convexity and quasi-convexity of the loss), which significantly reduces the range of hyperparameter tuning induced by @math -loss. Increasing the degree of convexity of the optimization landscape is conducive to faster optimization. Hence, our approach could serve as an alternative to other approaches whose objective is to accelerate the optimization process, e.g., the activation function tuning in @cite @cite @cite and references therein.
- For binary classification, where @math , it is common to use classification functions of the form @math such that the classifier, for any given @math , outputs the hypothesis (hard decision) @math @cite @cite @cite @cite @cite . A classification function corresponds to the certainty of an algorithm's prediction (e.g., SVM). Examples of loss functions that act on classification functions include logistic loss, hinge loss, and square loss.
- While the previous result establishes an equivalent margin-based form for @math -loss, our next result establishes some of its basic optimization characteristics. Finally, we conclude this section with another basic property of @math -loss that highlights its suitability for classification. For binary classification and margin-based losses, Bartlett in @cite introduce as a means to compare the performance of a loss function relative to the 0-1 loss. A margin-based loss function @math is classification-calibrated if its minimum conditional risk given @math is attained by a @math such that @math , where @math is the true posterior. Building upon such a result in @cite for @math -loss, the following proposition shows that @math is classification-calibrated for all @math . The proof of Proposition is given in Appendix .
- In the tabular setting, there is a finite number of state-action pairs that can directly define a decreasing function of the true visitation count as the exploration bonus. MBIE-EB @cite adds the square root of counts of state-action pairs as the bonus reward to the augmented Bellman equation for exploring less visited ones with theoretical guarantee.
- In finite MDPs, @math @cite , R-Max @cite and UCRL @cite all make use of state-action counts and are activated by the idea of optimism under uncertainty. @math @cite determines online to choose an efficient learning policy. R-Max @cite assumes the received reward is not in quality area and trains a fictitious model to learn the optimal policy. UCRL @cite chooses an optimistic policy by using upper confidence bounds. Bayesian RL methods maintain a distribution of belief state as the uncertainty over possible MDPs @cite @cite @cite @cite and use counts to explore .
- In the continuous and high-dimensional space, the number of states is too large to be counted. @cite @cite , the exploration bonus reward is designed based on a state pseudo-count quantity, which is estimated from a density model. In the hash-based method @cite , the hash function encodes states to hash codes and then it explores with the reciprocal of visitation as a reward bonus, which performs well on some hard exploration games. Hash-based method is limited by the hash function. Static hashing, using locality-sensitive hashing, is stable but random. Learned hashing, using an autoencoder (AE) to capture the semantic features, updates during the training time. A related work is @cite , which record the number of cluster center and action pairs which used to select an action from the Gibbs distribution given to a state.
- Whole networks have been studied to characterize their learning ability. It has been demonstrated @cite that the same classification networks that successfully generalize meaningful labels are also able to memorize data with meaningless random labels. Conversely, it has been observed @cite that networks are susceptible to being fooled on tiny perturbations of test inputs. These observations have challenged traditional assumptions about what a network learns, and why. Our current work contributes to the effort to characterize the mechanisms deep networks use to achieve generalization.
- Networks have also have been studied at the granularity of individual layers. It has been found that the representations produced by a layer summarize the lessons learned in training in a meaningful way @cite : low-level layers can be used to summarize simpler patterns, and higher-level layers encode more abstract representations. Layers trained on one problem can be reused to transfer knowledge to other problems @cite , with a small amount of additional training. This fact has profound practical importance, reducing the need to build separate large training sets for every problem.
- Finally, the role of individual network units has been studied. It has been found that individual units are sensitive to specific concepts that can be visualized by generating or sampling inputs that maximize unit activations @cite @cite , and that unit directions match meaningful concepts more closely than other random directions in representation space @cite . However, whether meaningful units are helpful or harmful to a network's ability to generalize has been questioned: @cite found that units that are selective to one class do not appear to damage overall classification performance more than other units when removed from a network. Our current work is a further examination of the impact of individual units on generalization accuracy: we ask, what is the role of a single unit in contributing to classification performance, and what characteristics of units are predictive of their contribution?
- Action recognition. Action recognition is an important branch of video related research areas and has been extensively studied. Earlier methods such as improved Dense Trajectory (iDT) @cite @cite mainly adopt hand-crafted features such as HOF, HOG and MBH. In recent years, convolutional networks are widely adopted in many works @cite @cite @cite @cite and have achieved great performance. Typically, two-stream network @cite @cite @cite learns appearance and motion features based on RGB frame and optical flow field separately. C3D network @cite adopts 3D convolutional layers to directly capture both appearance and motion features from raw frames volume. Action recognition models can be used for extracting frame or snippet level visual features in long and untrimmed videos.
- Boundary probabilities are also adopted in LocNet @cite for revising the horizontal and vertical boundaries of existing proposals. Our method differs in (1) BSN aims to generate while LocNet aims to revise proposals and (2) boundary probabilities are calculated repeatedly for all boxes in LocNet but only once for a video in BSN.
- Temporal action detection and proposals. Temporal action detection task aims to detect action instances in untrimmed videos including temporal boundaries and action classes, and can be divided into proposal and classification stages. Most detection methods @cite @cite @cite take these two stages separately, while there is also method @cite @cite taking these two stages jointly. For proposal generation, earlier works @cite @cite @cite directly use sliding windows as proposals. Recently some methods @cite @cite @cite @cite @cite generate proposals with pre-defined temporal durations and intervals, and use multiple methods to evaluate the confidence score of proposals, such as dictionary learning @cite and recurrent neural network @cite . TAG method @cite adopts watershed algorithm to generate proposals with flexible boundaries and durations in fashion, but without proposal-level confidence evaluation for retrieving. In our work, BSN can generate proposals with flexible boundaries meanwhile reliable confidence scores for retrieving.
- Recently temporal action detection method @cite detects action instances based on class-wise start, middle and end probabilities of each location. Our method is superior than @cite in two aspects: (1) BSN evaluates probabilities score using temporal convolution to better capture temporal information and (2) " fashion adopted in BSN brings more precise boundaries and better retrieving quality.
- Recently there are two schemes to train deep models for face recognition: classification and verification. The classification scheme considers each identity as a unique category and classifies each sample into one of the classes. During testing, the classification layer is removed and the top-level feature is regarded as the face representation @cite . The most popular loss is softmax @cite @cite @cite . Based on that, the center loss @cite proposes to learn the class-specific feature centers to make features more compact in the embedding space. The L2-softmax @cite adds a L2-constraint on features to promote the under-represented classes. The normface @cite normalizes both features and prototypes to make the training and testing phases closer. Recently, enhancing margins between different classes is found to be effective in improving feature discrimination, including large-margin softmax @cite , A-softmax @cite , GA-softmax @cite and AM-softmax @cite . Benefiting from the prototypes in the classification layer, the scheme can distinguish a sample from all the other classes, leading to fast convergence and good generalization ability @cite .
- Most contemporary face recognition methods are based on wild datasets, e.g., CASIA-Webface @cite , Ms-Celeb-1M @cite , MF2 @cite and VGG2 @cite . These well-posed datasets have a limited number of identities and sufficient samples per identity. However, this is not the case in IvS datasets. Table gives a brief comparison between wild and IvS datasets. Our CASIA-IvS has more than @math million identities but only two samples per identity, on which existing well-studied methods cannot work well any more. Exploring IvS-specific training strategies is necessary.
- intends to recognize new classes by few samples @cite . Generally, low-shot learning transfers the knowledge from a well-posed source domain to the low-shot target domain. Siamese net @cite trains a siamese CNN by same-or-different classification on the source domain and extracts the deep features for nearest neighbour matching in the target domain. MANN @cite @cite @cite memorizes the features of examples in the source domain to help predict the under-labeled classes. Model regression @cite @cite directly transfers the neural network weights across domains. The L2-regularization on features @cite @cite @cite can prevent the network from ignoring low-shot classes. Besides, virtual sample generation @cite @cite and semi-supervised samples @cite are found effective in promoting low-shot classes. Although both low-shot learning and bisample learning intend to learn a concept with insufficient samples, they differ in that low-shot learning is close-set classification but bisample learning is open-set classification where the testing samples definitely belong to unseen classes.
- refers to the situation that only a limited number of classes appear frequently, while most of the others remain far less existing. Deep models trained on long-tailed data tend to ignore the classes in the tail. To resolve the problem, @cite retrieves more samples from the tail classes. @cite makes samples uniformly distributed by random sampling. @cite proposes a range loss to balance the rich and poor classes, where the largest intra-class distance is reduced and the shortest class-center distance is enlarged.
- Large-scale classification aims to perform classification on a vast number of classes, where the class number reaches millions or tens of millions. This task presents a great problem for deep learning: the common softmax loss can not be adopted due to the prohibitive parameter size and computation cost. The Megaface challenge @cite proposes four methods for training models on @math k identities. Model-A trains the network on random @math identities via softmax. Model-B finetunes Model-A on all the @math k identities with the triplet loss. Model-C adopts rotating softmax that randomly selects @math identities every @math epoches. After each rotation the parameters in the softmax layer are randomly initialized. Model-D further triplet-finetunes Model-C on all the identities.
- Researchers have presented challenges in safety and security against cyber attacks that need to be addressed while designing a CPS , @cite @cite @cite . , @cite explained the integration of IoT and SCADA systems with a focus on security and how to integrate and create intelligent ICS using the Internet. @cite surveyed literature on cyber physical systems security, and presented a orthogonal framework consists of security, components, system perspectives. They focused mainly on four CPS systems such as ICS, smart grids, medical devices, and smart cars.
- Attacks have been modeled as noise in sensor data , @cite . Attack models designed specifically for CPS include a variety of deception attacks including surge, bias, and geometric , @cite . Such models have been used in experiments to understand the effectiveness of statistical techniques in detecting cyber attacks. The attacks designed in this work are based on a cyber-physical attacker model , @cite . @cite proposed a detailed procedure for modeling cyber systems using attack graphs. Such graphs model practical vulnerabilities in distributed networked systems. , @cite have proposed argument graphs as a means to capture the workflow in a CPS. The graphs are intended to assess a system in the presence of an attacker. The graphs are formed based on information in the workflow such as use case or state, physical system topology such as network type, and an attacker model such as an order to interrupt, power supply, physical tampering, network connection, denial of service, etc. Typed graphs , @cite and Bayesian defense graphs , @cite are a few other important contributions to the modeling of cyber attacks.
- Attack detection in water systems: Mitchel and Chen surveyed , @cite intrusion detection techniques for CPS. They presented existing works based on a classification tree. They also presented the advantages and limitations of the techniques. The use of invariants for detecting attacks on CPS has been proposed and evaluated by several researchers such as in , @cite @cite @cite . In this work it is claimed that the use of controlled invariant sets in detecting cyber attacks uses little information about the controller and hence is useful for a large range of control laws. Yuqi et. al. , @cite proposed an approach for learning physical invariants that combine machine learning with ideas from mutation testing. Data driven , @cite @cite approaches for attack detection is studied on a water treatment system.
- Liu focused on energy efficiency by changing the foraging time for each robot @cite . The time spent foraging for resources depended on various cues such as personal successful food retrievals, collisions with teammates for food, and success among other robots in food retrieval. So the method was able to find an adaptable and optimal time for foraging, but did not optimize the amount of energy allocated for it.
- Stirling took a novel approach at finding energy efficient searching algorithms for flying swarms in indoor environments @cite . The strategy involves having robots create a network of beacons that communicate with each other to direct where other robots in the swarm should go. When an exploring robot arrives at an unexplored location, it becomes a beacon to help sense for the other exploring robots. They found that launching the robots incrementally rather than all at once decreased total energy consumption as well as collision rate but increased search time. Their method provides a trade-off between energy consumption and search time. Once again the metric is energy consumption and not energy allocation so it ignores the unused charge in each robot after the task was done.
- Labella took inspiration from ants. He created an adaptation method that controls the number of robots foraging in the environment @cite . Each robot has a probability variable that increases and decreases based on the number of successes and failures the robot had when foraging. The probability variable dictates the probability that the specific robot would leave the nest and start foraging. This allows for robots who consistently find and retrieve prey to keep foraging, while also eventually reaching an equilibrium or state in which the necessary number of robots are active. Thus, it efficiently uses energy when needed, similarly to @cite @cite in that they measure the time it takes to finish foraging but not how much energy was already allocated.
- Neural LMs have also been applied successfully to improve downstream applications @cite @cite @cite @cite . @cite @cite @cite @cite , researchers have shown that pre-trained LMs can be used as feature representations for a sentence, or a paragraph to improve NLP applications such as document classification, machine translation, question answering, etc. The combined evidence suggests that LMs trained on a massive amount of unlabeled data can capture many aspects of natural language and the world's knowledge, especially commonsense information.
- Previous attempts on solving the Winograd Schema Challenge usually involve heavy utilization of annotated knowledge bases, rule-based reasoning, or hand-crafted features @cite @cite @cite . In particular, Rahman and Ng @cite employ human annotators to build more supervised training data. Their model utilizes nearly 70K hand-crafted features, including querying data from Google Search API. @cite rely on a semantic parser to understand the question, query texts through Google Search, and reason on the graph produced by the parser. Similarly, Sch "u ller @cite formalizes the knowledge-graph data structure and a reasoning process based on cognitive linguistics theories. @cite introduces a framework for reasoning, using expensive annotated knowledge bases as axioms.
- Using language models in reading comprehension tests also produced many great successes. Namely @cite used bi-directional RNNs to predict the last word of a passage in the LAMBADA challenge. Similarly, LMs are also used to produce features for a classifier in the Store Close Test 2017, giving best accuracy against other methods @cite . In a broader context, LMs are used to produce good word embeddings, significantly improved a wide variety of downstream tasks, including the general problem of question answering @cite @cite .
- Exploration of unknown environments using autonomous robots has been considered as a fundamental problem in robotics applications such as search and rescue @cite , industrial inspection and 3D modelling. For exploration, the basic requirement for robots is to scan unknown space or detect free space as fast as possible. UGV (unmanned ground vehicle) @cite and UAV (unmanned aerial vehicle) @cite both have been employed for such a task with differences primarily in: (1) UGVs are more payload-capable. A ground vehicle can carry heavy, long-range laser scanners which are inapplicable for weight-constrained UAVs; (2) UAVs have superiors mobility and agility. UAV can fly above obstacles and cover areas that are inaccessible to UGVs, like obstacle's top surfaces. Consequently, a UGV often enjoys a larger sensor-coverage, yet cluttered and view-blocking environments could hamper its performance; on the other hand, a UAV may deliver inferior exploration efficiency due to its short-range sensor, but enjoys unblocked downward-looking view. Therefore, UGV favors open areas while UAV prefers cluttered places.
- For robotics exploration, @cite first proposed the concept of frontier, which is defined as unknown grid-map cells adjacent to free ones and thus represents accessible new information. Harmonic function, the solution to @math , is used to plan path to frontiers @cite . This method generates a scalar field in free-space based on its surrounding boundary conditions (occupied cells and frontier cells) and obtains the path using gradient-descent. For air-ground exploration, @cite uses the UAV as an back-up instead of an independent explorer. It is only deployed when UGV encounters high, invisible areas. @cite is also proposed based on the same spirit that one vehicle helps another, failing to exploit both vehicles' full potential. Compared to these works, the collaborative system proposed in this paper fully utilizes advantages of different vehicles and thus results in a more efficient exploration. We summarize our contribution as: 1. An efficient exploration framework that combines UAV and UGV's advantages. 2. A more efficient computation method of harmonic function for robotic exploration tasks. 3. Integration of the proposed collaborative exploration framework with the state estimation, sensor fusion and trajectory optimization. Extensive field experiments are presented to validate the efficiency and robustness of the proposed method.
- Stochastic gradient descent (SGD) @cite was introduced by where as Online convex optimization and the associated projected gradient (OGD) @cite were introduced by Zinkevich . derived accelerated versions of SGD and OGD; they refer to these algorithms as Stochastic Accelerated GradiEnt (SAGE) @cite . Recently, proposed stochastic average gradient (SAG) @cite and Johnson and Zhang proposed Stochastic variation reduced gradient (SVRG) @cite , both aimed at improving the convergence rate of SGD. @cite and McMahan and Streeter studied delay tolerant OGD algorithms @cite where parameter updates are based on stale gradient information. Cyclic block coordinate descent algorithms were introduced by Luo and Tseng @cite @cite . Nesterov proposed randomized block coordinate descent (RBCD) @cite algorithms for large scale optimization problems. Fercoq and Richtarik @cite and @cite proposed accelerated randomized block coordinate algorithms. More recently, Allen- @cite proposed faster accelerated coordinate descent methods in which sampling frequencies depend on coordinate wise smoothness parameters (i.e., Lipschitz parameters of the corresponding partial derivatives).
- More recently, there has been interest in machine learning settings in which training data and features are distributed across nodes of a computing cluster, or more generally, of a network. Nathan and Klabjan @cite proposed an algorithm where nodes parallelly update (possibly overlapping) blocks of feature parameters based on locally available data samples; this was seen as a combination of SVRG and block coordinate descent. @cite studied algorithms for nodes connected through a network; this scenario was referred to as federated learning.
- Some approaches to counteract this phenomena were proposed: recommended using spherical interpolations to avoid traversing unlikely regions; @cite suggest normalizing the norms of the points along the interpolation to match the prior distribution; propose using a modified prior distribution, which saturates the origin of the latent space. gives an interesting discussion on latent space traversal using the theory of Riemannian spaces.
- Second, among other results, also examined optimizing logistic regression with SGD on a fixed dataset using random sampling with replacement, iterate averaging and a vanishing learning rate. There, in Theorems 3.2 and 3.3, it is shown that the expectation of the loss converges as @math and the expectation of the averaged iterates converges in the norm as @math , which is slower than our result. Thus, in contrast to both works @cite @cite , we did not assume iterate averaging or decreasing learning rate. Additionally, our new results on sampling with replacement give a linear relationship between the learning rate and the minibatch size, and Corollary shows the affect of the minibatch size on the asymptotic convergence rate.
- Pregel @cite divide the graph by hashing the vertex id which ensure loading balancing. Yet Pregel uses message communication paradigm, messages that need to be processed will be huge when vertices has many adjacent points. Coincidentally, Pregel performs inefficiency in power-law graph and only allows global synchronization. X-Pregel @cite optimizes Pregel's messaging mechanism by reducing the number of messages that needed to be delivered in every iteration. Giraph @cite adds more features compare to Pregel, including master computation, out-of-core computation, etc. But the poor locality of data access limits its effective.
- GraphChi @cite is a vertex-centric graph processing system and improve IO access efficiency by parallel Sliding Window processing strategy. But the outgoing edges of all vertices have to be loaded into memory before computation, resulting in unnecessary transfer of disk data. Also, all memory blocks have to be scanned when accessing neighboring vertices, which lead to inefficient graph traversal. TurboGraph @cite proposed a Pin-And-Slide model to solve this problem. PAS has no delay in dealing with local graph data, but only applies to some specific parallel algorithms. Compare to the two above, VENUS @cite expands to nearly every algorithm and enables streamlined processing which performs computation while the data is streaming in. Moreover, it uses a fixed buffer to cache the v-shard, which can reduce random IO.
- GridGraph @cite uses a 2-level Hierarchical Partitioning scheme to reduce the amount of data transfer, enable streamlined disk access, and maintain locality. But it requires more disk data transfer using TurboGraph-like updating strategy. Besides, it cannot fully utilize the parallelism of multi-thread CPU without sorted edges. NXgraph @cite propose the Destination-Sorted Sub-Shard (DSSS) structure to store graph with three updating strategies: SPU, DPU and MPU. it adaptively choose suitable one to fully utilize the memory space and reduce the amount of data transfer. It achieves higher locality than v-shards in VENUS @cite and reduces the amount of data transfer and enables streamlined disk access pattern. Mosaic @cite combines fast host processors for concentrated memory-intensive operations, with coprocessors for compute and I O intensive components.
- @cite considered asymptotic existence and nonexistence of envy-free allocations. They showed that under additive utilities, envy-free allocations are unlikely to exist even when the number of goods is larger than the number of agents by a linear fraction. On the other hand, they proved that when the number of goods is larger than the number of agents by a logarithmic factor, such allocations are likely to exist under certain technical conditions on the probability distribution. @cite and @cite considered allocations that gives each agent a maximin share guarantee and showed that such allocations exist with high probability when utilities are additive. Asymptotic statements have been considered in other areas of economics as well. For instance, Manea @cite established that the allocation obtained by the random serial dictatorship mechanism is ordinally inefficient with high probability when agents' preference profiles are drawn at random. Incentives and stability in large matching markets have also been considered in the literature @cite @cite .
- We now make some comments on the relation between our results and those of @cite concerning envy-free allocations. As we will later elaborate, proportional fairness is a weaker notion of fairness than envy-freeness, i.e., envy-free allocations are also proportionally fair when utilities are additive. showed that under additive utilities, envy-free allocations are unlikely to exist when the number of goods is larger than the number of agents by a linear fraction. Theorem contrasts that result by showing that proportionally fair allocations are likely to exist even when the number of goods is the same as the number of agents.
- Some work in music generation has been focused on specific domains. One such popular domain for polyphonic music generation is Bach chorales: DeepBach @cite , CoCoNet @cite , and BachBot @cite are all different generative models for polyphonic Bach chorales that can respond to user input. In contrast, our model presented in this paper works simultaneously across multiple Western music styles including classical, jazz, and pop rock; essentially any music expressible with MIDI notes and program changes is compatible. Because of its latent space representation, our model is also able to interpolate between these different domains.
- Other recent systems for generating polyphonic music include JamBot @cite which generates chords then notes in a two-step process, DeepJ @cite which generates polyphonic piano music where a user can control several style parameters, a model from @cite that generates variations on lead sheets, and Song from PI @cite which generates melody, chord, and drum tracks using a hierarchical recurrent network combined with hand-engineered features.
- Perhaps the most similar systems to our current work are MusicVAE from @cite (which we directly extend) and MuseGAN from @cite (which builds upon the work of @cite ). MuseGAN @cite is based on generative adversarial networks (GANs) and is capable of modeling multiple instruments over multiple bars. Like our work, the system uses a latent space shared across tracks to handle interdependencies between instruments. However, in MuseGAN the set of instruments is a fixed quintet consisting of bass, drums, guitar, piano, and strings, whereas our system handles arbitrary instrument combinations. Separately, MuseGAN is focused on accompaniment and generation and is unable to represent or manipulate preexisting music. Our system can also generate from scratch, but in contrast with MuseGAN, it can also facilitate user-driven manipulation of existing music via the latent space.
- The MusicVAE architecture introduced by @cite is able to learn a latent space of musical sequences using a novel hierarchical decoder that allows it to model long-term structure and multi-instrument sequences. However, this work applies strict constraints to the sequences to reach its goals. In order to guarantee a constant number of events per measure, non-drum tracks are limited to monophonic sequences, and all tracks are represented with a single velocity and quantized at the level of 16 th notes. This reduces the challenge of modeling longer-term structure, but at the expense of expressiveness. Furthermore, while the trio'' MusicVAE is capable of modeling three broad instrument classes--melody, bass, and drums--it is limited to exactly three instruments, arbitrarily excluding potentially pivotal voices and disregarding the specific identity of each instrument (e.g. eletric guitar and piano are both considered melody'' instruments). Further, do not consider modeling fine-grained timing and velocity, nor do they develop a method for chord conditioning. Nevertheless, the MusicVAE architecture and its implementation provide a powerful basis for exploring a more expressive and complete multitrack latent space, and thus we position our work as an extension of it.
- In order to apply dictionary learning on audio data, it is helpful to regard a time-frequency representation which subdivides the problem into smaller time frames and highlights the frequency characteristics of the signal. Classically, such a representation is computed via application of the short-time Fourier transform (STFT) (cf. @cite ) on the audio signal, and the resulting spectrogram (i.e., the magnitude of the STFT) is then decomposed via non-negative matrix factorization (NMF) @cite in order to obtain a dictionary. This approach was initially studied by Smaragdis and Brown @cite for the purpose of polyphonic music transcription and then applied to audio source separation by Wang and Plumbley @cite .
- In many cases, a single musical instrument can generate different sounds which are perceptually similar but only vary in the pitch of the tones. In the STFT spectrogram, different pitch manifests in linear scaling of the distances between the peaks in the frequency axis, which is computationally hard to handle. Therefore, @cite applied the constant-Q transform (CQT) @cite , which turns scaling into shifts, and developed the shifted non-negative matrix factorization (SNMF), that is actually a tensor factorization, in order to train a shift-invariant dictionary. This approach was later refined by @cite @cite @cite .
- While the constant-Q transform ensures the shift-invariance of patterns of sinusoids when varying the pitch, its transient response varies with frequency. To overcome this, the scattering transform by Andn and Mallat @cite subsequently employs smoothing in the time domain.
- Another approach is the computation of the Mel spectrogram @cite , which applies logarithmically spaced windows in the frequency direction. This, however, unfavorably amplifies transients in the higher frequencies.
- The Heisenberg uncertainty principle (cf. @cite @cite ) sets a fundamental bound to the time-frequency resolution achievable. If we use Gaussian windows where @math is the standard deviation of the window in the time domain and @math is the standard deviation of its Fourier transform, then we have @math . If, for instance, @math , then @math . For speech processing, this may be sufficient, but considering music that contains frequencies as low as @math , this deviation equals almost half an octave (or a tritone).
- We were informed of a forthcoming paper @cite by Coudron and Slofstra that establishes a result similar (though strictly incomparable) to Theorem , using completely different techniques. In particular, the authors show that distinguishing between entangled value @math or @math for games with provers in the commuting operator model is hard for nondeterministic @math time (whereas our result shows hardness for nondeterministic @math time for games with @math provers in the tensor product model). This result relies on the group-theoretic framework that was pioneered in @cite @cite .
- As we noted above, most studies that examine misinformation spread focus on individual events such as natural disasters @cite , political elections @cite , or crises @cite and examine the response to the event on social media. A recent study by Vosoughi al found that news stories that were fact-checked and found to be false spread faster and to more people than news items found to be true. In contrast, our methodology considers immediate reactions to news of varying credibility, so we can determine whether certain reactions or reactions to trusted or deceptive news sources evoke more or faster responses from social media users.
- Recently, domain adaptation methods inspired by generative adversarial networks (GAN) @cite have gained popularity. For example, @cite added a domain discriminator to enforce domain invariance of the extracted features. It can be seen as two agents competing in a minmax game. One agent is trying to minimize classification error and maximize discriminator domain confusion. Another agent is trying to minimize discriminator domain confusion. Work by @cite use this technique to overcome different outdoor lighting conditions between source and destination domains. @cite add feature augmentation on top of the domain discriminator. GAN inspired methods differ from our approach, as our approach which does not involve a discriminator and or generator. GANs are also notoriously hard to train. Despite the development of techniques to improve the training process, gradient explosion and mode collapse are just a few of the many common practical issues faced when trying to train a GAN-based model.
- Recent surveys @cite , @cite , @cite may provide additional background on domain adaptation and transfer learning.
- Learning under Privileged Information (LUPI) is initially proposed by Vapnik and Vashist @cite @cite . It extends the Support Vector Machine (SVM) by empirically estimating the slack values via privileged information. This method is further applied to various computer vision problems @cite @cite @cite as well as ranking @cite , clustering @cite and metric learning @cite problems. These method are based on max-margin learning and are not applicable to CNNs or RNNs.
- One closely related work is @cite , extending Gaussian processes to the LUPI paradigm. Hern ' a ndez-Lobato @cite use privileged information to estimate the variance of the noise in their model. Similarly, we use the privileged information to control the variance of the dropout in CNN and RNN models. However, their method only applies to Gaussian processes, whereas we target neural networks.
- The LUPI paradigm has also been studied recently in the context of CNNs. In contrast to max-margin methods, the literature on learning CNNs under privileged information heavily uses the distillation framework, following the close relationship between distillation and LUPI studied in @cite .
- Hoffman demonstrated a multi-modal distillation approach to incorporating an additional modality as side information @cite . They start with a pre-trained network and distill the information from the privileged network to a main neural network in an end-to-end fashion.
- We use multiplicative Gaussian dropout instead of Bernoulli dropout. Gaussian dropout is first introduced in @cite . Its variational extension @cite uses local re-parameterization to perform Bayesian learning.
- The Information Bottleneck (IB) @cite is a powerful framework which can enforce various structural assumptions. The IB framework has been applied to CNNs and RNNs using stochastic gradient variational Bayes and the re-parametrization trick @cite . Perhaps closest to our method, Achille and Soatto @cite use the information bottleneck principle to learn disentangled representations when a CNN with Gaussian Dropout is used. The authors introduce many ideas upon which we build; specifically, our hypothesis class (Eqn. 4) is very similar to the architecture they propose. The main architectural difference is their choice to define the variance as a function of @math , whereas we make it a function of @math . We also use similar distributional priors and a similar training procedure. On the other hand, we apply these ideas to a completely different problem with a different theoretical analysis. The information bottleneck has been applied to LUPI for SVMs @cite . However, this method does not apply to neural networks.
- Although we use IB @cite , Gaussian dropout @cite and the re-parametrization trick @cite , we are the first to our knowledge to apply any of these methods to the LUPI problem.
- There has been extensive work on predicting the ontological types of entities in large knowledge graphs @cite @cite @cite @cite , in semi-structured resources such as Wikipedia @cite @cite , as well as in text @cite @cite @cite . However, the major shortcoming of these sorts of methods, including those aiming at more fine-grained typing, is that they assume that the set of candidate types is given as input, and the main remaining challenge is to pick the correct one(s). In contrast, our work yields descriptions that often indicate the type of entity, but typically are more natural-sounding and descriptive (e.g. ) than the oftentimes abstract ontological types (such as or ) chosen by type prediction methods.
- A separate, long-running series of work has obtained open vocabulary type predictions for named entities and concepts mentioned in text @cite @cite , possibly also inducing taxonomies from them @cite @cite @cite . However, these methods typically just need to select existing spans of text from the input as the output description.
- Generating entity descriptions is related to the task of text summarization. Most traditional work in this area was extractive in nature, i.e. it selects the most salient sentences from a given input text and concatenates them to form a shorter summary or presents them differently to the user @cite . Abstractive summarization goes beyond this in generating new text not necessarily encountered in the input, as is typically necessary in our setting. The surge of sequence-to-sequence modeling of text via LSTMs naturally extends to the task of abstractive summarization by training a model to accept a longer sequence as input and learning to generate a shorter compressed sequence as a summary.
- Generative adverserial networks or GAN for short, is well known for generating realistic samples through mapping latent manifolds with noise and conditionals inputed into generator(s).Many prior works has illustrates successful attempts to disentangle how GAN encode features into latent space and conditional GANs(cGAN) is a particular class of GANs that are proven to control output contents through concatenating tractable conditionals with selected noise. In the conditional setting, a lot of previous work has been done to explore possibilities to control samples generated from GANs through encoded conditionals @cite @cite @cite .Here in particular, Zhu et. al. @cite proposed a way to transform style of image from one domain to another through a cycle encoding and verification structure and has achieved decent results. We view the BiGAN proposed by Donahue ei.al. @cite as a concurrent work of ours in that it also explores the concept of conditional noise mapping. It would be interesting to see if the model can be adaptable to the setting not being trained end-to-end. The notion of approximating an implicit distribution is not restricted to GAN alone and many works have focused on incorporating adversarial loss to achieve more flexible latent representation approximation @cite @cite @cite .
- Autoencoder(AE) is another class of deep generative model that compiles inputs and reconstruct samples through encoding features in latent space. Recent work done by Liu et.al. @cite has shown that VAE can perform well on matching the latent space between images from two domains. The added adversarial loss facilitates the training of the VAE by preventing it from mode collapsing.
- Graph signal processing refers to methods that denoise, localize, detect, and predict signals over graphs. For example, each vertex corresponds to a low-powered sensor, and we would like to denoise sensor measurements, and we use the graph structure is based on communication between the sensors or spatial proximity. The driving assumption is that there is some underlying signal that in some way respects' the graph topology, and specifies the distribution of the observations. Many of the tools in signal processing and supervised learning can be extended to the graph case, such as Fourier analysis @cite @cite , wavelets @cite @cite @cite , graph kernels @cite , and convolutional networks @cite @cite . Graph structure has previously been used in matrix completion and network denoising problems (see for example, @cite @cite @cite @cite ), but these methods require some predetermined graph structure, such as knowledge graphs, so are not well suited to estimating graphons, and they do not perform segmentation, which is the focus of this work.
- There is an extensive body of literature on solving the fused lasso, . Algorithms for solving the fused lasso can be divided into two categories: solvers for a fixed @math , and path algorithms that find the solution for every @math within a range. The fused lasso for a fixed @math has a quadratic program dual form, and some popular algorithms for this are the projected Newton algorithm of @cite @cite , first-order primal-dual algorithm @cite , and split-Bregman iteration @cite . Some path algorithms include the generalized lasso path algorithm of @cite , and a max-flow version for the fused lasso in @cite . If applied directly to C2-power graphs, these methods would have computational and memory complexity that scale with the number of dyads, @math .
- We next turn our attention to graphon estimation using the PGFL on a learned graph. The statistical limits of graphon estimation have been well characterized for smooth graphons, and it was found that computationally intractable profile likelihood maximization is minimax optimal for H "older graphons, @cite @cite . One tractable approach to graphon estimation is to order the vertices according to some graph statistics, such as the degrees of @math , and then treat the resulting re-ordered matrix @math as an image and applying image segmentation tools ( @math is the permutation associated with this sorting). This methodology is called sorting and smoothing (SAS), and in @cite they use TV denoising to perform the image segmentation. The implicit assumption is that the degree is a decent proxy for the latent variable, @math , which does not hold for most graphons.
- Another related approach to segment the dyads is to group the vertices via a community detection method. The stochastic block model is a special instance of the graphon model which assumes that there are latent communities for the vertices and the probability of attachment between two vertices is a function only of the communities to which the vertices belong. This can be thought of as segmenting the dyads by taking the Cartesian product of the vertex communities, but this type of segmentation is restrictive because of this specialized structure. Heuristic or greedy methods for fitting the SBM for graphon estimation have been proposed in @cite @cite , but little is known about the statistical performance and whether these can achieve minimax performance. In another approach, @cite proposed a spectral method that thresholds singular values and provided some MSE consistency guarantees. Currently, the best rate guarantee for a computationally tractable estimator of Lipschitz graphons is achieved by the aforementioned neighborhood smoothing method of @cite , and the MSE scales like @math , which is significantly worse than the minimax rate of @math .
- For example, @cite show that the deployment of Secure-BGP can be facilitated by a pair of mechanisms, namely, (i) routing policies that prefer Internet paths safeguarded by Secure-BGP in partial deployment and (ii) offloading cryptography from autonomous systems without a customer (known as stub autonomous systems) to their providers. These techniques improve the incremental deployability of Secure-BGP, however, the incentive structure these techniques induce remains akin to the stag hunt, and, in fact, Gill propose concentrating peer pressure and regulatory efforts to a small fraction of core autonomous systems for their techniques to be effective in driving the growth of Secure-BGP. Our point, therefore, that, although such line of effort is certainly helpful to the adoption of emerging technologies it is not in general conclusive, remains.
- The problem of diffusing innovation in social systems has been the subject of extensive scrutiny primarily by social scientists. The study of diffusion of technology falls squarely in the ballpark of social science: Quoting @cite , The people who have thought hardest about the general questions of technology have mostly been social scientists and philosophers, and understandably they have tended to view technology from the outside as stand-alone objects. 0.2 mm Seeing technology this way, from the outside, works well enough if we want to know how technologies enter the economy and spread within it.'' But computer scientists have also been involved with the diffusion of innovation from an algorithmic perspective. The term innovation assumes various interpretations in the literature from agricultural practices, social norms (such as which hand to extent in a handshake), medical drugs, commercial products (such as fax machines and cellphones), to networking technologies such as secure versions of BGP and quality-of-service capabilities in IP networks.
- Related to the problem of diffusing innovation is that of inciting collective action (in the sense that the decision to act in a particular fashion diffuses in a social system) a problem whose study was initiated in the seminal work of @cite . The study of collective action was later taken up by several authors such as, for example, in the form of critical mass theories (bearing relevance to nuclear fission explosions in physics) . @cite motivates the study of collective action in settings bearing a political nature related to citizen oppression in rogue states, however, models of collective action range from crowdsourcing in the Internet to enabling countermeasures against climate change (such as controlling carbon emissions). Although various authors discuss the problem of inciting collective action through incentives mechanisms, the idea of creating institutions to that effect (as we do in this paper) and proving their efficacy has not, to the extent of our knowledge, been considered before by these earlier works. (An exception is where the analysis is at an elementary stage.)
- The archetypical model to study the evolution of cooperation in society is the prisoner's dilemma. The stag hunt is the archetypical model for the study of coordination. But as @cite notes the models are related: Iterated prisoner's dilemma with an infinite number of stages can assume the form of a stag hunt. The literature on the game-theoretic study of cooperation and coordination (starting with ) has been concerned with the emergence of cooperative phenomena without a centralized entity to facilitate their manifestation. For example, @cite studies how signaling systems can facilitate coordination. In this paper, we study the emergence of cooperation with the assistance of an exterior to the players entity that is able to enforce commitments but our analysis suggests that players may coordination while altogether eschewing commitments.
- In related work, @cite introduce a method for finding unknown unknowns in discriminative classifiers. Data points are clustered in an unsupervised manner followed by a multi-arm bandit algorithm (each cluster is an arm) for efficiently finding regions of the feature space where the classifier is most likely to make mistakes. It is not straightforward to apply this approach to RL because examples (states) are no longer i.i.d. as in supervised learning. In RL, states are visited according to a distribution induced by either executing the learned policy or that induced by an optimal oracle. There can also be multiple labels (actions) that are acceptable for each state, rather than a single correct" label. Finally, certain mistakes in the real-world can be catastrophic, which requires risk-sensitive classification to prioritize identifying rare blind spot states @cite .
- Anomaly detection @cite is related but not directly applicable, as blind spots are not rare instances. Instead, they are regions of the state space where the training environment does not match the testing environment, and we learn to efficiently identify these regions through oracle feedback.
- Many approaches improve transfer of information across tasks @cite @cite @cite @cite , as tasks cannot be learned from scratch each time. Much of this literature has focused on learning mappings between state and action spaces to enable Q-value function or policy transfer. Several works have also considered hierarchical approaches to RL that involve transferring subtasks across domains @cite . In distinction to transfer learning, where the labels (actions) of examples (states) may change, domain adaptation deals with situations where the distribution of examples changes from one domain to another @cite @cite . Our work differs from these in that we relax the assumption that the agent's state representation is complete and sufficient to learn autonomously.
- The key challenge of few-shot learning is to use primary knowledge obtained through original training data to make predictions about unseen classes of data with a limited number of available samples. Following the long history of research on few-shot learning approaches, the first work to leverage modern machine learning for one-shot learning was proposed by @cite . In recent years, the work in @cite and @cite have established two standard benchmarks, Omniglot and Mini-ImageNet respectively, to compare few-shot learning approaches in terms of accuracy. @cite leverages a Bayesian model while the authors of @cite utilized a Siamese network which learns pairwise similarity metrics to generalize the predictive power of the model to new classes. These works were followed by other pairwise similarity-based few-shot learning approaches in @cite @cite @cite .
- From a different perspective, few-shot learning through combining graph-based analytics with deep learning has been proposed in @cite . In a separate trend of work, meta-learners @cite @cite @cite are developed to generalize the DNN model to new related tasks. The aforementioned works have incrementally increased the accuracy on few-shot learning benchmarks. However, all these works are negligent to the model accuracy on old classes. Therefore, their proposal can degrade the predictive power of the model on old data. Additionally, many of the aforementioned approaches incur a high computation cost to adapt the model and thus are not amenable to resource-constrained settings. preserves the prior knowledge of the model on old data while outperforming all state-of-the-art approaches in terms of few-shot learning accuracy. Additionally, lightweight model updates of complies with stringent limitations of edge devices.
- @cite @cite @cite @cite are only few of many studies over several years stating that current interface designs on mobile devices are not suitable for one handed usage. Studies such as @cite @cite determine the functional area of the thumb, which quantifies the struggles that users face when operating these ill-designed interfaces.
- Researchers have tried to identify which hand is holding the phone with the help of additional sensors or equipment. @cite @cite @cite @cite @cite used capacitive sensors along the sides of the phone to detect grip while @cite used additional accelerometers. @cite on the other hand used the position of the fingers on the back of the phone to determine the finger interacting with the phone. This method also used additional capacitive sensors.
- : In addressing the optimization problems, SGD (Stochastic Gradient Descent) @cite with sound theoretic foundations has been one of the most frequently used algorithms. Given an objective function @math , SGD adopts an iterative updating schema to learn the model variables as follows: Here, the parameter @math is usually a small constant, which is referred to as the learning rate. Meanwhile, to further accelerate the learning process, a variant of SGD, namely SGDM (SGD with Momentum) @cite , has been proposed, which updates the variables according to the following equations In the equation, @math is a momentum parameter and vector @math is initialized with value @math .
- SGD and SGDM scale the gradient uniformly in all directions (i.e., all the variables), which makes the tuning of learning rate @math very tedious and laborious. To resolve such a problem, several adaptive optimization methods have been proposed, including Adam @cite , RMSprop @cite and Adagrad @cite , which uses distinct learning rates for different variables. Formally, the variable updating equations adopted in Adagrad @cite and RMSprop @cite can be represented as follows respectively: In Adagrad, vector @math increases monotonically as the update iteration continues, and its scaling factor will keep decreasing when updating vector @math . RMSprop addresses the problem by employing an average scale instead of a cumulative scale, and Adam @cite resolves such a problem with a bias correction, which adopts an exponential moving average for the step in lieu of the gradient.
- Previous work has grounded natural language navigational commands to executable representations. Graphical model-based approaches using syntactic parses have been applied to controlling robotic forklift actions @cite and mobile navigation in novel environments @cite @cite . Others have utilized CCG semantic parsing of robotic commands in synthetic environments @cite , and with weak supervision @cite . Howard (2014) @cite parsed natural language to constraints in trajectory space in order to reduce the search space of their graphical model that grounds language to instructions. Park (2017) @cite used a similar approach, grounding instructional language to a probabilistic graphical model, though they address the manipulation domain and learn soft cost functions to avoid dynamically-specified regions (e.g., don't put it there'').
- Language interaction provides useful information in solving many of the challenges in shared autonomy, which include how to correctly and accurately identify a human's intent through interaction @cite and observation @cite . Low-level control strategies from humans have also been successfully integrated and applied to search terrain @cite . Recent research aggressively incorporates high-level human information in shared autonomy @cite @cite . Language, which is a direct and natural way for a human to share information in collaboration, has been widely researched within the supervisor" paradigm @cite @cite @cite where only a goal is provided or where the human acts as a programmer" @cite @cite @cite that instructs the agent in tedious detail. Balancing these roles and investigating user preferences is still an open challenge for deploying communication in shared autonomy.
- The essence of deep learning is to compute hierarchical features or representations of the observational data @cite @cite . With the surge of deep learning research and applications in recent years, lots of research works have appeared to apply the deep learning methods, like deep belief network @cite , deep Boltzmann machine @cite , deep neural network @cite @cite and deep autoencoder model @cite , in various applications, like speech and audio processing @cite @cite , language modeling and processing @cite @cite , information retrieval @cite @cite , objective recognition and computer vision @cite , as well as multimodal and multi-task learning @cite @cite . Traditional deep learning models have too many disadvantages, introduce the deep forest as an alternative approach in @cite .
- In recent years, many research works propose to embed network data into a low-dimensional feature space, i.e., the network embedding problem, in which nodes are represented as feature vectors. In graphs, the relation can be treated as a translation of the entities, and many translation based embedding models have been proposed, like TransE @cite , TransH @cite and TransR @cite . At the same time, many network embedding works based on random walk model and deep learning models have been introduced, like Deepwalk @cite , LINE @cite , node2vec @cite , HNE @cite and DNE @cite . extends the word2vec model @cite to the network scenario and introduce the Deepwalk algorithm @cite . @cite propose to embed the networks with LINE algorithm, which can preserve both the local and global network structures. @cite introduce a flexible notion of a node's network neighborhood and design a biased random walk procedure to sample the neighbors.
- The problem we are interested in is that of finding anomalous sequences (traces) within a large database of discrete multivariate sequences. Different techniques have been proposed to solve this problem both in the anomaly detection field @cite @cite @cite @cite , as in the process mining field @cite @cite . Some of these techniques use signatures of known anomalies that can occur in the system. It is clear that these systems cannot recognize a new type of anomaly and are too limited for our purpose. We are interested in techniques that build a model, such as Markov Chains that represent normal behavior of a system.
- Recent advances in Generative Adversarial Networks (GANs) @cite have proven GANs as a powerful framework for learning complex data distributions. The core idea is to define the generator and the discriminator to be the minimax game players competing with each other to push the generator to produce high quality data to fool the discriminator.
- Mirza & Osindero introduced conditional GANs @cite to control the data generation by setting conditional constraints on the model. InfoGAN @cite , another information-theoretic extension to the GAN model, maximizes the mutual information between a small subset of the latent variables and the observations to learn interpretable and meaningful hidden representations on image datasets. SeqGAN @cite models the data generator as a stochastic policy in reinforcement learning and uses the policy gradient to guide the learning process bypassing the generator differentiation problem for discrete data output.
- Despite their successes, GANs are notably difficulty to train and prone to mode collapse @cite , especially for discrete data. Energy-based GAN (EBGAN) @cite tries to achieve a stable training process by viewing the discriminator as an energy function that attributes low energies to the regions near the data manifold and higher energies to other regions. However, EBGANs, which regularize the distribution distance as Jensen-Shannon (JS) divergence, share the same problem as classical GANs that the discriminator cannot be trained well enough, as the distance EBGANS adopted cannot offer perfect gradients. Replacing JS with the Earth Mover (EM) distance, Wasserstein-GAN @cite theoretically and experimentally solves the problem of model fragility.
- GANs are successfully applied in the field of computer vision for tasks including generating sample images. However, there are few attempts to apply GANs to other machine learning tasks. Recently, IRGAN @cite has been proposed as an information retrieval model in which the generator focuses on predicting relevant documents given a query and the discriminator focuses on distinguish whether the generated documents are relevant. It showed superior performance over the state-of-the-art information retrieval approaches.
- The software system @cite @cite @cite was developed by Galois inc. In 2010 there appeared the system @cite . Approximately at this time we started the development of the software system, which we describe in the present paper ( was first mentioned in papers in Russian in 2011). In 2012 there appeared the system @cite , aimed at reducing to SAT various constraint programming problems. It can be applied to construct encodings for cryptographic functions as well. Note that , and can encode to SAT algorithmic descriptions of a very wide class of functions working with binary data. Meanwhile, is designed to work only with keystream generators based on shift registers. We considered the pros and cons of all mentioned systems in detail in Section 4.
- In the basis of , and other systems lies the ideology of symbolic execution of a program specifying a considered function. Note that the idea to transform programs to Boolean formulas was first proposed by S.A. Cook in his paper @cite which led to the creation and development of the theory of NP-completeness. The notion "Symbolic Execution" first appeared in the paper @cite by J.C. King, where it is defined as a process of interpretation of a program in a special extended semantics, within the context of which it is allowed to take symbols as input and put formulas as output. Currently, symbolic execution using SAT is actively used in software verification (see for example @cite ).
- As we mentioned above, SAT-based cryptanalysis is still actively developing. The cryptographic attacks that employ SAT-solvers show very good results for a number of keystream generators: Geffe, Wolfram (present paper), Crypto-1, Hitag2 (see @cite ). In @cite there was described a successful SAT-based attack on the widely known A5 1 cryptographic keystream generator, which has been used in GSM networks to cipher traffic. Later several dozen cryptanalysis instances for this cipher were solved in the SAT@home volunteer computing project @cite . This result together with other attacks on A5 1 (see @cite @cite @cite ) provides an exhaustive argument towards not using A5 1 any more. The Bivium keystream cipher @cite is a popular object of algebraic and SAT-based cryptanalysis @cite @cite @cite @cite . In @cite there was constructed a SAT-based guess-and-determine attack on Bivium the estimated runtime of which is realistic for modern distributed computing systems. In @cite there were described SAT-based guess-and-determine attacks on several variants of the alternating step generator.
- As we already noted, @cite was the first paper to demonstrate the applicability of SAT-based cryptanalysis to relevant cryptographic algorithms. In that paper using the solver @cite it was possible to quite effectively find single-block collisions for MD4. Using new propositional encoding methods (in particular, the system) and state-of-the-art SAT solvers it is possible to find preimages for MD4 and MD5 several hundred times faster than it was done in @cite . Nevertheless, on the current stage SAT-based cryptanalysis is less effective than specialized methods (see, for example @cite @cite @cite ) on problems of finding collisions of cryptographic hash functions. However, as far as we know, it is the SAT-based approach that yields best known preimage attacks on truncated variants of hash functions @cite @cite @cite . In application to MD4-39 for a long time the SAT-based preimage attack from @cite was considered to be the best. In @cite we significantly improved the results from @cite . It was possible for the large part thanks to functional capabilities of the system.
- There is abundant research work on aspect based sentiment analysis. Actually, the name ABSA is used to describe two different subtasks in the literature. We classify the existing work into two main categories based on the descriptions of sentiment analysis tasks in SemEval 2014 Task 4 @cite : Aspect-Term Sentiment Analysis and Aspect-Category Sentiment Analysis.
- It is paradoxical to note that, while the question of the influence of aggregation on the properties of the formed graph series is largely ignored in most of the studies on dynamic networks, this question actually already received a lot of specific attention @cite @cite @cite @cite @cite @cite @cite @cite .
- Contrastingly, the goal of @cite is precisely to determine an ideal aggregation period. In their method, this period is obtained as a trade-off between two metrics that vary monotonically and oppositely with regard to aggregation: one describing the loss of information (increasing with aggregation) and one describing the noise contained in the series of snapshots (decreasing with aggregation). Compared to them, here, we are concerned only with the loss of information. This allows us to avoid some drawbacks and limitations inherent to the approaches based on a trade-off: i) the value selected for the aggregation period strongly depends on the importance given to each metrics and ii) the selected value does not reveal any particular behavior of the properties of the network used in the trade-off, as each of them varies smoothly and monotonically from one extremal value to another one. On the contrary, our method does not depend on any arbitrary choice of ponderation and reveals a natural change in the way the network responds to aggregation at a certain aggregation scale that we determine.
- @cite also aims at determining an appropriate time scale for aggregating a link stream into a graph series. Their method does not take into account the loss of information but is instead based on the modes of periodicity and on the self-similarity of the time series of some properties of the snapshots. They observe that the offset time for which the self similarity of these time series is zero is close to half of the period of the highest frequency visible in their spectra, which is the aggregation period suggested as a result of their method. Though this provides a very relevant time scale for analyzing dynamic networks, its meaning is different from the meaning of the saturation scale we are looking for in this paper. Indeed, an important part of the activity of dynamic networks takes place at time scales much smaller than their modes of periodicity. Therefore, using such periods for aggregation usually induces an important loss of information, which we aim at avoiding here. Let us mention that a similar approach based on modes of periodicity of some time series associated to the network was previously used in @cite @cite .
- The Tyrolean Tourism Knowledge Graph contains static (e.g. phone number, address) and dynamic (e.g. accommodation offers) data based on schema.org annotations collected from different sources such as Destination Management Organizations (DMO) and Geographical Information Systems (GIS). In our previous work @cite , we explained how we annotated the relevant data in the region with schema.org from different sources. Our knowledge graph consolidates these annotations and enables intelligent applications like chatbots to contribute the digitalization of tourism in Tyrol. Additionally, since we store the historical data, the knowledge graph allows data analytics to provide insights from the region.
- Statistical Priors. Since blind image deblurring is an ill-posed problem, it requires certain assumptions or prior knowledge to constrain the solution space. Early approaches, e.g., @cite , assume simple parametric blur kernels to deblur images, which cannot deal with complex motion blur. As image gradients of natural images can be modeled well by a heavy-tailed distribution, Fergus et al @cite use a mixture of Gaussians to learn the statistical prior for deblurring. Similarly, Shan et al @cite use a parametric model to approximate the heavy-tailed prior for natural images. @cite , Cai et al assume that the latent images and kernels can be sparsely represented by an over-complete dictionary based on wavelets. On the other hand, it has been shown that the most favorable solution for a maximum a posteriori (MAP) deblurring method with sparsity prior is usually a blurred image rather than a sharp one @cite . As @cite is usually computationally expensive, an efficient algorithm for approximation of marginal likelihood is developed @cite for image deblurring.
- Image Priors in Favor of Clear Images. Different image priors that favor clear images instead of blurred images have been introduced for image deblurring. Krishnan et al @cite present a normalized sparsity prior, and Xu et al @cite use the @math constraint on image gradients for kernel estimation. Non-parametric patch priors that model edges and corners have also been proposed @cite for blur kernel estimation. We note that although the use of sparse priors facilitates kernel estimation, it is likely to fail when the blurred images do not contain rich texture. @cite , Michaeli and Irani exploit internal patch recurrence for image deblurring. This method performs well when images contain repetitive patch patterns, but may fail otherwise. Class-specific image prior @cite has been shown to be effective for certain object categories and less effective for scenes with complex background. Recently, @cite develop an image prior based on the dark channel prior @cite for blur kernel estimation. However, this method does not perform well when clear images do not contain zero-intensity pixels or the blurred images contain noise.
- Edge Selection. In addition to statistical priors, numerous blind image deblurring methods explicitly exploit edges for kernel estimation @cite @cite @cite @cite . Joshi et al @cite and Cho et al @cite use the restored sharp edges from a blurred image for kernel estimation. @cite , Cho and Lee utilize bilateral and shock filters to predict sharp edges. The blur kernel is determined by alternating between restoring sharp edges and estimating blur kernels in a coarse-to-fine manner. As strong edges restored from a blurred image are not necessarily useful for kernel estimation, Xu and Jia @cite develop a method to select informative ones for deblurring. Despite demonstrated success, these methods rely largely on image filtering methods (e.g., shock and bilateral filters) and heuristics for restoring sharp edges, which are less effective for objects with specific geometric structures.
- Face Deblurring. A few algorithms have been developed to deblur face images for the recognition task. Nishiyama et al @cite learn subspaces from blurred face images with known blur kernels for recognition. As the set of blur kernels is pre-defined, the application domain of this approach is limited. Zhang et al @cite propose a joint image restoration and recognition method based on sparse representations. However, this method is most effective for well cropped and aligned face images with simple motion blurs.
- Example-based Deblurring. Recently, @cite propose a deblurring method which uses sharp reference examples for guidance. The method requires a reference image with the same contents as the input to obtain dense correspondence for reconstruction. Although it has been shown to deblur specific images well, the assumption of using reference images with same contents limit its application domain. In contrast, the proposed methods do not require the exemplar to have the same or closely similar contents of the input. The blurred face image can be of different identity and background when compared to exemplar images. The proposed methods only require the matched example to have similar structures (in terms of image gradients) for kernel estimation instead of using dense corresponding pixels. As such, the proposed algorithms can be applied to class specific image deblurring with fewer constraints.
- Convolutional Neural Networks. Convolutional neural networks have been widely used in low-level vision tasks including image denoising @cite , super-resolution @cite @cite , non-blind deconvolution @cite @cite , blind image deblurring @cite and image filtering @cite @cite . Schuler et al @cite incorporate a sharpening convolutional neural network into an iterative blind deconvolution method to estimate the blur kernel. However, this method needs to re-train different networks for kernels of different sizes, which limits the application domains. @cite , Xu et al propose a method to learn edge-aware filters using a deep convolutional neural network. However, we note that this method can only be applied to approximate edge-aware filters for clear images. This method cannot be directly applied to restore salient edges from blurry images for kernel estimation.
- More recently, Ghaffari, Haupler and Khabbazian @cite showed that collision detection can be used to surpass this lower bound, attaining an @math time algorithm. Work by Haeupler and Wajc @cite demonstrated that even without collision detection, the lower bound could be beaten assuming were permitted; that is, nodes have access to a global clock and are allowed to transmit before receiving the source message. Czumaj and Davies @cite extended this approach and obtained a running time of @math for the setting with spontaneous transmissions. However, these algorithms only work in undirected networks.
- Deterministic algorithms for broadcasting have also been studied; for undirected networks the fastest known algorithm is the @math -time algorithm of @cite , while for directed networks it is the @math -time algorithm of @cite .
- All of these results also , and algorithms that do not require such knowledge have been little studied. The closest analogue in the literature is the work of Jurdzinski and Stachowiak @cite , who give algorithms for wake-up in single-hop radio networks (those in which the underlying graph is a clique, i.e. @math ) under a wide range of node knowledge assumptions. Their Use-Factorial-Representation algorithm is the most relevant; the running time is given as @math for high-probability wake-up with a global clock (a slightly stronger task than broadcasting) in single-hop networks, but a similar analysis as we present here would demonstrate that the algorithm also performs broadcasting in multi-hop networks in @math time.
- A deterministic algorithm for broadcasting in radio networks without parameter knowledge is given in @cite , with a running time of @math , where @math is the range of unique IDs with which nodes are equipped.
- Neural networks have been proven effective in several domains such as image classification @cite , sequence modeling @cite , and inverse problems in signal processing @cite . In matrix completion, existing work involves autoencoders, graph convolutional networks and deep learning neural networks. Autoencoder-based models @cite @cite learn transformations from original row or column vectors to a latent space and decoders to predict the missing values. Geometric matrix completion models employ graph convolutional networks to learn the feature vectors from column (or row) graphs @cite or bipartite user-item graphs @cite . The CF-NADE (Collaborative Filtering - Neural Autoregressive Distribution Estimator) method @cite , on the other hand, learns directly the latent vectors from columns and rows. The Neural Collaborative Filtering (NCF) @cite and Collaborative Metric Learning (CML) @cite utilize implicit feedback, i.e. interactions between users and items such as like, follows, shares, rather than explicit feedback, e.g. ratings.
- This paper is closely related to the following areas: semantic part segmentation, joint pose and body part estimation, and weakly supervised learning. In this subtask of semantic segmentation, fully convolutional network (FCN) @cite and its variants @cite @cite @cite have demonstrated promising results. In @cite , Chen proposed atrous convolution to capture object features at different scales and they further combined the convolutional neural network (CNN) with a Conditional Random Field (CRF) to improve the accuracy. @cite , the authors proposed an attention mechanism that softly combines the segmentation predictions at different scales according to the context. To tackle the problem of scale and location variance, Xia @cite developed a model that adaptively zoom the input image into the proper scale to refine the parsing results.
- An alternative solution to assess the task outcome is statistically setting a threshold regarding the wrench signature and the pose trajectories. Costa al @cite verify the success of a process by setting a threshold for eccentricity and typicality, distance metrics for time series. Haidu al @cite define lower and upper bounds of the trajectory profile based on successful trails. Thus, trajectory profiles that exceed the threshold indicates a failure. Nevertheless, the movement reproduction varies for different task parameters (e.g. start and goal state) and demonstrations. In those cases, the bounds have to be changed accordingly which requires reprogramming.
- Rojas al @cite identify key segments of the wrench signals and create a task-specific hierarchical taxonomy based on its time derivative. The task outcome is indicated by the states at the highest level of this taxonomy. However, the segmentation threshold is predefined, and the taxonomy associated with segments is manually created which significantly reduces its applicability to different tasks.
- Haidu al @cite segment the task in smaller pose trajectories and train a Hidden Markov Model (HMM) using each segment as a state. They identify promising states by aligning the segmented samples to the HMM, which contains key information for failure detection. A predictor is trained to assess the task from the aligned data at those promising states. Similarly, Di Lello al @cite segment the wrench signatures by the control strategy of a finite state machine. Each wrench signal is treated as a Bayesian time-series model. A HMM is trained via a Bayesian non-parametric method to detect the deviation from the successful execution. Both of those two methods require the predefined segmentation of the trajectory. However, there is often no such clear segmentation such as Zero Velociy Crossing point @cite or contact events that are plausible in human demonstrations.
- In the field of imitation learning, Calinon al @cite evaluate a movement in order to optimize a controller. Firstly they align the recorded demonstrations with Hidden Markov Model (HMM) and project them into a latent space via PCA. Then they measure the weights of each vector in the latent space by the variations in multiple demonstrations subject to the same task constraint. Eventually, a similarity measure is defined by the weighted sum of Euclidean distances between a new trajectory and the successful one. The application of this method does not involve wrench in the task and also requires a preprocessing procedure which increases the complexity of implementation.
- In the field of reinforcement learning, Pastor al @cite propose an algorithm which learns the outcome of an assessment system given a predefined threshold based on a reward function. The system is merged with reinforcement learning framework. However, the definition of the reward function may be challenging for certain tasks and also limits the method's re-usability.
- We describe the results of in @cite . Let @math be the clusters of an optimal @math -way partition for the conductance problem. We choose an algorithm for solving the minimization problem of @math shown in ) and suppose that the algorithm has an approximation ratio of @math . Let @math be the output of a spectral clustering algorithm that uses a @math -means method based on the @math -approximation algorithm. They showed the following statement in Theorem 1.2 of @cite . Set @math as and suppose that @math is so large that @math . After suitable renumbering of the output of the algorithm, we have Accordingly, if @math , those can be written as Here, the notation @math denotes the symmetric difference of the sets @math and @math . The results tell us that, if @math is large, the difference between @math and @math is small and the conductance of @math is close to that of @math . also developed a nearly linear time algorithm and examined the performance.
- We next describe the results of Kolev and Mehlhorn in @cite . They showed the following statement in Theorem 1.2 of the paper. Set @math as After suitable renumbering of the output of the algorithm, we have Accordingly, if @math , We see that the results of Kolev and Mehlhorn improve the approximation accuracy of by a factor of @math and weakens the gap assumption. Kolev and Mehlhorn also studied a spectral clustering algorithm that uses a @math -means method based on the Lloyd's algorithm and examined the performance.
- There is also a considerable amount of research on spectral clustering on a random graph. In a planted partition model, we assume that the node set is partitioned into several clusters and edges connecting the nodes are stochastically generated: any two nodes in the same cluster have an edge with probability @math , and any two nodes in different clusters have an edge with probability @math . McSherry @cite showed that spectral clustering can extract the clusters with high probability if @math and @math lie within some range. Rohe @cite and Lei @cite studied KSC on a stochastic block model.
- S-SGD requires the set of computing units (e.g., GPUs) to exchange data iteratively, which can be implemented by either parameter server (PS) based methods @cite @cite or decentralized methods. In PS-based methods, there is one or more PSes that store the global model. The PS aggregates parameters at each iteration, updates the model, and then pushes the updated model to each computing unit. Performance models have been built by S. @cite to generalize the performance of the PS-based methods, which provides guidelines for better system scalability.
- Decentralized methods implement the gradients aggregation by using the reduction tree (RT) or ring based all-reduce @cite @cite @cite . The gradients are exchanged via MPI-like collectives (e.g., all-reduce). Very recently, some new collective communications libraries like Gloo https: github.com facebookincubator gloo and NCCL2 https: developer.nvidia.com nccl have been developed to support efficient communications among a set of GPUs. A. @cite @cite propose a high performance CUDA-Aware MPI to reduce the overhead of data communications across a GPU cluster. have shown that the optimized all-reduce implementation and the pipeline of all-reduce operations with gradient computation can lead to very good scalability @cite .
- Flow Fields @cite can be considered as the basis of our method. It was among the first approaches to achieve top performance across multiple data sets and has been refined several times since its publication. Flow Fields+ @cite improved the algorithm by more sophisticated matching, FlowFieldsCNN @cite used deep learning to evaluate the matching cost, and ProbFlowFields @cite improved the results by jointly estimating optical flow and a certainty measure. Our approach shares the basic concepts with the mentioned methods. That is, we also perform dense matching by multi-scale propagation and random search, followed by outlier rejection, post processed by an interpolation mechanism. However, the novel FlowFields++ differs from those approaches by combining the matching accuracy of Flow Fields with robust interpolation.
- Dense interpolation has become a very popular post processing step for many applications ever since the publication of the first successful interpolation method EPICFlow @cite . It was used by Flow Fields and many other matching methods to produce dense results. InterpoNet @cite tried to solve the task of optical flow interpolation with a neural network that showed improvements over EPICFlow depending on input matches and data set. Independent of these factors are the advantages of RICFlow @cite over EPICFlow. The basic idea of edge preserving interpolation is complemented by increased robustness in the computation of the piece-wise interpolation models. We will exploit this robustness for our approach and extend it further by improved edge detectors and adjusted variational refinement.
- In recent eras of deep learning, there are also approaches that use convolutional neural networks (CNNs) to aid or solve the task of optical flow estimation. Some try to compute optical flow in and end-to-end manner @cite @cite @cite , others use neural networks to compute a semantic segmentation as additional input @cite @cite @cite , and some use deep learning for matching cost computation @cite @cite @cite . Of course, all deep learning approaches require a lot of proper training data and none yet has showed to generalize well across different data sets without retraining or tuning. Our approach maintains its versatility by avoiding deep learning.
- has been a subject of extensive research in computer vision for a long time @cite @cite and the existing systems vary greatly in terms of feature representation and proposed classifiers. The input to pose estimation systems typically consists of 2D head images @cite @cite @cite , and often one has to cope with low resolution images @cite @cite @cite @cite . Additional modalities such as depth @cite and motion @cite @cite information has been exploited and provides useful cues. However, these are not always available. Also, information about the full body image could be used for joint head and body pose prediction @cite @cite @cite . Notably the work of @cite also promotes a probabilistic view and fuse body and head orientation within a tracking framework. Finally, the output of facial landmarks can be used as an intermediate step @cite @cite .
- Existing head pose estimation models are diverse and include manifold learning approaches @cite @cite @cite @cite , energy-based models @cite , linear regression based on HOG features @cite , regression trees @cite @cite and convolutional neural networks @cite . A number of probabilistic methods for head pose analysis exist in the literature @cite @cite @cite , but none of them combine probabilistic framework with learnable hierarchical feature representations from deep CNN architectures. At the same time, deep probabilistic models have shown an advantage over purely discriminative models in other computer vision tasks, e.g., depth estimation @cite . To the best of our knowledge, our work is the first to utilize deep probabilistic approach to angular orientation regression task.
- Online path finding algorithms is designed to deal with uncertainties and emergencies during the flight. @cite presents some common online algorithms, including potential field approach and particle swarm optimization (PSO). They have advantages and disadvantages: potential field approach are simple and straightforward to understand, but may easily fall into local optimum should no adjustment to algorithm is done. PSO is also intuitive, but it can also be too computationally expensive to reroute in real-time. @cite proposed a predictive control scheme that use shared knowledge between nearby UAVs to find collision-free route, meanwhile considering the dynamics of UAV itself.
- Off-line algorithms plan the path prior to departure. The aircraft are usually modelled as point mass moving in two dimensions with constant speed and upper-bounded turning rate. @cite uses Genetic Algorithm (GA) to find optimal flyable path through numerous iterations, with each iteration improving previous path by rerouting at waypoint. The problem with Genetic Algorithm is that runtime grows exponentially as number of iterations and flights grows. @cite @cite also uses GA, but with a parallel approach to speedup the calculation. @cite formulates vehicle dynamic model, collision avoidance constraint, and multiple waypoint constraint as mixed-integer linear program (MILP). One of its novelty is that it uses trigonometric functions to better approximate the radius constraint. However, it does not consider battery constraint, which is a limiting factor to UAV's delivery capability. In this paper, we will address both collision avoidance constraint and battery constraint.
- Given we have global information regarding the the existing schedules of UAVs and their current locations, we can optimize the route through off-line planning. One natural way is using A* algorithm. A* algorithm @cite is a best-first, efficient and optimal search algorithm. It uses heuristics to guide the search, and therefore, reduce the number of nodes needed to be explored. Another advantage for using A* is that it can apply on both 2D and 3D space as long as the graph is connected. @cite proposed an offline improved A* algorithm to deal with realistic constraints in UAV movement, including maximum moving angle, minimum route leg length, minimum flight height and maximum route length. Their algorithm can avoid collision with terrain while minimize the flight height. However, it only handles static graph, and can be computationally expensive. proposed fixes by trimming some less-permissble nodes, or imposing some hard nodes that the route must follows, but doing so will lose some potentially optimal solutions, and thus lose its completeness and optimality.
- Adversarial Example Generation Most existing works, e.g., @cite @cite @cite @cite @cite , apply various heuristic algorithms, generally using search algorithms based on gradient descent or evolutionary techniques. @cite construct a saliency map of the importance of the pixels based on gradient descent and then modify the pixels. In contrast with our approach based on global optimisation and works on safety verification, these methods may be able to find adversarial examples efficiently, but are of adversarial examples when the algorithm fails to find one.
- Output Range Analysis The safety verification approach can be adapted to work on this problem. Moreover, @cite consider determining whether an output value of a DNN is reachable from a given input subspace, and propose an MILP solution. @cite study the range of output values from a given input subspace. Their method interleaves local search (based on gradient descent) with global search (based on reduction to MILP). Both approaches can only work with small networks.
- Britz, et. al. study a massive analysis of NMT hyper-parameters aiming for better optimization being robust to the hyper-parameter variations @cite . Likewise, Bahar et. al. compare various optimization strategies for NMT @cite . In addition, Wu, et. al. @cite utilized the combination of Adam and a simple stochastic gradient descent (SGD) learning algorithm. They run Adam for a fixed number of iterations and switch to SGD to slow down the training phase. To the best of our knowledge, there has not been any work comparing different optimization strategies for NMT. Most of the work in this area focuses on the modeling problem on a vanilla NMT task without exploring the tradeoffs of parameter selection, in terms of performance and stability.
- Given the importance and time-intensive nature of medical coding, many scientists have started developing techniques for automated coding @cite . For most of these attempts, the task was narrow (i.e. attempting to predict a single ICD code or a small set of related codes) and utilized a relatively small dataset.
- At least one attempt has been made to use deep learning (namely character-level recurrent neural networks) to build more general coding models, but with limited success @cite . The main limitation with this method is that deep learning models require a large amount of data to train @cite , but even a large clinical dataset may only contain a few examples of case files with any given ICD code. Additionally, the ICD-9 standard contains over 14,000 different codes; treating coding as a simple multi-class classification problem with 14,000 different classes is infeasible.
- More successful automated coding models have utilized non-deep machine learning techniques such as support vector machines (SVMs) @cite and focus on training a model to detect the presence of a single ICD code or class of related codes. Some of these methods also leverage structured data stored in health records in addition to free-text narratives @cite .
- At least one previous attempt has been made to build an automatic coding model for neonatal jaundice; @cite uses an SVM on the text of clinical notes. Our research improves on this model by employing an additional preprocessing step and using ensemble classification methods. We explore and compare these alternative classification techniques and analyze their efficacy.
- Some works, e.g., @cite @cite @cite , were proposed to guide the minimal subset sampling process. These model fitting methods have achieved better performance than RANSAC on either speed or accuracy. However, they cannot achieve consistent and tractable fitting results. In contrast, the proposed method in this paper focuses on the deterministic fitting, and it can obtain more consistent results.
- Generally, a good keypoint correspondence between an image pair has a high matching score, while a bad keypoint correspondence has a low matching score. Based on this observation, some fitting methods, e.g., @cite @cite @cite @cite @cite , were proposed to guide sampling minimal subsets, by which they can effectively accelerate the process of promising model hypothesis generation. Most of these fitting methods can work well on single-structure data, but they may fail on multiple-structure data. This is because the keypoint correspondences with high matching scores may belong to different model instances in multiple-structure data, and thus it is hard to distinguish them from the global perspective.
- Deterministic sampling based fitting methods (e.g., @cite @cite @cite @cite @cite @cite ) are able to provide consistent and reliable fitting results. For example, Li @cite used a tailored branch-and-bound scheme to deterministically solve the global optimization problem for model fitting. @cite deterministically generated model hypotheses based on the maximum feasible subsystem framework for model fitting. @cite employed a method for inlier rate estimation to compute a globally optimal transformation for model fitting. @cite extended a branch-and-bound scheme to solve the two-view translation estimation problem. @cite proposed to use loss functions to perform model estimation. @cite used the Astar search algorithm @cite to provide a globally optimal solution for the model fitting problem.
- Due to the indifferentiability in the spiking mechanism of SNN, it is hard to directly apply classic learning methods of ANNs, such as backpropagation, to SNNs. In order to address this problem, researchers have devised a wide variety of learning algorithms to build SNNs. One approach is to directly learn from spikes with backpropagation-like algorithm by approximating the threshold function with linearity. For example, early work such as the @math @cite assumes the potential function to be differentiable for a small region around firing time. More recently, @cite demonstrated learning SNN with stochastic gradient descent (SGD) by regarding membrane potentials of spiking neurons as differentiable signals and discontinuities at spike times as noise.
- Another approach is to learn with spike-timing-dependent plasticity (STDP) @cite , a biologically plausible approach. In @cite @cite , STDP has been proven able to select features in an unsupervised manner. Improved STDP algorithms such as rectangular STDP @cite and exponential STDP @cite can achieve competitive performance on MNIST dataset with a simple two-layer network.
- Research on autonomous perception has commonly focused on modeling advanced hand-crafted features, such as HoG @cite or Haar @cite . Nonetheless, since the emergence of modern Convolutional Neural Networks (CNNs) and large-scale image datasets such as ImageNet @cite , object detection studies have progressively moved towards feature learning approaches, which produce more robust representations of objects, increasing the performance on classification tasks.
- Besides 2D detections, some approaches have been made attempting to provide spatial location of objects based on visual information. In @cite , 3D object candidates are placed over a ground plane prior and classified in image space. Similarly, 3D voxel patterns have been used to estimate position and orientation of objects in the scene @cite .
- Some other works have used point cloud data to compute object detections in 3D, either using information from stereo cameras or laser sensors. Although there are some methods which use hand-crafted features @cite @cite , latest approaches take advantage of feature learning capabilities.
- Among these latter group, two different strategies are being explored. On the one hand, some approaches work with spatial information by turning the 3D space into a voxel grid and applying 3D convolutions @cite @cite @cite . On the other hand, 2D CNNs are used by projecting LiDAR point cloud into a front view @cite or a bird's eye view (BEV) @cite .
- Since camera and LiDAR are complementary data sources, many works have tried to build robust object detection frameworks by fusing their information. Traditional methods used to obtain 3D ROIs from the point cloud and perform classification on their image projection @cite . However, when RPNs step in, the candidate proposal stage was outperformed both in ROIs quality and execution time. In this direction, a novel approach has been presented in @cite where regions of interest and classification are computed on the image space, and final location is performed over the LiDAR data.
- Considerable attention has also been paid to the problem of learning categories for which no, or only few training examples are available, especially within the area of image recognition. For example, in one common setting, each category is defined w.r.t. a set of features, and the assumption is that we have training examples for some of the categories, but not for all of them. Broadly speaking, the aim is then to learn a model of the individual features, rather than the categories, which then makes it possible to make predictions about previously unseen categories @cite @cite . Other approaches instead exploit the representation of the category names in a word embedding @cite . We will similarly exploit vector space representations of concept names.
- Togelius @cite defined Procedural Content Generation (PCG) as the @cite @cite @cite . Examples of game content include game rules, levels, maps mazes, characters, weapons, vehicles, background stories, textures and sound. Automatic game level generation, with little or no human intervention, is a challenging problem. For some games, the levels are represented as maps or mazes @cite . Examples include , , and , one of the classic platform video games created by .
- The first academic Procedural Content Generation competition was the 2010 Mario AI Championship @cite , in which the participants were required to submit a level generator which implements a provided Java interface and returns a new level within @math seconds. The competition framework was implemented based on https: tinyurl.com yan4ep7g , a public clone of .
- The availability and popularity of the Mario AI framework has led to several approaches for generating levels for . Shaker @cite evolved Mario levels using Grammatical Evolution (GE). In 2016, Summerville and Mateas @cite applied Long Short-Term Memory Recurrent Neural Networks (LSTMs) to generate game levels trained on existing Mario levels, and then improved the generated levels by incorporating player path information. This approach inspired a novel approach to level generation, in which new levels are generated automatically from a sketch of some desired path drawn by a human designer. Another approach that was trained using existing Mario levels is that of Jain @cite , which trained auto-encoders to generate new levels using a binary encoding where empty (accessible) spaces are represented by 0 and the others (e.g., terrain, enemy, tunnel, etc.) by 1. Though this approach could generate interesting levels, the use of random noise inputs into the trained auto-encoder sometimes led to problematic levels. Additionally, because of the binary encoding, no distinction was made between various possible types of tiles.
- Generative Adversarial Networks (GANs) were first introduced by Goodfellow @cite in 2014. Their training process can be seen as a two-player adversarial game in which a generator @math (faking samples decoded from a random noise vector) and a discriminator @math (distinguishing real fake samples and outputting 0 or 1) are trained at the same time by playing against each other. The discriminator @math aims at minimizing the probability of mis-judgment, while the generator @math aims at maximizing that probability. Thus, the generator is trained to deceive the discriminator by generating samples that are good enough to be classified as genuine. Training ideally reaches a steady state where @math reliably generates realistic examples and @math is no more accurate than a coin flip.
- In another paper @cite present an interactive evolutionary system, in which users can evolve the latent vectors for a GAN trained on different classes of objects (e.g. faces or shoes). Because the GAN is trained on a specific target domain, it becomes a compact and robust genotype-to-phenotype mapping (i.e. most produced phenotypes do resemble valid domain artifacts) and users were able to guide evolution towards images that closely resembled given target images. Such target based evolution has been shown to be challenging with other indirect encodings @cite .
- Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES) @cite is a powerful and widely used evolutionary algorithm that is well suited for evolving vectors of real numbers. The CMA-ES is a second-order method using the covariance matrix estimated iteratively by finite differences. It has been demonstrated to be efficient for optimizing non-linear non-convex problems in the continuous domain without a-priori domain knowledge, and it does not rely on the assumption of a smooth fitness landscape.
- As part of the general interest in search-based software engineering (SBSE) approaches @cite , much research attention has been given to the application of meta-heuristic search techniques to address the combinatorial test generation problem. Meta-heuristic techniques have had a big impact on the construction of @math and variable-strength test suites, especially in terms of the optimality of the test suite @cite @cite @cite @cite @cite .
- @cite @cite developed a simulated annealing-based strategy for supporting the construction of a uniform and variable-strength @math test suite. A large random search space is generated in the implementation. When the algorithm iterates, the strategy chooses better test cases to construct the final test suite using the binary search process and a transformation equation. The search space is transformed from one state to another according to a probability equation. The results of the study are mainly concerned with the interaction strengths of two and three @cite .
- @cite implemented a @math strategy based on ant colony optimization (ACO). The strategy simulates the behaviour of natural ant colonies in finding paths from the colony to the location of food. Each ant generates one candidate solution and walks through all paths in this solution by probabilistically choosing individual values. When the ant reaches the end of the last path, it returns and updates the initial candidate solution accordingly. This process continues until the iteration is complete. The final test case is chosen according to the maximum coverage of the t-interaction. Unlike the SA, the final test suite is further optimized by a merging algorithm that tries to merge the test cases.
- @cite adopted a genetic algorithm (GA) based on natural selection. Initially, the GA begins with randomly created test cases called chromosomes. These chromosomes undergo crossover and mutation until the termination criterion is met. In each cycle, the best chromosomes are probabilistically selected and added to the final test suite.
- @cite developed a @math strategy based on the harmony search algorithm (HSS). The HSS mimics the behaviour of musicians trying to compose good music either by improvising on the best tune they remember or by random sampling. In doing so, the HSS iteratively exploits the harmonic memory to store the best solution found through a number of defined probabilistic improvisations within its local and global search processes. In each improvisation, one test case is selected for the final test suite until all the required interactions are covered. The notable feature of the HSS is that it supports constraints using the forbidden tuple approach.
- @cite adopted the cuckoo search algorithm (CS), which mimics the unique lifestyle and aggressive reproduction strategy of the cuckoo. First, the CS generates random initial eggs in host nests. Each egg in a nest represents a vector solution that represents a test case. In each generation, two operations are performed. Initially, a new nest is generated (typically through a Lvy flight) and compared with the existing nests. The new nest replaces the current nest if it has a better objective function. Then, the CS adopts probabilistic elitism to maintain the elite solutions for the next generation.
- Particle swarm optimization (PSO) @cite is perhaps the most popular implementation of @math test suite generation. The PSO-based @math strategy searches by mimicking the swarm behaviour of flocking birds. In PSO, global and local searches are guided by the inertia weight and social cognitive parameters. Initially, a random swarm is created. Then, the PSO algorithm iteratively selects a candidate solution within the swarm to be added to the final test suite until all interaction tuples are covered (based on velocity and displacement transformation). developed early PSO-based strategies called the PSTG @cite @cite and APSO @cite . APSO is an improvement on the PSTG integrated with adaptive tuning based on the Mamdani fuzzy inference system @cite @cite . implemented discrete PSO @cite by substantially modifying the displacement and velocity transformation used in PSO. The benchmark results of DPSO @cite demonstrate its superior performance when compared with both the PSTG and APSO.
- Despite the significant number of proposed algorithms in this field, the adoption of new meta-heuristic algorithms is most welcome. The no free lunch (NFL) theorem @cite suggests that no single meta-heuristic algorithm can outperform others even when there is a slight change in the problem of ( @math ) configurations. Therefore, the NFL theorem allows researchers to propose new algorithms or modify current ones to enhance the current solution. In fact, the results could also be applied in other fields.
- Hybrid integration with machine learning appears to be a viable approach to improving the state-of-the-art meta-heuristic algorithms. Machine learning relates to the study of the fundamental laws that govern the computer learning process concerning the construction of systems that can automatically learned from experience. Machine learning techniques can be classified into three types: supervised, unsupervised, and reinforcement @cite . Supervised learning involves learning a direct functional input-output mapping based on some set of training data and being able to predict new data. Unlike supervised learning, unsupervised learning does not require explicit training data. Specifically, unsupervised learning involves learning by drawing inferences (e.g., clustering) from an input dataset. Reinforcement learning allows mappings between states and actions to maximize reward signals using experimental discovery. This type of learning differs from supervised learning in that it relies on a punishment and reward mechanism and never corrects input-output pairs (even when dealing with suboptimal responses).
- By building on and complementing the work mentioned above, our work explores the hybridization of the Q-learning algorithm with a recently developed meta-heuristic algorithm called the SCA @cite . Unlike most meta-heuristic algorithms that mimic certain physical or natural phenomena, the equation transformation used in the SCA is solely based on the sine and cosine operations. Therefore, the learning curve of the SCA is low. Although its exploitation is commendable, the exploration of the SCA is strictly bounded due to the (adaptive) shrinking magnitude of the sine and cosine functions multipliers during the search process. To address the issues mentioned above, we propose a new algorithm, the QLSCA. Moreover, we augment the QLSCA with two further operations (Lvy flight motion and crossover) to counterbalance its exploration and exploitation. Then, we use the Q-learning algorithm (which is based on the penalty and reward mechanism) to dynamically identify on the best operation (sine, cosine, Lvy flight motion, or crossover) during runtime.
- While our focus is on directly optimizing ranking performance in the implicit feedback partial-information setting, several approaches have been proposed for the same task in the full-information supervised setting, i.e. when the relevances of all the documents in the training set are known. A common strategy is to use some smoothed version of the ranking metric for optimization, as seen in SoftRank @cite and others @cite @cite @cite @cite . In particular, SoftRank optimizes the expected performance metric over the distribution of rankings induced by smoothed scores, which come from a normal distribution centered at the query-document mean scores predicted by a neural net. This procedure is computationally expensive with an @math dependence on the number of documents for a query. In contrast, our approach employs an upper bound on the performance metric, whose structure makes it amenable to the Convex Concave Procedure for efficient optimization, as well as adaptable to non-linear ranking functions via deep networks.
- We report an emerging trend which can be observed in the works cited above. With the notable exception of @cite , existing state of the art on UAV network optimisation tends to ignore the effects of interference, instead focusing on scenarios where individual UAVs are operating in isolation. Without interference the wireless links are limited by the geometry of the environment, and therefore the network performance is generally optimised through minimising the distance between the UAV and the receiver, as this minimises the pathloss, increases the LOS probability and enables the network to reduce transmit power. These optimisation strategies may not apply to a scenario where multiple UAVs are operating concurrently and creating interference for each other. In the presence of interference, decreasing the distances between transmitters and receivers may also have the result of decreasing the distances between interferers and receivers, potentially causing a net decrease in channel performance.
- Most prior studies on unsupervised domain adaptation focused on aligning the distributions between domains in feature space, by minimizing measures of distance between features extracted from the source and target domains. For example, the Maximum Mean Discrepancy (MMD) was minimized together with a task-specific loss to learn the domain-invariant and semantic-meaningful features in @cite . The correlations of layer activations between the domains were aligned in the study of @cite . Based on this, @cite further extended the work and minimized domain difference based on both the first and second order information between source and target domains. Alternatively, with the emergence of generative adversarial network (GAN) @cite and its powerful extensions @cite @cite , the mapping between domains were implicitly learned via the adversarial loss. The @cite proposed to extract domain-invariant features by sharing weights between two ConvNet classifiers. Later, the @cite introduced a more flexible adversarial learning method with untied weight sharing, which helps effective learning in the presence of larger domain shifts. Another GAN based direction of solution is to learn a transformation in the pixel space @cite , adapting the source-domain images to appear as if drawn from the target domain.
- Boosted decision tree in @cite is trained with hard example mining strategy but hard examples are mined one time only. To begin with, all the positive examples and a random set of negative examples are blended together as the original training set. After reaching convergence on the original training set, the trained model is applied to the rest of negative examples. Then, only false positive examples are selected as hard ones and added to the original training set to form the final training set. Finally, the model is trained on the refreshed training set until convergence.
- Another hard exampling mining technique named boostrapping is used to train Support Vector Machines(SVMs) @cite and hard examples are mined several times in this case. A working set containing a tiny number of samples is used in boostrapping. Samples are added to and removed from this set according to some specific rules. Processes of training model to convergence on the existing working set and utilizing the trained model to modify the working set are finished alternatively. When modifying the working set, samples in the working set classified correctly by the existing model are removed from it while samples out of the working set misclassified by the model are added to it.
- The history of utilizing techniques of hard example mining in object detection can date back to the time when it was used to train SVMs for pedestrian detection @cite . After the prevalence of CNN-based model in object detection, hard example mining still played an important role as an SVM classifier is usually attached to the top of detectors for classification, e.g. @cite @cite .
- However, after SVMs being replaced by layers consisting of neural units in subsequent object detection methods @cite @cite , hard example mining strategies were not utilized in the training of CNN-based detectors until Online Hard Example Mining(OHEM) proposed in @cite . OHEM depends on the RoIs proposed by the region-proposing stage heavily but that stage is removed in state-of-the-art real-time detectors for higher speed, which makes OHEM serve no purpose on those detectors.
- To tackle severe imbalance issues between backgrounds and foregrounds in real-time detectors, proposed Focal Loss @cite and tried to modify the loss function to mine the hard examples from easy ones. However, Focal Loss depends on the definition of loss function heavily and cannot be applied to plenty of state-of-the-art real-time detectors straightly.
- Two multi-task loss functions broadly adopted in object detection are illustrated in Fig.. Loss1 consists of two tasks, namely, classification loss and box regression loss. Classification loss in Loss1 is calculated for both foregrounds and backgrounds, but box regression loss is computed for foregrounds only. Though classification loss is the majority of Loss1 and Focal Loss can be applied to it, the impacts produced by box regression loss are totally neglected. Additionally, for the methods in @cite @cite which adopt Loss2 as their loss functions, Focal Loss serves no purpose. This is mainly because Loss2 possesses four subtasks, namely, object loss, non-object loss, classification loss and box regression loss, but all of them cannot be shared by both foregrounds and backgrounds. Though replacing Loss2 with Loss1 makes Focal Loss available for those detectors, it does harm to detection accuracy significantly, as Loss2 fit those detectors better. Our experiments in Section demonstrate that for those detectors, applying Focal Loss compulsively is detrimental but using Loss Rank Mining is helpful.
- Our approach and results are closely related to work by (2016) @cite , who also applied TDA to object data. In their case they considered brain artery structures extracted from magnetic resonance images and applied persistence homology, which was encoded in vectors by the order statistic on the most persistent points in the persistence diagram. Also, closely related is work by Kovacev- (2016) @cite who applied persistent homology and persistence landscapes to protein structure data.
- Traditional approaches to affordance learning often involves behavioral babbling'' @cite @cite @cite wherein the agent physically interacts with objects in a goal-free manner to discover their affordances. Hence, the resulting affordance representation is dissociated from a task, focusing instead on object properties. Such approaches involve several agent-object interactions affecting the scalability of the learning process, making it unfeasible in situations where there is an implicit cost or time constraint on the robot. ACR helps mitigate this cost by the grouping of actions into categories.
- Two works closest to our approach are @cite and @cite . In @cite , describe an approach to visual object-action recognition that use demonstrations to categorize semantically labeled objects based on their functionality. This approach bridges the gap between affordance learning and task context since the learning is coupled with a task demonstration. However, it is unclear how the system would incorporate previously unseen objects unless they are observed from additional demonstrations. For instance, given a demonstration of pouring water into a cup'', the agent would require additional demonstrations to identify the similar functionality of a bowl''.
- in @cite learn visual object categories for affordance prediction (Category-Affordance model), reducing the physical interactions with the objects. They use visual features of objects to categorize them on the basis of their functionality. However, it is unclear how the agent would deal with changing features and categories @cite , since the model is learned offline as compared to ACR which allows online learning of new objects and categories (Details in Sec 3). Regardless, their approach highlights some of the benefits of categorization on the scalability of learning, which motivates our work.
- Human demonstrations have been used for both high-level task learning and low-level skill learning @cite ; a traditional assumption of LfD is that the human demonstrator is an expert, and the demonstrations are examples of desirable behavior that the agent should emulate. Our work focuses on high level task learning, but considers demonstrations more broadly as examples of what the agent do, rather than what it . This interpretation of the data enables our technique to benefit even from non-expert human users. Demonstration errors can be classified to one of 3 categories @cite : Correct but suboptimal (contains extra steps), conflicting or inconsistent (user demonstrates 2 different actions from the same state) and entirely wrong (user took a wrong action) and we demonstrate the robustness of ACR to suboptimal demonstrations in Sec 6.
- Researchers have explored what NMT models learn about other linguistic phenomena, such as morphology @cite @cite , syntax @cite , and lexical semantics @cite , including word senses @cite @cite
- In many respects the ideas of rehabilitation-driven games share similarities with that of dynamic difficulty adjustment. However, as noted in the likes of @cite , the challenges faced are more nuanced given the physical challenges faced by patients. A notable example can be found in the Intelligence Game Engine for Rehabilitation (IGER) detailed in @cite @cite which is focussed on the recording and management of physiotherapy for stroke patients. This system adopts an adaptive fuzzy-driven approach to catering for the patients rehabilitation process. This is achieved while working with a variety of small mini-games that utilise peripherals such as the Microsoft Kinect and Nintendo Wii Fit board. This system places emphasis on ensuring that the parameters of the current game are such that it should not result in activities that player might find painful.
- Task-specific architectures for end-to-end deep learning require large datasets and work very well when such data is available, as in the case of neural machine translation @cite . General purpose end-to-end architectures, suitable for multiple tasks, include the Neural Turing Machine @cite and its successor, the Differential Neural Computer @cite (DNC). There is no external application integration in these architectures. Other architectures, such as the Neural Programmer architecture @cite allow end-to-end training while constraining parts of the network to execute predefined operations by re-implementing specific operations as static differentiable components. This approach has two drawbacks. It requires re-implementation of the API in a differentiable way, which may be difficult, and it lacks the accuracy and possible scalability advantages of an external API.
- Program induction is a different approach to interaction with external APIs. The goal is to construct a program comprising a series of operations based on the input, and then execute the program to get the results. When the input is a natural language query, as in our focus, it is possible to use semantic parsing to transform the query into a logical form that describes the program @cite . Early works required natural language query-program pairs to learn the mapping. Recent works, (e.g., @cite ) require only query-answer pairs for training.
- Learning to execute the right operation can be viewed as a reinforcement learning problem. For a given input, the agent has to select the optimal action from a set of available actions. The action selection repeats following feedback based on the previous action selection. Earlier works that took this approach include @cite , and later @cite . Recently, @cite proposed a reinforcement extension to Neural Turing Machines @cite . In @cite , the authors pose a value iteration based solution for reinforcement learning tasks as an end-to-end learning task with a Value Iteration Network (VIN). VIN are shown to learn how to plan a sequence of actions for a given task.
- Most methods of face attribute manipulation are based on generative models. There are two main groups of these methods: the group with extra input vector, and the group that directly learn the image-to-image translation along attributes. The first group often takes an attribute vector as the guidance for manipulating the desired attribute. The CAAE method @cite concatenates the one-hot age label with latent image features to be fed into the generator for age progression purposes. StarGAN @cite takes the one-hot vector to represent domain information for "domain transfer". However, such global transformation based on external code usually cannot well preserve the facial details after attribute manipulation. The second group of methods only operate in image domains and learn the image-to-image translation directly. The CycleGAN @cite and UNIT method @cite are such examples, supervised by a cycle consistency loss that requires the manipulated image can be mapped back to the original image. @cite further proposed to only learn the residual image before and after attribute manipulation, which can be easier and lead to higher-quality image prediction. Unfortunately, these methods still have difficulty of manipulating the target attribute while keeping others unchanged.
- In the literature, the development of a neural network for workspace generation has been already tested on both serial-link and parallel mechanisms, utilizing several model architectures and optimization methods @cite @cite . The feasibility of using an artificial neural network for workspace analysis is first proposed and investigated by @cite , where a two-layer perceptron learns to generate the orientation workspace of a 6-3 SPM parallel mechanism based on a 3-tuple input. In a similar approach, @cite constructs a three-layer deep network that takes the input joint angles of a 2-DOF parallel manipulator and gives the workspace of the mechanism in a particular Cartesian plane. Levenberg-Marquardt method is used for optimization, and the lowest mean squared error obtained by the algorithm is 0.026.
- Li @cite proposed an indoor navigation system based on off-the-shelf smartphone sensors and magnetic features. They used several approaches to enhance the accuracy of the system including multi-dimensional dynamic time warping, weighted k-nearest neighbors, and exploiting magnetic gradient fingerprints. They also mitigated the impact of magnetic matching mismatches to reduce the position errors. They reported root mean square error between 4.3 m and 5.6 m.
- Chen @cite propose a path planning system for emergency guidance in indoor environments based on IoT technologies. They use the statistical properties of mobility of groups of people to provide an individualized path for each group. In this work, a graph of the corridors, doors, and exits is optimized to minimize the total evacuation time for all groups. They implemented their work by utilizing the iBeacon technology and smartphones.
- GANs have been used widely for visual data. The most related work to path planning is reported by Hirose @cite in which GANs are used to classify images as safe to go to or not for robot navigation purposes. In that work, authors used the observed scene by a robot's camera and classified it as GO" or NO GO."
- While there are many academic papers that discuss the machine learning antimalware approaches, most use outdated and unrealistic datasets and do not map directly to real-world problems @cite . The classifier and feature types that we use in this paper were chosen based on techniques that have worked well for real-world antimalware problems involving detection of malicious PE files. Saxe and Berlin employ a deep neural network similar to ours @cite , while Anderson and Roth employ gradient boosted decision trees @cite . However, document and archive file formats have their own unique challenges because they are specifically designed to store user provided content which may or may not be executed, while PE files contain specified streams of execution.
- As classifiers, we use feed-forward deep neural networks and gradient boosted decision ensembles. While one could try more sophisticated types of neural networks -- e.g., convolutional and recurrent, these are difficult to implement in practice due to large file sizes, computational overhead, and a dearth of generic byte-level embeddings. Though character-level embeddings have yielded success for certain antimalware problems, e.g., @cite , these do not yet work well for generic byte-level embeddings of arbitrary length to our knowledge. Thus, we instead transform each document archive to a fixed-length feature vector before using it to train a classifier. Finally, we note that our focus in this paper is on on static detection, because machine learning models require a lot of data in order to work well. While antimalware stacks consist of both static and dynamic components, dynamic detection is very expensive computationally and is often employed to post-process detections from static engines, which operates much faster at scale. Dynamic detection is an important, complementary, and orthogonal area of research to that presented in this paper.
- To the best of our knowledge, the first well-known cross-modal correlating model may be the CCA based model proposed by Hardoon et. al @cite . It learnt a linear projection to maximize the correlation between the representation of different modality in the projected space. Inspired by this work, many CCA based models are designed for cross-modal analyzing @cite @cite @cite @cite . @cite utilized CCA to learn two maximally correlated subspaces, and multiclass logistic regression was performed within them to produce the semantic spaces respectively. @cite proposed a Struncated-SVD based algorithms to compute the full regularization path of CCA for multi-modal retrieval efficiently. @cite developed a new hypergraph-based Canonical Correlation Analysis(HCCA) to project low-level features into a shard space where intra-pair and inter-pair correlation be maintained simultaneously. Heterogeneous high-order relationship was used to discover the structure of cross-modal data.
- For the rapid growth of data volume, the cost of finding nearest neighbor cannot be dismissed. Hashing is a scalable method for finding nearest neighbors approximately @cite . It projects data into Hamming space, where the neighbor search can be performed efficiently. To improve the efficient of finding similar multi-modal objects, many cross-modal hashing methods have been proposed @cite @cite @cite @cite @cite . Kumar and Udupa @cite proposed a cross view hashing method to generate such hash codes that minimized the distance in Hamming space between similar objects and maximized that between dissimilar ones. @cite used a co-regularization framework to generate such binary code that the hash codes from different modality were consistent. @cite constructed a Hamming space for each modality and build the mapping between them with logistic regression. @cite proposed a sparse multi-modal hashing method for cross-modal retrieval.
- Besides these methods above, there still are other models proposed for multi-modal problems. @cite employed voxel-based multi-modal partial least square(PLS) to analyze the correlations between FDG PET glucose uptake-MRI gray matter volume scores and apolipoprotein E epsilon 4 gene dose in cognitively normal adults.
- Although these methods have achieved great success in multi-modal learning, most of them need a mass of training data to learn the complex correlation between objects from different modality. To reduce the demand of training data, @cite proposed an active similarity learning model for cross-modal data. Nevertheless, without extra information, the improvement is limited.
- For the past several years, advances in deep neural networks have shown to be a powerful tool for a variety of machine learning problems in multiple domains, including computer vision @cite @cite @cite @cite , speech @cite @cite , and text @cite @cite . For many of these domains, and especially for vision, each layer of the deep neural network learns features relevant to the target objective @cite @cite . For many objectives, a deep neural network requires a large-scale dataset to converge and obtain good accuracy @cite . For most tasks, however, large-scale datasets do not exist or are unobtainable. To circumvent this issue, existing deep neural networks can be fine-tuned for specific objectives. Fine-tuning repurposes the learned features of a pretrained deep neural networks which then can learn the unknown features needed for the new objective. Deep convolutional neural networks (CNN) trained on ImageNet @cite are commonly fine-tuned for different computer vision tasks. Fine-tuning significantly reduces the amount of training examples required to converge to a target objective @cite .
- Transfer learning has also been explored for unsupervised learning as well. In survey of how transferability can be applied to unsupervised learning @cite , the author stated that while the results look promising, transfer learning applications would improve significantly if the underlying variation in high-level features could be disentangled and made more invariant. In this work, we use applicability to demonstrate where in a network the features of an input go from invariant to variant. This point of inflection is where the CactusNet creates a branch and circumvents invariance at the more varying and more specific layers.
- The human mind identifies and clusters objects based on their features regardless of whether an object is known or not @cite . Adaptive resonance theory (ART) @cite @cite is a machine learning theory that attempts to determine whether an object belongs to a known object class by comparing the detected features of the object with the expected features of all known classes individually. If the smallest difference between the detected features of the object and some known class's expected features is within a set threshold then the object is classified and is considered to belong to that class. This threshold is known as the vigilance parameter. If the difference exceeds the vigilance parameter, however, the object is considered to belong to a new class. This allows ART to perform unsupervised learning as it classifies not based on a target class, but differences in features. Over the years, several new variations of ART have been proposed including Fuzzy ART @cite which, uses fuzzy logic to improve ART's stability.
- In addition to the results mentioned above, sparse light spanners with small stretch were studied also for: planar graphs @cite @cite , apex graphs @cite , bounded pathwidth graphs @cite @cite , bounded treewidth graphs @cite @cite , and graphs excluding fixed minors @cite @cite @cite . From the algorithmic perspective, there is a rich study of efficient spanners construction @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite .
- However, the hand-designed network architecture raises a high demand of knowledges in specific tasks and experience in building neural networks. Motivated by this, researchers investigate the automatic design of deep architectures more recently. Cross-stitching network @cite is proposed to learn an optimal linear combination of shared representations, and he2017adaptively @cite adaptively learn the weights of individual tasks. The work most close to our approach is @cite which first initializes a thin network from a pre-trained model by SOMP @cite and then widening the network through a branching procedure. However, these approaches generally explore a relatively limited search space.
- Several approaches explore to accelerate the searching procedure by reducing the expense of neural network training. baker2018accelerating @cite early stop the architecture evaluation process by predicting the performance of unobserved architectures based on a set of architecture features. brock2018smash @cite propose a hypernetwork to generate the neural network weights conditioned on its architecture instead of conducting back propagation training. pham2018efficient @cite search for an optimal sub-graph within a large computational graph where the neural network weights are shared across sub-graphs.
- In this work, we propose GNAS to novelly develop neural architecture optimization to multi-task learning. Different from existing neural architecture optimizing approaches, we propose two greedy strategies which largely reduce the computation cost of architecture optimization procedure. The intra-layer greedy strategy of GNAS is proposed based on the property of tree structure. And the inter-layer greedy strategy of GNAS is inspired by the layer-wise pretraining strategy of restricted Boltzmann machine (RBM) @cite @cite @cite . The greedy strategies lead to the efficiency of GNAS, also leading to effectiveness by ensuring a highly efficient searching in a very large search space.
- All supervised learning-based approaches use the labeled information to disentangle features. For example, @cite proposed a restricted Boltzmann machine-based method that considered a complicated manifolds as collections of sub-manifolds. @cite combined a semi-supervised VAE @cite and a probabilistic graphical model. @cite proposed a hierarchical model that first disentangled identity and non-identity features from face images and then disentangled poses and landmarks from the non-identity features. @cite proposed a disentangled representation learning method for pose-invariant face recognition based on a GAN @cite . To disentangle the GAN generator's input variables, @cite used a VAE, and @cite proposed a semi-supervised InfoGAN.
- The most studied unsupervised learning-based methods employ generative models, especially VAEs and InfoGANs @cite . VAEs combine Bayes and autoencoder approaches to embed encoded features based on a given probability distribution. Dupont @cite proposed a joint-VAE that disentangled continuous and discrete representations. @cite introduced an adjustable hyperparameter, @math , to a standard VAE to balance latent channel capacity and independence constraints with reconstruction accuracy. Kim and Mnih @cite added a discriminator to @math -VAE to estimate Total Correlation, and @cite decomposed a @math -VAE equation and refined it to improve the disentanglement ability without using additional hyperparameters. InfoGANs attempted to learn a generator such that they cheated the discriminator and maximized the mutual information between synthetic samples and newly introduced latent codes. Inspired by InfoGAN, @cite proposed combining generative adversarial imitation learning @cite with a variational lower bound of the mutual information. On the application side, for example, unsupervised approaches for sequential data @cite @cite @cite , and control and planing @cite @cite were also proposed.
- The extraction process focuses on segmentation process which involves dividing the document into blocks that are the smallest logical entity and then proceed with extraction in the later stages @cite . The segmentation process is divided into sub parts, which include generating neighbourhood graph, creating page divisions and generating whitespace density graph. For our research, we need to detect headings to make sure the algorithm knows from which part of the document to extract the text. To make sure that the extracted text does not have document header footer or text from another columns(if document has multiple columns), detection of header footer text and multiple columns is also required. So, the most efficient way for our application is to detect the relevant headings and then analyze the area of the document which contains that heading before extracting the text. The authors have not seen such work before.
- Multi-surface representations of 3D shapes are popular for categorization tasks. The seminal work by Chen al @cite proposes a 3D shape descriptor based on the silhouettes rendered from the 20 vertices of a dodecahedron surrounding the object. More recently, Su al @cite and Qi al @cite train CNNs on 2D renderings of 3D mesh models for classification. Qi al @cite compares CNNs trained on volumetric representations to those trained on multiview representations. Although both representations encode similar amounts of information, they showed that multiview representations significantly outperform volumetric representations for 3D object classification. Unlike our approach, these approaches use multiple projections as input rather than output.
- To synthesize multi-surface output representations, we train multiple decoders. Dosovitskiy al @cite show that CNNs can be used to generate images from high-level descriptions such as object instance, viewpoint, and transformation parameters. Their network jointly predicts an RGB image and its segmentation mask using two up-convolutional output branches sharing a high-dimensional hidden representation. The decoder in our network learns the segmentation for each output view in a similar manner.
- In experiments on 2D symbols, Tarr and Pinker @cite found that human perception is largely tied to viewer-centered coordinates; this was confirmed by McMullen and Farah @cite for line drawings, who also found that object-centered coordinates seem to play more of a role for familiar exemplars. Note that in the human vision literature, viewer-centered'' usually means that the object shape is represented as a set of images in the viewer's coordinate frame, and object-centered'' usually means a volumetric shape is represented in the object's coordinate frame. In our work, we consider both the shape representation (volumetric or surface) and coordinate frame (viewer or object) as separate design choices. We do not claim our computational approach has any similarity to human visual processing, but it is interesting to see that in our experiments with 3D objects, we also find a preference for object-centered coordinates for familiar exemplars (i.e., novel view of known object) and for viewer-centered coordinates in other cases.
- Networks of collaboration between scientists have been studied by Newman @cite . The authors argue that simple unweighted networks are unable to capture the strength of collaboration ties and propose a method to model the strength of collaboration by relying on the number of co-authored papers. The same author has later studied various properties of such networks @cite . @cite have studied collaboration networks from an evolving and self-organizing perspective. Behavioral experiments on the ability to solve problems collaboratively have been conducted by Kearns @cite . Online collaboration with different network topologies has been studied by Suri and Watts @cite . The exploration-exploitation trade-off in a collaborative problem solving task has been discussed by Mason and Watts @cite . Kittur and Kraut @cite studied various types of collaboration taking place between Wikipedia editors and measured the impact on quality of the resulting articles.
- The study, as well as the interpretation of proximity data from a social perspective has been a prolific research area. Recent studies @cite , have extracted social network properties from proximity sensor data. In particular, the authors propose a method to distinguish between strong and weak social ties, using the Bluetooth signal strength of users' cellphones. The authors observe that weak links, i.e. the interactions that have been observed less than once per day, have a lower probability of being observed at later times. Collaboration patterns between university students, collaborating in teams for their course assignments, have also been studied @cite . The authors consider the time spent in physical proximity, using university wifi logs, as a proxy to measure ties between students. Their analysis suggests that only strong ties matter in order to predict team performances. We also notice that the study of social properties from positional tracking is not limited to the human species, as a colony of ants as been recently tracked, at individual level, revealing complex hierarchical social structures @cite .
- The phenomenon of emergence has been studied in different domains, notably in the field of Complexity Science and in the context of agent based modeling. The term emergence has various definition across fields @cite , alike complexity @cite from which emergence has been suggested to arise from. Emergence generally refers to system-wide behaviors that cannot be explained by the sum of individual behaviors. Moreover, means of modeling emergence are still subject to debate. Counting interaction between agents is, however, a widely used method to infer complex behaviors in a system @cite .
- The task of community detection has been a well-studied problem, whose goal is to assign users to communities @cite . The most relevant line of research is probably the task of detecting communities, whose members can be part of multiple groups. Those lines of research have made use of Matrix Factorization methods in order to relax the assumption of communities being disjoint @cite @cite . Methods providing a direct way to embed the nodes of a network, thus generalizing the notion of network neighborhood, have recently been proposed @cite .
- The idea of constraining the Lipschitz constant of a network is conceptually related to quantifying the flatness of minima. While there is no single formalisation for what constitutes a flat minimum, the unifying intuition is that a minimum is flat when a small perturbation of the model parameters does not have a large impact on the performance of the model. @cite have shown that Lipschitz continuity is not a reliable tool for quantifying the flatness of minima. However, there is a subtle but very important difference between how they employ Lipschitz continuity, and how it is used by @cite and in this work. Neural networks are functions parameterised by two distinct sets of variables: the model parameters, and the features. @cite consider Lipschitz continuity with respect to the model parameters, whereas we consider Lipschitz continuity with respect the features being supplied to the network. The crux of the argument given by is that the Lipschitz constant of a network with respect to its weights is not invariant to reparameterisation.
- Dropout @cite is one of the most widely used methods for regularising neural networks. It is popular because it is efficient and easy to implement, requiring only that each activation is set to zero with some probability, @math , during training. An extension proposed by @cite , known as maxnorm, is to constrain the maginitude of the weight vector associated with each unit in some layer. One can also use multiplicative Gaussian noise, rather than Bernoulli noise. @cite provide a technique that enables automatic tuning of the amount of noise that should be applied in the case of Gaussian dropout. A similar technique exists for automatically tuning @math for Bernoulli dropout---this extension is known as concrete dropout @cite .
- : Supervised image-to-image translation @cite has achieved outstanding results where the data used for training is available in one-to-one pairs. Apart from adversarial loss, it uses L1 (reconstruction) loss as well, which has now become a common practice in these types of tasks.
- Unsupervised methods take samples of images from two distributions and learn to cross-translate between them. This introduces the well known issue of there being infinitely many mappings between the two unpaired image domains @cite @cite @cite @cite @cite and so further constraints are required to do well on the problem. @cite introduces the requirement that translations be cycle-consistent; mapping image @math to domain @math and back again to @math must yield an image that is close to the original. @cite takes a different approach, enforcing weight sharing between the early layers of the generators and later layers of the discriminators. @cite combines these two ideas and models each image domain using a VAE-GAN. @cite utilizes reconstruction loss and teacher loss instead of VAE using a pretrained teacher network to ensure the encoder output lies in a meaningful subregion.
- To our knowledge, only @cite has presented results in generating translations between multiple distribution samples. However, their generator is conditioned on supervised labels.
- Click @cite proposed the idea of modularization and applies it to routers. Recently, Slick @cite and OpenBox @cite were proposed to detailedly discuss modularized NFs and decouple control plane and data plane of modularized NFs for easy management. Besides, OpenBox focused on merging elements to shorten the processing path length. However, above works mainly focus on orchestration-level module management and are orthogonal to our optimizations on performance-aware placement and dynamically scaling.
- CoMb @cite designed a detailed mechanism to consolidate middleboxes together to reduce provisioning cost. Furthermore, Flurries @cite and NFVnice @cite were proposed to share CPU cores among different NFs with the technique of Docker Container @cite . By modifying Linux scheduling methods, they achieved almost no loss in NF sharing. However, they operated on monolithic NF level and did not consider the problem of which elements (NFs) to consolidate. However, their development details and infrastructure designs could complement our work as the low-level implementation.
- Several learning-based and CNN-based approaches are also developed for color-guided depth image enhancement @cite @cite @cite , where the structural interdependency between intensity and depth image is modeled and exploited to reconstruct high quality depth image. For guided depth image enhancement, @cite present a CNN model to learn multi-scale guidance, while @cite incorporate weighted analysis representation and truncated inference for dynamic guidance learning. For general guided filtering, @cite construct CNN-based joint filters to transfer structural details from guided image to reconstructed image. However, these approaches assume that the guided image is spatially well aligned with the degraded observation. Due to that the guided image and degraded observation usually are different in pose and expression, such assumption generally does not hold true for guided face restoration. To address this issue, a WarpNet is introduced in our GFRNet to learn a flow field for warping the guided image to the desired pose and expression.
- Recently, spatial transformer networks (STNs) are suggested to learn a spatial mapping for warping an image @cite , and appearance flow networks (AFNs) are presented to predict a dense flow field to move pixels @cite @cite . Deep dense flow networks have been applied to view synthesis @cite @cite , gaze manipulation @cite , expression editing @cite , and video frame synthesis @cite . In these approaches, the target image is required to have the similar lighting condition with the input image to be warped, and the dense flow networks can thus be trained via reconstruction learning. However, in our guided face restoration task, the guided image and the target image usually are of different lighting conditions, making it less effective to train the flow network via reconstruction learning. Moreover, the ground-truth dense flow field is not available, further increasing the difficulty to train WarpNet. To tackle this issue, we use the face alignment method @cite to extract the face landmarks of guided and target images. Then, the landmark loss and TV regularization are incorporated to facilitate the WarpNet training.
- Several RSP systems have been implemented over the last few years. The most popular ones correspond to centralized engines, C-SPARQL @cite , CQELS @cite , ETALIS @cite and SPARQL @math @cite . Systems like CQELS-cloud @cite and Strider @cite tackle the scalability issue but distributing stream processing. Available RSP systems are equipped with their own syntax, which generally take the form of continuous versions of the standard SPARQL grammar. This limits the expressiveness on temporal logical operators, the combination or even nesting of window operators. Moreover, the support of recursion is also missing.
- Both StreamRule @cite and its recent parallelized version StreamRule @math @cite use a RSP engine for data stream pre-filtering and Clingo as the ASP solver. The expressiveness of BSP implementation in BigSR can fully cover StreamRule and StreamRule @math , since the implementation in these two reasoners stay on positive stratified Datalog program. Evaluation of StreamRule StreamRule @math showcases that the average throughput is around thousand-triples second with second-level delay. Comparatively, our BSP implementation has almost three orders of magnitude gains. Laser @cite and Ticker @cite are both stream processing systems based on the LARS framework but do not concentrate on scalability. Ticker concentrates incremental model maintenance and sacrifices performance by relying on an external ASP engine (Clingo). Laser also proposes an incremental model based on time interval annotations which can prevent unnecessary re-computations. Although Laser claims to represent a trade-off between expressiveness and data throughput, it cannot scale the way BigSR enables to. This is mainly due to Laser's inability to distribute stream processing.
- One traditional approach for color enhancement is transferring the color of an example image to a given input image. It is originated from @cite in which the global color distribution of an input image is warped to mimic an example style. There are many subsequent works to improve this technique @cite . While this approach can provide expressive enhancement and diverse stylizations, the results highly depend on example images while providing proper exemplars is challenging. Recent works @cite @cite @cite (semi-)automate exemplar selection by image retrieval methods. Liu al @cite used a keyword-based image search to choose example images. Lee al @cite learn a content-specific style ranking using a large photo collection and select the best exemplar images for color enhancement. For pixel-wise local enhancement, Hwang al @cite find candidate images from a database then search local color enhancement operators.
- Learning-based color enhancement is another dominant stream @cite @cite @cite . Bychkovsky al @cite present a number of input and retouched image pairs called MIT-Adobe FiveK, which is created by professional experts. They used this data to train a model for color and tone adjustment. Yan al @cite propose a deep learning method to learn specific enhancement styles. Given the features of color and semantic context, a deep neural network as a non-linear mapping function is trained to produce the pixel color of specific styles.
- As in the work of @cite , one simple approach to exploit multiple visual features is to build an ensemble of distance functions, in which each distance function is learned using a single feature and the final distance is calculated from a weighted sum of these distance functions. However, the usage of predetermined weights is undesirable as highly discriminative features in one environment might become irrelevant in another one. In their work, a model to learn weights of these distance functions by optimizing the relative distance or by maximizing the average rank-k recognition rate is proposed. @cite proposed a novel re-ranking method based on a fusion scheme that reweights an ensemble of distance metric outcomes according to their discriminative capacity. They particularly show that the fused distance perform largely better than any of the distances inferred by each feature separately.
- To consider spatial information, a common usage in person re-identification is to divide the person image into few regions stripes and concatenate dense local features to implicitly encode the spatial layout of the person. @cite proposed a model for person re-identification that combines spatial constraints and the polynomial feature map @cite into a unified framework. They mention that enforcing the matching within corresponding regions can effectively reduce the risk of mismatching and become more robust to partial occlusions. In addition, their framework can benefit from the complementarity of global and local similarities.
- The post-ranking method for person re-identification is a relatively unexplored area @cite which has been attracting a lot of attention from the research community. Prates and Schwartz @cite presented a Color-based Ranking Aggregation (CBRA) meth -od, which explores different feature representations to obtain complementary ranking lists, and combine them in order to improve person re-identification. In their work, the KISSME @cite metric learning was adopt -ed and different strategies for ranking aggregation, based on the Stuart rank aggregation method @cite , were proposed. Garc ' @cite @cite related that inspections on the ranked matches can be applied to refine the output in such a way that the correct match will have higher probability to be found in the first ranks. Hence, their work is founded on the idea that a ranking, achieved by any algorithm, contains valuable information which can be further exploited to improve the rank of the true match. To achieve such a goal, they propose an unsupervised post-ranking framework. Once the initial ranking is available, content and context sets are extracted. Then, these are exploited to remove the visual ambiguities and to obtain discriminant feature space which is finally exploited to compute the new ranking.
- @cite studied person re-identification with man -i -fold-based affinity learning. In their work, a novel affinity learning algorithm called Supervised Smoothed Manifold (SSM) is proposed, which can be plunged into most existing algorithms, serving as a generic postprocessing procedure to further boost identification accuracy.
- In relation to domain adaptation in machine learning, @cite proposed a schema called Mirror Representation to address the view-specific feature distortion problem in person re-identification. It embeds the view-specific feature transformation and enables alignment of the feature distributions across disjoint views for the same person. Zhang and collaborators @cite argue that most existing approaches focus on learning a fixed distance metric for all instance pairs, while ignoring the individuality of each person. They formulate person re-identification as an imbalanced classification problem and learn a classifier specifically for each pedestrian such that the matching model is highly tuned to the individual appearance.
- Although a large number of existing approaches have exploited state-of-the-art visual features, advanced metric learning algorithms, post-ranking or ranking aggregation strategies, domain adaptation based models or even CNN based ones, state-of-the-art results on commonly evaluated person re-identification benchmarks is still far from the accuracy performance needed for most real-world surveillance applications @cite .
- Activity recognition has been a popular research topic in computer vision @cite @cite @cite @cite @cite . Hand-crafted features, such as dense trajectories @cite gave promising results on many datasets. More recent works have focused on learning CNNs for activity recognition @cite @cite . Two-stream CNNs take spatial RGB frames and optical flow frames as input @cite @cite . 3D XYT convoltuional models have been trained to learn spatio-temporal features @cite @cite @cite @cite . To train these CNN models, large scale datasets such as Kinetics @cite , THUMOS @cite , and ActivityNet @cite have been created.
- Many works have explored temporal feature aggregation for activity recognition. @cite compared various pooling methods and found that LSTMs and max-pooling the entire video performed best. @cite found that pooling intervals of different locations lengths was beneficial to activity recognition. @cite found that learning important sub-event intervals and using those for classification improved performance.
- Recently, segment-based 3D CNNs have been used to capture spatio-temporal information simultaneously for activity detection @cite @cite @cite . These approaches all rely on the 3D CNN to capture temporal dynamics, which usually only contain 16 frames. Some works have studied longer-term temporal structures @cite @cite @cite @cite , but it was generally done with a temporal pooling of local representations or (spatio-)temporal convolutions with larger fixed intervals. Recurrent neural networks (RNNs) also have been used to model activity transitions between frames @cite @cite @cite .
- Sound source separation, also known as the cocktail party problem" @cite @cite , is a classic problem in engineering and perception. Classical approaches include signal processing methods such as Non-negative Matrix Factorization (NMF) @cite @cite @cite . More recently, deep learning methods have gained popularity @cite @cite . Sound source separation methods enable applications ranging from music vocal separation @cite , to speech separation and enhancement @cite @cite @cite . Our problem differs from classic sound source separation problems because we want to separate sounds into visually and spatially grounded components.
- Our work builds off efforts to learn perceptual models that are self-supervised'' by leveraging natural contextual signals in images @cite @cite @cite @cite @cite , videos @cite @cite @cite @cite @cite @cite , and even radio signals @cite . These approaches utilize the power of supervised learning while not requiring manual annotations, instead deriving supervisory signals from the structure in natural data. Our model is similarly self-supervised, but uses self-supervision to learn to separate and ground sound in vision.
- Sai et. al @cite have introduced a simple baseline that addresses the discrete output space problem without relying on gradient estimators and shows that it is able to achieve state-of-the-art results on a Chinese poem generation dataset and presented quantitative results on generating sentences from context-free and probabilistic context-free grammars, and qualitative language modeling results. A conditional version is also described that can generate sequences conditioned on sentence characteristics.
- Ofir et. al @cite have shown that recurrent neural networks can be trained to generate text with GANs from scratch using curriculum learning, by slowly teaching the model to generate sequences of increasing and variable length. They empirically show that their approach vastly improves the quality of generated sequences compared to a convolutional baseline.
- Henning et. al @cite present theoretical arguments why using a weaker regularization term enforcing the Lipschitz constraint is preferable. These arguments are supported by experimental results on several data sets. For stable training of Wasserstein GANs, they propose to use the following penalty term to enforce the Lipschitz constraint that appears in the objective function:
- Early studies of abdominal organ segmentation focused on atlas-based methods @cite @cite @cite . The frameworks are usually problematic because 1) they are not able to capture the large inter-subject variations of abdominal regions and 2) computational time is tightly dependent on the number of atlases. Recently, learning-based approaches with relatively large dataset have been introduced for multi-organ segmentation @cite @cite @cite . Especially, deep Convolutional Neural Networks (CNNs) based methods have achieved a great success in the medical image segmentation @cite @cite @cite @cite @cite @cite @cite in the last few years. Compared with multi-atlas-based approaches, CNNs based methods are generally more efficient and accurate. CNNs based methods for multi-organ segmentation can be divided into two major categories: 3D CNNs @cite @cite @cite based and 2D CNNs @cite @cite @cite @cite based. 3D CNNs usually adopt the sliding-window strategy to avoid the problem, leading to high time complexity. Compared with 3D CNNs, 2D CNNs based algorithms can be directly end-to-end trained using 2D deep networks, which is less time-consuming.
- The most commonly used techniques for semi-supervised learning include self-training @cite @cite , co-training @cite , multi-view learning @cite and graph-based methods @cite @cite .
- In self-training, the classifier is iteratively re-trained using the training set augmented by adding the unlabeled data with their own predictions. The procedure repeated until some convergence criteria are satisfied. In such case, one can imagine that a classification mistake can reinforce itself. Self-training has achieved great performances in many computer vision problems @cite @cite and recently has been applied to deep learning based semi-supervised learning in the biomedical imaging domain @cite .
- Co-training @cite assumes that (1) features can be split into two independent sets and (2) each sub-feature set is sufficient to train a good classifier. During the learning process, each classifier is retrained with the additional training examples given by the other classifier. Co-training utilizes multiple sets of independent features which describe the same data, and therefore tends to yield more accurate and robust results than self-training @cite . Multi-view learning @cite , in general, defines learning paradigms that utilize the agreement among different learners. Co-training is one of the earliest schemes for multi-view learning.
- Graph-based semi-supervised methods define a graph where the nodes are labeled and unlabeled examples in the dataset, and edges reflect the similarity of examples. These methods have been widely adopted in non-deep-learning based semi-supervised learning algorithms in the biomedical imaging domain @cite @cite @cite .
- Regarding the problem of searching in graphs without errors, many papers have been devoted to trees, mainly because it is a structure that naturally generalizes paths, which represents the classical binary search (see e.g. @cite for search in a path with non-uniform query times). This query model in case of trees is equivalent to several other problems, including vertex ranking @cite or tree-depth @cite . There exist linear-time algorithms for finding optimal query strategies @cite @cite . A lot of effort has been done to understand the complexity for trees with non-uniform query times. It turns out that the problem becomes hard for trees @cite @cite . Also refer the reader to works on a closely related query game with edge queries @cite @cite @cite @cite @cite . For general graphs, a strategy that always queries a 1-median (the minimizer of the sum of distances over all vertices) has length at most @math @cite .
- @cite @cite pointed out that UESS can be divided into two parts: predicting expressive information from text; and synthesizing the speech with a particular expression. In this paper only the latter stage is considered for simplicity.
- While we did not find any other framework addressing wireless experimentation with CI support, we have identified three existing frameworks that are to some extent similar to COINS, i.e., the control and Management Framework (OMF) @cite , the Network Implementation Testbed Laboratory (NITOS) @cite and the Berlin Open Wireless Lab (BOWL) @cite . We performed a feature-wise comparison of these frameworks and summarized it in Table ,.
- In the comparison, we considered an extensive list of features that can be used to compare the experimentation systems @cite . The list of features, however, lacks the CI-specific properties that our work focuses on; therefore, we extended the list with four core CI concepts @cite @cite : Existence of a single source repository that contains everything needed for the completely automated build process. Support for completely automated build process executed on each commit. Self-tests included inside the repository, which run on each build. Fast build process so that each commit can be built and tested.
- Whilst online fora have attracted much attention as a way of exploring political dynamics @cite @cite @cite @cite @cite @cite , and the effect of abuse and incivility in these contexts has been explored @cite @cite @cite @cite , little work exists regarding the abusive and intimidating ways people address politicians online; a trend that has worrying implications for democracy. collected tweets centred around candidates for the European Parliament election in 2014 from Spain, Germany, the United Kingdom and France posted in the month surrounding the election. They find that the extent of the abuse and harrassment a politician is subject to correlates with their engagement with the medium. Their analysis focuses on the way in which uncivil behaviour negatively impacts on the potential of the medium to increase interactivity and positively stimulate democracy. Stambolieva studies online abuse against female Members of Parliament (MPs) only; in studying male MPs as well, we are able to contrast the level of abuse they each receive. Furthermore, we contrast proportional with absolute figures, creating quite a different impression from the one she gives.
- A larger body of work has looked at hatred on social media more generally @cite @cite @cite @cite . Williams and Burnap present work demonstrating the potential of Twitter for evidencing social models of online hate crime that could support prediction, as well as exploring how attitudes co-evolve with events to determine their impact @cite @cite . use natural language processing (NLP) to identify the groups targeted for hatred on Twitter and Whisper.
- Work exists regarding accurately identifying abusive messages automatically @cite @cite @cite @cite @cite . The work of has been described as the state of the art, with precision recall of 0.63 being reported as equivalent to human performance. report a human interannotator agreement of only 0.69 in annotating racial slurs, and report a human agreement of 0.8 with a Krippendorf's alpha of 0.25 on UK data, demonstrating that the limiting factor is the complexity of the task definition. Burnap and Williams particularly focus on hate speech with regards to protected characteristics such as race, disability and sexual orientation. Waseem and Hovy also focus on hate speech, and share a gold standard UK Tweet corpus. seek to identify the problem accounts rather than the problem material. Schmidt and Wiegand provide a review of prior work and methods.
- The study of safe learning dates back to the beginning of this century @cite . In @cite and @cite , Lyapunov-based reinforcement learning is used to allow a learning agent to safely switch between pre-computed baseline controllers. Then, in @cite , risk-sensitive reinforcement learning is proposed, in which the expected return is heuristically weighted with the probability of reaching an error state. In several other papers, including @cite , @cite and @cite , safe exploration methods are utilized to allow the learning modules to achieve a desired balance between ensuring safe operation and exploring new states for improved performance. In @cite , a general framework is proposed for ensuring safety of learning-based control strategies for uncertain robotic systems. In this framework, robust reachability guarantees from control theory are combined with Bayesian analysis based on empirical observations. The result is a safety-preserving, supervisory controller of the learning module that allows the system to freely execute its learning policy almost everywhere, but imposes control actions to ensure safety at critical states. Despite its effectiveness for ensuring safety, the supervisory controller in this approach has no role in reducing tracking errors.
- Focusing our attention on safe, learning-based inverse dynamics control, we refer to @cite @cite @cite . In @cite , a model reference adaptive control (MRAC) architecture based on Gaussian processes (GPs) is proposed, and stability of the overall control system is proved. While the approach in @cite is based on adaptive control theory, our approach is based on robust control theory. In particular, in @cite , the mean of the GP is used to exactly cancel the uncertainty vector, while in our approach, we use both the mean and variance of the GP to learn an upper bound on the uncertainty vector to be used in a robust, outer-loop controller. Hence, unlike @cite , in our approach, the uncertainty of the learning module is not only incorporated in the stability analysis but also in the outer-loop controller design. Intuitively, the less certain our GPs are, the more robust the outer-loop controller should be for ensuring safety. When more data is collected and the GPs are more certain, the outer-loop controller can be less conservative for improved performance. While the results of @cite are tested in simulations on a two-dimensional system, we test our results experimentally on a 6 DOF manipulator.
- In @cite @cite , GPs are utilized to learn the errors in the output torques of the inverse dynamics model online. In @cite , the GP learning is combined with a state-of-the-art gradient descent method for learning feedback terms online. The main idea behind this approach is that the gradient descent method would correct for fast perturbations, while the GP is responsible for correcting slow perturbations. This allows for exponential smoothing of the GP hyperparameters, which increases the robustness of the GP at the cost of having slower reactiveness. Nevertheless, @cite does not provide a proof of the robust stability of the closed-loop system. @cite , the variance of the GP prediction is utilized to adapt the parameters of an outer-loop PD controller online, and the uniform ultimate boundedness of the tracking error is proved under some assumptions on the structure of the PD controller (e.g., the gain matrix was assumed to be diagonal, which imposes a decentralized gain control scheme). The results of @cite are verified via simulations on a 2 DOF manipulator.
- Profiling plays a key role in managed runtimes, either for code optimization or memory management decisions @cite @cite @cite @cite @cite @cite @cite . We focus on getting quality profiling information to drive object pretenuring. is, to the best of our knowledge, the first online profiler targeting the dynamic pretenuring of objects in Big Data applications running on HotSpot. This section compares our work with state-of-art systems, namely, off-line and online profilers that guide systems where small changes are needed in the heap organization and collection. It ends with a comparative analysis of systems that demand a more profound change, either to the application framework or the runtime itself, in some cases manipulating application-defined types and or organizing the heap in special purpose regions, and placing data directly in an off-heap space.
- @cite introduced an algorithm where an object lifetime is tracked based on timestamps assigned when the object lost an incoming reference, and when GC identifies it as an unreachable object. This is implemented in a tracing tool called Merlin that, when is analyzing a program, it can be up to 300 times slower compared to a non-profiled run. @cite uses the same algorithm but adds new functionalities to his Elephant track tool in terms of precision and comprehensiveness of reference sources (i.e. weak reference). Another system, Resurrector @cite , relaxes on precision to provide faster profiling but still introduces 3 to 40 times slowdown depending on the workload.
- @cite extends the profile-based pretenuring of Cheng's solution @cite using the Merlin tracing tool @cite . They have a two stage profiling. The first stage happens during the build of the JVM @. Profiling at this stage is used to improve the performance of JVM itself, since Jikes RVM @cite is a meta-circular JVM. reports this is particularly useful for tight heaps (i.e. heaps that are just above the minimum size for a given application, reaching at most 150MB) and not suitable for heaps with Gigabytes of objects. The second stage is an application-specific process, based on the off-line profiling made with Merlin @cite .
- @cite presents an headroom schema which drives pretenuring based on the space left on the heap before garbage collection is necessary. Although their solution brings advantages to collection times, they push much of the overhead to the mutator and also to the off-line process, which is not always possible or accurate. This approach makes the classification not only dependent on the application but also on the overall heap size. Finally, @cite do not target large heaps or a modern garbage collector like G1.
- NG2C @cite extends G1 to support object pretenuring. However, it also needs offline profiling and programmer's help to identify the generation where a new object should be allocated. Thus, we can say that it uses an off-line profiler to establish a relation between allocation sites and object lifetimes, missing the opportunity to avoid inhomogeneous allocation behavior @cite . @cite extends the operation of the Immix garbage collector in Jikes RVM @cite with a new programming interface between the application and the GC, in order to manage dominant data structures (i.e. a data structure holding most of the objects during the lifetime of the program) more efficiently.The main advantage comes from reducing the occurrence of highly entangled deep-shaped data structures lay-out in memory, thus improving performance of the parallel tracing stage.
- needs the calling context to profile objects at relevant allocation sites. Ball and Laurus @cite compute a unique number for each possible path of a control flow graph inside a procedure. The computation is done offline and added to the source code. This is not suited for because modern workloads have many possible paths inside each routine, and the technique can not capture the inter-procedure path needed for to distinguish allocation sites. Bond and McKinley @cite also compute a single value, but at runtime, to determine the sequence of active stack frames in a inter-procedural way. However, they need to maintain non-commutativity to differentiate call orders, such as calling methods @math and @math . This is not a requirement for and so we can have a smaller impact on code instrumentation, with low impact on the throughput. uses an adaptive solution to store allocation contexts, with a low ratio of collisions (as shown in Table ) while using fewer bits to store information.
- @cite performs online profiling to optimize memory allocation operations; it performs a single allocation (improvement over allocation inlining) of a block big enough to fit multiple objects that are allocated in sequence. Objects need not have parent-child or sibling relationships among them because it is the control-flow relations between hot code flows and locations that are monitored, as we address in our work. So, while allocations may take place in different methods, equivalent GC behavior is ensured. However, while doing online profiling, it only addresses initial allocation and not the issue of pretenuring objects.
- Recently, the vision community has witnessed the success of deep learning, and researchers have used the models in the field of deep learning, such as convolutional deep belief network @cite , deep convolutional neural network @cite , and deep convolutional generative adversarial nets (GAN) @cite , to model 3D objects for the sake of synthesis and analysis. Our proposed 3D model is also powered by the ConvNets. It incorporates a bottom-up 3D ConvNet structure for defining the probability density, and learns the parameters of the ConvNet by an analysis by synthesis'' scheme.
- Our model is related to the following descriptive models. The FRAME (Filters, Random field, And Maximum Entropy) @cite model, which was developed for modeling stochastic textures. The sparse FRAME model @cite @cite , which was used for modeling object patterns. Inspired by the successes of deep convolutional neural networks (CNNs or ConvNets), @cite proposes a deep FRAME model, where the linear filters used in the original FRAME model are replaced by the non-linear filters at a certain convolutional layer of a pre-trained deep ConvNet. Instead of using filters from a pre-trained ConvNet, @cite learns the ConvNet filters from the observed data by maximum likelihood estimation. The resulting model is called generative ConvNet, which can be considered a recursive multi-layer generalization of the original FRAME model.
- Building on the early work of @cite , recently @cite @cite have developed an introspective learning method to learn the energy-based model, where the energy function is discriminatively learned.
- Analogous to region proposals in image domain, temporal action proposals are candidate temporal windows that possibly contain actions. Sparse-prop @cite applies dictionary learning for generating class-independent proposals. S-CNN @cite uses 3D convolutional neural networks (CNNs) @cite to generate multi-scale segments (proposals). TURN TAP @cite uses clip pyramid features in their model, and it predicts proposals and refines temporal boundaries jointly. DAPs @cite first applies Long Short-Term Memory (LSTM) @cite to encoding video content in a sliding window and then predicts proposals covered by the window. Built on @cite , SST @cite further takes long sequence training problem into consideration and generates proposals in a single pass. However, all these methods either fail to produce long proposals or do not exploit future context. In contrast, our model for temporal proposal tackles these two problems simultaneously.
- While aforementioned captioning methods generate only one sentence for an input video, video paragraph generation focuses on producing multiple semantics-fluent sentences. Rohrbach adapted statistical machine translation (SMT) @cite to generate semantic consistent sentences with desired level of details @cite . Yu proposed a hierarchical RNN to model both cross-sentence dependency and word dependency @cite .
- Our approach belongs to the class of model-based meta-learning methods called memory-augmented neural networks @cite . We similarly use a memory containing the training data and maximize test set performance over the distribution of datasets, but rather than construct this memory one sample at a time through a Neural Turing Machine, we make use of a self-attention based architecture to process all of the training set and its internal relationships in parallel, in a strictly permutation-invariant fashion.
- Metric-based meta-learning approaches have also been applied to the general problem of one-shot learning, with most of the results being in the image domain. This is a family of approaches which use a combination of an embedding transformation with interpretation of the embeddings in the context of an imposed metric space in order to achieve one-shot learning of similarity relationships in a shared domain (e.g. visual similarity). These approaches include matching networks @cite , siamese networks @cite , and FaceNet @cite . The work Towards a Neural Statistician'' @cite learns latent spaces which explain the statistics of the input (it extracts summary statistics from domain examples), which naturally produces a space that has good properties for metric-based one-shot learning approaches on datasets.
- In optimization-based meta-learning, the process of gradient descent can be formulated as a recurrently applied differentiable transform which should be optimized to reduce the final loss at the end of a period of training @cite . In MAML @cite , this is cast as a form of transfer learning problem, and as such the parameters of the model used to solve the particular task are still the network parameters (rather than some latent representation of the task inferred directly from the training set), but for which the model as a whole has been adapted such that only a small number of gradient descent steps will be needed in order to obtain high performance. This sort of optimization-based approach has recently been explicitly applied to the problem of few-shot classification @cite .
- Attentive meta-learning @cite is similar to our approach in that an attention mechanism is used to effectively make use of rich conditioning information which acts to specify the particular sub-task which must be solved in a given context.
- ZSL relies on the semantic space to associate source and target classes. Various semantic spaces have been investigated, including attributes @cite @cite @cite @cite @cite , word vector @cite @cite , text description @cite @cite and human gaze @cite . The attribute has been shown to be an effective semantic space @cite @cite @cite for ZSL. However, its superior performance is obtained at the cost of much more expensive human labor. As an alternative, the word vectors are gaining more attention recently @cite @cite since they are learned from the large text corpus in an unsupervised way. Albeit their popularity, the word vectors often suffer from visual-semantic discrepancy problem @cite @cite @cite . In addition to the word vectors, human gaze @cite is recently proposed to replace the attributes, as its annotation can be performed by non-experts without domain knowledge.
- @math D face reconstruction was first introduced for recognition by Blanz and Vetter @cite . They reconstructed @math D faces by fitting @math DMM to @math D face images, and used the obtained @math DMM parameters as features for face recognition. Their employed @math DMM fitting method is essentially an image-based analysis-by-synthesis approach, which does not consider the features unique to different individuals. This method was recently improved by @cite via pooling the @math DMM parameters of the images of the same subject and using a CNN to regress the pooled parameters. They experimentally proved the improved discriminative power of their obtained @math DMM parameters.
- Instead of using @math DMM parameters for recognition, @cite proposed to recover pose and expression normalized @math D face shapes directly from @math D face landmarks via cascaded regressors and match the reconstructed @math D face shapes via the iterative closest point algorithm for face recognition. Other researchers @cite @cite utilized the reconstructed @math D face shapes for face alignment to assist extracting pose-robust features.
- To summarize, . @cite and @cite , even though the identity of @math D face shapes in the training data is stressed, respectively, by pooling @math DMM parameters and by normalizing pose and expression, their methods of learning mapping from @math D images to @math D face shapes are in the sense of utilizing identity labels of the training data (see Fig. ).
- @cite proposed to represent @math D face shapes by @math D volumetric coordinates, and train a CNN to directly regress the coordinates from the input @math D face image. Considering the high dimensionality of original @math D face point clouds, as a compromise, they employed @math D volumetric representations. In consequence, the @math D face shapes generated by their method are of low resolution, which are apparently not favorable for face recognition.
- One of the standard ways of handling missing values is imputing values based on some predictive model, and then applying the analysis on a fully observed dataset. To exploit the graph structure, previous studies have proposed imputation of missing values based on the exponential random graph model @cite . The limitation of such an approach is that it is slow, as it requires Gibbs sampling, and so it cannot handle large graphs. Imputation of missing values can also be accomplished using matrix (or tensor) factorization methods. These methods can impute missing values with high accuracy even when large percentages (up to 95 , imputation-based methods use only point estimates of the missing values, effectively ignoring the prediction uncertainty when learning with imputed values. Techniques known as Multiple Imputation (MI) try to correct for this drawback, by sampling from the posterior distribution of missing values. On the other hand, these techniques can be less effective when a larger fraction of data is missing @cite , and can be computationally very demanding.
- There also exist variants of the conditional graphical CRF models for regression (e.g. the CCRF @cite , or GCRF @cite models). However, these structured regression models are not designed to cope with unlabeled data, other than ignoring the portion of data with missing labels.
- Within GTSAM framework, @cite uses door signs and walls as landmarks in SLAM. Door-signs are detected by a SVM-based classifier upon Histogram of Oriented Gradient (HOG) features. Walls are extracted from laser data using RANSAC. It is important to note that the SLAM works offline, i.e., it works after all observations data available. This is understandable regarding the SLAM works only with small number of landmarks (i.e., walls and door-signs) which is insufficient to make GTSAM framework works accurately online.
- Video QA is a relatively new task compared with image QA. Yu al @cite adopted a semantic attention mechanism, which combines the detected concepts in videos with text encoding decoding to generate answers. Comparing with images, temporal domain is unique to videos. A temporal attention mechanism is leveraged to selectively attend to one or more periods of a video in @cite @cite @cite . Besides temporal attention mechanism, Jang al @cite and Xu al @cite also utilized motion information along with appearance information in videos. Recently Na al @cite and Kim al @cite both introduced the memory mechanism to their models for video QA. However, their models @cite @cite both lack motion analysis and dynamic memory update mechanism.
- To answer the video-based questions correctly, temporal analysis of videos is necessary. Shou al @cite presented a multi-stage Segment-CNN model to generate action proposals and localize actions in videos. Temporal Unit Regression Network (TURN) @cite and Cascaded Boundary Regression (CBR) @cite exploit the temporal boundary regression mechanism for proposal generation and action detection. Recently Gao al @cite and Hendricks al @cite proposed to localize activities by language queries, their methods involve of joint modeling of the videos and language queries, which also related to video QA.
- To the best of our knowledge, this paper is the first that presents a theoretical analysis of the numerical error of Toom-Cook convolution. We demonstrate that the algorithm is unstable because the properties of algorithm parameters matrices we use ( @math , @math , @math ). We formulate the boundaries of the floating point errors for @math and @math dimensional kernels and inputs so we can resonable choose what we should focus on to improve the accuracy. We formulated the error boundaries for Toom-Cook convolution using similar techniqes to those used for another bilinear problem: the fast matrix multiplication, error estimation by Bini and Lotti @cite , @cite and @cite . We show that algorithm is unstable and how the error depends on each component.
- Nowadays, generative models for image data are highly popular, including the Variational Autoencoder @cite and GANs @cite . Among them, GANs is a common way for generating convincing images. DCGAN @cite changes the structure of vanilla GANs, which uses convolutional structure instead of full connected networks. The fascinating experience is that they try to do vector arithmetic. Like "smiling woman - neutral woman + neutral man" to get a "smiling man". BEGAN @cite set the discriminator in auto-encoder format, which also improves the stability in training process.
- The latent spaces in DCGAN @cite and BEGAN @cite are in uniform distribution in the range of [-1,1]. They both have operated linear interpolation between some random points in their latent spaces. Many GANs @cite are trying to understand the latent space in vector, which makes sense at a high cost of time. But if the network's structure is changed, the latent space in GANs will also be changed. We introduce one perfect Euclidean latent space to entitle GANs the power of understanding images.
- FaceNet @cite can directly learn the mapping from images to their latent vector in Euclidean space end-to-end. It is used for face verification since we could compare the similarity between two face images by their Euclidean distances in this Euclidean space. In this paper, we directly use the pre-trained FaceNet model for cropped CelebA images.
- Image-based human pose estimation has many applications, for a comprehensive survey, see @cite . Early approaches such as the histogram of oriented gradients (HOG) and deformable parts model (DPM) rely on hand-craft features and graphical models @cite @cite @cite @cite @cite @cite . These methods suffer from the limited representation capabilities and are not extensible to complex scenarios.
- Recently, Wei @cite used very deep sequential conv-deconv architecture with large receptive fields to directly perform pose matching on the heatmaps. They also enforced intermediate supervision between conv-deconv pairs to prevent gradient vanish, thus a very deep network became feasible, and the deeper network can learn the keypoints relationship with lager receptive field. The hourglass module proposed by Newell @cite is an extension of Wei with the addition of residual connections between the conv-deconv sub-modules. The hourglass module can effectively capture and combine features across scales. Chu @cite adopted stacked hourglass networks to generate attention maps from features at multiple resolutions with various semantics. Yang @cite designed a pyramid residual module (PRM) to enhance the deep CNN invariance across scales, by learning the convolutional filters on various feature scales.
- State-of-the-art DNNs for pose estimation are still limited in the capability of modeling human body structural for effective keypoint matching. Existing methods rely on a brute-force approach by increasing network depth to implicitly enrich the keypoint relationship modeling capability. A major weakness in this regard is the ambiguities arising from the occlusions, clutter backgrounds, or multiple body parts in the scene. In the MPII pose benchmark @cite , many methods @cite @cite @cite @cite @cite rely on repeating their pose estimation pipeline multiple times in various scales, in order to improve performance by a small margin using averaging of results. This indicates the lack of an effective solution to handle scale and structural priors in the modeling.
- This work builds on earlier work by the author. In particular, cyclical learning rates were introduced by @cite and later updated in @cite . Section provides updated experiments on super-convergence . There is a discussion in the literature on modifying the batch size instead of the learning rate, such as discussed in @cite .
- Several recent papers discuss the use of large learning rate and small batch size, such as @cite @cite @cite . They demonstrate that the ratio of the learning rate over the batch size guides training. The recommendations in this report differs from those papers on the optimal setting of learning rates and batch sizes.
- There also exist approaches to learn optimal hyper-parameters by differentiating the gradient with respect to the hyper-parameters (for example see @cite ). The approach in this report is simpler for the practitioner to perform. @PARASPLIT
- Gando et al @cite presented a deep learning method based on a fine-tuned deep convolutional neural network. This method can automatically distinguish illustrations from photographs and achieve 96.8 Rahmouni et al @cite presented a custom pooling layer to extract statistical features and a CNN framework to distinguish computer-generated graphics from real photographic images. A weighted voting scheme was used to aggregate the local estimates of class probabilities and predict the label of the whole picture. The best accuracy in @cite is 93.2
- Different digital cameras introduce different noise to their output digital images. The main noise sources are due to the imperfection of CCD or CMOS sensors. It has been named as sensor pattern noise (SPN) and is used as a fingerprint to characterize an individual camera. In particular, SPN has been used in image forgery detection @cite and source camera identification @cite .
- Villalba et al @cite presented a method for video source acquisition identification based on sensor noise extraction from video key frames. Photo response non-uniformity (PRNU) is the primary part of the sensor pattern noise in an image. @cite , the PRNU is used to calculate the sensor pattern noise and characterize the fingerprints into feature vectors. Then, the feature vectors are extracted from the video key frames and trained by a SVM-based classifier.
- To preserve multi-scale structure information, some random walk and matrix factorization methods @cite @cite @cite have been proposed. GraRep @cite accurately calculates @math -order proximity matrix, and computes specific representation for each @math using SVD based dimension reduction method, and then concatenates these embeddings. Another line of the related work is deep learning based methods. SDNE @cite , DNGR @cite utilize this ability of deep autoencoder to generate an embedding model that can capture non-linearity in graphs. AIDW @cite proposes an adversarial network embedding framework, which leverages the adversarial learning principle to regularize the representation learning. However, existing approaches usually lack weight learning for different scales.
- Recent trends in reinforcement learning have shown improved generalization capabilities, by exploiting deep learning techniques. For example, @cite use a deep @math -network to learn directly from high-dimensional visual sensory inputs on Atari 2600 games. @cite @cite use deep value networks and policy networks to respectively evaluate board positions and select moves to achieve superhuman performance in Go. @cite , instead, the authors execute multiple agents in parallel, on several instances of the same environment, to learn a variety of tasks using actor-critic with asynchronous gradient descent. Similar advancements have been shown in the robotics domain. For instance, @cite represent policies through deep convolutional neural networks, and train them using a partially observed guided policy search method on real-world manipulation tasks. Moreover, @cite use deep learning in simulation, and propose progressive networks to bridge the reality gap and transfer learned policies from simulation to the real world. Unfortunately, planning and learning with deep networks is computationally demanding (i.e., requires a huge number of heavy simulations). For this reason, @cite introduce I2As to exploit outcome of virtual policy executions, in the Sokoban domain, learned with a deep network.
- In this paper, we address generalization at learning time by introducing , an iterative algorithm for policy generation that makes use of deep @math -networks to drive a focused exploration. relies on previous work @cite and extends it with a different representation to obtain improved generalization capabilities over higher dimensional problems. In particular, we approximate the @math function using @math -learning with a deep convolutional neural network. Similar to @math -CP @cite and TD-search @cite , we aim at reducing the variance of value estimates during the search procedure by means of temporal difference learning. However, as in @cite , extends TD-search by constructing upper confidence bounds on the value function, and by selecting optimistically with respect to those, instead of performing @math -greedy exploration. Thanks to the generalization capabilities of deep networks, improves over @math -CP both in terms of policy and exploration. Our representation, in fact, not only enables the algorithm to explore more informative portions of the search space @cite @cite , but also is able to generalize better among them with positive effects on the overall policy and search space expansion.
- In literature, most object detection studies were focused on the detection with relatively low localization quality, with a default IoU threshold of 0.5. There are only a few related studies for high-quality detection. LocNet @cite learns a postprocessing network for location refinement, which however does not optimize the whole system end-to-end and is not designed for high-quality detection tasks. MultiPath Network @cite proposed to learn multiple detection branches for different quality thresholds. However, it still suffers insufficient training samples and it is computationally slow due to the nature of two-stage detectors. Cascaded RCNN @cite learns regressors in a cascaded way, which gradually increases qualified proposal numbers towards high-quality detection. However, it is still based on two-stage RCNN and its slow inference speed is a critical drawback, especially the feature re-extraction step is operated by time-consuming ROI Pooling or ROI Warping.
- Our work is also related to the studies for multi-scale feature fusion, which has been proved to be an effective and important structure for object detection with different scales. ION @cite extracts region features from different layers by ROI Pooling operation; HyperNet @cite directly concatenates features at different layers using deconvolution layers. FPN @cite and DSSD @cite fuses features of different scales with lateral connection in a bottom-up manneer, which effectively improve the detection of small objects. However, the vanilla feature pyramid @cite only considers boosting shallow layer features with deep layer features, but ignores the fact that the instance information in shallow layer features can be helpful to deep semantic layer features. We overcome this limitation by the proposed Bidirectional Feature Pyramid structure.
- Now, we include some references on the subject. Sufficient conditions for the @math -nuclearity of spectral multipliers associated to the harmonic oscillator, but, in modulation spaces and Wiener amalgam spaces have been considered by J. Delgado, M. Ruzhansky and B. Wang in @cite @cite . The Properties of these multipliers in @math -spaces have been investigated in the references S. Bagchi, S. Thangavelu @cite , J. Epperson @cite , K. Stempak and J.L. Torrea @cite @cite @cite , S. Thangavelu @cite @cite and references therein. Hermite expansions for distributions can be found in B. Simon @cite . The @math -nuclearity and Grothendieck-Lidskii formulae for multipliers and other types of integral operators can be found in @cite @cite . On Hilbert spaces the class of @math -nuclear operators agrees with the Schatten-von Neumann class @math in this context operators with integral kernel on Lebesgue spaces and, in particular, operators with kernel acting of a special way with anharmonic oscillators of the form @math @math has been considered on Schatten classes on @math in J. Delgado and M. Ruzhansky @cite . The proof of our results will be presented in the next section.
- Wrapping C C++ libraries is not new. SWIG @cite , first released in 1996, generates wrappers for C C++ libraries so they can be called from other languages like Python, Go and Lua. SWIG does not provide library interception for extracting, e.g., performance data. Furthermore, it is not possible to create C++ or C wrappers for C C++ libraries. SWIG uses its own C C++ preprocessor and parser.
- One possible application for tools interfaces and library wrappers is to check for correct API usage. For example MUST @cite uses MPI's profiling interface to ensure correct use and to detect possible deadlocks. The wrapping code is generated manually with a simple proprietary wrapper generator.
- Concerning workload modelling, @cite used data traces obtained from a data centre to characterise and predict workload on VMs. Their goal was to explore cross-VM workload correlations, and predict workload changes due to dependencies among applications running in different VMs -- while we approach the load prediction from the workflow enactment point of view.
- @cite developed CloudProphet to predict legacy application performance in clouds. This tool is able to trace the workload of an application running locally, and to replay the same workload in the cloud for further investigations and prediction. In contrast, our work presents a technique to identify load characteristics independent from the workflow ran on cloud resources.
- @cite also identified performance uncertainties of multi-tenant virtual machine instances over time in Cloud environments. They proposed a model called R-MOHEFT that considered uncertainty intervals for workflow activity processing times. They developed a three-objective (i.e., makespan, monetary cost, and robustness) optimisation for Cloud scheduling in a commercial setting. In contrast to this approach our goal is to identify patterns in earlier workloads to overcome the uncertainty, and apply simulations to predict future background load of the infrastructure.
- @cite offers cloud workload prediction based on autoregressive integrated moving average. They argue that proactive dynamic provisioning of resources could achieve good quality of service. Their model's accuracy is evaluated by predicting future workloads of real request traces to web servers. Additionally, @cite developed a workload model for the CloudSim simulator using generalised extreme value lambda distributions. This model captures user behavioural patterns and supports the simulation of resource utilisation in clouds. They argue that user behaviour must be considered in workload modelling to reflect realistic conditions. Our approach share this view: we apply a runtime behaviour analysis to find a workflow enactment plan that best matches the infrastructure load including user activities.
- @cite used workload prediction based on identifying similar past occurrences of the current short-term workload history for efficient resource scaling. This approach is the closest to ours (albeit, we have a different focus support for on-line decision making in scientific workflow enactors etc.), as it uses real-world traces from clouds and grids. They examine historic data to identify similar usage patterns to a current window of records, and their algorithm predicts the system usage by extrapolating beyond the identified patterns. In contrast, our work's specific focus on scientific workflows allows the analysis and prediction of recently observed execution time discrepancies, by introducing simulations to the prediction and validation phases.
- Outlier Detection To the best of our knowledge, the outlier issue has not been considered in PU classification. Thus, here we provide a brief overview of outlier detection methods in general settings. Existing outlier detection researches can be roughly categorized as variance-based approaches and model-based approaches. The variance-based approaches detect the outliers through a set of criterion used to measure the difference between the data and the rest of the dataset. These criterion can come from statistical analysis , distance metric and density ratio . They usually work well in low dimensional space when the data amount is not too huge. But they are difficult to be integrated into other models like PU learning. The model-based approaches detect outliers through certain models. Typical methods include the regularized principal component regression , regularized partial least square , SVM based algorithms and others. More detailed reviews can be found in @cite . None of these works were integrated with PU learning models.
- There have been other works that discussed the interactions of end-nodes and SDN. Nevertheless, and to the best of our knowledge, this paper is the first ever to carefully explore all implications of a full SDN deployment of end-nodes. For instance, the authors of @cite enable SDN within a mobile device to aggregate all its available interfaces. However, and contrary to our work, they do not consider that end-nodes can be transient, scattered, with low traffic locality and very numerous. Similarly, the authors of @cite extend OpenFlow to bring SDN to end-nodes and complement existing approaches in the field of Software-defined Radio. They -intentionally- kept out the scope the complete architecture to support SDN-aware end-nodes. Similarly, meSDN @cite proposes a mobile extension for SDN to optimize wireless channel transmission on an existing SDN network. However, it does not consider devices connecting to legacy -non SDN- networks or transient devices that roam frequently.
- Having small patches as proposed by @cite lacks the ability to automatically localize and determine what part is in effect for an expression. It can also have trouble distinguishing patches when applied on faces of different gender and ethnicity. Having a learned detector for the face and parts as proposed by @cite cannot guarantee the robustness as much as face detection, alignment and localization techniques do, especially when there are peculiar samples provided. Both proposals used the JAFFE and CK+ database, which are not very diverse when it comes to their subjects ethnicities. In their experiments, they adopt a cross-validation approach, in which they do not ensure a subject independent protocol. This can result in a high performance which can be inconsistent when a new subject is ever tested.
- Two notable exceptions are the and datasets. @cite is a dataset of newswire texts from CNN with questions and answers written by crowdsourcing workers. closely resembles our own data collection with respect to the method of data acquisition. As for our data collection, full texts were not shown to workers as a basis for question formulation, but only the text's title and a short summary, to avoid literal repetitions and support the generation of non-trivial questions requiring background knowledge. The NewsQA text collection differs from ours in domain and genre (newswire texts vs. narrative stories about everyday events). Knowledge required to answer the questions is mostly factual knowledge and script knowledge is only marginally relevant. Also, the task is not exactly question answering, but identification of document passages containing the answer.
- @cite is a corpus that contains automatically collected question-answer pairs from 14 trivia and quiz-league websites, together with web-crawled evidence documents from and . While a majority of questions require world knowledge for finding the correct answer, it is mostly factual knowledge.
- A closely related model is the aforementioned recurrent VAE of @cite . Like ours, their model is effectively a VAE that uses RNNs for both the encoder and decoder. With careful optimization, @cite demonstrate the ability to generate and interpolate between sentences which have been modeled at the character level. A very similar model was also proposed by @cite , which was applied with limited success to music. This approach was also extended to utilize a convolutional encoder and decoder with dilated convolutions in @cite . The primary difference between these models and ours is the decoder architecture; namely, we use a hierarchical RNN. The flat RNN decoder we use as a baseline in sec:experiments exhibits significantly degraded performance when dealing with very long sequences.
- Various additional VAE models with autoregressive decoders have also been proposed. @cite consider extensions of the recurrent VAE where the RNNs are replaced with feed-forward and convolutional networks. The PixelVAE @cite marries a VAE with a PixelCNN @cite and applies the result to the task of natural image modeling. Similarly, the Variational Lossy Autoencoder @cite combines a VAE with a PixelCNN PixelRNN decoder. The authors also consider limiting the power of the decoder and using a more expressive Inverse Autoregressive Flow @cite prior on the latent codes. Another example of a VAE with a recurrent encoder and decoder is SketchRNN @cite , which successfully models sequences of continuously-valued pen coordinates.
- The hierarchical paragraph autoencoder proposed in @cite has several parallels to our work. They also consider an autoencoder with hierarchical RNNs for the encoder and decoder, where each level in the hierarchy corresponds to natural subsequences in text (e.g., sentences and words). However, they do not impose any constraints on the latent code, and as a result are unable to sample or interpolate between sequences. Our model otherwise differs in its use of a flat bidirectional encoder and lack of autoregressive connections in the first level of the hierarchy.
- More broadly, our model can be considered in the sequence-to-sequence framework @cite , where an encoder produces a compressed representation of an input sequence which is then used to condition a decoder to generate an output sequence. For example, the NSynth model learns embeddings by compressing audio waveforms with a downsampling convolutional encoder and then reconstructing audio with a WaveNet-style decoder @cite . Recurrent sequence-to-sequence models are most often applied to sequence tasks where the input and output sequences are different. Nevertheless, sequence-to-sequence autoencoders have been occasionally considered, e.g., as an auxiliary unsupervised training method for semi-supervised learning @cite or in the paragraph autoencoder described above. Again, our approach differs in that we impose structure on the compressed representation (our latent vector) so that we can perform sampling and interpolation.
- The polynomial method'' in combinatorics @cite is a powerful tool for deriving bounds on the size of combinatorial objects and for proving non-existence of extremal objects. In particular, a recent breakthrough by Croot, Lev and Pach @cite (see also @cite ) has stimulated interest in collections of multivariate polynomials that vanish on some configuration in a finite vector space. Here, we work in characteristic zero and are interested in how the ideal we obtain is related to the structure of the design. By contrast, the authors of @cite @cite work over fields of positive characteristic. To see the connection, we note that, since our zero set lies in @math , the polynomial generators @math may always be chosen from @math . So application of the reduction @math maps the ideal @math into the ideal of the same finite variety considered modulo @math . It will be an interesting follow-up task to determine when the image under this map is the full ideal in positive characteristic.
- Regret minimization has been studied extensively for infinite bandit models , whereas @cite is the first work dealing with pure-exploration for general reservoirs. The authors consider the fixed-budget setting, under the tail assumption for the reservoir distribution, already discussed.
- AI-based data analytics and deep neural network applications have become increasingly important in recent years. These applications lead to rapid development of software and hardware that efficiently express and support tensor operations, which are fundamental for deep neural network applications. TensorFlow is among the most popular open-source programming framework that uses a computational graph with tensor operations as nodes of the graph @cite . Caffe, Torch and Microsoft CNTK are other popular programming frameworks for developing deep neural networks @cite .
- The seminal paper by Krizhevsky @cite has established GPUs as the main workforce in training deep neural networks and triggered a renaissance of deep-learning applications. Besides NVIDIA Tensor Cores @cite discussed in this paper, several companies are also employing and developing specialized hardware for high-performance inference. Microsoft deployed the Catapult system that uses FPGAs @cite . Movidius developed the Myriad 2 Vision Processing Unit @cite . Google designed and developed Tensor Processing Unit (TPU) specifically for inference workloads. The main engine of the TPU is a MAC matrix multiply unit containing 256 @math 256 MACs, each capable of performing 8-bit multiply-and-adds on signed or unsigned integers. In December 2017, Intel announced the release of the Neural Network Processor (NPP) @cite , which implements a new memory architecture for tensor operations. NPP does not have standard caches and data movement is programmable by software. In addition, neuromorphic hardware, such as the IBM TrueNorth @cite and SpiNNaker @cite chips, mimics the functioning of spiking neural network. Although their original design purpose is to simulate the brain, they may also find usage in AI applications.
- In the last decade, the VNEP has attracted much attention due to its many applications and the survey @cite from 2013 already lists more than 80 different algorithms for its many variations @cite . The VNEP is known to be @math -hard and inapproximable in general (unless @math ) @cite . Based on the hardness of the VNEP, most works consider heuristics without any performance guarantee @cite @cite . Other works proposed exact methods as integer or constraint programming, coming at the cost of an exponential runtime @cite @cite @cite .
- A column generation approach was proposed by in @cite to efficiently compute solutions to the VNEP by generating feasible mappings on-the-fly subject to a specific cost measure. In particular, compute feasible mappings by relying on heuristics and, if no feasible heuristical solution was found, rely on Mixed-Integer Programming to compute a cost optimal feasible embedding in non-polynomial time. We believe that our work can bridge the gap between the heuristic generation of feasible mappings and the optimal generation of feasible mappings as our approach can be used to compute mappings.
- Acknowledging the hardness of the general VNEP and the diversity of applications, several subproblems of the VNEP have been studied recently by considering restricted graph classes for the virtual networks and the substrate graph. For example, virtual clusters with uniform demands are studied in @cite @cite , line requests are studied in @cite @cite @cite and tree requests were studied in @cite @cite .
- Considering approximation algorithms, employed randomized rounding in @cite to obtain a constant approximation for embedding line requests on arbitrary substrate graphs under strong assumptions on the benefits and the capacities. In their interesting work, @cite give an @math time @math -approximation algorithm for minimizing the load of embedding @math -depth trees based on a @math -node substrate. Their result is based on a strong LP relaxation inspired by the Sherali-Adams hierarchy.
- In our preliminary technical report @cite similar results were presented. The current work presents a significantly simpler LP formulation and also provides an extensive computational evaluation. Additionally, in our recent technical report @cite , the approximation approach presented in this work is extended beyond cactus request graphs. However, approximating more general request graphs comes at the price of non-polynomial runtimes.
- Previous works have also taken advantage of structured data to increase the computational efficiency of GP. For input data that lie on a grid, @cite @cite sidestep the matrix inversion by solving @math with gradient-based methods. Importantly, the optimization is fast because Toeplitz circulant structure can be exploited for fast matrix multiplication. These fast multiplications take on forms similar to . For input data that do not lie on a grid, @cite (KISS-GP) introduces a latent grid, finds a sparse, approximate representation of @math in terms of @math , and makes inference using the same kind of optimization problem in @cite @cite . The sparsification in KISS-GP and that in our LGSWD-GP share the same flavor in ignoring the diminishing long-range correlations among data points. Recently, @cite further extended this optimization approach to efficiently handle high dimensional data by applying the Nystr " o m approximation for eigen-decomposition and by exploiting properties of Kronecker and Khatri-Rao products for fast matrix multiplication. Overall, our method and these methods all take advantage of the grid structure to form fast matrix multiplication; however, our method differs in that it overcomes the matrix inversion bottleneck via analytic diagonalizaion rather than optimization.
- Variational approaches are another popular method for GP inference. Variational methods turn the inference of the predictive mean and variance into an optimization problem @cite . Variational GP often provides more accurate prediction than FITC @cite and has been made progressively faster through a series of development---from stochastic variational inference @cite , to distributed variational inference @cite , to asynchronous distributed variational inference @cite ---to handle billions of input data. Similar to our work and @cite , @cite uses grid inducing points with stochastic variational inference, which allows added efficiencies in computation via the Kronecker and Khatri-Rao products.
- requires learning similarity between visual and language modalities. Karpathy @cite first align sentence fragments and image regions in a subspace, and later apply a bi-directional RNN for multimodal alignment in @cite . Hu @cite employ a 2-layer LSTM to rank proposals based on encoded query and visual features. Rohrbach @cite employ a latent attention network conditioned on query which ranks proposals in weakly supervised scenario. Recently, Plummer @cite augment the CCA model @cite to leverage extensive linguistic cues in the phrases. Chen @cite introduce regression mechanism in phrase grounding to improve proposals' quality. Xiao @cite leverage query's language structural information to guide the learning of phrase grounding model in weakly supervised scenario. Chen @cite apply reinforcement learning techniques to leverage context information. In this paper, we explore consistency in visual and language modalities and leverage complementary knowledge to further boost performance of weakly supervised grounding.
- is a method aims at learning a model without heavy manual labeling work. It is widely used in different computer vision tasks. Crandall @cite leverage the class labeling to learn a part-based spatial model without detailed annotation of object location and spatial relationship. Maxime @cite propose to learn the interaction between human and objects purely from action labeling for still images. Recently, Prest @cite apply a deep convolutional neural network and its score maps to address object localization with image level class labels. For phrase grounding task, Rohrbach @cite propose to adopt an attention model which is optimized by learning to reconstruct query's information, and avoids human labeling for object locations for each query in the training set. Based on this, Xiao @cite leverage a continuous attention map and explore detailed structural reconstruction of language modality. Inspired by the success of weakly supervised learning, we propose to apply another visual consistency to further boost performance.
- is a technique widely used for tasks in different domains. Hinton @cite propose to compress knowledge learned from one model into another one which is too computationally expensive to train. Inspired by this, Aytar @cite apply visual knowledge to train a sound classification network. Owens @cite use ambient sound information to train an object detection network. Lin . @cite leverage knowledge learned in Visual Question Answering (VQA) task in image retrieval. Zhang @cite apply knowledge learned in image captioning and VQA to train a network detecting visual relation in images. For phrase grounding, we propose to leverage knowledge learned from pre-trained deep neural network to filter out unrelated proposals for visual consistency.
- There are many follow-up studies on EVENODD codes @cite and RDP codes @cite along different directions, such as the extensions of fault tolerance @cite @cite @cite , the improvement of repair problem @cite @cite @cite @cite and efficient decoding methods @cite @cite @cite @cite of their extensions.
- Huang and Xu @cite extended the EVENODD codes to be STAR codes with three parity columns. The EVENODD codes were extended by Blaum, Bruck and Vardy @cite @cite for three or more parity columns, with the additional assumption that the multiplicative order of 2 mod @math is equal to @math . A sufficient condition for the extended EVENODD codes to be MDS with more than eight parity columns is given in @cite . Goel and Corbett @cite proposed the RTP codes that extend the RDP codes to tolerate three disk failures. Blaum @cite generalized the RDP codes that can correct more than three column erasures and showed that the extended EVENODD codes and generalized RDP codes share the same MDS property condition. Blaum and Roth @cite proposed Blaum-Roth codes, which are non-systematic MDS array codes constructed over a Vandermonde matrix. Some efficient systematic encoding methods for Blaum-Roth codes are given in @cite @cite @cite . We call the existing MDS array codes in @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite as Vandermonde MDS array codes, as their constructions are based on Vandermonde matrices.
- In the literature, most existing efforts of person Re-ID are mainly carried in two aspects: the discriminative representation learning and the effective matching strategy learning. For image representation, a number of approaches pay attention to designing robust descriptors againist misalignments and variations. Early studies employ hand-crafted features including HSV color histogram @cite , SIFT @cite , LBP @cite features or the combination of them. Recently, several deep convolutional architectures @cite @cite have been proposed for person Re-ID and have shown significant improvements over those with hand-crafted features.
- For matching strategy, the essential idea behind metric learning is to find a mapping function from the feature space to the distance space so as to minimize the intra-personal variance while maximizing the inter-personal margin. Many approaches have been proposed based on this idea including LMNN @cite and KISSME @cite . Recently, some efforts jointly learn the representation and classifier in a unified deep architecture. For example, patch-based methods @cite @cite decompose images into patches and perform patchwise distance measurement to find the spatial relationship. Part-based methods @cite divide one person into equal parts and jointly perform bodywise and partwise correspondence learning since the pedestrians keep upright in general. Different from all the above efforts which focus on feature distance measurement, our proposed method aims at learning the semantic correspondence of semantic-components based on the semantics-aware features and is robust to the variation and misalignment posed by viewpoint changes.
- A simple example of distributed beamforming with aggregated feedback is the one bit feedback algorithm @cite @cite , in which transmitters use small random phase perturbations to perform stochastic ascent on the received signal strength, based on the receiver feedback, broadcast to all transmitters, of one bit per iteration. This approach is simple and has formed the basis for several prototypes @cite @cite . The relatively slow convergence of the original algorithm (e.g., @math iterations to reach 75 knowledge of previous perturbations @cite , or using knowledge of channel statistics at the receiver @cite . However, the method is fundamentally mismatched to low per-node SNRs @cite , roughly speaking, because noise masks the effect of small phase perturbations. This motivates our approach, in which the transmitters employ large phase perturbations during a training phase, rather than attempting to make small adjustments while beamforming.
- The one-bit feedback algorithm has been extended to wideband regimes in a prior paper by the authors @cite , by adding an additional bit per subcarrier which enables enforcement of phase continuity across subcarriers. This is fundamentally different from the training-based approach in this paper, which enables explicit channel estimation on pilot subcarriers for each transmitter, and hence is amenable to standard interpolation across subcarriers.
- Once we commit to a training phase, one possible approach is for the transmitters to take turns transmitting in training slots, with the receiver sending (quantized) feedback corresponding to each slot. Such time-multiplexed training has been successfully prototyped @cite , and has been studied with quantized feedback in @cite . The algorithm is energy efficient, since only one node is active per iteration, but unlike the DOST scheme proposed in this paper, it does not utilize integration over time to combat noise, so that its performance suffers in the low per-node SNR regime, as we show in our numerical results. The time-multiplexed approach also does not scale well at the protocol level because of the dependence of the training frame structure on the number of transmitter nodes, and the coordination required between them to take turns.
- Our work, and the related work discussed above, falls into the category of all-wireless distributed beamforming with explicit feedback, which allows flexible deployment of DBS supporting FDD operation. It is worth mentioning recent work on all-wireless distributed beamforming based on channel reciprocity, that relies on tight pre-synchronization of the cooperating nodes to emulate a centralized array with a common time base @cite @cite @cite . Finally, there is also significant recent work on distributed MIMO based on coordination of infrastructure nodes (WiFi access points or cellular base stations) via a fast wired backhaul @cite @cite @cite @cite . Our emphasis is to scale up the number of nodes without fast wired backhaul and achieve massive MIMO gains @cite @cite @cite .
- The present paper builds on our prior conference paper @cite , which introduced the DOST scheme, and also discussed extensions to wideband systems. However, it goes well beyond @cite by presenting a DBS concept system built around DOST, including an OFDM system design roughly consistent with LTE, specific prescriptions for pilot design, and outage capacity analyses for downlink and uplink which compactly characterize system-level performance.
- Efforts to parallelize and accelerate deep RL algorithms have been underway for several years. Gorila @cite parallelized DQN using distributed computing. It achieved significant although sub-linear speedups using hundreds of computing units as samplers or learners, with central parameter servers for managing parameter updates. This effort suffered in sample complexity relative to single-threaded DQN. More recently, @cite showed that a distributed, prioritized replay buffer can support faster learning while using hundreds of CPU cores for simulation and a single GPU for training. The same work used increased batch sizes, with a brief study of the effect of learning rate.
- The policy gradient method A3C is itself a parallelized algorithm. In GA3C @cite , a speedup over CPU-only A3C was achieved by using a GPU. It was employed asynchronously, with predictor'' and trainer'' threads queuing observations and rewards for batched inferences and training updates. GA3C induced a policy lag'' between generation and consumption of training data, worsening sample complexity. In independent work simultaneous to ours, @cite extended policy gradient methods to a distributed setting, enabling an alternative approach to multi-GPU training called IMPALA. They introduced a more heavily modified algorithm, , to mitigate policy lag--which we avoid--and did not employ GPU inference. In PAAC @cite , the authors explored the use of many simulators and increased batch sizes learning rates in (single-GPU) batched A2C--ideas central to our studies. Our contributions to actor-critic methods exceed this work in a number of ways, chiefly: improved sampling organization, tremendously enhanced scale and speed using multiple GPUs, and inclusion of asynchronous optimization.
- Although lots of works have been done to fool CNN-based models, only a few studies have examined RNN-based models, and the majority of them focus on simple text classification problems. first uses Fast Gradient Sign Method (FGSM) to conduct an attack on RNN LSTM-based classification problems. In order to generate text adversarial examples, @cite proposes to use reinforcement learning to locate important words that could be deleted in sentiment classification. and generate adversarial sequences by inserting or replacing existing words with typos and synonyms. aims to attack sentiment classification models in a black-box setting. It develops some scoring functions to find the most important words to modify. These approaches differ from our work in that they study simple text classification problems while we focus on the more challenging seq2seq model with sequential outputs. Other than attacking text classifiers, aims to fool reading comprehension systems by adding misleading sentences, which has a different focus than ours. uses the generative adversarial network (GAN) to craft natural adversarial examples. However, it can only perform the untargeted attack and also suffers from high computational cost.
- The GelSight sensor is a vision-based tactile sensor that measures the 2D texture and 3D topography of the contact surface. It utilizes a piece of elastomeric gel with an opaque coating as the sensing surface, and a webcam above the gel to capture contact deformation from changes in lighting contrast as reflected by the opaque coating. The gel is illuminated by color LEDs with inclined angles and different directions. The resulting colored shading can be used to reconstruct the 3D geometry of the gel deformation. The original, larger GelSight sensor @cite @cite was designed to measure the 3D topography of the contact surface with micrometer-level spatial resolution. Li . @cite designed a cuboid fingertip version that could be integrated in a robot finger. Li's sensor has a @math cm @math sensing area, and can measure fine 2D texture and coarse 3D information. A new version of the GelSight sensor was more recently proposed by Dong . @cite to improve 3D geometry measurements and standardize the fabrication process. A detailed review of different versions of GelSight sensors can be found in @cite .
- GelSight-like sensors with rich 2D and 3D information have been successfully applied in robotic manipulation. Li . @cite used GelSight's localization capabilities to insert a USB connector, where the sensor used the texture of the characteristic USB logo to guide the insertion. Izatt . @cite explored the use of the 3D point cloud measured by a GelSight sensor in a state estimation filter to find the pose of a grasped object in a peg-in-hole task. Dong . @cite used the GelSight sensor to detect slip from variations in the 2D texture of the contact surface in a robot picking task. The 2D image structure of the output from a GelSight sensor makes it a good fit for deep learning architectures. GelSight sensors have also been used to estimate grasp quality @cite .
- Another approach is to use two stream networks. @cite explicitly modeled the temporal featurs in a stack of optical flow vectors in parallel to a network for capturing the spatial context. These networks are trained separately and then combined using an SVM. The predicted output is averaged across the sampled frames. This approach suffers from the false label assignment problem. In other words, the label is applied to all the sampled frames, and the labeled action may not be represented in all frames.
- More recently, 3D convolutional networks have shown ImageNet-level success for video classification. @cite have shown that these models can retrace the effectiveness of 2D networks and can be as deep (e.g. 152 layers for ResNet). 3D convnets can be resource intensive for training and inference, however, they present an elegant framework for processing video data. @cite extend the two stream work with 3D networks and leverage pre-trained 2D models by repeating the weights in the third dimension. Our approach is closest to this line of work and explores using parallel networks (2D and 3D) consuming image and audio features which is, to the best of our knowledge, a novel approach.
- Identifying bias in datasets @cite is another important usage of the network attention. @cite analyses the location of attention maps of a trained model to find out the dataset bias, which helps them to build a better unbiased dataset. However, in practical applications, it is hard remove all the bias of the dataset and time-consuming to build a new dataset. How to garantee the generalization ability of the learned network is still challenging. Different from the existing methods, our model can fundamentally solve this problem by providing supervision directly on network's attention and guiding the network to focus on the areas critical to the task of interest, therefore is robust to dataset bias.
- However, the Wider Face dataset @cite pushes the challenge to another level. In addition to heavy occlusion, extreme pose, and strong illumination, the ultra small sizes of faces in crowd images have become one of the most challenging problems in robust face detection. To solve this, CMS-RCNN @cite incorporates body contextual information to help infer the location of faces. HR @cite builds multi-level image pyramids for multi-scale training and testing which finds upscaled tiny faces. SFD @cite addresses this with scale-equitable framework and new anchor matching strategy. In this paper, we introduce a novel anchor design for finding more tiny faces, leading to state-of-the-art detection performance.
- The effect of post-debate coverage on public opinion. Studies have shown that media choices about coverage can have serious consequences @cite @cite @cite @cite @cite @cite @cite . For instance, show that in the 2004 U.S. election, citizens who only read the news coverage rated Kerry more negatively compared to those who watched the debate firsthand, because media outlets highlighted the moment of Kerry outing Cheney's lesbian daughter, although this moment did not catch much attention from the live audience. develop a mobile app to collect real-time feedback for presidential debates. discuss the media's critical tendency and its partisan consequences in the U.S.
- Influences between the media and politicians. Although our work focuses on media selection of highlights, politicians often behave based on their beliefs about media preferences, which suggests complex dynamics between the media and politicians @cite @cite @cite @cite @cite . For instance, discuss politicians' increasing adaptation to different news values and formats in the presence of media abundance. Also relevant is research on the influence of politicians on the media, including agenda-setting, rhetorical positioning, and framing @cite @cite @cite @cite @cite .
- Power dynamics in debates and other types of coverage. Studies have shown that language use and topic control in debates can reflect influence between candidates and indicate power dynamics @cite @cite @cite . More recently, social media have also become an important channel to monitor public opinion on debates in real time @cite @cite and potentially change news media coverage.
- Evolutionary strategies, such as the Covariance Matrix Adaptation Evolution Strategy (CMA-ES @cite ), are commonly used as a baseline approach in reinforcement learning tasks @cite @cite @cite @cite . Here, we only discuss the most recent related works, which also followed up on the work by ; in particular, we discuss three related arXiv preprints that scientists at Uber released in the last two months about work concurrent to ours.
- G "udemann and Ortmeier @cite present a language for probabilistic system modeling for safety analysis. Formalized as , they propose two ways of failure mode modeling ( per-time and per-demand failure modes), and two ways of deductive cause consequence reasoning ( quantitative and qualitative). Their model and reasoning can extend our approach. However, our work (i) adds stronger guidelines on how to build planning models and (ii) puts hazard analysis into the context of autonomous systems and mitigation planning.
- @cite present an algorithm for finding permissive robot action plans optimal to safety and performance. They employ (helpful in regarding uncertainty and robot limitations) to model robot behavior, and two abstractions from this model to capture a system's modes and hazards. Our framework uses three layers of abstraction ( @math , @math , @math ), operational situations to capture control modes, and a structure to capture hazards. While they directly encode hazard severity for plan selection, our framework allows the planner to calculate the risk priority based on a causal event tree towards mishaps. As opposed to complete behavioral planning, our approach focuses the construction of mitigation planning models. For example, for system faults we can plan mitigations by using adaptation mechanisms of a given control system architecture.
- Jha and Raman @cite discuss the synthesis of vehicle trajectories from probabilistic temporal logic assertions. Synthesized trajectories take into account perception uncertainty through approximation of sensed obstacles by combining Gaussian polytopes. In a similar context, Rizaldi and Althoff @cite formalize safe driving policies to derive safe control strategies implementing worst-case braking scenarios in autonomous driving. They apply a hybrid-trace-based formalization of physics required for model checking of recorded @cite and planned @cite strategies. @cite @cite @cite discuss low-level control for a specific class of driving scenarios, whereas our approach provides for (i) the investigation and combination of many related operational situations, thus, forming a more comprehensive perspective of driving safety, (ii) regarding various kinds of hazards that might play a role in high- and low-level control beyond safe and optimal trajectory planning and collision avoidance.
- Wei et al. @cite describe an autonomous driving platform, capable of bringing vehicles to a safe state and stop, activating a fail-operational mode on critical failure, and a limp-home mode on less critical failure. These are mitigation strategies we can assess in our framework. Their work elaborates on designing a specific class of architectures. Additionally, we provide an approach to systematically evaluate risks and, consequently, derive an architecture design.
- Babin et al. @cite propose a system reconfiguration approach developed with the Event-B method in a correct-by-construction fashion using a behavior pattern similar to our approach (particularly, fig:statemodel ). Reconfiguration as one way to faults is discussed in this work. Wardzi ' n ski @cite discusses hazard identification and mitigation for autonomous vehicles by predetermined risk assessment ( with safety barriers) and dynamic risk assessment. For both, he provides argumentation patterns for creating AV safety cases. In addition to his work, the abstraction and the method we propose covers both paradigms in one framework. We provide formal notions of all core concepts.
- However, most of these works attempted forecasting of changes in subjects' CS, which deals with a limited number of future outcomes (i.e., either binary or a three-class). By contrast, we aim to forecast ADAS-Cog13, defined on a more fine-grained scale (85 levels), which poses a more challenging machine learning problem. While in our recent work @cite , we investigated forecasting of ADAS-Cog13 along with other scores (CDSRB, CS, and MMSE), we did so for one step ahead (6 months). In this work, we attempt forecasting of up to 24 months ahead by focusing on ADAS-Cog13.
- Our work is firstly related to previous studies on legal assistant systems. Previous work considers the task of charge prediction as a text classification problem @cite @cite @cite @cite . Recently, investigate deep learning methods for this task. Besides, there are also works on identifying applicable articles for a given case @cite @cite @cite , answering legal questions as a consulting system @cite @cite and searching relevant cases for a given query @cite @cite . As a legal assistant system, @math can benefit automatic legal document generation by generating court views from fact descriptions obtained from the last phase, through legal professionals or other technics like information extraction @cite from raw documents in a case, if we generate legal documents step by step. Our work is also related to recent studies on model interpretation @cite @cite @cite . Recently, much work has paid attention to giving textual explanations for classifications. generate visual explanations for image classification. propose to learn to select most supportive snippets from raw texts for text classification. @math @math @math can improve the interpretability of charge prediction systems by generating textual court views when predict the charges.
- Our label-conditioned Seq2Seq model steams from widely used encoder-decoder paradigm @cite which has been widely used in machine translation @cite @cite , summarization @cite @cite @cite @cite , semantic parsing @cite and paraphrase @cite or other NLG problems such as product review generation @cite and code generation @cite @cite . propose to encode image labels for visual-language models to generate justification texts for image classification. We also introduce charge labels into Seq2Seq model to improve the charge-discriminations of generated rationales. Widely used attention mechanism @cite @cite is applied to generate fact details more accurately.
- The field of graph representation learning is seeing a resurgence of research interest in recent years, driven in part by the latest advances in deep learning. The aim is to learn a mapping that encodes the input graph into low-dimensional feature embeddings while preserving its original global structure. @cite succinctly articulate the diverse set of previously proposed approaches for graph representation learning, or graph embedding, as belonging within a unified encoder-decoder framework. In this section, we summarize three classes of encoder-decoder models most related to our work: matrix factorization (MF), autoencoders, and graph convolutional networks (GCNs).
- Our work is inspired by recent successful applications of autoencoder architectures for collaborative filtering that outperform popular matrix factorization methods @cite @cite @cite , and is related to Structural Deep Network Embedding (SDNE) @cite for link prediction. Similar to SDNE, our models rely on the autoencoder to learn non-linear node embeddings from local graph neighborhoods. However, our models have several important distinctions: 1) we leverage extensive parameter sharing between the encoder and decoder parts to enhance representation learning; 2) our @math LoNGAE model can optionally concatenate side node features to the adjacency matrix for improved link prediction performance; and 3) the @math LoNGAE model can be trained end-to-end in a single stage for multi-task learning of link prediction and semi-supervised node classification. On the other hand, training SDNE requires multiple steps that are difficult to jointly optimize: i) pre-training via a deep belief network; and ii) utilizing a separate downstream classifier on top of node embeddings for LPNC.
- Recent research @cite @cite @cite proposes that incorporate anonymous communication as a service of network infrastructures in the Internet and next generation network architectures @cite @cite @cite . The basic assumption of a network-layer anonymity system is that Autonomous Systems (AS) can conduct efficient cryptographic operations when forwarding packets to conceal forwarding information. Additionally, a network-layer anonymity system uses direct forwarding paths rather than reroute packets through overlay networks as in Tor @cite . This processing would be done on (software) routers, for instance, but more abstractedly the term is used to refer to the device or set of devices dedicated to the anonymity system within an AS.
- The first class of network-layer anonymity protocols proposed is the so-called system, which consists of two proposals, @cite and @cite . These systems defend against topological attacks by encrypting forwarding information in packet headers. However, in both schemes, packets stay unchanged from hop to hop, thus enabling bit-pattern correlation of packets at distinct compromised nodes.
- Adversarial attacks (samples with adversarially-crafted small perturbations) have recently emerged as a significant threat to the deep learning techniques, thereby this finding may hinder the large scale adoption of DNNs-based systems in practical applications. There exists a number of methods to generate adversarial samples or adversarial examples (AEs) mainly based on the gradient of networks @cite or solving optimization problems @cite , and so on. Very few works have attempted to give scientific reasoning for the vulnerability phenomena of DNNs to AEs. For instance, authors in @cite gave preliminary explanation that since the input space is densely populated by low-probability adversarial pockets, thus each point in the space is closer to many AEs. These AEs points can be easily doctored to attain the desired model outcome. In turn, Goodfellow @cite argued that linear nature of DNNs-based classifiers is the main source of vulnerability. While, the work in @cite discussed boundary tilting'' view and argued that usually AEs lie in regions where the decision boundary is close to the manifold of training data.
- The proposed defenses for mitigating AEs can be grouped into two categories. The first category techniques try either improving the robustness of DNNs or suppressing the success rates of attacks. For instance, @cite , which is training the system with AEs to augment the regularization and loss functions and making the system more resilient. The other technique is @cite in which additional DNNs with softmax are trained to obstruct the deep learning system from fitting too tightly to the data. However, it has been demonstrated that defensive distillation method can be easily circumvented with a minimal modified attack @cite . Other approaches are or @cite , i.e., removing the adversarial noise from the input samples before feeding them to neural networks, and @cite , i.e., modifying the traditional neural network architectures, e.g., adding extra specific robust layers and functions.
- Object tracking is one of the fundamental problems in computer vision and has been extensively studied @cite and applied in many different tasks.
- The tracker @cite first transforms the image into an appropriate feature space, and uses a classifier as well as a motion model to determine the presence of an object in a frame, which is referred to as tracking by detection.
- More recent methods like @cite employ convolutional neural networks to learn motion and appearance of objects. The feature maps of higher convolutional layers provide robust and accurate appearance representations, but lack spatial resolution. Lower layers on the other hand provide higher spatial resolution and less refined appearance representations. This hierarchical structure is used in @cite by inferring responses of correlation filters on each corresponding layer pair.
- In @cite a semi-supervised approach is described that provides a fusion between using sparse ground truth data from a LiDAR sensor and stereo view synthesis, estimating one image in a stereo pair from the other, to infer dense depth maps. Others @cite @cite in turn rely solely on stereo as a supervision signal, which comes with the benefit of easily available or obtainable data.
- As a consequence, the input of Parseval networks can be recovered if but only if the built-in non-linearities are invertible as well, which is typically not the case. @cite derive conditions under which pooling representations are, but our method directly overcomes this issue. The Scattering transform is an example of predefined deep representation, approximately invariant to translations, that can be reconstructed when the degree of invariance specified is small. Yet, it requires a gradient descent optimization and no guarantee of convergences are known. In summary, the references make clear that invertibility requires special care in designing the architecture or special care in designing the optimization procedure. In this paper, we introduce a network, that overcomes these issues and has an exact inverse by construction.
- Our main inspiration for this work is the recent reversible residual network (RevNet), introduced in @cite . RevNets are in turn closely related to NICE and Real-NVP architectures , which make use of constrained Jacobian determinants for generative modeling. All these architectures are similar to the lifting scheme and Feistel cipher diagrams , as we will show. RevNets illustrate how to build invertible ResNet-type blocks that avoid storing intermediate activations necessary for the backward pass. However, RevNets still employ multiple non-invertible operators like max-pooling and downsampling operators as part of the network. As such, RevNets are not invertible by construction. In this paper, we show how to build an invertible type of RevNet architecture that performs competitively with RevNets on Imagenet, which we call @math -RevNet for invertible RevNet.
- Our proposed image captioning system follows a great deal of recent caption-generation literature in exploiting end-to-end deep learning with a CNN image-analysis front end producing a distributed representation that is then used to drive a natural-language generation process, typically using RNNs @cite @cite @cite . Our grammatical interpretation of the structural roles of words in sentences makes contact with other work that incorporates deep learning into grammatically-structured networks @cite @cite @cite @cite . Here, the network is not itself structured to match the grammatical structure of sentences being processed; the structure is fixed, but is designed to support the learning of distributed representations that incorporate structure internal to the representations themselves --- filler role structure.
- Methods for automatic constituency parsing of a sentence, our third task, include methods based on probabilistic context-free grammars (CFGs) @cite , the shift-reduce method @cite , sequence-to-sequence LSTMs @cite . Our constituency parser is similar to the sequence-to-sequence LSTMs @cite since both use LSTM neural networks to design a constituency parser. Different from @cite , our constituency parser uses TPR and unbinding role vectors to extract features that contain grammatical information.
- In @cite , Chang, Tang and Lee gave an @math -time algorithm for computing a bottleneck matching of a point set, but allowing crossings. This result was extended by Efrat and Katz in @cite to higher-dimensional Euclidean spaces.
- A variant of the bichromatic case is the so-called bicolored (or multicolored, when there are arbitrary many colors) case, where only the points of the same color are allowed to be matched. Abu-Affash, Bhore and Carmi in @cite examined bicolored matchings that minimize the number of crossings between edges matching different color sets. They presented an algorithm to compute a bottleneck matching of points in convex position among all matchings that have no crossings of this kind.
- . Numerous cfi schemes have been introduced in the last 30 years. However, techniques providing fine-grained cfi and code integrity, as required to detect physical attacks, are quite rare @cite .
- Another common approach to enforce cfi is to augment the processor with hardware monitors @cite @cite @cite . Typically, these monitors continuously check that the processor behaves as expected and raise an alert when an error is observed. The disadvantage of this approach is that deciding between correct and incorrect behavior (1-bit of information) effectively introduces a single point of failure for the error detection. As a result, implementing a reliable monitor becomes a challenge on its own. Additionally, these techniques can only provide integrity authenticity and do not offer confidentiality.
- . tee typically also provide confidentiality and authenticity for code. However, tee operate on a completely different level of granularity than scfp . The authenticated encryption in Intel SGX @cite , for example, only ensures that the code and data in memory is protected against tampering. Physical faults in caches or on processor internal buses, on the other hand, are still possible. Also, SGX does not prevent common software attack techniques like code-reuse attacks within enclaves. Therefore, additional cfi schemes are needed to reach similar properties as scfp for code.
- This section reports the most relevant previous works on sketch vectorization. @cite proposed a line enhancement method, based on Gabor and Kalman filters. It can be used to enhance lines for subsequent vectorization. However, this approach fails to correctly extract all the drawing components when the image is noisy or presents parallel strokes, resulting, for instance, in gaps in the final vectorized result or strokes incorrectly merged. Moreover, experiments are conducted with quite simple images.
- @cite reported a first proposal of a framework transforming raw images to full vectorized representations. However, the binarization'' step is not considered at all, by presenting directly the skeleton processing and vectorization steps. In addition to this limitation, this paper also bases the vectorization to the simple fitting of straight lines and circular arcs (instead of using Bezier interpolation), which represents a too simplified and limited representation of the resulting path.
- @cite provided a more complete study of the whole vectorization process. They provide a neat derivation-based algorithm to estimate accurate centerlines for sketches. They also provide a good insight of the problem of correct junction selection. Unfortunately, they work under the assumption of somewhat clean'' lines, that does not hold in many real case scenarios, such as those we are aiming at.
- Another recent work, @cite , provided a good proposal for a vectorization system. For the line extraction part they rely on Vector Fields, which give high quality results with clean images, but fail in presence of noise and fuzzy lines. Still, they dedicated a lot of attention to correctly disambiguate junctions and parallel strokes.
- The sketch vectorization field also partially overlaps with the so-called Coherence Enhancing'' field. @cite estimated Tangent Vector Fields from images, and used them in order to clean or simplify the input. They do that by averaging a pixel value with its corresponding neighbors along the Vector Fields. This could be integrated as a useful preprocessing step in our system, or could be used as a standalone tool if the objective is just to obtain a simplified representation of the input image.
- Learning holistic representation for word images was popularized under the name of word spotting which was first coined by in @cite for indexing handwritten word images. Initial attempts @cite @cite @cite @cite , mostly focused on variable length representations of word images by considering it as a temporal sequence. Most of these methods used profile features @cite @cite which are computed at each column of the word image and are summarized using various pixel level statistics. @cite , used profile features namely vertical profile, upper & lower word profile, and background to ink transitions. @cite , used profile features along with moments based features. @cite , used a combination of profile, structural, and transfer domain (Discrete Fourier Transform) features for word image representation. Dynamic Time Warping () based algorithms were found to be useful for matching variable length representations and is quite popular in speech @cite @cite and other sequence matching problems. @cite @cite , profile features were combined with shape based structural features for a partial matching scheme using . Although these features are fast to compute, it's susceptible to noise and common degradations present in documents and thereby limiting to a reasonable quality of printed documents.
- With the popularity of the local gradient features such as @cite , @cite which describes a patch using histograms of edge orientations computed from a gradient image, the features are less susceptible to stray pixels and variations in brightness and contrast. Methods such as @cite @cite adapted local gradient features for word spotting where @cite used a continuous algorithm for partial word matching from the line images and @cite used Hidden Markov Model () based classification method. Most of the features discussed above are not robust to different fonts, writing styles and required careful image pre-processing techniques such as binarization, slant and skew correction which remain hard for handwritten and historical documents. Moreover, the methods such as and based scheme of matching variable length representations do not scale to large datasets due to the higher time complexity. Hence, the later methods appreciated more on fixed length representations built on top of highly engineered features proposed in computer vision.
- Although our use of constructive algebra within toposes to make models of intensional type theory appears to be new, we are not the only ones to consider using some form of path with strictly unitary and associative composition to model identity types with a judgemental computation rule. Van den Berg and Garner @cite use topological Moore paths and a simplicial version of them to get instances of their notion of for modelling identity types. The results of Sections and show that any ordered abelian group in a topos induces a path object category structure on that topos; and since the notion of fibration we use (Definition ) is closely related to the one used in @cite (see Proposition 6.1.5 of that paper), one can get alternative, more abstract categorical proofs of Theorems and from the work of Van den Berg and Garner. However, the concrete calculations in the internal language that we give are quite simple by comparison; and this approach proves its worth in , whose results on obtaining function extensionality from the ordered ring structure are new.
- The PhD thesis of North @cite uses a category-theoretic abstraction of the notion of Moore paths, called , as part of a complete analysis of when a weak factorization system gives a model (in terms of display map categories, rather than CwFs) of identity- , @math - and @math -types. A Moore relation system is a piece of category theory comparable to our use of ordered abelian groups in categorical logic in ; it would be interesting to see if it can be extended in the way we extended from groups to rings in in order to validate function extensionality.
- Spitters [Section 3] SpittersB:cubstt uses a somewhat different formulation of Moore path in the cubical topos @cite . His notion is the reflexive-transitive closure of the usual path types given by the bounded interval. For better properties and to get a closer relationship with our version, one would like to quotient these cubical Spitters-Moore'' path objects up to degenerate paths; but the undecidability of degeneracy seems to stop one being able to do that while retaining the (uniform) Kan-fibrancy of such path objects. Here we can side-step such issues, since notably our models manage to avoid using a notion of Kan fibrancy at all.
- The problem of next response selection in multi-turn conversation is more general than a traditional question answering (QA) problem @cite @cite . The prediction is made based on the entire conversation context which does not necessarily include a question. In single turn response selection, the model ignores the entire context and only leverages the last utterance to select response @cite @cite @cite . Since an utterance can change the topic or negate affirm the previous utterances, it is of paramount importance that models for response selection in multi-turn conversation have a certain understanding of the entire context. Moreover, next response selection system is a supervised dialogue system since it incorporates explicit signals specifying whether the provided response is correct or not @cite . This system is of interest because it admits a natural evaluation metric, namely the recall and precision measures (See for a detailed explanation.). We consider Ubuntu Dialogue Corpus @cite to evaluate our retrieval-based model since the dataset is the most relevant public dataset to supervised dialogue systems @cite .
- The original paper that introduced the Ubuntu Dialogue dataset have implemented a TF-IDF model in addition to neural network models with vanilla RNN and LSTM @cite . Later, evaluated the performances of various LSTMs, Bi-LSTMs and CNNs (Convolutional Neural Networks @cite ) on the dataset and created an ensemble by averaging predictions of multiple models. An RNN-CNN model combined with attention vectors is implemented by . Further, Multi-view Response Selection @cite proposed an RNN-CNN model which integrates information from both word sequence view and utterance sequence view. A deep learning model incorporating background knowledge to enhance the sequence semantic modeling ability of LSTM is implemented in that achieved the state-of-the-art result.
- @cite are among the first to study Twitter, aiming to understand its role on the Web. They show that Twitter is a powerful network that can be exploited to assess human behavior on the Web. However, the Web's information ecosystem does not naturally build on a single or a few Web communities; with this motivation in mind, @cite study how mainstream and alternative news propagate across multiple Web communities, measuring the influence that each community have on each other. Using a statistical model known as Hawkes Processes, they highlight that small fringe'' Web communities within Reddit and 4chan can have a substantial impact on large mainstream Web communities like Twitter.
- With the same multi-platform point of view, @cite propose an approach, called Bag of Communities, which aims to identify abusive content within a community. Using training data from nine communities within 4chan, Reddit, Voat, and Metafilter, they outperform approaches that focus only on in-community data.
- Other work also focuses on characterizing relatively small alt-right Web communities. Specifically, @cite study 4chan's Politically Incorrect board ( pol ), and show that it attracts a high volume of hate speech. They also find evidence of organized campaigns, called raids , that aim to disrupt the regular operation of other Web communities on the Web; e.g., they show how 4chan users raid YouTube videos by posting large numbers of abusive comments in a relatively small period of time.
- define additive functions over embeddings. In many translational approaches, the embedding for each entity @math is a single vector @math and the embedding for each relation @math is a vector @math and two matrices @math and @math . The dissimilarity function for a triple @math is defined as @math (i.e. encouraging @math ) where @math represents norm @math of vector @math . Translational approaches having this dissimilarity function usually differ on the restrictions they impose on @math and @math . In TransE @cite , @math , @math . In TransR @cite , @math . In STransE @cite , no restrictions are imposed on the matrices. FTransE @cite , slightly changes the dissimilarity function defining it as @math for a value of @math that minimizes the norm for each triple. In the rest of the paper, we let represent the FTransE model where no restrictions are imposed over @math and @math .
- generally use a neural network that learns how the head, relation, and tail embeddings interact. E-MLP @cite considers the embeddings for each entity @math to be a vector @math , and for each relation @math to be a matrix @math and a vector @math . To make a prediction about a triple @math , E-MLP feeds @math into a two-layer neural network whose weights for the first layer are the matrix @math and for the second layer are @math . ER-MLP @cite , considers the embeddings for both entities and relations to be single vectors and feeds @math into a two layer neural network. In @cite , once the entity vectors are provided by the convolutional neural network and the relation vector is provided by the long-short time memory network, for each triple the vectors are concatenated similar to ER-MLP and are fed into a four-layer neural network. Neural tensor network (NTN) @cite combines E-MLP with several bilinear parts (see Subsection for a definition of bilinear models).
- Some work circumvents the main problem of CLTC by using a black-box machine translation tool, such as Google Translate and others, to translate whole documents @cite @cite . The translated labeled documents are then used, perhaps with additional labeled documents that were originally in the target language, to learn a target-language classifier. Some works assume the availability of unlabeled target language documents, and use the tool to translate them into the source language @cite , thus transforming the problem into a semi-supervised learning task.
- The mapping from the source and target languages to the intermediate representation is usually learned from some external source. For Wikipedia concepts, usually the source is the text of the articles @cite @cite @cite @cite . Topic-based features and corresponding features are typically inferred from comparable corpora @cite . Mapping from each language to these language-independent features is commonly learned either from the original labeled training set @cite @cite @cite , or from independent (unlabeled) corpora in the required language @cite @cite .
- @cite , authors train FearNet's long-term memory store with a process called intrinsic replay @cite . This process is similar to our recursive training method, where pseudo-items are generated by the decoder and then rehearsed with the new items. Our generative model is isolated from the classification network, whereas FearNet combines these models. This means that when the long-term system is being trained, the classification loss and reconstruction loss are minimised concurrently. A further difference is that the reconstruction loss is minimised across each layer of the autoencoder. This limitation means that the function of neurons in intermediate layers of the network are being constrained.
- FearNet was shown to counteract CF on supposedly complex tasks such as CIFAR-100. However, FearNet was never trained on the CIFAR-100's raw images but rather the output of the first 49 weight layers (including the mean pooling layer) in ResNet-50 @cite that had been pre-trained on ImageNet. This means that the classification is done by a much smaller multi-layer perceptron and the majority of the work has been pre-trained into the ResNet architecture. Subsequently, the autoencoder is not learning to reproduce CIFAR-100 images but rather the items' output from the ResNet architecture. Although this is a more difficult task than training on MNIST, this method is incomparable to training a convolutional neural network to classify the task from raw input.
- A recent review of methods for overcoming CF concluded that current algorithms do not solve CF @cite . They also found that EWC performed the best for learning multiple tasks and thus, we compare pseudo-recursal to EWC. The main contributions of our paper are; we show that pseudo-recursal can be used to overcome the CF problem in DNNs, sequentially learning CIFAR-10, SVHN and MNIST and we demonstrate that pseudo-rehearsal can be applied recursively to a separate classification and generative model. This architecture also satisfies the previously mentioned criteria. Although we limit ourselves to image classification in this paper, our techniques are applicable to other problems.
- There has been significant literature over the last decade on the problem of Ontology Learning (OL). Most of these works can be categorized into two approaches as discussed earlier: (i) , and (ii) . Light-weight OL from text documents is arguably the most widely used approach in the field of OL @cite . It can be further divided into three general approaches: (i) , (ii) , and (iii) . It is to be noted that the general disadvantage of light-weight OL of not being able to generate definitional T-Box (and corresponding A-Box), as discussed in the introduction section, is inherent in all the three approaches. This is where @math parts away significantly from light-weight OL approaches. We first discuss light-weight OL in the following three sub-sections for providing a contrasting perspective, and then conclude the section with a discussion on formal OL.
- Research on light-weight OL heavily draws models and methodologies from the area of statistical NLP with the assumption of . The central objective is to model a similarity measure for concept comparison, thereby creating semantic spaces derived from two alternative models: (i) vector-space models (VSM), and (ii) probabilistic models. In VSM approaches, concept similarity is computed using distance (or similarity) measures in high-dimensional concept vector space. Such computation can be based on: (i) (such as cosine similarity @cite , Jaccard similarity @cite ), (ii) based (such as term-document based matrix models like LSA @cite or term-context terms based matrix models like HAL @cite ). Similar concepts are then clustered into taxonomies using variations of known hierarchical clustering algorithms. An early work on OL as an end-to-end application was proposed in @cite @cite where noun terms were organized into hypernymic trees using bottom-up hierarchical clustering algorithm. In @cite a clustering based algorithm called CBC was proposed for generating concept lists of similar instances with sense discrimination. However, such lists were unlabeled and hence, not useful in applications such as question-answering as observed in @cite .
- Doppio @cite and Whalesong @cite implement bytecode interpreters in the browser that do not use the JavaScript stack. Therefore, they can suspend and resume execution. However, since these are bytecode interpreters for other platforms (JVM and Racket, respectively), existing compilers and libraries would have to change significantly to use them. Browsix @cite acts as an operating system'' for processes in Web Workers. Therefore, it inherits Web Workers' restrictions: workers cannot share JavaScript values and cannot interact with the Web page. It also does not provide deep stacks. allows code to run in the main browser thread, enabling access to the DOM and allowing execution control for IDEs.
- Pivot @cite isolates untrusted JavaScript into an iframe and rewrites the program to use generators, which allows blocking I O between the iframe and the outside world. is not an isolation framework and implements blocking without generators or ( sec:callcc ).
- Many past projects have investigated implementing continuations in other platforms that do not natively support them, from C to .NET @cite @cite @cite @cite @cite @cite @cite . They use a variety of strategies ranging from with trampolines, to C's |setjmp| and |longjmp| to effectively provide tail calls. These systems do not provide 's other features ( intro ).
- Infinite media problems have been well studied in Flatland as well as spaces of general dimension @cite @cite @cite @cite @cite @cite @cite @cite @cite and for beams @cite . Some exact solutions have been presented for bounded @cite and layered @cite media, and the singular eigenfunctions for Flatland have been derived @cite . However, to the best of the authors' knowledge, solutions to the classic Milne and albedo problems for the half-space and the @math -function have not been presented. @cite , using an asymptotic analysis, have presented solutions to the classic Milne and albedo problems in two dimensions. The solutions make use of the Flatland equivalent of Chandrasekhar's @math -function, which is left as the solution to an integral equation. We present a complementary derivation of the Milne and albedo problem solutions using the Wiener-Hopf technique. In addition, we present new solutions and benchmark values for @math and provide some Monte Carlo comparisons for the albedo problem.
- In the study of energy-dependent neutron transport in three-dimensional volumes with plane symmetry, @cite presented a general family of solutions to the Milne problem using the method of singular eigenfunctions. When their variable- @math factor @math takes on the specific quantity (in our notation) @math , their energy-dependent 3D solution becomes equivalent to our monoenergetic Flatland solution. Thus, the Flatland @math -function has, if only inadvertently, been presented long ago.
- The differences between existing methods concern mainly the solver for the image-partitioning problem. In a continuous setting, following the approach of Mumford and Shah @cite , the problem has been addressed with an implicit level-set representation of the partitioning curve in @cite @cite @cite . A primal-dual optimization strategy was used in @cite . In a discrete setting, iterated conditional modes and high confidence first approaches were exploited in @cite @cite @cite . Graph-cuts methods have also been used in @cite , and more recently in @cite . Layered models, introduced in @cite , involve a similar optimization problem but add a depth information between the different regions, from which occlusions can be derived. This model has been revitalized in @cite @cite @cite @cite .
- The importance of initialization when optimizing ) with an alternating scheme is illustrated in @cite @cite , where the optimization is initialized through advanced motion estimation methods @cite @cite . In @cite , an alternating direction method of multipliers (ADMM) approach is used to solve ) without intermediate segmentation steps. However, the underlying model is piecewise-constant and not rich enough in most practical scenarios; it is initialized by a block matching algorithm.
- Most of the computational effort is spent on the image-partitioning problem. The earliest works retain at most five regions to make the problem tractable . More recently, the layered approach handles a larger number of regions but requires several hours of computation, and the primal dual approach can take up to one hour despite a GPU implementation. The method proposed in @cite achieves around fifteen minutes for @math image, with a graph cut minimization approach.
- Beyond solving ), other techniques can be involved to improve the results. They include the handling of occlusions @cite @cite @cite , label cost terms to limit the number of regions @cite @cite , edge-driven models to fit image boundaries @cite , deviations from the parametric models to estimate more complex deformations @cite , smoothness of the parameters of neighboring regions @cite , or post-processing refinements with a variational optimization of TV-based models @cite . Yet other methods rely on similar principles but incorporate additional information obtained from their applicative context, like epipolar constraints @cite @cite , temporal consistency @cite , or semantic information about the type of moving objects in the scene @cite .
- Extensions of TV to second order derivatives result in approximately piecewise-affine solutions @cite @cite . However, the @math norm does not delineate moving objects as sharply as the Mumford-Shah model ). In this line, the over-parametrized approach @cite @cite , which models a spatially varying parameter field with TV regularization, also shows this undesirable effect.
- The typical behavioral patterns of a driver are usually referred to by the term . This includes the choice of driving speed, headway, overtaking of other vehicles, or the tendency to commit traffic violations @cite .
- Defensiveness-aggressiveness is the most commonly used metric for defining driving style. Prior work refers to drivers as aggressive assertive versus defensive @cite ; or mild versus moderate versus aggressive @cite . In the Multidimensional Driving Style Inventory (MDSI), Taubman-Ben-Ari identified four broad driving styles: (1) reckless and careless driving, characterized by, for example, higher speed; (2) anxious driving; (3) angry and hostile driving, characterized by more use of the horn and flash functionality; and (4) patient and careful driving @cite . Similarly, Huysduynen categorized driving style as angry driving, anxious driving, dissociative driving, distress-reduction driving and careful driving style @cite . Horswill provided a valuable distinction between skill and style in the context of driving behaviors @cite . Hong @cite differentiated styles in terms of defensiveness, as well as by propensity for violation of rules. Scherer defined driving style in terms of comfort @cite . Lee @cite analyzed lane changes as a function of its severity (degree to which the vehicle in the destination lane was cut off), urgency (how soon the lane change was needed), and type classification for the full population of 8,667 lane changes.
- DBS has been used in multiple topics, such as dialogue response generation @cite , machine translation @cite , but also abstractive summarization @cite . However, DBS on its own contributes only marginally (+0.25 @math -score) to the performance of abstractive summarization. This is why we combine it with a candidate selection algorithm used in multiple fields, Maximal Marginal Relevance (MMR) @cite @cite . MMR is an algorithm that balances relevance and diversity in multiple set-based information retrieval tasks.
- Currently, a popular model for text generation is the sequence-to-sequence model . However, the sequence-to-sequence model tends to generate short, repetitive @cite , and dull text @cite . Recent researches have focused on developing methods to generate informative @cite and diverse text . Reinforcement learning is incorporated into the model of conversation generation to generate more human-like speeches . Moreover, there are also other methods to improve the diversity of the generated text by using mutual-information, prototype editing, and self attention .
- The optimization problem is considerably harder. A version of the problem, called DPP, where the each of the path need to have length bounded by some integer @math . This problem is -hard in the strong sense even in the One-face case for non-fixed @math @cite . For the shortest @math -DPP, where we want to minimise the sum of the lengths of the paths, very few instances are known to be solvable in polynomial time. For general undirected graphs, very recently, Bj " o rklund and Husfeldt @cite have shown that shortest @math -DPP admits a randomised polynomial time algorithm. The deterministic polynomial time bound for the same  to this date  remains an intriguing open question.
- Many methods to compute similarity between images have been proposed. Recently, DNNs have been used to extract image features to compute similarities @cite @cite . For example, DNNs based similarities have been applied to image retrieval @cite , person reidentification @cite , facial recognition @cite , and visual similarity for product design @cite .
- However, a method to estimate the similarities between classes has been proposed @cite @cite . In that method, images are divided into patches, and features are extracted from each patch using traditional methods, such as RGB color moment. In addition, to compute the distance between classes, we must assume that the images are generated from Gaussian mixture models (GMMs). Note that the number of GMM components must be determined manually relative to the number of target classes. In addition, the distance between classes expresses an inverse relation with similarities; they are not normalized, and their absolute values are meaningless. Here, two distances are involved, i.e., parametric distance (PD), which is the quadratic distance of the means and variances of a GMM, and an approximation of KL divergence. These two methods return similar results. Here, strong assumptions and simplifications were used to treat inter-class similarities realistically.
- Open set classification problems @cite are inherent and difficult in real-world applications. Thus, few studies have addressed such problems.
- However, a solution that employs features extracted using a DNN and meta-recognition has been proposed @cite . This solution is useful to eliminate dissimilar unknown unknowns and is, in particular, effective for fooling images.
- They present several examples including binary search, the Bellman--Ford algorithm and union-find. We provide several advanced examples including those involving the Akra--Bazzi method. We also demonstrate that verification of amortized analysis of functional programs @cite can be converted to verification of imperative programs with little additional effort.
- @cite present TiML, a functional programming language which can be annotated by invariants and specifically also with time complexity annotations in types. The type checker extracts verification conditions from these programs, which are handled by an SMT solver. They also make the observation that annotational burden can be lowered by not providing a closed form for a time bound, but only specifying its asymptotic behaviour. For recursive functions, the generated VCs include a recurrence (e.g. @math ) and one is left to show that there exists a solution for @math which is additionally in some asymptotic bound, e.g. @math . By employing an recurrence solver based on heuristic pattern matching they make use of the Master Theorem in order to discharge such VCs. In that manner they are able to verify the asymptotic complexity of merge sort. Additionally they can handle amortized complexity, giving Dynamic Arrays and Functional Queues as examples. Several parts of their work rely on non-verified components, including the use of SMT solvers and the pattern matching for recurrence relations. In contrast, our work is verified throughout by Isabelle's kernel.
- On the other end of the scale we want to mention Automatic Amortized Resource Analysis (AARA). Possibly the first example of a resource analysis logic based on potentials is due to Hofmann and Jost @cite . They pioneer the use of potentials coded into the type system in order to automatically extract bounds in the runtime of functional programs. successfully developed this idea further @cite @cite . @cite @cite extend this work to imperative programs and automatically solve extracted inequalities by efficient off-the-shelf LP-solvers. While the potentials involved are restricted to a specific shape, the analysis performs well and at the same time generates Coq proof objects certifying their resulting bounds.
- Recently, some hypergraph based methods, e.g., @cite @cite @cite @cite , have been proposed for robust model fitting due to its effectiveness . For example, Liu and Yan @cite proposed the random consensus graph (RCG) to fit multiple structures in data. @cite proposed to use large hyperedges for face clustering and motion segmentation.
- Compared with the hypergraph constructed in the previous methods (e.g., @cite @cite @cite @cite ), where a hyperedge is constrained to connect with a fixed number of vertices, the hyperedge of hypergraphs constructed in this paper can connect with a varying number of vertices (that is we construct non-uniform hypergraphs as those in @cite ) . In addition, the vertices of the hypergraph constructed in the previous methods (e.g., @cite @cite @cite @cite ) represent data points, while the vertices of the hypergraph constructed in this paper denote model hypotheses. Therefore, we can directly deal with the model fitting problem in the parameter space.
- In addition to the above-mentioned robust fitting methods, there are several other related fitting methods, such as KF @cite , J-linkage @cite , T-linkage @cite , SCAMS @cite , PM @cite , PEARL @cite , AKSWH @cite , HS @cite , RELRT @cite and GMD @cite . KF, J-linkage, T-linkage and SCAMS directly deal with data points for model fitting but they are sensitive to unbalanced data distributions that are quite common in practical applications . In addition, these methods have difficulties in dealing with the data points near the intersection of model instances. The computational costs of J-linkage and T-linkage are high due to the use of the agglomerative clustering procedure. The other robust fitting methods also have some problems. For example, PM requires the input of the number of model instances in data; PEARL is sensitive to the initial generated hypotheses; AKSWH may remove some good model hypotheses corresponding to the correct model instances involving a small number of data points, during the procedure of selecting significant hypotheses; HS encounters the computational complexity problem due to the expansion and dropping strategy used; both RELRT and GMD only work for single-structure data.
- There is a long line of research revolving around the use of randomly drawn features in machine learning. Extreme Learning Machines show the utility of keeping some layer of a neural net fixed - but this is usually done only for one or two layers, and not within layers @cite or across multiple (more than two) layers. @cite has shown how picking random features has merits over matching kernels to the data. @cite have analytically shown useful properties of random nets with Gaussian weights. As mentioned in the work of @cite , many of the theoretical works on deep neural networks assume specific conditions which are not known to hold in practice; we show empirically what happens when weights are selected randomly (and fixed) throughout various layer of the network and within layers.
- A very recent result is that of @cite , showing - quite surprisingly - that using a fixed, Hadamard matrix @cite for a final classification layer does not hinder the performance of a classifier. In contrast, we do not impose any constraints on the values of any of the fixed weights (except drawing them from the same distribution as that of the learned ones), and evaluate the effect of fixing many different subsets of weights throughout the network.
- Many works attempt to learn a compact representation by pruning unimportant filters: for example, compressing the network after learning @cite @cite @cite @cite ; performing tensor-decompositions on the filter representations @cite or regularizing their structure to have a sparse representation @cite ; and designing networks which are compact to begin with, either by architectural changes @cite @cite , or by learningdiscrete weights , @cite @cite .
- : Studies have focused on simplifying the temporal aspect of dynamics @cite @cite @cite in order to model co-occurrences of information across the modalities. In these models, each modality is summarized in a representation by collapsing the time dimension, such as averaging the modality information through time @cite . While these models are successful in understanding co-occurrences, the lack of temporal modeling is a major flaw as these models cannot deal with multiple contradictory evidences, eg. if a smile and frown happen together in an utterance. Furthermore, these approaches cannot accurately model long sequences since the representation over long periods of time become less informative.
- : Approaches have used multimodal input feature concatenation instead of modeling and dynamics explicitly. In other words, these approaches rely on generic models (such as Support Vector Machines or deep neural networks) to learn both and dynamics without any specific model design. This concatenation technique is known as early fusion @cite @cite . Often, these early fusion approaches remove the time factor as well @cite @cite . We additionally compare to a stronger recurrent baseline that uses early fusion while maintaining the factor of time. A shortcoming of these models is the lack of detailed modeling for dynamics, which in turn affects the modeling of dynamics, as well as causing overfitting on input data @cite .
- : Extensions of Hidden Markov Models @cite and Hidden Conditional Random Fields @cite @cite have been proposed for learning from multiple different views (modalities) @cite @cite . Extensions of LSTMs have also been proposed in a multi-view setting @cite .
- Studies involving subsequences and supersequences encompass a wide variety of problems that arise in various contexts such as formal languages, coding theory, computer intrusion detection and DNA sequencing to name a few. Despite their prevalence in such a wide range of disciplines, they remain largely unexplored and still present a considerable wealth of unanswered questions. In the realm of stringology and formal languages, the problem of determining the number of distinct subsequences obtainable from a fixed number of deletions, and closely related problems, have been studied extensively in @cite @cite @cite @cite . Perhaps it is worth noting that the same entropy minimizing and maximizing strings conjectured in @cite and characterized in the present work, have been shown to lead to the minimum and maximum number of distinct subsequences, respectively. The problems of finding shortest common supersequences (SCS) and longest common subsequences (LCS) represent two well-known NP-hard problems @cite @cite @cite that involve subproblems similar to our work. Finally, devising efficient algorithms for subsequence combinatorics based on dynamic programming for counting the number of occurrences of a subsequence in DNA sequencing is yet another important and closely related line of research @cite @cite .
- In coding theory, and more specifically in the context of insertion and deletions channels, similar long-standing problems have been studied extensively, and yet many problems still remain elusive. This includes designing optimal coding schemes and determining the capacity of deletion channels, both of which incorporate the same underlying combinatorial problem addressed in the present work. Considering a finite number of insertions and deletions for designing correcting codes for synchronization errors @cite @cite @cite and reconstructing the original string from a fixed subsequence @cite represent two specific and related research areas. More recent work on the characterization of the number of subsequences obtained via the deletion channel @cite @cite @cite , e.g., in terms of the number of runs in a string, shows great overlap with our work. A graph-theoretic approach for deletion correcting codes, closely related to our clustering analysis, including an alternative proof for the Hamming weight clustering given in Theorem based on a different approach, is given in @cite .
- An important body of research in this area is dedicated to deriving tight bounds on the capacity of deletion channels @cite @cite @cite @cite and developing bounding techniques @cite .
- Perhaps rather surprisingly, the problem of determining the number of occurrences of a fixed subsequence in random sequences has not received the same amount and level of attention from the various communities. The state-of-the-art in the finite-length regime remains rather limited in scope. More precisely, the distribution of the number of occurrences constitutes a central problem in coding theory, with a maximum likelihood decoding argument, which represents the holy grail in the study of deletion channels. A comprehensive survey, which among other things, outlines the significance of figuring out this particular distribution is given by Mitzenmacher in @cite .
- A variety of tree-based GP approaches to FC have been proposed, including for problems such as classification and clustering @cite @cite . Most work uses a representation where a single GP tree produces a single constructed feature, as the output of the tree. The input to the tree is generally the set of features, and an optional random value input. This representation has been extended so that multiple features may be constructed in a single GP individual, commonly using a multi-tree representation @cite @cite . Other representations have also been proposed @cite , including using multiple sub-trees as a set of constructed features @cite @cite , using specially-tailored node designs @cite , cooperative co-evolutionary GP @cite , and even by performing multiple GP runs (each producing a single constructed feature) @cite . These works share similarity with this paper in that they perform a transformation of the original feature space, but they do so in order to improve the performance of a data mining task, rather than to perform feature creation.
- Computer vision has been adopted for various applications in the sports domain. Prominent tasks include sports type @cite and activity recognition @cite @cite , tracking athletes and other objects of interest in videos @cite @cite and human pose estimation @cite @cite . @cite offer an overview of a wide range of application.
- For performance analysis of individual athletes, @cite propose a method to facilitate speed and stride length estimation for runners based on hand-held camera recordings. Specific to an aquatic environment, @cite describe how swimmers can be tracked when filmed by a moving camera above the water surface. @cite discuss the identification of characteristic poses of swimmers. @cite present a CNN approach for automatic stroke-rate estimation in swimmer videos.
- The traditional approach to human pose estimation are pictorial structures, where the human body is modeled as a collection of interacting parts @cite @cite @cite @cite @cite . The model describes the appearance of individual parts and the relationship between interacting parts in a probabilistic fashion. The goal is to find the most probable part configuration given an input image.
- Recent literature focuses on methods using CNNs to overcome the drawbacks of hand-crafted image features and limited part interactions present in classical approaches. The currently best results on popular human pose estimation benchmarks like the Leeds Sports Pose (LSP) @cite and MPII Human Pose @cite datasets all apply CNNs. @cite describe an architecture that directly re -gres -ses the image coordinates of body joints. Subsequent publications regress confidence maps that indicate the likelihood of all possible joint locations in an image @cite @cite @cite @cite . This spatial encoding of the learning objective seems to be more natural to CNNs compared to the direct regression of image coordinates. Another common design are architectures performing iterative refinement @cite @cite @cite . After producing an initial pose estimate it is progressively refined in the deeper layers of the network. There are also proposals to use classical part-based models to refine the pose estimates from CNN-based methods, either as a separate post-processing step @cite or by mapping the domain-specific part interactions into the neural network itself for an end-to-end trainable architecture @cite .
- While most publications focus on human pose estimation on single 2D images, we are additionally interested in human pose estimation on videos. @cite @cite use pictorial structures to model humans in videos. They extend the spatial interactions between body parts by temporal dependencies that describe the change of body part configurations over time. Flowing Conv-Nets @cite combine a CNN for human pose estimation on single images with a second CNN for the optical flow in videos that enables an estimate of the movement of body parts. In @cite , optical flow and both spatial and temporal part interactions are used jointly in a single network architecture. @cite describe a recurrent neural network (RNN) architecture applied to sequential video frames. In our approach we avoid the computational expensive extraction of optical flow and the data-intensive training of RNNs due to limited video material.
- Criteria for good' graph visualization have been investigated extensively @cite . Graph drawing algorithms over the years typically take into account one or more aesthetic criteria for better of the drawing. These aesthetic criteria include, for example, enumerate minimizing the number of edge crossings @cite ; minimizing the total area @cite ; edge lengths should be short but not too short @cite . Amongst these aesthetics, small number of edge crossings is one of the most common criterion from previous user studies @cite . Besides, the amount of ink and minimum total edge length has been used in many layout algorithms; for example, @cite @cite @cite @cite . Achieving small total area is another common approach @cite . Overall, improving multiple aesthetics can produce better graph drawings @cite .
- A number of graph drawing algorithms have aimed for a minimum total edge length, or more precisely, a minimum amount of ink. This criterion has been studied @cite @cite @cite @cite .
- @cite analyze 6M access logs from the Wayback Machine, aiming to understand what users are looking for, and why they use it. They find that users visit the site predominantly via referrals, and that they mostly look for English pages, while most popular country-specific domains are from Japan, Russia, and Germany. @cite simulate a Web archiving service, studying social discourse through the URLs as well as relevant entities and metadata, by analyzing millions of tweets as well as a case study related to fake news. @cite measure how much content is available on Web archiving services: they sample URL shorteners and search engines, query 12 public archives, and find that 35 , @cite assess whether the Wayback Machine archives a purely random sample of Web pages, finding some bias towards more visible and prominent pages.
- Archived content. @cite study the evolution of JavaScript using historical data of 3M Web pages from the Wayback Machine, while @cite analyze how Web trackers have evolved in the previous decade. @cite predict whether an uncompromised website will become malicious also using the Wayback Machine. @cite study how the German Web has evolved in the past 18 years using data from the Wayback Machine for 100 popular domains, highlighting the exponential growth of the number of pages. Then, @cite characterize the accessibility of websites using a random sample of sites from the Wayback Machine between 1997 and 2002.
- Security. Researchers have also focused on the security aspects of Web archiving services and link shorteners. @cite study and address vulnerabilities on the Wayback Machine which allow attackers to manipulate the archived content by injecting JavaScript code. @cite find that the 5- and 6-character space of link shortneners is small and can be easily scanned using simple algorithms, thus, an attacker could access personal sensitive data. @cite perform a two-year measurement of users' interactions with 622 URL shorteners, showing that a small subset of the users encounter malicious URLs and content. Finally, @cite show that ad-based shorteners are more hazardous compared to traditional ones.
- Video Summarization. As introduced in , although having different goals, the technical aspects of video summarization are quit similar and can be sufficiently utilized by video composition. Many video summarization approaches have been proposed via different image-based feature representations and optimization methods, either through low-level feature such as optical flow @cite and image differences @cite , or high-level representations, including object trackers @cite and importance scores @cite . On the other hand, subshot-based methods represent summarizations via spatio-temporal features @cite . Numerous supervised approaches select the subshots to represent the videos based on submodular function @cite and exemplas @cite . All these methods require ground truths for training.
- However, the labeling of the ground truth for either video summarization or video caption is too subjective and difficult as a consistent limitation to the above methods. In contrast, our model is learned in an unsupervised manner, making the framework more flexible to utilize large amount of data to improve the performance. Story Composition. The story composition methods typically focus on identifying the temporal alignment of the image sets (photo albums). @cite use static and dynamic features to find the temporal order of the image sequence. @cite learn the pairwise transition to construct the storyline graphs. Recently, an unsupervised method proposed by @cite use a skipping Recurrent Neural Network to learn long-term correlations.
- Recent studies on image captioning have been focusing on the application of deep neural networks since the release of MS-COCO Image Captioning Challenge http: mscoco.org dataset #captions-challenge2015 . The authors from @cite @cite first apply deep learning to predicting words and phrases from the given images. The captions are then generated by another language model, which composes the candidate words into a sentence.
- Meanwhile, most of recent publications have been using an encoder-decoder framework @cite @cite for decoding the encoded images into a sentence. The work in @cite proposed a CNN-RNN framework, which is simple but very effective. Their model ranked first in the 2015 MS-COCO Image Captioning Challenge. Both @cite and @cite employed the multimodal RNN for learning the semantic mapping between images and words, where the encoded image is supplied at each step of the RNN for learning the multimodal layer. More recently, attention model @cite @cite @cite @cite @cite , which tries to learn the alignment between source language and target language in machine translation, has widely been adopted for building better image captioning systems.
- In summary, generating different styles of captions requires us to not only bridge the semantic meanings between image and text, but also build a language generative model that can understand the differences between different language styles. This makes it more difficult for building sentimental captioning systems. There have been several preliminary studies @cite @cite @cite . However, @cite proposed a model that cannot be trained in an end-to-end fashion. There were only some preliminary example results in @cite . On the other hand, @cite tried to learn the model from large scale weakly and noisily supervised data, where additional care needs to be taken to reduce the noises. In contrast, our proposed approach is simple to train, yet it significantly outperforms the state-of-the-art.
- Understanding migrant integration and the effectiveness of policy measures to favor assimilation is a longstanding challenge. A wide range of aspects such as civic integration policies' @cite or multiculturalism @cite have been analyzed. These studies developed evaluation metrics based on concepts such as political trust @cite as well as lack of electoral participation or composite measures of civic integration @cite . For a review of empirical and theoretical challenges see @cite (2005).
- New information, like Web and social media data, are a main source of innovation in the context of migration studies. Research in this area has focused on using online data to improve estimates of migration flows and stocks. After Zagheni and Weber used geo-located Yahoo! e-mail data to estimate international migration flows @cite , several platforms have been used to understand the network structure of migration, including Facebook @cite and Google+ @cite . Geo-located Twitter data has proved useful for studying the relationship between internal and international migration @cite , as well as short-term mobility versus long-term migration @cite @cite . LinkedIn data has provided insights into global patterns of migration for professionals @cite .
- Imaging denoising is a classic low-level vision problem which have been widely studied in past decades. The image prior modeling often play a central role in image denoising. Traditional methods that used image prior knowledge as regularization techniques, scuh as nonlocal self-similarity models @cite @cite @cite @cite , Markov Random Field (MRF) @cite @cite @cite and spares models @cite @cite , have shown very promising performance. However, in traditional denoising methods, image prior knowledge are explicitly pre-defined, which are often limited in capturing the full characteristics of image structure and limited in blind image denoising.
- Jain and Seung @cite demonstrated that convolutional neural networks (CNNs) can be used for image denoising and claimed that CNNs have achieved comparable or even superior performance than the MRF methods. @cite proposed to incorporate residual learning and batch normalization learning strategies into very deep CNN for denoising. @cite proposed to use skip-layer connection to symmetrically link convolutional and deconvolutional layers, which is able to train even deeper CNN architecture for denoising. Peng and Fang @cite proposed a wider CNN network which has relatively fewer layers but has larger size and number of filters in each layer. They claimed that for low-level vision tasks, the depth of the network is not the key, while the width of the architecture is more important. They state that for denoising tasks, deep learning denoising models learn prior pixel distribution information from original image and then use the learned filter banks to restore degrade images. Thus, the more concentrated convolutions to capture the prior image distribution from noisy images, the better the denoising performances.
- Residual learning is a technique to solve the gradient vanish problem @cite . As the the number of layers increases, the training accuracy of CNN begins to decrease due to gradient vanishing in lower layers. By constructing residual units (i.e., identity shortcuts or skip connections) between a few layers, residual network learns a residual mapping which is much easier to train and prevent gradient vanish. With residual learning strategy, training extremely deep CNN become possible. @cite shows improved performance when using residual learning for image classification and object detection.
- There are several studies that incorporate residual learning for denoising tasks @cite @cite @cite . In @cite , they used Skip shortcuts to connect from convolutional feature maps to their corresponding deconvolutional feature maps every a few layers, which help ease back-propagation and reuse details. In @cite , proposed DnCNN to using a mapping directly from an input observation to the corresponding reference observation.
- Overshadow implemented a shadow page table containing multiple mappings (encrypted and unencrypted) of a guest's physical memory, and actively tracks the identity'' of the guest process attempting to read a page. When the accessing process does not have the correct identity, an encrypted page is presented (on read) or the machine is terminated (on write). When the accessing process does have the correct identity, an unencrypted page is presented with the permissions originally granted to the page. Unfortunately, its dependence on shadow page tables means it's likely to be insufficiently efficient @cite for modern operating systems and heavy load systems.
- Finally, Spider @cite comes the closest to implementing a solution that maximizes efficiency, while retaining the flexibility of traditional binary modification breakpoints. Similar to VAMPiRE, it leverages virtual page permissions to determine what view'' of memory should be provided to the processor. On read write, a sanitized'' view of memory (sans breakpoints) is provided to hardware to prevent detection. On execute, the modified page (containing int3 instructions) is provided directly to the processor.
- One way to organize related work is use the five types of algorithms for super-voxel video segmentation analyzed by Xu and Corso @cite . (A) Paris and Durand @cite propose a method that achieves hierarchical segmentation in videos using topological persistence using the classic mode-seeking meanshift algorithm interpreted under Morse theory as a topological decomposition of the feature space. (B) @cite use , in which the Nystrom approximation is applied to solve the normalized cut problem, for spatiotemporal grouping. (C) @cite is a variant of optimizing the normalized cut that computes a hierarchy of sequentially coarser segments by an algebraic multigrid solver. (D) is an adaptation of the Felzenszwalb and Huttenlocher image segmentation algorithm @cite to video segmentation by building the graph in the spatiotemporal volume where voxels (volumetric pixels) are nodes connected to 26 neighbors. (E) is an algorithm for video segmentation proposed in @cite that iteratively builds a tree structure of region graphs, starting from over-segmented spatiotemporal volumes obtained using the method illustrated above. The regions are described by LAB histograms of the voxel members, the edge weights are defined by the @math distance, and the regions are merged using the same technique as in @cite .
- Unsupervised learning ( @cite ) and unsupervised deep learning ( @cite , @cite , @cite ) are central topics to Machine Learning. Unsupervised deep learning has been shown to improve results on classification tasks per @cite , especially given small datasets and complicated high dimensional data such as video. This has been explored by many representations including sequence to sequence learning and textual representations ( @cite , @cite ).
- Our work focuses on unsupervised deep learning for discovering visual object categories. This has also been shown to improve results such as in @cite . Unsupervised discovery of visual objects has been a large topic of interest in computer vision ( @cite @cite @cite @cite @cite @cite ).
- Building specialized, deep embeddings to help computer vision tasks is also a popular approach such as in @cite . Transfer learning from supervised tasks has proven to be very successful. Further, @cite propose learning the lower dimensional embedding through unsupervised learning and show improved performance when transfered to other supervised tasks.
- Despite the popularity of building different embeddings, there is little work investigating the use of clustering to modify the embedding in an end-to-end deep learning framework. @cite investigate a differentiable version of the kmeans algorithm and examine its convergence properties. Our work focuses on learnable feature representations (instead of fixed ones as in @cite ) and introduces memory units for the task.
- In addition to weather condition classification, more weather properties have also been investigated. Jacobs and his colleagues @cite initiated a project for collecting outdoor scene images captured by static webcams over a long period of time. The collected images form the Archive of Many Outdoor Scene (AMOS) dataset @cite . Based on the AMOS dataset, they proposed that webcams installed across the earth can be viewed as image sensors and can enable us to understand weather patterns and variations over time @cite . More specifically, they adopted principal component analysis and canonical correlation analysis to predict wind velocity and vapor pressure from a sequence of images.
- Recently, @cite estimated scene attributes like lighting, weather conditions, and seasons for images captured by webcams based on a set of regressors. @cite studied the correlation between pixel intensity camera motion and temperature and found a moderate correlation. With this observation, a regression model considering pixel intensity was constructed to predict temperature. Following the discussion in @cite , @cite showed that, with appropriate fine tuning, deep features can be promising for temperature prediction. @cite proposed a selective comparison learning scheme and showed that the state-of-the-art temperature prediction performance can be obtained by a CNN-based approach.
- In this work, we aim at predicting temperature from a single image, as well as forecasting the temperature of the last image in a given image sequence. Deep learning approaches will be developed to consider temporal evolution of visual appearance, and promising performance will be shown. Compared with @cite , @cite , and @cite , we particularly advocate the importance of modeling temporal evolution with designed deep networks.
- The metric model is one of the earliest models developed to represent range-limited communication between biological swarm agents @cite @cite @cite . This model is used as a benchmark for comparison testing relatively newer communication models @cite @cite @cite . The model is widely-used in the field of multi-robot systems, as well, due to its ability to capture sensor range constraints @cite @cite @cite . However, a field study of European starlings, Sturnus vulgaris , indicates that the swarm uses a topological, rather than a range limited model @cite . Specifically, starlings coordinate with their nearest six to seven neighbors (topological distance). An artificial swarm, in response to a simulated predator, decomposes into fewer groups, and produces more cohesive swarms, when using the topological model compared to the metric model @cite . Strandburg-Peshkin @cite introduced the visual model, and show that it best predicts how golden shiners, Notemigonus crysoleucas , behave in response to stimuli. The model's low clustering makes it fundamentally different from the metric and topological models, from a network-theoretic perspective.
- In another related study @cite , we investigate breaking changes in 317 real-world Java libraries, including 9K releases and 260K client applications. We show that 15 compatibility with previous versions and that the frequency of breaking changes increases over time. Using data from the BOA ultra-large dataset @cite , we report that less than 3 Dig and Johnson @cite studied API changes in five frameworks and libraries (Eclipse, Mortgage, Struts, Log4J, and JHotDraw). They report that more than 80 changes in these systems were due to refactorings. By contrast, using a large dataset of 400 popular Java libraries and frameworks, we also found that BCs are usually related to refactorings, but at a lower rate (47
- One final group of related approaches is to build new representations, which mask the protected attribute @cite . The use of neural networks have become popular for this task, such as variational auto encoders @cite and adversarial networks @cite . One of the seminal works in this field used an autoencoder with three separate terms in the loss @cite , and provides one of the largest comparisons on three now-standard datasets. We replicate their evaluation procedure in this work.
- NER for social media content is however difficult, leading to much work, including general approaches @cite , topic-specific approaches @cite , adapting from known genres @cite ; these are driven by and evaluated in multiple recent shared tasks @cite @cite . The task is generally cast as a domain adaptation problem from newswire data, integrating the two kinds of data for training @cite or including a lexical normalisation step @cite to shift text to territory more familiar to existing models and methods. Major challenges are that NEs mentioned in tweets change over time @cite , and that diversity of context makes NER more difficult @cite . This paper addresses NER without using large amounts of labelled in-domain data, in order to track entity propagation at scale.
- Studies of information diffusion have largely concentrated on signals of diffusion such as tracking URLs, hashtags, quotes @cite , and adoption behaviour (e.g. group signups); however to the best of our knowledge such studies have yet to focus on how entities diffuse. We now focus on key pieces of work that are closely-aligned to the study of entity-diffusion in the context of social networks -- should the reader wish to know more about information diffusion models, and in greater detail, then please refer to 's @cite comprehensive survey of such models.
- The study of information adoption and sharing was undertaken by @cite who conducted a large-scale randomised controlled trial to examine the effects of on information diffusion, using the Facebook platform. The authors were able to assign Facebook users with a @math probability to a group, and the remainder to a group and then information (i.e. status posts) posted within the latter's group. found that users who were to information (i.e. those in the feed group) from their friends are more likely to share it on -- implying that such exposure has an influential effect.
- Prior work on Reddit has examined the site's evolution since launch, seeing it evolve from a bulletin-like page to a large community site with many segragated and unique sub-communities that reinforce a general perception of the overall community @cite . This observation supports the use of Reddit as a study venue for information diffusion, finding that communities are large, well-defined, and cohesive. Later work covers the mapping of popular content @cite and of network structure @cite , though not the diffusion of information through those networks.
- @cite predicted adoption probabilities in social networks by controlling for potential confounding, unobservable variables -- proposing a modification of expectation-maximisation to induce a Naive bayes predictive model. The authors found that social influence alone is insufficient to recover the diffusion process, and thus external factors -- that are latent -- must be countered for within any predictive model -- this was in the context of predicting the adoption of social ties. The adoption of information within a social network and its propagation was studied by @cite by considering the role of temporal dynamics. The authors found that the probability of diffusion between users ( on Chinese microblogging platform Sina Weibo) reduces as a function of time from the last interaction between the users, thereby suggesting that have a strong effect in diffusion. We build time into our adaptation of 's @cite general threshold diffusion model -- by comparing static and discrete time versions of adoption probabilities.
- @cite presented the concept of Social Robot Architecture, which integrates the key elements of agenthood and robotics in a coherent and systematic manner. The ethical and social implications of robotics were discussed by in @cite . @cite examined social-psychology concepts to apply them to the human-robot interaction. @cite presented a case study where they analysed the effects of robot features (human-likeness and gender) and user characteristics on the human-robot interaction acceptance and psychological anthropomorphism. @cite analysed the effects of gesture on the perception of psychological anthropomorphism, by conducting a case study using the Honda humanoid robot. @cite conducted a cross-cultural study on generation of culture dependent facial expressions of humanoid robot. @cite discussed the use of observational studies of human-robot social interaction in open human-inhabited environments. Klein and Cook @cite analysed and compared the findings in the UK and Germany on robot-therapy with emotional robots as a treatment approach for people with cognitive impairments.
- There were also a number of surveys and literature reviews on the related topics. A survey on social robots for long-term interaction was presented in @cite . A systematic review on application of social robotics in the Autism Spectrum Disorders treatment was presented in @cite . @cite presented a survey on the roles and benefits of social robots in the therapy of children with autism.
- @cite introduced a design framework enabling the development of social robotics applications by cross-disciplinary teams of programmers and interaction designers.
- Apart from the , some studies proposed methods to improve the performance of the headline generation task. incorporated AMR @cite into the encoder to use the syntactic and semantic information of the source. also encoded additional information of the source such as TF-IDF, part-of-speech tags and named entities. modeled the typical structure of a headline, such as Who Action What'' with a variational auto-encoder. These approach improved the performance of the headline generation but it is unclear whether they can reduce the .
- Since objects describe smooth trajectories in image space, as a function of viewing angle, it has long been known that such trajectories span a 3D manifold in image space, parameterized by the viewing angle. Hence, many of the manifold modeling methods proposed in the literature @cite @cite @cite could, in principle, be used to develop trajectory transfer algorithms. However, many of these methods are transductive, , they do not produce a function that can make predictions for images outside of the training set, and do not leverage recent advances in deep learning. While deep learning could be used to explicitly model pose manifolds, it is difficult to rely on CNNs pre-trained on ImageNet for this purpose. This is because these networks attempt to collapse the manifold into a space where class discrimination is linear. On the other hand, the feature trajectories in response to pose variability are readily available. These trajectories are also much easier to model. For example, if the CNN is successful in mapping the pose manifold of a given object into a single point, , exhibits total pose invariance for that object, the problem is already solved and trajectory leaning is trivial for that object.
- .5ex One of the main goals of trajectory transfer is to fatten'' a feature space, by augmenting a dataset with feature responses of unseen object poses. In this sense, the problem is related to extensive recent literature on GANs @cite , which have been successfully used to generate images, image-to-image translations @cite , inpainting @cite or style-transfer @cite . While our work uses an encoder-decoder architecture, which is fairly common in the GAN-based image generation literature, we aim for a different goal of generating CNN feature responses. This prevents access to a dataset of real'' feature responses across the pose manifold, since these are generally unknown. While an ImageNet CNN could be used to produce some features, the problem that we are trying to solve is exactly the fact that ImageNet CNNs do not effectively model the pose manifold. Hence, the GAN formalism of learning to match a real'' distribution is not easily applicable to trajectory transfer.
- Instead, trajectory transfer is more closely related to the topic of transfer learning, where, now, there is extensive work on problems such as zero-shot @cite @cite @cite or @math -shot @cite @cite @cite @cite learning. However, these methods tend to be of general purpose. In some cases, they exploit generic semantic properties, such as attributes or affordances @cite @cite , in others they simply rely on generic machine learning for domain adaptation @cite , transfer learning @cite or, more recently, meta-learning @cite @cite @cite . None of these methods exploits specific properties of the pose manifold, such as the parametrizations of Figure . The introduction of networks that enforce such parameterizations is a form of regularization that improves on the transfer performance of generic procedures. This was shown on the AGA work @cite and is confirmed by our results, which show even larger gains over very recent generic methods, such as feature hallucination proposed in @cite .
- .5ex Finally, trajectory transfer is of interest for problems involving multi-view recognition. Due to the increased cost of multi-view imaging, these problems frequently include some degree of learning from computer generated images. This is, for example, an established practice in the shape recognition literature, where synthetic image datasets @cite @cite are routinely used. The emergence of these artificial datasets has enabled a rich literature in shape recognition methods @cite @cite @cite @cite @cite @cite and already produced some interesting conclusions. For example, while many representations have been proposed, there is some evidence that the problem could be solved as one of multi-view recognition, using simple multi-view extensions of current CNNs @cite . It is not clear, however, how these methods or conclusions generalize to real world images. Our results show that feature trajectory transfer models, such as FATTEN, learned on synthetic datasets, such as ModelNet @cite , can be successfully transferred to real image datasets, such as SUN-RGBD @cite .
- Several recent works have revisited NAM and computed bounds on its convergence rate based on different techniques. In addition to @cite , the following works are relevant. @cite views NAM as a linear coupling between GD and Mirror Descent, and, for @math , re-derives the previously known bound @math @cite , with the choice @math and @math , which is different from Nesterov's bound. This rate is not of the type that we consider in this paper.
- The work of @cite views (an adaptive version of) NAM with @math as the discretization of the second-order ODE @math . For @math and @math they obtain @math If @math , this leads to @math . Unfortunately, @cite show that their framework is incapable of providing linear convergence rates in general, which we know to hold for @math .
- The work of @cite focuses only on a convex quadratic function @math , and obtain @math for @math and @math , basically re-deriving .
- Finally, @cite gives a possible geometric interpretation of why NAM accelerates convergence. For their NAM-type method, the result @math is obtained, basically re-deriving .
- Since Widrow's early work, research into the use of neural architectures and machine learning techniques to address problems in communications has taken off in many directions. Ibnkahla's comprehensive review of the intersection of communications and machine learning @cite is a testament to the impressive efforts made in this area of research. His survey lists various learning-based approaches to adaptive equalization, nonlinear channel modeling, coding, error correcting codes, spread spectrum applications, network planning, modulation detection and many more. Due to the vastness of this field, we refer the reader to the review. Research in making radio agents adaptive to achieve cooperative goals began in 1999, when Mitola introduced the concept of the cognitive radio @cite . His proposal was for cognitive radios to use model-based reasoning to achieve competency in radio related tasks using both supervised and unsupervised learning. A review of cognitive radio work in years after that is in @cite .
- Beyond the applications to game-playing and control tasks, some researchers have begun to investigate the use of reinforcement learning in various communication-based cooperative multi-agent tasks. @cite applied variants of deep Q-networks (DQN) without experience replay to prisoner's games with multiple agents. The considered problems require the agents to communicate over a very simple noiseless channel of small bandwidth and develop a collaborative strategy. Mordatch and Abbeel @cite show how a grounded, compositional language can emerge when agents have to communicate their intentions to each other in order to maximize their reward. Finally, Abadi and Andersen @cite study the problem of learning encryption between agents with the help of an adversary.
- can leverage gradient information to solve the problem. Levin and Weiss, for instance, require manual input to separate gradients of the reflection and the transmission @cite . Methods that are fully automated can distinguish the gradients of the reflected and transmitted images by leveraging the defocus blur @cite : reflections can be blurry because the subject behind the semi-reflector is much closer than the reflected image @cite , or because the camera is focused at infinity and the reflected objects are close to the surface @cite . Moreover, for the case of double-pane or thick windows, the reflection can appear doubled'' @cite , and this can be used to separate it from the transmitted image @cite . While these methods show impressive results, their assumptions are stringent and do not generalize well to real-world cases, causing them to fail on common cases.
- can also be used to remove reflections. Several methods propose different ways to estimate the relative motion of the reflected and transmitted image, which can be used to separate them @cite @cite @cite @cite @cite . It is important to note that these methods assume static scenes---the motion is the apparent motion of the reflected layer relative to the transmitted layer, not scene motion. Other than that, these methods make assumptions that are less stringent than those made by single-image methods. Nonetheless, these algorithms work well when reflected and transmitted scenes are shallow in terms of depth, so that their velocity can be assumed uniform. For the case of spatially and temporally varying mixes, Kaftory and Zeevi propose to use sparse component analysis instead @cite .
- offer a third venue to tackle this problem. Assuming that images taken at different polarization angles offer independent measurements of the same scene, reflection and transmission can be separated using independent component analysis @cite @cite @cite . An additional prior that can be leveraged is given by double reflections, when the semi-reflective surface generates them @cite . Under ideal conditions, and leveraging polarization information, a solution can also be found in closed form @cite @cite . In our experiments, we found that most of the pictures captured in unconstrained settings break even the well-founded assumptions used by these papers, as shown in Figure .
- In a broad sense we address anomaly detection in sequential data @cite while focusing on intrusion detection in cyber-physical systems. Intrusion @cite refers to possible security breaches in (cyber-)systems, namely malicious activity or policy violations. It covers both intrusions , i.e. attacks from the outside, and misuse, i.e. attacks from within the system. An intrusion detection system (IDS) is thus a device that monitors a system for detecting potential intrusions. The IDS will be referred to as NIDS if the detection takes place on a network and HIDS if it takes place on a host of a network. Furthermore, we distinguish i) signature-based IDS approaches, that detects attacks by looking for predefined specific patterns, such as byte sequences in network packets, or known malicious sequences of instructions used by malware, to ii) anomaly-based intrusion detection systems that were primarily introduced to detect unknown attacks (zero-day attacks).
- In this work we exclusively address host intrusion detection system (HIDS) through semi-supervised anomaly-based approaches. Since Forrest's pioneering work @cite most of HIDS (at least in the UNIX LINUX sphere) use system call sequencesas their primary source of information. Generally, sequences of system calls are represented as sequences of integer symbols, for which the order of occurrence of the symbol is of crucial importance. Numerous work and surveys have been published in the area of anomaly detection in the scope of intrusion detection, see @cite @cite , @cite for recent studies. If we reduce the area of interest to anomaly detection in sequential data, four avenues for handling symbolic sequences are mainly followed:
- @cite are quite popular since a fixed size window enables a wide range of statistical, knowledge-based and machine learning techniques to be applied in a straightforward manner. A fixed-size window is first defined, and then it progressively slides along the tested sequence. Each window (basically a fixed-size subsequence) is in general represented by a feature vector. Then, models such as the one class Support Vector Machine @cite , Multi Layer Perceptron and Convolutional Neural Networks @cite are used to provide a score for deciding whether an anomaly is present or not.
- @cite process each sequence as a whole and a pair-wise sequence kernel (string kernel) is used to provide the sequence space with a similarity measure. The @math -Near-Neighbor rule or any of the so-called kernel machine methods can then be applied to model the 'normal' clusters and isolate the 'anomalies'. These approaches find their roots in text processing @cite (Levenshtein's distance) or in Bioinformatics @cite @cite (Smith and Watermans), @cite (Needleman-Wunsch), @cite (BLAST) and Longest Common Subsequence (LCS) or Longest Similar Subsequence @cite @cite . Such methods do not seem to outperform window-based approaches and are in general much more costly in term of algorithmic complexity than state of the art methods.
- , essentially Hidden Markov Models (HMM) @cite @cite @cite , Conditional random Fields (CRF) @cite @cite or Recurrent Neural Networks (RNN, LSTM, etc.) @cite @cite have been used with apparent success on various intrusion detection tasks, such as payload analysis or Network Layer Intrusion Detection or HIDS. However, the choice of parameters such as the order of the Markovian dependency, number of hidden variables, etc., is often the result of a compromise to avoid over-fitting, and long-term dependency is not necessarily easily modeled.
- have been proposed initially to extract very simple n-gram features to enhance a vector space model similar to the one used in text mining @cite , @cite . Recently, a much ambitious model has been proposed that proposes to enact phrases and sentences, hence a language, from sequences of system calls @cite . Nevertheless, these approaches suffer from the combinatorics explosion. When simple @math -grams models are used (with @math lower than @math or @math ) the size of the vector space model is very high (several millions of dimension) and in general the lack of available data to train the model limits its accuracy. In the case of approach @cite , the combinatorics is much higher with an estimated feature space dimension of @math which makes this model intractable for common hardware.
- One essential issue for positioning the UAV is to predict the air-to-ground channel, since a precise UAV-user channel knowledge is usually not available before the UAV is sent to a target position. As a way to circumvent this problem, some prior works simply assumed LOS propagation from the UAV to the ground user, and hence the channel gain is merely an explicit continuous function of the UAV position. Using the LOS model, @cite @cite @cite @cite focused on UAV navigation problems, and @cite studied the mimo channel to jointly optimize the UAV position and the orientation of a ula mounted at the UAV for the best spatial channel to multiple users. However, LOS models overestimate the channel gain in urban scenarios, especially in low altitude UAV applications, because there the air-to-ground signal is likely blocked by obstacles surrounding the user, and such a user-side shadowing effect significantly dominates the relay performance.
- Over the years, the size of datasets for video understanding has grown steadily. KTH @cite and Weizmann @cite were early datasets for human action understanding. UCF101 @cite and THUMOS @cite are built from web videos and have become important benchmarks for video classification. Kinetics @cite and YouTube-8M @cite introduced a large number of event classes by leveraging public videos from YouTube. The micro-videos dataset @cite uses social media videos to study an open-world vocabulary for video understanding. ActivityNet @cite explores recognizing activities in video and AVA @cite explores recognizing amd localizing fine-grained actions. The something something'' dataset @cite and Charades @cite used crowdsourced workers to collect video datasets while the VLOG dataset @cite collects daily human activities with natural spatio-temporal context.
- Environmental and ambient sound recognition is a rapidly growing area of research. @cite collected an early dataset and assembled a challenge for sound classification, Piczak @cite collected a dataset of fifty sound categories and enough to train deep convolutional models, @cite released a dataset of urban sounds, and @cite use web videos for sound dataset collection. Recent work is now developing models for sound classification with deep neural networks. For example, Piczack @cite pioneered early work for convolutional networks for sound classification, @cite transfer visual models into sound for auditory analysis, and @cite develop large-scale convolutional models for sound classification, and Arandjelovi 'c and Zisserman @cite train sound and vision representations jointly. In Moments in Time dataset, many videos have both visual and auditory signals, enabling for multi-modal video recognition.
- Traditionally, image-based search methods drew their inspiration from textual retrieval systems @cite . By using @math -means clustering method in the space of local feature descriptors such as SIFT @cite , they are able to mimic textual word entities with the so-called visual words . Once the mapping from image salient keypoints to visually representative words was established, typical textual retrieval methods such as Bag-of-Words @cite could be used. Video Google @cite was one of the first visual search engines that relied on this concept. Several extensions of this concept were proposed, e.g. spatial verification @cite that checks for geometrical correctness of initial query or fine-grained image search @cite that accounts for semantic attributes of visual words.
- Successful applications of deep learning techniques in other computer vision applications have motivated researchers to apply those methods also to visual search. Although preliminary results did not seem promising due to the lack of robustness to cropping, scaling and image clutter @cite , later works proved potential of those methods in the domain of image-based retrieval @cite . Many other deep architectures such as Siamese networks were also proposed, and proved successful when applied to content-based image retrieval @cite .
- Comparing the style similarity of two objects or scenes is one of the challenges that have to be addressed when training a machine learning model for interior design or fashion retrieval application. This problem is far from being solved mainly due to the lack of a clear metric defining how to measure style similarity. Various approaches have been proposed for defining style similarity metric. Some of them focus on evaluating similarity between shapes based on their structures @cite @cite and measuring the differences between scales and orientations of bounding boxes. Other approaches propose the structure-transcending style similarity that accounts for element similarity @cite . In this work, we follow @cite , and define style as We enforce this definition by including context information that groups different objects together (in terms of clothing items in an outfit or furniture in a room picture in interior design catalog). This allows us to a take data-driven approach that measures style similarity without using hand-crafted features and predefined styles.
- There has been a significant number of works published in the domain of fashion item retrieval or recommendation due to the potential of their application in highly profitable e-commerce business. Some of them focused on the notion of fashionability, e.g @cite rated a user's photo in terms of how fashionable it is and provided fashion recommendations that would increase overall outfit score. Others focused on fashion items retrieval from online database when presented with user photos taken 'in the wild' usually with phone cameras @cite . Finally, there is ongoing research in terms of clothing cosegmentation @cite @cite that is an important preprocessing step for better item retrieval results.
- @cite present an encoder-decoder pipeline that learns a joint multimodal embedding (VSE) from images and a text, which is later used to generate text captions for custom images. Their approach is inspired by successes in Neural Machine Translation (NMT) and perceives visual and textual modalities as the same concept described in different languages. The proposed architecture consists of LSTM RNNs for encoding sentences, CNN for encoding images and structure-content neural language model (SC-NLM) for decoding. The authors show that their learned multimodal embedding space preserves semantic regularities in terms of vector space arithmetic e.g. image of a blue car - "blue" + "red" is near images of red cars. However, results of this task are only available in some example images. We would like to leverage their work and numerically evaluate multimodal query retrieval, specifically in the domain of fashion and interior design.
- Xintong @cite train bi-LSTM model to predict next item in the outfit generation. Moreover, they learn a joint image-text embedding by regressing image features to their semantic representations aiming to inject attribute and category information as a regularization for training the LSTM. It should be noted, however, that their approach to stylistic compatibility is different from ours in a way that they optimize for generation of a complete outfit (e.g. it should not contain two pairs of shoes) whereas we would like to retrieve items of similar style regardless of the category they belong to. Also, they evaluate compatibility with "fill-in-the-blanks" test that does not incorporate retrieval from the full dataset of items. Only several example results are illustrated and no quantitative evaluation is presented.
- Load balancing mechanisms similar to the type considered here have been studied in many works. Specifically, the Join-the-Shortest-Queue (JSQ), Join-the-Idle-Queue (JIQ), and Power-of- @math (also known as the supermarket model) routing schemes have garnered quite a bit of attention (see @cite @cite @cite @cite @cite @cite @cite @cite @cite and references therein). The papers @cite @cite were the first to study the tail behavior of the fixed point of the ODE system associated with the Power-of- @math routing scheme, showing that in steady-state the fraction of queues with lengths exceeding @math decay super-exponentially in @math , a large improvement over the exponential rate for the setting where jobs are routed to servers uniformly at random. Later works on a similar theme include @cite @cite @cite . In all these works, the authors study fluid and diffusion approximations for various types of load balancing mechanisms. In each case, stable fixed points are identified for the LLN limit and the interchangeability of limit property is established.
- There exist many generative models for the problem of image generation @cite @cite @cite @cite @cite @cite @cite . Among them, GANs are conceptually closely related to our problem as they employ an adversarial loss that forces the generated images to be as photorealistic as the ground-truth images.
- Several methods adopt an adversarial training to learn a parametric translating function from a large-scale dataset of input-output pairs, such as super-resolution @cite @cite @cite @cite @cite and inpainting @cite . These approaches often use the @math or @math norm and adversarial losses to compare the generated image to the corresponding ground truth image. Although these methods produce impressive photorealistic images, they fail to preserve identities of subjects.
- Conditional GANs have been used for the task of generating photographs from sketches @cite , and from semantic layout and scene attributes @cite . Li and Wand @cite train a Markovian GAN for the style transfer -- a discriminative training is applied on Markovian neural patches to capture local style statistics. Isola @cite develop pix2pix'' framework which uses so-called Unet'' architecture and the patch-GAN to transfer low-level features from the input to the output domain. For faces, this approach produces visual artefacts and fails to capture the global structure of faces.
- Moreover, there exist several methods to synthesize sketches from photographs (and vice versa) @cite @cite @cite @cite . While sketch-to-face synthesis is a related problem, our unified framework can work with various more complex styles.
- Style transfer is a technique which can render a given content image (input) by incorporating a specific painting style while preserving the contents of input. We distinguish and style transfer methods. The seminal optimization-based work @cite transfers the style of an artistic image to a given photograph. It uses an iterative optimization to generate a target image which is randomly initialized (Gaussian distribution). During the optimization step, the statistics of the neural activations of the target, the content and style images are matched. The idea @cite inspired many follow-up studies. Yin @cite presents a content-aware style transfer method which initializes the optimization algorithm with a content image instead of a random noise. Li and Wand @cite propose a patch-based style transfer method by combining Markov Random Field (MRF) and CNN techniques. The work @cite proposes to transfer the style by using linear models. It preserves colors of content images by matching color histograms.
- Gatys @cite decompose styles into perceptual factors and then manipulate them for the style transfer. Selim @cite modify the content loss through a gain map for the head portrait painting transfer. Wilmot @cite use histogram-based losses in their objective and build on the Gatys 's algorithm @cite . Although the above optimization-based methods further improve the quality of style transfer, they are computationally expensive due to the iterative optimization procedure, thus limiting their practical use.
- To address the poor computational speed, feed-forward methods replace the original on-line iterative optimization step with training a feed-forward neural network off-line and generating stylized images on-line @cite @cite @cite .
- Johnson @cite train a generative network for a fast style transfer using perceptual loss functions. The architecture of their generator network follows the work @cite and also uses residual blocks. Another concurrent work @cite , named Texture Network, employs a multi-resolution architecture in the generator network. Ulyanov @cite @cite replace the spatial batch normalization with the instance normalization to achieve a faster convergence. Wang @cite enhance the granularity of the feed-forward style transfer with multimodal CNN which performs stylization hierarchically via multiple losses deployed across multiple scales.
- These feed-forward methods perform stylization @math 1000 times faster than the optimization-based methods. However, they cannot adapt to arbitrary styles that are not used for training. For synthesizing an image from a new style, the entire network needs retraining. To deal with such a restriction, a number of recent approaches encode multiple styles within a single feed-forward network @cite @cite @cite @cite .
- Dumoulin @cite use conditional instance normalization that learns normalization parameters for each style. Given feature activations of the content and style images, @cite replaces content features with the closest-matching style features patch-by-patch. Chen @cite present a network that learns a set of new filters for every new style. Li @cite also adapt a single feed-forward network via a texture controller module which forces the network towards synthesizing the desired style only. We note that the existing feed-forward approaches have to compromise between the generalization @cite @cite @cite and quality @cite @cite @cite .
- For Similarity measures in heterogeneous information networks, Sun @cite proposed a meta path based similarity measure in , called @math . Lao and Cohen @cite @cite studied the problem of measuring the entity similarity in labeled directed graphs, and defined a Biased Path Constrained Random Walk ( @math ) model. It can be applied to . @cite proposed a similarity @math , which can capture more complex semantics. @cite proposed a relevance measure @math which can be used to evaluate the relatedness of two object with different types. For a user-specified meta path, @math is based on the pairwise random walk from its two endpoints to its center. @cite studied the problem of finding the @math similar object pairs by virtue of locality sensitive hashing. @cite proposed an integrated framework for the development, evaluation and application of semantic similarity for knowledge graphs which can be viewed as complicated heterogeneous information networks. This framework included many similarity tools and allowed users to compute semantic similarities. In the article @cite , the authors studied the similarity search problem in social and knowledge networks, and proposed a dual perspective similarity metric called Forward Backward Similarity.
- Despite the numerous steps of progress that brought us to an almost complete understanding of the approximate case of SSSP, computing exact SSSP remained open until very recently. But then came the breakthrough of Elkin @cite , which provided the first sublinear-time algorithm for exact SSSP. More precisely, Elkin gave an algorithm that computes exact SSSP in @math rounds if @math , and in @math rounds if @math .
- In this paper, we present algorithms that improve on this state of the art both quantitatively and qualitatively for integer-weighted graphs. We will soon discuss the quantitative aspect, i.e., the improvements in the bounds. The qualitative aspect is the fact that our algorithms extend to networks with asymmetric weights, i.e., when the two directions of an edge can have different weights. Although this asymmetric case is relevant for practical networks, it has gained a far more important motivation recently even when working on graphs with symmetric weights, due to the introduction of the @cite to distributed computing. The scaling framework massages the weights in a way that simplifies the problem considerably, modulo the potential of making the weights asymmetric. We will discuss this framework in the next section.
- We next overview our round complexity improvements. Our basic result gives an SSSP algorithm with complexity @math , for graphs with polylogarithmic diameter @math . This improves on the @math bound of Elkin @cite , and gets closer to the @math lower bound of Peleg and Rubinovich @cite , which holds for graphs of diameter @math . More generally, the result is as follows:
- We also show in section:MultipleSources how to extend our algorithm to the case with @math -sources, providing some improvements on the results of @cite for small @math and @math .
- Partial Transfer Learning : Partial transfer learning was proposed in @cite , where the target domain label space is a subspace of the source domain label space. Because the extra source classes might cause negative transfer when classifying the target domain, it makes the domain adaptation problem more challenging. In this work, in order to solve the partial transfer problem, instead of using single-discriminator domain adversarial network, the authors proposed to use multi-discriminator domain adversarial network, each discriminator is responsible for matching the source and target domain data associated with each label.
- Many researches @cite @cite @cite @cite have focused on event evolution analysis for a single medium. The event popularity was evaluated by hourly page view statistics from Wikipedia in @cite . Reference @cite chose the density-based clustering method to group the posts in social text streams into events and tracked the evolution patterns. Breaking news dissemination is studied via network theory based propagation behaviors in @cite . Reference @cite proposed a TF-IDF based approach to analyze event popularity trends. In all, network-based approaches usually have high computational complexity, while frequency-based methods are usually less accurate on reflecting the real event popularity. Reference @cite proposed a keyword extraction method for tweet message collections. However, it missed some valuable semantic information, which could have contributed to better results. Reference @cite presented a topic labeling model by learning semantic representations of topic words and clustering coherent topic labels. A context-sensitive method based on PageRank to extract topical keyphrases was proposed in @cite . Their work sheds light on integrating lexical, contextual and semantic relations in our TF-SW model.
- From a cross-platform perspective, existing researches focus on topic detection, cross-social media user identification, cross-domain information recommendation, etc. Reference @cite selected Twitter, and Flickr to represent multimedia streams, and provided an emerging topic detection method. An attempt, trying to combine Twitter and Wikipedia to do first story detection, was discussed in @cite . Reference @cite proposed an algorithm based on multiple social networks like Twitter, Weibo, and Facebook to identify anonymous identical users. The relationship between social trends from social network and web trends from search engine are discussed in @cite @cite . Recently, a good prediction of social links between users from aligned networks using sparse and low rank matrix is well discussed in @cite . However, few studies have been conducted for event popularity analysis from cross-platform perspective.
- There have been many attempts that use virtual world to carry out scientific researches. Bainbridge @cite investigated the feasibility of utilizing Second life and World of Warcraft as sites for research in the social, behavioral, and economic sciences, as well as computer science. With a virtual living lab, Prendinger @cite conducted several controlled driving and travel studies.
- Bilinear models were first analyzed by Tenenbaum and Freeman @cite to manipulate two factors from images, style and content. Recently bilinear models have achieved success in multiple tasks. Lin al @cite fused two convolutional neural networks to obtain orderless descriptors and improved results in fine-grained visual recognition. Carreira al @cite used second order statistics of the local descriptors for semantic segmentation. RoyChowdhury al @cite used bilinear CNNs to improve results in face identification tasks. Gao al @cite improves the bilinear methods by developing a compact pooling method.
- of the previous works for CNN acceleration focus on approximating the convolution filters by low-rank decomposition @cite @cite @cite @cite . As a pioneering work, @cite approximate the filters of a pre-trained CNNs with a linear combination of low-rank filters. @cite devise a basis of low-rank filters that are separable in the spatial domain and further develop two different schemes to learn these filters, i.e., Filter reconstruction" that minimizes the error of filter weights and Data reconstruction" that minimizes the error of the output responses. @cite adopt a two-stage method that first approximates the convolution kernels using the low-rank CP-decomposition, and then fine-tunes the amended CNN.
- and activation quantization are widely used for network compression and acceleration. As a representative work, XNOR-Net @cite binarizes the input to convolutional layers and filter weights, and approximates convolutions using primarily binary operations, resulting in significant speed-up but an evident drop in performance. @cite further introduce Half-Wave Gaussian Quantization to improve the performance of this method. On the other hand, pruning the unimportant connections or filters can also compress and accelerate deep networks. @cite remove the connections with weights below a threshold, reducing the parameters by up to 13 @math . This method is further combined with weight quantization to achieve an even higher compression rate. Similarly, @cite measure the importance of a filter by calculating its absolute weight sum and remove the filters with small sum values. @cite employ the Taylor expansion to approximate the change in the cost function induced by pruning filters. @cite further formulate filter pruning as an optimization problem.
- works explore more optimal network structures for efficient training and inference. @cite develop a low-dimensional embedding method to reduce the number and size of the filters. @cite show that stacked filters with small spatial dimensions (e.g., @math ) could operate in the same receptive field of larger filters (e.g., @math ) with less computational complexity. @cite further replace some @math filters with @math filters, and decrease the number of input channels to @math filters to simultaneously speed up and compress the deep networks.
- In @cite , the authors study (which are nothing more than maps @math except that @math and @math are surfaces, i.e. Riemannian manifolds of dimension @math ) and define a Dirichlet energy in the same way as Koorevaar, Schoen and Jost. These maps are seen as relaxations of "classical" maps @math , and they focus on numerical computation and visualization of theses soft maps, see also @cite for applications to supervised learning. On the other hand, they do not analyze in detail the theoretical properties of the Dirichlet energy and harmonic mappings, which in contrast is the main topic of the present article. In @cite , the author provides some theoretical analysis of soft maps by focusing on the cases where the boundary measures on @math are either Dirac masses or Gaussian measures.
- Finally, in @cite the authors also study mappings valued in the space of probability measures, but are rather interested in the bounded variation norm (the integral of the norm of the gradient) than in the Dirichlet energy. Their provide applications to the denoising of measure-valued images.
- Many algorithms have been proposed over the years for exposure fusion. However, the main idea remains the same in all the algorithms. The algorithms compute the weights for each image either locally or pixel wise. The fused image would then be the weighted sum of the images in the input sequence. Burt @cite performed a Laplacian pyramid decomposition of the image and the weights are computed using local energy and correlation between the pyramids. Use of Laplacian pyramids reduces the chance of unnecessary artifacts. Goshtasby @cite take non-overlapping blocks with highest information from each image to obtain the fused result. This is prone to suffer from block artifacts. Mertens @cite perform exposure fusion using simple quality metrics such as contrast and saturation. However, this suffers from hallucinated edges and mismatched color artifacts.
- Metric learning has been successfully used to recognize faces of new identities @cite @cite and fine-grained objects @cite @cite @cite @cite @cite . The idea is to learn a mapping from inputs to vectors in an embedding space where the inputs of the same identity or category are closer than those of different identities or categories. Once the mapping is learned, at test time a nearest neighbors method can be used for retrieval and classification for new categories that are unseen during training.
- Contrastive loss @cite minimizes the distances between inputs with the same label while keeping the distances between inputs with different labels far apart. Rather than minimizing absolute distances, recent approaches formulate objectives focusing on relative distances. FaceNet @cite optimizes a triplet loss and develops an online negative mining strategy to form triplets within a mini-batch. Instead of penalizing violating instance-based triplets independently, alternative loss functions regulate the global structure of the embedding space. Magnet loss @cite optimizes the distribution of different classes by clustering the examples using @math -means and representing classes with centroids. Lifted structured loss @cite incorporates all pair-wise relations within a mini-batch instead of forming triplets. The @math -pair loss @cite requires each batch to have examples from @math categories for improved computational efficiency. All these methods require some online or offline batch generation step to form informative batches to speed up training. Structured clustering loss @cite optimizes a clustering quality metric globally in the embeddings space.
- The Proxy-NCA loss @cite demonstrates faster convergence without requiring batch generation by assigning trainable proxies to each category, which we will describe in more detail in Section . NormFace @cite explores a similar idea with all feature vectors normalized. The embedding can generalize to unseen categories, however the nearest neighbor model needs to store the embeddings of all reference points during testing. In our work, we retain the parametric form of ConvNet models and demonstrate that semantic embeddings can be used to imprint weights in the final layer. As a result, our approach has the same convergence advantages as @cite and @cite during training, yet does not require storing embeddings for each training example or using nearest-neighbor search during inference.
- In the previous works on dialog systems there was not enough attention paid to the echoing problem. The possible reason for this are soft'' evaluation conditions: test samples are constructed from a relatively small number of negative responses @cite @cite @cite which usually do not echo'' the test context. In @cite the lexical repetition'' is regularized by utilizing a word overlap feature during training a SMT-based dialog system. In @cite @cite @cite the echoing is avoided by considering only responses the dataset's contexts of which have high TF-IDF similarity with the given context. However, the latter approach is not applicable if only a set of responses is available for ranking during the testing stage, which can be the case for some domains and applications @cite .
- A large number of objective IMs have emerged as a result of the application of ARM across different domains. It is also documented that not all measures are capable of capturing the strength of associations and in some cases provide conflicting information of the strength of patterns @cite . Given the abundance of measures and difficulty in choosing the appropriate IM, researchers have suggested various classification schemes (of the IMs) to help identify the appropriate measure for a given application @cite , @cite , @cite , @cite , @cite , @cite . There are two different types of classification that exist in literature: classification based on the properties of IMs (e.g. @cite , @cite , @cite , @cite ) and classification based on empirical results of IMs on different datasets (e.g. @cite ).
- @cite proposed the following 5 properties in addition to the 3 proposed by @cite : symmetry under variable permutation (O1), row column scaling invariance (O2), anti-symmetry under row column permutation (O3), inversion invariance (O4) and null invariance (O5). They conducted a comparative study, testing 21 different IMs against the resulting 8 properties. The authors further proposed that the optimal way of finding a suitable IM would be to let the user define a property vector indicating the properties that would be ideally required for the given application. This property vector would then be compared to the property vectors of the different objective measure to pick out the ideal interestingness measure for that particular case. For instance, the null-invariance property is considered to be important for interestingness measures used in the context of small probability events in a large dataset @cite . While there has been further work in introducing new properties (e.g., @cite , @cite , @cite , @cite , @cite ), these have not been as commonly used or cited as the work of @cite and @cite .
- There has been limited work on classification of IMs based on empirical results on different datasets. Research by @cite proposed the classification of 35 different interestingness measures based on their empirical performance on 2 different datasets by studying the correlation of the interestingness measures. These measures were classified using a graph based clustering approach to create high correlation and low-correlation graphs. The work of @cite performed a comprehensive classification of 61 different objective IMs on the based on empirical results on 110 different datasets. It suggested that there exist 21 clusters of measures which are distinct and each of these clusters were studied in detail.
- Ensembling @cite multiple models has been a successful method for improving accuracy. Model compression @cite is proposed to improve test-time efficiency of ensembling by compressing an ensemble of models into a single student model. This method is extended in knowledge distillation @cite , which uses soft predictions as the student's target.
- The idea of distillation has been adopted in various scenarios. FitNet @cite adopts a shallow and wide teacher models to train a deep and thin student model. Cross modal distillation @cite is proposed to address the problem of limited labels in a certain modality. In @cite distillation is unified with privileged information @cite . To avoid explicitly training multiple models, Laine and Aila @cite exploit multiple checkpoints during training to generate the ensemble predictions. Following the success of these existing works, our approach distills knowledge from a lightweight ensemble formed by multiple data transformations.
- There is a great volume of work on semi-supervised learning, and comprehensive surveys can be found in @cite @cite @cite . Among semi-supervised methods, our method is most related to self-training, a strategy in which a model's predictions on unlabeled data are used to train itself @cite @cite @cite @cite @cite @cite @cite @cite . Closely related to our work on keypoint object detection, Rosenberg al @cite demonstrate that self-training can be used for training object detectors. Compared to prior efforts, our method is substantially simpler. Once the predicted annotations are generated, our method leverages them as if they were true labels; it does not require any modifications to the optimization problem or model structure.
- Multiple views or perturbations of the data can provide useful signal for semi-supervised learning. In the co-training framework @cite , different views of the data are used to learn two distinct classifiers that are then used to train one another over unlabeled data. Reed al @cite use a reconstruction consistency term for training classification and detection models. Bachman al @cite employ the pseudo-ensemble regularization term to train models robust on input perturbations. Sajjadi al @cite enforce consistency between outputs computed for different transformations of input examples. Simon al @cite utilize multi-view geometry to generate hand keypoint labels from multiple cameras and retrain the detector. In an auto-encoder scenario, Hinton al @cite propose to use multiple capsules'' to model multiple geometric transformations. Our method is also based on multiple geometric transformations, but it does not require to modify network structures or impose consistency by adding any extra loss terms.
- Regarding the large-scale regime, Fergus al @cite investigate semi-supervised learning on 80 millions tiny images. A Never Ending Image Learner (NEIL) @cite employs self-training to perform semi-supervised learning from web-scale image data. These methods were developed before the recent renaissance of deep learning. In contrast, our method is evaluated with strong deep neural network baselines, and can be applied to structured prediction problems beyond image-level classification ( , keypoints and boxes).
- Traditionally, single-image shadow detection is done by exploiting physical models of illumination and color @cite @cite @cite . This approach, however, tends to produce satisfactory results only for wide dynamic range images @cite @cite . Another approach learns shadow properties using hand-crafted features based on annotated shadow images. It first describes image regions by feature descriptors and then classifies the regions into shadow and non-shadow regions. Features like color @cite @cite @cite , texture @cite @cite @cite , edge @cite @cite @cite and T-junction @cite are commonly used for shadow detection followed by classifiers like decision tree @cite @cite and SVM @cite @cite @cite . However, since hand-crafted features have limited capability in describing shadows, this approach often fails for complex cases.
- Convolutional neural network (CNN) is recently demonstrated to be a very powerful tool to learn features for detecting shadows, with results clearly outperforming previous approaches. @cite used multiple CNNs to learn features in super pixels and along object boundaries, and fed the output features to a conditional random field to locate shadows. @cite presented a deep structured shadow edge detector and employed structured labels to improve the local consistency of the predicted shadow map. @cite trained stacked-CNN using a large data set with noisy annotations. They minimized the sum of squared leave-one-out errors for image clusters to recover the annotations, and trained two CNNs to detect shadows.
- Very recently, @cite detected shadows using a patch-level CNN and a shadow prior map generated from hand-crafted features, while @cite developed scGAN with a sensitivity parameter to adjust weights in the loss functions. Although the shadow detection accuracy keeps improving on the benchmark datasets @cite @cite , existing methods may still misrecognize black objects as shadows and miss unobvious shadows in the testing images. The most recent work by @cite emphasized the importance of reasoning global semantics for shadow detection. Compared to this work, we suggest to consider the directional variance when analyzing the spatial context. Results show that our method can further outperform @cite in terms of both the accuracy and the BER value.
- DeepMind Technologies has successfully learned to play Atari 2600 games using an emulator @cite . DeepMind used a convolutional neural network combined with a Q-learning variant to learn to play games using just the games' pixel output. We plan to build on this success by focusing on a relatively newer game with more complex interactions. Instead of pixels, we look at the game's internal memory, including features such as characters' positions and velocities.
- The work most relevant to this project is that of @cite , which applies Reinforcement Learning to create an agent for Melee. Utilizing an actor-critic model with a deep architecture and self-play, they were able to achieve excellent results with the Captain Falcon character on the Battlefield stage, going on to even beat a number of professional players. Their agent is however known to have difficulties dealing with unusual opponent behavior that has not been seen in training, such as the opponent simply crouching.
- There have been more recent papers that build on the DeepMind's earlier success using variations of the regular DQN architecture. One of these is the Double DQN introduced by @cite . This architecture uses two networks playing alternating roles in order to reduce the tendency of the regular DQN to over-estimate action values. Another recent success was the Dueling DQN of @cite . This uses two parallel networks that estimate the value function and advantage function, and finally combine these to estimate the Q-value functions.
- Another class of algorithms that has gained popularity in the era of Deep Reinforcement Learning is the policy gradient method, such as the actor-critic algorithm. In particular, the Asynchronous Advantage Actor Critic method (A3C) introduced in @cite has become one of the standard algorithms for distributed learning in many game-playing applications of reinforcement learning.
- General purpose applications are more frequently relying on the second approach, based on the Wagner-Fischer's algorithm. In @cite the authors adopt the plain Wagner-Fischer's algorithm, along with an improved version that better exploits each cell. In @cite the Wagner-Fischer's algorithm has been applied to multimedia information retrieval, also considering text search paradigms such as wildcards and idioms. In @cite the authors provide an FPGA implementation of the Wagner-Fischer's algorithm particularly suited for dealing with regular expressions.
- FPGA implementations of systolic architectures have been proposed for ASM @cite @cite and, in bioinformatics, for DNA sequence alignment @cite @cite @cite . In the field of Music Information Retrieval (MIR), circuits implementing ASM with dynamic programming cannot retrieve fragments with different sizes @cite and other architectures with higher flexibility only support symbol substitution @cite . Better performance was achieved by Application-Specific Integrated Circuit and FPGA with a comparable computational time on different MIR approaches @cite @cite @cite @cite . An efficient FPGA implementation of ASM has been proposed for text mining, where a restricted class of regular expressions is used to define the patterns to search @cite . A work closely related to this paper is a successful implementation of online ASM on FPGA for NIDS applications @cite .
- Server to server data transfers in industry environments have been evaluated in @cite . The proponents have analyzed what benchmarks should be followed when transferring data from one server to another at an industry level. They have evaluated and emphasized the importance of logging and time-stamping the transfer activity at every stage of the transfer for security and auditing purpose. Additionally, the authors provided a mechanism for identifying nearest server when the transfer needs to be speed effective. However, besides security and speed, how to support multi-protocol data transfers and design criteria for such a distributed data delivery system were not discussed. Also the overhead of time-stamping at every stage have been ignored.
- Use of cloud platform to provide data transfer as a service to scientists with increased performance in terms of speed and security has been addressed in @cite which identifies Globus as a probable solution. Recent developments have focused on secured data transmission using Globus with transfer optimization using static parameters @cite @cite . The authors showed results of data transfers using static parameters over the network using REST APIs, however Globus does not employ dynamic throughput optimization or interprotocol translation (at the time of the writing of this article).
- Recently, researchers are devoted to combining data mining techniques with recruitment market analysis, including offer categorization @cite , talent career path analysis @cite , market trend analysis @cite @cite , and talent circles @cite . However, few of them studied the problem of measuring the popularity of job skill in recruitment market, not to mention the multi-faceted popularity ranking, which is the focus of this paper. .
- In the past years, a number of variations of the classical Latent Dirichlet Allocation (LDA) @cite have been proposed for solving different kinds of problems. To utilize the existing document labels, supervised LDA models @cite have shown the effectiveness on classification and regression tasks, while they are limited to assign one topic for each document. To this end, Labeled-LDA @cite has emancipated this limitation by allowing multiple topics for a single document, thus outperforms the previous supervised models. However, the above models cannot take the hierarchical information of words into account. Therefore, some hierarchical topic models have been proposed, such as Hierarchical LDA @cite , and LDAC (Latent Dirichlet Allocation on Context) @cite .
- While explicit PIR schemes which achieve capacity are constructed in @cite , @cite , and @cite , they require the base field to be large. If @math is the number of servers and @math is the number of files, the capacity-achieving schemes of @cite require a field size of @math , since they rely on the existence of MDS codes of high lengths. Realistic storage systems, however, may operate over fields of small size to keep the complexity of the involved operations manageable. One would naturally then like to construct explicit PIR schemes over small base fields.
- In this work we construct PIR schemes based on general linear codes, and concentrate in particular on binary Reed--Muller (RM) codes. The schemes described in @cite employed Generalized Reed-Solomon (GRS) codes, and the resulting analysis of the achievable rate relied on the @math of two GRS codes @math and @math again being a GRS code. The class of RM codes is closed under the star product operation as well, and thus naturally lends itself to be employed using the PIR scheme of @cite . However, RM codes have the advantage of being defined over the binary field @math . When comparing GRS and RM codes of equal length and dimension, it is shown here that the same PIR rates as with GRS codes can be achieved in the non-colluding case. For a fixed PIR rate, however, RM codes provide less protection against collusion due to their lower minimum distance. Nevertheless, it is shown that the @math -PIR RM schemes presented here still provide protection against a substantial fraction of colluding sets of sizes slightly bigger than @math . In more detail, the main contributions of this paper are:
- RM codes have previously been considered for PIR in other settings @cite . There, the system model is different from the present paper, in that the coding is between different files and the primary goal is to minimize storage overhead for a given PIR scheme, along the same lines as in @cite . In our work, coding is between different blocks of the same file, and the goal is to minimize the download overhead for fixed storage codes.
- Training CNN for DA can be realized through various strategies. Ghifary al proposed using an autoencoder for the target domain to obtain domain-invariant features @cite . Sener al proposed using clustering techniques and pseudo-labels to obtain discriminative features @cite . Taigman al proposed cross-domain image translation methods @cite . Matching distributions of the middle features in CNN is considered to be effective in realizing an accurate adaptation. To this end, numerous methods have been proposed @cite @cite @cite @cite @cite @cite .
- Consensus regularization is a technique used in multi-source domain adaptation and multi-view learning, in which multiple classifiers are trained to maximize the consensus of their outputs @cite . In our method, we address a training step that minimizes the consensus of two classifiers, which is totally different from consensus regularization. Consensus regularization utilizes samples of multi-source domains to construct different classifiers as in @cite . In order to construct different classifiers, it relies on the different characteristics of samples in different source domains. By contrast, our method can construct different classifiers from only one source domain.
- Optical Character Recognition (OCR) has been a very active field for many years. More specifically, many applications addressing printed characters recognition already achieve good performance levels. Tesseract @cite , ABBYY FineReader https: www.abbyy.com en-eu are widely used. In our specific application, they do not achieve perfect text recognition. Recently, the Google Vision API https: cloud.google.com vision proved to be one of the most efficient OCR but remains as a blackbox and does not ensure all the text to be recognized. We however use this as the best compromise. In addition, since the focus of this paper relates to the first steps of the processing chain, we do not need to go further into the details of the OCR topic and use it as a black-box tool.
- In the past few years, researchers have designed various algorithms for person re-identification. These algorithms mainly focus on two aspects: feature representations @cite @cite @cite and metric learning @cite @cite @cite @cite @cite . Gray and Tao @cite learned viewpoint invariant features for pedestrian recognition by combining spatial and color information. @cite extracted the texture histograms by studying the perceptual principles of symmetry and asymmetry. @cite proposed a novel intradistribution structure based on the color distributions, which can learn the illumination invariant features under a variety of imaging conditions. @cite adopted the idea of large margin nearest neighbor metric (LMNN) to gather the @math -nearest neighbors and separate examples from different classes by a large margin. @cite utilized relative distance comparison (RDC) to maximize the likelihood of a pair of true matches. @cite employed an effective architecture called Local Maximal Occurrence (LOMO) to learn a stable representation against viewpoint changes by maximizing the occurrence.
- The problem of decomposing shape, reflectance and illuminance from a single image is a classical problem in computer vision and has been studied in various forms such as intrinsic image decomposition @cite and Shape from Shading (SfS) @cite @cite . Recent work of SIRFS @cite performs decomposition of an object into surface normal, albedo and lighting assuming lambertian reflection by formulating extensive priors in an optimization framework. The problem of inverse rendering in the form of SfS gained particular attention in the domain of human facial modeling. This research was precipitated by the advent of the 3D Morphable Model (3DMM) @cite as a potential prior for shape and reflectance. Recent works used facial priors to reconstruct shape from a single image @cite @cite @cite @cite or from multiple images @cite . Classical SfS methods fail to produce realistic decomposition on unconstrained real images. More recently, Saito al proposes a method to synthesize a photorealistic albedo from a partial albedo obtained by traditional methods @cite .
- In recent years, researchers have focused on data driven approaches for learning priors rather than hand-designing them for the purpose of inverse rendering. Attempts at learning such priors were presented in @cite using Deep Belief Nets and in @cite using a convolutional encoder-decoder based network. However these early works were limited in their performance on real world unconstrained faces. Recent work from Shu al @cite aims to find a meaningful latent space for normals, albedo and lighting to facilitate various editing of faces. Tewari al @cite solves this facial disentanglement problem by fitting a 3DMM for shape and reflectance and regressing illumination coefficients. Both @cite @cite learn directly from real world faces by using convolutional encoder-decoder based architectures. Decompositions produced by @cite are often not realistic; and @cite only captures low frequency variations. In contrast, our method learns from a mixture of labeled synthetic and unlabeled real world faces using a novel decomposition architecture. Although our work concentrates on decomposing faces, the problem of inverse rendering for generic objects in a learning based framework has also gained attention in recent years @cite @cite @cite @cite .
- Another direction of research is to estimate shape or illumination of a face independently. Recently many research works aim to reconstruct the shape of real world faces by learning from synthetic data; by fitting a 3DMM @cite @cite @cite , by predicting a depth map and subsequent non-rigid deformation to obtain a mesh @cite and by regressing a normal map @cite . Similarly @cite proposed a method to estimate lighting directly from a face. These learning based independent component estimation methods can not be trained with unlabeled real world data and thus suffer from the ability to handle unseen face modalities. In contrast our joint estimation approach performs the complete decomposition while allowing us to train on unlabeled real world images using our SfS-supervision'.
- In @cite , a convolutional auto-encoder was used for disentanglement and generating normal and albedo images. However recent advances in skip-connection based convolutional encoder-decoder architectures for image to image translations @cite @cite @cite have also motivated their use in @cite . Even though skip connection based architectures are successful in transferring high frequency informations from input to output, they fail to produce meaningful disentanglement of both low and high frequencies. Our proposed decomposition architecture uses residual block based connections that allow the flow of high frequency information from input to output while each layer learns both high and low frequency features. A residual block based architecture was used for image to image translation in @cite for style transfer and in a completely different domain to learn a latent subspace with Generative Adversarial Networks @cite .
- (VQG) was recently proposed as an alternative to image captioning . Our work is related to VQG in the sense that we require the learner to generate questions about images, however, our objective in doing so is different. Whereas VQG focuses on asking questions that are relevant to the image content, LBA requires the learner to ask questions that are both relevant and informative to the learner when answered. A positive side effect is that LBA circumvents the difficulty of evaluating the quality of generated questions (which also hampers image captioning ), because the question-answering accuracy of our final model directly correlates with the quality of the questions asked. Such evaluation has also been used in recent works in the language community @cite @cite .
- (AL) involves a collection of unlabeled examples and a learner that selects which samples will be labeled by an oracle . Common selection criteria include entropy @cite , boosting the margin for classifiers @cite @cite and expected informativeness @cite . Our setting is different from traditional AL settings in multiple ways. First, unlike AL where an agent selects the image to be labeled, in LBA the agent selects an image and . Second, instead of asking for a single image level label, our setting allows for richer questions about objects, relationships for a single image. While @cite did use simple predefined template questions for AL, templates offer limited expressiveness and a rigid query structure. In our approach, questions are generated by a learned language model. Expressive language models, like those used in our work, are likely necessary for generalizing to real-world settings. However, they also introduce a new challenge: there are many ways to generate invalid questions, which the learner must learn to discard (see Figure ).
- centers on settings in which an agent explores the environment to acquire supervision ; it has been studied in the context of, among others, computer games and navigation , multi-user games , inverse kinematics @cite , and motion planning for humanoids . Exploratory learning problems are generally framed with reinforcement learning in which the agent receives (delayed) rewards, which are used to learn a policy that maximizes the expected rewards. A key difference in the LBA setting is that it does have sparse delayed rewards. Contextual multi-armed bandits are another class of reinforcement learning algorithms that more closely resemble the LBA setting. However, unlike bandits, online performance is irrelevant in our setting: our aim is not to minimize regret, but to minimize the error of the final VQA model produced by the learner.
- is one of a number of similar infinite-state inductive model checkers including Kind 2 @cite , @cite , with generalized PDR @cite , and @cite . They operate over a transition relation described either as a program ( 2, , and ), an extension of the SMV language ( ), or as a set of Horn clauses ( ). Each tool uses a portfolio-based solver approach, with , , and 2 all supporting both @math -induction and a variant of PDR IC3. also supports guided reachability and @math -liveness. Other tools such as ESBMC-DepthK @cite , VVT @cite CPAchecker , @cite , CPROVER @cite use similar techniques for reasoning about C programs.
- There are several tools that support reuse or exchange of verification results, similar to our advice feature. Recently, there has been progress on standardized formats @cite of exchange between analysis tools. Our current advice format is optimized for use and performance with our particular tool and designed for re-verification rather than exchange of partial verification information. However, supporting a standardized format for exchanging verification information would be a useful feature for future use.
- Although there are extensive studies about AG methods for convex optimization, the analysis of AG for non-convex optimization is still limited. @cite analyzed a variant of AG for minimizing non-convex smooth functions. However, its rate of convergence to a critical point is the same as standard GD method. @cite analyzed variants of accelerated proximal gradient (APG) methods for minimizing a family of non-convex functions consisting of a smooth component and a non-smooth component. For general functions in this family, they only proved the asymptotic convergence to a critical point. Asymptotic results with explicit convergence rates are established for functions that satisfy the Kurdyka - Lojasiewicz (KL) property. @cite analyzed two variants of AG methods in a stochastic setting for non-convex optimization under a unified framework. Again, their convergence analysis are only for finding critical points and the convergence rates of the analyzed two AG methods in the stochastic setting is the same as stochastic gradient method.
- A fundamental concern in the design of non-convex optimization algorithms for finding a SSP is how to escape from saddle points. The proposed first-order method dubbed addresses this concern by extracting the negative curvature from a Hessian matrix with negative eigen-values. It is inspired by a recent work , which is the first work that develops a first-order method (named ) for extracting negative curvature from a Hessian matrix with negative eigen-values. Their method is a gradient descent method and suffers from an iteration complexity of @math for finding a negative curvature for a Hessian matrix whose minimum eigen-value is less than @math . In contrast, the proposed @math is based on PHB or NAG and improves the iteration complexity of to @math . By utilizing @math in the framework developed by @cite , we obtain an AG algorithm for finding an @math -SSP with an iteration complexity of @math . As a byproduct, can be also leveraged in stochastic non-convex optimization to accelerate the convergence for finding a SSP.
- The work on the adoption of digital financial services has been dominated by macroeconomic work related to regulatory issues around interoperability and the logistics of mobile money agents @cite @cite and a very few research papers have explored the adoption of mobile money from a quantitative perspective. However, mining of insights from mobile communication meta-data has been a popular area of research and some examples of work in this area include predicting the socioeconomic status @cite , gender @cite of mobile phone subscribers, customer churn behavior @cite and analysis of gender disparities using social networks extracted from mobile communication logs @cite .
- We have used deterministic finite automata (DFA) based feature engineering over CDR data which is quite similar to the approach used in @cite and @cite . Lastly, the differences in the usage of technology across men and women, poor & rich, and the urban & rural population has been a popular theme of research in the ICTD domain @cite , @cite ,etc. In comparison to these studies, our work employs a more comprehensive feature engineering approach over a bigger dataset to evaluate the role of the different type of features in the adoption of mobile money for people with different demographic backgrounds.
- A number of aspects of privacy protection has been studied in the biometric literature @cite @cite @cite @cite . On one hand, there are face de-identification techniques @cite @cite @cite where a face image is modified in order to confound a face matcher. On the other hand, as inspired by the work of Othman and Ross @cite and later promoted by Sim and Zhang @cite , the goal is to selectively confound or preserve a set of attributes that can be deduced from face images. Specifically, a few methods for suppressing the gender attribute have been presented @cite @cite @cite . Recently, a new method for protecting privacy with practical applications for biometric databases was proposed in @cite , where input face images were modified with respect to a specific gender classifier. In this case, perturbations were derived based on a specific gender classifier, the perturbations did not significantly impact the match scores of a face matcher.
- He et al. @cite @cite proposed ResNets to combat the vanishing gradient problem during training very deep convolutional networks. A ResNet is usually composed of multiple stacked residual units, each of which either directly passes through or makes some modifications to its input. ResNets have outperformed previous state-of-the-art models in various tasks, such as object detection @cite and semantic image segmentation @cite . They have gradually replaced VGGNets @cite in the computer vision community, as the standard feature extractors. Nevertheless, note that the real mechanism underpinning the effectiveness of ResNets is not yet clear. Veit et al. @cite claimed that they behave like ensembles of relatively shallow networks. On the other hand, Greff et al. @cite interpreted a ResNet as learning unrolled iterative estimation. They claimed that the first residual unit would learn a coarse estimation, while the following units would learn to refine this estimation iteratively.
- Most of the recent state-of-the-art approaches to semantic image segmentation are based on FCNs, which were proposed by Long et al. @cite . FCNs soon became the mainstream approache to dense prediction tasks, such as depth estimation and image super-resolution, primarily due to their efficiency. On top of FCNs, Chen et al. @cite fused the score maps obtained from two versions of an input image with different sizes using scale-aware weights. Empirical results in the literature @cite also show that stronger pre-trained features (using ResNets instead of VGGNets as the backbone networks) can further improve FCN performance.
- Blur carries information about the object's distance. Depth from Focus Defocus (DfF DfD) recovers scene depth from a collection of images captured under varying focus settings. In general, DfF @cite @cite @cite determines the depth by analyzing the most in-focus slice in the focal stack, while DfD @cite @cite infers depth based on the amount of the spatially varying blur at each pixel. To avoid ambiguity in textureless region, Moreno-Noguer @cite used active illumination to project a sparse set of dots onto the scene. The defocus of the dots offers depth cue, which could be further used for realistic refocusing. @cite combined focal stack with varying aperture to recover scene geometry. Moeller @cite applied an efficient nonconvex minimization technique to solve DfD in a variational framework. Suwajanakorn @cite proposed the DfF with mobile phone under uncalibrated setting. They first aligned the focal stack, then jointly optimized the camera parameters and depth map, and further refined the depth map using anisotropic regularization.
- Deep learning benefits stereo matching at various stages. A number of approaches exploit CNN to improve the matching cost. The seminal work by Z bontar and LeCun @cite computed a similarity score from patches using CNN, then applied the traditional cost aggregation and optimization to solve the energy function. Han @cite jointly learned feature representations and feature comparison functions in a unified network, which improved on previous results with less storage requirement. Luo @cite speeded up the matching process by using a product layer, and treated the disparity estimation as a multi-class classification problem. @cite @cite @cite @cite conducted similar work but with different network architecture. Alternatively, CNN can also help predict the confidence of disparity map to remove outliers. Seki and Pollefeys @cite leveraged CNN for stereo confidence measure, and incorporated predicted confidence into Semi-Global Matching by adjusting its parameters. In order to automatically generate the dataset for learning based confidence measure, Mostegel @cite checked the consistency of multiple depth maps of the same scene obtained with the same stereo approach, and collected labeled confidence map as the training data.
- Network embedding aims to learn representations for vertexes in a given network. Some researchers regard network embedding as part of dimensionality reduction techniques. For example, Laplacian Eigenmaps (LE) @cite aims to learn the vertex representation to expand the manifold where the data lie. As a variant of LE, Locality Preserving Projections (LPP) @cite learns a linear projection from feature space to embedding space. Besides, there are other linear @cite and non-linear @cite network embedding algorithms for dimensionality reduction. Recent network embedding works take advancements in natural language processing, most notably models known as word2vec @cite , which learns the distributed representations of words. Building on word2vec, define a vertex's context'' by their co-occurrence in a random walk path @cite . More recently, propose a mixture of width-first and breadth-first search based procedure to generate paths of vertexes @cite . further develop the model to handle heterogeneous networks @cite . LINE @cite decomposes a vertex's context into first-order (neighbors) and second-order (two-degree neighbors) proximity. preserve community information in their vertex representations @cite . However, all of above methods focus on preserving microscopic network structure and ignore macroscopic scale-free property of networks.
- Scale-free Networks. The scale-free property has been discovered to be ubiquitous over a variety of network systems @cite @cite @cite , such as the Internet Autonomous System graph @cite , the Internet router graph @cite , the degree distributions of subsets of the world wide web @cite . newman2005power provides a comprehensive list of such work @cite . However, investigating the scale-free property in a low-dimensional vector space and establishing its cooperation with network embedding have not been fully considered.
- Recently, there have been remarkable progress in this field with the development of generative models that do not explicitly require this integration and can be trained using back-propagation algorithm. Two famous examples of such models are Generative Adversarial Networks (GANs) @cite and Variational Autoencoders(VAE) @cite . These models are able to produce convincing image samples but not flexible enough to handle an image-to-image translation task with multiple attributes. Recent research related with GANs are mostly based on the work of DCGAN (deep convolutional generative adversarial network) @cite . DCGAN has been proved to learn good feature representation from image pixels in many research @cite @cite . And the deep architectures have also shown the effectiveness of synthesizing photorealistic images in the adversarial networks @cite .
- The recent work called InfoGAN @cite proposed an information-theoretic extension to GANs to explore the potential of the noise vector . In their work, they decompose the input noise vector into two parts: incompressible noise @math and latent code @math . And they try to make use of latent code @math based on the mutual information. Meanwhile, use a composite loss and provide an image-to-image translation solution that combines a GAN and a regression term @cite . Instead of a one-dimensional conditional vector, they use a two-dimensional image as the input condition to control the input and the output performance. The authors applied their model on various datasets and demonstrate that their model is able to synthesize 256*256 images for given semantic layouts.
- Due to the development of GANs, image translation successfully drawn researchers attention. The key research of this topic is to find the mapping function from one image domain to another image domain @cite @cite , which can be used for image painting and the image style conversion. proposed a method which contains two mapping functions, and relatively there are two adversarial discriminators @cite . They introduced two cycle consistency losses that can capture the image domain distribution and the translation from one image domain to another image domain on unpaired image samples. Although we can collect unpaired images more easily than paired samples, we need paired samples with multiple attributes to design a precise image translation system. That's why we choose 's model for our research @cite .
- In action segmentation and detection, a new class of temporal models, Temporal Convolutional Network (TCN) @cite , is proposed to capture long-range temporal dependencies, which outperforms other LSTM-based Recurrent Networks in both accuracy and training time. The work of @cite employs a re-designed TCN model (Res-TCN) with residual connections @cite @cite to skeleton based human action recognition task. The proposed Res-TCN also learns both spatial and temporal attention for human action recognition. The authors of @cite argue that Res-TCN is specifically designed to enhance the interpretability of model parameters and features compared to other recurrent models. However, the work of @cite remains mostly intuitive and unsystematic as opposed to our work.
- Recently, there has been an increase in efforts to interpret deep neural networks @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . The behavior of a complex black-box deep neural network can be understood by generating a locally faithful interpretable model around the test point and observing the output @cite . The authors of @cite @cite perturb the test point and observe how the prediction of the model changes. Additionally, @cite @cite @cite @cite attempt to visualize the synthesized inputs to the network that maximize the activations of hidden units.
- Another important class of approaches attempts to understand a CNN-based deep learning model by looking directly into the hidden representations with well-designed visualization techniques, which is most relevant to our current work. In the work of @cite , the proposed Network Dissection method performs a binary segmentation on every feature map with respect to every visual concept in Broden Dataset @cite . The intersection-over-union (IoU) score for every segmentation task is computed. A visual concept is matched to every neuron in the network by ranking the IoU scores. The work of @cite proposes an optimization method to compute an approximate inverse of the hidden representations. Note that the motivation of the work of @cite is to reconstruct the input image from the hidden representations as accurate as possible. We argue that the proposed optimization method has no interpretable basis as opposed to our Feature Map Decoder which can be viewed as an inverse computation of a single forward pass in the neural network.
- Our work is closely related to two research areas: election control and influence maximization. These bodies of work are separate: to our knowledge there is no previous work which considers election control social influence. The computational study of election control was started by @cite , who considered constructive control. The destructive control setting was introduced by @cite . A large body of work has studied election control under different settings and voting rules @cite @cite including bribing voters @cite @cite @cite @cite @cite , adding or deleting voters @cite @cite @cite @cite , and adding or deleting candidates @cite @cite @cite . Another topic is strategic behavior on the part of the voters themselves @cite @cite @cite . The main difference between our work and previous work on election control is that we introduce and analyze social influence as a novel mechanism for both constructive and destructive control.
- @cite generalized both the cascade setting and combinatorial cascade setting with contextual information, position discounts and more general reward functions. For the binary OR case, they provided a regret bound for @math rounds with order @math where @math is probability to check all recommended items and could be small. At the same time, @cite also generalized the cascade setting with linear payoff and brought up a UCB-like algorithm, CascadeLinUCB, as well as a Thompson sampling (TS) algorithm without a proof. They proved a regret bound of @math rounds for the CascadeLinUCB algorithm of order @math . In this paper, we consider the basic cascade setting, where the random feedback stops at the first click position, together with the online clustering to explore user structure. We provide a regret bound of order @math . Cast in this framework, the existing results studied the degenerate case of @math .
- Instance segmentation. Instance segmentation is a highly active research area @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite , with Mask R-CNN @cite representing the current state-of-the-art. These methods assume a fully supervised training scenario in which categories of interest have instance mask annotations during training. Fully supervised training, however, makes it difficult to scale these systems to thousands of categories. The focus of our work is to relax this assumption and enable training models even when masks are available for only a small subset of categories. To do this, we develop a novel transfer learning approach built on Mask R-CNN.
- Weight prediction and task transfer learning. Instead of directly learning model parameters, prior work has explored them from other sources ( , @cite ). In @cite , image classifiers are predicted from the natural language description of a zero-shot category. In @cite , a small neural network is used to predict the classifier weights of the composition of two concepts from the classifier weights of each individual concept. Here, we design a model that predicts the class-specific instance segmentation weights used in Mask R-CNN, instead of training them directly, which is not possible in our partially supervised training scenario.
- Our approach is also a type of transfer learning @cite where knowledge gained from one task helps with another task. Most related to our work, LSDA @cite transforms whole-image classification parameters into object detection parameters through a domain adaptation procedure. LSDA can be seen as transferring knowledge learned on an image classification task to an object detection task, whereas we consider transferring knowledge learned from bounding box detection to instance segmentation.
- Weakly supervised semantic segmentation. Prior work trains semantic segmentation models from weak supervision. (Note that is a pixel-labeling task that is different from , which is an object detection task.) Image-level labels and object size constraints are used in @cite , while other methods use boxes as supervision for expectation-maximization @cite or iterating between proposals generation and training @cite . Point supervision and objectness potentials are used in @cite . Most work in this area addresses only semantic segmentation, treats each class independently, and relies on hand-crafted bottom-up proposals that generalize poorly. Our work is complementary to these approaches, as we explore generalizing segmentation models trained from a subset of classes to other classes without relying on bottom-up segmentation.
- Many approaches have been proposed to learn the individual modules. For content selection module, one approach builds a content selection model by aligning records and sentences @cite @cite . A hierarchical semi-Markov method is proposed by @cite which first associates the text sequences to corresponding records and then generates corresponding descriptions from these records. Surface realization is often treated as a concept-to-text generation task from a given representation. reiter2000building , walker2001spot and stent2004trainable utilize various linguistic features to train sentence planners for sentence generation. Context-free grammars are also used to generate natural language sentences from formal meaning representations @cite @cite . Other effective approaches include hybrid alignment tree @cite , tree conditional random fields @cite , tree adjoining grammar @cite and template extraction in a log-linear framework @cite . Recent work combines content selection and surface realization in a unified framework @cite @cite @cite @cite
- Our model borrowed the idea of representing a structured table by its field and content information from @cite . However, their n-gram model is inefficient to model long-range dependencies while generating descriptions. mei2015talk also proposed a seq2seq model with an aligner between weather records and weather broadcast. The model used one-hot encoding to represent the weather records as they are relatively simple and highly structured. However, the model is not capable to represent the tables with complex structure like Wikipedia infoboxes.
- Holistic techniques rely on the global ear appearance and exploit representations that encode the ear structure as whole. As the appearance of ears varies significantly with pose or illumination, care needs to be taken before computing holistic features from input images and normalization techniques need to be applied to correct for these changes prior to feature extraction. Examples of global techniques can be found in @cite @cite @cite .
- The last groups of techniques, so-called hybrid techniques, combine elements from other categories or use multiple representations to increase the ear recognition performance @cite . Techniques from this group offer superior performance compared to competing techniques, but often at the cost of higher computational complexity @cite @cite @cite . As suggested by recent surveys on ear recognition @cite @cite @cite , hybrid techniques together with local descriptor-based methods represent the current state-of-the-art in this area.
- CNN-based recognition models: Various CNN architectures have been developed and presented in the literature over recent years. One of the most well-known problems that highlighted the potential and power of CNN-based approaches was object classification within the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) @cite . The AlexNet architecture introduced by @cite achieved an unprecedented performance on the ImageNet data and triggered a surge in the use and popularity of CNN-based models in computer vision @cite . The next milestone in terms of ILSVRC results marked the introduction of the 16-layer VGG @cite architecture which provided an additional boost to the recognition rates of ILSVRC. These two architectures are nowadays also used in other tasks for example as part of larger architectures such as Faster-RCNNs @cite or SegNet @cite @cite . A more recent architecture, called ResNet, was presented by @cite . The architecture introduced shortcut connections to CNNs and made it possible to reliable train deep networks with several hundreds or even thousands of network layers.
- The architecture and utility of Apache Spark was first presented to the community by the authors of @cite . It gives a brief overview regarding the programming model, which includes RDDs, parallel computing etc. It also introduces a few implementations in the environment. @cite introduces Apache Sparks machine learning library, MLlib. This includes its core features as well as the different components constituting the MLlib library. The work done in @cite analyzes Sparks primary framework by running a sample ML instance on it. The authors present comprehensive results based on their evaluations that highlight Sparks advantages. The authors of @cite present a similar usage of Spark by analyzing twitter data using Sparks framework. The massive size of the twitter dataset used highlights the significance of using a tool like Spark. The authors of @cite have performed similar twitter sentiment analysis using Apache Spark.
- The two stages of the framework mentioned in this paper, namely analysis using Spark and multi-layer perceptron, is connected via cascading. Cascading or Cascade Learning is typically used for cases where the classes are heavily imbalanced and a suitable inference cannot be gathered from the data. This approach has been previosuly applied in diverse fields such as Natural Language Processing, Computer Vision etc. and has proven to be quite effective. @cite proposed a time efficient two-stage cascaded classifier for the prediction of buy sessions and purchased items within such sessions. @cite proposed a cascade of feature-sharing deep classifiers, called OnionNet, where subsequent stages added both new layers as well as new feature channels to the previous ones. Recently, @cite cascaded two fully convolutional neural networks @cite for a combined image segmentation of the liver and its lesion and produced state-of-the-art results on the task.
- The rapid development of deep learning has accounted for recent exciting progress in image generation, especially the introduction of generative adversarial networks (GAN) @cite . Conditioning variables were then introduced to GAN @cite @cite @cite . Related to our contextual GAN for joint images is deep image completion with perceptual and contextual losses @cite . Pretrained with uncorrupted data, the @math and @math networks are trained to reconstruct a complete image. Their impressive examples show that even a large region of a facial image is cropped from the input, the generated complete facial image looks very realistic. Another impressive work on image completion @cite is based on autoencoders with a standard reconstruction loss and an adversarial loss. Autoencoders have also been successfully applied to generating images from visual attributes @cite . Analogous to image completion, where the uncropped part of the image provides the proper context for facial image completion, in our sketch-to-image generation, the entire input sketch is regarded as the uncropped context" for completing the entire natural image part of the joint image.
- The majority of anti-virus programs reply on analyzing the executable file to determine whether it is a malware. As Filiol and Josse @cite establish, most current anti-virus programs do not detects variant of malware. They propose a method for automatic signature generation by analyzing the executable's code and substrings, and measure statistical distribution of code across variants of malware. Their experiments were performed on short (small sized) malware such as Nimda, Code Red Code Red II, MS Blaster, Sober, Netsky and Beagle. This method is less accurate when applied to larger malware. Most real world malware are large, containing many modules and sub-modules, and so a statistical analysis would not be sufficient to accurately classify them. @cite generates a list of signatures for a malware by splitting its executable to segments of equal sizes. For each segment a signature is generated, and the list of signatures is subsequently ranked. This method is more resilient to small modifications in the executable, but a malware can evade this method by encrypting the executable (which is a simple and popular method for many malware programs), and thus evading any method which inspects the executable file for signature comparison.
- Convolutional neural network (CNN) @cite @cite and recurrent models like the long short-term memory (LSTM) @cite network are popular neural encoders for the QA ranking problem. @cite proposed to use CNN for learning features and subsequently apply logistic regression for generating QA relevance scores. Subsequently, an end-to-end neural architecture based on CNN and bilinear matching was proposed in @cite .
- In many recent works, the key innovation in most models is the technique used to model interaction between question and answer pairs. The CNN model introduced in @cite uses a MLP to compose vectors of questions and answers. @cite adopted tensor layers for richer modeling capabilities. @cite proposed holographic memory layers. @cite proposed Multi-Perspective CNN which matches multiple perspectives' based on variations of pooling and convolution schemes. Recent work has showed the effectiveness of learning QA embeddings in non-Euclidean spaces such as Hyperbolic space @cite . Models based on soft-attention such as Attentive Pooling networks (AP-BiLSTM and AP-CNN) @cite , AI-CNN @cite and aNMM (attention-based neural matching) @cite have also been proposed. These models learn weighted representations of QA pairs using similarity matrix based attentions.
- Regularization methods 5pt There are a number of ways to impose regularity to the model in order to improve generalization for better prediction, among which are data augmentation @cite @cite , batch normalization @cite @cite , or dropout @cite @cite @cite @cite . One can also incorporate regularization in the network architectures, including pooling @cite , maxout @cite , or skip connections @cite @cite . There is also an explicit regularization that is integrated with the objective function with classical weight decay @cite @cite , lasso @cite , group lasso @cite , or Hessian @cite . Our method acts in concert, not in alternative, to other forms of regularization.
- Early object detection methods are based on the sliding-window paradigm, which apply the hand-crafted features and classifiers on dense image grids to find objects. As one of the most successful methods, Viola and Jones @cite use Haar feature and AdaBoost to train a series of cascaded classifiers for face detection, achieving satisfactory accuracy with high efficiency. DPM @cite is another popular method using mixtures of multi-scale deformable part models to represent highly variable object classes, maintaining top results on PASCAL VOC @cite for many years. However, with the arrival of deep convolutional network, the object detection task is quickly dominated by the CNN-based detectors, which can be roughly divided into two categories, , the two-stage approach and one-stage approach.
- Unsupervised approaches have recently been introduced to estimate depth @cite , egomotion @cite and 3D reconstruction @cite . These methods are attractive for large-scale use as they only require raw video footage from a monocular or stereo camera, without any ground-truth motion estimates or semantic labels. In particular, @cite introduces an explainability mask'', which highlights image regions that disagree with the dominant motion estimate. However, the explainability mask differs from the ephemerality mask in that it only recognises non-dominant moving objects, and hence will still produce incorrect motion estimates when significantly occluded by a large, independently moving object.
- Recent works usually use RNN-based methods to capture the aggregated features of the sequential behaviors automatically. The real valued temporal information is exploited by @cite along with the discrete positions along the sequence. methods to evaluate similarities of different entities w.r.t different edge types. Context Aware Recommendation. An industrial recommendation system usually has varieties of models to extract different aspects of the user features, e.g., user gender, income, affordance, categorial preference, etc, all from the user behaviors, trying to capture both long-term and short-term user interests. Then it builds different models for each recommendation scenario using those extracted features, either continuous or categorial @cite @cite . Sometimes it also introduces factors like fatigue using exhausted feature engineering to prevent from seeing too many items of the same tastes. These are multi-step tasks and it's hard to optimize jointly.
- RNN based methods are studied for recommendation in academic fields in recent years, which builds the recommendation context directly from the behaviors for each user @cite @cite @cite . Though it's an more elegant way to encode user context, it still suffers from several difficulties. First, RNN is hard to parallelize in prediction phase, which makes it not easy to ensure the response time to be low enough for a commercial recommendation system. Second, the RNN embedding of the user behaviors is a fix-sized, aggregated status, which is not well suited for modeling both long and short behavior sequences. It can easily fail to preserve specific behavior information when enrolled in the downstream applications.
- The basic idea of attention is that instead of attempting to learn a single vector representation for each sentence, we instead keep around vectors for every word in the input sentence, and reference these vectors at each decoding step. Attention and Self-Attention. Attention is introduced by @cite firstly in the encoder-decoder framework, to provide more accurate alignment for each position in the machine translation task. Instead of preserving only one single vector representation for the whole object in the encoder, it keeps the vectors for each element as well, so that the decoder can reference these vectors at any decoding step. Recently, attention based methods are widely applied in many other tasks like reading comprehension @cite @cite @cite , ads recommendation @cite @cite , computer vision @cite , etc.
- Self-attentions are also studied in different mechanisms @cite @cite @cite , in which inner-relations of the data at the encoder side are considered. Note that, both work of @cite @cite show that project each word onto multiple spaces could improve the performance of their own tasks.
- @cite captures the attention score calculated by the importance of each word in the projected space, and it interpretates this phenomenon as that different subspaces may reveal different semantics.
- A user can be modeled as aggregated features extracted manually from each behavior group @cite , while those behaviors have different attributes. Heterogeneous Behavior Modeling. Heterogeneous data representation is heavily studied in domains like knowledge graph completion and multimodal learning. In the domain of knowledge graph completion, lots of works have been proposed to learn heterogeneous entity and relation representations by minimizing the distances of the linear projected entities in the relation-type semantic subspace @cite @cite @cite . Similar idea is adopted in multimodal learning tasks like image caption @cite and audio-visual speech classification @cite .
- 1. Self-attention is introduced by @cite ??, and is further studied by @cite @cite @cite [CNN, corpus].
- 2. Google, Reading Both works of @cite @cite show that project each word into multiple subspaces could improve the performance, and @cite interpreted this phenomenon as different subspaces may reveal different semantics.
- is a popular problem in both computer vision and computer graphics. Usually, synthesizing novel views requires estimating disparity from multiple input images, and then the synthesis is performed by warping input images with the disparity. @cite @cite @cite synthesize automultiscopic images from stereo images. @cite presents a disparity based method to reconstruct light field from micro-image pairs. However, in our problem, the surrounding views need to be predicted by a single image.
- reconstructs per-pixel depth of an input image, which is an important information in the view synthesis task. @cite tries to predict depth by learning algorithms. Deeper networks have also been tested by @cite @cite . These methods have limited accuracy in natural scenes because scene variations are restricted by the coverage of their training datasets. Inspired by depth based image rendering, unsupervised methods @cite @cite based on stereo constraints have shown great potential in depth estimation. However, the camera parameters are required for these techniques. Varying from previous methods, @cite proposes a relative depth-based framework to estimate depth for unconstrained images ( in the wild"). It leverages a large dataset containing 421k images and performs well in deciding the relative position of objects in the scenes.
- Patch based methods have the advantage of being easy to understand, network architecture agnostic, and frequently straightforward to implement. They also tend to perform well on standard measures of transfer learning. For instance, @cite is a top performer on PASCAL VOC 2007 detection @cite , even among a large number of new arrivals. @cite is almost tied for the top score on PASCAL VOC 2007 classification @cite and has the top score for PASCAL VOC 2007 detection and the second highest score for PASCAL VOC 2012 segmentation @cite @cite .
- An often-cited worry in all patch context works relates to trivial low-level boundary pattern completion @cite @cite @cite @cite @cite . The neural network may learn the alignment of patches not based on (for instance) desirable semantic information, but instead by matching the top or bottom part of simple line segments. Two common approaches are to provide a large enough gap between patches and to randomly jitter the patches. This last technique may be dubious since a convolutional neural network can align simple patterns at arbitrary offsets. This issue may also be implicitly addressed by having non-4-connected adjacent patches. Half the patches in @cite are arranged diagonally which should make them resistant to trivial low-level boundary pattern completion. Also, we should note that while we would not want a self-supervised learner to use this cheat all the time, it could be used as a cue to help form low level features. So, it is somewhat unclear how much of a problem this might be.
- In another problem for self-supervised networks in general, mid-layers in the network may not train as well as the early and later layers. For instance, @cite created a self-supervised network using an ensemble of different methods. They then created an automated lasso to grab layers in the network most useful for their task. The lasso tended to grab layers very early or very late in the network. This suggests that for many self-supervised tasks, the information in the middle network layers is not very essential for training. Another piece of evidence comes from the CSAIL Places linear test @cite @cite , which shows how well each layer in the network performs on transfer learning. Many self-supervised networks perform as well or better than a supervised ImageNet trained network at the first and second convolutional layers in AlexNet, but struggle at deeper layers.
- Zhang al proposed a dual-attention model using images and optional texts to make accurate prediction @cite . In @cite , Zhang al proposed an image-to-text model to establish a direct mapping from medical images to diagnostic reports. Both models were evaluated on a dataset of bladder cancer images and corresponding diagnostic reports. Wang al took advantage of a large-scale chest X-ray dataset to formulate the disease diagnosis problem as multi-label classification, using class-specific image feature transformation @cite . They also applied a thresholding method to the feature map visualization @cite for each class and derived the bounding box for each disease. Their qualitative results showed that the model usually generated much larger bounding box than the ground-truth. Hwang al @cite proposed a self-transfer learning framework to learn localization from the globally pooled class-specific feature maps supervised by image labels. These works have the same essence with class activation mapping @cite which handles natural images. The location annotation information was not directly formulated into the loss function in the none of these works. Feature map pooling based localization did not effectively capture the precise disease regions.
- The robustness of image classifiers against adversarial perturbations has gained significant attention in the last few years @cite , @cite , @cite , @cite , @cite , @cite , @cite . Deep neural networks became the center of attention in this area after @cite first demonstrated the existence of adversarial perturbations for such networks. See @cite for a recent review of literature in this direction. @cite computed adversarial examples for the networks by adding quasi-imperceptible perturbations to the images, where the perturbations were estimated by maximizing the network's prediction error. Although these perturbations were image-specific, it was shown that the same perturbed images were able to fool multiple network models. reported encouraging results for improving the model robustness against the adversarial attacks by using adversarial examples for training, a.k.a. .
- @cite built on the findings in @cite and developed a fast gradient sign method' to efficiently generate adversarial examples that can be used for training the networks. They hypothesized that it is the linearity of the deep networks that makes them vulnerable to the adversarial perturbations. However, Tanay and Griffin @cite later constructed the image classes that do not suffer from the adversarial examples for the linear classifiers. Their arguments about the existence of the adversarial perturbations again point towards the over-fitting phenomena, that can be alleviated by regularization. Nevertheless, it remains unclear how a network should be regularized for robustness against adversarial examples.
- Moosavi-Dezfooli @cite proposed the DeepFool algorithm to compute image-specific adversarial perturbations by assuming that the loss function of the network is linearizable around the current training sample. In contrast to the one-step perturbation estimation @cite , their approach computes the perturbation in an iterative manner. They also reported that augmenting training data with adversarial examples significantly increases the robustness of networks against the adversarial perturbations. Baluja and Fischer @cite trained an Adversarial Transformation Network to generate adversarial examples against a target network. @cite analyzed the transferability of adversarial examples. They studied this property for both targeted and non-targeted examples, and proposed an ensemble based approach to generate the examples with better transferability.
- The above-mentioned techniques mainly focus on generating adversarial examples, and address the defense against those examples with adversarial training. In-line with our take on the problem, few recent techniques also directly focus on the defense against the adversarial examples. For instance, @cite mitigate the issues resulting from the adversarial perturbations using foveation. Their main argument is that the neural networks (for ImageNet @cite ) are robust to the foveation-induced scale and translation variations of the images, however, this property does not generalize to the perturbation transformations.
- @cite used distillation @cite to make the neural networks more robust against the adversarial perturbations. However, Carlini and Wagner @cite later introduced adversarial attacks that can not be defended by the distillation method. @cite specifically studied the adversarial training for making large models (e.g. Inception v3 @cite ) robust to perturbations, and found that the training indeed provides robustness against the perturbations generated by the one-step methods @cite . However, @cite found that this robustness weakens for the adversarial examples learned using different networks i.e. for the black-box attacks @cite . Hence, ensemble adversarial training was proposed in @cite that uses adversarial examples generated by multiple networks.
- @cite studied the effects of JPG compression on adversarial examples and found that the compression can sometimes revert network fooling. Nevertheless, it was concluded that JPG compression alone is insufficient as a defense against adversarial attacks. @cite took advantage of localization of the perturbed pixels in their defense. @cite proposed SafetyNet for detecting and rejecting adversarial examples for the conventional network classifiers (e.g. VGG19 @cite ) that capitalizes on the late stage ReLUs of the network to detect the perturbed examples. Similarly, a proposal of appending the deep neural networks with detector subnetworks was also presented by @cite . In addition to the classification, adversarial examples and robustness of the deep networks against them have also been recently investigated for the tasks of semantic segmentation and object detection @cite , @cite , @cite .
- Whereas the central topic of all the above-mentioned literature is the perturbations computed for images, Moosavi-Dezfooli @cite were the first to show the existence of image-agnostic perturbations for neural networks. These perturbations were further analyzed in @cite , whereas @cite also showed their existence for semantic image segmentation. To date, no dedicated technique exists for defending the networks against the universal adversarial perturbations, which is the topic of this paper.
- Optical flow estimation was introduced as a fundamental computer vision problem since the pioneering works @cite @cite . Starting from then, the accuracy of optical flow estimation has been improving steadily as evidenced by the results on Middlebury @cite and MPI-Sintel @cite benchmark dataset. Most classical optical flow algorithms belong to the variants of the energy minimization problem with the brightness constancy and spatial smoothness assumptions @cite @cite . Other trends include a coarse-to-fine estimation or a hierarchical framework to deal with large motion @cite @cite @cite @cite , a design of loss penalty to improve the robustness to lighting change and motion blur @cite @cite @cite @cite , and a more sophisticated framework to handle occlusion @cite @cite which we will describe in more details in the next subsection.
- Since occlusion is a consequence of depth and motion, it is inevitable to model occlusion in order to accurately estimate flow. Most existing methods jointly estimate optical flow and occlusion. Based on the methodology, we divide them into three major groups. The first group treats occlusion as outliers and predict target pixels in the occluded regions as a constant value or through interpolation @cite @cite @cite @cite . The second group deals with occlusion by exploiting the symmetric property of optical flow and ignoring the loss penalty on predicted occluded regions @cite @cite @cite . The last group builds more sophisticated frameworks such as modeling depth or a layered representation of objects to reason about occlusion @cite @cite @cite @cite . Our model is similar to the second group, such that we do not take account the difference where the occlusion happens into the loss function. To the best of our knowledge, we are the first to incorporate such kind of method with a neural network in an end-to-end trainable fashion. This helps our model to obtain more robust flow estimation around the occlusion boundary @cite @cite .
- @cite first introduces an end-to-end differentiable neural architecture that allows unsupervised learning for video motion prediction and reports preliminary results on a weakly-supervised semantic segmentation task. Later, @cite @cite @cite adopt a similar unsupervised learning architecture with a more detailed performance study on multiple optical flow benchmark datasets. A common philosophy behind these methods is that instead of directly supervising with ground-truth flow, these methods utilize the Spatial Transformer Networks @cite to warp the current images to produce a target image prediction and use photometric loss to guide back-propagation @cite . The whole framework can be further extended to estimate the depth, camera motion and optical flow simultaneously in an end-to-end manner @cite . This overcomes the flow annotation problem, but the flow estimation accuracy in previous works still lags behind the supervised learning methods. In this paper, we show that unsupervised learning can obtain competitive results to supervised learning models. After the initial submission of this paper, we became aware of a concurrent work @cite which tries to solve the occlusion problem in unsupervised optical flow learning with a symmetric-based approach.
- For the technology of taking human value from experiment practically, we have illustrated an approach to take the human value (trust and trustworthy) from human subjects experiment. These parameters can be transit to AI in similar non-cooperation game condition, avoiding AI to be too tough. Though it is well known that there exists the human value(e.g., fairness, justices) in the experimental game theory @cite @cite @cite , our approach is the first to take the human value quantitatively as AI ethics controlling parameter for AI design.
- Several implementations of related models are available as free software packages. The R-package implements the estimation of a time-varying AR model using a rolling window approach. The R-package implements the estimation of (time-varying) state-space models, of which the time-varying VAR model is a special case. While the state-space model framework is very powerful due to its generality, it requires the user to specify the way parameters are allowed to vary over time, for which often no prior theory exists in practice . In parallel efforts @cite developed the R-package , which estimates time-varying AR and VAR models using kernel smoothing similarly to kernel-smoothing approach described in the present paper. An interesting way to modeling time-varying parameters is by using the fused lasso . However, to our best knowledge there are currently only implementations availabe to estimate time-varying Gaussian Graphical Models with this type of method: a Python implementation of the SINGLE algorithm and a Python implementation of the (group) fused-lasso based method as presented in @cite .
- GANs @cite constitute another recent framework for learning a generative model. Recent extensions of GAN have focused on boosting the performance of image generation by improving the generator @cite , discriminator @cite or the training algorithm @cite @cite @cite . More recently, some researchers @cite @cite @cite have employed a bidirectional network structure within the adversarial learning framework, which in theory guarantees the matching of joint distributions over two domains. However, non-identifiability issues are raised in @cite . For example, they have difficulties in providing good reconstruction in latent variable models, or discovering the correct pairing relationship in domain transformation tasks. It was shown that these problems are alleviated in DiscoGAN @cite , CycleGAN @cite and ALICE @cite via additional @math , @math or adversarial losses. However, these methods lack of explicit probabilistic modeling of observations, thus could not directly evaluate the likelihood of given data samples.
- A key component of the proposed framework concerns integrating a new VAE formulation with adversarial learning. There are several recent approaches that have tried to combining VAE and GAN @cite @cite , Adversarial Variational Bayes (AVB) @cite is the one most closely related to our work. AVB employs adversarial learning to estimate the posterior of the latent codes, which makes the encoder arbitrarily flexible. However, AVB seeks to optimize the original VAE formulation in ), and hence it inherits the limitations of ML-based learning of @math . Unlike AVB, the proposed use of adversarial learning is based on a new VAE setup, that seeks to minimize the symmetric KL distance between @math and @math , while simultaneously seeking to maximize the marginal expected likelihoods @math and @math .
- The clearest connection is to work in traditional language-to-language translation. The Seq2Seq model was first created and used in conjunction with statistical methods to perform machine translation @cite . The model consists of a recurrent neural network acting as an encoder which produces an embedding of the full sequence of inputs. This sentence embedding is then used by another recurrent neural network which acts as a decoder and produces a sequence corresponding to the original input sequence.
- Long Short-Term Memory(LSTM) @cite was introduced to allow a recurrent neural network to store information for an extended period of time. Using a formulation of LSTM which differs slightly from the original @cite , the Seq2Seq model was adapted to use multiple LSTM layers on both the encoding and decoding sides @cite . This model demonstrated near state-of-the-art results on the WMT-14 English-to-French Translation task. http: www.statmt.org wmt14 translation-task.html In another modification an attention mechanism was introduced @cite which allowed the decoder network to learn to focus on relevant parts of the encoded source sentence during decoding. This again achieved near state-of-the-art results on English-to-French translation.
- Another paper proposed a version of the model which could translate into many languages @cite . They use a single encoder, but a different decoder for each target language. This idea was extended to study multi-way multilingual machine translation with a recurrent network @cite using a model that translates from many different language pairs by training a separate encoder for every source language and a separate decoder for every target language. They use the correct encoder and decoder for the encountered language pair, but have an attention mechanism that is shared across all pairs.
- Recently, a neural machine translation model capable of both multilingual translation and zero-shot translation was introduced @cite . The authors make no major changes to the Seq2Seq architecture, but introduce special tokens at the start of each input sentence indicating the target language. The model can learn to translate between two languages which never appeared as a pair in the training data, provided it has seen each of the languages paired with others. They call this task zero-shot translation .
- Tasks such as text simplification @cite @cite can be viewed as a form of style transfer. Paraphrase targeting a more general interpretation of style was first introduced in 2012 @cite . Therein the authors use a dataset of Shakespearean plays and their modern translations'' and train several models to convert text between the styles.
- The advances mentioned previously in neural machine translation have only started to be applied to more general stylistic paraphrasing. One approach proposed the training of a neural model which would disentangle'' stylistic and semantic features, but did not publish any results @cite . Another attempt at text simplification as stylistic paraphrasing is @cite . They generate artificial data and show that the model performs well, but do no experiments with human-produced corpora. The Shakespeare dataset @cite recently was used with a Seq2Seq model @cite . Their results are impressive, showing significant improvement over statistical machine translation methods as measured by automatic metrics. They experiment with many settings, but their best models all require the integration of a human-produced dictionary which translates approximately 1500 Shakespearean words to their modern equivalent. None of the models they explore without using this dictionary are able to outperform the statistical model. Since such dictionaries do not exist for most style pairs we show that Seq2Seq can outperform statistical methods without such specialized data. This generality opens up a much broader collection of text as candidates for style transfer using Seq2Seq.
- Our work extends the variational dropout technique originally proposed in @cite . It was suggested that applying a multiplicative random noise to inputs of a fully-connected neural network layer is equivalent to computing the posterior distribution of the pre-activation outputs ( @cite ). Using the reparametrization of the weights @math as @math , where @math is a deterministic weight value and @math is a random variable determining the scale of the @math weight. As described in @cite , it is possible to split the posterior distribution over the weights @math into an additive (i.e., location) parameter @math and a multiplicative (i.e., scale) parameter @math . Formally, @math , using set notation.
- Being the relative generalization of Shannon's entropy @cite , KL is by far the most widely known divergence measure. It is very closely related to maximum likelihood estimation since minimizing the KL divergence is equivalent to maximizing the likelihood of the target parameters given the observed data @cite . Below we detail some essential properties of Kullback-Leibler divergence: @math by Gibbs' inequality; @math for @math and @math ; @math unless @math . Kullback-Leibler hence is not a distance because it is not symmetric. It also does satisfy the triangle inequality.
- Another influential approach to ontology repair is discussed in @cite and in @cite . That approach, like ours, attempts to weaken problematic axioms; but it does so by adding to value restrictions @math , Another difference is that we are also interested in repairing TBoxes, whereas the approach of @cite operates only over ABoxes. rather than by means of a more general-purpose transformation.
- We leave to future work the evaluation of our approach in comparison to other state-of-the-art ontology repair frameworks. As already stated, this is not an entirely well-posed problem; but if, as in this work, we accept that a suggested repair @math is preferable to another suggested repair @math whenever @math then the question becomes amenable to analysis. Possibly, complementary metrics for further evaluations can be chosen from @cite . Experiments involving user evaluation could be also considered in this context.
- There are several algorithms for computing strategies. In zero-sum games (where @math , one can use e.g. linear programming, fictitious play @cite , replicator dynamics @cite , or regret minimization @cite . Some of these techniques have been extended to extensive (sequential) form @cite @cite @cite @cite with an exponential increase in the size of the state space. However, these extensions have almost exclusively treated the two-player case, with some notable exceptions @cite @cite . Fictitious play also converges in potential games which includes cooperative (identical payoff) games.
- The double oracle (DO) algorithm @cite solves a set of (two-player, normal-form) subgames induced by subsets @math at time @math . A payoff matrix for the subgame @math includes only those entries corresponding to the strategies in @math . At each time step @math , an equilibrium @math is obtained for @math , and to obtain @math each player adds a best response @math from the full space @math , so for all @math , @math . The algorithm is illustrated in Figure . Note that finding an equilibrium in a zero-sum game takes time polynomial in @math , and is PPAD-complete for general-sum @cite .
- Clearly, DO is guaranteed to converge to an equilibrium in two-player games. But, in the worst-case, the entire strategy space may have to be enumerated. For example, this is necessary for Rock-Paper-Scissors, whose only equilibrium has full support @math . However, there is evidence that support sizes shrink for many games as a function of episode length, how much hidden information is revealed and or effects it has on the payoff @cite @cite @cite . Extensions to the extensive-form games have been developed @cite @cite @cite but still large state spaces are problematic due to the curse of dimensionality.
- One study used evolutionary dynamics in the space of known expert meta-strategies in Poker @cite . Recently, reinforcement learning has been used to validate strategies found via EGTA @cite . In this work, we aim to discover new strategies through learning. However, instead of computing exact best responses, we compute approximate best responses using reinforcement learning. A few epochs of this was demonstrated in continuous double auctions using tile coding @cite . This work follows up in this line, running more epochs, using modern function approximators (deep networks), a scalable implementation, and with a focus on finding policies that can generalize across contexts.
- applied forced decoding on the training set to improve the training process of phrase-based SMT and prune the phrase-based rule table. They also used word insertions and deletions for forced decoding, but they used a high penalty for all insertions and deletions. In contrast, our soft forced decoding algorithm for NMT outputs uses a small penalty for function words and a high penalty for content words, because function words are usually translated very flexibly and more likely to be inserted or deleted compared to content words. For example, the under-translation of a content word can hurt the adequacy of the translation heavily. But function words may naturally disappear during translation (e.g. the English word the" disappears in Chinese). By assigning a high penalty to words that should not be deleted or inserted during translation, our soft forced decoding method aims to improve the adequacy of NMT, which is very different from previous forced decoding methods that are used to improve general SMT training @cite @cite .
- Using unlabeled data to improve generalization has a long and rich history and the literature in SSL is vast @cite @cite . So in this section we focus on reviewing the closely related papers, especially the recent advances in SSL with deep learning.
- Besides aforementioned discriminative approaches, another line is generative models, which pay efforts to learn the input distribution @math that is believed to share some information with the conditional distribution @math @cite . Traditional models such as Gaussian mixtures @cite try to maximize the joint log-likelihood of both labeled and unlabeled data using EM. For modern deep generative models, variational auto-encoder (VAE) makes it scalable by employing variational methods combined with deep learning @cite while generative adversarial networks (GAN) generate samples by optimizing an adversarial game between the discriminator and the generator @cite @cite @cite @cite . The samples generated by GAN can be viewed as another kind of data augmentation'' to tell'' the decision boundary where to lie. For example, fake'' samples can be generated in low density regions where the training data is rare @cite @cite based on the assumption. Alternatively, more pseudo'' samples could be generated in high density regions to keep away from the decision boundary thus improve the robustness of the classifier @cite . Our work is complementary to these efforts and can be easily combined with them. We observe improvements over feature matching GAN @cite with SNTG (see ).
- Local image descriptors have been widely used in finding similar and dissimilar regions in images. Nowadays, the trend has changed from hand-crafted and carefully-designed methods (SIFT @cite or DAISY @cite ) to a new generation of learned descriptors including unsupervised and supervised techniques like boosting @cite , convex optimization @cite and Linear Discriminant Analysis (LDA) @cite @cite .
- In our approach, however, we propose a descriptor based on deep convolutional neural networks (CNN) with batch normalization units accelerating learning and convergence. The first papers which utilized CNN based representations for finding matching image patches were @cite and @cite . More recently, Z bontar and LeCun @cite proposed a method for comparing image patches in order to extract stereo depth information. Their method is based on using convolutional networks minimizing a hinge loss function and showed the best performance on KITTI stereo evaluation dataset @cite . However, as that approach operates on very small patches ( @math pixels), it restricts the area of applicability.
- Recent approaches @cite @cite @cite propose CNN descriptors trained with two-branch (Siamese) architecture which significantly exceed the accuracy of hand-crafted descriptors. However, in contrast to SIFT, in @cite @cite the feature representations of input patches are compared by a set of fully connected layers (match network) that learns a complex comparison metric. Nevertheless, @cite and Simo- @cite also conducted experiments in which the match network was replaced with Euclidean distance metric between the outputs of two branches and, hence, they are the closest works to ours. The implementation of @cite is not yet publicly available. Thus, in order to compare performance, we reproduced the network architecture of @cite and evaluated it using the standard protocol. The results show that our network architecture outperforms those of @cite @cite . More detailed comparison is presented in Sec. .
- Finally, we note that the importance of using an appropriate prior for GAN models has also been discussed in @cite which suggested to infer the continuous latent factors in order to maximize the data log-likelihood. However this approach still makes use of a simple fixed prior distribution over the latent factors and does not use the inferred latent factors to construct a prior as suggested by our approach.
- Apprenticeship learning, the problem of learning correct behavior by observing the policies or behavioral trajectories of one or more experts, has predominantly been accomplished by (IRL) @cite @cite . IRL algorithms generally compute a reward function that explains'' the observed trajectories (typically, by maximally differentiating them from random behavior). Complete discussion of the many types of IRL algorithms is beyond the scope of this paper. The proposed approach bears some resemblance to IRL, particularly in its inputs (sets of finite behavioral trajectories). Instead of computing a reward function based on the observed trajectories, however, the proposed approach computes a that optimally explains'' the data. This addresses the criticisms of @cite , who claim that IRL is insufficient in morally and socially important domains because (1) reward functions can be difficult for human instructors to understand and correct, and (2) some moral and social goals may be too temporally complex to be representable using reward functions.
- Stochastic variants of heavy ball method have been employed widely in practice, especially in the area of deep learning @cite @cite @cite . Despite the popularity of the method both in convex and non-convex optimization its convergence properties are not very well understood. Recent papers that provide complexity analysis of SHB (in different setting than ours) include @cite and @cite . In @cite the authors analyzed SHB for general Lipshitz continuous convex objective functions (with bounded variance) and proved the sublinear rate @math . In @cite , a complexity analysis is provided for the case of quadratic strongly convex smooth coercive functions. A sublinear convergence rate @math , where @math , was proved. In contrast to our results, where we assume fixed stepsize @math , both papers analyze SHB with diminishing stepsizes. For our problem, variance reduction methods like SVRG @cite , S2GD @cite , mS2GD @cite , SAG @cite and SAGA @cite are not necessary. To the best of our knowledge, our work provides the first linear convergence rate for SHB in any setting.
- As -based malicious attacks started to considerably grow in @math , the number of detection approaches is rather limited. FlashDetect @cite is one of the first approaches to the detection of @math -based malware. The authors instrumented Lightspark , an open source viewer, to perform dynamic analysis of malicious files. From this analysis, the system extracts features such as the number of ByteArray -related method calls, the presence of the loadBytes method, and so forth. FlashDetect was employed inside the Wepawet service, which is sadly not available anymore.
- Hidost @cite is a static system that only focuses on the structure of the file. More specifically, it considers sequences of objects belonging to the structure of the file as features. The system evaluates the most occurring paths in the training dataset, and extracts features that are based on the training data. This might be dangerous from the perspective of targeted attacks, as a malicious test file with completely different paths might be able to evade detection. Moreover, the system does not analyze the embedded ActionScript code. In this way, an attacker might simply evade the system by perfectly mimicking the structure of the file, regardless of the type of code it contains.
- The Maximum Entropy method has been proposed as a part of Formalizing Subjective Interestingess ( forsied ) framework of data mining @cite @cite modelling the user's knowledge by a background distribution. forsied has been studied in the context of dimensionality reduction and EDA @cite @cite . To the best of our knowledge, ours is the first instance in which this background distribution can be updated by a direct interaction of the user, thus providing a principled method of EDA.
- Many other special-purpose methods have been developed for active learning in diverse settings, e.g., in classification and ranking, as well as explicit models for user preferences. However, as these approaches are not targeted at data exploration, we do not review them here. Finally, several special-purpose methods have been developed for visual iterative data exploration in specific contexts, e.g., for itemset mining and subgroup discovery @cite @cite @cite @cite , information retrieval @cite , and network analysis @cite .
- The system presented here can be also considered to be an instance of visually controllable data mining @cite , where the objective is to implement advanced data analysis methods understandable and efficiently controllable by the user. Our approach satisfies the properties of a visually controllable data mining method (see @cite , Sec. II B): (VC1) the data and model space are presented visually, (VC2) there are intuitive visual interactions allowing the user to modify the model space, and (VC3) the method is fast enough for visual interaction.
- Dimensionality reduction for EDA has been studied for decades starting with multidimensional scaling (MDS) @cite @cite and projection pursuit @cite @cite . Recent research on this topic (referred to as manifold learning) is still inspired by MDS: find a low-dimensional embedding of points representing well the the distances in the high-dimensional space. In contrast to PCA @cite , the idea is to preserve small distances, and large distances are irrelevant, as long as they remain large, e.g., Local Linear and (t-)Stochastic Neighbor Embedding @cite @cite @cite . This is typically not possible to achieve perfectly, and a trade-off between precision and recall arises @cite .
- Biometric based mobile authentication solutions leverage unique human characteristics, e.g., faces @cite , fingerprints @cite , gait @cite , to authenticate users. In particular, the Pixie form factor makes it similar to camera based biometric authentication solutions based on face @cite @cite @cite and gaze @cite @cite . Consequently, Pixie shares several limitations with these solutions, that include (i) vulnerability to shoulder surfing attacks and (i) susceptibility to inappropriate lighting conditions, that can spoil the performance and usability of the authentication mechanism @cite @cite .
- In contrast to biometrics, Pixie enables users to change the authenticating physical factor, as they change accessories they wear or carry. This reduces the risks from an adversary who has acquired the authentication secret from having lifelong consequences for the victims, thereby mitigating the need for biometric traceability and revocation @cite .
- Table compares the user entry times of Pixie with various other authentication solutions. While Pixie takes longer than biometric authentication based on face @cite , it is still faster than several authentication solutions based on gaze @cite @cite . We note that while fingerprint based authentication is fast and convenient @cite , it is only applicable to devices that invest in such equipment. In contrast, cameras are ubiquitously present, including on wearable devices such as smartwatches and smartglasses.
- Pixie needs to solve a harder problem than existing biometrics based authentication solutions, due to the diversity of its trinkets: while existing biometrics solutions focus on a single, well studied human characteristic, Pixie's trinkets can be arbitrary objects. We note that Pixie can be used in conjunction with biometric authentication solutions, e.g., @cite : in touchscreen devices, one could use a touch gesture to mark the trinket, as an additional authentication factor.
- Solutions such as @cite @cite @cite treat the mobile device as a second factor and eliminate user interaction to retrieve a token from the mobile device to the authentication device (e.g. a desktop) by leveraging proximity based connectivity (e.g., Bluetooth, Wi-Fi). In contrast, Pixie assigns the duty of storing the token for the second factor to a physical object outside the mobile device. The mobile device is the sole device that is used to access the services on remote servers. As an added benefit, the physical factor of the trinket renders Pixie immune to the 2FA synchronization vulnerabilities'' introduced by @cite , that exploit the ongoing integration of apps among multiple platforms. Since Pixie authentication requires a simple interaction with the user, it is also possible to combine Pixie with a token stored on the mobile device. The combined Pixie and mobile device token authentication would require the user to possess both the particular mobile device that stores the token and the trinket.
- Pixie also differs from 2FA methods that involve visual tokens, e.g, QR-codes @cite @cite , as the trinket is secret to the user, the attacker needs to discover the trinket and also take possession of it. We note that a Pixie variant could be used in conjunction with the security token concept: the token displays a pattern (e.g., a QR code, random art), which the user captures using Pixie.
- To address the limited input space of wearable devices, available sensors (e.g. camera) are commonly exploited to provide alternative input techniques: Omata and Imai @cite identify the input gesture of the user by sensing the deformation of the skin under the smartwatch. @cite use infrared sensors to capture the gesture input of the user to interact with a wearable device. @cite exploit the ambient light sensor to capture the changes in light state as a form of PIN entry for wearable devices.
- Similar to Pixie, cameras integrated in wearable devices have been used to capture the input for authentication. Van @cite exploit the smartwatch camera to provide the device with an input (e.g. PIN) that is drawn on a canvas, then use image processing techniques to interpret the captured input. @cite propose to pair and unlock smartglasses with the user smartphone by exploiting the glass camera to scan a QR code that is displayed on the user's phone screen. Similarly, @cite use the smartglass camera to scan a QR code that is displayed on a point-of-service terminals (e.g. ATM) to connect to a cloud server for obtaining an OTP.
- Pixie's visual nature is similar to graphical passwords, that include recall, recognition and cued recall systems (see @cite for a survey). Recall based solutions such as DAS (Draw-A-Secret) @cite and variants @cite @cite ask the user to enter their password using a stylus, mouse or finger. For instance, De @cite proposed to enter the stroke based password on the front or back of a double sided touch screen device. In recognition-based systems (e.g., Passfaces @cite @cite ), users create a password by selecting and memorizing a set of images (e.g., faces), which they need to recognize from among other images during the authentication process.
- The usability of traditional text-based passwords has been well studied in literature, see e.g., @cite @cite @cite @cite . @cite found that face biometrics can be entered faster than text based passwords and Table shows that Pixie is also faster than text based passwords. Several limitations are associated with text passwords on memorability and usability especially when adopted in mobile platforms. For instance, @cite have shown through a large user study of different password-composition policies, that more than 20 user entry time for text passwords ranges between 11.6-16.2s (see Table ) in line with our evaluation (see @math ). Pixie is also perceived as more memorable than text passwords (see ).
- @cite found that creating and entering passwords on mobile devices take longer than desktops and laptops. In mobile devices, text-based passwords need to be entered on spatially limited keyboards on which typing a single character may require multiple touches @cite , due also to typing the wrong key. Pixie replaces typing a password with pointing the camera to the trinket and snapping a photo of it.
- A function @math is @cite under a set of transformations @math if for any transformation @math of the input, we can associate a transformation @math of the output such that for all @math . Transformations @math and @math represent the same underlying transformation but in different spaces, denoted @math .
- In 2015, Blom and Hansen @cite mapped the use of forward-referencing headlines in online news by analyzing 100,000 headlines published in ten different Danish news websites. They found that commercialization and tabloidization seem to lead to the recurrent use of forward-referencing in Danish online news headlines.
- Several studies involving the identification of abusers have been conducted. In 2012, @cite proposed a method that clusters users according to the similarity of the posted URL and then classifies each cluster as either malicious or not by extracting clusters' behavioral and content features.
- In 2013, a method for the identification of crowdturfers on Twitter was presented by Lee at al. @cite . They extracted features that were related to account properties, activity patterns, and linguistic properties.
- @cite and @cite used ML techniques for bot detection. @cite based their detection on sentiment analysis, social network analysis, posted content, and account property features. @cite presented BotOrNot, a bot identification platform that can be used through a Web user interface. They detected bots based on all of the features used by @cite , as well as behavior features.
- Recently, @cite identified hoaxes within Facebook based on the users who interacted with these hoaxes rather than the hoaxes' content. @cite proposed a method for estimating the authenticity of online discussions based on several similarity functions of OSM accounts participating in an online discussion. They found that the similarity function with the best performance across all of the datasets was the bag-of-words.
- The GNSS jamming and spoofing threat has been recognized in the literature for more than a decade. A survey of the current state-of-the-art in spoofing and anti-spoofing techniques is presented in @cite . Recent works on GNSS anti-spoofing techniques have specifically focused on the case of timing security. Collaborative multi-receiver @cite and direct time estimation @cite techniques have been proposed for robust GNSS clock synchronization.
- @cite propose measuring the propagation delays during initialization of clock synchronization and monitoring the propagation delays during the normal operation of the time synchronization protocol. However, @cite does not prove that such a defense would be sufficient to prevent the delay attacks.
- In @cite , it is remarked that the clock offset computed between multiple master clocks over a symmetric channel must be zero, and thus, if multiple master clocks are available, they can detect any malicious delay introduced by an adversary. However, this defense does not consider the possibility that the adversary may only delay the packets sent to the slave nodes.
- The work presented in @cite is perhaps in closest relation to the current paper. upper bound the clock drift between subsequent synchronization signals using a drift model, and perform two-way exchange of timestamps such that the master clock is able to verify the time at the slave. Furthermore, given the maximum clock drift rate and the maximum and minimum propagation delay of the timing signal, they derive an upper bound on the adversarial delay that can go unnoticed. However, with conservative bounds on the maximum clock drift rate and the variation in path delays, the accuracy guarantees derived in @cite may be insufficient for certain applications. Moreover, as will be shown in this paper, they fail to take account of one the necessary conditions for secure synchronization.
- Cooperative relaying is mostly considered at the physical layer, and is based on information-theoretic considerations. The classical relay channel was first examined in @cite and later in @cite . Recently cooperative communications have received renewed attention, as a powerful technique to combat fading and attenuation in wireless networks; e.g., @cite @cite . Most of the research has concentrated on information-theoretic studies. Recent works @cite @cite @cite @cite shown that similar gains can be achieved by network-layer cooperation. By network-layer cooperation, relaying is assumed to take place at the protocol level avoiding physical layer considerations.
- In addition, random access recently re-gained interest due to the increased number of communicating devices in 5G networks, and the need for massive uncoordinated access @cite . Random access and alternatives schemes and their effect on the operation of LTE and LTE-A are presented in @cite , @cite , @cite . In @cite , the effect of random access in Cloud-Radio Access Network is considered.
- The characterization of the stable throughput region, i.e. the stability region, which gives the set of arrival rates such that there exist transmission probabilities under which the system is stable, is a meaningful metric to measure the impact of bursty traffic and the concept of interacting nodes in a network; e.g., @cite @cite @cite .
- Driving behavior analysis is an interdisciplinary field and has drawn increasing attention in the recent decade, specifically in the context of vehicle automation @cite . An in-depth recent review of the literature on driving style analysis frameworks is given in @cite .
- Different kernel functions (interpolators) are available in previous NUFFT implementations, including: (1)the min-max interpoaltor @cite , (2) the fast radial basis functions @cite @cite , (3)least square interpolator @cite , (4)least mean-square error interpolato @cite , (5) fast Gaussian summation @cite , (6) Kaiser-Bessel function @cite , (7) linear system transfer function or inverse reconstruction @cite @cite .
- NUFFT has been accelerated on single and multiple GPUs. Fast iterative NUFFT using the Kaiser-Bessel function was accelerated on GPU with total-variation regularization @cite and generalized total-variation regularization @cite . A real-time inverse reconstruction is developed in @cite and @cite , but this inverse reconstruction does not perform the full interpolation and gridding during iterations. The patent of @cite describes a custom multi-GPU buffer to improve the memory access for image reconstruction with non-uniform @math -space. An mripy package applies the Numba compiler in its NUFFT with Gaussian kernel (https: github.com peng-cao mripy).
- Despite different methodological tools (SVM, subspace learning, autoencoders, etc), existing methods approach DG based on a few different intuitions. One is to project the data to a new domain invariant representation where the differences between training domains is minimized @cite @cite , with the intuition that such a space will also be good for an unseen testing domain. Another intuition is to predict which known domain a testing sample seems most relevant to, and use that classifier @cite . Finally, there is the idea of generating a domain agnostic classifier, for example by asserting that each training domain's classifier is the sum of a domain-specific and domain-agnostic weight vector @cite . The resulting domain-agnostic weight vector can then be extracted and applied to held out domains. Our approach lies in this latter category. However, prior work in this area has dealt with shallow, linear models only. We show how to extend this intuition to end-to-end learning in CNNs, while limiting the resulting parameter growth, and making the sharing structure richer than an unweighted sum.
- There has been more extensive work on CNN models for domain , with methods developed for encouraging CNN layers to learn transferable features @cite @cite . However, these studies have typically not addressed our domain setting. Moreover, as analysis has shown that the transferability of different layers in CNNs varies significantly @cite , these studies have had carefully hand designed the CNN sharing structure to address their particular DA problems. In our benchmark, this is harder, as the gaps between our more diverse domains are unknown and likely to be more variable. However, our low-rank modeling approach provides the benefit of automatically estimating both the per-domain and per-layer sharing strength.
- The most popular DG benchmarks are: Office' @cite (containing Amazon Webcam DSLR images), later extended to include a fourth Caltech 101 domain @cite (OfficeCaltech) and Pascal 2007, LabelMe, Caltech, SUN09 (VLCS) @cite @cite . The domains within Office relate to different camera types, and the others are created by the biases of different data collection procedures @cite . Despite the famous analysis of dataset bias @cite that motivated the creation of the VLCS benchmark, it was later shown that the domain shift is much smaller with recent deep features @cite . Thus recent DG studies have used deep features @cite , to obtain better results. Nevertheless, we show that a very simple baseline of fine-tuning deep features on multiple source domains performs comparably or better than prior DG methods. This motivates our design of a CNN-based DG method, as well as our new dataset (Fig ) which has greater domain shift than the prior benchmarks. Our dataset draws on non-photorealistic and abstract visual domains which provide a better motivated example of the sort of relatively sparse data domain where DG would be of practical value.
- Non-photorealistic image analysis is a growing subfield of computer vision that extends the conventional photo-only setting of vision research to include other visual depictions (often more abstract) such as paintings and sketches. Typical tasks include instance-level matching between sketch-photo @cite @cite , and art-photo domains @cite , and transferring of object recognizers trained on photos to detect objects in art @cite @cite . Most prior work focuses on two domains (such as photo and painting @cite @cite , or photo and sketch @cite @cite ). Studies have investigated simple blind' transfer between domains @cite , learning cross-domain projections @cite @cite , or engineering structured models for matching @cite . Thus, in contrast to our DG setting, prior non-photorealistic analyses fall into either cross-domain instance matching, or domain adaptation settings. To create our benchmark, we aggregate multiple domains including paintings, cartoons and sketches, and define a comprehensive domain-generalization benchmark covering a wide spectrum of visual abstraction based upon these. Thus in contrast to prior DG benchmarks, our domain-shifts are bigger and more challenging.
- Standard compressive sensing with partial circulant matrices. In standard (unquantized) compressive sensing, the task is to recover an (approximately) sparse vector @math from measurements @math , where @math with @math . A number of reconstruction algorithms have been introduced, most notably @math -minimization which computes the minimizer of [ z ^N |z |_1 subject to A z = Ax. ] The ( @math -)restricted isometry property is a classical way of analyzing the performance of compressive sensing @cite . The restricted isometry constant @math is defined as the smallest constant @math such that If @math then all @math -sparse signals can be reconstructed via @math -minimization exactly, see e.g. @cite @cite . Stability under noise and sparsity defect can be shown as well and similar guarantees also hold for other reconstruction algorithms @cite . It is well-known that Gaussian random matrices satisfy @math with probability at least @math if @math [Chapter 9] fora13 .
- The situation that @math is a subsampled random circulant matrix (see below for a formal definition) has been analyzed in several contributions @cite @cite @cite @cite @cite @cite . The best available result states @cite that a properly normalized (deterministically) subsampled random circulant matrix (generated by a Gaussian random vector) satisfies @math with probability at least @math if [ m ^ -2 s ( ^2(s) ^2(N) + (1 )). ] The original contribution @cite by Romberg uses random subsampling of a circulant matrix and requires slightly more logarithmic factors, but is able to treat sparsity with respect to an arbitrary basis. In the case of randomly subsampled random convolutions and sparsity with respect to the standard basis, stable and robust @math -sparse recovery via @math -minimization could recently be shown via the null space property @cite in @cite in a small sparsity regime @math under the optimal condition We note that the proof in @cite provides the lower RIP-bound in and may be extended to show the lower @math RIP bound in below under condition .
- Non-uniform recovery results have been shown in @cite @cite @cite which require only @math measurements for exact recovery from (deterministically) subsampled random convolutions via @math -minimization.
- The pair-wise co-occurrence based methods @cite @cite @cite typically construct a co-association matrix by considering the frequency that two data samples occur in the same cluster among the multiple base clusterings. The co-association matrix is then used as the similarity matrix for the data samples, upon which some clustering algorithms can thereby be performed to obtain the final clustering result. Fred and Jain @cite first introduced the concept of the co-association matrix and proposed the evidence accumulation clustering (EAC) method, which applied a hierarchical agglomerative clustering algorithm @cite on the co-association matrix to build the consensus clustering. To extend the EAC method, @cite took the cluster sizes into consideration and proposed the probability accumulation method. @cite dealt with the uncertain entries in the co-association matrix by first labeling them as unobserved, and then recovering the unobserved entries by the matrix completion technique. @cite proved that the spectral clustering of the co-association matrix is equivalent to a weighted version of @math -means, and proposed the spectral ensemble clustering (SEC) method to effectively and efficiently obtain the consensus result.
- The graph partitioning based methods @cite @cite @cite generally construct a graph model for the ensemble of multiple base clusterings, and then partition the graph into several disjoint subsets to obtain the final clustering result. Strehl and Ghosh @cite solved the ensemble clustering problem by using three graph partitioning based algorithms, namely, cluster-based similarity partitioning algorithm (CSPA), hypergraph partitioning algorithm (HGPA), and meta-clustering algorithm (MCLA). Fern and Brodley @cite formulated a bipartite graph model by treating both clusters and data samples as nodes, and partitioned the graph by the METIS algorithm @cite to obtain the consensus result. @cite dealt with the ensemble clustering problem by sparse graph representation and random walk trajectory analysis, and presented the probability trajectory based graph partitioning (PTGP) method.
- The median partition based methods @cite @cite @cite typically formulate the ensemble clustering problem into an optimization problem which aims to find the median partition such that the similarity between the base partitions (i.e., base clusterings) and the median partition is maximized. The median partition problem is NP-hard @cite . To find an approximate solution, @cite cast the median partition problem into a maximum likelihood problem and solved it by the EM algorithm. Franek and Jiang @cite reduced the ensemble clustering problem to an Euclidean median problem and solved it by the Weiszfeld algorithm @cite . @cite formulated the ensemble clustering problem into a binary linear programming problem and obtained an approximate solution based on the factor graph model and the max-product belief propagation @cite .
- There are large amount of work introducing orthogonality to the weight matrix @cite @cite @cite @cite @cite @cite @cite in deep neural networks to address the gradient vanish and explosion problem. Solving the problem with such orthogonality constraint is usually limited to the hidden-to-hidden transformation in Recurrent neural networks @cite @cite @cite @cite . Some work also consider orthogonal weight matrix in feed forward neural networks @cite @cite @cite , while their solutions introduce expensive computation costs.
- Normalizing the activations @cite @cite @cite in deep neural networks have also been studied. Batch normalization @cite is a famous and effective technique to normalize the activations. It standardizes the pre-activation of each neuron to zero-mean and unit-variance over each mini-batch. Layer normalization @cite computed the statics of zero-mean and unit-variance over all the hidden units in the same layers, targeting at the scenario where the size of mini-batch is limited. Division normalization @cite is proposed from a unified view of normalization, which includes batch and layer normalization as special cases. These methods focus on normalizing the activations and are data dependent normalization, while our method normalizing the weights and therefore is data independent normalization. Based on the fact that our method is orthogonal to these methods, we provide analysis and experimental results showing that our method can improve the performance of batch normalization by combining them together.
- Concurrent to our work, Cho and Lee @cite propose to optimize over Grassmann manifold, aiming to improve the performance of neural networks equipped with batch normalization @cite . The differences between their work and our work are in two aspects: (1) they only use the traditional Riemannian optimization method ( Riemannian gradient + exponential maps' @cite ) to solve the constraint optimization problem, which introduce non-trivial commutation cost; while we consider both Riemannian optimization method ( Riemannian gradient+ retraction' @cite ) and further proposed a more general and efficient projection based weight normalization framework, which introduces negligible extra computation cost; (2) @cite requires gradient clipping technique @cite to make optimization stable and also needs tailored revision for SGD with momentum. On the contrary, our method is more general without requiring any extra tailored revision, and it can also collaborate well with other techniques of training neural networks.
- A Melnikov potential whose critical points give rise to transverse homoclinic intersections in perturbation of periodic points is introduced in @cite @cite . In @cite @cite the Melnikov potential is established in a situation where the hyperbolic part consists of a single pendulum. A geometric version of the Melnikov integrals for periodic points with codimension one manifolds is studied in @cite . The paper @cite assumes explicitly that the perturbations vanish on the manifold.
- Some other related papers are @cite @cite @cite @cite @cite @cite @cite @cite @cite .
- One of the challenges in developing a useful robotic system is designing a platform or workspace that lets the robot to effectively complete the desired task. Initial work on path planning and reachability was typically performed for simple grasp points on objects @cite , which involves creating a map representing the areas of high dexterity for the manipulators. This work was then extended to the use of reachability maps to solve the inverse reachability task @cite and @cite , where the optimal base placement was found in order to perform the desired grasp on an object. Work done by @cite examined ways to simplify the reachability map by generating a capability map, a simplified structure that permits faster and more efficient searches of the map in order to solve the inverse reachability problem.
- To overcome the single grasp location issue, several papers extended the inverse reachability problem to solve for trajectories. Various methods have been used to search the reachability map in order to find the location where the desired trajectory can be executed with the highest level of dexterity. @cite uses sampling of the trajectory to find and overlay multiple base placements; @cite uses a pattern search to fit the trajectory into the area representing the field of high dexterity; and @cite uses a cross-correlation technique to fit the desired trajectory to the model of the robot reachability map. Further improvements to the reachability map method were developed by @cite to include the ability to add a transform offset from the original end-effector location, which is useful if the robot is grasping a tool with a non-zero length.
- These methods allow planning for simple tasks, but they require a task to be completely specified prior to evaluation. @cite explores the case where a specific trajectory is not given; rather the task has been simplified into a generalized workspace environment where the robot must work. Here, competing constraints are given. However the operator still evaluates and confirms the final base placement of the robot on the mobile platform. For a simple operation with few operating points, this can be done manually, but for more complex parts and tasks such as welding pipes or assembling parts in constrained spaces, a manual approach for validation may not be feasible.
- Money and Angius @cite provide an extensive literature survey on video summarization. According to the taxonomy proposend in that paper, related work can be classified into three categories: (1) internal summarization techniques; 2) external summarization techniques; and 3) hybrid summarization techniques. By definition, rely only on information provided by the video (and audio) streams of the event. These techniques extract low-level image, audio, and text features to facilitate summarization and for several years have been the most common summarization techniques. require additional sources of information, not contained in the video streams. These are usually user-based information -- information provided directly from users-- and contextual information --such as the time and location in which the video was recorded. As for , both internal and external information are analyzed, allowing to reduce the semantic gap between the low level features and the semantic concepts.
- According to Hsieh al @cite , the quantity of comments and re-tweets can represent the most exciting moments in a sport event. A highlight can be determined by analyzing the keywords in the comments and observing if the number of comments and re-tweets passes a certain threshold. Fi a o al @cite uses emotions shared by the spectators during the match via social networks to build a system capable of generating automatic highlight videos of sports match TV broadcasts. Auxiliary sources of information are TV broadcast videos, the audio, the analysis of the movement and manual annotations (when available). The system also allows for the user to query the video to extract specific clips ( attacking plays of a specific team).
- @cite , Suksai and Ratanaworabhan propose an approach that combines on-line information retrieval with text extraction using OCR techniques. This way, they are able to limit the number of false positives.
- Rui al @cite presents a method that uses audio signals to build video highlights for baseball games. It analyzes the speech of the match announcer, both audio amplitude and voice tone, to estimate whether the announcer is being excited or not. In addition, the ambient sound from the surrounding environment and the audience are also taken into considerations. Built on this work, Xiong al @cite handpicked the highlight events and analyzed the environment and audience sounds at each of those highlight events. They discovered that there exists a strong correlation between loud and buzzing noise and some major highlight events. This correlation exists in all the three sports being analyzed: baseball, golf, and soccer.
- Peng al @cite propose the Interest Meter (IM), a system able to measure users interest and thus use it to conduct video summarization. The IM takes account attention states ( eye movement, blink, and head motion) and emotion states ( facial expression). These features are then fused together by a fuzzy fusion scheme that outputs a quantitative interest score, determine interesting parts of videos, and finally concatenate them as video summaries. @cite , Conigliaro al use motion cues ( optical flow intensity and direction entropy) to estimate the excitement level of audience of a team sport event and to identify groups of supporters of different teams. @cite , these features are used to identify highlights in team sport events using mean shift clustering.
- The arrival of smartwatches and fitness bands have fueled a similar line of research in the area of private user-input, activity and context inference threats that take advantage of data available from sensors on-board these commercial wrist-wearable devices. However, unlike smartphones, as smart wearables are always carried by users on their body in the same natural position, the resulting continuous nature of sensor data available from them is more vulnerable to misuse and related inference threats more likely to succeed. Smartwatch motion sensors, similar to the smartphone case, have been exploited to infer keystrokes @cite @cite @cite @cite @cite , user-activities @cite @cite , handwriting @cite @cite and driving behavior @cite . Recently, ambient light sensors on these devices have also been used to infer private keystroke information @cite . Given this plethora of research results, it is clear that sensors on-board mobile and wearable devices pose a significant privacy threat. It is alarming though that common mobile and wearable device users are unaware of such threats @cite .
- In this work, we investigate the feasibility of a new kind of privacy threat, i.e., inferring unlock combinations of mechanical locks using wrist-wearable motion sensors, which has never been investigated before. Several modern smart locks offer a numeric keypad which can be compromised using known smartwatch-based keystroke inference techniques in the literature @cite @cite @cite @cite @cite . However, in this work we target traditional rotation-based mechanical locks which are still very popular and where existing attack techniques will not work. Blaze @cite @cite systematically examined physical and design weaknesses in both combination and pin-tumbler locks. However, our primary contribution in this work is to show how external side-channel attacks can make even a securely designed lock vulnerable.
- Previous research on many-to-one matching either focused on syntactic matching (functions with fixed arity and no special properties) @cite @cite @cite or AC matching (variadic, associative and commutative functions) @cite @cite @cite @cite @cite @cite . This research did not consider function symbols which only have some of those properties, nor sequence variables. The work of Kutsia does include sequence variables, but the focus is on theoretical aspects of one-to-one matching @cite @cite .
- To achieve maximum energy efficiency while using CNNs, System-on-Chips (SoCs) with hardware accelerators for CNN workload or more generally 2D convolutions can be considered. Such platforms can provide speed-ups by a factor of around @math and an improved energy efficiency of about @math @cite @cite . Such system perform optimal if the CNN comprises a simple and structured architecture. While this concept can provide a relief on the admissible computational effort, the strong limitations on available memory remain because any external memory would deteriorate the device's energy efficiency substantially. By removing memory-demanding, non-convolutional layers a CNN architecture is ideal to maximize the efficiency of hardware convolution accelerators @cite
- For acoustic event detection different algorithms have been presented based on Non-Negative Matrix Factorization @cite , Hidden Markov Models @cite or Recurrent Neural Networks @cite . Like in many other machine learning applications, CNNs have been proven to be the key for high classification accuracy. The advantage of such an architecture for acoustic event detection is its inherent inclusion of a temporal neighborhood since acoustic events are strongly characterized by temporal changes. Besides the mentioned CNN for acoustic event detection, so far CNNs have been used mainly for speech recognition @cite or music @cite classification tasks. These algorithms are all computationally expensive, and the current state-of-the-art also comes at the expense of having a huge number of parameters @cite . Implementing such a network on mobile devices or sensor nodes is difficult due to memory and computational restrictions on these devices. A way to reduce the complexity of CNN-based classification systems has been presented for image classification @cite and a similar structure is used in @cite .
- Research focusing on the design of a less redundant network architecture has gained much more attention recently. One of the prominent design pattern is the unit which was first introduced in the ResNet architecture @cite . The pattern is formed by two @math convolution layers with some @math convolution layers in between. The first @math convolution layer is used to reduce the number of input feature maps while the latter is used to restore the number of output feature maps. Several works such as @cite @cite @cite have incorporated the units into their network structure to reduce computation and memory consumed. Recently MobileNet architecture @cite was proposed which replaced normal convolution operation by depthwise separable convolution layers. Constituted by depthwise convolution and pointwise convolution, the depthwise separable convolution layer performs the filtering and combining steps independently. The resulting structure is many times more efficient in terms of memory and computation. It should be noted that bottleneck design or depthwise separable convolution layer is a design on a macro level of the network structure in which the arrangements of layers are investigated to reduce computation.
- As can be seen from ), the authors simplify a convolutional layer by two types of parameter sharing. The first is the sharing of right singular vectors ( @math ) across all @math input channels within the @math -th filter while the second enforces the sharing of left singular vectors ( @math ) across all @math filters. The work in @cite is closely related to ours since we avoid designing a particular initialization scheme by including a Batch Normalization step @cite . The resulting structure was easily trained from scratch with different network configurations.
- Previous work on broadband networks has often focused on characterizing services in terms of performance (e.g., link capacity and latency) from a range of platforms and vantage points, including customized home gateways @cite @cite , applications in end-user devices @cite @cite @cite @cite , Web-based tests @cite @cite @cite , and well-provisioned measurement nodes outside the access networks @cite . We use longitudinal data collected by two of these efforts, the FCC's MBA initiative and Namehelp @cite , to study broadband reliability.
- Recent work has explored the effect of network factors on user experience with applications, including VoIP @cite , Web @cite and Internet video @cite . Rather than deriving a model for user experience based on multiple factors, our work focuses on the effect of reliability on user demand. Others have started to explore the use of alternative experimental designs to evaluate user experience. @cite apply quasi-experimental design to evaluate the effect of video stream quality on viewer behavior, @cite relies on it for causal analysis of user behavior in social media. @cite explores the effect of contextual factors such as price and competition on user demand. We apply similar methods to understand the effect of service reliability on user behavior.
- Baltr = u @cite presented a study of the reliability of four mobile broadband providers in Norway using the Nornet Edge dedicated infrastructure. This work illustrates the value of end-to-end measurements to identify failures and performance events not always captured by the operators' monitoring systems. Broadband reliability has received little attention until recently. @cite discuss some of the challenges of characterizing reliability and their economic and policy implications and identify three different ways in which the reliability'' of broadband services can be measured: (1) the reliability of the service itself; (2) the reliability of network services offered by the ISP (e.g., DNS); and (3) the consistency of the service's performance. We focus on characterizing reliability in terms of the former two categories and leave the latter as future work.
- Beyond improvements in access link technology, one way to enhance the reliability of access networks is through redundancy. @cite propose a detouring approach to recover from Internet path failures. improved web availability with their system, MONET, an overlay network of multihomed proxy servers @cite . We have seen the recent introduction of consumer-grade residential gateways that support a second WAN connection (such as a 3G or 4G modem) @cite and some work exploring the performance benefits of the on-loading of broadband traffic using a 3G connection @cite . Det presented MiMBox, a system for translating between TCP and MPTCP connections at the middlebox @cite . Other works have explored the benefits of using MPTCP on mobile devices @cite @cite and the possibility of bonding multiple access links (such as DSL and cable) to increase performance @cite @cite @cite .
- In recent two years a lot of works on generative adversarial networks (GAN) have appeared. They have researched various aspects of GAN, from theory to applications, and made great improvement to the original method. Many works put their emphasis on improving the performance of GAN, by introducing new loss functions @cite @cite @cite , integrating it with other deep learning architectures @cite @cite @cite @cite , or making amendments to the original GAN with strong theoretical analysis @cite @cite @cite . A number of works also apply GAN to practical issues and solved problems in those domains @cite @cite . The purpose of our paper is to survey a new architecture of GAN which makes it possible to learn the samples with certain desired effect.
- In conversational turn-taking analysis, linguistic cues such as pause duration @cite and pitch levels @cite have been found to play a key role in turn-taking regulation. The study of de Ruiter el al. @cite revealed that syntax and semantics cues are also important in projecting the end of a speaker's turn. Non-verbal behaviors such as gaze and posture shifts have been investigated and found to be relevant to turn regulations @cite .
- Physical turn-taking refers to the process where a hybrid human-robot team take turns on a physical task. Turn-taking has been studied in robotics and HRI to regulate shared resources among team members. Those resources include time (i.e., only one person can work on the shared task at a time) and space (i.e., only one person can access the working space at a time). In the context of human robot interaction, the types and usage frequencies of various implicit communication cues have been studied in a robot-assisted assembly task @cite . The timing in multimodal turn-taking (i.e., speech, gaze, and gesture) was investigated through a collaborative Towers of Hanoi challenge accomplished by a hybrid human robot team @cite . Nonverbal cues for timing coordination in physical turn-taking were studied in manufacturing @cite . Such research focuses on turn-taking process modelling and the robot control, while neglecting predictions of the turn-taking timing and intention.
- Our use of lexical similarity neighborhoods is comparable to context windows used in word vector training @cite . Proximity of words within text and lexical similarity both serve as a filter which reveals semantics through distributional statistics of the corpus. More generally, results in manifold learning demonstrate that a weak metric such as lexical similarity can be used to extract semantic similarity through distributional statistics @cite @cite .
- The problem of constructing MSR codes, i.e., MDS codes that attain the cut-set bound (cf. ), has been explored by many researchers. In @cite , present an explicit construction for MSR codes. This construction works with the sub-packetization level @math . However, this small sub-packetization is achieved at the expense of low rate which is bounded as @math . Towards constructing high-rate MSR codes, @cite show the existence of such codes when sub-packetization level approaches infinity. Motivated by this result, the problem of designing high-rate MSR codes with finite sub-packetization level is explored in @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite and references therein. The constructions presented in @cite @cite @cite work with the sub-packetization level @math which is exponential in @math . For @math and all values of @math , @cite construct MSR codes with the sub-packetization level @math . The constructions with @math and the similar sub-packetization levels that enable exact-repair of only @math systematic nodes are also presented in @cite @cite . The construction of @cite is generalized for all possible values of @math with the sub-packetization level @math in @cite .
- The problem of devising exact repair mechanism with small repair bandwidth for known MDS codes has been studied in @cite @cite @cite @cite @cite @cite @cite . In particular, @cite @cite @cite consider the exact repair problem for the well-known Reed-Solomon codes. In @cite , Guruswami and Wootters present a framework to design linear schemes to repair RS codes and more generally scalar MDS codes, which are linear over @math . They further characterize optimal repair bandwidth for RS codes in certain regimes of system parameters. In @cite , utilizing the framework from @cite , Ye and Barg show that it is possible to construct RS codes with asymptotically optimal repair bandwidth (as the code length @math tends to @math ). The repair scheme in @cite works with a sub-packetization level of @math over a base field. Recently, construct RS codes that meet the cut-set bound in @cite . Their construction requires the sub-packetization level of @math . Furthermore, they show that such a super-exponential scaling of the sub-packetization level with the code length is necessary for linear repair schemes of scalar MDS codes to attain the cut-set bound.
- The first Convolutional Neural Networks (CNN) was introduced by LeCun @cite for hand writing recognition and could achieve outstanding performance. @cite later extented the CNN with an additional layer for Support Vector Machine (SVM) which could detect and classify traffic signs. They demonstrated excellent performance and reported classification accuracies of $98.24 More recently @cite Fully-Convolutional Networks have been shown to produce the best results for multi-label pixel-wise classification by training on overlapping patches, however a significant disadvantage is the fact that the produced output is considerably downsampled compare to the input and further processing is required. When dealing with semantic segmentation of fine structures such as the ones appearing in urban datasets this generates spurious results. Of similar performance but same shortcoming is the CRF-based approach proposed in @cite where again upsampling interpolation is required on the generated output.
- The state-of-the-art in urban reconstruction can be better categorized according to the type and scale of the input data. For a comprehensive survey of urban reconstruction of various types and scales we refer the reader to the survey by @cite . In this section, we provide a brief overview of state-of-the-art in large-scale urban reconstruction from remote sensor data, most relevant to our work.
- Many techniques have been developed to utilize a GPU as a predictable, shared computing resource. TimeGraph @cite is a real-time GPU scheduler that schedules GPU access requests from tasks with respect to task priorities. This is done by modifying an open-source GPU device driver and monitoring GPU commands at the driver level. RGEM @cite allows splitting a long data-copy operation into smaller chunks, reducing blocking time on data-copy operations. Gdev @cite provides common APIs to both user-level tasks and the OS kernel to use a GPU as a standard computing resource. GPES @cite is a software technique to break a long GPU execution segment into smaller sub-segments, allowing preemptions at the boundaries of sub-segments. While all these techniques can mitigate the limitations of today's GPU hardware and device drivers, they have not considered the schedulability of tasks using the GPU. In other words, they handle GPU requests from tasks in a predictable manner, but do not formally analyze the worst-case timing behavior of tasks on the CPU side, which is addressed in this paper.
- @cite @cite @cite modeled GPUs as mutually-exclusive resources and proposed the use of real-time synchronization protocols for accessing GPUs. Based on this, they developed GPUSync @cite , a software framework for GPU management in multi-core real-time systems. GPUSync supports both fixed- and dynamic-priority scheduling policies, and provides various features, such as budget enforcement, multi-GPU support, and clustered scheduling. It uses separate locks for copy and execution engines of GPUs to enable overlapping of GPU data transmission and computation. The pros and cons of the synchronization-based approach in general will be thoroughly discussed in .
- The self-suspension behavior of tasks has been studied in the context of real-time systems @cite @cite @cite @cite . This is motivated by the fact that tasks can suspend while accessing hardware accelerators like GPUs. @cite proposed segment-fixed priority scheduling , which assigns different priorities and phase offsets to each segment of tasks. They developed several heuristics for priority and offset assignment because finding the optimal solution for that assignment is NP-hard in the strong sense. @cite reported errors in existing self-suspension analyses and presented corrections for the errors. Those approaches assume that the duration of self-suspension is given as a fixed task parameter. However, this assumption does not comply with the case where a task accesses a shared GPU and the waiting time for the GPU is affected by other tasks in the system. In this work, we use the results in @cite @cite to take into account the effect of self-suspension in task schedulability, and propose techniques to bound the worst-case access time to a shared GPU.
- Recently, there have been efforts to understand the details of GPU-internal scheduling on some NVIDIA GPUs. @cite report that multiple GPU execution requests from different processes may be scheduled concurrently on a single NVIDIA TX1 GPU but the execution time of individual requests becomes longer and less predictive compared to when they run sequentially. @cite discuss the internal scheduling policy of NVIDIA TX2 for GPU execution requests made by threads sharing the same address space, i.e., those belonging to the same process. While these efforts have potential to improve GPU utilization and overall system efficiency, it is unclear whether their findings are valid on other GPU architectures and other types of accelerators. Although we do not consider these recently-found results in this work as our goal is to develop a broadly-applicable solution, we plan to consider them in our future work.
- Iterative Closest Point (ICP) @cite is one of the most popular scan registration algorithms used to estimate the optimal transformation between two overlapping scans. In ICP, closest points between scan @math and reference scan @math are used to obtain a closed-form solution by optimizing the sum of squared distances (usually Euclidean distance). Performing the nearest neighbor search in ICP becomes a bottleneck due to its high computational cost, however, using a K-D Tree @cite does mitigate the problem to an extent. Iterative Dual Correspondence (IDC) @cite generates corresponding points for both rotation and translation separately, with optimization done in an alternate fashion. This improves the alignment accuracy when the initial estimate has large rotational error. Iterative Closest Line (ICL) @cite , @cite , @cite is a variant of ICP in which instead of matching the points in both the scans, the query points in scan @math are matched to lines extracted from points in reference scan @math . Generalized ICP (GICP) @cite , attaches a probabilistic model in estimating the correct corresponding points by taking the covariance structure derived from the local neighborhood in the environment.
- Another widely used algorithm is Normal Distribution Transform (NDT) which was initially proposed for 2D data @cite and was later extended to three-dimensions. 3D-NDT @cite is a point-to-distribution method in which the maximum likelihood estimate (MLE) of points in @math is maximized over the distribution of points in a gaussian mixture model derived from @math . Supervoxel based NDT (SV-NDT) @cite is an extension to 3D-NDT in which segmentation on the basis of local-spatial structure is done over @math before creating a GMM model.
- The problem of scan alignment with fused sensor data as input has also been extensively researched in the past two decades. Some of the work includes incorporating color information in the ICP framework ( @cite , @cite ) by augmenting RGB color channels to 3D coordinates and by exploiting the co-registration of 3D data with the available camera imagery to associate scale invariant feature transform (SIFT) @cite or speeded up robust features (SURF) @cite features to the 3D points as in @cite .
- Mutual information based alignment methods was first proposed in @cite for multi-modality medical images. Since then literature has been filled with work inspired by mutual information, these include minimization of distribution of joint histograms @cite , data alignment from multiple modalities (different sensors) as in @cite , @cite , @cite , and @cite . In @cite , a comprehensive survey of mutual information based techniques being used in medical images is presented.
- The proposed method is closely related to FPFH @cite mainly because we use mutual information to compute the registration parameters. However, the feature selection and histogram creation method is different. In @cite high dimensional features are computed using FPFH which are then quantized into one of the precomputed codewords. There is no denying that FPFH are better descriptors of a scan than simple voxelized features like z-variance, but in a MI based framework, we don't need to find correspondences between patches, therefore computing FPFH only increases the run-time complexity due to nearest neighbor search and normal computation at each point. Moreover, unlike voxelized features, FPFH fails to take into account the empty region in the scan.
- A number of approaches have been proposed that are aimed at learning relation vectors for a given set of word pairs ( @math , @math ), based on sentences in which these word pairs co-occur. For instance, introduced a method called Latent Relational Analysis (LRA), which relies on first identifying a set of sufficiently frequent lexical patterns and then constructs a matrix which encodes for each considered word pair ( @math , @math ) how frequently each pattern @math appears in between @math and @math in sentences that contain both words. Relation vectors are then obtained using singular value decomposition. More recently, proposed an approach inspired by the GloVe word embedding model @cite to learn relation vectors based on co-occurrence statistics between the target word pair @math and other words. Along similar lines, learn relation vectors based on the distribution of words occurring in sentences that contain @math and @math by averaging the word vectors of these co-occurring words. Then, a conditional autoencoder is used to obtain lower-dimensional relation vectors.
- Several authors have tried to improve word embeddings by incorporating external knowledge bases. For example, some authors have proposed models which combine the loss function of a word embedding model, to ensure that word vectors are predictive of their context words, with the loss function of a knowledge graph embedding model, to encourage the word vectors to additionally be predictive of a given set of relational facts @cite @cite @cite . Other authors have used knowledge bases in a more restricted way, by taking the fact that two words are linked to each other in a given knowledge graph as evidence that their word vectors should be similar @cite @cite . Finally, there has also been work that uses lexicons to learn word embeddings which are specialized towards certain types of lexical knowledge, such as hypernymy @cite @cite , antonymy @cite @cite or a combination of various linguistic constraints @cite .
- Our method differs in two important ways from these existing approaches. First, rather than relying on an external knowledge base, or other forms of supervision, as in e.g. @cite , our method is completely unsupervised, as our only input consists of a text corpus. Second, whereas existing work has focused on methods for improving word embeddings, our aim is to learn vector representations that are to standard word embeddings.
- @cite use Convolutional Neural Networks (CNNs) to automatically estimate the spatial arrangement of landmarks in projection images. Their method utilizes a CNN to regress transformation residuals, @math , which refines the required transformation to register a source volume to a target X-ray image from an initially assumed position @math . Registration is then performed iteratively using synthetic Digitally Reconstructed Radiography (DRR) images generated from the source volume using @math . To address inaccurate transformation mappings caused by direct regression of transformation parameters, train their CNN using Pose-index features (landmarks) extracted from source and target image pairs and learn their @math . Pose-index features are insensitive to transform parameters @math yet sensitive to change in @math . This insensitivity to @math can be expressed as @math . The method requires a robust landmark detection algorithm, which is domain and scanner specific and the detection quality degrades for motion-corrupted data.
- Similarly, @cite use CNNs to perform 2D to 3D registration between a target 2D slice and a source 3D reference volume. Instead of regressing on transformation residuals, the authors regress to a scalar that estimates the dissimilarity between two image patches, and leverage the error from back-propagation to update the transformation parameters. To this end, all operations can be efficiently computed on a GPU for high throughput.
- @cite trained CNNs to regress the transformation parameters for 2D to 3D registration. Their regression is performed directly on image slices without feature extraction. The input to the network are pairs of target 2D X-ray images with synthetic DRR source images that are generated from a Cone-Beam Computed Tomography (CBCT) volume. Each image pair is augmented with varying levels of anisotropic diffusion. Iterative updates of the CNN yield new transformation parameters to generate new DRR images until similarly to the target X-Ray converges.
- The 2D 3D registration methods, by @cite , @cite and @cite , use CNNs to compute the unknown transformation of a given 2D slice with respect to a reference 3D volume while @cite @cite @cite @cite @cite @cite @cite use general registration algorithms to register slices to an initial 3D target. In both cases, a motion free reference initial volume is required for successful 2D 3D registration, which is not guaranteed to be obtainable from a clinical scan due to unpredictable subject motion. @cite proposed to perform slice-to-volume registration by minimizing the energy of weighted Mean Square Difference (wMSD) of all slice intersection intensity profiles. This method does not require an initial target registration volume nor intermediate 3D reconstructions. The authors were able to recover motion up to 15 mm of translation and 30 @math of rotation for individual slices".
- Our method also estimates slice motion without the need for volume reconstruction. However, we focus on tackling the problem of a reasonable initial slice alignment in 3D canonical space, which is not guaranteed in real scan scenarios. This goal is related to the natural image processing work of @cite who proposed PoseNet, which regresses 6-DoF camera poses from single RGB images. PoseNet is trained from RGB images taken from a particular scene, , on an outdoor street or in a corridor. The CNN is then used to infer localization within the learned 3D space. Expanding on this idea, for a given 2D image slice, we would like to infer pose, relative to a 3D atlas space, without knowing any initial information besides the image intensities.
- @cite demonstrated the potential of CNNs for tackling the volume initialization problem for slice-to-volume 3D image reconstruction. The network architecture in @cite showed promising results, initializing scan slices for fet al brain in-utero volume reconstruction and for pose estimation of DRR scan images. However, it does not provide means for estimating incorrect predictions and outlier rejection. Failing to account for grossly misaligned slices, that constitute outlying samples, hinders reconstruction performance and may result in volume reconstruction failure. We extend @cite by rigorous evaluation of several network architectures and introduce Monte Carlo Dropout @cite for the purpose of establishing a prediction confidence metric.
- The bad Nash equilibrium in Example appears in several works @cite @cite @cite @cite to show that even for two machines the price of anarchy is unbounded, thus suggesting that the notion should be refined. Among these, the , which considers NE, is studied in @cite @cite @cite . The , which considers equilibria arising in extensive form games, is studied in @cite @cite @cite @cite . In @cite the authors investigate equilibria and the resulting , while @cite focuses on the equilibria produced by the . A further distinction is between (randomized) and (deterministic) equilibria: in the former, players choose a probability distribution over the strategies and regard their expected cost, in the latter they choose deterministically one strategy. In this work we focus on pure equilibria and in the remaining of this section we write @math to denote the bounds on the price of anarchy for mixed equilibria.
- The price of anarchy is for , and grows otherwise. Specifically, @math and @math @cite , while @math @cite . The case of a is of particular interest. For and machines, @math and @math @cite , respectively. For machines, exact bounds as a function of the @math on both @math and @math are given in @cite . The price of anarchy bounds are similar to related machines: @math and @math @cite , where the analysis of @math is also in @cite . The price of anarchy and the strong price of anarchy for pure equilibria are and bounded by a : @math @cite , where the upper and lower bounds on @math can be deduced from @cite and @cite , respectively. Finally, @math @cite @cite @cite . For further results on other problems and variants of these equilibrium concepts we refer the reader to e.g. @cite @cite @cite @cite and references therein.
- Character-level neural network models are gaining interest in many research areas such as language modeling @cite , spelling correction @cite , text classification @cite and more. Most similar works from the area of character-level word representations can be found in @cite @cite @cite . In these works, word and character level representations are successfully learned and combined to improve Part-of-Speech (POS) tagging and Named Entity Recognition (NER).
- The local binary pattern (LBP) is a widely adapted local descriptor @cite . It encodes the local relationship of the pixels. Basically, it finds a binary pattern for each pixel and represent it into the histogram form. A binary pattern for a pixel consists of the 8 binary values corresponding to 8 local neighbors of the concerned pixel distributed in a circular fashion. For a neighbor, the binary value is 1 if the intensity value of neighboring pixel is greater than or equal to the intensity value of the center pixel, otherwise the binary value is 0. Originally, LBP was proposed for the texture classification @cite . However, later on, it is proved as the foundation to solve many computer vision problems @cite . Several variants of LBP have been investigated by several researchers for various problems such as local image matching @cite , content based image retrieval @cite , @cite , @cite , texture classification @cite , @cite , @cite , biomedical image analysis @cite , @cite , @cite , @cite , etc.
- @cite introduced the idea of egomotion in CNN training by concatenating the output of two parallel neural networks with two different views of the same image; at the end, this architecture learns valuable features independent of the point of view.
- In @cite , the authors concluded that sophisticated architectures compensate for lack of training. @cite explore this idea for single view depth estimation where they present a stereopsis based auto-encoder that uses few instances on the KITTI dataset. Then, @cite , @cite , and @cite continued studying the use of elaborated CNN architectures for depth estimation.
- Moving from depth to pose estimation was the next logical step. One of the first 6D camera pose regressors was presented in @cite via a general regression NN (GRNN) with synthetic poses. More recently, PoseNet is presented in @cite , where they regress the camera pose using a CNN model. In the same sense, @cite presented VidLoc, where they improve PoseNet results in offline video sequences by adding a biderctional RNN that takes advantage of the temporal information in the camera pose problem. This idea is also explored in @cite for image matching via training a CNN for frame interpolation through video sequences.
- The methods @cite are limited to the softmax layer (output layer) and are only based on random sampling, while our method does not have those limitations. The sparsely-gated mixture-of-experts @cite only sparsifies the mixture-of-experts gated layer and it is limited to the specific setting of mixture-of-experts, while our method does not have those limitations. There are also prior studies focusing on reducing the communication cost in distributed systems @cite @cite , by quantizing each value of the gradient from 32-bit float to only 1-bit. Those settings are also different from ours.
- For the deterministic setting, a lot of effort has been dedicated to the study of the feasibility of rendezvous, and to the time required to achieve this task, when feasible. For instance, deterministic rendezvous with agents equipped with tokens used to mark nodes was considered, e.g., in @cite . Deterministic rendezvous of two agents that cannot mark nodes but have unique labels was discussed in @cite @cite . These papers are concerned with the time of rendezvous in arbitrary graphs. In @cite the authors show a rendezvous algorithm polynomial in the size of the graph, in the length of the shorter label and in the delay between the starting time of the agents. In @cite rendezvous time is polynomial in the first two of these parameters and independent of the delay.
- Memory required by two anonymous agents to achieve deterministic rendezvous has been studied in @cite for trees and in @cite for general graphs. Memory needed for randomized rendezvous in the ring is discussed, e.g., in @cite .
- Several authors have investigated asynchronous rendezvous in the plane @cite @cite @cite and in network environments @cite @cite @cite . In the latter scenario it was assumed that the agent chooses the edge which it decides to traverse but the adversary controls the speed of the agent. Under this assumption rendezvous in a node cannot be guaranteed even in very simple graphs, and hence the rendezvous requirement is relaxed to permit the agents to meet inside an edge. In @cite the authors studied rendezvous in grids of two agents sharing a common coordinate system. The feasibility of asynchronous rendezvous of two anonymous agents in arbitrary graphs was discussed in @cite , both in the deterministic and in the randomized version.
- Diaz- @cite describe an algorithm for real-time recommendations called Stream-Ranking Matrix Factorization (RMFX) in the context of recommending for social media. This performs matrix factorization and ranking of recommendations on streaming data. However their system requires specifying the set of users and items in advance, which is not appropriate in our setting where we must handle new users and items (in our case new articles) all the time.
- The xStreams system of @cite does handle new users and items, however it does not incorporate the matrix factorization algorithms which provide the current state-of-the art recommendations.
- Before emojis were commonplace, there was a long history of trying to better represent and understand text, both formal and informal. One of the most influential resources is WordNet @cite , which represents words not only by their definitions, but also provides a graph of the relationship between the words. Extensions to this approach abound, but perhaps the most relevant one to our work is SentiWordNet @cite , which considers the sentiment of the words when building the resource. In the context of informal text, SlangSD @cite provides a sentiment resource for mapping slang words to sentiment scores by leveraging Urban Dictionary's data. Crowdsourcing has also been used to extract the emotional meanings for words @cite .
- As one of the fundamental tasks in NLP, linguistic sequence labeling, including POS tagging, chunking, and NER, has been studied for years. Handcrafted features were widely used in traditional methods like CRFs, HMMs, and maximum entropy classifiers @cite @cite @cite @cite , but also make it hard to apply them to new tasks or domains. Recently, getting rid of handcrafted features, there are attempts to build end-to-end systems for sequence labeling tasks, such as BiLSTM-CNN @cite , LSTM-CRF @cite , and the current state-of-the-art method in NER and POS tagging tasks, LSTM-CNN-CRF @cite . These models all incorporate character-level structure, and report meaningful improvement over pure word-level model. Also, CRF layer has also been demonstrated to be effective in capturing the dependency among labels. Our model is based on the success of LSTM-CRF model and is further modified to better capture the char-level information in a language model manner.
- Integrating word-level and character-level knowledge has been proved to be helpful to sequence labeling tasks. For example, word embeddings @cite @cite can be utilized by co-training or pre-training strategies @cite @cite . However, none of these models utilizes the character-level knowledge. Although directly adopting character-level pre-trained language models could be helpful @cite . Such pre-trained knowledge is not task-specific and requires a larger neural network, external corpus, and longer training. Our model leverages both word-level and character-level knowledge through a co-training strategy, which leads to a concise, effective, and efficient neural network. Besides, unlike other multi-task learning methods, our model has no reliance on any extra annotation @cite or any knowledge base @cite . Instead, it extracts knowledge from the self-contained order information.
- The works on meta networks adopt a two-level learning, a slow learning of a meta-level model performing across tasks and a rapid learning of a base-level model acting within each task @cite @cite . @cite proposed a kind of meta networks for one-shot classification via fast parameterization for the rapid generalization .
- @cite for the first time proposed the combination of content loss and style loss based on the pre-trained neural networks on ImageNet @cite . They approached the optimal solution image with hundreds of gradient descent iterations and produced high quality results. Then @cite proposed to use image transformation networks to directly approach the near-optimal solution image instead of gradient descent. However, it needs to train an image transformation network for each new style, which is time consuming.
- While support vector machines often work better with a small amount of labeled data without outliers @cite , Deep learning methods have produced state-of-the-art results on many classification tasks, but typically require many labeled training instances. For image classification, researchers usually compare their model by training on large datasets such as ImageNet @cite . VGG-16 network (2014) achieves 90.1 All of these models highly rely on the vast amount of labeled data. However, with transfer learning, the pre-trained weights (using ImageNet) can move to other image classification tasks @cite . But, still we need annotated data on the target domain, and alternative approaches such as LLP are required in the absence of labeled data.
- Only a handful of researchers attempt to use deep learning for LLP settings. @cite propose a model for the particular case of LLP when the label of bags are available. For example, for text classification task, when we have the label of a bag of multiple sentences, they propose a model to infer the label of each sentence using an objective function to smooth the posterior probability of samples based on sample similarity and bag constraints. They use a convolutional neural network to infer sentence similarity for text classification.
- While these methods have promising results on a particular domain, to the best of our knowledge, no method has proposed a framework that can be readily applied to diverse classification tasks. Inspired by label regularization @cite , we fill this gap by introducing Batch Averager as a regularizer layer.
- The traditional L2 regularization appears to not be sufficient for deep neural network because overfitting is a severe problem in these networks. @cite introduce a Dropout layer that randomly drops some units in backpropagation step and show that it can significantly reduce overfitting. Furthermore, the Batch Normalizer is introduced to normalize the output of cells and reduce internal covariate shift of weights @cite . Similar to these two normalization layers, our proposed Batch Averager layer applies only at training time (Batch Normalizer uses the moving average and standard deviation to normalize the output at testing time without updating the moving average and standard deviation).
- Recently, demographic classification with deep learning has been proposed by researchers. @cite offer a method to infer demographic attributes (gender) from the wild (unconstrained images). Similarly, @cite propose convolutional neural network to classify attributes such as age, gender, and race in the wild. @cite provide a fast RCNN to localize and recognize all faces in the scene with their attributes (e.g. gender, pose). Few attempts to use both textual and image features together.
- Co-training is a semi-supervised method that trains on two views of features on a small set of labeled data and iteratively adds pseudo labels from a large set of unlabeled data @cite . @cite demonstrate a co-training algorithm that trains on captioned image to recognize human actions with SVMs. Their model also learns from videos of athletic events with commentary.
- In @cite , inertial sensors were used to measure body point acceleration and orientation. The gymnast was required to wear a body suit containing ten inertial measurement units. The sensor data streams were transformed into a feature sequence for classification. For each skill, a motion template was learned. The feature sequence of the unknown trampoline motions were compared with a set of skill templates using a variant of dynamic time warping. The best accuracy achieved was 84.7 A survey of vision-based methods for general human motion representation, segmentation and recognition can be found in @cite . In @cite , judging of rhythmic gymnastics skills from video was investigated. The movement of the gymnast was tracked using optical flow. Velocity field information was extracted across all frames of a skill and projected into a velocity covariance eigenspace. Similar movements were found to trace out unique, but similar, trajectories. New video recordings were classified based on their distance from reference trajectories. The system's specificity was approximately 85
- @cite is considered the seminal work which pushed forward research on methods and representations of operating system process monitoring based on system calls. @cite provides an early comparison of machine learning methods for modeling process behavior. @cite introduces the model of execution graph, and behavior similarity-measure based on the execution graph. @cite combines multiple models into an ensemble to improve anomaly detection. @cite applies continuous time Bayesian network (CTBN) to system call processes to account for time-dependent features and address high variability of system call streams over time. @cite applies a deep LSTM-based architecture to sequences of individual system calls, treating system calls as a language model.
- Initially, only system call indices were used as features @cite @cite . @cite compares three different representations of system calls: @math -grams of system call names, histograms of system call names, and individual system calls with associated parameters. @cite proposes the use of system call sequences of varying length as features. @cite @cite investigate extracting features for machine learning from arguments of system calls. @cite studies novel techniques of anomaly detection and classification using @math -grams of system calls. @cite conducts a case study of @math -gram based feature selection for system-call based monitoring, and analyses the influence of the size of the @math -gram set and the maximum @math -gram length on detection accuracy.
- Other work attempted to detect behaviors in a semantic way, using abstract representations of behaviors based on low level events and various techniques for detection. They all carry the notion of state, keeping track of effects of previous events. @cite is the first to introduce semantics to characterize malicious behaviors. It builds behavior templates from binaries using formal semantics, which is used through a semantics-aware algorithm for detection. @cite builds multi-layered behavior graphs from low level events used through a behavior matcher. @cite uses attribute grammars for abstraction and specification, using parallel automata for parsing and detection. @cite specifies behaviors through UML activity diagrams from which one generates colored Petri Nets for detection. @cite uses first-order linear temporal logic to specify behaviors and model checking techniques for detection. @cite offers an advanced state-full approach where behaviors are specified as finite state machines. Our approach is more fine-grained and general. We model the actual operators, not the target behaviors, although the model is informed by the behaviors. We illustrate this using the example in the next section.
- Behavior recognition is closely related to plan and goal recognition @cite . Given a sequence of observed actions, the goal is to try to infer the actor's intentions. Typically, the output is a ranked list of hypothesized goals. Most work assumes a library of possible behavior instances, i.e., plans, an approach limited in its ability to go beyond known instances. Probabilistic techniques, such as @cite use Bayesian methods to assess the probability of various goals based on the actions involved. An influential recent approach is plan-recognition as planning @cite , where the authors do away with the assumption of an explicit plan library. The plan library is replaced by a model of the domain (which implicitly defines the set of possible plans), and the goal is to compute a good plan that is closest to the observed behavior. This line of work is appropriate when the observations are a subset of the actual actions taken, or when we attempt to recognize the goal before plan completion. We attempt to recognize malicious behavior off-line given a complete trace, although extensions for the online setting are natural.
- In the interdependent networks literature, the model which is closest to ours is the interdependent lattice model, first proposed in @cite and further studied in @cite @cite . In the lattice model, nodes in a network are represented by the open sites (nodes) of a square lattice, where every site is open independently with probability @math . Network links are represented by the bonds (edges) between adjacent open sites. Every node in one lattice is interdependent with one randomly chosen node within distance @math in the other lattice. The distance @math indicates the geographical proximity of the interdependence. The percolation threshold of the interdependent lattice model is characterized as a function of @math , assuming the same @math in both lattices @cite . Percolation of the model where some nodes do not need to have supply nodes was studied in @cite . The analysis relies on quantities estimated by simulation and extrapolation, such as the fraction of nodes in the infinite component of a lattice for any fixed @math , which cannot be computed rigorously. In contrast, we study the percolation of the interdependent RGG model using a mathematically rigorous approach.
- In recent years, one of the first attempts to segment hands from egocentric images was proposed by @cite . In order to determine regions containing hands and active objects, they modeled the background pixels using texture and boundary features. From the extracted foreground pixels, they distinguish between hands and objects using color histograms. Additionally, they introduced the Georgia Tech Ego-centric Activity (GTEA) dataset to test their model.
- More recently, a new egocentric dataset named EgoHands was introduced by @cite . This dataset consists of videos where a pair of persons wear camera glasses in front of each other while playing a board game. Specifically, its purpose is to detect left and right hands and their respective owner at the pixel level. The pipeline of their approach is similar to R-CNN, but they provide a probabilistic region proposal and perform a pixel-level segmentation at the end of it. Besides, they performed an activity classification of the four board games played in the dataset using images containing only the detected hands, thus preserving the original location and sizes.
- Authors are not aware of algorithms developed specifically for enumeration of diagonal Latin squares. Papers @cite @cite @cite describe the approaches that led to enumeration of Latin squares of orders 9, 10 and 11. The corresponding algorithms heavily rely on the ability to permute rows and columns to construct equivalence classes and evaluate their properties. To the best of our knowledge the results of these papers are not applicable to diagonal Latin squares because the vast majority of row-column permutations break diagonal property. Also, diagonal Latin squares form relatively small equivalence classes.
- Quite similar approach to the one used in our paper was employed in @cite . In that paper the hypothesis about the minimal number of clues in Sudoku was proven. The authors developed the fast algorithm to enumerate and check all possible Sudoku variants. The algorithm was implemented and launched on a modern computing cluster. It took about 11 months for this cluster to check all variants. The volunteer computing project Sudoku@vtaiwan @cite was used to confirm the solution of this problem.
- The KL divergence between @math and @math is represented as follows. We call @math Kullback Leibler divergence under the condition that we determine @math by reference to the fixed @math ; we call @math reverse-KL divergence @cite . Bregman divergence includes both of KL and RKL divergence @cite , so we expect it provides an unified formulation of above-mentioned algorithms.
- Let us introduce the RKL-based RL algorithms. Relative Entropy Policy Search (REPS) @cite is one of the pioneering algorithms focusing on the information loss during the policy search process. The information loss is defined as the relative entropy, also known as the RKL divergence, between the old policy and the new policy. The new policy is determined under the upper bound constraints of the RKL divergence. Episode-based REPS also considers information loss bound with regard to the upper-level policy @cite . The method is proposed as an extension of REPS to be an episode-based algorithm. The paper @cite discussed the similarity between Episode-based REPS and the proximal point algorithm; they proposed the Online-REPS algorithm as an theoretically guaranteed one. MOdel-based Relative Entropy stochastic search (MORE) also employed RKL divergence @cite , which extends the episode-based REPS to be a model-based RL algorithm. These algorithms employ RKL divergence in their formulation.
- There are some methods employing KL divergence. Trust Region Policy Optimization (TRPO) @cite , which is one of the suitable algorithms to solve deep reinforcement learning problem, updates the policy parameters under the KL divergence bound. The research @cite showed that KL divergence between policies plays a key role to derive the well-known heuristic algorithm: Co-variance Matrix Adaptation Evolutionary Strategy (CMA-ES) @cite . Authors named the method Trust-Region Co-variance Matrix Adaptation Evolution Strategy (TR-CMA-ES). TR-CMA-ES is similar to episode-based REPS but uses the KL divergence. Proximal Policy optimization (PPO) algorithm also introduces KL divergence in their penalized objective @cite .
- PI @math @cite @cite would be one of the worth mentioning RL algorithm. PI @math encouraged researchers @cite @cite to focus on the relationship between RL algorithms and black box optimization. For example, @cite proposes a reinforcement learning algorithm PI @math on the basis of black box optimization algorithm: CMA-ES. The authors @cite @cite discussed the connection between PI @math and KL control. We further discuss PI @math from a viewpoint of our proposed methods at section .
- Previous studies also proposed reinforcement learning algorithms on the basis of MD method @cite @cite . Mirror Descent TD( @math ) (MDTD) @cite is a value based RL algorithm. The paper @cite employs Minkowski distance with Euclidean space rather than KL divergence. By contrast, we basically employ the Bregman divergences on the simplex space, i.e. non-Euclidean space. Mirror Descent Guided Policy Search (MDGPS) @cite is also associated with our proposed method. They showed mirror descent formulation improved the Guided Policy Search (GPS) @cite . MDGPS has a distinctive feature that it depends both on KL divergence and RKL divergence. However, as is shown in @cite , there are the variety of Bregman divergences on simplex space other than KL divergence and RKL divergence. Moreover, it plays an important role in accelerating the mirror descent @cite . So we explicitly use Bregman divergence in this research.
- Our proposed workflow draws inspiration from prior work in decreasing image labeling cost through crowdsourcing while maintaining high annotation precision. Previous works have explored methods to speed up binary annotation tasks by minimizing penalties for worker errors @cite . Others have exploited relationships between categories for efficient multi-label annotations @cite . Methods to acquire expert level labels through crowdsourcing have also been extensively studied by investigating the use of multiple noisy workers to annotate data @cite @cite . In all these works, crowdsourcing has proved to be a reliable technique to obtain labels with expert level accuracy @cite .
- Our work is also related to previous techniques in class list generation and categorization @cite @cite . These methods use crowdsourcing algorithms to generate taxonomies for textual data @cite , and a collaboration between humans and machine learning algorithms to cluster text @cite . Here, we want to group visually indistinguishable objects together regardless of their textual taxonomy. Thus, we cannot rely on textual data to create our class list of cars. Our task also differs from taxonomy creation in that category names are known beforehand. Thus, crowd workers only have to answer questions regarding objects' visual similarity. They are not required to create category names for objects. The objects we consider might also have very small visual distinctions making the categorization prone to errors by non-experts. Thus, we introduce a new crowdsourcing algorithm to create a visual taxonomy of synthetic fine grained categories.
- Several CNN-based methods have been applied for segmenting medical images (e.g. EM @cite , brain @cite , gland @cite , and 3D MR @cite images). Yet, segmenting breast biopsy images, with a full range of diagnosis from benign to invasive, still remains a challenge. Our approach applies previous work on encoder-decoders (e.g. @cite @cite ) and improves upon them with carefully designed components that address their limitations on WSI applications.
- Though has been observed in attention training of speech recognition @cite , where the authors proposed an MTL framework that combines CTC and AN to handle this issue, our paper is the first work that formally puts forward the concept of . Furthermore, we design a focus-mechanism to solve this problem. It is worth of noting that we have tried to use CTC and AN to solve the attention drift problem in scene text recognition, unfortunately our extensive experiments showed that this idea does not work well, so we discarded it.
- There has been a flurry of approaches tailored to specific problems such as link prediction in multi-relational graphs. Examples are knowledge base factorization and embedding approaches @cite @cite @cite and random-walk based ML models @cite @cite . More recently, the focus has been on integrating additional attribute types such as text , temporal graph dynamics @cite , and multiple modalities @cite . Another line of research is concerned with extensions of the link prediction problem to multi-hop reasoning @cite . We cannot list all prior link prediction methods here and instead refer the reader to two survey papers @cite @cite . Contrary to existing approaches, we address the problem of answering visual-relational queries in knowledge graphs where the entities are associated with web-extracted images. We also address the zero-shot learning scenario, a problem that has not been addressed in the context of link prediction in multi-relational graphs.
- Image retrieval is a popular problem and has been addressed by several authors @cite @cite @cite @cite @cite . In @cite a re-ranking of the output of a given search engine by learning a click-based multi-feature similarity is proposed. The authors performed spectral clustering and obtained the final ranked results by computing click-based clusters. In @cite the authors fine-tune a DNN to rank photos a user might like to share in social media as well as a mechanism to detect duplicates. In @cite a joint user-image embedding is learned to generate a ranking based on user preferences. Contrary to these previous approaches we introduce a set of novel visual query types in a web-extracted KG with images and provide methods to answer these queries efficiently.
- We focus on exploring ways in which KGs can be used to find relationships between visual data of unseen entities, that is, entities not part of the KG during training, and visual data of known KG entities. This is a form of zero-shot learning (ZSL) where the objective is to generalize to novel visual concepts. Generally, ZSL methods ( @cite @cite ) rely on an underlying embedding space, such as one based on visual attributes, to recognize unseen categories. With this paper, we do not assume the availability of such a common embedding space but we assume the existence of an external visual-relational KG. Similar to our approach, when this explicit knowledge is not encoded in the underlying embedding space, other works rely on finding the similarities through linguistic patterns ( @cite @cite ), leveraging distributional word representations so as to capture a notion of similarity. These approaches, however, address scene understanding in a single image, these models are able to detect the visual relationships in one given image. Our approach, on the other hand, finds relationships between different images and entities.
- In addition to general studies addressing mental health, related work has also specifically studied suicide and self-harm through social media @cite @cite @cite @cite @cite . Recently, CLPsych 2016 @cite investigated approaches for detecting the self-harm risk of mental health forum posts @cite . Most related work in this area uses variations of linear classifiers with some sort of feature engineering; successful methods have employed: a combination of sparse (bag-of-words) and dense (doc2vec) representation of the target forum posts @cite , a stack of feature-rich Random Forest and linear Support Vector Machine (SVM) @cite , an RBF SVM classifier utilizing similar sets of features @cite , and various contextual and psycholinguistic features @cite @cite . In contrast to the above works, our model does not use any general or domain specific feature engineering; it learns appropriate representations of documents by considering only their textual content.
- Our proposed models consist of a shared architecture based on a CNN, a merge layer, model-specific loss functions, and an output layer (as we will describe in ). While our model shares similarities with CNN-based models in prior work @cite @cite @cite , it focuses on learning representations of user's posts and combining the post representations into an overall representation of the user's activity. In the case of self-harm risk assessment, we experiment with several loss functions to determine whether considering the ordinal nature of self-harm risk labels (i.e., green, amber, red, and crisis) can improve performance. Evaluation results suggest that the model variant using this loss function is more robust than our other variants.
- Counting via density estimation. Another method to count objects in an image is by estimating their distribution, using local features. In @cite , the authors have developed a loss function which aims to minimize Maximum Excess over SubArrays (MESA) distance. Other methods include density estimation by per-pixel ridge and random forest regression. Similar approaches can be found in @cite @cite @cite , where regressors are used to infer local densities. However, these approaches are difficult to use for leaf counting, as they are challenged by the huge scale variability of leaves, as well as heavy occlusions and overlaps.
- Direct count. Leaf counting results using machine learning solutions have been reported in past CVPPP challenges as well as in other independent reports which have identified plant datasets as compelling ways to test models. The winner of the previous CVPPP challenge @cite adopted a direct regression model through support vector regression. The method involved converting the image into a log-polar coordinate system before learning a dictionary of image features in an unsupervised fashion. The features were learned only in regions of interest determined by texture heuristics. The use of the log-polar domain provided the method with rotation and scale invariance, however the scale of the leaves is an important feature to learn, as is can vary considerably within a plant and is directly correlated with the growth stage of the plant. In @cite , the authors used a set of geometrical features to fit several classification and regression models. Using different tools available in WEKA @cite , they found that the Random Subspace method @cite could obtain lowest @math DiC @math only using geometrical features.
- Let @math be the Heawood graph (or, equivalently, the incidence bipartite graph of the Fano plane) on @math vertices shown in Figure . In @cite , Henning and Yeo proved some theorems about strong transversal in hypergraphs and then as an application of their hypergraph results they proved the following theorem.
- For question generation (QG), our work extends previous work by performing query understanding. @cite @cite joins the QG task with the QA task, but they still conduct the QG task. The only difference is that they directly optimize the QA performance rather than a general metric (such as BLEU). On the other hand, our model can conduct both tasks of QG and QA.
- For question answering (QA), most previous works focus on the extractive QA scenario, which predicts a continuous span in the passage as the answer. Obviously, they rely on the assumption that the answer can be exactly matched in the passage. On the other hand, our model performs generative QA, which generates the answer word-by-word, and does not rely on this assumption. The generative QA is valuable for studying, as we can not guarantee the assumption being true for all scenarios. @cite claims to perform generative QA, but it still relies on an extractive QA system by generating answers from the extractive results. One notable exclusion is @cite , which generate factoid answers from a knowledge base (KB). One significant difference is that their method matches the query against a KB, whereas ours performs matching against unstructured texts. Besides, we leverage policy gradient learning to alleviate the exposure bias problem, which they also suffer from.
- The main hypothesis behind this approach is that, in some cases, the optimal solutions cannot be found by simply maximizing the objective function. This is because the algorithm first needs to find stepping stones that are ineffective according to the objective function, but lead to promising solutions afterwards. A good illustration of this problem is the deceptive maze'' @cite in which following the objective function inevitably leads to a dead-end (a local extremum). The algorithm has to investigate solutions that lead the agent further from the goal before being able to find solutions that actually solve the task.
- The authors of Novelty Search also introduced the Novelty Search with Local Competition'' algorithm (NSLC) @cite , in which the exploration focuses on solutions that are both novel (according to the novelty score) and locally high-performing. The main insight consists of comparing the performance of a solution only to those that are close in the descriptor space. This is achieved with a local quality score'' that is defined as the number of the k-nearest neighboring solutions in the novelty archive with a lower performance (e.g., slower walking speed @cite ) than the considered solution. The exploration is then achieved with a multi-objective optimization algorithm (e.g., NSGA-II @cite ) that optimizes both the novelty and local quality scores of the solutions. However, the local quality score does not influence the threshold used to select whether an individual is added to the novelty archive. The final result of NSLC is the population of the optimization algorithm, which contains solutions that are both novel and high-performing compared to other local solutions. In other words, the population gathers solutions that are both different from those saved in the novelty archive, and high-performing when compared to similar types of solutions.
- The first applications of NSLC consisted of evolving both the morphology and the behavior of virtual creatures in order to generate a population containing diverse species, ranging from slow and massive quadrupeds to fast and lightweight unipedal hoppers by comparing velocity only between similar species @cite . In this experiment, the solution descriptor was defined as the height, the mass and the number of active joints, while the quality of the solutions was governed by their walking speed. At the end of the evolutionary process, the population contained 1,000 different species. These results represent the very first step in the direction of generating a collection of diverse and high-performing solutions covering a significant part of the spectrum of possibilities.
- Instead of considering the population of NSLC as the result of the algorithms, @cite suggested to consider the novelty archive as the result. Indeed, the aim of the novelty archive is to keep track of the different solution types that are encountered during the process, and thus to cover as much as possible of the entire descriptor space. Therefore, the novelty archive can be considered as a collection of diverse solutions on its own. However, the solutions are stored in the collection without considering their quality: as soon as a new type of solution is found, it is added to archive. While this procedure allows the archive to cover the entire spectrum of the possible solutions, in the original version of NSLC only the first encountered solution of each type is added to the archive. This implies that when finding a better solution for a solution type already present in the archive, this solution is not added to the archive. This mechanism prevents the archive from improving over time.
- Based on this observation, a variant of NSLC, named Behavioral Repertoire Evolution''(BR-Evolution @cite ), has been introduced to progressively improve the archive's quality by replacing the solutions that are kept in the archive with better ones as soon as they are found. This approach has been applied to generate Behavioral Repertoires'' in robotics, which consists of a large collection of diverse, but effective, behaviors for a robotic agent in a single run of an evolutionary algorithm. It has also been used to produce collections of walking gaits, allowing a virtual six-legged robot to walk in every direction and at different speeds. The descriptor space is defined as the final position of the robot after walking for 3 seconds, while the quality score corresponds to an orientation error. As we reproduce this experiment in this paper, we provide additional descriptions and technical details in section .
- The concepts introduced with BR-Evolution have also later been employed in the Novelty-based Evolutionary Babbling (Nov-EB) @cite that allows a robot to autonomously discover the possible interactions with objects in its environment. This work draws a first link between the QD-algorithms and the domain of developmental robotics, which is also studied in several other works (see @cite for overview).
- One of the main results that has been demonstrated with BR-Evolution experiments is that this algorithm is able to generate an effective collection of behaviors several times faster than by optimizing each solution independently (at least 5 times faster and about 10 times more accurate @cite ). By recycling'' and improving solutions that are usually discarded by traditional evolutionary algorithms, the algorithm is able to quickly find necessary stepping stones. This observation correlates with the earlier presented hypothesis that QD-algorithms are likely to benefit from the diversity contained in the collection to improve their optimization and exploration abilities.
- Following different inspirations from the works presented above, the Multi-dimensional Archive of Phenotypic Elites () algorithm @cite has been recently introduced. While this algorithm was first designed to illuminate'' the landscape of objective functions @cite , it showed itself to be an effective algorithm to generate a collection of solutions that are both diverse and high-performing. The main difference with NSLC and BR-Evolution is that, in MAP-Elites, the population of the algorithms is the collection itself, and the selection, mutations and preservation mechanisms directly consider the solutions that are stored in the collection.
- The behaviors contained in these collections can be seen as locomotion primitives and thus can be combined to produce complex behaviors. Following this idea, the Evolutionary Repertoire-Based Control (EvoRBC @cite ) evolves a neural network, called the arbitrator'', that selects the appropriate behavior in the repertoire, which was previously generated with MAP-Elites. This approach has been applied on a four-wheeled steering robot that has to solve a navigation task through a maze composed of several sharp angles, and a foraging task in which the robots needs to collect and consume as many objects as possible.
- However, the obligation to discretize the descriptor space may be limiting for some applications, and the uniform random selection may not be suitable for particularly large collections, as it dilutes the selection pressure. Indeed, the uniform random selection of individuals among the collection makes the selection pressure inversely proportional to the number of solutions actually contained in the collection. A simple way to mitigate this limitation is to use a biased selection according to the solution performance or according to its novelty score (like introduced by @cite @cite ). Another direction consists in having a number of cells irrespective of the dimensionality descriptor space, for example by using computational geometry to uniformly partition the high-dimensional descriptor space into a pre-defined number of regions @cite , or by using Hierarchical Spatial Partitioning @cite .
- While this definition is shared with the existing literature, we also stress the importance of the coverage regularity of the produced collections. In the vast majority of the applications presented previously, not only is the coverage of importance but its uniformity is as well. For example, in the locomotion tasks, an even coverage of all possible turning abilities of the robot is required to allow the execution of arbitrary trajectories @cite .
- In addition to direct applications, several other works focus on studying the properties of QD-algorithms. For example, @cite revealed that extinction events (i.e., erasing a significant part of the collection) increases the evolvability of the solutions @cite and allow the process to find higher-performing solutions afterwards. For example, with MAP-Elites, erasing the entire collection except 10 solutions every 100 000 generations increases the number of filled cells by $20 setups @cite .
- These different works illustrate the interest of the community in QD-algorithms and that our understanding of the underlying dynamics is only in its early stages. However, very few works compare MAP-Elites and NSLC on the same applications (the few exceptions being @cite @cite @cite @cite ), or investigate alternative approaches to produce collections of solutions. One of the goals of this paper is to introduce a new and common framework for these algorithms to exploit their synergies and to encourage comparisons and the creation of new algorithms. The next section introduces this framework.
- Finally, and due to the fact that our formulation effectively provides a convolutional network with sparse kernels, our approach is reminiscent of works attempting to sparsify the filters in deep learning models. For instance, the work in @cite showed that the weights of learned deep convolutional networks can be sparsified without considerable degradation of classification accuracy. Nevertheless, one should perpend the fact that these works are motivated merely by cheaper and faster implementations, whereas our model is intrinsically built by theoretically justified sparse kernels. We do not attempt to compare our approach to such sparsifying methods at this stage, and we defer this to future work.
- The DWT splits the input signal into two components, according to a parity of its samples. The components are often referred to as L and H. The transform can be computed by a pair of quadrature mirror filters, referred to as G, followed by subsampling by a factor of 2. Formally, this can be represented by the polyphase matrix , where operators @math and @math denote the even and odd terms of @math . This equation defines one-dimensional convolution scheme. Further, Sweldens showed @cite how the convolution scheme can be decomposed into a sequence of simple steps. These filters are referred to as the lifting steps and the scheme as the lifting scheme. The following paragraph discusses the lifting scheme in detail.
- Usually, the 2-D transform @cite is defined as the tensor product of 1-D transforms. Unlike the 1-D case, the 2-D transform splits the input signal into a quadruple of wavelet coefficients (LL, HL, LH, and HH). To describe 2-D matrices, the predict and update operators must be extended into two dimensions. Considering the separable lifting scheme, the predict and update lifting steps can be applied in both directions sequentially. It should be noted that the horizontal and vertical steps can be arbitrarily interleaved. The 2-D lifting then follow from a sequence | . | . | . | . Note the synchronization @math before the matrices. As the sequence can be hard to imagine, the individual matrices are illustrated in Figure for the CDF 5 3 wavelet @cite . For multiple lifting pairs, the scheme is separately applied to each such pair. Recall that the separable lifting scheme has the smallest possible number of arithmetic operations and the highest number of steps.
- So far, several studies have compared the performance of the separable lifting and convolution schemes on parallel architectures. In an exemplary manner, the authors of @cite compared these schemes on GPUs. Although the results of their comparison are ambiguous, they concluded that the separable convolution is more efficient than the separable lifting counterpart in most cases. They also claimed that fusing several consecutive steps might significantly speed up the execution, even if the complexity of the resulting fused step is higher. In this regard, the authors failed to consider the possibility of a partial fusion, where the number of steps is reduced but it remains greater than a single step. Other notable works can be found in @cite @cite @cite .
- This work is based on our previous work in @cite @cite . In these papers, we introduced several non-separable schemes for calculation of 2-D DWT suitable for graphics cards (GPUs). We also presented a trick leading to a reduction of arithmetic operations. The trick is also exploited in this paper. In this paper, we extend previously presented schemes to multi-core CPU platform. This is the point investigated in the following section.
- A complete overview on visual odometry can be found in @cite and @cite . Feature descriptors have been used in visual odometry for many years. But descriptors became more important in this application after @cite . They suggested using descriptors to match features between the left and right camera images and by using them to track features over time. This technique has recently become more popular and consequently, computing feature descriptors has become one of the major steps in the success of a visual odometry algorithm.
- One of the well-known and robust feature descriptors is . It has been successfully used for more than one decade in many applications including visual odometry, scene reconstruction, object recognition, etc. The descriptor vector contains 128 floating point numbers. The main issue with the descriptor is its computation and matching times. In real-time applications such as visual odometry or Simultaneous Localization and Mapping (), time limitation is a serious challenge. Moreover, in , it is needed to store feature descriptor vectors in order to find loop closure which is the main step of the path optimization. Therefore, the size of the descriptor vector is very important. The descriptor, which is a fast version of , constructs a 64-D descriptor vector and reduces the computation time compared to the descriptor. However, it still has high computation and matching time. @cite showed that using dimensionality reduction techniques such as Principal Component Analysis (PCA) or Linear Discriminant Embedding (LDE) can reduce descriptor size without any loss in recognition performance. Another way to reduce the descriptor vector size has been presented by @cite . They took advantage of a quantization method to use only 4 bits to store floating numbers of the descriptor.
- There are some approaches to construct the descriptor vector by learning a convolutional neural network such as @cite , @cite , and @cite . Most of these works tried to train a convolutional neural network works to extract the features of an input patch using an enormous amount of training patches. Then use these features as a descriptor. These descriptors are similar to the SIFT-like descriptors and The Euclidean distance reflects the features similarity. As a result, feature matching is more time-consuming than the binary descriptors.
- Movements of agents at low reactive level are assumed to be planned by some cooperative path-finding - CPF ( multi-agent path-finding - MAPF) @cite @cite @cite algorithm where agents of own team cooperate while opposing agents are considered as obstacles. In CPF the task is to plan movement of agents so that each agent reaches its unique target in a conflict free manner.
- Suboptimal CPF algorithms include rule-based polynomial time methods and search-based algorithms. Rule-based algorithms like BIBOX @cite and Push-and-Swap @cite @cite guarantee finding solution in polynomial time. These algorithms scale up well for large number of agents and large graphs however solutions generated by them are usually very far from the optimum with respect to any common objective.
- Aside from CPF algorithms, systems with mobile agents that act in the adversarial manner represent another related area. These studies often focus on patrolling strategies that are robust with respect to various attackers trying to penetrate through the patrol path @cite . Theoretical works related to APP also include studies on pursuit evasion @cite or predator-prey @cite problems. The major difference between these works and the concept of APP is that we consider relatively higher number of agents and our agents are more limited in their abilities.
- One of the more recent face image data sets is the Adience benchmark @cite , which has been published in 2014, containing 26,580 photos across 2,284 subjects with a binary gender label and one label from eight different age groups (0-2, 4-6, 8-13, 15-20, 25-32, 38-43, 48-53, 60-) , partitioned into five splits. The key principle of the data set is to capture the images as close to real world conditions as possible, including all variations in appearance, pose, lighting condition and image quality, to name a few. These conditions provide for an unconstrained and challenging learning problem: The first results on the Adience benchmark achieved @math 79.3 The first time a DNN model was applied to Adience benchmark was with @cite . The authors did resort to an end-to-end training regime, the face frontalization preprocessing from @cite was omitted and the model was completely trained from scratch, in order to demonstrate the feature learning capabilities of the neural network type classifier. The architecture used in @cite is very similar to the BVLC Caffe Reference Model @cite , with the fourth and fifth convolution layers being removed. The best reported accuracy ratings increased to @math 64 The @math 88.75
- Krueger @cite in 2015 introduced the so-called (CVST) which uses nonparametric testing together with sequential analysis in order to choose the best performing configuration on the basis of linearly increasing subsets of data. At each step, the Friedman @cite or the Cochran's Q test @cite (for regression and classification tasks respectively) are employed in order to detect statistically significant differences between configurations' performances. Then, the seemingly under-performing configurations are further tested through sequential analysis to determine which of them will be discharged. Finally, an early stopping criterion is employed to further speed up the CV process. The winner configuration is the one that has the best average ranking, based on performance, in the last few iterations specified in advance. The disadvantage of CVST is that it initially operates on smaller subsets, thus risking the early elimination of good models when the original dataset is already small.
- Truth discovery is a process to integrate multi-source noisy information by estimating the reliability of each source. A comprehensive survey on truth discovery can be found at @cite . Generally, a truth discovery task can be modeled in three ways: iterative methods @cite , optimization based methods @cite , and probabilistic graphical model based methods @cite . In this work, we formulate the falsified data detection as a truth discovery problem and propose a probabilistic graphical model.
- The sparsity characteristic sought in this letter is different than that sought in many papers using group-based penalty functions. Specifically, @cite @cite @cite @cite @cite @cite aim to promote signals that can be represented with a few groups, where within groups, the coefficients are less stringently penalized. In contrast, @cite @cite @cite @cite aim a similar characteristic as the proposed penalty. In this collection, the SWAG penalty @cite , which the proposed penalty aims to modify, separates from the rest in that it is a non-convex penalty. In @cite , it was argued that this property reduces the bias in the non-zero estimates produced by the threshold function (see also @cite @cite for related discussions). For a more detailed comparison between the SWAG penalty and the penalties in @cite @cite @cite , we refer to @cite .
- The proposed modification to the SWAG penalty aims to introduce further flexibility in forming the groups. First, groups are allowed to overlap. Second, while the original SWAG penalty in @cite uses constant weights within each group, the modified penalty allows the weights within a group to vary. These in turn allow to achieve a more localized and translation-invariant behavior, which is of interest for processing time-domain signals. However, these modifications come at an expense. While it is possible to realize the SWAG threshold function with a finite terminating procedure @cite , such a procedure is not available for the proposed penalty. Therefore, forward-backward splitting type algorithms that might utilize @math @cite @cite @cite @cite are not readily applicable for the proposed penalty. We propose instead a descent algorithm for a generic formulation that employs the proposed penalty. This algorithm is specific to the proposed penalty, and makes use of the quadratic nature of the penalty. Therefore, it has not appeared elsewhere in the literature as far as we are aware.
- Even though perceptrons have been utilized for a long time, its capacities have been rarely explored beyond discussion of linear separability. Moreover, catastrophic forgetting has so far not been explained satisfactorily. Catastrophic forgetting @cite @cite describes the effect that when the net is first trained on one set of labels and then on another set of labels, it very quickly looses its capability to classify the first set of labels. Our interpretation is that one cause for this would be a capacity overflow in the second round of training.
- One measure that handles the properties of given data is the Rademacher complexity @cite . For understanding the properties of large neural networks, @cite recently performed randomization tests. They show that their observed networks can memorize the data as well as the noise. This is proven by evaluating that their neural networks perfectly learn with random labels or with random data. This shows that the VC dimension of the analyzed networks is above the size of the used dataset. But it is not clear what the full capacity of the networks is. This observation also gives a good reason for why smaller size networks can outperform larger networks even though they have a lower capacity. Their capacity is still large enough to memorize the labeling of the data. A more elaborate extension of this evaluation has been provided by @cite . Our paper indicates the lower limit for the size of the network.
- Sequential recommendation. Markov chains are powerful methods for modeling stochastic transitions; they have been leveraged to model decision processes (e.g. @cite ) and more generally uncover sequential patterns (e.g. @cite @cite ). In the sequential recommendation domain, Rendle proposed FPMC that combines MF and (factorized) Markov chains to be able to capture personalization and sequential patterns simultaneously @cite . Our work follows this thread but extends these ideas by making use of social, in addition to sequential, dynamics.
- Social recommendation. In the recommender systems literature, there has been a large body of work that models social networks for mitigating cold-start issues in recommender systems, e.g. @cite @cite @cite @cite @cite . For example, -based methods (e.g. @cite @cite ) assume that users' preferences should be similar to those of their social circles. Given the social network information @math of user @math , this framework uses regularization to force @math 's preference factors @math to be close to those users in @math . Finally, feedback and social regularization are optimized simultaneously. Likewise, -based methods (e.g. @cite @cite ) try to find a factorization of the social network matrix such that the resulting user representation can be directly used to explain users' preferences.
- The simplest decoding algorithm is to pick the most likely character at each frame. This is commonly used to provide Character Error Rates (CER) during training of the acoustic model and can also be used to calculate Word Error Rates (WER), given that the acoustic model has a notion of word boundaries. Word boundaries can be modeled with a space symbol or by capitalizing the first letter of each word @cite . While decoding CTC acoustic models without adding external linguistic information works well, a vast amount of training data should be used to get competitive results @cite .
- The last approach presented is to treat the decoding problem as a general sequence to sequence task. For each frame the acoustic model outputs a probability distribution over all labels. This information can be processed by another CTC model @cite or by an attention based system @cite to produce a more linguistically reasonable transcription. Recent approaches combine a CTC model with an attention based mechanism and are able to train this model jointly @cite .
- As deep learning has gained a lot of interest and showed promising results in various fields of automatic speech analysis @cite , it is being actively investigated in the field of SER too. Especially, by using representation learning @cite , there has been effort to extract unsupervised features which generalise emotional speech rather than engineered features (e.g. pitch) @cite @cite . @cite @cite proposed an intrinsic way to build a high-level representation of emotion using the engineered features. They extracted segment-level engineered features (e.g. Mel-Frequency Cepstral Coefficients (MFCC) and pitch) and modelled probabilities of emotional categories using Deep Neural Network (DNN) @cite and Bi-directional LSTM (BLSTM) @cite . Then, functionals of the probabilities were used to extract utterance-level features, denoted as high-level feature representation. Extreme Learning Machine (ELM) using the utterance-level features outperformed conventional approaches such as HMM, SVM, and BLSTM. More recently, @cite showed that unsupervised representation learning has limitation in complex subsequent structures for affect compared to @cite 's approach. Therefore, we chose @cite @cite 's architectures as baselines (single task learning) and compared it to our proposal and will investigate the effectiveness of MTL in various settings.
- There are a limited number of works addressing fixed-parameter tractability in stable matching problems. Marx and Schlotter @cite gave the first parameterised complexity results on . They show that the problem is in FPT when parameterised by the total length of the ties, but is W[1] -hard when parameterised by the number of ties in the instance, even if all the men have strictly ordered preference lists. Very recently, three different works have studied hard stable matching problems from the perspective of parameterised complexity. In @cite , the authors obtained results on the parameterised complexity of finding a stable matching which matches a given set of distinguished agents and has as few blocking pairs as possible. In @cite it is shown that several hard stable matching problems, including , are W[1] -hard when parameterised by the treewidth of the graph obtained by adding an edge between each pair of agents that find each other mutually acceptable. In @cite , the authors study above guarantee parameterisations of the problem of finding a stable matching that balances the dissatisfaction of men and women, with parameters that capture the degree of dissatisfaction.
- Settings in which agents are partitioned into different types, or derive their preferences based on a set of attributes assigned to each candidate, have been considered for the problems of sampling and counting stable matchings in instances of SM or SR (see, e.g., @cite @cite @cite ). In @cite , the authors study the problem of characterising matchings that are rationalisable as stable matchings when agents' preferences are unobserved. They focus on a restricted setting that translates into assigning each agent a type based on several attributes, and assuming that agents of the same type are identical and have identical preferences. They remark that empirical studies on marriage typically make such an assumption @cite . Bounded agent types have been considered in @cite @cite to derive polynomial-time results for the coalition structure generation problem, an important issue in cooperative games when the goal is to partition the participants into exhaustive and disjoint coalitions in order to maximise the social welfare.
- Many studies organized models of different categories in a single system. The CNN @cite encodes knowledge of thousands of categories in numerous neurons. The black-box representation of a CNN is not fully chaotic. @cite made a survey of studies to understand feature representations in neural networks. For example, as shown in @cite , each filter in a convolutional layer usually encodes a mixture of visual concepts. For example, a filter may represent both the head part and the tail part of an animal. However, how to clearly disentangle different visual concepts from convolutional filters is still a significant challenge.
- Recently, there has been a growing interest in modeling high-level knowledge beyond object detection. @cite @cite mined models for different categories subcategories from web images. @cite constructed a hierarchical taxonomic relationship between categories. @cite @cite @cite @cite formulated the relationships between natural language and visual concepts. @cite further built a Turing test system. @cite modeled the contextual knowledge between objects. Knowledge in these studies was mainly defined upon object-level models ( the affordance and context). In contrast, we explore deep structures within objects. The deep hierarchy of parts provides a more informative understanding of object statuses.
- Many weakly-supervised methods and unsupervised methods have been developed to learn object-level models. For example, studies of @cite @cite @cite @cite , object co-segmentation @cite , and object discovery @cite @cite learned with image-level annotations (without object bounding boxes). In particular, @cite @cite did not require any annotations during the learning process. @cite @cite @cite @cite learned visual concepts from web images.
- However, when we explore detailed object structures, manual annotations are still necessary to avoid model drift. Therefore, inspired by active learning methods @cite @cite @cite @cite @cite , we hope to use a very limited number of human-computer QAs to learn each object pose viewpoint. In fact, such QA ideas have been applied to object-level models @cite @cite @cite . Branson @cite used human-computer interactions to point out locations of object parts to learn part models, but they did not provide part boxes. In contrast, we focus on deep object structures. We design six types of human-computer dialogues QAs for annotations (see Fig. ). Our QA system chooses questions based on the generative and discriminative losses of AOG nodes, thereby explicitly refining different AOG nodes. In experiments, our method achieved good performance when we only label parts on 3--5 objects for each pose viewpoint. Similarly, @cite used active QA to learn a semantic tree to disentangle neural activations inside neural networks into hierarchical representations of object parts.
- is closely related to the deep understanding of object statuses. Beyond the object bounding box, we can further parse the object and align visual concepts at different layers to different object parts sub-parts, which provides rich information of local appearance, poses, and viewpoints. In previous studies, many part models were designed with single-layer latent parts @cite @cite or single-layer semantic parts @cite @cite @cite @cite @cite @cite , and trained for object detection with strong supervision. @cite @cite proposed to automatically learn multi-layer structures of objects from web images, which models the object identity, object viewpoints, semantic parts and their deformation locations. Whereas, we have a different objective, weakly-supervised mining a nine-layer deep structural hierarchy of objects, which models detailed shape primitives of objects. @cite learned an interpretable CNN with middle-layer filters representing object parts, and @cite further used an explanatory tree to represent the CNN's logic of using parts for object classification. @cite learned an explainer network to interpret the knowledge of object parts encoded in a pre-trained CNN. @cite further designed an interpretable modular structure for a neural network for multiple categories and multiple tasks, where each network module is functionally interpretable.
- : and use pattern and Regex matching which rely on cue words or orthographic features for POS-tagging. TwitIE-GATE adapts rules from ANNIE @cite for extraction.
- : : The majority of the methods trained Stanford NER on tweets @cite @cite or retrained OpenNLP @cite . : use tools like Stanford NER and OpenNLP.
- : use beam search and structured perceptron for extraction and linking to Foursquare entities. However, they did not address the noise that is prevalent in such sources (e.g., my sofa'' or our house'') @cite .
- The closest works to ours are TwiNER @cite and LEX @cite . Both use Microsoft Web @math -grams (which capture language statistics) for chunking but the former uses DBpedia for entity linking. However, our method exploits a region-specific gazetteer for delimitation and linking. Moreover, LEX worked with web data and relies heavily on capitalization.
- The method most closely related to our approach is Tagger @cite , which similarly learns perceptual grouping in an unsupervised fashion using @math copies of a neural network that work together by reconstructing different parts of the input. Unlike in case of N-EM, these copies additionally learn to output the grouping, which gives Tagger more direct control over the segmentation and supports its use on complex texture segmentation tasks. Our work maintains a close connection to EM and relies on the posterior inference of the E-Step as a grouping mechanism. This facilitates theoretical analysis and simplifies the task for the resulting networks, which we find can be markedly smaller than in Tagger. Furthermore, Tagger does not include any recurrent connections on the level of the hidden states, precluding it from next step prediction on sequential tasks. RTagger @cite : a recurrent extension of Tagger that does support sequential data was developed concurrent to this work.
- Unsupervised Segmentation has been studied in several different contexts @cite , from random vectors @cite over texture segmentation @cite to images @cite @cite . Early work in unsupervised video segmentation @cite used generalized Expectation Maximization (EM) to infer how to split frames of moving sprites. More recently optical flow has been used to train convolutional networks to do figure ground segmentation @cite @cite . A related line of work under the term of multi-causal modelling @cite has formalized perceptual grouping as inference in a generative compositional model of images. Masked RBMs for example extend Restricted Boltzmann Machines with a latent mask inferred through Block-Gibbs sampling.
- Gradient backpropagation through inference updates has previously been addressed in the context of sparse coding with (Fast) Iterative Shrinkage Tresholding Algorithms ((F)ISTA; @cite @cite @cite ). Here the unrolled graph of a fixed number of ISTA iterations is replaced by a recurrent neural network that parametrizes the gradient computations and is trained to predict the sparse codes directly @cite . We derive RNN-EM from N-EM in a similar fashion and likewise obtain a trainable procedure that has the structure of iterative pursuit built into the architecture, while leaving tunable degrees of freedom that can improve their modeling capabilities @cite . An alternative to further empower the network by untying its weights across iterations @cite was not considered for flexibility reasons.
- -5pt Learning from a teacher is a well-studied problem in the literature on supervised learning @cite and imitation learning @cite @cite . However, we are not aware of any work using a teacher to control specific behaviors of a student. The typical use case of a student--teacher framework in RL is in policy compression,'' where the objective is to train a student from a collection of well-trained RL policies. Policy distillation @cite and actor--mimic @cite are two methods that distill the trained RL agents, in a supervised learning fashion, into a unified policy of the student. In contrast, we follow a completely distinct objective, where a student is continually interacting with an environment and it only uses the teacher's signals as a guideline for shaping her policy.
- Our approach was influenced by recent successes of deep learning for image segmentation. @cite proposed a fully convolutional network (FCN), where common convolutional architectures for image classification (e.g. AlexNet, the VGG net, and GoogLeNet) are used as encoders, and counterpart deconvolution layers are used for upsampling as decoders. U-Net @cite improved upon the FCN architecture by introducing so-called skip channels between encoding layers and decoding layers into the architecture, so that high-level features and low-level features are concatenated to prevent information loss along deep sequential layers. This architecture was proven successful when applied to segmentation of neuronal structures in electron microscopic images in the original paper. It was subsequently applied to several other biomedical image segmentation tasks (e.g. @cite @cite @cite ) as well as image segmentation problems in earth science (e.g. @cite ), remote sensing (e.g. @cite ), and automated driving (e.g. @cite ).
- Our transfer learning work follows the success of deep transfer learning in image and natural language processing. The design of synthetic pre-training data was in part inspired by @cite . Our fine-tuning strategies incorporated some techniques presented in the Fast.AI courses https: www.fast.ai .
- Data augmentation proved to be an important part of the research. Our time series augmentation strategies extended the previous work of @cite .
- A large amount of work has been published concerning the demand forecasting methods, for different applications (facilities, physical and online retail,...). The most widely used methods are classical times series models such as ARIMA models @cite and exponential smoothing variants @cite . However forecasting in the E-commerce space commonly needs to address challenges such as irregular sale trends, presence of highly bursty and sparse sale data, . Some of those limitations can be overcome through modified likelihood function and extended linear models @cite . But this methods fails to achieve good performance when the series are small.
- Other regression methods have been used, such as generalized additive methods @cite , support vector machines @cite and Recurrent Network @cite . All this method performs only univariate forecasting and therefore run into the same problems.
- Recently, neural networks @cite have been proposed to use cross-series information for the specific purpose of E-commerce. They adapt a Long Short-Term Memory Neural Network (LSTM) architecture to treat all the series at the same time. They also separate effects of longitudinal and temporal features and seem to have a good performance. This suggests that non-linearity is important for modelling such data.
- aims to recognize camera wearer's actions from first-person videos. Since the ego-motion is a dominant characteristic of egocentric videos, most methods used dense flow or trajectory based statistical features @cite @cite @cite @cite @cite to recognize the actions of the camera wearer. In some object-manipulated actions, some works extracted hands and objects descriptors for recognition @cite @cite @cite @cite , and others further explored gaze information according to hand positions and motions @cite @cite . Recently, deep neural networks have also been applied to egocentric action recognition. Frame-based feature series analysis showed their promising results @cite @cite @cite . CNN networks with multiple information streams were also trained on recognition task @cite @cite . However, these methods target on individual actions which are a bit different from human-human interactions.
- specifically focuses on first-person human-human interactions. Ryoo al recognized what the persons in the videos are doing to the static observer @cite @cite , but it is unrealistic in most daily life scenarios. Some works used face orientations, individual locations descriptors and hand features to recognize interactions @cite @cite . Others used motion information based on the magnitudes or clusters of trajectories and optical flows @cite @cite . A convLSTM was utilized to aggregate features of successive frames for recognition @cite . These methods commonly learned interaction descriptors by direct appearance or motion learning, but didn't considered explicit relation modeling with individual action representations of the camera wearer and the interactor. Yonetani al learned individual action features of the two persons but also lacked explicit relations modeling @cite .
- Worst-case timing analysis of applications in multi- many-core systems has long been conducted in two steps: First, a context-independent analysis is applied to bound the WCET of each task in isolation, i.e., in absence of interferences. An overview of tools and methods for context-independent WCET analysis is provided in @cite . Following the context-independent analysis, an interference analysis is performed to bound the additional latencies that may be imposed on shared resources due to external interferences.
- Multi- many-core timing analyses predominantly focus on worst-case interference analysis based on the context-independent characteristics of each task and message. For instance, the analyses presented in @cite @cite @cite @cite @cite @cite @cite @cite bound the WCRT of tasks in a multi-core setup where several cores are connected to one or more memories over shared buses. @cite @cite , a framework for multi-core response time analysis is presented for a preemptive FP core scheduling policy coupled with multiple memory bus arbitration policies, e.g., TDM , RR , and FP . Targeting mixed-criticality systems, @cite @cite analyze WCRT under a non-preemptive FP core scheduling policy and a RR memory bus arbitration. The authors of @cite @cite present response time analyses for non-preemptive FP core scheduling policy and a multi-level bus arbitration scheme which combines RR and FP policies. @cite , memory bus interference is bounded using an ILP-based timing analysis under a RR bus arbitration policy. The framework presented in @cite , analyzes memory bus interference under a variety of arbitration policies, e.g., TDM and FP . Concerning the NOC , in @cite and @cite NOC transfer delays are analyzed under FP and RR link arbitration policies, respectively.
- arbitration policies based on time slicing, e.g., TDM and WRR , offer a practical approach for predictable inter-application resource sharing without violating composability. For instance, the authors of @cite @cite consider a NOC @cite with WRR link arbitration policy and analyze worst-case NOC delays based on the reserved link budget for each communication, independent of the other communication flows that may share the same links. For WCRT analysis, @cite considers a preemptive RR core scheduling policy which, however, restricts its scope of coverage to core interferences originating from within the application under analysis only. @cite , on the other hand, a WRR core scheduling policy is considered, and based on that, a response time analysis is presented which accounts also for core interferences that may be imposed by other (currently unknown) applications. Both @cite @cite , however, consider single-core tiles and, hence, cannot capture NA and memory bus interferences that may arise in a , e.g., in @cite @cite . Contrarily, we present a timing analysis that captures also the NA and memory bus interferences in multi-core tiles.
- To bridge the gap between the complexity on deep networks and the limitations of resource-constrained devices, device-aware optimization strategies have also been presented. The work @cite introduced FINN-R to quantize and deploy a generic model into constrained FPGA architectures. Their quantization approach makes use of integer thresholds @cite @cite @cite for data compression. This method enabled a lossless integer representation of a fake-quantized networks, but demands larger memory footprint with respect to our proposed method. In contrast, the integer-only deployment in @cite presented a compact fixed-point 8 bit quantization strategy, which performs the folding of batch-normalization and scaling factors into weights before applying a uniform quantizer. Additionally, per-layer fixed-point parameters are needed for adapting the dynamic range when passing data from a layer to the next one. In contrast with this work, our methodology generalizes the deployment process when a more effective quantization strategy is used, i.e. per-channel mixed-precision quantization.
- @cite used integral images to speed up the calculation of the mean of gray levels inside a window. @cite extended the trick to calculate the standard deviation. Let @math , then @math as shown in Fig. . The integral image @math can be computed recursively using the following recurrence relation: @math . Therefore, the time complexity of Sauvola's method can be reduced to @math , which is independent of window size. However, consumption of memory increased significantly from @math to @math because the two additional integral images need to be stored.
- Counting by detection is a 2-step process that first detects the objects then counts them. Counting by regression uses regression models, such as neural networks, with the object count as a target for the loss function @cite . In Counting by segmentation the image is segmented into foreground and background, and the counts are estimated from the foreground @cite . Recently proposed Detection by regression approaches @cite @cite attempt to reconstruct an image density map @cite as well as detect the cell in microscopy images. This is the closest to our proposed segmentation approach, however, our problem is much more complex since panicles are much more heterogeneous within and across the various varieties of Sorghum compared to cells that are mostly homogeneous in shape and appearance.
- Crowd and Traffic Monitoring : Counting people in a crowd @cite @cite @cite @cite is perhaps the focus area of new counting approaches in many mainstream computer vision venues, and hence expanded in Section . There is also a good amount of literature on methods used for counting vehicles @cite @cite @cite @cite . Medical imaging : For general cell counting @cite , cell counting and detection using fully convolutional regression networks @cite , counting cells for images showing a developing human embryo @cite , and counting bacteria colonies @cite .
- Agronomy : For general plant counting @cite , counting palm trees @cite , plant stalks @cite , fruits @cite @cite and maize tassels @cite . This is a less mainstream application for counting, but a rich domain, that has real impact in people's lives, and this will be the focus of our paper.
- For agronomy applications, such as ours, understanding the crop is fundamental to developing an effective algorithm. A well executed approach that illustrates this is @cite that count tomatoes by first segmenting the image then a decision tree extracts the fruit segments. The decision tree used color, shape, texture and size features to locate the tomatoes. A similar approach was taken in @cite for counting panicles in Sorghum. This is the only other published work we are aware of for panicle counting.
- In crowd counting, a key challenge is to build effective CNN architectures that handle perspective scale distortions. The central building block for several counting systems is the counting CNN (CCNN) @cite that we also use and fine-tune in our work. The CCNN, shown in Figure , is simply a deep CNN with an image density map as a regressor. A family of models have been developed that take an ensemble approach to the scale distortion problem. For the models in the family, the two main decisions that need to be made are: how to design the components in the architecture for the different head sizes, and how to combine these components.
- A simple but elegant member of this family is the multi-column CNN (MCNN) @cite , as seen in Figure , where the component models in the ensemble are CCNNs with the receptive fields in the convolutional kernel designed for a particular head size. The resulting CCNN predictors are then combined into one density map with a fully convolutional layer. The CCNN tends to over estimate the count when the density is very low and under estimate the count near the horizon line where the density is extremely high. This was used advantageously in the DecideNet model @cite where they noted that the counting by detection approach is particularly good when the density is very low. They then used another neural network to estimate an attention map and combine the two approaches.
- While these papers used the CCNN type architecture, the popular U-net architecture has been used in @cite together with a scale consistency regularizer that forces collaborative predictions from the ensemble models. The U-net model has been widely used in medical imaging. , Ronnenberger , @cite use the U-net for cell tracking in bio-medical imaging. Examples of works that allow for very deep networks include @cite and @cite , both of which use a VGG @cite type architecture. Shi @cite utilize a multi-scale architecture, where a part of their network uses complex ensemble averaging to combine image density maps from models built for different scales. Li @cite introduce the dilated convolution to aggregate multi-scale contextual information in the CSRNet model (see Figure ). @cite extended CSRNet by adding parallel branches for the last predictive layer. This allowed for uncertainty estimation as well as a way to improve the performance through ensemble averaging. Finally, it is worth mentioning the approach taken in @cite that first estimates the head size and position in order to choose the hyper parameters for the CCNN.
- We have used the CCNN, MCNN and CSRNet models in our experiments, but it should be noted that none of these models achieve the best performance on the common crowd counting benchmarks. The CSRNet was the best performing model when published in CVPR 2018, but has since been surpassed on several benchmarks by SANet @cite , ic-CNN @cite and by ASD @cite . Table shows the performance of these 6 systems on the large and popular UCF CC 50 benchmark. The code for CCNN, MCNN and CSRNet have been released, but we are unaware of codes for SANet, ic-CNN, and ASD, which makes it almost impossible for us to make comparisons. We believe CSRNet is the best system for crowd counting that have been independently verified.
- Finding largest inscribed objects inside a polygon is not restricted to neither rectangles nor convex polygons. @cite presented @math algorithms for finding inscribed equilateral triangle and square of maximum area inside a convex polygon and an @math algorithm for finding the largest inscribed equilateral triangle inside a general polygon. @cite developed an @math algorithm for the problem of finding the largest empty rectangle among a point set. Jin proposed an @math algorithm @cite and an @math algorithm @cite for finding all locally maximal area parallelogram inside a convex polygon. Recently, @cite showed that a long lasting linear time algorithm for finding the maximal triangle inside a convex polygon proposed in 1979 by Dobkin and Snyder back @cite was in fact incorrect and then provided an @math algorithm for this problem. Later, Jin @cite proposed another linear time algorithm for the problem of finding the largest triangle inside a convex polygon.
- To our knowledge, except Amenta's model @cite for LIAR and the model of @cite for LIR, no other optimization based algorithm published so far for these two problems. Again, except Amenta's model for LIAR @cite , we are not aware of any model or algorithm for the higher dimension problem. Our optimization model for LIAR is much more efficient than Amenta's model. Also none of the algorithms in the literature are able to solve both problems, while our parametric approach is able to do so in 2-dimensional case. Moreover, except 's algorithm @cite , no algorithm is published so far for either of these problems that is capable of dealing with other geometric convex sets such as ellipses and not just polygons. In 's algorithm @cite it is not clear whether those two queries are doable for any convex set and if yes what would @math be in their algorithm's running time. Our algorithm is unique in the sense that its performance depends to the number of inequalities defining a convex set, e.g. while for a polygon this is @math for an ellipse this is one.
- For Recommender Systems, classical approaches use either collaborative filtering or content-based filtering using supervised learning. Matrix factorization @cite is a prime example of collaborative filtering, however the main problem is that it can suffer from cold-start issue with new users. This is partially solved using recurrent neural networks on the data alone, such as in @cite .
- Another classical problem is the task of Intent Prediction, which aims at finding the intention of a user during the current session. Recent methods include predicting if the user will buy during the current session using recurrent neural networks @cite . Other approaches involve splitting the intent of a user in multiple disjoint classes, such as informational, transactional, considerational or navigational, corresponding to the marketing funnel.In @cite , we have predicted these classes using only the search query that the user employed to land on the page. We used a semi supervised approach, by first creating an auto-labeling process and annotating a large amount of queries from a big corpus and train a partial model. Then, we fine tuned the model using a small subset of queries that were manually labeled, which proved better than using any of the steps individually.
- Using Google Analytics data for Machine Learning purposes is a relatively new and rarely used method, perhaps because the analytics platform's original focus is on aggregated data. The User Explorer https: support.google.com analytics answer 6339208?hl=en report, which provides insights at the user level, is relatively new. In @cite , they predict the amount of people visiting a city based on website traffic of various touristic websites. In @cite , they try to identify the demography of the users on a website, studying the evolution of location as well as device used for some time frames. In @cite , they analyzed the website traffic to sort the most visited pages for some online library. Then, based on the traffic analysis, they put more emphasis on the most visited ones, promoting them to the main page more often which resulted in a decrease of the bounce rate by a large margin. In @cite , they analyze the performance of various e-commerce sites using computed statistics of the features offered by Google Analytics.
- Visual attributes, including color, shape, and texture, have been successfully used to model clothing images @cite @cite @cite @cite @cite @cite @cite . Relative attributes (e.g., more formal than these'', shinier than these'') @cite @cite have been exploited as a richer form of feedback for interactive fashion image retrieval @cite @cite @cite @cite . In @cite , a system for interactive fashion search with attribute manipulation was presented, where the user can choose to modify a query by changing the value of a specific attribute. All these methods rely on a fixed, pre-defined set of attributes, whereas our work explores the use of feedback as relative queries in natural language , allowing more flexible and more precise descriptions of the items to be searched.
- Learning with privileged information, i.e., side information that is available at training time but not at test time, is a popular machine learning paradigm @cite , with many applications in computer vision @cite @cite . In the context of fashion, @cite showed that visual attributes mined from online shopping stores serve as useful privileged information for cross-domain image retrieval. Text surrounding fashion images has also been used as side information to discover attributes @cite , learn weakly supervised clothing representations @cite , and improve search based on noisy and incomplete product descriptions @cite . In our work, for the first time, we explore the use of side information to improve user feedback modeling and dialog-based image retrieval.
- Earlier MOT works mostly adopt hand-crafted features for object representation @cite @cite @cite @cite . Color histograms are commonly used to represent object appearance in multi-object tracking @cite @cite , and histograms of oriented gradients (HOG) @cite is also a popular choice @cite @cite . In @cite , optical flow that reflects the motion information is incorporated for object representation. In addition, appropriate fusion of multiple cues can yield improved results @cite @cite @cite . Moreover, sophisticated machine learning techniques @cite @cite are introduced to better describe object appearance models. However, conventional object representation methods are often badly affected by challenging factors like illumination variations, object deformation, background clutters, etc., which limits their performance and generalization ability to various complex scenarios.
- Recently, researchers actively learn object appearance features with deep learning based models due to their powerful representation learning ability, e.g., convolutional neural networks (CNNs) @cite @cite and recurrent neural networks (RNNs) @cite @cite . A fully convolutional neural network is adopted in @cite for object tracking, where features from top and lower layers that characterize the target from different perspectives are jointly used with a switch mechanism. In @cite , a recurrently target-attending tracking method is presented, which attempts to identify and exploit reliable parts that are beneficial for the tracking process. But these mentioned deep learning based methods mainly focus on single object tracking with the object being indicated at the first video frame. As for MOT, recently Leal- @cite exploit siamese CNN for pairwise pedestrian similarity mesurement in offline tracking, while Gaidon and Vig @cite take advantage of the convolutional features in online domain adaption between instances and category in a Bayesian tracking framework. Different from these methods, in this paper we employ a MBN network for instance-aware object representations, in which a backbone-subnet is trained with a novel multi-task loss and instance-subnets are dynamically initialized from a det-pruning-subnet and trained discriminatively online.
- To address the data association problem, existing MOT works can mainly be roughly divided into two categories: offline methods @cite @cite @cite and online methods @cite @cite @cite .
- Most MOT methods belong to the first category and process the video in an offline way, where the data association is optimized over the whole video or a span of frames and requires future frames to determine objects' states in the current frame. Network flow-based MOT methods @cite @cite are quite typical in this category, and they generally solve the MOT problem using minimum-cost flow optimization. In @cite , linking person hypotheses over time is formulated as a minimum cost lifted multicut problem. In order to track interacting objects well, @cite propose novel intertwined flows to handle this issue. Integer program is also often used for formulating data association in MOT @cite @cite . In @cite , the quadratic integer program formulation is solved to local optimality by custom heuristics based on recursive search. Mixed integer program is introduced to handle the interaction of multiple objects in @cite . In @cite , a non-Markovian approach is proposed to impose global consistency by using behavioral patterns to guide the association. These offline methods generally yield better performance by incorporating future frames into formulation and optimization, but this characteristic and the resulted high complexity also add great constraints to their application.
- The online methods only use information up to the current frame and require no deferred processing, which are more practical in real-world applications. In @cite , the data association between consecutive frames is formulated as bipartite matching and solved by structural support vector machines. @cite perform online multi-object tracking by combination of local and global association based on tracklet confidence. Recently, more sophisticated learning methods are introduced to handle this problem. In @cite , the online association is modeled by Markov Decision Process (MDP) with reinforcement learning. In @cite , RNNs are employed to learn the data association from data for online multi-object tracking. While the recent works spend costly computation in online joint association, this paper introduces an efficient solver for the online association based on the outputs of the MBN network.
- avoid the direct definition of translation @math by utilizing the convolution theorem: for any two functions @math and @math , @math . Therefore, we have @math , where @math and @math represent generalized Fourier transform and inverse Fourier transform provided through the associated Laplace-Beltrami (LB) eigensystem on manifolds. To avoid computing convolution through full eigenvalue decomposition, polynomial approximation is proposed and yields convolution as action of polynomials of the LB operator @cite @cite . Thus, convolutional neural networks can be designed @cite @cite @cite . Spectral methods, however, suffer two major drawbacks. First, these methods define convolution in the frequency domain. As a result, the learned filters are not spatially localized. Secondly, spectral convolutions are domain-dependent as deformation of the ground manifold will change the corresponding LB eigensystem. This obstructs the use of learning networks from one training domain to a new testing domain @cite .
- are more intuitive and similar to the Euclidean case, and this is one of the reasons why most of the existing works fall into this category. The philosophy behind these methods is that the tangent plane of a @math -dimensional manifold is embedded to a @math -dimensional Euclidean domain where convolution can be easily defined. In this paper, we make the first attempt to interpret some of the existing mesh-based methods in a unified framework. We claim that most of the spatial mesh-based methods can be formulated as Here, @math is a convolution kernel and @math with @math being the size of the kernel. The mapping @math is defined as where @math , @math . For simplicity, we will denote @math . Most of the designs of the existing manifold convolutions focused on the designs of @math . We remark that possible singularities will lead to no convolution operation at those points. These are isolated points on a closed manifold and do not effect experiment results. In addition, singularities from a given vector field can be overcome using several pairs of vector fields and pooling @cite .
- For example, GCNN @cite and ACNN @cite construct a local geodesic polar coordinate system on a manifold, formulating the convolution as @math where @math is a local interpolation function with interpolation domain an isotropic disc for GCNN and an anisotropic ellipse for ACNN. A local geodesic polar coordinate system on a manifold can also be transformed to a 2-dimensional planar coordinate system on its tangent plane. Such transformation is the mapping @math which is defined by the inverse exponential map: @math with @math being a point in the local geodesic polar coordinate system at @math with coordinates @math . With this, we can easily interpret ACNN within the framework of ). Indeed, ACNN essentially chooses @math as the directions of the principal curvature at point @math . For GCNN, on the other hand, it avoids choosing a specific vector field on the manifold by taking max-pooling among all possible directions of @math at each point. Such definition of convolution, however, ignores the correspondence of the convolution kernels at different locations.
- The newly proposed PTC @cite defines convolution directly on the manifold, while uses tangent planes to transport kernels by a properly chosen parallel transport. PTC can be equivalently cast into the form of ) using the inverse exponential map, and implementation of the proposed parallel transported is realized through choosing specific vector fields @math guided by a Eikonal equation for transforming vectors along geodesic curves on manifolds.
- There are mainly two types of point-based convolution. The first type is to combine the information of points directly. These methods can be formulated as where @math is a neighborhood of @math and kernel @math takes different forms in different methods. PointNet @cite is an early attempt to extract features on point cloud. PointNet is a network structure without convolution, or alternatively we can interpret the convolution defined by PointNet has the simplest kernel @math where @math is the Kronecker-Delta. Various later works attempt to improve PointNet by choosing different forms of the kernel @math . For example, PointNet++ @cite introduces a max pooling among local points, i.e. choosing kernel @math as an indicator function: @math . Pointcnn @cite chooses @math where @math and @math are trainable variables with @math . DGCNN @cite proposes an "edge convolution" that can be viewed as fixing @math and @math MLP @math , where MLP means the Multi-Layer Perceptron.
- The second type of convolution is defined by first projecting the point cloud locally on an Euclidean domain and then employ regular convolution. This type of methods can also be formulate as ). For example, Tangent convolutions @cite define kernels on the tangent plane, and use 2 principal directions of a local PCA as @math . Pointconv @cite constructs local kernels by interpolation in @math , i.e. letting @math which is essentially a local Euclidean convolution.
- The first category uses a specialized loss function or objective to recover the structure information. @cite initially suggests a multi-instance loss (MIL) function to recognize the most discriminatory region for each category. Later, @cite adds some linear constrains into an iteratively updating process to restrict the structure information of the output, leading to an improved segmentation result. On the other hand, @cite proposes a weakly-supervised learning transfer layer to discover complementary regions for the subcategories which belong to the same category, whereas @cite builds a guided attention inference network and uses an attention mining loss to constrain the output. However, this kind of methods only restricts a small part of pixels in the network output.
- Different from the first category, the methods in the second category generate pseudo-label to incorporate the structure information, and use the new generated labels to constrain all of the pixels in the network output. For example, @cite obtains the pseudo-label using CAM, @cite uses the activations from the middle layers of VGG-16 @cite to generate some candidate regions, whereas @cite sets attention map of each image as labels. @cite proposes a simple-to-complex mechanism to generate the pseudo-label iteratively. The authors then apply an erasing mechanism into their network structure and discover new regions to expand the truth area in the pseudo-label @cite . Despite substantial progress made by these methods, they always use static pseudo-label which is likely to accumulate errors incurred by initial inaccurate seeds or a dense conditional random field (DenseCRF) model @cite that is sensitive to noisy labels.
- In this paper, we introduce two feedback chains into existing weakly-supervised semantic segmentation network to form a close-loop system, which enables iterative extraction of robust structure information and dynamical correction of errors made by inaccurate seed localization. Specifically, the first feedback chain updates the seed by the network output to recover the cross-image structure, whereas the second feedback chain incorporates robust inner-image structure information characterized by a relationship matrix built upon superpixels to further expand seed regions. It is worth noting that deep seed region growing (DSRG) @cite also uses dynamic seed. However, it does not correct the inaccurate seed localization explicitly, which constrains the upper bound of segmentation performance. On the other hand, Mining Common Object Features (MCOF) @cite also suggests using superpixels to robustly rebuild inner-image structure information in the presence of noisy labels. Unlike MCOF, we use a simple relationship matrix to characterize superpixels, rather than an extra network that requires significantly higher computational and memory burden. Moreover, experiments demonstrate that our network outperforms both DSRG and MCOF with a large margin.
- Image registration refers to a process that transforms data into a shared coordinate system. For DWI, the common way to register two images is to use scalar-based methods on quantitative measures, such as the fractional anisotropy (FA) or the mean diffusivity (MD) @cite . However, as such methods disregard most of the directional information in DWI, methods have been developed to also account for the reorientation of the diffusion profile. Most of these are created on top of scalar-based methods and iteratively reorients the gradients after changing the deformation field. Some of the most popular methods can be found in @cite @cite @cite @cite . However, registration frameworks have also been designed with an objective function that explicitly optimizes over the reorientation of the gradients, such as DTI-TK @cite , DT-REFineD @cite , and the more recent DR-TAMAS @cite . These frameworks have been shown to generally outperform scalar-based frameworks for DWI @cite @cite @cite @cite .
- Imitation learning through behavioral cloning dates back to Widrow and Smith, 1964 @cite , and has remained popular through today @cite @cite @cite @cite @cite @cite @cite @cite @cite . The distributional shift problem, wherein a cloned policy encounters unfamiliar states during autonomous execution, has been identified as an issue in imitation learning @cite @cite @cite @cite @cite @cite @cite . This is closely tied to the feedback'' problem in general machine learning systems that have direct or indirect access to their own past states @cite @cite . For imitation learning, various solutions to this problem have been proposed that rely on iteratively querying an expert based on states encountered by some intermediate cloned policy, to overcome distributional shift; DAgger @cite has come to be the most widely used of these solutions.
- We also compare against : to prevent imitators from copying past actions, they train with dropout @cite on dimensions that might reveal past actions. While our approach seeks to find the true causal graph in a mixture of graph-parameterized policies, dropout corresponds to directly applying the mixture policy. In our experiments, our approach performs significantly better.
- Lots of works has researched the discounting factor in the exponentially discounted form return function. @cite proposed to generalize the exponentially discounting form through discount functions that change with the agents' age. Precisely choosing different @math at different time has been proposed in . @cite treated hyper-parameter [ @math , @math ] in TD- @math as learnable parameters that could be optimized by meta-gradient approach. @cite introduced a flexible state-action discounting factor by characterizing rationality in sequential decision making. @cite generalized the work of @cite to propose a multi-time scale TD learning model, based on which @cite @cite @cite explored to decompose the original discounting factor into multiple ones. @cite proposes to decompose the delayed return to previous time steps by analyzing the hidden states of a LSTM. All these works suggest that the form of return function is one of the key elements in RL. Our work generalizes them by proposing to use a general form return function.
- Another family of works focus on manipulating the reward functions to enhance the learning of the agents. Reward shaping shows what property a reward modification should possess to remain the optimal policy, i.e., the potential-based reward. Lots of works focus on designing intrinsic reward to help learning, e.g., used hand-engineered intrinsic reward that greatly accelerate the learning. Exploration bonus is another class of works that add bonus reward to improve the exploration behaviours of the agent so as to help learning, e.g., @cite @cite @cite propose to use the pseudo-count of states to derive the bonus reward. propose to augment the reward by considering information coming from the task itself.
- The attention mechanism has been proven successful to extract the relationships of words in sentences in natural language processing @cite @cite , and has recently been combined with relational learning to extract the relations between entities in a single state, so as to enhance learning in RL @cite . To encode the trajectory information in the general return function, we also employ the attention mechanism, but not to different entities in a single state, but to different state-action pairs in a trajectory. More details are given in Section .
- introduced looking around corners" using 5D time-resolved light transport @cite . This was later realized using streak cameras and a femto-second laser to perform elliptic backpropagation @cite @cite . Researchers demonstrated NLOS reconstruction using continuous-wave time-of-flight sensors via optimization @cite and NLOS tracking using array signal processing @cite . Single-photon avalanche diodes (SPADS) @cite have been used for NLOS reconstruction @cite @cite . Advances in reconstruction algorithms for SPADs include accounting for partial occluders and surface normals @cite , confocal NLOS reconstruction @cite , geometric modeling @cite , backpropagation @cite @cite and space-carving @cite . Higher level applications such as detection and tracking have been shown @cite @cite . Recently, neural networks have been applied to time-resolved measurements for NLOS imaging @cite @cite .
- transposed the light transport matrix to see an object in the camera's NLOS, although it is still in the projector's LOS @cite . Accidental pinholes and pinspeck cameras can visualize the NLOS scene @cite . visualize 1D and 2D slices of the NLOS scene using a physical wall corner @cite . Known occluders in the NLOS can help reconstruct the light field @cite and hidden scene @cite . used laser speckle for tracking NLOS objects @cite . Similar to this paper, demonstrate a real-time tracking algorithm using radiosity @cite , perform deep learning for conventional cameras for NLOS imaging @cite @cite , and recently show high quality reconstruction from RGB cameras using a convolutional neural network @cite . However, our work differs from the previous works by introducing the idea of adaptive lighting to improve the performance of data-driven NLOS localization, and show this for non-planar LOS scenes.
- proposed a similar to that of for . They reported results on several benchmark datasets, most notably: @cite , containing news articles; @cite , containing legal documents; Amazon-12K @cite , containing product descriptions; and Wiki-30K @cite , containing Wikipedia articles. Their proposed method outperformed both tree-based methods (e.g., , @cite ) and target-embedding methods (e.g., @cite , @cite ).
- s with self-attention have been employed in a wide variety of tasks, such as Natural Language Inference @cite , Textual Entailment @cite , and Text Classification @cite . used s with self-attention in comparing with tree-based methods and deep learning approaches including vanilla s and s . Their method outperformed the other approaches in three out of four datasets, demonstrating the effectiveness of attention-based s .
- investigated the use of label-wise attention mechanisms in medical code prediction on the and datasets @cite . and contain over 20,000 and 47,000 documents tagged with approximately 9,000 and 5,000 code descriptors, respectively. Their best method, Convolutional Attention for Multi-Label Classification, called here, includes multiple attention mechanisms, one for each one of the @math labels. outperformed weak baselines, namely logistic regression, vanilla s and s . Another important fact is that was found to have the best interpretability in comparison with the rest of the methods in human readers' evaluation.
- discuss the challenge of few-shot and zero-shot learning on the datasets. Over 50 Note that and were not compared so far with strong generic text classification baselines. Both and proposed sophisticated attention-based architectures, which intuitively are a good fit for , but they did not directly compare those models with s with self-attention @cite or even more complex architectures, such as Hierarchical Attention Networks ( s ) @cite .
- The existing work on open domain QA @cite has distinct similarities with our problem, largely owing to the overwhelming large corpus that a machine reader has to reason over. In recent years, a multitude of techniques have been developed. @cite proposed reinforcement learning to select passages using the reader as the reward. @cite proposed ranking the minimal context required to answer the question. @cite proposed shared norm method for predicting spans in the multi-paragraph reading comprehension setting. @cite proposed ranking and de-noising techniques. @cite proposed evidence aggregation based answer re-ranking. Most techniques focused on constructing a conducive and less noisy context for the neural reader. Our work provides the first evidence of diverse sampling for training neural reading comprehension models.
- State-of-the-art neural question answering models are mainly based on cross-sentence attention @cite @cite @cite @cite . Self-attention @cite @cite has also been popular for reading comprehension @cite @cite . However, its memory complexity makes it a challenge for reading long context. Notably, the truncated summary setting of the NarrativeQA benchmark have been attempted recently @cite @cite @cite @cite . However, this summary setting bypasses the difficulties of long context reading comprehension, reverting to the more familiar RC setup.
- While most of the prior work in this area has mainly focused on span prediction models @cite and or multiple choice QA models @cite , there have been recent interest in generation based QA @cite . S-NET @cite proposed a two-stage retrieve then generate framework.
- Flexible neural mechanisms that learn to point and or generate have been also popular across many NLP tasks. Our model incorporates Pointer-Generator networks @cite which learns to copy or generate new words within the context of neural summarization. Prior to Pointer Generators, CopyNet @cite incorporates a copy mechanism for sequence to sequence learning. Pointer generators have also been recently adopted for learning a universal multi-task architecture for NLP @cite .
- The majority of related work in the shared control literature focuses on the development and analysis of techniques that statically and or dynamically allocate control between human and robot partners. The main objective in these works is to improve system performance while reducing the control burden on the human operator @cite . In some applications, the autonomous partner is allocated the majority of the low-level control while the human operator acts in a supervisory role @cite while in others, the roles are reversed, and the autonomous partner takes on the supervisory role @cite . There is also related work in which neither partner acts as a supervisor and instead control is shared via a mixed-initiative paradigm. For example, researchers have developed techniques in which the autonomous partner is explicitly aware of the human operator's intention @cite as well as techniques in which the autonomous partner has an implicit understanding of the human operator's control policy @cite . While these examples aim to improve task performance, the main motivation for our work is to extend the human operator's control ability in domains where safety is a primary concern.
- For this reason, the most closely related shared control paradigms are safety, or context, aware. In this area, researchers have explored the impact of autonomous obstacle avoidance on teleoperated robots in search and rescue @cite . Additionally, safety is a particular concern when the human and robot partner are co-located, such as with autonomous vehicles @cite . Co-located partners are also common in assistive robotics where researchers have developed environment-aware smart walkers @cite to help visually-impaired people avoid dynamic obstacles.
- Related to our goal of generating autonomous policies that recreate the behavior demonstrated during system operation, there is prior work in the field of Imitation Learning (IL). Most commonly, the demonstration data is provided by a human partner @cite , though it can come from variety of sources including trajectory optimization and simulation @cite . Example data is commonly assumed to come from an expert (or optimal) controller @cite ; however, researchers also have explored techniques that use demonstrations provided by novice (or suboptimal) sources @cite . In this work we describe how Imitation Learning can be used even when . We further describe how our safety-aware shared control algorithm can be used to address the covariate shift problem @cite , a common issue in Imitation Learning that stems from the fact that the data used to train the policy may come from a different distribution than the data observed at runtime.
- This work is connected to the plan recognition literature, since it involves intended recognition @cite @cite , that is, the actor is aware of the observers in the environment, and takes actions that either explicitly convey information or hide information from the observers. Then the observers can use the emitted observations to perform goal recognition. There are several prior works on goal plan recognition @cite @cite @cite @cite . However, most of these recognition systems assume one-to-one mapping of observation symbols to actions or states, which complicates the goal recognition when the sensor model supports noisy many-to-one mapping of observations. In our framework, we assume the observer constructs a sequence of belief updates to derive a list of possible goals in the final belief. In the adversarial case, this list does not reveal information about the actor's true goal, whereas, in the cooperative case, this list is indeed what the actor wants to convey to the observer.
- The problem of computing a minimum 2-ECSS is known to be NP-hard, by a reduction from the Hamiltonian cycle problem @cite . This already shows that 2-ECSS is much harder than the closely related problem of MST, which admits a linear-time (randomized) centralized exact algorithm @cite . The best known centralized (polynomial-time) approximation algorithms for 2-ECSS give a @math -approximation @cite @cite . These algorithms also give a @math -approximation for the more general minimum-weight @math -edge-connected spanning subgraph ( @math -ECSS). If all edge-weights are equal (i.e., in the unweighted case), then the best known approximation factors become @math and @math , respectively, for 2-ECSS @cite and @math -ECSS @cite .
- Recent interest in distributional methods for RL has grown with the introduction of deep RL approaches for learning the distribution of the return. @cite presented the C51-DQN which partitions the possible values @math into a fixed number of bins and estimates the p.d.f. of the return over this discrete set. @cite extended this work by representing the c.d.f. using a fixed number of quantiles. Finally, @cite extended the QR-DQN to represent the entire distribution using the Implicit Quantile Network (IQN). In addition to the empirical line of work, @cite and @cite have provided fundamental theoretical results for this framework.
- Our approach uses an adaptive approach to finding the optimal action. @cite discretize the action space and introduce an Ordinal Distribution Network Architecture' which considers the value of each discrete action.
- The work that is closest to ours is @cite , where the problem was studied for a set of anonymous, oblivious and asynchronous robots in a polygon. The problem asks to design a distributed movement strategy for @math robots, so that eventually at least two of them come to see each other and become mutually aware'. by two robots in polygons have been studied in @cite @cite @cite @cite , where the two robots have to meet at a point or get arbitrarily close to each other. However, their model is significantly different from @cite . Another problem related to our map construction problem is that of constructing the visibility graph of a polygon by mobile robots @cite @cite @cite @cite . The paper is organized as follows. In Section , the basic model and relevant definitions are presented. In Section , a brief overview of the positional encoding technique is presented. Then in Section , we describe our main algorithm. In Section , we briefly discuss how our algorithm and the techniques used in it can be used to solve some distributed coordination problems in polygonal environment.
- Security management has been investigated in various research fields including computer networks @cite , communications @cite , cloud computing @cite and infrastructures @cite . With the advances in ICTs, a growing number of works have focused on the emerging critical issue of IoT security @cite @cite . Due to the interconnectivity between different agents, the security of one agent is also dependent on its connected ones which gives rise to the notion of interdependent security'' @cite . The authors in @cite @cite @cite @cite have further investigated the security interdependencies in multilayer cyber-physical systems. The authors in @cite @cite @cite have developed optimal contracts to address the cyber-physical security issues in IoT. In @cite @cite @cite , the authors proposed optimal schemes for designing secure and resilient multi-layer IoT networks through graph-theoretical approaches.
- Games over networks have caught a lot of attention recently especially from the economics perspective @cite @cite @cite . The couplings between players in the network can be either in a strategic exclusive or strategic complement manner. Based on the features of security management in IoT, our problem falls into the latter class. For the engineering applications, the authors in @cite @cite have studied the resource allocation game over interdependent critical infrastructures where both players aim to increase the connectivity of the network. Huang @cite @cite @cite @cite have adopted a stochastic Markov game model to design resilient operating strategies for multilayer networks. Zhu @cite have proposed a game-theoretic framework for collaborative intrusion detection systems through resource management to mitigate network cyber threats. Our work differs from @cite in that we take into account the cognitive factors of human behaviors during decision making.
- Humans with limited knowledge or cognitive resources are bounded rational, since they cannot pay attention to all the information @cite @cite . Gabaix has proposed a sparse max'' operator to model the limited attention of players in which each agent builds a simplified model of the network based on an @math norm @cite . Built upon @cite which includes some preliminary results, our work leverages on the established sparse max'' operator and formulates a constrained game program to capture the bounded cognition ability of players in the IoT. In addition, we further consider the risk management of each user based on their underperceived cyber risks over the network.
- @cite proposed a content-based phishing website detection method called CANTINA. In their proposed framework, the tf-idf score of each term on the web page and generated lexical signatures based on the top five of tf-tdf scores are utilized for deciding about the classification. Then, the lexical signature is provided to a search engine like www.Google.com to look for additional data. If the search query returns the domain name matching the website under consideration, then it is classified as legitimate; otherwise it is classified as a phishing website.
- @cite proposed CANTINA+ in which they have used 14 different features categorized in high level webpage features, HTML features, and web-based features. They have applied six different machine learning algorithms on a sample dataset and reported that Bayesian network outperformed the other techniques. As a major drawback, their approach is not resilient to popular attacks such as cross-site scripting attacks.
- @cite proposed a data mining-based approach for phishing URL classification. Their Multi-label Classifier based Associative Classification (MCAC) algorithm functions in three distinct steps: 1) Rules discovery, 2) Classifier building, and 3) Class assignment. In the first step, the algorithm iterates over the training data and uncovers the distinct and salient features. In step two, the rules are sorted in order of confidence, length and support to define the classification directive. Finally, in step three, the URLs are classified using the rules with higher support and confidence. The authors extracted 16 different features from their sample URLs and tested their algorithm on 1350 websites with 601 legitimate and 752 phishing sites.
- On the other hand, in @cite proposed deep-learning based approach for thermal infra-red object tracking. To overcome the scarcity of thermal images dataset, they utilised DA based on the CycleGAN architecture to transform images from visual domain to the thermal infra-red domain.
- Recently, proposed SqueezeSeg @cite , a novel approach for the semantic segmentation of a LiDAR point cloud represented as a spherical range-image @cite . This representation allows to perform the segmentation by using simple 2D convolutions, which lowers the computational cost while keeping good accuracy. The architecture is derived from the SqueezeNet image segmentation method @cite . The intermediate layers are "fire layers", layers made of one squeeze module and one expansion module. Later on, the same authors improved this method in @cite by adding a context aggregation module and by considering focal loss and batch normalization to improve the quality of the segmentation. A similar range-image approach was proposed in @cite , where a Atrous Spatial Pyramid Pooling @cite and squeeze reweighting layer @cite are added. These range-image methods succeed in real-time computation. However, their results often lack of accuracy which limits their usage in real scenarios.
- In next section, we propose RIU-Net: an adaptation of U-net @cite for the semantic segmentation of point clouds represented as range-images.
- Gaussian Processes (GPs) @cite and Conditional Random Fields (CRFs) @cite are staples for (SOP) regression and classification. However, they have serious shortcomings when inference has to scale to high dimensional data. Moreover, they do not generally allow for exact marginalization. Deep mixtures of GPs have been introduced in @cite . However, while partially alleviating GP inference scalability issues, they are limited to continuous domains and with CSPNs we directly tackle , i.e., the scenario where @math is multivariate. In a nutshell, CSPNs might be seen as an efficient way to aggregate univariate (leaf) predictor to tackle SOP in a principled probabilistic way.
- Examples in improving VAEs with adversarial learning include @cite , which allows the VAEs to take implicit encoder distribution, and adversarial auto-encoder @cite and Wasserstein auto-encoder @cite , which drop the mutual information term from the ELBO and use adversarial learning to match the aggregated posterior and prior. Examples in allowing GANs to perform inference include @cite and @cite , which use GANs to match the joint distribution @math defined by the encoder and the one @math defined by the generator. However, they often do not provide good data reconstruction. Examples in using VAEs or maximum likelihood to help GANs resist mode collapse include @cite @cite @cite . Another example is VAEGAN @cite that combines unit-wise likelihood at hidden layer and adversarial loss at original space, but its update of the encoder is separated from the GAN mini-max objective. On the contrary, IntroVAE @cite retains the pixel-wise likelihood with an adversarial regularization on the latent space. Sharing network between the VAE decoder and GAN generator in VAEGAN and IntroVAE, however, limit them to model a single modality.
- Different components of dialogue systems have previously been used for zero-shot application, e.g., intention classifiers @cite , slot-filling @cite , and dialogue policy @cite . For language generation, propose single encoder-decoder models for zero-shot machine translation, and propose cross-domain zero-shot dialogue generation using action matching. Moreover, few-shot learning in natural language applications has been applied in semantic parsing @cite , machine translation @cite , and text classification @cite with meta-learning approaches @cite @cite . These tasks usually have multiple tasks to perform fast adaptation, instead in our case the number of existing domains are limited. Lastly, several approaches have been proposed for continual learning in the machine learning community @cite @cite @cite @cite @cite , especially in image recognition tasks @cite @cite . The applications within NLP has been comparatively limited, e.g., for opinion mining, for document classification, and for hybrid code networks @cite .
- Surveys of multi-agent intelligence research can be found here @cite @cite @cite @cite @cite . Different ideas have been explored on competitive and collaborative multi-agent settings, respectively.
- 3D human pose estimation has basically been approached in two ways. The first way is to decompose the problem into two steps where the first step estimates 2D from RGB, and the second step lifts 2D to 3D. @cite demonstrate very promising result with a simple multi-layer perceptron using 2D skelet al joints as the only input. In similar work, @cite propose to estimate relative depth from skeleton label map @cite . More recently, @cite explore different input representations and establish a very solid system using color-encoded segmentation alone. The performance of these methods is limited, though, owing to the inherent depth ambiguity problem from 2D-3D lifting. @cite argue that generating multiple hypotheses is more reasonable provided this fundamental depth ambiguity nature. We take inspiration from the representation in @cite and merge it with original RGB cue.
- Another line of works directly regress 3D from RGB image usually featuring a powerful end-to-end deep learning architecture. The major difference from the previous direction lies in the inclusion of RGB image cue where the image appearance also contributes to the estimation of 3D joints. @cite is the first to employ fully convolutional network in 3D human pose. Later @cite showcase a FCN network with volumetric representation. A recent work @cite power this representation with joint training strategy and a strong ResNet-based architecture. @cite regress a novel representation called orientation map by virtue of fully convolutional network. This method then binds orientation together with each limb region, which better associates image regions and 3D predictions. We draw on the success of this orientation representation and association.
- Combining local part appearance with holistic image has been proven beneficial in 2D pose @cite @cite @cite @cite @cite . @cite capture spatial relationship within image patches of different parts with DCNN and graphical model. @cite crop features around coarse 2D keypoint prediction to regress 2D offset. In the scenario of 3D pose, however, few works have explicitly processed the information of local part. @cite extract local regional feature map. @cite @cite @cite perform local 3D refinement from local view in depth image. Different from these works that only utilize depth image as input, we capitalize on local RGB and segmentation cues.
- To make use of the low-resolution local image patch, a common strategy is to recover high-resolution map via subsequent upsampling. @cite refine parsing result by adaptively zooming in local region. Lin @cite integrate low-resolution semantic features with fine-grained low-level features to generate high-resolution semantic feature maps. We choose the simple upsampling operation to recover high resolution from low resolution.
- A myriad of methods embed refinement into their pose estimation architectures. @cite @cite @cite @cite @cite @cite improve 2D keypoint estimation accuracy with multi-stage architecture. @cite @cite bring better 3D prediction by repetitive projection and reprojection.
- An alternative solution of refinement is to separate the pose estimation and refinement into two parts. Recent work @cite put forward a model-agnostic refinement network by synthesizing pose from error statistics prior. @cite improve the initial estimation by modelling input image space and output pose space. Similarly, our method does not perform pose estimation and refinement in one go.
- To address uneven assignment, robots could schedule further meetings to share additional information. However, scheduled meetings induce interruptibility: robots cease new knowledge acquisition when the rendezvous time arrives. They stop knowledge acquisition to travel to the rendezvous point. It has been shown that interruptibility can consume up to half of the search time @cite . Methods exist which mitigate interruptibility. For example, the exploration method proposed in @cite indirectly mitigated interruptibility. The method consists of two types of robots (explorer and relay) and a base station. The explorers intentionally meet and share knowledge with the relays. The latter transfer the shared information to the base station. Interruptibility is mitigated by the fact that the explorer and relay meet closer to the frontiers of the explorer. In the approach taken in this work, all robots are explorers and no base station is present.
- Another solution in the literature which mitigates interruptibility has robots plan to meet other robots before the rendezvous time arrives @cite . Two challenging aspects need to be handled in this case: forecasting of the current positions of the robots involved; and forecasting of the paths taken by the robots from their current positions. To address these challenges, the method proposed by Hourani is based on serendipity @cite . Usually, when robots interrupt exploration to attend a rendezvous, new information might not be gained. To address this negative impact, some robots, which are referred to as serendip robots, plan to interact with other members before the rendezvous time arrives by forecasting their paths. Thus interruptibility is mitigated.
- Deep learning models have made a big progress on learning 3D shape features from different raw representations, such as meshes @cite , voxels @cite , point clouds @cite and views @cite . Because of page limit, we focus on reviewing view-based deep learning models to highlight the novelty of our view aggregation.
- View-based methods represent a 3D shape as a set of rendered views @cite or panorama views @cite . Besides direct set-to-set comparison @cite , pooling is the widely used way of aggregating multiple views in deep learning models @cite . In addition to global feature learning, pooling can also be used to learn local features @cite @cite for segmentation or correspondence by aggregating local patches.
- Moreover, some methods have employed graphs to retrieve 3D shapes from multiple views @cite @cite . Different from these methods, 3DViewGraph employs a more efficient way of view aggregation in deep learning models, which makes the learned features useful for both classification and retrieval.
- Previous efforts have aimed to improve compression via better algorithms or hardware acceleration, e.g. in @cite @cite . Efficient decompression has been relatively less studied, except in the following works. A fine-grained hardware-accelerated decompression algorithm achieves a 2x speedup compared to gunzip @cite . A recent unpublished highly-optimized sequential implementation ( libdeflate ) achieves roughly a 3x speedup github.com ebiggers libdeflate . Perhaps the closest work to ours is an attempt to recover corrupted DEFLATE-compressed files @cite @cite . It is proposed that damaged -compressed files containing English text can be partially reconstructed from random locations. However, the bulk of the method hinges on frequency analysis which does not directly translate to files containing DNA, and also settles for lossy reconstruction.
- Classical interlingua approaches @cite aim at finding a universal or common language representation that involves a conceptual understanding of all languages over the world. This has been the case of Esperanto @cite or Universal Networking Language @cite and many others. Very differently, in this work, we are focusing on training a common language representation with deep learning techniques. The objective is to train an intermediate representation that allows using independent encoders and decoders for each language. In this scenario, translation systems in a highly multilingual environment get reduced from quadratic to linear and also, translation is available for languages pairs that have not been explicitly trained. Differently, from the classical approach, there is no requirement of semantics for this intermediate representation. Following a similar objective or methodology, most related works are the following ones.
- These approaches vary from many encoders to one decoder (many-to-one) @cite , one encoder to many decoders (one-to-many) @cite and, finally, one encoder to one decoder (one-to-one), which we are focusing on because they are closest to our approach. 2016 firat:2017 propose to extend the classical recurrent NMT bilingual architecture @cite to multilingual by designing a single encoder and decoder for each language with a shared attention-based mechanism. (2017) schwenk:2017 and Espana- (2017) espana:2017 evaluate how a recurrent NMT architecture without attention is able to generate a common representation between languages. Authors use the inner product or cosine distance to evaluate the distance between sentence representations. Recently, , (2018) lu:2017 train single encoders and decoders for each language generating interlingua embeddings which are agnostic to the input and output languages.
- While unsupervised MT @cite @cite is not directly pursuing a common intermediate representation, but it is somehow related to our approach. (2017) and (2017) propose a translation system that is able to translate trained only on a monolingual corpus. The architecture is basically a shared encoder with pre-trained embeddings and two decoders (one of them includes an autoencoder). On the other hand, our work is also related to recent works on sentence representations @cite @cite @cite . However, the main difference is that these works aim at extending representations to other natural language processing tasks, while we are aiming at finding the most suitable representation to make interlingua machine translation feasible. While interesting for further research, it is out-of-scope of this study the evaluation and adaptation of this intermediate representation to multiple tasks.
- Apart from the new concepts proposed in this paper, the design of holpy is largely based on that of Isabelle @cite . Many choices in the design, including the use of higher-order logic, structure of terms, syntax for inductive definitions, conversion combinators, or even the typesetting of HOL terms used in this paper, are taken from there. In addition, the author examined the design of HOL Light @cite and HOL4 @cite during this research.
- Lean @cite already proposed the use of macros to shorten proof terms. However, this functionality does not appear to be widely used in user-defined theories. The virtual machine for Lean evaluates operations on natural numbers, integers, and arrays directly using C++ functions, rather than within the logic, resulting in a large speedup in many applications @cite . The main difference in our work is that definition of macros is easily accessible and widely applied at the user level. We also demonstrate the use of macros in logical foundations without explicit Curry-Howard correspondence.
- OpenTheory @cite and Dedukti @cite both provide a universal language for representing theories in a very general logic, at least in part with the aim to allow communication between different proof assistants. While we also provide a common format for theories, our aim is different. Our common format is based on a relatively simple logic. Rather than aiming for interoperability with other proof assistants, we aim to allow many different applications, including user interfaces, to work on the same base library. In this way, we do not have to worry about compatibility of definitions of basic concepts. The use of macros also mean that the common format is able to scale to very large proofs.
- Languages for defining proof automation is an active area of research. Lean's metaprogramming language @cite allows users to define automation in Lean itself. Another research direction is to add typing to tactic languages @cite @cite . Recent efforts to implement proof assistants in imperative languages include Lean (implemented in C++), and Platzer's work on Orbital @cite and KeYmaera KeYmaera X @cite @cite for reasoning about hybrid systems (implemented in Java Scala).
- Many of the ideas in the current work arose from the author's previous work on the auto2 prover @cite . In particular, the definition of macros is similar to that of proof steps in the earlier work. The style of programming proof automation based on building proof terms is also already present there.
- Early convolutional models for semantic segmentation had only a few pooling layers and were trained from scratch @cite . Later work built on image classification models pre-trained on ImageNet @cite @cite @cite , which typically perform 5 downsamplings before aggregation. The resulting loss of spatial resolution requires special techniques for upsampling the features back to the resolution of the source image. Early upsampling approaches were based on trained filters @cite and cached switches from strided max-pooling layers @cite @cite . More recent approaches force some strided layers to produce non-strided output while doubling the dilation factor of all subsequent convolutions @cite @cite @cite . This decreases the extent of subsampling while ensuring that the receptive field of the involved features remains the same as in pre-training.
- In principle, dilated filtering can completely recover the resolution without compromising pre-trained parameters. However, there are two important shortcomings due to which this technique should be used sparingly @cite or completely avoided @cite @cite . First, dilated filtering substantially increases computational and GPU memory requirements, and thus precludes real-time inference and hinders training on single GPU systems. Practical implementations alleviate this by recovering only up to the last two subsamplings, which allows subsequent inference at 8 @math subsampled resolution @cite @cite . Second, dilated filtering treats semantic segmentation exactly as if it were ImageNet classification, although the two tasks differ with respect to location sensitivity. Predictions of a semantic segmentation model must change abruptly in pixels at semantic borders. On the other hand, image classification predictions need to be largely insensitive to the location of the object which defines the class. This suggests that optimal semantic segmentation performance might not be attainable with architectures designed for ImageNet classification.
- To the best of our knowledge, there are only two published works on DenseNet-based semantic segmentation @cite @cite . However, these approaches fail to position DenseNet as the backbone with the largest potential for memory-efficient feature extraction Here we do not consider reversible models @cite due to poor availability of ImageNet-pretrained parameters, and large computational complexity due to increased dimensionality of the deep layers @cite . . This potential is caused by a specific design which encourages inter-layer sharing @cite instead of forwarding features across the layers. Unfortunately, automatic differentiation is unable to exploit this potential due to concatenation, batchnorm and projection layers. Consequently, straightforward DenseNet implementations actually require a little bit more memory than their residual counterparts @cite . Fortunately, this issue can be alleviated with checkpointing @cite @cite . Previous work on checkpointing segmentation models considered only residual models @cite , and therefore achieved only two-fold memory reduction. We show that DenseNet has much more to gain from this technique by achieving up to six-fold memory reduction with respect to the baseline.
- A wide range of TSP variants have been applied to a wide range of domains. Examples include the use of TSP in music for conjunct melody generation in computer-aided composition, as well as for forming automatic playlists and track artist suggestions - now used by Spotify and Tidal @cite . TSP heuristics have also been used for diffractometer guidance in X-ray crystallography @cite ; and for Telescope scheduling in exoplanet discovery @cite as well as in galaxy imaging @cite . It can also be applied to bioinformatics as a genome ordering problem by posing the ordering as a path traveling through each gene marker @cite or as a clustering problem to solve gene expression clustering @cite .
- Previous research has employed neural nets in order to complement local search heuristics @cite @cite . One recent example is that authors in @cite use Kohonen nets and devise a solver. Self organizing nature of Kohononen net iteratively executes and tries to map neurons which are dispersed onto the 2-D euclidean plane to the cities. At each iteration neurons attempt to decrease the distance between the cities in the neighborhood region which is a parametric set that consists of the nearby cities. The learning rate and the neighborhood function are the hyperparameters. Due to the fact that the number of neurons are higher than the cities, the algorithm presents an ordering mechanism for selecting the best fit neurons that represent the cities.
- @cite proposed a TSP solver RL framework based on neural nets. Pointer-nets have been employed for policy gradient and expected tour length prediction. The sequential nature of the framework resembles tour construction algorithms. Stochastic sampling and the actor-critic architecture updates the expected tour length with the current policy (on-policy) and the gradient updates are worked out using reinforce algorithm @cite . In addition, a separate neural net for tour exploration incorporate an expected reward based value iteration approximation. All neural nets are constructed using pointer nets @cite . Both pre-training and random initialization of the weights are realized as the initial starting state. The framework presents improved tour lengths and execution times compared to Christofides and supervised learning methods @cite @cite .
- The authors of @cite present a deep Q-learning @cite and a graph embedding based solution to target combinatorial optimization problems. Given a problem as graph, they first perform function mappings such as belief propagation that learns feature vectors from latent variable models. Then, the learned embeddings allow to learn a construction graph building heuristic. The Q-learning allows to construct the solution based on the reward which is defined as the change in the cost function among the candidates. A helper function is also employed that helps to satisfy the constraints of the combinatorial optimization from the partial solution throughout the construction process.
- Deudon el at. @cite proposes a TSP solver framework based on the attention architecture @cite . The attention mechanism consists of the input encoder and output decoder. The input encoder uses multi-head attention mechanism to compute the embeddings of the input nodes. Given an input set, the output decoder calculates a probability distribution over the input nodes using attention. The decoder uses the chain rule to factorize the probability of a tour as the mean of all the node embeddings that are generated at each hidden attention layer. The critic net computes a glimpse vector, a weighted sum of the action vectors. The authors also improve the best tour results by using 2-opt heuristic.
- The work presented in @cite uses an actor-critic based learning on permutation matrices. Authors target combinatorial problems that coincide with graph theory such as Maximum Weight Matching problem (MWM). Authors rely on GRU structures for representation learning over all possible match pairs in a given graph after non-linear embeddings followed by an outer dot product for pairwise correlation mappings. The actor-network maximizes the average reward by choosing the deterministic action @math generated by the network and with the reward obtained from the critic net @math .
- Furthermore, the critic network optimizes the mean square error between the rounded matrix produced by the Hungarian algorithm @cite and the output of the Sinkhorn layer. The Sinkhorn layer generates a doubly stochastic permutation matrix from the output of the correlation mappings. Similar to the DDPG @cite algorithm, the critic network takes action and a representation of the learned state generated by actor net. TauRieL does not enforce any double stochasticity and can generate permutations from the transition matrix either stochastically or deterministically.
- Beside military and security operations, the usage of UAVs is envisioned in a plethora of civil applications, ranging from agriculture to environmental monitoring and disaster management (see @cite for a thorough taxonomy and survey). In the following, we focus on the three types of tasks encompassed in the scenario under study.
- UAVs can be leveraged in a number of wireless networking applications, e.g., complementing existing cellular systems by providing additional capacity where needed, or to ensure network coverage in emergency or disaster scenarios (see @cite for a comprehensive overview). Differently from the works in @cite , our model jointly optimizes the scheduling of the UAV mobility and actions.
- As overviewed in @cite , fleets of UAVs operating as distributed processing systems can be adopted for various monitoring tasks, including, e.g., surveillance, object detection, movement tracking, support to navigation. A prototype of UAV-based architecture for sensing operations has been described in @cite . In our paper, we consider a conceptually similar UAV equipment of hardware and software modules.
- Several recent studies have already investigated optimization strategies for drone-assisted delivery models (see @cite for a literature review). In particular, variations of the Travelling Salesman Problem leveraging UAVs for last-mile delivery have been introduced @cite .
- Interactions among traffic agents have been explored in prior work by means of explicit communication between pedestrians and drivers @cite , pedestrians and cars @cite @cite @cite , implicit communication @cite , crosswalks scenarios @cite @cite , through the usage of gestures @cite , and by analyzing interactions with objects that may interfere with pedestrian awareness of the environment @cite . Recent work presented by Rasouli and Tsotsos @cite summarized pedestrian behavior studies, discussing interactions between pedestrians and autonomous vehicles, and also presented factors that pedestrians take into consideration when crossing streets. @cite used VR to insert pedestrians in virtual environments. The focus of the study was to estimate the pedestrians' acceptance of autonomous cars. Rothenb " u @cite used a Wizard of Oz approach to analyze the interactions between pedestrians and autonomous cars, where the driver camouflaged themselves to blend in with the seat, simulating the appearance of autonomous vehicles.
- More closely related to our work are approaches that use vehicle mounted cameras and use information of pedestrian dynamics coupled with the pedestrian's awareness @cite @cite @cite @cite @cite @cite . However they are usually focused on specific scenarios as signalized crosswalks @cite and intersections @cite , or does not have a high diversity in number of scenarios and pedestrians @cite @cite @cite @cite . Our approach is not restricted to a specific scenario and we reason about pedestrian future positions by learning such cues from realistic data.
- Volumetric and Multi-view approach: Volumetric approach converts the point sets to a regular 3D grid where the 3D convolution can be applied @cite @cite . However, the 3D convolution usually introduces high computation cost, and the volumetric representations are often inefficient due to the sparse property of the point sets. Some existing works @cite @cite @cite @cite @cite aim at improving computational performance. For instance, some representations for deep learning with sparse 3D data are proposed such as Octree @cite , Kd-Tree @cite . @cite , the authors use a feature-centric voting scheme to implement a fast 3D convolution. While in @cite , a new sparse convolutional operation is introduced to perform efficient 3D convolution on sparse data. Multi-view approaches convert the 3D point sets to a collection of 2D views so that the popular 2D convolutional operations can be applied on the converted data @cite @cite . As an example, the multi-view CNN @cite constructs the CNN for each view, and a view pooling procedure is used to aggregate the extracted features of each view.
- Point-based approach : PointNet @cite is the milestone work for directly processing point sets using the deep neural network. It extracts each point's feature with a shared MLP and aggregates them with a symmetric function, such as max pooling, which is independent of input order. However, PointNet @cite cannot combine the information of neighbor points. To address this issue, PointNet++ @cite uses FPS and neighborhood query algorithms to sample centroids and their neighbor points and then extracts their features using a shared MLP and max pooling. The feature extraction operations mentioned above still do not take the local spatial distribution into account as shown in fig:compare (a). That is, in existing methods, the operations on points at different spatial locations use the same weighting factors. On the contrary, by combining the SDWs with subsequent operations, we can make such process spatially variable.
- There are some other concurrent point-based approaches to process point sets using deep learning, such as @cite @cite @cite @cite @cite . Especially, SO-Net @cite applies the self-organizing network on the point sets processing. RSNet @cite uses Recurrent Neural Network (RNN) to process point sets. KCNet @cite introduces the kernel correlation to combine the information of the neighborhood. And PointCNN @cite learns a @math transform from the point sets to permute them in canonical order. @cite , they project the point features into regular domains, so that the typical CNNs can be applied. The sparse data can also be represented as meshes @cite or graphs @cite @cite , and there are some works that aim at learning feature from these representations. We refer the reader to @cite for a more comprehensive survey.
- While previous research @cite have utilized deep learning techniques for intrusion detection in traditional networks, in this study, we extend this research area by specifically applying deep learning for intrusion detection in the context of IoT. We then demonstrate that deep learning models used for intrusion detection in IoT can be confused with adversarial samples.
- @cite in the original paper that described the IoT dataset that we used for our experiments implemented LSTM, SVM and RNN machine learning techniques to analyze the IoT dataset but they did not evaluate the adversarial robustness of their machine learning models in their study. Additionally, their study only carried out binary classification on the dataset and the prediction output of the machine learning models was classified as either attack or normal traffic. We note that the usefulness of such studies prevails for network forensic analysis use cases where it is essential to classify the output into the various categories of attacks.
- @cite analyzed the threat of intrusion detection against IoT using a very limited dataset sample of 2313 training samples, 496 validation samples and 496 test samples. The dataset also contains only DDoS DoS traffic and normal traffic. In a realistic IoT environment, we expect a larger network traffic dataset with hundreds of thousands or millions of records with a more heterogeneous attack profile on the network. We used a dataset containing over 3.6 million records and a more heterogeneous attack profile consisting of 5 target labels.
- Zheng @cite implemented several adversarial attack algorithms against a deep learning based intrusion detection system in a traditional network using multi-layer perceptron Feed-forward Neural Network and compared the results from the various adversarial attacks. The author demonstrated that the deep learning based IDS classifier using the FNN was adversely impacted by the adversarial samples. However, the NSL-KDD dataset that was used was generated over a decade ago and may not represent the type of network attack traffic that would be expected in today's IoT networks. @cite also evaluated the NSL-KDD dataset by training a FNN to classify the network packets, and then tested the resilience of their model to adversarial examples. The dataset used in their experiment may not represent a typical IoT network traffic.
- Recently, many learning-based approaches for human pose and hand pose estimation have been presented in the literature @cite @cite @cite @cite @cite @cite @cite . These methods are highly specialized and rely on large collections of training data. In contrast, our SfAM is a general approach which can cope with different articulated structures, with no need for labeled datasets.
- The MW-0-CPEMP is an extension of the longest common subsequence problem on given two sequences @cite . It has an application in the sequence alignment problem that appears in bioinformatics @cite and in natural language processing @cite .
- @cite studied the problem of finding a subgraph that has few edge crossings; given a graph (not necessarily bipartite) and its geometric drawing (i.e., every vertex is specified by a 2D point, all edges are drawn as straight line segments, and no two edges overlap or intersect at a vertex), we are asked to find a subgraph of a certain class that makes the minimum number of edge crossings. They showed that, for spanning trees, @math - @math paths, cycles, matchings of a fixed size, and 1- or 2-factors, it is NP-hard to approximate the minimum number of edge crossings within a factor of @math for any @math , where @math denotes the number of edge crossings in the given graph. They also presented fixed-parameter algorithms to decide whether there is a non-crossing subgraph of one of the above graph classes, where @math is used as the parameter.
- The non-crossing (or crossing-free) constraint has been considered for some problems of finding an optimal'' subgraph. It is @cite who first studied the algorithmic aspect of the MW-0-CPEMP explicitly. For the edge-unweighted case, they provided a polynomial-time algorithm that runs in @math time or in @math time, where @math denotes the cardinality of a maximum 0-CPE matching. They also extended the algorithm to the edge-weighted case, which yields an @math time algorithm. A bipartite graph is convex if, for every @math , @math @math implies @math for all @math . For the MW-0-CPEMP in edge-unweighted convex bipartite graphs, @cite presented an algorithm whose running time is @math . @cite considered the Euclidean non-crossing bipartite matching problem, where each vertex is represented by a 2D point. The objective is to find a non-crossing perfect matching whose longest edge is minimized. They showed that the problem is NP-hard in general, but that it is polynomially-solvable in some special cases. More recently, @cite showed that the minimum cost non-crossing flow problem on a layered network is NP-hard. Ruangwises and Itoh @cite studied the stable marriage problem under the non-crossing constraint, showing that there exists a weakly stable non-crossing matching for any instance.
- The conflict pair constraint (or negative disjunctive constraint ) is a generalization of the non-crossing constraint. Represented by a conflict graph @math , this constraint prohibits a solution from including two edges @math such that @math . The minimum cost perfect matching problem with conflict pair constraints is strong NP-hard for a general graph @math even if the conflict graph @math is a collection of single edges @cite and is NP-hard even for @math that consists of 4-cycles @cite ; it turns out that the problem is NP-hard for a bipartite graph. For this type of constraint, there are also studies on the transportation problem @cite @cite , the minimum spanning tree problem @cite @cite @cite , and the max-flow problem @cite .
- . Recently, relatively fewer works have focused on G-ReID tasks @cite @cite @cite @cite @cite , compared to general ReID tasks. Some of them mainly attempted to extract global or semi-global features. For example, Cai al @cite proposed a discriminative covariance descriptor to obtain both global and statistical features. Zheng al @cite proposed semi-global features by segmenting a group image into several ring regions. Since persons in a group often change their locations under different views ( , layout-change), global and semi-global features are usually sensitive to such changes. In order to make use of individuals' features in the groups, Zhu al @cite introduced patch matching between two group photos. However, it requires prior restrictions on vertical misalignments, making it unworkable under certain circumstances. Xiao al @cite leveraged multi-grain information and attempted to fully capture the characteristics of a group. This approach, however, involves too much redundant information and employs common hand-crafted features, thereby making its accuracy not satisfactory enough.
- . Recently, Generative Adversarial Networks (GANs) have been applied to transfer image styles from a source domain to a target domain @cite @cite @cite @cite @cite . Gatys al @cite separated the image content and image style apart and recombined them afterwards, so that the style of one image can be transferred into another. Taigman al @cite proposed a domain transfer network to translate images to another domain while preserving original identities. By making use of the existing domain-transfer techniques, we are able to take advantage of the abundant ReID datasets to improve the performance of our method.
- Prior work has explored techniques that guide robots motions via synthesized constraints, i.e., potential fields and virtual fixtures. Potential fields, originally developed by Khatib @cite , guide a user towards or away from a goal or obstacle @cite . Virtual fixtures create virtual barriers or forbidden regions in the task space that provide assistance in protecting sensitive areas. Rosenberg @cite provided early work in the development of virtual fixtures. A number of guidance and or preventative virtual fixture designs were compared using a Fitts' law paradigm for a remote teleoperation peg placement task. Later work @cite has developed on-the-fly forbidden region generation techniques using real-time sensor data.
- The work of Dragan and Srinivasa @cite @cite formalizes assistive teleoperation under a framework of policy blending. The system, grounded in inverse reinforcement learning, attempts to predict the intentions of the human operator in order to arbitrate the operator and robot control policies. Other learning-based methods have also been developed by Abi- @cite and attempt to refine unskilled operator input based on skilled operator input learned by exploiting learning from demonstration techniques.
- From the concrete spraying literature, a task space control framework was developed by Honegger and Codourey @cite based on the inverse-kinematics method. The system allows the operator to control the end-effector of the concrete spraying unit using a 6-DoF space mouse. Later work by Girmscheid and Moser @cite developed a sliding autonomy approach for a high-DoF concrete spraying unit. They define three levels of autonomy: (1) A manual mode, consisting of no autonomy and implementing common joint-level control, (2) A semi-automated mode that, using a pre-collected laser scan of a tunnel cavity and computed geometry, allows the operator to command only the position on the wall to spray whilst the system accounts for motion constraints, and (3) An automated mode that plans and executes an entire trajectory. The latter mode, they note, is only in development for tunnel-boring machine projects where the excavated tunnel surface is regular and very smooth. Modes of teleoperation compared in this work are inspired by these control techniques developed for concrete spraying applications.
- The main purpose of triplet loss @cite is to distinguish identities in the projected space with the guidance of distances among an anchor sample, a positive sample, and a negative sample. There are several revisions for the original triplet loss, which mainly fall into the following three categories: (1) Adding new constraints to the objective function to improve the generalization performance @cite @cite ; (2) Optimizing the selection of triplet samples to make the triplet samples more informative, which can lead to faster convergence and better performance @cite @cite @cite @cite @cite ; (3) Proposing dynamic margins for different triplet combinations, such as @cite @cite which use handcrafted methods to determine the similarities among different identities. Our method belongs to the last category. Different from previous approaches, we exploits a teacher model to obtain the similarity information among identities to set the dynamic margins.
- The Multistage Max-Min Fair Allocation problem has been studied in the offline and the online settings in @cite . This problem corresponds to a multistage variant of the Santa Klaus problem. For the off-line setting, the authors showed that the multistage version of the problem is much harder than the static one. They provided constant factor approximation algorithms for the off-line setting. For the online setting they proposed a constant competitive ratio for SSFS-type evolving instances and they proved that it is not possible to find an online algorithm with bounded competitive ratio for GE-type evolving instances. Finally, they showed that in the 1-lookahead case, where at time step @math we know the instance of time step @math , it is possible to get a constant approximation ratio.
- @cite and Buchbinder, Chen and Naor @cite considered a multistage model and they studied the relation between the online learning and competitive analysis frameworks, mostly for fractional optimization problems.
- The Greenwald-Khanna algorithm @cite is generally regarded as the best deterministic quantile summary. The space bound of @math follows from a somewhat involved proof, and it has been questioned whether this approach could be simplified or improved. Our work answers this second question in the negative. For a known universe @math of bounded size, Shrivastava al @cite designed a quantile summary using @math words. Note that their algorithm is not comparison-based and so the result is incomparable to the upper bound of @math .
- If we tolerate randomization and relax the requirement for worst-case error guarantees, it is possible to design quantile summaries with space close to @math . After a sequence of improvements @cite @cite @cite @cite , Karnin, Lang, and Liberty @cite designed a randomized comparison-based quantile summary with space bounded by @math , where @math is the probability of not returning an @math -approximate @math -quantile for some @math . The algorithm combines careful sampling, and uses the Greenwald-Khanna summary as a subroutine. They also provide a reduction to transform the deterministic @math lower bound into a randomized lower bound of @math , implying optimality of their approach in its model. We discuss further how the deterministic and randomized lower bounds relate in .
- Luo al @cite compared quantile summaries experimentally and also provided a simple randomized algorithm with a good practical performance. This paper studies not only streaming algorithms for insertion-only streams (i.e., the cash register model), but also for turnstile streams, in which items may depart. Note that any algorithm for turnstile streams inherently relies on the bounded size of the universe. We refer the interested reader to the survey of Greenwald and Khanna @cite for a description of both deterministic and randomized algorithms, together with algorithms for turnstile streams, the sliding window model, and distributed algorithms.
- Other results arise when relaxing the requirement for correctness under adversarial order to assuming that the input arrives in a random order. For random-order streams, Guha and McGregor @cite studied algorithms for exact and approximate selection of quantiles. Among other things, they gave an algorithm for finding the exact @math -quantile in space @math using @math passes over a random-order stream, while with @math memory we need to do @math passes on the worst-case stream. The Shifting Sands algorithm @cite reduce the magnitude of the error from @math to @math . Since our lower bound relies on carefully constructing an adversarial input sequence, it does not apply to this random order model.
- Our proposed approach builds on the substantial progress made in pedestrian detection and human action recognition. However, in this section, we concentrate on literature more directly relevant to our contributions, that are focused on (a) pedestrian trajectory forecasting and (b) alternative supervision methods for training models in the absence of large-scale human annotated datasets. For pedestrian detection, see recent surveys such as @cite @cite . For action recognition, see recent surveys such as @cite @cite .
- Given the absence of large pedestrian trajectory datasets, previous works have modeled the dynamic motion of pedestrian's using linear dynamic systems (LDS) that combine the assumptions of constant velocity (CV) or constant acceleration (CA) with a filtering algorithm such as the Kalman filter @cite . To model non-linear, dynamic motion, a switching linear dynamic system (SLDS) uses a discrete Markov chain to select between multiple LDS at each timestep based on past observations. However, the SLDS is limited to to pedestrian motion rather than a change in dynamics. To address this issue, existing works @cite @cite focus on additional cues such as pedestrian head pose, motion state, and road scene context or use a non-linear filtering algorithm such as the unscented Kalman filter @cite .
- Data-driven approaches for trajectory forecasting have gained attention in recent years resulting from the success of deep learning models for related problems such as image classification, action recognition, and pedestrian detection. In particular, deep learning models have been applied to trajectory forecasting in a surveillance setting with a fixed overhead camera on datasets such as UCY @cite and ETH @cite , or forecasting vehicle trajectories @cite . In @cite , pedestrian trajectory is forecast by encoding pedestrian location as a sparse vector which is used directly as input to a convolutional neural network (CNN). In @cite , pedestrian trajectory is forecast from a static, overhead camera using a long short-term memory (LSTM) network. The authors introduce social pooling, which models the social interactions between multiple pedestrians.
- Trajectory forecasting is considered from a first-person perspective in @cite . The authors propose a model combining features from the pedestrians pose, estimated ego-motion, and past location information. Similarly, in @cite , an LSTM is used to predict the future location of pedestrian bounding boxes by first estimating future ego-motion and then using these estimates with observed bounding boxes to forecast the location of future bounding boxes. All data-driven approaches, however, are limited by the lack of available training data.
- In this section, we review previous works related to this paper. Over the past decade, numerous image retargeting works have been proposed @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . The majority of image retargeting methods are based on hand-crafted visual attention map such as gradient map, color contrast, object segmentation, face detection, . We refer the readers to survey literature @cite for more details. Here, we mainly discuss representative image retargeting methods and a recent deep learning based method. Besides, we will review the successful applications of cycle constraint in other fields.
- : Most of conventional image retargeting approaches can directly retarget input images to target aspect ratios according to the previous calculated visual attention map, in which the bright areas indicate the interesting areas in the input image. These approaches commonly exploit traditional methods to extract pixel saliency information, and then define a manipulating way to insert or remove pixels in order to obtain the target image. Seam carving @cite is a good example, which uses three ways ( @math , pixel gradient, entropy energy, visual saliency) to measure the importance of pixels in the input image. Then, it defines a way of seam carving to change the aspect ratio of the input image. A seam is an 8-connected path of pixels from each column or row in the source image. The retargeted image is obtained by iteratively removing or inserting seams.
- Different from the seam carving method in a discrete way, Panozzo @math @cite propose to continuously transform the input image into the target image. Specifically, they overlay a uniform grid on the input image and assign different scaling factors to each grid cell. The scaling factors of important regions are large, and conversely, small. As a result, those important regions are preserved as much as possible, while less important regions are shrinking obviously. Wang @math @cite propose a scale-and-stretch warping method. They calculate optimal scaling factors for image regions based on the edge and saliency map. Jin @math @cite cast image retargeting problem into a quadratic program based on a triangular mesh. All these conventional approaches need to calculate an importance map based on low-level visual cues or other human priors. The poor quality of importance map significantly degrades the performance of these methods, which seriously limits their generality.
- : Because the visual quality of retargeted images highly depends on the subjective assessment, it is hard to build a large-scale image retargeting dataset for supervisory learning a deep model. Thus, there are few supervised retargeting methods. Cho @math . @cite introduce deep learning technique into the image retargeting task. Their deep model learns a shift map to implement pixel-wise mapping from the source image to the target image. They define a loss function containing two terms: content loss and structure loss. The content loss enables the retargeted image outputted by the deep network to have the same class as the source image. Therefore, their approach requires image-level annotations to construct the content loss. This approach may fail to deal with those images that do not belong to any class in the training set. Besides, in order to reduce distortions in the target image, the method proposes to use 1D duplicate convolution to smooth the learned shift map. This operation is helpful for reducing visual artifacts in the target image but requires the input image to be a fixed size.
- : Cycle constraints have been explored in other fields in order to regularize model predictions and improve its stability. For language translation, Richard shows the translation quality can be improved effectively by using a back translation and reconciliation strategy @cite . For computer vision, high-order and multimodal cycle consistency have been used to improve the stability and quality of model predictions such as co-segmentation @cite , motion prediction @cite , 3D shape matching @cite , semantic alignment @cite , depth estimation @cite , image-to-image translation, video frame interpolation @cite . For deep learning, researchers have incorporated the concept of cycle consistency into the regularization of optimizing deep networks @cite . In this work, we demonstrate a novel and feasible way of exploiting the concept of circle consistency to address the image-retargeting problem. To the best of our knowledge, we are the first to improve image retargeting by using circle consistency. Our Cycle-IR is capable of producing high-quality target images and achieves the state-of-the-art result.
- While past connections can hint at receiver network conditions, we rather see them as a building block that can be used in combination with a receiver-side bandwidth hint. In our work, we highlighted possible angles in which to tackle bandwidth estimates but do not go into detail. @cite shows how to utilize cellular (4G) PHY-layer information at the client for bandwidth estimations, they even implement it in the Google QUIC framework but do not show the effects on other flows, fairness and losses nor focus on the startup phase. Further bandwidth estimations @cite @cite utilize client context information ( location, signal quality, speed) in cellular networks to learn and predict channel throughput, which could be used in an implementation of our system.
- Systematic codes have been proposed in @cite , which are coded generalizations of selective repeat ARQ, and the adaptive coded ARQ model with cumulative feedback as in @cite .
- Using FEC, in order delivery delay over packet erasure channels can be reduced @cite , and the performance of SR ARQ protocols @cite can be boosted. Delay bounds for convolutional codes have been provided in @cite , @cite . Packet dropping to reduce playback delay of streaming over an erasure channel is investigated in @cite , @cite , @cite . Delay-optimal codes without feedback for burst erasure channels, and the decoding delay of codes for more general erasure models have been analyzed in @cite . Transmission with delay constraints have been considered in @cite by combining the PHY and NET layer aspects, where bit level FEC is performed at the PHY layer, and Random Linear Network Coding (RLNC) is performed at the NET layer. To prevent packet loss in the presence of interference and large RTT, a network coded TCP solution has been proposed in @cite . In Fig. , we refer to two of the coding approaches that are the closest to ours.
- Delay and throughput gains of coding in unreliable networks have been discussed in @cite . The delay advantage of coding in packet erasure networks has been studied in @cite . A capacity-achieving coding scheme for unicast or multicast over lossy packet networks has been proposed in @cite , where intermediate nodes perform recoding and send out coded packets formed from random linear combinations of previously received packets. Joint optimization of coding (for delay sensitivity) and scheduling (time-division) in wireless systems for varying delay sensitivities for single-hop wireless erasure channels and single broadcast channel (with multiple receivers having different delay sensitivities) has been considered @cite . The single hop model is later generalized to multi-hop @cite .
- Generalization in computer vision and machine learning is the ability of image and video processing algorithms such as object detectors or classifiers to perform well not only on the dataset they are trained on but also on previously unseen data acquired by different cameras or under different environmental conditions such as daytime vs. nighttime. Usually, such datasets underlie different image quality perturbations such as specific sensor noise, contrast, or blur that can severely affect the processing quality @cite . Detector or classifier models exclusively and extensively trained on this data are prone to overfit @cite or getting biased @cite . In contrast to explicit methods that aim at reducing or avoiding such effects during model training @cite @cite @cite @cite or application @cite , we are inspired by Torralba and Efros @cite and @cite and compare models trained on different datasets in a cross-validation manner. In addition, different datasets for domain adaptation and transfer learning @cite @cite @cite are exploited that best support the multispectral fusion.
- Various versions of Langevin dynamics have been studied in many recent papers, see @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . The convergence rate of HMC is also studied recently in @cite @cite @cite @cite @cite @cite . The first bound for our setting was obtained by Mangoubi and Smith @cite , who gave an @math bound on the convergence rate of ideal HMC.
- A popular error reporting approach applied for bottom-up parsing is based on associating an error message to a parse state and a lookahead token @cite . To determine the error associated to a parse state, it is necessary first to manually provide a sequence of tokens that lead the parser to that failure state. We can simulate this technique with the use of labels. By using labels we do not need to provide a sample invalid program for each label, but we need to annotate the grammar properly.
- The error recovery approach for predictive top-down parsers proposed by Wirth @cite was a major influence for several tools. In Wirth's approach, when there is an error during the matching of a non-terminal @math , we try to synchronize by using the symbols that can follow @math plus the symbols that can follow any non-terminal @math that we are currently trying to match (the procedure associated with @math is on the stack). Moreover, the tokens which indicate the beginning of a structured element (e.g., while , if ) or the beginning of a declaration (e.g., var , function ) are used to synchronize with the input.
- As its default recovery strategy, ANTLR attempts single token insertion and deletion to synchronize with the input. In case the remaining input can not be matched by any production of the current non-terminal, the parser consumes the input '' @cite . ANTLR allows to modify the default error recovery approach, however, it does not seem to encourage the definition of a recovery strategy for a particular error, the same recovery approach is commonly used for the whole grammar.
- The evaluation of our error recovery technique was based on Pennelo and DeRemmer's @cite strategy, which evaluates the quality of an error recovery approach based on the similarity of the program obtained after recovery with the intended program (without syntax errors). This quality measure was used to evaluate several strategies @cite @cite @cite , although it is arguably subjective @cite . Differently from Pennelo and DeRemmer's approach, we did not compare programming texts, we compared the AST from an erroneous program after recovery with the AST of what would be an equivalent correct program.
- Architecture search and hyperparameter optimisation are two important topics in the field of machine learning, leading to numerous publications. Much work focuses on Bayesian optimisation (BO) @cite @cite @cite , to address general tasks in selecting hyperparameters. Other authors directly examine the problem of CNN architecture search. We offer our contribution to the latter and combine it with the former to obtain a comprehensive solution. While some papers introduce architecture search methods which are carefully engineered, but often difficult to use in practice @cite @cite @cite @cite , we aim for a plain and effective method to provide grounds for performance estimation and selection decision, that could be applied to nearly any type of neural network.
- Personalized email prioritization through collaborative filtering has also been studied for broadcast emails @cite , where the prediction is based on the feedback from a small subset of recipients of the same email. Despite showing great performance, the problem scope considered is rather limited. It can only be applied to emails sent to large number of recipients and can not perform the prediction on delivery time since it needs to wait for responses from some subset of the recipients. Therefore, it can not be applied to the general prediction problem including predictions for emails sent to limited number of recipients and time-sensitive situations.
- . Leveraging the powerful representation ability of neural networks, Gatys pioneered on the Neural Style Transfer @cite , where the style was effectively formulated as the Gram matrix @cite of deep features. Johnson trained a feed-forward StyleNet @cite using the loss of Neural Style Transfer @cite for fast style transfer @cite @cite @cite @cite @cite . In parallel, Li @cite @cite represented styles by neural patches, which can better preserve structures for photo-realistic styles. Meanwhile, other researchers regard style transfer as an image-to-image translation problem @cite @cite , and exploited Generative Adversarial Network (GAN) @cite to transfer specialized styles such as cartoons @cite , paintings @cite and makeups @cite @cite . Compared to Gram-based and patch-based methods, GAN learns the style representation directly from the data, which can potentially yield more artistically rich results.
- . The problem of artistic text style transfer was first raised by Yang @cite . The authors represented the text style using image patches, which suffered from a heavy computational burden due to the patch matching procedure. Driven by the progress of neural network, Azadi @cite trained an MC-GAN for fast text style transfer, which, however, can only render @math capital letters. Yang @cite recently collected a large dataset of text effects to train the network to transfer text effects for any glyph. Unlike the aforementioned methods that assume the input style to be well-designed text effects, a patch-based model UT-Effect @cite stylized the text with arbitrary textures and achieved glyph deformations by shape synthesis @cite , which shows promise for more application scenarios. Compared to UT-Effect @cite , our GAN-based method further enables the continuous adjustment of glyph deformations via a controllable parameter in real-time.
- . To the best of our knowledge, the research on the multi-scale style control currently focuses on two kinds of scales: the and the of the texture. The texture strength determines the texture similarity between the result and the style image (Fig. (c)). It is mainly controlled by a hyper-parameter to balance the content loss and style loss @cite . As a result, one has to re-train the model for different texture strengths. Babaeizadeh @cite performed efficient adjustment of the texture strength, with an auxiliary network to input additional parameters to modulate the style transfer process. Meanwhile, the stroke size depicts the scale of texture patterns (Fig. (d)), , size or spatial frequency. Jing @cite proposed a stroke-controllable neural style transfer network (SC-NST) with adaptive receptive fields for stroke size control. Our work explores the glyph deformation degree (Fig. (b)), a different and important dimension of scale'' that is unexplored in prior work.
- In this section, we present an overview of the approaches on collision avoidance and navigation in dynamic environment. Quite a few approaches @cite , @cite , @cite , @cite assume that the obstacles are static and plan for the control to avoid the collision. In case of moving obstacles these replan based on the updated positions of the obstacles. But these fail to generate the safe trajectories around the fast moving obstacles. In @cite , @cite , @cite , @cite the future position of obstacles are computed by extrapolating with the current velocity to handle high velocities. But these approaches cannot handle the reactive nature of the other agents. Many works like @cite , @cite , @cite , @cite have focused on crowd simulation in which each agent considers the other agents as obstacles and navigates independently.
- With the commercial availability of 30 cm pixel satellite imagery and low-cost aerial photography drones, more public high-resolution datasets become available @cite @cite @cite @cite @cite . These new datasets and industrial interests lead to a proliferation of research activities recently @cite @cite @cite @cite . Semantic segmentation models based on the fully convolutional neural net architecture become main stream @cite . In a recent challenge @cite , all top solutions used variants of U-net @cite or Deeplab @cite to segment an entire image at once, up to 1024 x 1024 pixels. A larger input size gives more context, which often leads to more structured and accurate prediction results.
- Comparing to other computer vision applications, road mapping has little margin for error. Prediction gaps make the entire road useless for routing, and therefore have attracted lots of attention. Mnih noticed the problem early on and used Conditional Random Fields in post-processing to link broken roads @cite . Another popular technique to link roads is shortest path search @cite @cite . Line Integral Convolution can smooth out broken roads in post-processing too @cite . More recent works try to address the problem in prediction instead of post-processing, e.g., through a topology-aware loss function @cite or through an iterative search process guided by CNNs @cite . We must be careful to link roads because incorrect connections are more dangerous than missing connections in routing. Our approach complements the above mentioned methods because GPS data can confirm the connectivity or the absence of it regardless of image occlusion or other issues.
- Recently the field of multi-agent collision avoidance has seen much success in using a decentralized non-communicating framework in ground robots * 27,28 . In this work, the authors develop an extension to the policy-based learning algorithm (GA3C) that proves to be efficient in learning complex interactions between many agents. We find that the field of collision avoidance can be adapted to conflict resolution by considering larger separation requirements, so our framework is inspired by the ideas set forth by @cite .
- In recent years, numerous FPGA designs for CNN inference have been proposed (generally targeting prominent networks such as LeNet-5 @cite , AlexNet @cite , and VGG-16 @cite ) with the key objective of designing a system with low latency and high energy efficiency. A common strategy deployed by these designs is to minimize the degree of weight and data movement, especially from off-chip memory, as they add significant overhead in terms of both latency and power consumption.
- One approach to minimizing data movement is layer fusion, where multiple CNN layers are processed at the same time in a pipelined manner to allow for instant use of intermediate data without external memory accesses @cite @cite @cite . Another approach, used for 3 @math 3 or larger convolutional filters, is to determine the ordering of inference computation which minimizes the number of partial sums that must be stored @cite @cite . Since our streamlined CNN architecture () uses only 1 @math 1 filters, convolution is reduced to matrix multiplication, which can be efficiently implemented using systolic arrays. Additionally, different computation strategies are often taken for the first layer @cite , as it has only three input channels in the case of RGB images and final fully connected layer @cite , where there are significantly more weights than data. In this work, we propose to use the same systolic array building block for efficient implementations of all layers in a CNN by using various full-stack optimization techniques such as input reshaping discussed in .
- Since VGG-16 @cite was introduced in 2014, there has been a general trend towards designing deeper CNNs through the use of residual connections (ResNets @cite ) and concatenative connections (DenseNet @cite ) as deeper networks tend to achieve higher classification accuracy for benchmark datasets such as ImageNet @cite . However, as pointed out in Table 2 of the original ResNet paper @cite , residual connections appear to add little improvement in classification accuracy to a shallower (18 layer) CNN. Based on these observations, we have chosen to use a shallower CNN (19 layers) without any residual or concatenative connections, which we outline in . In our evaluation () we show that for this shallower CNN, the exclusion of additional connections has minimal impact on classification accuracy while significantly simplifying our hardware implementation and improving its efficiency.
- Additionally, several alternatives to standard convolution have recently been proposed to reduce the computation cost. Depthwise separable convolution @cite dramatically reduces the number weights and operations by separating a standard convolution layer into two smaller layers: a depthwise layer that only utilize neighboring pixels within each input channel and a pointwise layer which operates across all channels but does not use neighboring pixels within a channel ( it only uses 1 @math 1 filters). Wu al showed that a channel shift operation can be used to replace the depthwise layer without significant impact on classification accuracy @cite . As described in , our proposed CNN use this channel shift operation immediately preceding a 1 @math 1 convolution layer.
- Several methods have been proposed to quantize the CNN weights after training, using 16-bit @cite and 8-bit @cite fixed-point representations, without dramatically impacting classification accuracy. More recently, low-precision quantization methods ( 1-4 bits) such as binary @cite @cite and ternary quantization @cite @cite @cite methods have also been studied, which to smaller models may incur some cost to classification accuracy. Generally, for these low-precision approaches, training is still performed using full-precision weights, but the training graph is modified to include quantization operations in order to match the fixed-point arithmetic used at inference. In this paper, log quantization @cite is adopted for weights, with each quantization point being a power of two. This allows for significantly more efficient inference, as fixed-point multiplication is replaced with bit shift operations corresponding the power of two weight, as discussed in .
- In addition to weight quantization, there are many quantization methods for activation data output from each CNN layer @cite @cite @cite @cite @cite . Data quantization reduces the cost of memory access for these intermediate output between layers in a CNN and also the computation cost of inference. However, it has been shown that low-precision quantization of activation ( 1-4 bits) leads to a significantly larger degradation in classification accuracy compared to weight quantization @cite @cite . Due to these considerations, we use 8-bit linear quantization for data in this paper and focus on an efficient implementation of multiplication-free computations with 8-bit data.
- Additionally, we note that the majority of proposed methods for low precision weights and data omit two details which are critical for efficient end-to-end system performance. First, works in this area often treat the first and last layers in a special manner by keeping the weights and data full-precision for these layers @cite @cite @cite . Second, they often explicitly omit quantization considerations of batch normalization and use standard full-precision computation as performed during training @cite @cite . Since batch normalization is essential to the convergence of low-precision CNNs, this omission makes it difficult to efficiently implement many low-precision approaches. In this work, as discussed in , we handle both of these issues by (1) quantizing the weights and data in all layers (including the first and last layers) under a single quantization scheme and by (2) including batch normalization quantization in the training graph (depicted in Figure ) so that it adds zero overhead during inference.
- It is well known in the literature that the majority of weights in a CNN (up to 90 Unlike previous work, column combining is a new pruning method which allows for sparse CNN layers, but requires that the remaining sparse weights can be packed into a denser format when deployed in hardware @cite . In our proposed training pipeline, we use column combining in addition to weight and data quantization as discussed in the previous section, in order to achieve efficient sparse CNN inference. Figure shows how a sparse pointwise convolution layer with power of two weights is converted into a denser format by removing all but the largest nonzero entry in each row across the combined channels when stored in a systolic array. In this example, column combining reduces the width of the small layer by factor of 4 @math from 8 to 2. In , we describe bit-serial design for efficient hardware implementation of this packed format shown on the right side of Figure .
- For subspace clustering in feature space, we further divide the existing methods into two types. The first type uses , which is induced via a Mercer kernel, , @cite @cite @cite @cite , or constructed via matrix decomposition, , @cite , @cite . The second type use , which is designed by manual feature extraction, , @cite , or is learned from data, , @cite @cite .
- On the other hand, the latent feature space has also been constructed via matrix decomposition, , @cite , @cite . In @cite , a linear transform matrix and a low-rank representation are computed simultaneously; in @cite , a linear transform and a sparse representation are optimized jointly. However, the representation power of the learned linear transform is still limited.
- Explicit Feature Space Deep learning has gained a lot of research interests due to its powerful ability to learn hierarchical features in an end-to-end trainable way @cite @cite . Recently, there are a few works that use techniques in deep learning for feature extraction in subspace clustering. For example, in @cite @cite , a fully connected deep auto-encoder network with hand-crafted features ( , SIFT or HOG features) combined with a sparse self-expression model is developed; in @cite , a stacked convolutional auto-encoder network with a plus-in self-expression model is proposed. While promising clustering accuracy has been reported, these methods are still suboptimal because the potentially useful supervision information from the clustering result has been taken into the feature learning step a joint optimization framework for fully combining feature learning and subspace clustering has been developed. More recently, in @cite , a deep adversarial network with a subspace-specific generator and a subspace-specific discriminator is adopted in the framework of @cite for subspace clustering. However, the discriminator need to use the dimension of each subspace, which is usually unknown.
- In this paper, we attempt to develop a joint optimization framework for combining feature learning and subspace clustering, such that the useful self-supervision information from subspace clustering result could be used to guide the feature learning and to refine the self-expression model. Inspired by the success of Convolutional Neural Networks in recent years for classification tasks on images and videos datasets @cite and the recent work @cite , we integrate the convolutional feature extraction module into subspace clustering to form an end-to-end trainable joint optimization framework, called Self-Supervised Convolutional Subspace Clustering Network (S @math ConvSCN). In S @math ConvSCN, both the stacked convolutional layers based feature extraction and the self-expression based affinity learning are effectively self-supervised by exploiting the feedback from spectral clustering.
- Code clone detection techniques have been studied extensively for dozens of years, including text-based techniques @cite @cite , token-based techniques @cite @cite @cite @cite , counting-based techniques @cite @cite , and syntactic approaches @cite @cite @cite , etc. These techniques were also widely explored in related domains, such as mobile app repackaging detection @cite @cite @cite @cite . In this work, we take advantage of a customized fuzzy hashing technique @cite , which is both light-weight and effective. Note that we did not rely on heavy-weight methods such as comparing the control-flow graph (CFG) and program dependency graph (PDG), mainly due to two reasons. First, our approach should be scalable. Second, the simplicity of smart contracts and EVM bytecode, , the relatively simple logic and function invocations, makes it unnecessary to adopt those heavy approaches. A limited number of studies have explored to study code clones in the smart contracts. For example, Kiffer . identified substantial code reuse in Ethereum @cite . Furthermore, Liu proposed ECLONE @cite , which is able to detect semantic clones for smart contracts. However, none of them have measured the ecosystem in large-scale, and characterized their security impacts.
- Traditionally time series forecasting was under the scope of statistics and models like Auto Regression Integrated Moving Average (ARIMA), Seasonal ARIMA were extensively used linear models @cite . But due to possible non-linearity in the data Artifical Neural Networks @cite were introduced and subsequently hybrid ARIMA models were used @cite @cite . Random Forest based methods were reported used in forecasting. @cite , however the above were outperformed by Gradient Boosted Regression Tree (GBRT) while used for forecasting problem in Global Energy Forecasting Competition 2014 @cite . A summary of related PV power forecasting models is available in @cite .
- In deep learning encoder-decoder network using LSTM and RNN were first used for machine translation as a sequence-to-sequence learning problem, where length of the input and output sequence is not fixed. @cite @cite . However it was then applied to forecasting problems where the lengths were known, yet they are not reported to be used in solar irradiation forecasting so far. @cite @cite . In this paper we implement such model both with and without neighboring location.
- In the recent years, the computer vision community has created benchmarks for video related tasks such as scene recognition, pedestrian & object detection, object tracking, action recognition, anomaly behavior detection and etc. Despite the potential pitfalls of such datasets, they have proved to be extremely helpful to advance the state-of-the-arts in the corresponding areas @cite @cite @cite @cite . An overview of examples of the existing datasets in urban environments for object detection and tracking is shown in Tab. .
- The MIT Traffic dataset @cite is an example of the recent efforts to build more realistic urban traffic surveillance video datasets for research on pedestrian detection and activity analysis. It includes a traffic video sequence of 90 minutes long, recorded by a stationary camera and the whole sequence is divided into 20 clips. The size of the scene is 720 by 480. In order to evaluate the performance of human detection on this dataset, the ground truth of the pedestrians of some sampled frames are manually labeled. There are in total 520 annotated frames and 2054 bounding boxes in the dataset.
- The Caltech Pedestrian dataset @cite @cite consists of approximately 10 hours of @math @math Hz video taken from a vehicle driving through the regular traffic in an urban environment. All the data is roughly divided in half, setting aside 6 sessions for training and 5 for testing. About @math frames with a total of @math bounding boxes and @math unique pedestrians are annotated.
- The Daimler Monocular Pedestrian Detection dataset @cite is another dataset for pedestrian detection in urban environments. The training set contains @math pedestrian samples (image cut-outs at @math resolution) and 6744 additional full images without pedestrians for extracting negative samples. The test set contains an independent sequence with more than @math images and @math pedestrian labels (fully visible or partially occluded), captured from a vehicle during a 27 min driving through the urban traffic.
- Recently, the KITTI benchmark @cite is introduced for challenges in autonomous driving, which includes stereo flow, odometry, road and lane estimation, object detection as well as tracking. Some of the sequences include crowded pedestrian crossings, making the dataset quite challenging, but the camera is moving, while the conventional traffic surveillance video varies greatly.
- MOT challenge 2015 @cite , challenge 2016 @cite and challenge 2017 @cite are the recent challenging benchmarks for multiple object tracking. The videos in the benchmarks are diverse, and some of which are selected from the existing datasets, KITTI Tracking dataset. However, the dataset consists of various video types including surveillance videos and moving-camera videos. Therefore, it motivates us to establish a public challenging urban surveillance video dataset which is more realistic to evaluate the performance of various algorithms for object tracking and behavior analysis.
- In the CONGEST model and in related models, where the nodes synchronously communicate with @math bits, much research is devoted to subgraph detection, e.g., @cite @cite @cite @cite @cite @cite @cite @cite .
- A related problem is property-testing of subgraphs, where the nodes need to determine, for a given subgraph @math , whether the graph is @math -free or from being @math -free @cite @cite @cite @cite @cite @cite .
- Whilst online fora have attracted much attention as a way of exploring political dynamics @cite @cite @cite @cite @cite @cite , and the effect of abuse and incivility in these contexts has been explored @cite @cite @cite @cite , little work exists regarding the abusive and intimidating ways people address politicians online; a trend that has worrying implications for democracy. collected tweets centred around candidates for the European Parliament election in 2014 from Spain, Germany, the United Kingdom and France posted in the month surrounding the election. They find that the extent of the abuse and harrassment a politician is subject to correlates with their engagement with the medium. Ward and McLoughlin ward2017turds find similar results to ours, for example regarding the greater abuse received by male MPs, in a two and a half month period running from late 2016 to early 2017; we contribute with a more in-depth exploration of how prominence relates to abuse received, and by studying four time periods over five years. They consider hate speech gendered slurs separately, however, and find that women receive more of these. They find that most Twitter abuse takes the form of a reply.
- A larger body of work has looked at hatred on social media more generally @cite @cite @cite . Williams @cite and Burnap @cite present work demonstrating the potential of Twitter for evidencing social models of online hate crime that could support prediction, as well as exploring how attitudes co-evolve with events to determine their impact. use natural language processing (NLP) to identify the groups targeted for hatred on Twitter and Whisper. Munger presents intriguing work on automated (bot) social sanctions, (e.g. munger2017tweetment munger2017tweetment ).
- A surge of recent interest aims to detect abuse, hate speech and toxicity automatically, resulting in increasing availability of training data and workshops focused on the topic @cite @cite @cite @cite @cite @cite @cite @cite . http: tinyurl.com alw-workshop-2017 http: tinyurl.com alw-workshop-2018 http: tinyurl.com alw-stack-overflow Schmidt and Wiegand provide a review of prior work and methods, as do Fortuna and Nunes . Reported performances vary widely depending on the specifics of the task. In very recent times attention has begun to turn to the issue of bias in abuse classification. Unintended bias, for example being more likely to label a text as abusive if it was penned by a particular demographic because that bias was present in the training data, has been highlighted as an issue in Google's Perspective'' toxicity scorer, http: tinyurl.com jigsaw-unintend-bias which they have recognized, and the overcoming of which is formulated as an explicit objective in their new competition. http: tinyurl.com jigsaw-kaggle-toxic Whilst unintended bias has been the subject of much research in recent years with regards to making predictive systems that don't discriminate, it has only just begun to be taken up within abuse classification @cite .
- The evolution of appearance feature extractor is from hand-craft @cite @cite @cite @cite to deep networks @cite @cite @cite @cite @cite . In this paper, we also utilize deep networks as our base appearance feature extractor. One crucial difference between the previous approaches lies in the way to build similarity from appearances. We utilize a hybrid of feature concatenation, cosine distance, location motion priors to compute the final similarities.
- The utilization of location motion features is common as well. Most existing methods assume a prior motion model, such as slow velocity @cite and linear non-linear motion model @cite . For example, the IoU trackers @cite rely on the assumption that objects in consecutive frames are expected to have high overlap, which is often not hold by fast moving objects. Other hard motion models also face the similar problem resulting in limited application. In this paper, instead of using hard location motion priors, we integrate both unary location and motion information into the feature and learn the soft location motion representation from data. Empirical studies on several benchmarks have proved the effectiveness of the learnable location representation.
- The topological information is also crucial for measuring similarity @cite . However, leveraging such non-grid topology of multiple objects is challenging. Only a few works successfully encode the topological information, e.g. the occupancy grid in @cite . However, this occupancy grid only counts the distribution of objects, without differentiating individual objects. In this paper, we utilize relation networks to encode the topological information for making the individual object differentiable and identifiable.
- Most existing methods utilize one or two cues for similarity computation, while only a few works trying to jointly learn all of them simultaneously @cite . Aggregating information across time @cite @cite @cite @cite is also rare. In addition, in order to learn the representations of different cues, these works usually adopt separate networks and sophisticated training strategy, e.g. a four-stage training is required by @cite .
- Recently, relation networks have been successfully applied in the fields of NLP, vision and physical system modeling @cite @cite @cite @cite @cite , in order to capture long-term, non-grid and heterogeneous dependencies between elements.
- Our approach is motivated by these works by extending the relation networks to multi-object tracking. In order to model the topological information of objects in the spatial domain and perform information aggregation over the temporal domain, we propose a spatial-temporal relation network. Although some recent works @cite @cite attempt to incorporate the attention mechanism into the multi-object tracking problem, they mainly aim at recovering salient foreground areas within a bounding box, thus alleviating the occlusion problem and ignoring the topology between objects.
- In this approach, researchers aimed to extract detectable features from a parsing tree for use in machine-translated text identification. For example, Chae and Nenova @cite claimed that parsing of machine text is commonly simpler than that of human one. The authors indicated that a simple parsing often contains short main constituents, that is noun, verb, and adjective phrases. Following the intuition, the authors extracted meaningful features, such as parsing tree depth, phrase type proportion, average phrase length, phrase type rate, and phrase length, before using them to distinguish computer- with human-generated text.
- @cite inherited several above features including parsing tree depth and phrase type proportion. In addition, they investigated that the structure of human parsing is more balancing than that of machine one. They thus suggested some useful features: the ratio of right- compared to left-branching nodes, the number of left-branching nodes for noun phrases. The main limitation of parsing-based methods is that they just generate a parsing tree for an individual sentence. An integrated tree cannot be built for larger scope of multiple sentences such as paragraph or document.
- To overcome the limitation of parsing-based approach, Arase and Zhou recommended another method @cite based on fluency estimation. They mainly used @math -gram language model to estimate the fluency of continuous words. The restriction of this model is that it efficiently examines only on few continuous words, common in three. The authors reduced the deficiency by using sequential pattern mining to measure the fluency of non-continuous words. In-fluent patterns in human text are mined, such as ,'' ,'' that contrast with that in machine-generated text, for example, ,'' .'' There are two other reasonable combinations also aim to diminish the restriction of @math -gram model. The first combination @cite extracted the specific noise words often used by a human, that is misspelled and reduction words, or by a machine, namely untranslated words. This combination, however, is only efficient in online social network in which contains a substantial number of such noises. The second combination @cite focused on functional words abundantly occurring in machine-translated text. Additional features in the three combinations achieve non-high performances but these features effectively improve the overall performances when they are integrated with the original @math -gram model.
- Another approach recognizes machine-generated text by analyzing a histogram of word distribution. For example, Labb 'e and Labb 'e suggested an inter-textual metric for estimating the similarity of word distributions @cite . This metric is perfectly used for classifying artificial and real papers with accuracy up to 100 method pointed out that a word distribution of human text is closer with a Zipfian distribution than that of machine one. They also offered some valuable features to support the word distribution, that is specific phrases (e.g., idiom, clich 'e , ancient, dialect, phrasal verb) and co-reference resolution. The restriction of distribution-based methods is that they are only stable with a large number of words. However, the deficiency is revealed on homologous texts that refers to same sources such as paraphrasing and translation because such text mostly contains a same set of words.
- The closest method with our work was suggested by Nguyen- @cite . They matched similar words in pairwise sentences of a paragraph. In two sentences, each word is only matched with another word at most so that total similarity of matching is maximum. We extend this idea by matching similar words in both internal and external sentences throughout a paragraph, so a word can be used as a bridge of other words in the text.
- The aim of single image SR is to estimate the mapping from LR to HR image pairs. Recently, DNN has been widely applied in image SR due to its ability of simulating complex mappings. Dong al @cite first proposed to approximate the mapping from LR to HR image pairs using a three layers convolutional neural network. Since then, other architectures, such as RNN @cite @cite @cite , ResNet @cite , and GAN @cite , have been applied in image SR.
- ResNet was first proposed by He al @cite for the task of classification. Ledig al @cite successfully introduce it to image SR and developed an approach referred as SRResNet. SRResNet preserved the batch normalization from original residual blocks, which was showed to consume amounts of memory in computation and restrict the range flexibility from networks for image deblurring @cite and SR @cite . Therefore, Lim al @cite proposed a new residual block by removing the batch normalization and developed an enhanced deep residual network referred as EDSR. The comparison of ResNet, SRResNet and EDSR is showed in Figure .
- The skip connections in ResNet are critical since they constraint a residual block to learn the residual between its input and output @cite . To explore the advantages of skip connection, DenseNet proposed to link all layers in the networks to efficiently train a very deep networks @cite . To adopt the idea of DenseNet for image SR, Tong al @cite developed a method, referred as SRDenseNet, to estimate the mapping from LR to HR image pairs. Motivated by the idea of EDSR and SRDenseNet, we will design two multi-scale deep neural networks for single image SR with unknown upscaling factors and downscaling operators in Section .
- There is a large array of recent results on community detection and graph clustering, in particular, under Z2, CBM and SBM. The readers are referred to the surveys @cite @cite @cite for comprehensive reviews. Without trying to enumerate this body of work, here we restrict attention to those that study sharp performance bounds, with a particular focus on work on the SDP relaxation approach. A more detailed, quantitative comparison with our results is provided in after our main theorems. To begin, we note that existing work has considered several recovery criteria for an estimator @math of @math : means @math is better than random guess, that is, @math ; means @math for a given @math ; means @math @cite .
- For SBM, SDP has been proven to succeed in exact and weak recovery above the corresponding optimal thresholds (sometimes under additional assumptions). In particular, see @cite @cite @cite for exact recovery, and @cite for weak recovery. Prior to our work, SDP was not known to achieve the optimal error rate between the exact and weak recovery regimes. Sub-optimal polynomial rates are first proved in @cite , later improved to exponential in @cite , and further generalized in @cite @cite . Robustness has been recognized as a distinct feature of the SDP approach as compared to other more specialized algorithms for SBMs. Work in this direction has established robustness of SDP against random erasures @cite @cite , atypical node degrees @cite and adversarial corruptions @cite @cite @cite @cite . The work in @cite investigates the relationship between statistical optimality and robustness under monotone semirandom models; we revisit this result in more details later.
- Interpretable models already exist. Of note, there are a number of classification models that are readily interpretable, for example decision trees, rule-based systems, and nearest-neighbour methods, each with varying levels of usability @cite . Decision trees generate a tree-like structure representing a series of tests on different features in a training dataset where leaf nodes represent various labeled classifications. Rules-based systems explicitly map an input to an action through some explicitly defined series of logical assertions ( rules). Nearest-neighbour algorithms qualify a classification based on the values of attributes in the immediate neighbourhood of some input.
- In @cite the authors produced an Open the black box taxonomy'' to better understand different approaches to explaining black box models. At the top level there are two options: To formulate a black box explanation, and design a transparent box. The latter has been discussed in some depth in @cite where its author argues that explanations are often unreliable and misleading, and therefore black box models should not be used for critical systems. She cites several examples where lack of transparency and accountability of predictive models have had severe consequences. Counter-arguments to interpretable ML models include black boxes holding significant value and intellectual property for the model authors, and that interpretable models can be more costly to construct. They further define the that requires an explanation model that is globally interpretable''; that is to be able to mimic the behaviour of the black box should be understandable by a human.
- Approaches to generating human-readable explanation models include the Automatic Statistician, a system that discovers plausible models from data and automatically presents its findings as figures and as natural language @cite @cite , LIME (Local Interpretable Model-agnostic Explanations) @cite , a method for isolating parts of a given input that contribute most to a classification, and QII (Quantitative Input Influence) @cite , a technique for calculating influences of individual inputs or groups of inputs, each provides explanations as summaries of local causal phenomena. A recent survey lists various methods in explainable AI, and notes that due to rise in autonomous systems and complex models, there is even more need for interpretable models @cite .
- To give a different example, Goerss-Hopkins towers were also used by the first author to prove that at large primes, there is an equivalence @math of homotopy categories between @math -local spectra and differential @math -comodules @cite . The proof of that result proceeds by comparing topological and algebraic towers and so crucially depends on the additional generality provided in this note.
- A different approach to Goerss-Hopkins obstruction theory is present in the work of Mazel-Gee @cite . Compared to ours, the approach of Mazel-Gee is closer in spirit to the original works of Goerss and Hopkins, generalizing the theory to a setting of an arbitrary presentable @math -category through the use of model @math -categories.
- There are a few recent papers @cite @cite @cite which develop unified account of various notions of algebraic theory.
- The papers by Curien @cite and Hyland @cite concentrate on clones, symmetric operads and non-symmetric operads, and concern primarily the conceptual understanding of the substitution monoidal structures. Via the theory of pseudo-distributive laws @cite , they reduce substitution monoidal structures to certain 2-monads on @math , for example the free cartesian category 2-monad in the case of clones. Their work illuminates the relationship between the notions of algebraic theory they treat and their standard metamodels, because the standard metamodels arise as Eilenberg--Moore algebras of the 2-monad from which the corresponding substitution monoidal structure is induced. On the other hand, monads and generalised operads do not seem to be captured by their framework.
- The framework by Avery @cite is relative to a well-behaved 2-category (which he calls a ). In the basic setting of @math he identifies algebraic theories with identity-on-objects functor from a certain category @math of arities, calling them . In this case, the relationship to our work may be established by the fact that (putting size issues aside) identity-on-objects functors from @math correspond to monoid objects in @math (with the profunctor composition as the monoidal structure). This way we may understand Avery's framework (with respect to the setting @math ) within ours, although for general setting probably we cannot do so. However we remark that for specific examples of settings treated in @cite , it seems that proto-theories therein can be identified with monoid objects in the category of a suitable variant of profunctors.
- Several prior works have considered streams of multiple images (or videos) for generating captions longer stories @cite @cite @cite @cite . Related tasks involving multi-image multi-sentence data, include: , who consider sorting aligned (image, caption) pairs into stories, image textual cloze tasks @cite @cite , and question-answering tasks @cite ; these tasks are usually supervised. Our work is similar in spirit to , who use align video clips subtitles with full books using labeled data.
- @cite proposes the so-called bottleneck block that attempts to reduce the number of @math filters using 2 convolutional layers with a @math kernel that project the features into a lower dimensional subspace and back. The authors from @cite introduce a new convolutional block that splits the module into a series of parallel sub-blocks with the same topology. The resulting block has a smaller footprint and higher representational power. In a similar fashion, MobileNet @cite and its improvement @cite make use of depth-wise convolutional layers with the later proposing an inverted bottleneck module. @cite , the authors combine point-wise group convolution and channel shuffle incorporating them in the bottleneck structure.
- Note, that in this work we do not attempt to improve the architecture itself and simply use the already well-established basic block with pre-activation introduced in @cite (see Fig. ).
- While a complete review of recent work on human pose estimation goes beyond the scope of this paper, the current state-of-the-art on single person human pose estimation is based on the so-called ''Hourglass (HG) architecture @cite and its variants @cite @cite @cite @cite . Most of this prior work focuses on achieving the highest performance without imposing any computational restrictions. Only recently, the work in @cite and @cite study this problem in the context of quantized neural networks. @cite the authors propose an improved HG architecture that makes use of dense connections @cite while @cite introduces a novel residual block specially tailored to binarized neural networks. In contrast with the aforementioned methods, in this work, instead of improving the network architecture itself, we propose a novel, improved binarization technique that is independent of the network and task at hand.
- One traditional way to analyze a 3D shape is to convert it into the regular volumetric occupancy grid and then apply 3D CNNs @cite @cite . The major limitation of these approaches is that 3D convolutions are more expensive in computations than 2D cases. In order to make the computation affordable, the volume grid size is usually in a low resolution. However, lower resolution means loosing some shape geometric information, especially in analyzing large-scale 3D shapes scenes. To overcome these problems, octree-based methods @cite @cite @cite have been proposed to allow applying 3D CNNs on higher adaptive resolution grids. PointGrid @cite is a 3D CNN that incorporates a constant number of points within each grid cell and allows it to learn better local geometric details. Similarly, @cite presented a 3D convolution operator based on a uniform grid kernel for semantic segmentation and object recognition on point clouds.
- PointNet @cite is the first attempt of applying deep learning directly on point clouds. PointNet model is invariant to the order of points, but it considers each point independently without including local region information. PointNet++ @cite is a hierarchical extension of PointNet model and learns local structures of point clouds at different scales. But @cite still considers every point in its local region independently. In our work, we address the aforementioned issues by defining the convolution operator that learns the relationship between neighboring points in a local region, which helps to better capture the local geometric properties of the 3D object.
- @cite proposed a new deep learning architecture called Kd-networks, which uses kd-tree structure to construct a computational graph on point clouds. KCNet @cite improves PointNet model by considering the local neighborhood information. It defines a set of learnable point-set kernels for local neighboring points and presents a pooling method based on a nearest-neighbor graph. PCNN @cite is another method to apply convolutional neural networks to point clouds by defining extension and restriction operators, and mapping point cloud functions to volumetric functions. SO-Net @cite is a permutation invariant network that utilizes spatial distribution of point clouds by building a self-organizing map. There are also some spectral convolution methods on point clouds, such as SyncSpecCNN @cite and spectral graph convolution @cite . Point2Sequence @cite learns the correlation of different areas in a local region by using attention mechanism, but it does not propose a convolution on point clouds. PointCNN @cite is a different method that proposes to transform neighboring points to the canonical order and then apply convolution.
- Recently, there are several approaches proposed to process and analyze large-scale point clouds from indoor and outdoor environments. @cite extended PointNet model to exploit the large-scale spatial context. @cite proposed a pointwise pyramid pooling to aggregate features at local neighborhoods as well as two-directional hierarchical recurrent neural networks (RNNs) to learn spatial contexts. However, these methods do not define convolutions on large-scale point clouds to learn geometric features in the local neighborhoods. TangentConv @cite is another method that defines the convolution on point clouds by projecting the neighboring points on tangent planes and applying 2D convolutions on them. The orientation of the tangent image is estimated according to the local point shape curvature, but as we know the curvature computation on the local region of the point clouds is not stable and not robust (see the discussion in Sec. ), which makes it orientation-dependent. Instead, our method proposes an annular convolution, which is invariant to the orientations of local patches. Also, ours does not require additional input features while theirs needs such features (e.g., depth, height, etc.).
- Besides point cloud based methods, several approaches have been proposed to develop convolutional networks on 3D meshes for shape analysis. Geodesic CNN @cite is an extension of the Euclidean CNNs to non-Euclidean domains and is based on a local geodesic system of polar coordinates to extract local patches. Anisotropic CNN @cite is another generalization of Euclidean CNNs to non-Euclidean domains, where classical convolutions are replaced by projections over a set of oriented anisotropic diffusion kernels. Mixture Model Networks (MoNet) @cite generalizes deep learning methods to non-Euclidean domains (graphs and manifolds) by combining previous methods, e.g., classical Euclidean CNN, Geodesic CNN, and Anisotropic CNN. MoNet proposes a new type of kernel in parametric construction. Directionally Convolutional Networks (DCN) @cite applies convolution operation on the triangular mesh of 3D shapes to address part segmentation problem by combining local and global features. Lastly, Surface Networks @cite propose upgrades to Graph Neural Networks to leverage extrinsic differential geometry properties of 3D surfaces for increasing their modeling power.
- Some works @cite @cite learn deep image priors based on generative models, such as generative adversarial networks (GAN) @cite and variational auto-encoders (VAE) @cite which are successful in modelling the complex data distribution of realistic images. In @cite , a deep densely connected generative network is trained with a Markovian patch discriminator. DeblurGAN is proposed in @cite where Wasserstein loss is used to circumvent the problems such as model collapse and gradient vanishing.
- To handle degenerated images with different levels, a line of works @cite @cite @cite learn CNN optimizers to mimic the propagation of image update in conventional gradient-based optimization. Particularly, in @cite , all main operations of image propagation using gradient descent, including the gradient of image prior, are parameterized with a trainable network for non-blind deconvolution. With given fixed kernel, the iterative gradient descent update is estimated by the network based only on the last update. Hence for the recurrent network each estimation remains to be the same task, which limits this architecture to non-blind deconvolution. In @cite @cite , the propagations of image updates are performed by pretrained CNNs which is irrelevant to deconvolution task, and the prior is corrected by optimization-based projection.
- Many existing techniques target facial expression recognition in images and video sequences @cite . Earlier works on facial expression recognition were concentrated on images @cite @cite @cite @cite @cite @cite @cite @cite @cite . However, they do not consider temporal variations. Facial expression process is a dynamic event which takes minute motion changes through time into account. Before the era of deep learning, hand-crafted features were used to extract spatio-temporal information and to classify facial expressions. We give a brief overview of various methods that have achieved good performance on facial expression video sequences below.
- For facial expression analysis in video sequences, many image-based features are extended in order to get temporal features along with spatial information such as LBP-TOP @cite , 3D-HOG @cite , and 3D-SIFT @cite . Jain used conditional random fields and manually created shape-appearance features for temporal modeling of each facial shape @cite . Sanin proposed spatio-temporal covariance descriptors using Riemannian locality preserving projection approach for action and gesture recognition @cite . Wang proposed an Interval Temporal Bayesian Network (ITBN) for capturing complex spatio-temporal relations among facial muscles @cite . Liu proposed an expressionlet-based spatio-temporal manifold method for dynamic expression recognition @cite . Ptucha proposed a Manifold-based Sparse Representation (MSR) for expression recognition by mapping features in low dimensional manifolds using supervised locality preserving projections @cite . Recently, Sikka proposed a Latent Ordinal Model (LOMo) for facial expression recognition in videos @cite . LOMo integrates features extracted from SIFT around the facial landmarks and LBP using a weakly supervised classifier to learn the expressions as hidden variables.
- Deep learning-based models have achieved state-of-the-art results in facial expression recognition. Liu applied 3D CNN with deformable action part constraints (3D CNNDAP) to the problem of expression recognition @cite . Recent models use geometric features like facial landmarks to further boost the accuracy. Jung proposed two separate networks called DTAN and DTGN and jointly fine-tuned the two networks to achieve state-of-the-art performance @cite . The DTAN network is a simple 3D convolutional network that captures spatio-temporal information from temporal image sequences. The DTGN network is a fully-connected network that captures temporal variations in facial landmarks. Guo improved Jung 's result and trained a spatial network (MSCNN) and a temporal network (PHRNN) separately and jointly fine-tuned them @cite . MSCNN is a simple convolutional network on peak expression images. PHRNN is a collection of subnets (recurrent neural networks) that are connected in a binary tree-like structure. Facial landmarks are divided into four parts and passed at the bottom of this structure and the outputs of the subnets are concatenated at the next layer. The process is repeated for upper layers and the final layer is a softmax classification layer.
- As mentioned previously, existing temporal action detection methods can be divided into two categories: two-stage approaches and one-stage approaches. The two-stage approaches first generate action instances and then classify them. Some works focus on proposal generation @cite @cite @cite @cite @cite @cite while others concentrate on classification @cite @cite @cite . The one-stage approaches integrate proposal and classification into a single step. SSAD @cite utilizes 1D convolution to generate multiple temporal anchor action instances for action classification and boundary box regression. The Single Stream Temporal Action Detection (SS-TAD) @cite utilizes the RNN-based architecture to learn action proposal and classification jointly. @cite explored RNN to learn a glimpse policy for predicting the starting and ending points of actions in an end-to-end manner. Nevertheless, both two-stage and one-stage approaches have drawbacks that the former adopts the indirect optimization and the later usually generates inaccurate proposals. In this research, we leverage the advantages of both directions and overcome their weaknesses by involving two separate branches into the one-stage framework.
- . Language-based methods take advantage of the mapping between phonemes and their visual counterpart visemes. For example, @cite proposed the JAw and LIp (JALI) model, a two-dimensional space that represents the jaw and lip movements of a facial animation based on psycholinguistic considerations. The main disadvantage of their study is the need for the speech signal, its text transcript and their alignment to create the facial animation. In another study, @cite first proposed generating dynamic units for visual speech for realistic visual speech animation. In a more recent study, the authors @cite initially transcribe the speech signal to phoneme labels, which are then fed to a deep fully connected network to predict person-specific shape and appearance parameters obtained by Active Appearance Model (AAM). A main limitation of this work is the need for speech to phoneme labels conversion.
- Also related are network formation games where not centrality but some other property is the goal of each agent. There are games where agents simply want to be connected to all other agents, e.g. @cite @cite @cite . Among them, the work by @cite is closely related to our work. In their wireline strong connectivity game agents are points in the Euclidian plane who strategically buy incident edges to create a connected network. The edge price equals the length of the edge. This is similar to our model in the Euclidian plane with @math but the focus on connectivity changes the game completely. Another related geometric game was proposed by @cite . Also there the agents are points in some metric space but agents pay a fixed price for each edge and try to minimize the total stretch towards all other agents. Guly 'a @cite considered a network formation game in the hyperbolic plane where agents strive for maximum navigability. This is also a geometric model but drastically different from our approach.
- There are many classical optimization problems related to network design, e.g. see the survey by Magnanti & Wong @cite . Many of them are NP-complete, e.g. all the problems labeled ND'' in @cite . Our model is closely related to the Network Design Problem @cite and the Optimum Communication Spanning Tree Problem (ND7 in @cite ). In particular, finding the social optimum network corresponds to a variant of the Network Design Problem, where, instead of having separate budgets for buying edges and for the routing cost, the sum of edge costs and routing costs, i.e., the total distance between all pairs of nodes, is to be minimized. Hence we strongly suspect that computing the social optimum in all versions of our model, with the and the as exceptions, is NP-hard.
- In past few years, word embeddings have found a major role for solving different tasks in multiple domains like Natural Language Processing and Machine Learning. Word embedding is basically a representation of a word in a vector space and there are different ways to learn this representation- to model semantic or syntagmatic relationships. Traditionally, techniques like Singular Value Decomposition (SVD) and Principal Component Analysis (PCA) were applied to generate dense word representations. With the advent of neural networks, it became possible to learn more enhanced word representations. Different variants of word embeddings such as word2vec @cite , GloVe @cite have been proposed for learning dense distributed representation of words. These window based methods count the number of co-occurrences of terms in the training corpus and suggest how likely is a term to occur with other terms. Such methods have been highly effective in solving different problems such as Text Summarization, Sentiment Analysis, and Machine Translation. Later, the idea of neural word embeddings was extended to learn document-level embeddings @cite which opened further interesting use-cases. Information Retrieval is one such domain.
- These days, mobile applications form a large chunk of data which is available for consumption. Some work has been done to mine user-reviews on mobile applications @cite . Mobile app retrieval has been experimented by few researchers to learn application representation using topic modeling @cite and intention in social media text @cite . Very recent work has been done to build a unified mobile search framework using neural approaches to learn high-dimensional representations for mobile apps @cite . Also, there are recent attempts made to use Learning-to-Rank algorithms for optimizing retrieval of mobile notifications triggered by installed apps @cite . However, application embeddings have been rarely utilized to perform query expansion, nearest neighbor analysis and other tasks related to mobile applications.
- @cite does not mitigate a stateless retry during the first connection to a member of a group of hostnames that have an exiting trust-relation with each other. Furthermore, these groups are usually rather small @cite , thus only about 60 , validation tokens obtained from a member of such a group can be considered as trustworthy because these members share secret cryptographic state with each other such as a private key of a X.509 certificate.
- A large body of work exists in differential privacy @cite @cite . Differential privacy provides guarantees that a model trained on some dataset will produce statistically similar predictions as a model trained on another dataset which differs in exactly one sample. This is one way in which Alice can defend her model @cite , but note that differential privacy is a stronger notion and often involves a cost in Alice's model accuracy. Membership inference assumes that content of the data is known to Bob and only is concerned whether it was used. Differential privacy also protects the content of the data (i.e., the actual words in should not be inferred).
- Initially introduced by @cite , Dense Trajectories are classically generated by computing motion and texture features around motion trajectories. Due to their popularity, many researchers have extended this original formulation in order to enhance their performance @cite @cite @cite @cite @cite .
- As a first attempt, @cite proposed to reinforce Dense Trajectories by using the Random Sampling Consensus (RANSAC) algorithm to reduce the noise caused by motion. In addition to that, they have replaced the Bag-of-Visual-Words representation with Fisher Vectors.
- Then, @cite suggested enriching motion trajectories using depth information. They proposed a model grouping the videos in two types: videos with high level of motion and others with low amount of motion. For the first group, an extension of Trajectory Shape Descriptor @cite which includes depth information has been used, while for the second group a novel descriptor called Speeded Up Robust Features (SURF) has been introduced in order generate local depth patterns.
- To further improve the accuracy of recognition, @cite proposed to use deep learned features instead of heuristic spatio-temporal local ones such as Trajectory-Shape Descriptor (TSD) @cite , Histogram of Oriented Gradients (HOG) @cite , Histogram of Optical Flow (HOF) @cite , and Motion Boundary Histogram (MBH) @cite .
- On the other hand, in @cite , a novel approach to encode relations between motion trajectories has been presented. Global and local reference points have been used to compute Dense Trajectories, offering robustness to camera motion.
- Finally, @cite had the idea of focusing on trajectory groups which contribute more importantly to a specific action by defining an optimization problem. Towards the same direction, @cite proposed the extraction of features around joint trajectories, increasing the discriminative power of the original Dense Trajectories approach @cite . Although all the aforementioned methods have shown their effectiveness, they unfortunately lack locality information related to the human body. This piece of information is crucial when actions include similar motion patterns performed by different body parts. For this reason, we propose a novel dense trajectory-based approach by taking into consideration the local spatial repartition of motion with respect of the human body.
- The first class of methods extracts directly human motion information from depth maps @cite @cite @cite @cite @cite @cite @cite @cite @cite . The second group gathers approaches which make use of the 3D skeletons extracted from depth maps. During the past few years, a wide range of methods have been designed using this high-level modality @cite @cite @cite @cite @cite @cite @cite . Compared to depth-based descriptors, skeleton-based descriptors require low computational time, are easier to manipulate and can better discriminate local motions. However, they are more sensitive to noise since they widely depend on the quality of the skeleton. Thus, to reinforce action recognition, a third class of methods called makes use of more than two modalities. These approaches usually exploit the skeleton information to compute local features using RGB and or depth images. These local RGB-D based features have shown noteworthy potential @cite @cite @cite . Inspired by this relevant concept which aims at computing local depth-based and RGB-based features around specific joints, we propose to adapt the same idea to Dense Trajectories which have been proven to be one of the most powerful action representations.
- Hydranets @cite is a hard-routing approach for vision that can also be trained with gradient descent, but requires an unsupervised clustering-based method to partition examples to perform well. Increasing the number of experts evaluated at inference time improves performance of Hydranets, but results in increased inference cost. does not require auxiliary loss functions for learning the routing functions, and allows for the use of all experts at a small inference cost.
- . propose the use of a small network to generate weights for a larger network. For vision, these weights are not conditional on the input, and thus have lower parameter count but also higher computation cost and reduced performance. Attention-based methods scale previous layer inputs based on learned attention weights. Hard-attention @cite uses only a subset of the inputs and has parallels to hard conditional routing. Soft-attention @cite @cite uses all parts of the inputs but upweights some parts, which is more in-line with our proposed technique.
- . In recent work, SplineNets @cite propose the use of B-splines to model learnable weights for continuous neural decision graphs. Compared to SplineNets, explores more complex weight-generating functions.
- Recent years have witnessed significant advances in single image deblurring by CNN-based models. Several methods @cite @cite use CNNs to estimate the non-uniform blur kernels. A conventional non-blind deblurring algorithm @cite is used removing blur, which is time-consuming. More recently, many end-to-end CNN models for image deblurring have also been proposed @cite @cite @cite @cite @cite @cite . To obtain a large receptive field in the network for blur removal, @cite and @cite develop a very deep multi-scale networks in coarse-to-fine manner. Different from @cite , Tao @cite share the weights of the network at three different spatial scales and use the LSTM to propagate information across scales. To handle spatially variant blur in dynamic scenes, Zhang @cite adopt a VGG network to estimate the pixel-wise weights of the spatially variant RNNs @cite for blur removal in feature space. Noroozi @cite build skip connections between the input and output, which reduces the difficulty of restoration and ensures color consistency. In addition, the adversarial loss is used in @cite @cite to restore more texture details.
- Single image de-raining is a challenging and ill-posed task. Traditional methods are designed by using handcrafted image features to describe physical characteristics of rain streaks, or exploring prior knowledge to make the problem easier. Kang @cite attempt to separate rain streaks from the high frequency layer by sparse coding. In @cite @cite , low-rank assumptions are used to model and separate rain streaks. Kim @cite first detect rain streaks and then remove them with the nonlocal mean filter. Luo @cite propose a framework to rain removal based on discriminative sparse coding. Li @cite exploit Gaussian mixture models to separate the rain streaks. A limitation of many of these methods is that they tend to over-smooth the resulting image output @cite @cite .
- Recently, deep learning has sped up the progress of single image deraining. In @cite , a deep network takes the image detail layer as its input and predicts the negative residual as output. In @cite , a recurrent dilated network with multi-task learning is proposed for joint rain streaks detection and removal. In @cite , Zhang propose a density-aware multi-stream densely connected convolutional neural network (DID-MDN) for joint rain density estimation and rain removal. These methods learn a mapping between synthesized rainy images and their corresponding ground truths. A major drawback however, is that this can lead to poor generalization ability to real rainy images that are not easily synthesized for training.
- Since the segmentation is a character-level task and POS tagging is a word-level task, an intuitive idea is to transfer both the tasks into character-level and incorporate them in a uniform framework. A popular method is to assign a cross-tag to each character @cite . The cross-tag is composed of a word boundary part and a POS part, e.g., B-NN'' refers to the first character in a word with POS tag NN''. Thus, the joint CWS and POS tagging can be regarded as a sequence labeling problem. Following this work, utilized neural models to alleviate the efforts of feature engineering. Another line of the joint segmentation and POS tagging method is transition-based method @cite @cite , in which the joint decoding process is regarded as a sequence of action predictions. used a simple yet effective sequence-to-sequence neural model to improve the performance of the transition-based method.
- As RPL or an extension of RPL are the most used routing protocols in IoT networks. We now briefly discuss the security challenges that these protocols might face during the routing process. Authors in @cite propose a sinkhole attack mitigation method that integrates rank authentication with parent fail-over. The proposal uses DIO message along with the one way hash function technique for rank authentication. The root node generates hash value by selecting random numbers, and broadcast these values through DIO messages. When the root node again broadcast the initially selected random number securely then intermediate nodes can verify its parent rank using the intermediate hops number. @cite authors propose a Merkle tree authentication based solution which can be used to prevent wormhole attack on RPL protocol. In this proposal, the RPL tree is formed in the reverse direction by using the node ID and public key which are used to calculate the hash values. After the Merkle tree formation, the authentication for any node starts from the root node and if any intermediate node fails to authenticate, a possible wormhole is detected.
- Authors in @cite investigates the forwarding misbehaviour (i.e., selective packet discarding) and propose a countermeasure of the same in LLNs running with RPL protocol. The basic idea is to monitor the forwarding (mis)behaviour of each node to observe the packet loss rate, and then compare the packet loss rate of the parent node with the neighbor nodes. To ensure that the packet loss is due to misbehaviour and not due to bad channel quality, the nodes use one time retransmission techniques. Similarly, using the monitoring information of the nodes about the data packets forwarding, a trust-based intrusion detection system based on RPL is presented in @cite , to countermeasure mobile sybil attacks in IoT networks.
- Traditional word-level vector representations, such as word2vec @cite , GloVe @cite , and fastText @cite , express all possible meanings of a word as a single vector representation and cannot disambiguate the word senses based on the surrounding context. Over the last two years, ELMo @cite and BERT @cite present strong solutions that can provide contextualized word representations. By pre-training on a large text corpus as a language model, ELMo can create a context-sensitive embedding for each word in a given sentence, which will be fed into downstream tasks. Compared to ELMo, BERT is deeper and contains much more parameters, thus possessing greater representation power. More importantly, rather than simply providing word embeddings as features, BERT can be incorporated into a downstream task and gets fine-tuned as an integrated task-specific architecture.
- BERT has, in general, been found to be superior to ELMo and far superior to non-contextual embeddings on a variety of tasks, including those in the clinical domain @cite . For this reason, we only examine BERT here, rather than including ELMo or non-contextual embedding methods.
- Several works have explored the utility of contextual models in the clinical and biomedical domains. BioBERT @cite trains a BERT model over a corpus of biomedical research articles sourced from PubMed https: www.ncbi.nlm.nih.gov pubmed article abstracts and PubMed Central https: www.ncbi.nlm.nih.gov pmc article full texts. They find the specificity offered by biomedical texts translated to improved performance on several biomedical NLP tasks, and fully release their pre-trained BERT model.
- On clinical text, @cite uses a general-domain pretrained ELMo model towards the task of clinical text de-identification, reporting near state-of-the-art performance on the i2b2 2014 task @cite @cite and state of the art performance on several axes of the HIPAA PHI dataset.
- @cite trains an ELMo model over a corpus of mixed clinical discharge summaries, clinical radiology notes and medically oriented wikipedia articles, then demonstrates improved performance on the i2b2 2010 task @cite . They release a pre-trained ELMo model along with their work, enabling further clinical NLP research to work with these powerful contextual embeddings.
- @cite , released in late February 2019, train a clinical note corpus BERT language model and uses complex task-specific models to yield improvements over both traditional embeddings and ELMo embeddings on the i2b2 2010 and 2012 tasks @cite @cite and the SemEval 2014 task 7 @cite and 2015 task 14 @cite tasks, establishing new state-of-the-art results on all four corpora. However, this work neither releases their embeddings for the larger community nor examines the performance opportunities offered by fine-tuning BioBERT with clinical text or by training note-type specific embedding models, as we do.
- The earliest work of applying machine learning on reasoning in large theories is @cite . The most most similar works to ours are TacticToe @cite and GamePad @cite . TacticToe is the first published result on machine learning tackling higher-order theorem proving at a relatively large scale at tactic level @cite . Although TacticToe is a great success that came with significant improvements over previous automated theorem proving systems, they do not propose an easy to use benchmark or environment for machine learning researchers. TacticToe does not employ deep learning nor reinforcement learning. They rely on the HOL4 @cite system that has a significantly less theorems with more complex human proof scripts with a larger number of more elementary tactics.
- GamePad has very similar objectives to ours @cite . They also provide an easy-to-use Python API for an interactive theorem prover, and they present test and training sets. They chose to base their system on Coq @cite , an interactive theorem prover based on the calculus of inductive constructions. While enabling automatic code extraction, it comes with a much smaller coverage of fundamental mathematics. Even including the formalization of the Feit-Thompson theorem, their benchmark comprises only 1602 theorems and lemmas, while ours features 29462 theorems and lemmas. Besides presenting a much larger data set, we also demonstrate the feasibility of achieving state-of-the-art prover performance based on our data and environment by presenting a deep learning based theorem prover. We also report the results as theorem proving performance instead of proxy metrics.
- Other interactive theorem provers we could have based a learning environment on include Mizar @cite , Isabelle @cite , HOL4 @cite , and Lean @cite . The Mizar mathematical library is probably the most comprehensive formalization effort, but its declarative style makes it hard to employ proof search, and its source code is not freely available. Like Coq and HOL Light, also Isabelle @cite was used for major formalization efforts, such as the formalization of the seL4 microkernel @cite . We are not aware of a comprehensive coverage of fundamental mathematics in Isabelle, HOL4, or Lean.
- In closely related work, translate from HOL Light and Flyspeck to automated theorem provers and SMT solvers, for which they learn a premise selector. In contrast to our work, they use neither deep learning nor reinforcement learning. Similar methods for premise selection on the HOL Light corpora were proposed in @cite .
- The first use of deep neural networks for large scale theorem proving was proposed in @cite . They have used convolutional networks for premise selection in large theories, particularly on Mizar mathematical library @cite . Those methods were used as a pre-selection for applying the first order logic automated theorem prover E @cite . We have reused several ideas from that paper, including some aspects of our neural network architecture and the hard negative mining methodology.
- proposed deep neural networks to augment theorem prover E @cite to rank given clauses during proof search. Here, we propose a neural prover written from scratch, relying solely on a small set of preexisting tactics and neural networks for all high level decisions.
- Earlier works on employing (non-deep) machine learning for theorem proving in general and for reasoning in large theories include @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . Recently, proposed a premise selection method utilizing deep graph embeddings.
- RESCAL @cite was the first bilinear model, where each relation is represented by a square matrix and the score of the triple @math is calculated by a bilinear map which corresponds to the matrix of the relation @math and whose arguments are @math and @math . Hence, RESCAL represents the most general form of a bilinear model. Extensions of RESCAL have been proposed by restricting bilinear functions, for example, DistMult @cite and ComplEx @cite restrict the matrices representing the relations to diagonal matrices.
- The main advantage of observed feature models over knowledge graph embedding models is their interpretability. Additionally, proposed a relatively simple logistic regression model, the Node+LinkFeat model, which utilizes only one-hop information in a knowledge graph and demonstrated that it performs far better for link prediction on standard datasets than most existing knowledge graph embedding models. However, it has also been shown that the Node+LinkFeat model cannot deal with a low-redundancy dataset because the model uses information which is too local. On the other hand, it has shown that a logistic regression model, the PRA model @cite @cite , which utilizes multi-hop information do not have sufficient accuracy @cite . This suggests logistic regression does not have enough power to deal with deep information. These studies have motivated research toward developing a more efficient model utilizing deeper information.
- UDA is a specific area of transfer learning @cite , which is to learn a discriminative model in the presence of the domain-shifts between domains. The main problem of UDA is how to reduce the domain shift between the source and target domains. There are many methods to tackle this problem: traditional (shallow) learning and deep learning.
- Traditional (shallow) learning methods have several aspects: 1) Subspace learning. Subspace Alignment (SA) @cite aligns the base vectors of both domains and Subspace Distribution Alignment (SDA) @cite extends SA by adding the subspace variance adaptation. CORAL @cite aligns subspaces in second-order statistics. 2) Distribution alignment. Joint Distribution Adaptation (JDA) @cite is proposed to match both distributions with equal weights. Later works extend JDA by adding structural consistency @cite and domain invariant clustering @cite . But these works treat the two distributions equally and fail to leverage the different importance of distributions. Recently, Wang proposed the Manifold Embedded Distribution Alignment (MEDA) @cite @cite approach to dynamically evaluate the different effect of marginal and conditional distributions and achieved the state-of-the-art results on domain adaptation.
- As for deep learning methods, CNN can learn nonlinear deep representations and capture underlying factors of variation between different tasks @cite . These deep representations can disentangle the factors of variation, which enables the transfer of knowledge between tasks @cite . Recent works on deep UDA embed domain-adaptation modules into deep networks to improve transfer performance @cite @cite @cite @cite , where significant performance gains have been obtained. UDA has wide applications in computer vision @cite @cite and natural language processing @cite and is receiving increasing attention from researchers.
- These years, for better accuracy, designing deeper and wider CNN models has become a general trend, such as VGGNet @cite and ResNet @cite . However, as the CNN grow bigger, it is harder to deploy these deep models on resource constrained devices. Network compression becomes an efficient way to solve this problem. Network compression methods mainly include network quantization, low-rank approximation and weight pruning. Network quantization is good at decreasing the presentation precision of parameters so as to reduce the storage space. Low-rank approximation reduces the storage space by low-rank matrix techniques, which is not efficient for point-wise convolution @cite . Weight pruning mainly includes two methods, neural pruning @cite @cite and channel pruning @cite @cite @cite @cite .
- Channel pruning methods prune the whole channel each time, so it is fast and efficient than neural pruning which removes a single neuron connection each time. It is a structured pruning method, compared to network quantization and low-rank approximation, it does not introduce sparsity to the original network structure and also does not require special software or hardware implementations. It has demonstrated superior performance compared to other methods and many works @cite @cite @cite have been proposed to perform channel pruning on pre-trained models with different kinds of criteria.
- TCP is primarily motivated by @cite , while our work is different from it. TCP is presented for pruning unsupervised domain adaptation models. To be more specific, we take the discrepancy between the source and target domains into consideration so we can prune the less important channels not just for the source domain but also for the unlabeled target domain. We call this Transfer Channel Evaluation, which is highlighted in yellow in Fig. .
- As supervised learning methods often require a lot of training data, active learning is a technique that selects a subset of data to annotate for training the best classifier. Existing active learning (AL) algorithms can be generally considered as three categories: 1) uncertainty sampling @cite @cite , which selects the data about which the current classifier is the most uncertain; 2) query by committee @cite , which selects the data about which the committee'' disagree most; and 3) expected error reduction @cite , which selects the data that can contribute the largest model loss reduction for the current classifier once labelled. Applications of active learning to NLP include text classification @cite @cite , relation classification @cite , and structured prediction @cite @cite @cite @cite . Qian2014BilingualAL used uncertainty sampling to jointly perform on English and Chinese. stratos2015simple and zhang2016name deployed uncertainty-based AL algorithms for languages with the minimal supervision.
- Deep reinforcement learning (DRL) is a general-purpose framework for decision making based on representation learning. Recently, there are some notable examples include deep Q-learning @cite , deep visuomotor policies @cite , attention with recurrent networks @cite , and model predictive control with embeddings @cite . Other important works include massively parallel frameworks @cite , dueling architecture @cite and expert move prediction in the game of Go @cite , which produced policies matching those of the Monte Carlo tree search programs, and squarely beaten a professional player when combined with search @cite . DRL has been also studied in NLP tasks. For example, recently, DRL has been studied for information extraction problem @cite . They designed a framework that can decide to acquire external evidence and the framework is under the reinforcement learning method. However, there has been fairly little work on using DRL to learn active learning strategies for language processing tasks, especially in cross-lingual settings.
- Recent deep learning work has also looked at transfer learning @cite . More recent work in deep learning has also considered transferring policies by reusing policy parameters between environments @cite @cite , using either regularization or novel neural network architectures, though this work has not looked at transfer active learning strategies between languages with shared feature space in state.
- We presented related work in three dimensions: cascade structures, dynamics and topics. The cascade structures have been studied in different scenarios, including blogs @cite @cite @cite , communication network @cite , Facebook @cite , Twitter @cite , LinkedIn @cite . , or to infer structures @cite . Deep-tree structures are found and modeled in Ref. @cite . Goel @cite studied the virality of cascades by using Wiener index. However, we find information flows in cascades have four directions, indicating complex structures than tree patterns. Furthermore, ignoring the high-order correlations between structure metrics in previous studies prevent our understandings of cascade structures.
- The dynamics of cascades have been empirically studied in marketing @cite , Twitter @cite , Facebook @cite , QQ @cite , and so on. Another line of works try to model the dynamics from differential equation and micro-process @cite @cite . The prediction of the dynamics are studied in Refs. @cite @cite @cite . Time series clustering methods are applied to discover the dynamics patterns or shapes @cite @cite . Our work is based on the clustering method to find out the dynamics clusters in empirical cascade growth dynamics. However, in order to study the correlations between dynamics and structures, we propose new similarity intuitions and find the existing shift-variant and scale-variant algorithm can not do this job.
- Previous study shows different topics of information spreading follow different mechanism @cite . Celebrated topic modeling method LDA @cite is not applicable to Twitter or Weibo scenarios @cite . In oder to get high precision in topic modeling, we adopt information filtering methods inspired by @cite @cite .
- Extractive summarization has seen growing popularity in the past decades @cite . The methods focus on selecting representative sentences from the document(s) and optionally deleting unimportant sentence constituents to form a summary @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . A majority of the methods are unsupervised. They estimate sentence importance based on the sentence's length and position in the document, whether the sentence contains topical content and its relationship with other sentences. The summarization objective is to select a handful of sentences to maximize the coverage of important content while minimizing summary redundancy. Although unsupervised methods are promising, they cannot benefit from the large-scale training data harvested from the Web @cite @cite @cite .
- The core idea of training a deep generative model by making it compete with an opponent discriminator model was proposed in @cite where a two-player minimax game is formulated. A straightforward extension of incorporating condition information into both generator and discriminator was put forward in @cite in which the generator can produce realistic images of particular categories. There are mainly two aspects that can be modified to enhance the stability of GAN training: model setup @cite @cite @cite and optimization method @cite @cite @cite . Although there have been a large number of studies on realistic image generation tasks which provided promising results, much fewer research efforts, to our knowledge, have been devoted to continuous time-series data generation with adversarial training @cite @cite . Our task is different from existing works where the generators are trained in unsupervised learning settings without any visualizable groundtruth distribution. This makes it hard to evaluate the learning performance. On the contrary, in this work the groundtruth actions can be utilized to monitor the training quality by calculating a proper error metric as well as provide a heuristic during the training process.
- In recent years, widely studied multi-target tracking and prediction approaches can be mainly classified into two groups: (a) end-to-end solutions based on deep learning and computer vision techniques; (b) state estimation based on Bayesian inference. In this paper, we only focus on the latter one with an emphasis on particle filter (aka. sequential Monte Carlo) due to its effectiveness and flexibility. @cite provided a comprehensive summary of particle filter techniques. In the authors' previous work @cite , a generic multi-target tracking framework without explicit data association was proposed which can incorporate arbitrary learning-based state evolution models to enhance tracking performance. The framework can be applied to prediction problems as well provided that the measurement update is removed. In this work, we employ the generator network in GAN as an implicit proposal distribution with high flexibility and generalizability which serves as the state evolution model in the tracking and prediction framework.
- . Model compression and acceleration aim to create network with few computation and parameters cost meanwhile maintaining high performance. A straight way is to design lightweight but powerful network since the original convolution network has many redundant parameters. For example, depthwise separable convolution is used to replacing standard convolution for building block in @cite . Pointwise group convolution and channel shuffle are proposed to reduce the burden of computation while maintaining high accuracy in @cite . Another way is network pruning which boosts the speed of inference by pruning the neurons or filters with low importance based on certain criteria @cite @cite . In @cite @cite , weights were decomposed through low-rank decomposition to save memory cost. Quantization seeks to use low-precision bits to store model's weights or activation outputs @cite @cite @cite .
- Different from above offline training methods, several works adopts collaboratively training strategy. Deep mutual learning @cite conducts distillation collaboratively for peer student models by learning from each other. Anil al @cite further extend this idea by online distillation of multiples networks. In their work, networks are trained in parallel and the knowledge is shared by using distillation loss to accelerate the training process.
- Besides, there are several works utilizing adversarial method to modeling knowledge transfer between teacher and student @cite @cite @cite . In @cite , they adopt generative adversarial networks combined with distillation to learn the loss function to better transfer teacher's knowledge to student. Byeongho al @cite adopt adversarial method to discover adversaial samples supporting decision boundary.
- In the last decades, neural network and learning based methods were widely employed in image processing tasks and achieved impressive success. Some image fusion methods also try to utilize machine learning to help focus level estimation. For example, some methods @cite @cite @cite @cite adopt pulse coupled neural networks (PCNN) to do image fusion where PCNN does not require the training procedure. Some other methods @cite @cite @cite train neural networks or dictionaries where focus level estimation was regarded as a classification problem to divide a local region into an in-focus or out-of-focus one. To do the training, @cite @cite collect some all-in-focus images and manually blur them using the Gaussian filters, while @cite manually labels in-focus out-of-focus regions from source images as training sets.
- Besides, different edge-preserving smoothing filters were explored in earlier works to make full use of spatial consistency during the fusion process. CBF @cite adopts the cross bilateral filter (CBF) to compute the weights to measure the strength of details in multi-focus source images. Li @cite , Nejati @cite and Guan @cite adopted the guided filter to do the fusion. In this work, a recent proposed state-of-the-art edge-preserving smoothing filter, the fast bilateral solver, is first adopted in image fusion task. Moreover, the confidence map is proposed to help the focus-level estimation with the fast bilateral solver in BS-QEBIF.
- Resource allocation management for communications has been researched for many years @cite . Despite years of struggle, investment and results achieved, the subject keeps its importance mainly due to the communication network's evolution.
- As discussed in @cite and @cite , cognitive management using artificial intelligent techniques became more recently a relevant asset for communications management. Cognitive management has been applied using different AI techniques for cellular systems @cite , vehicular networks @cite , satellite communications @cite , small cells mobile systems @cite and, more extensively, to cognitive radio @cite .
- Bandwidth allocation model research has been focused on developing new models that, somehow, complement the basic ones: i) MAM proposed by @cite , ii) RDM discussed in @cite ; and iii) the AllocTCSharing (ATCS) model proposed by @cite . The research group associated with this work has proposed 2 new basic BAM: i) the AllocTCSharing (ATCS) model @cite ; and ii) the Generalized Bandwidth Allocation Model (GBAM) @cite . Some additional hybrid models that fundamentally provide some extra capabilities to existing models have also been proposed by @cite @cite and @cite .
- conduct systematic experiments to explore the generalization ability of deep neural networks @cite . They show that neural networks can almost perfectly fit the training data even when the training labels are random. This paper attracts the community of learning theory to the important topic that how to theoretically interpret the success of deep neural networks.
- discuss many open problems regarding the excellent generalization ability of deep neural networks despite the large capacity, complexity, possible algorithmic instability, nonrobustness, and sharp minima @cite . They also provide some insights to solve the problems.
- study the sample complexity of deep neural networks and present upper bounds on the Rademacher complexity of the neural networks in terms of the norm of the weight matrix in each layer @cite . Compared to previous works, these complexity bounds have improved dependence on the network depth, and under some additional assumptions, are fully independent of the network size (both depth and width). The upper bounds on the Rademacher complexity further lead to @math upper bounds on the generalization error of neural networks.
- explore several methods that could explain the generalization ability of deep neural networks, including norm-based control, sharpness, and robustness @cite . They study the potentials of these methods and highlight the importance of scale normalization. Additionally, they propose a definition of the sharpness and present a connection between the sharpness and the PAC-Bayes theory. They also demonstrate how well their theories can explain the observed experimental results.
- explore the capacity measures for deep neural networks from a geometrical invariance viewpoint @cite . They propose to use Fisher-Rao norm to measure the capacity of deep neural networks. Motivated by information geometry, they reveal the invariance property of the Fisher-Rao norm. The authors further establish some norm-comparison inequalities which demonstrate that the Fisher-Rao norm is an umbrella for many existing norm-based complexity measures. They also present experimental results to support their theoretical findings.
- conduct comparative experiments to study the generalization ability of deep neural networks @cite . The empirical results demonstrate that the input-output Jacobian norm and linear region counting play vital roles in the generalization ability of networks. Additionally, the generalization bound is also highly dependent on how close the output hypothesis is to the data manifold.
- Two recent works respectively by @cite and @cite provide upper bounds for the generalization error of chain-like deep neural networks. Specifically, @cite proposes an @math spectral-normalized margin-based generalization bound by upper bounding the Rademacher complexity covering number of the hypothesis space through the divide-and-conquer strategy. Meanwhile, @cite obtains a similar result under the PAC-bayesian framework. Our work is partially motivated by the analysis in @cite .
- Existing TL methods can be summarized into two main categories: (a) , which reuses samples from the source domain according to some weighting technique; and (b) , which performs subspace learning or distribution adaptation @cite @cite @cite @cite @cite . Unfortunately, these methods are all approaches. They depend on extensive hyperparameter tuning through cross-validation for the feature transformation @cite @cite , or the prediction model @cite @cite , or both @cite @cite . Most methods require multiple iterations of training @cite @cite . The L2T framework @cite is similar to EasyTL in spirit, but L2T is still based on model iteration and parameter tuning. Deep TL methods @cite @cite @cite @cite require heavy hyperparameter tuning. In TL, is often not available since there are almost no labeled data in the target domain @cite . In contrast, EasyTL is a non-parametric TL approach that directly learns from , which requires no model selection and hyperparameter tuning and much more efficient than existing methods.
- Nearest-neighbor (NN) classifier is the most common non-parametric method. However, NN computes the distance between each sample in two domains, which is more likely to be influenced by domain shift. Naive Bayes NN (NBNN) classifier is used for domain adaptation @cite , which still requires hyperparameter tuning and iterations. Nearest Centroid (NC) classifier is based on the distance between each target sample to the class center of the source domain. EasyTL uses a linear programming to get the softmax probabilities (float weights), while NC is basically 0 1 weights. This means that EasyTL could not only consider the relationship between sample and center, but also the relations of other samples. Open set DA @cite has similar idea with EasyTL, while it solves a binary programming and requires other classifiers.
- Many researchers demonstrate the feasibility of leveraging wireless signal (including Wi-Fi) for recognizing various human movements including human activity and gesture recognition @cite @cite @cite @cite @cite as well as person identification using gait patterns @cite @cite @cite . All of these research utilize machine learning classification that requires the collection of training data. While also utilizes the analysis of CSI signals, we address an inherently more challenging problem as we cannot expect to collect training data. Bagci @cite considers the problem of physical tampering of the Wi-Fi enabled cameras (e.g., moving or rotating the surveillance camera), which is different from our problem.
- Recent body of work studies how to utilize heterogeneous sensing modalities for added benefits. Researchers correlate video camera image with other sensing modalities such as RSSI value or IMU data to provide additional verification in different applications @cite @cite @cite . We are inspired by these techniques but addresses an inherently more difficult challenge of performing cross-modality correlation without the need of deploying specific sensors on users or objects, but rather utilizing the sensed information from existing infrastructure.
- The idea of using neural networks to estimate the outcome of an FEM simulation is not new. @cite have used a fully connected net to predict the velocity and angle of a tennis ball after hitting a racket. A similar approach has been adopted by @cite for predicting the movement of a tumor during brain surgery, but the displacement of the healthy tissue is not considered. @cite estimate the displacement of a patient's rib cage, but only predict the displacement of surface nodes. In contrast, @cite estimate the displacement of a full liver model using neural networks by superimposing basic deformation modes. These approaches show that neural networks can indeed be trained to estimate soft tissue behavior. However, they work with the data of a single patient, requiring re-training (and in some cases even re-designing) the network for every new patient. Additionally, they use the acting surface forces as input which are very difficult to obtain intra-operatively.
- @cite show that neural networks can estimate the deformation of a liver from the known displacement of a partial surface. They report very small errors while using only 3 Approaches which generalize to new patients were made by @cite , who estimate the liver's deformation from breathing motion using various machine learning methods and Martnez- @cite who estimate breast compression. However, both methods focus on single scenarios where the main direction of the forces stay similar throughout all experiments.
- Our method is inspired by the work of @cite , who have used neural networks to estimate the results of fluid dynamics simulations for real-time applications. One major difference is that fluid dynamics are often computed on regular grids with a fixed number of cells which makes them easier to handle with neural networks. In contrast to this, we need to re-sample the irregular simulation domain before we pass the data to the network.
- To improve the quality of software systems, the software engineering community has proposed many approaches to leverage user's feedback. Since the feedback is based on first-hand experience from end-users, valuable insights can be mined to help developers identify patterns from issues and bugs. studied user reviews from iOS apps and analyzed how user complaints, such as functional errors and app crashes, negatively affect app ratings in the Apple iOS App Store @cite . analyzed negative app reviews due to app updates in the Google Play Store, and found that updates with feature removal and user interface issues lead to the highest increase of negative reviews ratio @cite . studied customer reviews on Amazon.com and identified the value of review extremity, review depth, and product type in helping customers make purchasing decisions @cite . Poch 'e et al found that around 30 Similar to previous studies that leverage user feedback in improving existing software artifacts, our study of comments under Stack Overflow answers investigates how comments add value to their associated answers. Furthermore, we examine whether informative comments are effectively presented using the current comment hiding mechanism.
- Named Entity Recognition (NER), or alternatively Named Entity Recognition and Classification (NERC), is the task of detecting entities in an input text and to assign them to a specific class. It starts to be defined in the early '80, and over the years several approaches have been proposed @cite . Early systems were based on handcrafted rule-based algorithms, while recently several contributions by Machine Learning scientists have helped in integrating probabilistic models into NER systems.
- In particular, new developments in neural architectures have become an important resource for this task. Their main advantages are that they do not need language-specific knowledge resources @cite , and they are robust to the noisy and short nature of social media messages @cite . Indeed, according to a performance analysis of several Named Entity Recognition and Linking systems presented in @cite , it has been found that poor capitalization is one of the main issues when dealing with microblog content. Apart from that, typographic errors and the ubiquitous occurrence of out-of-vocabulary (OOV) words also cause drops in NER recall and precision, together with shortenings and slang, particularly pronounced in tweets.
- Music Information Retrieval (MIR) is an interdisciplinary field which borrows tools of several disciplines, such as signal processing, musicology, machine learning, psychology and many others, for extracting knowledge from musical objects (be them audio, texts, etc.) @cite . In the last decade, several MIR tasks have benefited from NLP, such as sound and music recommendation @cite , automatic summary of song review @cite , artist similarity @cite and genre classification @cite .
- In the field of IE, a first approach for detecting musical named entities from raw text, based on Hidden Markov Models, has been proposed in @cite . @cite , the authors combine state-of-the-art Entity Linking (EL) systems to tackle the problem of detecting musical entities from raw texts. The method proposed relies on the intuition, so if two or more different EL systems perform the same prediction in linking a named entity mention, the more likely this prediction is to be correct. In detail, the off-the-shelf systems used are: DBpedia Spotlight @cite , TagMe @cite , Babelfy @cite . Moreover, a first Musical Entity Linking, MEL http: mel.mtg.upf.edu has been presented in @cite which combines different state-of-the-art NLP libraries and SimpleBrainz, an RDF knowledge base created from MusicBrainz https: musicbrainz.org after a simplification process.
- Furthermore, has also been at the center of many studies done by the MIR community. As example, for building a music recommender system @cite analyzes tweets containing keywords like or . @cite , a similar dataset it is used for discovering cultural listening patterns. Publicly available corpora built for MIR investigations have been created, among others the dataset http: www.cp.jku.at datasets MMTD @cite and the dataset http: dbis-nowplaying.uibk.ac.at @cite .
- A common technique to measure packet loss in the Internet is through the use of (e.g., in the RTP protocol @cite ). The sequence number is incremented for each data packet sent, allowing the receiver to detect packets that do not arrive. We apply a similar approach (referred to as the sequence method) to measure data loss, which we describe in Section . The challenges in mirroring data between unreliable endpoints have been studied in detail by the database community @cite @cite . While telemetry gathered from our apps do not require transaction guarantees, mobile environments have constraints in terms of network reliability and local storage. Moreover, most analysis systems do not tolerate delays of more than a day. Nevertheless, client apps can borrow ideas such as lazy replication to improve telemetry reliability, especially if the experimentation and analysis system can tolerate longer delays.
- * Program Repair and Test-Input Generation. Due to the pressing demand for reliable software, automatic program repair has steadily gained research interests and produced many novel repair techniques. repair approaches, e.g., AFix @cite , Angelix @cite , SemFix @cite , FoRenSiC @cite , @cite , @cite , generate constraints and solve them for patches that are correct by construction (i.e., guaranteed to adhere to a specification or pass a test suite). In contrast, repair approaches, e.g., GenProg @cite , Pachika @cite , PAR @cite , Debroy and Wong @cite , Prophet @cite , find multiple repair candidates (e.g., using stochastic search or invariant inferences) and verify them against given specifications.
- The field of test-input generation has produced many practical techniques and tools to generate high coverage test data for complex software, e.g., fuzz testing @cite @cite , symbolic execution @cite @cite , concolic (combination of static and dynamic analyses) execution @cite @cite , and software model checking @cite @cite . Companies and industrial research labs such as Microsoft, NASA, IBM, and Fujitsu have also developed test-input generation tools to test their own products @cite @cite @cite @cite . Our work allows program repair and synthesis approaches directly apply these techniques and tools.
- Collaborative Filtering was popularised in the early 1990s as a technique for recommender systems with applications such as mail filtering @cite , and article @cite and movie recommendation @cite . Model-based algorithms soon became popular @cite to overcome the cold start problem arising for unseen users or items at test time. The most successful one of these, in turn, is matrix factorisation, as applied in this paper, which represents users and items as (dense) vectors in the same latent feature space and measures their compatibility by taking the dot product between the two representations @cite @cite . Beyond recommender systems, matrix factorisation has shown successes in a wide variety of subareas of NLP @cite @cite @cite @cite @cite .
- Adversarial learning can be frequently seen in computer vision tasks. For example, and @cite aimed to craft adversarial images that make an image classifier work improperly. In addition, many previous works constructed a more robust classifier by leveraging generated adversarial examples (e.g., @cite ), generative adversarial net (GAN) being a representative example of such work.
- The relationship among multiple agents can be categorized into cooperative, competitive, and both. Most previous work on MARL addresses cooperative tasks, in which the cumulative reward is maximized as a group . In particular, @cite proposed a method with a centralized critic for a fully cooperative multi-agent task.
- Algorithms on MARL applicable with competitive settings have recently been proposed by @cite , @cite , @cite , and @cite . @cite consider a two-player zero-sum game in which the protagonist gets a reward @math while the adversary gets a reward @math . In @cite , a centralized critic approach called multi-agent deep deterministic policy gradient (MADDPG) is proposed for mixed cooperative and competitive environments; MADDPG is a similar idea as that in @cite .
- Our proposed AdvRA and CI are varieties of credit assignment methods . Most existing methods allocate the proper reward to each agent by utilizing difference reward under the assumption that all agents are trained by RL. Also, prioritized experience-sampling techniques were previously proposed in and ; these techniques enable efficient learning by replaying important experience frequently. For previous work on autonomous driving testing, we refer readers to and @cite . Many previous studies have addressed test-case-generation problems related to image recognition rather than the decision making, which we address in this work.
- In recent years, a number of algorithms have been proposed for different learning problems over nodes of an static graph. @cite studied the classification problem for nodes of an static graph and showed the connection of their general formulation to Markov random fields. With @math the number of possible node labels, they also presented an @math -approximation algorithm for the classification problem, which is run in polynomial time. @cite studied the problem of online label prediction of a graph with the perceptron. The key difference between online and dynamic settings is that online setting is used in the cases where it is computationally infeasible to solve the learning problem over the entire dataset. However, in dynamic setting the learning problem can be solved over the entite dataset and the challenge is to efficiently update the solution when the dataset changes. Some other examples of online learning for graphs include @cite , @cite , @cite and @cite .
- @cite presented representative multi-dimensional view smoothers on graphs, that are based on graph-based transductive learning @cite . @cite proposed a family of learning algorithms based on a new form of regularization so that some of transductive graph learning algorithms can be obtained as special cases. @cite extended a model for nonparametric regression of nodes of an static graph, where distance between estimate and observation is measured at nodes by @math norm, and roughness is penalized on edges in the @math norm. In the literature, there also exist iterative methods, such as gradient descent and its improvements, that try to solve some of (static graph) learning problems without explicitly computing the Moore-Penrose pseudoinverse. However, the number of iterations they need is usually proportional to the condition number of the matrix embedding of the graph (e.g., the Laplacian matrix), which might be a (linear) function of @math .
- More recent algorithms are mostly methods for learning embeddings (representations) for nodes or subgraphs of a graph @cite , @cite , @cite , even though learning embeddings for a graph dates back to several decades ago. For example, @cite presented vector embeddings for nodes of a graph such that the inner product of the vector embeddings of any two nodes @math and @math is negative iff @math and @math are connected by an edge; and it is @math otherwise. They also studied the least embedding size @math , necessary for satisfying such a property.
- The MIPS problem is closely related to the @math nearest neighbor search problem as there are multiple ways to transform MIPS into equivalent instances of @math nearest neighbor search. For example, Shrivastava and Li @cite proposed augmenting the original vector with a few dimensions. Neyshabur and Srebro proposed another simpler transformation to augment just one dimension to original vector: @math . Empirically, the augmentation strategies do not perform strongly against strategies that work in the unaugmented space.
- . ADC transforms inner product computations into a lookup table based operation, which can be implemented in different ways. The original ADC paper @cite used L1 cache based lookup table. @cite used an GPU implementation for ADC lookup. A SIMD based approach was also developed by @cite @cite . Again, this is orthogonal to the local decomposition idea of this work, as any ADC implementation can be used in this work.
- Rotations and codebooks are often applied in IVFADC variations, but there are significant costs associated with them. In the most extreme cases, Locally Optimized Product Quantization (LOPQ) @cite learns a separate rotation matrix and codebook for each partition. This leads to an extra memory cost of @math and @math more multiplications for each query at search time. where @math is the number of VQ partitions we search. When @math and @math increase, the overhead become quickly noticeable and may become even more expensive than ADC itself. For example, when @math , @math , @math , performing the rotation once is as expensive as performing 6,400 ADC computations under an optimized implementation. In practice, it is often desirable to avoid per partition rotation or codebooks, but learn global codebooks and rotation.
- Transactional memory is a programming paradigm initially proposed by @cite intended to simplify concurrent programming by allowing user-specified blocks of code to be executed in hardware, exhibiting both atomicity and isolation. Software transactional memory, proposed by @cite , was developed to facilitate transactional programming without hardware transactional memory support. @cite present DSTM, an application programming interface for obstruction-free STM designed to support dynamic-sized data structures. @cite present NOrec, a low-overhead STM that utilizes a single global sequence lock shared with the transactional mutex lock system, an indexed write set, and value-based conflict detection to provide features such as livelock freedom, full compatibility with existing data structure layouts, and starvation avoidance mechanisms. @cite present Transactional Locking II (TL2), an STM algorithm that uses a novel version-clock validation to guarantee that user code operates only on consistent memory states. Other STM designs include @cite @cite @cite . STM implementations rely on low-level conflict detection to enable transactions. These implementations generally suffer from high spurious abort counts, making them less desirable for concurrent data structures.
- Initial performance experiments were performed with Hardware Transactional Memory (HTM) by @cite . Intel introduced Transactional Synchronization Extensions (TSX) to the x86 instruction set architecture of the Intel 4th Generation Core Processors @cite . IBM introduced HTM in the Power ISA @cite . Both implementations offer a best-effort HTM, which means that there is no guarantee provided that a hardware transaction will commit to memory. The disadvantage of a best-effort strategy is that HTM may experience frequent aborts due to data access conflicts, hardware interrupts, limited transactional resources, or false sharing due to unrelated variables mapping to the same cache line @cite .
- @cite present transactional boosting, a methodology for transforming highly-concurrent linearizable objects into highly-concurrent transactional objects. Transactional boosting uses a high-level semantic conflict detection to allow commutative operations in separate transactions to proceed concurrently using the thread-level synchronization of the base linearizable data structure; non-commutative operations require transaction-level synchronization through the acquisition of an abstract lock. If a transaction aborts, it recovers the correct abstract state by invoking the inverse operations recorded in the undo log.
- Transactional linked list implementations based on transactional boosting use coarse-grained locking to ensure that non-commutative method calls are never allowed to execute simultaneously. The underlying linked list algorithm's linearizability is preserved during this process to handle thread level synchronization. Rollbacks are performed by calling a method's inverse operation, which causes a performance loss for aborted transactions. Zhang and Dechev @cite present a lock-free transactional linked list alongside LFTT which takes advantage of a node based conflict detection scheme to preserve the underlying algorithm's lock-freedom. This approach additionally reduces the performance hit of rollbacks by introducing a logical status update scheme capable of aborting a transaction in a single atomic step. LFTT provides transformation templates for the set abstract data type, which does not account for operations in which elements are related to each other.
- @cite presented a transactional skiplist that uses STM-like techniques combined with node locking in an attempt to reduce overhead and false aborts. additionally present a transactional queue using a pessimistic lock-based approach. In this queue, the execution of operations are deferred to the final phase of the transaction, the commit phase, in order to avoid keeping track of the current head of the queue. Meanwhile, operations acquire a lock on the queue until their transaction is complete. Zhang and Dechev @cite preserved lock-freedom in their algorithm by transforming a skiplist using LFTT which, again, offers a performance improvement on transaction rollbacks.
- Encoder-decoder structure is often used to recover the spatial details in semantic segmentation. U-type structure, proposed in U-Net @cite , use multi-path to help low-level features skip the middle layers and be combined with the refined high-level feature in the decoder, which enhance the performance on details. FC-DenseNet @cite extended DenseNets @cite in U-type structure and improve the upsampling path in decoder to reduce computation.
- Besides of U-type structure, multiple feature fusion is also commonly used to recover details. RefineNet @cite proposes a generic multi-path refinement network that fuses multi-resolution features from different layer to generate high-resolution and high-quality results. However, before multi-resolution fusion, many convolutions are added, and the speed on 512x512 images is only 20fps. Light-Weight RefineNet @cite increases the speed by modifying the RCU and CRP module in original RefineNet, and operates at the speed of 55fps.
- BiSeNet @cite uses bilateral segmentation network, Spatial Path and Context Path to achieve both rich spatial information and sizeable receptive field. In order to improve efficiency, the BiSeNet uses a decoder of interpolation. ICNet @cite sets up a image cascade network with multi-resolution branches under proper label guidance to reduce much computation in pixel-level segmentation inference.
- Decoder of single interpolation is often adopted to avoid computing redundancy such as DeepLabV3 @cite and ESPNet @cite . Here we regard the methods with interpolation decoder as one of single-shot structures. But this single-shot structure can result in the loss of details, and a post processing with conditional random field (CRF) @cite is often used to refine the coarse segmentation, which adds more extra latency.
- Residual learning is first proposed in classification network @cite to improve the network degradation and gradients vanishing problems. With many advantages in network training, the concept of residual learning has been migrated to other tasks, such as super resolution and semantic segmentation tasks.
- Super resolution can be considered as a process of details restoration, so learning sparse residuals is more efficient than learning full images itself of higher resolution. VDSR @cite sets up a deep neural network to learn the high frequency details, which achieve great improvement on accuracy. LapSRN @cite expands the conception to cascade residual learning blocks and progressively reconstructs the sub-band residuals of high-resolution images at multiple pyramid levels and further increase the performance.
- In semantic segmentation tasks, there are also many attempts of residual learning. LRR @cite uses low-resolution results to generate a boundary mask for high-resolution feature. Then high-resolution boundary is predicted to refine the result. Global-Local-Refinement @cite iteratively predicts global residuals using the input which concatenates the original image and confidential map.
- PSPNet @cite and BiSeNet @cite @cite use mid predictors to construct auxiliary loss layer followed by hidden layer. The auxiliary loss, illustrated in @cite , helps the optimization during learning process, while the master branch loss takes the most responsibility. The ablation study in @cite is also presented to help decide which weight values to use. The Impatient DNNs @cite attempts to use multiple early prediction layers to deal with dynamic time budgets during application, where the intermediate predictors are learned jointly with the weighted losses. In the paper @cite , the authors compare different weights per loss component and choose the best weights to train the network.
- In some specific designed network, joint training of multiple losses will break the structure of learned feature. In LRR @cite , the coarse and fine semantic segmentations are predicted from top to down in the network, where fine segmentation predictions depend on the higher coarse predictions. So the level-wise training is necessary in LRR.
- Real-time segmentation models prefer thin networks with fewer filters so that computing cost can be reduced such as ENet @cite . But simply reducing computation will lead to degradation in performance.
- In segmentation task, decoder structure is often removed at the expense of spatial details to reduce computing cost in some methods such as DeepLab v3 and BiSeNet @cite @cite .
- Another way is to optimize convolution blocks. ERFNet @cite uses residual connections and factorized convolutions @cite to maintain efficiency and accuracy. DeepLab v3+ @cite proposes Atrous Separable Convolution to speed up standard convolution. ESPNet @cite decomposes a standard convolution into a point-wise convolution and a spatial pyramid of dilated convolutions to improve efficiency. BiSeNet @cite @cite uses shallow Spatial Path and Context Paths to generate high-resolution feature and sufficient receptive field, and then combines the two paths to predict the target. This two-path structure can transfer the computation of depth to two sub-networks and improve the parallelism of sub-networks.
- Seizure non-seizure classification distinguishes seizure segments from non-seizure segments, which can be used to recognize whether a data segment contains seizure or not. For this task, extensive studies have been performed. Because seizure detection, which is often of a real-time flavor, is often treated as the seizure non-seizure classification problem, many machine learning methods have been developed @cite @cite @cite @cite @cite @cite @cite @cite @cite . Recently, deep learning techniques have been applied to the seizure detection problem @cite @cite @cite @cite @cite . The evaluations of these methods are conducted with patient-specific or across-patients experiments. The classification across patients is more challenging because it needs to overcome the inter-patients variabilities.
- Shoeb and Guttag propose a method to construct a patient-specific detector for seizure detection by using the support vector machine (SVM) @cite . The method leverages filters to extract spectral features over each channel, and then stacks feature vectors to catch time-evolution information. A sensitivity of 96 propose a wavelet-based algorithm for real-time detection of epileptic seizures using scalp EEG @cite . In this algorithm, the EEG from each channel is decomposed by wavelet packet transform, and a patient-specific measure is deployed by using wavelet coefficients to separate the seizure and non-seizure states. Utilizing the measure, a combined seizure index is derived for each epoch of every EEG channel. Through inspecting the combined seizure index, proper channel alarms are generated.
- explore two kinds of neural networks over TUH EEG Corpus @cite . Their experiment results show that convolutional LSTM network outperforms convolutional GRU network. Different initialization and regularization methods are considered. LSTM and GRU are limited when using multiple layers.
- design a deep neural network for seizure non-seizure classification by using LSTM @cite . It extracts temporal features by using LSTM. present a 13-layers deep neural network for seizure non-seizure classification by using CNN @cite . The two approaches are evaluated over the same EEG data set provided by University of Bonn @cite . The LSTM approach achieves performances of 100 These developed seizure-detection methods based on traditional machine learning techniques can work well with small amount of samples. They often need crafted features and manually selecting features. In the currently developed seizure non-seizure classification methods based on deep learning techniques, most methods are based on classical neural network models, such as CNN, RNN, and LSTM. These models have their limitations for processing EEG signal data, such as suffering from gradient vanishing or exploding problem.
- Gaussian noise removal Application of neural networks to noise removal has a long history @cite @cite @cite @cite @cite . Mao al @cite proposed REDNet, which consists of multiple convolutional and de-convolutional layers with symmetric skip connections over them. Tai al @cite proposed MemNet with local memory blocks and global dense connections, showing that it performs better than REDNet. However, Suganuma al @cite showed that standard convolutional autoencoders with repetitive pairs of convolutional layers with large- and small-size kernels outperform them by a good margin, which are found by architectural search based on evolutionary computation.
- Motion blur removal This task has a long history of research. Early works @cite @cite @cite @cite attempt to simultaneously estimate both blur kernels and sharp images. Recently, CNN-based methods @cite @cite @cite @cite @cite achieve good performance for this task. Nah al @cite proposed a coarse-to-fine approach along with a modified residual block @cite . Kupyn al @cite proposed an approach based on Generative Adversarial Network (GAN) @cite . New datasets were created in @cite and @cite .
- Haze removal Many studies assume the following model of haze: @math , where @math denotes a hazy scene image, @math is the true scene radiance (the clear image), @math is a transmission map, @math is global atmospheric light. The task is then to estimate @math , @math , and thus @math from the input @math @cite @cite @cite @cite @cite . Recently, Zhang al @cite proposed a method that uses CNNs to jointly estimate @math and @math , which outperforms previous approaches by a large margin. Ren al @cite and Li al @cite proposed method to directly estimate @math without explicitly estimating @math and @math . Yang al @cite proposed a method that integrates CNNs to classical prior-based method.
- Raindrop detection and removal Various approaches @cite @cite @cite @cite @cite have been proposed to tackle this problem in the literature. Kurihata al @cite proposed to detect raindrops with raindrop-templates learned using PCA. Ramensh @cite proposed a method based on K-Means clustering and median filtering to estimate clear images. Recently, Qian al @cite proposed a hybrid network consisting of a convolutional-LSTM for localizing raindrops and a CNN for generating clear images, which is trained in a GAN framework.
- Rain-streak removal Fu al @cite use guided image filtering'' @cite to extract high-frequency components of an image, and use it to train a CNN for rain-streak removal. Zhang al @cite proposed to jointly estimate rain density and de-raining result to alleviate the non-uniform rain density problem. Li al @cite regards a heavy rainy image as a clear image added by an accumulation of multiple rain-streak layers and proposed a RNN-based method to restore the clear image. Li al @cite proposed an non-locally enhanced version of DenseBlock @cite for this task, their network outperforms previous approaches by a good margin.
- The total sequential rounds of playing bandits is @math . For each round @math with @math , a learner receives contextual information from the set of @math , where @math can be an extremely large integer representing a high-dimensional space. In this work, high-dimensional contextual data precisely mean that @math or even @math , which is the case mentioned in @cite . Let @math be the number of arms and @math the reward of arm @math on round @math with @math and @math . We adopt @math to denote the @math norm of a vector @math , and @math to denote the identity matrix with dimensions of @math . For a positive definite matrix @math , the weighted norm of vector @math is defined as @math . The inner product is represented as @math , and the weighted inner product is @math .
- One common technique for dimensionality reduction is to perform linear random projection @cite @cite . In this paper, we consider projecting the contextual data of @math onto a low-dimensional space of @math . Without loss of generality, we denote the random projection matrix by @math . Then, we have where @math and @math .
- @cite , @math is constructed as a random matrix where each element follows a normal distribution of @math . By setting @math in the next section, we name our algorithm of CBRAP with Standard Gaussian (SG) matrix (abbreviated as CBRAP.SG ).
- @cite , the authors proposed new methods for constructing sparse random sign matrix for dimensionality reduction. In the ensuing section, we name our algorithm of CBRAP with Random Sign (RS) matrix (abbreviated as CBRAP.RS ).
- In addition to the above work, there have been other ways of constructing a matrix for random projection @cite @cite @cite @cite . These investigations consider how to speed up the dimensionality reduction, or how to conduct the random projection with the assumption of low-rank matrix. In this paper, since our focus is to conduct the dimensionality reduction of contextual bandits in a general way, we only consider the construction of random matrix in @cite @cite .
- @cite , the authors proposed an algorithm named BallExp for high-dimensional linear bandits, where the regret bound is relatively loose, and is directly related to the dimension of data.
- Early methods often require multiple frames to deal with the deraining problem @cite @cite @cite @cite @cite @cite @cite @cite . Garg and Nayar @cite proposed a rain streak detection and removal method from a video by taking the average intensity of the detected rain streaks from the previous and subsequent frames. @cite further improved the performance by selecting camera parameters without appreciably altering the scene appearance. However, those methods are not applicable to single image deraining.
- Many deraining methods capitalize on clean image or rain type priors to remove rain @cite @cite @cite @cite @cite . @cite decomposed an input image into its low and high frequency components. Then they separated the rain streak frequencies from the high frequency layer via sparse coding. @cite introduced a rain removal method based on the prior that rain streaks typically span a narrow range of directions. Chen and Hsu @cite decomposed the background and rain streak layers based on low-rank priors. @cite use patch-based priors for both the clean background and rain layers in the form of Gaussian mixture models. All of the above approaches rely on good (and relatively simple) crafted priors. As a result, they tend to have unsatisfactory performances on real images with complicated scenes and rain forms.
- Recently, CNNs have achieved dominant success for image restoration @cite @cite including single image deraining @cite @cite . @cite proposed a deep detail network (DDN) for removing rain from single images with detailed preserved. @cite presented a CNN based method to jointly detect and remove rain streaks, using a multi-stream network to capture the rain streak component. A density-aware multi-stream densely connected convolutional neural network was introduced in @cite for joint rain density estimation and image deraining. @cite addressed a different problem of removing raindrops from single images, using visual attention with a generative adversarial network (GAN). Despite the progress of deep-learning-based approaches compared with prior-based rain removal methods, their performance hinge on the synthetic training data, which may become problematic if real rainy images show a domain mismatch.
- Several datasets were used to measure and compare the performance of image deraining algorithms. @cite introduced a set of 12 images using photo-realistic rendering techniques. @cite synthesized a set of training and testing images with rain streak, using the same way in @cite . The training set consists of 700 images and the testing set consists of 100 images. In addition, @cite also collects a dataset of 50 real-world rainy images downloaded from the web for qualitative visual comparison. @cite released a set of clean and rain-drop corrupted image pairs, using a special lens equipment. However, existing datasets are either too small in scale and limited to one rain type (streak or drop), or lack sufficient real-world images for diverse evaluations. Besides, none of them has any semantic annotation nor consider any subsequent task performance.
- The intention model presented in this paper is mainly driven by eye gaze data. Therefore, we review work on human gaze behaviour to inform the underlying assumptions of our model. found that fixations towards an object often precede a subsequent manual interaction by around 0.6 s @cite . Subsequent work revealed that the latency between eye and hand varies between different tasks @cite . Similarly, @cite found that objects are most salient for human's when they are relevant for tasks planning and preceding saccades were linked to short-term memory processes in @cite . The purpose of preceding fixations in manual tasks was furthermore explored through virtual @cite and real @cite block design tasks. The results show that humans gather information through vision rather than memorising e.g. all object locations.
- Tissue examination under a microscope reveals important information to render accurate diagnosis and thus, provide effective treatment for different diseases @cite . DP offers several opportunities and also presents challenges to the image processing architectures @cite . Presently, only a small fraction of glass slides are digitized @cite , but even if WSI was more widely available, there are a number of technical issues that would need to be addressed for their effective usage. One of the main challenges is data management and storage @cite . Most importantly, the large dimensions of the WSI files require a large amount of memory and a expensive computational power.
- Satisfying TL constraints during motion planning for robots is an active area of research. Approaches include hierarchical control @cite , ensuring probabilistic satisfaction guarantees @cite , and sensing-based strategies @cite .
- Synthesis of memoryless strategies for POMDPs in order to satisfy a specification was shown to be NP-hard and in PSPACE in @cite . In @cite , a discretization of the belief space is carried out , resulting in a fully observable MDP. However, this approach might not be practical if the state space is large @cite . The complexity of determining a winning strategy to solve the problem of determining the probability of satisfaction of parity objectives was shown to be undecidable in @cite . However, determining finite-memory strategies for the qualitative problem of parity objective satisfaction was shown to be EXPTIME-complete in @cite .
- Dynamic programming (DP) for POSGs has been studied in @cite , resulting in an algorithm that generalizes both DP for POMDPs and iterated elimination of dominated strategies for normal form games. This work, however, considered the finite horizon case, and all agents had to maximize their own expected rewards. When agents cooperate to earn rewards, the framework is called a decentralized-POMDP (Dec-POMDP). The infinite horizon case for Dec-POMDPs was studied in @cite , where the authors proposed a bounded policy iteration algorithm for policies represented as joint FSCs. A complete and optimal algorithm for deterministic FSC policies for DecPOMDPs was presented in @cite . Optimization techniques for fixed-size controllers' to solve Dec-POMDPs were investigated in @cite . A survey of recent research in Dec- POMDPs is presented in @cite .
- To address these problems, many efforts have been devoted to estimating multiple transformation. @cite estimated dual-homography for the image alignment when the scene can be divided into a distant plane and a ground plane. @cite proposed a smoothly varying affine transformation, according to the smoothly varying depth of the scene. Similarly, @cite presented an as-projective-as-possible method (APAP) to estimate multiple homographies for better alignment. Lou and Gevers @cite described a piecewise planar region matching method to calculate multiple affine transformations, and they used multiple planes to approximate the image. These methods improve the alignment quality but heavily depend on keypoint detection and feature matching algorithms to offer sufficient and uniformly distributed keypoint correspondences. Additionally, keypoint detection, feature matching, and transformation estimation are time-consuming for real-time applications.
- More recently, deep convolutional neural networks have been exploited to handle the problems of low efficiency and sparse keypoint correspondences for image alignment. @cite designed a HomographyNet to directly estimate a homography between two images in an end-to-end manner. With the success of the HomographyNet, several deep learning-based homography estimation networks have been presented. @cite proposed a hierarchy network using a twin convolutional network, while @cite presented a cascade Lucas-Kanade network by combining the Lucas-Kanade algorithm with the convolutional neural network. Apart from these supervised learning-based methods, an unsupervised homography estimation network was introduced by @cite for UAV image alignment.
- The alignment of wide-angle images and or fisheye images is more challenging, and suffers from heavy radial distortion @cite . In order for bringing wide-angle images into alignment, Jin @cite and Byr " @cite estimated jointly the lens distortion and the alignment transformation, assuming that all images share the same distortion factors. For cameras in different radial distortions, Ju and Kang @cite estimated the lens distortion factor for each image, and then computed a homography for the alignment of synthetic images, whereas @cite estimated a homography and different distortion factors to bring images of real scene into alignment. In addition, Ho and Budagavi @cite proposed to align two images captured by a dual-fisheye lens camera. They unwarped the fisheye images into spherical 2-Dimensional space, and then employed a two-step alignment to register the unwarped images. Due to the unwarping process, the regions near edges of original images are stretched, leading to shape distortions in alignment results.
- There are several commercial video stitching softwares, such as VideoStitch Studio https: www.orah.co software videostitch-studio and AutoPano http: www.kolor.com autopano . These softwares usually compute a 2D transformation relating two cameras, and then bring all pairs of video images into alignment for the post-production of stitched videos. To improve the quality of the stitched video, @cite found double-seam to eliminate intensity misalignment, and similar work has been presented for designing a content-aware adaptive blending @cite . Some works were presented to obtain better alignment results. Lee and Sim @cite stitched videos by projecting the background plane and the foreground objects separately, while Jiang and Gu @cite stitched videos using spatial-temporal content-preserving warps. For videos captured by handheld cameras, @cite @cite and @cite combined the stitching and stabilization techniques together into a unified optimization framework for video stitching, whereas @cite stitched videos by reconstructing the 3D scene using the recovered 3D camera paths and the 3D scene points. These methods stitched videos in an iterative manner with low computational efficiency.
- Besides, some work was designed for real-time processing or time-critical applications. For video surveillance applications, He and Yu @cite employed a background modeling algorithm and a change-detection-based optimal seam selection approach to stitch videos captured by fixed cameras. A Multi-UAV-based video surveillance system, SkyStitch @cite , was designed and implemented for real-time aerial surveillance, employing flight information (e.g., the UAV attitude and GPS location) got from the flight controller as assistance. @cite introduced a real-time video stitching method by implementing and improving a feature-based algorithm on a field-programable gate array (FPGA). Apart from hardware acceleration, EI- @cite developed a real-time method to stitch independent videos streamed by different mobile phones, while @cite stitched several live videos into a @math field of view and spread the stitched video based on GPU.
- Since most existing approaches are designed for stitching videos from conventional cameras, they can not handle videos with heavy lens distortions captured by the wide-angle lens camera or fisheye lens camera. Considering the distortion, a simple method is to undistort the video images through a rectilinear projection, and then stitch the undistorted videos frame-by-frame @cite . Nevertheless, the undistortion may incur unnatural stretches on the regions near the borders of video images, particularly in video images captured by fisheye lens cameras.
- Research related to knowledge graph integration comes from the database community and focuses on ontology matching---referred to as record-linkage, entity resolution, or deduplication. Examples include @cite , @cite , and the work of . The primary difference between this work and ours is that they assume relational structure and that the tables to be matched have been already aligned using schema matching techniques. These systems cannot be directly applied to entity matching across knowledge graphs due to differences in structure between the relational model and the RDF model.
- The Semantic Web community has studied the problem of matching entities across knowledge graphs, for example, the Ontology Alignment Evaluation Initiative (OAEI) on ontology matching in knowledge graphs. However, the benchmarks used in these evaluations are quite small. For example, the benchmark @cite has a total of only 1800 instances and 50,000 triples.
- The benchmark introduced in @cite generates networks with power law distributions of community sizes and node degrees, with tunable intra total ratio (mixing parameter). This benchmark, commonly known by the authors initials (LFR), has been widely accepted and used to test community detection algorithms for static networks. For instance in @cite @cite a lengthy list of algorithms tested against this benchmark can be found.
- There have been some proposals to generate synthetic temporal networks. In @cite the authors propose a generator for simple networks with a cyclic nature based on a variation of the stochastic block model. In @cite the authors have adapted the LFR benchmark @cite , while introducing over time ad-hoc modifications to the network. In @cite the authors propose RDyn, a system to generate temporal networks respecting a power-law distribution of community sizes and node degrees with tunable clustering and injected lifecycle events that, while disrupting cluster quality, are subsequently re-balanced through re-wiring of node links.
- Our reward model is based on (stochastic) linear bandits which has seen a vast number of studies since @cite . Our work leverages ideas from @cite for both the reward model and intermediate results for the GLM case. Extending the linear bandit problem, @cite first proposed the GLM bandit problem and provided a no-regret algorithm in the OFU paradigm. This paper builds up on the ideas from @cite and @cite who also studied the logistic bandit and GLM Bernoulli bandit case. We provide the extension of both methodologies to a generic multinomial GLM setting. Consequently, our bounds also show a dependence on the strong convexity parameter @math which was recently shown to be unavoidable by @cite for proper learning in online logistic regression. Further, @cite have recently proposed an optimal regret bound for the GLM Bernoulli bandit problem by using phases for removing the statistical dependence between parameter estimates and covariates. However, the proposed algorithm is inefficient and obtaining such an optimal algorithm is still an open problem.
- @cite first proposed a no-regret online learning algorithm for average reward infinite horizon MDPs, and the problem has been extensively studied afterwards. More recently, there has been an increased focus on fixed horizon problems where the gap between the upper and lower bounds has been effectively closed. @cite and @cite , both provide optimal regret guarantees ( @math ) for repeated interaction with a finite episodic MDP by using empirical Bernstein based bonus and a fine-grained analysis. Both these methods do not directly carry over to our setting and therefore, improving the dependence on @math is an interesting problem for future work which would require a novel approach. Contextualized version of MDPs are relatively unexplored with @cite and @cite being the only works to the best of our knowledge.
- Unlike the approaches above, @cite utilizes the reflectance intensity to estimate the corners of the chessboard from the 3D laser point cloud. If the corners of the 3D laser point cloud are identified, the extrinsic calibration is converted to a 3D-2D matching problem. However, the noise and sparsity of the point cloud data present a challenge in this approach. Moreover, like the other appearance-based methods, this method has a limitation where there must be a shared common field of view. Even in the application scenario where the condition is met, the requirement of the common field of view constrains the scale of the scene and limits the number of targets that can be detected, thus affecting the accuracy of the calibration.
- Following spectral-based formulation @cite @cite @cite , the graph convolution operator @math ' is introduced as the multiplication of a graph signal @math with a kernel @math , where @math is a vector of Fourier coefficients, as where @math is the graph Fourier basis, which is a matrix of eigenvectors of the normalized graph Laplacian @math ( @math is an identity matrix and @math is the diagonal degree matrix of adjacency matrix @math with @math ); while @math is the diagonal matrix of eigenvalues of @math @cite . In order to localize the filter, the kernel @math can be restricted to a truncated expansion of Chebyshev polynomials @math to @math order with the rescaled @math as @math , where @math is a vector of Chebyshev coefficients @cite . Hence, the graph convolution can then be expressed as, where @math is the Chebyshev polynomial of order @math evaluated at the rescaled Laplacian @math .
- Apart from convolutional operations on graphs, there are also several recent studies focusing on structured sequence learning. Structured RNN @cite attempts to fit the spatio-temporal graph into a mixture of recurrent neural networks by associating each node and edge to a certain type of the networks. Based on the framework of convLSTM @cite , graph convolutional recurrent network (GCRN) @cite is firstly proposed modeling structured sequences by replacing regular 2D convolution with spectral-based graph convolution. And it has set a trend of GCN-embedded designs for the follow-up studies @cite @cite . Recently, an encoder-decoder model on graphs is developed for graph embedding tasks. The model known as graph U-Net @cite brings pooling and upsampling operations to graph data. However, the scope of its uses is bounded by the static graph. Additionally, it introduces extra training parameters for node selection during the pooling procedure. Furthermore, the pooling operation it proposed does not keep the original structure of the input graph that may raise an issue for those tasks whose local spatial relations are critical.
- Similar ideas to our own have recently been used in CycleGAN and DualGAN, which handle the bi-directional translations within two domains together @cite @cite @cite , significantly advance image-to-image translation @cite @cite @cite @cite @cite . Our MirrorGAN is partly inspired by CycleGAN but has two main differences: 1) we specifically tackle the T2I problem rather than image-to-image translation. The cross-media domain gap between text and images is probably much larger than the one between images with different attributes, @math styles. Moreover, the diverse semantics present in each domain make it much more challenging to maintain cross-domain semantic consistency. 2) MirrorGAN embodies a mirror structure rather than the cycle structure used in CycleGAN. MirrorGAN conducts supervised learning by using paired text-image data rather than training from unpaired image-image data. Moreover, to embody the idea of learning T2I generation by redescription, we use a CE-based reconstruction loss to regularize the semantic consistency of the redescribed text, which is different from the L1 cycle consistency loss in CycleGAN, which addresses visual similarities.
- Network sparsification @cite , sometimes referred to as unstructured pruning, reduces the number of connections in deep networks by imposing sparsity constraints. The work in @cite proposed recasting the sparsified network into separate groups of operations where the filters in each layer are only connected to a subset of the input channels. However, this method requires training the network from scratch which is not practical or efficient. Furthermore, current hardware is not designed for efficient sparse operations making the process less efficient.
- Filter factorization methods reduce computations at the cost of increased memory load for storing intermediate feature maps. Initial works focused on factorizing the three-dimensional convolutional kernels into three separable one-dimensional filters @cite @cite . In @cite CP-decomposition is used to decompose the convolutional layers into five layers with lower complexity. More recently @cite performed a channel decomposition that found a projection of the convolutional filters in each layer such that the asymmetric reprojection error was minimized.
- Channel pruning methods @cite @cite @cite @cite remove entire feature kernels for network compression. In @cite kernels are pruned based on their magnitudes, under the assumption that kernels with low magnitudes provide little information to the network. Li al @cite suggested a similar pruning technique based on kernel statistics. He al @cite proposed pruning filters based on minimizing the reconstruction error of each layer. Luo al @cite further extended the concepts in @cite to prune filters that have minimal impact on the reconstruction of the next layer. Yu al @cite proposed Neuron Importance Score Propagation (NISP) to calculate the importance of each neuron based on its contribution to the final feature representation and prune feature channels that provide minimal information to the final feature representation.
- There is a body of work available in the literature on software visualization @cite @cite @cite @cite @cite @cite @cite . We direct the reader to comprehensive surveys of different methods available in the work of Teyseyre and Campo @cite and also Caserta and Olivier @cite . For example SeeSoft @cite , one of the earliest visualization metaphors, allows one to analyze up to 50,000 lines of code by mapping each line of code into a thin row. Marcus @cite added a new dimension to SeeSoft to support an abstraction mechanism to achieve better representation of higher dimensional data.
- Recently, there have been more work focusing on 2D visualization. Code Bubbles @cite suggested a collection of editable fragments that represent functions in a class. Code Gestalt @cite used tag overlay and thematic releations. Lanza and Ducasse @cite proposed categorizing classes and their internal objects into blocks called Blue Prints. Gutwenger @cite proposed an approach for improved aesthetic properties of UML diagrams when visualizing hierarchical and non-hierarchical relations. Balzer @cite introduced hierarchy-based visualization for software metrics using Voroni Treemaps. Additionally, Holten @cite used both hierarchical and non-hierarchical data to visualize adjacency relation in software. The common observation among these efforts is that they are all based on 2D environments and were mostly suitable for expert users.
- Remembering code structure will result in faster development so it is an essential part of being a programmer. Specifically, 3D environments tap into the spatial memory of the user and help with memorizing the position of objects @cite . These objects could be classes or methods. There are also studies which provide evidence that spatial aptitude is a strong predictor of performance with computer-based user interfaces. For instance, Cockburn and McKenzie @cite have shown that 3D interfaces that leverage the human's spatial memory result in better performance even though some of their subjects believed that 3D interfaces are less efficient. Robertson @cite have also shown that spatial memory does in fact play a role in 3D virtual environments. A number of researchers have attempted to solve the problem of understanding code structure. Graham @cite suggested a solar system metaphor, in which each planet represented a Java class and the orbits showed various inheritance levels. Balzer @cite presented the static structure and the relation of object-oriented programs using 3D blocks in a 2D landscape model.
- What sets CP apart from current tools in the literature is that it is designed to be suitable for both beginner and experienced developers alike. Saito @cite examined the learning effects between a visual and a text-based environment on teaching programming to beginners. Their results deemed the visual environment as a more suitable option for teaching beginners. Indeed, at the lines of code level, CP differs little from a traditional IDE, other than code can be read from within a 3D environment. Where CP diverges from a traditional IDE is in how one interacts with a project: file hierarchies vs buildings in 3-space. The exo-centric view in CP helps the user glean a holistic understanding of the codebase without involving them in unnecessary details. Conversely, the ego-centric view enables a fine-grained understanding of the codebase. The addition of the ego-centric view is the key distinction between the current work and CodeCity @cite .
- The publication @cite proposes three risk level thresholds for THW and TTC. Level 1 describes scenarios, which exceed normal'' driving. Level 2 scenarios show a higher collision risk and level 3 scenarios indicate an imminent collision risk. The THW is combined with a relative velocity threshold. The TTC threshold is combined with a brakelight indication, see Table in Section . Longitudinal and lateral accelerations can increase or decrease the level according to their defined tresholds, see Section . This paper provides occurrences of scenarios in the highD data set according to that risk definition. The authors in @cite state, that the false positive rate for detecting critical scenarios was reduced considering the driver intention, which can be achieved for example by considering the yaw-rate, turn or brake signals, accessible from the bus system. Another aspect stated in @cite is, that drivers usually see the presence or absence of a vehicle in front of their leader vehicle and adjust their driving strategy to that.
- The publication @cite combines TTC with a safety braking distance, which considers vehicle and environmental variables. This approach assumes an existing estimation of the environmental conditions.
- A survey on motion prediction and risk assessment considers various aspects on motion prediction and the risk definition. The authors highlight, that the risk definition is influenced by the choice of the motion model @cite . Besides deterministic risk indicators like TTC, which assume a collision, probabilistic measures are discussed as well. The probabilistic collision predictions take the uncertainty of the predicted trajectories of the human driven ego vehicle and the surrounding objects into account.
- These two indicators fail for crossing scenarios. In such cases the indicators have to be adapted or replaced as discussed for example in @cite .
- On the contrary, literature on theoretical guarantees for manifold clustering is rather scarce, with the exception @cite . Near-optimal exact recovery of some emblematic clustering methods based on pairwise distances of data is derived under a condition that the minimal signal separation strength over all pairs of submanifolds is larger than a threshold. Compared with our diffusion @math -means with local scaling, results established in @cite are non-adaptive to the local density and (geometric) structures of the submanifolds (cf. Theorem and ahead).
- Neural Architecture Search (NAS) @cite is one of the early reinforcement learning methods achieving strong results on CIFAR-10 using 800 GPUs for an entire month. Inspired by this, new methods were developed to reduce the search time by limiting the search space to repeating cells @cite @cite , smart reusing @cite or sharing of the model weights @cite @cite .
- The use of inductive transfer in the met alearning community is very common @cite @cite . However, so far these methods were developed to select traditional machine learning models (e.g. SVM, Random Forest) or tune their hyperparameters for tabular data @cite @cite . Most of these methods also rely on metafeatures, i.e. features describing the properties of the dataset which are often not defined for unstructured data @cite @cite . Furthermore, many methods also expect that a possible candidate can be expressed with a fixed size and low-dimensional vector which is not the case for neural architectures @cite @cite @cite . For all those reasons most methods are not applicable to the problem of architecture selection. Thus, we propose a novel method based on these principles to select neural architectures.
- Another important branch of risk representation is through implicit probabilistic models in belief space. Risk is trivial given a perfectly known world and action model. It is the model uncertainty that introduces risk into path execution, e.g., not knowing exactly where the robot is may lead to collision with obstacles. Belief Roadmap (BRM) @cite , Rapidly-exploring Random Belief Trees @cite , linear-quadratic controller based on an ensemble of paths @cite , local optimization over Partially Observable Markov Decision Process (POMDP) @cite , Feedback-based Information Roadmap (FIRM) @cite were used, representing risk as model uncertainty, to plan safe path. Most works were done in simulation with theoretical belief models. However, when planning with real robot in physical environments, a convincing method to quantify the probabilistic model is difficult to acquire.
- The trade-off between reward and risk was mostly addressed as chance or probability of success failure. @cite @cite proposed chance-constrained rapidly-exploring random tree (CC-RRT) approaches, which used chance constraints to guarantee probabilistic feasibility at each time step and over entire trajectory. Another popular approach to handle reward and risk is to use (PO)MDP. As standard MDP inherently contains reward but not risk, researchers have looked into representing risk as negative reward (penalty) @cite or constraints (C-POMDP) with unit cost for constraint violation @cite @cite @cite . Going beyond unit cost, CC-POMDP was proposed by @cite , which was based on a bound on the probability (chance) of some event happening during policy execution. RMPC is another alternative, with an emphasis on risk allocation, i.e., to allocate more risk for more rewarding actions @cite @cite .
- All existing methods require an artificial assignment of the adverse impact caused by risk, in the form of negative reward (penalty), unit cost, or bound on probability. Such assignment is not clear and a desired value may not even exist @cite . It is subjective to human bias and hard to determine, e.g., how to define the value of better situational awareness compared to the cost of losing the robot. This paper avoids the necessity of such artificial assignment, e.g., manually arbitrating the value of either maximum acceptable risk or minimum expected reward. A utility function of the ratio between reward and risk along the entire path is proposed, as a measurement of how much risk is taken to achieve one unit of reward. In addition, the desirable goal state is not specified to the planner beforehand, but discovered by the planner based on optimal utility during the planning process.
- Many hyperbolic embedding algorithms are based on two-dimensional hyperbolic disk, which was popularised by the Popularity-Similarity (or PS) model @cite . In @cite , Maximum Likelihood (ML) was used in to search the space of all PS models with similar structural properties as the observed network, to find the one that fit it best. This was extended by the authors in @cite . Due to the computationally demanding task of maximum likelihood estimation, often heuristic methods are used. For example, @cite used Laplacian Eigenmaps to estimate the angular coordinates of nodes in the PS model. The authors then combined both approaches to leverage the performance of ML estimation against the efficiency of heuristic search with a user controlled parameter in @cite . Additionally, @cite propose the use of classical manifold learning techniques in the PS model setting with a framework that they call .
- Beyond the two-dimensional hyperbolic disk, an @math -dimensional Poincar 'e ball can give more degrees of freedom to the embedding and capture further dimensions of attractiveness than just popularity'' and similarity'' @cite @cite . By embedding general graphs to trees, @cite were able to achieve state-of-the-art results by extending the work of @cite .
- Most of the existing studies involving breakdowns have concentrated on models with a single job queue served by one or more processors, e.g. @cite @cite . Other related results where jobs from a single source are directed to one of several parallel queues, and where breakdowns result in the loss of jobs, or the direct transfer to other queues are given in @cite @cite @cite . A network of two nodes subject to breakdowns and repairs was also analysed in @cite . Approximate solutions to obtain performance measures, based on replacing an interruptable server with an uninterruptable but slower one, choosing the new service rate without affecting the overall service capacity was given in @cite @cite .
- Queues with coupled processors were initially studied in @cite . To gain quantitative insights about the queueing process, they derived a solution for the generating function of the stationary joint queue-length distribution using the theory of Riemann-Hilbert boundary value problems. Later, in @cite a systematic and detailed study of the technique of reducing a two dimensional functional equation of a random walk or queueing model to a boundary value problem was presented, while several numerical issues were discussed. Important generalizations were given in @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite (not exhaustive list) where various two-dimensional queueing models with the aid of the theory of Riemann (-Hilbert) boundary value problems.
- A tandem queue with two coupled processors was analyzed in @cite , while later computational issues, as well as asymptotic results were discussed and presented in @cite @cite , respectively. There, it was shown that the problem of finding the bivariate generating function of the stationary joint queue length distribution is reduced to a Riemann-Hilbert boundary value problem.
- Applications of coupled processor models arise systems where limited resources are dynamically shared among processors, e.g., in data transfer in bidirectional cable and data networks @cite , in bandwidth sharing of data flows @cite , in the performance modeling of device-to-device communication @cite , to model the complex interdependence among transmitters due to interference @cite @cite @cite @cite , as well as in assembly lines in manufacturing @cite .
- Other approaches to analyze two-dimensional queueing models have been developed in @cite (compensation method), and @cite @cite (power series algorithm). In the latter one, power series expansions of steady-state probabilities as functions of a certain parameter of the system (usually the load) were derived. Recently, the authors in @cite studied generalized processor sharing queues and introduced an alternative method, by providing power series expansions of the generating function; see also @cite @cite @cite .
- Malte Helmert @cite published the Fast Downward planner, which is a classical planning system based on heuristic search. It can deal with general deterministic planning problems encoded in the propositional fragment of PDDL2.2, including advanced features like ADL conditions and effects and derived predicates (axioms). Like other well-known planners such as HSP and FF, Fast Downward is a progression planner, searching the space of world states of a planning task in the forward direction. This planner can be used in the cyber field by converting any attack problem to pddl problem and solve it using this planner. used this tool combined with the Jork Huffman POMDP converter @cite and some guidelines from work @cite in order to achieve "a worthy opponent" to our method.
- In cyber security, deception is often used to create a controlled path for attackers to follow starting from the the initial reconnaissance phase. For example, Cohen and Koike @cite @cite showed how deception can control the path of an attack using red teams in experiments attacking a computer network. Repik @cite makes a strong argument in favor of planned actions taken to mislead hackers. If the deception is obvious to the adversary, even unintentionally, the attacker can avoid, bypass, and even overcome the deceptive traps. We aim to avoid this situation and make the deception hard to identify by an adversary. RP Move to intro
- Recent lifelong learning research mainly focuses on overcoming the phenomenon @cite @cite @cite @cite , i.e., knowledge of previous tasks is abruptly forgotten when learning on a new task.
- Existing research mainly follow two directions: the first one is @cite @cite , which saves some previous samples and optimizes a new task with a forgetting cost defined on the saved samples. These methods have shown strength in alleviating catastrophic forgetting, but the computational cost grows rapidly with the number of previous tasks. The second direction is to @cite @cite @cite @cite . For example, Elastic Weight Consolidation (EWC) @cite slows down learning on weights that are important to previous tasks. These methods usually do not need to save any previous data and only train on each task once. But their abilities to overcome catastrophic forgetting are limited.
- There is another related direction on dynamically changing the model structure (i.e., adding new modules) in order to learn the new task without interfering learned knowledge for previous tasks, such as @cite @cite @cite . These approaches could successfully prevent forgetting. However, they do not suit many lifelong settings in NLP. First, it cannot benefit from the positive transfer between tasks. Second, the size of the model grows dramatically with the number of observed tasks, which makes it infeasible for real-world problems where there are a lot of tasks.
- Few-shot learning aims to learn transferable knowledge that can be generalized to new classes with scarce labeled training data. There exist many formulations on few-shot classification, including recurrent neural network with memories @cite @cite , learning to fine-tune models @cite @cite , network parameter prediction @cite @cite , and metric learning @cite @cite @cite . Metric learning based methods achieve state-of-the-art performance in the few-shot classification tasks and they have the trait of being fast and predicting in a feed-forward manner. Our work is most related to Relation Network @cite . Relation Network meta-learns a deep distance metric to compare images and compute the similarity score for classification. The network consists of an embedding module which generates the representations of the images and a relation module that compares the embeddings and outputs a similarity score. Both modules are in the form of convolutional operations. The dense comparison module in our network can be seen as an extension of Relation Network in a dense form to tackle the task of segmentation.
- While the aim of video object segmentation is to segment out the foreground objects from a video sequence, this approach can indeed be used to perform background extraction. However, many of these works require some degree of human intervention, such as the semi-supervised methods @cite @cite @cite @cite @cite @cite which require a small amount of manual annotation, and the fully-supervised methods @cite @cite @cite which require repeating result correction by the user. Our work is completely autonomous, like the unsupervised methods of video object segmentation @cite @cite @cite @cite @cite @cite @cite . However, the underlying assumption of our work is less brittle compared to those subscribed to by these works. For instance, @cite @cite rely on the ability of object proposals to detect the foreground objects; this might fail to work when the foreground objects have complex non-compact shapes. @cite @cite detect foreground objects by analysing the 2D motion field based on a simple assumption that they usually move differently from their surroundings. However, the 2D motions of some background objects may also have this property if the background has large depth variation.
- Trajectory-based motion segmentation methods usually produce multiple independent motion groups; however, they usually stop short of actually identifying the background motion, or just use a simple background metric such as size. Furthermore, due to the computational demand of the 3D motion segmentation methods @cite @cite @cite @cite , these works lack the ability to deal with large amount of feature trajectories. For long and densely sampled feature trajectories, some fast 2D methods @cite @cite may be used. However, their results may become over-segmented when the figure-ground is complex, for instance, scenes with large depth variation. Our method not only processes large amount of trajectory in an efficient way, but also handle videos with complex scene structures.
- Two broad classes of SP-based and IR-based approaches have been proposed for KBQA. The former attempts to convert NL questions to logic forms. Recent work focused on approaches based on weak supervision from either external resources @cite @cite @cite @cite @cite @cite , schema matching @cite , or using hand-crafted rules and features @cite @cite @cite @cite @cite @cite @cite @cite @cite . A thread of research has been explored to generate semantic query graphs from NL questions such as using coarse alignment between phrases and predicates @cite , searching partial logical forms via an agenda-based strategy @cite , pushing down the disambiguation step into the query evaluation stage @cite , or exploiting rich syntactic information in NL questions @cite @cite . Notably, another thread of SP-based approaches try to exploit IR-based techniques @cite @cite @cite @cite @cite @cite @cite by computing the similarity of two sequences as features, leveraging a neural network-based answer type prediction model, or training end-to-end neural symbolic machine via REINFORCE @cite . However, most SP-based approaches more or less rely on hand-crafted rules or features, which limits their scalability and transferability.
- The idea of bidirectional attention proposed in this work is similar to those applied in machine reading comprehension @cite @cite @cite . However, these previous works focus on capturing the interactions between two bodies of text, in this work, we focus on modeling the interactions between one body of text and a KB.
- The difference between double backpropagation and our gradient penalization is that we penalize the @math -norm of the gradient of each output with respect to the input while, in backpropagation, the @math -norm of the loss is penalized. Hence, we should not have the problem that occurs when the penalization term is multiplied by a small error vector. Nevertheless, this might not be a problem when using a different loss function. Although empirical evidence show double backpropagation's efficiency to enhance generalization, it is insufficient to defend against adversarial examples. That is what @cite highlights saying limiting sensitivity to infinitesimal perturbation [e.g., using double backpropagation][] drucker1992improving only provides constraints very near training examples, so it does not solve the adversarial perturbation problem . But there are evidence that coupling that gradient penalty with adversarial training increases the robustness.
- @cite present a method for WordNet construction and enlargement with the help of sense tagged parallel corpora. Since parallel sense tagged data are not always available, they use to translate a manually sense tagged corpus. In addition they apply automatic sense tagging of a manually translated parallel corpus, whereby they report worse performance compared to the previous approach. We try to overcome this issue by engaging up to ten languages to improve the performance of the automatic sense tagging. Similarly, BabelNet @cite aligns the lexicographic knowledge from WordNet to the encyclopaedic knowledge of Wikipedia. This is done by assigning WordNet synsets to Wikipedia entries, and making these relations multilingual through the interlingual links. For languages, which do not have the corresponding Wikipedia entry, the authors use to translate English sentences containing the synset in the sense annotated corpus. After that, the most frequent translation is included as a variant for the synset for the given language.
- The use of parallel corpora has been previously exploited for word sense disambiguation, for example to construct sense-tagged corpora in another language @cite or by using translations as a method to discriminate senses @cite . It has been shown that the combination of these techniques can improve supervised word sense disambiguation @cite . A similar approach to the one proposed in this paper is that of @cite , where they show that using the interlingual index of WordNet with the help of parallel text can improve word sense disambiguation of a monolingual approach and we generalize this result to generate wordnets for new languages.
- @cite gives a model for evaluation in subway fire, which emphasizes the number and width of escape exits. The authors believe that a subway station should have at least two exits in different directions, and the wider the exits, the better. @cite presents a model based on the parallel computational tool of cellular automata capable of simulating the process of disembarking in a small airplane seat layout, in search of ways to make it faster and safer under normal evacuation conditions, as well as emergency scenarios. @cite provides a comprehensive survey of evacuation planning models.
- @cite the likely behaviours of train passengers in an emergency evacuation are explored in the form of questionnaires. The results showed that respondents are more likely to be reactive (e.g., wait for instruction from station staff) rather than proactive (e.g., move to exit); more likely to be cooperative (e.g., helping other people) than competitive (e.g., push other passengers); more likely to show herding or symmetry behaviour (e.g., following other passengers) than symmetric breaking behaviour (e.g., choose least crowded exit); and less likely to use the lifts, escalators and tunnels to escape in an emergency situation. In terms of demographic differences in behaviours, it's demonstrated that there are significant differences in the evacuation behaviours between males and females. Males are less likely to use emergency call buttons, call the emergency phone number or wait at the assembly area. Also, they are more likely to display competitive behaviour, choose the least crowded exit, and use the lifts, escalators and tunnels to escape in the event of an emergency evacuation. In contrast, differences in behaviours are not as obvious among different age groups.
- @cite emphasizes the need for greater participation from individuals with disabilities during the evacuation planning process. Understanding and addressing the psychological needs of individuals with disabilities during emergency evacuations will help ensure the safety of everyone.
- @cite proposes a behavior-based cellular automaton (CA) model for pedestrian dynamics and show that for a given range of emergency degree, the enlarged emergency degree can shorten the evacuation time yet depresses cooperation enthusiasm. On the contrary, as dependence (familiarity) of pedestrians increases, the evacuation time and the fraction of cooperation increase.
- Unlike other approaches, the proposed trellis encoder-decoder architecture attempts to generate high-quality density estimation maps by preserving the spatial information in the encoding feature hierarchy. More importantly, it incorporates a multi-path decoder to enhance the aggregation and fusion of multi-scale features with rich spatial and semantic information. As a result, pixel-wise regression accuracy in the estimated map is enhanced. In a broader view, density estimation is similar to other localization-oriented tasks, such as tracking @cite @cite @cite and detection @cite , which also generate localization estimation maps as outputs. These tasks are inter-correlated with density estimations such that the resulting localization maps can be fused to integrate task-specific localization response @cite @cite . Moreover, semantic segmentation also relies on powerful encoder-decoder architecture to integrate multi-scale features and to improve localization precision. Consequently, efforts have been made to enhance the hourglass architecture. In @cite , SDN stacks multiple single-path hourglass networks into a deeper sequence to improve the feature fusion and guarantee fine recovery of localization information. In @cite @cite , the single-path hourglass network is extended by adding residual units inside the skip connections.
- Detection-based methods, such as maneuver recognition approaches, have been studied extensively. Houenou @cite identify maneuver patterns such as LK and LC based on a handcrafted model which evaluates the similarity between the vehicle trajectory and lane center line. However, user-specified parameters and thresholds are required. Mandalia @cite classify the LC maneuver using support vector machine (SVM), while Schlechitriemen @cite use Gaussian mixture models to estimate driving intentions. Woo @cite also adopt an SVM, and suggest using trajectory prediction to reject false alarms from the intention estimations. These methods detect LCs based on the target's own maneuver pattern and only have a limited prediction horizon. In contrast, our method captures the driver intention, even in the absence of any clear maneuver pattern, by making use of the vehicle behavior interactions.
- Many methods focus on trajectory prediction and generate a time-profiled trajectory directly. Regression methods, such as linear regressions @cite and non-linear Gaussian process regression models @cite @cite @cite , are extensively studied. Recurrent neural networks (RNNs) have recently been widely applied to various trajectory prediction tasks including trajectory prediction for pedestrians and vehicles. For vehicles, apart from predicting trajectories, many methods also aim to predict high-level intentions @cite @cite @cite @cite . Moreover, some recent works suggest that trajectory prediction can be augmented by the prediction of high-level semantic behaviors, e.g., LCs and insertions, due to the multimodal nature of the problem @cite @cite @cite . In this paper, we focus on the behavior prediction problem.
- Many learning-based prediction methods follow an RNN encoder-decoder structure @cite and use it for predicting the trajectory of pedestrians @cite @cite @cite and vehicles @cite @cite . RNNs are an alternative to the traditional methods (e.g., SVM, Gaussian process) @cite @cite for capturing maneuver patterns. Some works move one step further and consider the multi-agent interactions among pedestrians @cite @cite @cite @cite and vehicles @cite @cite in the RNN structure. The interaction-aware models for vehicles mainly adopt a social pooling strategy using RNN hidden states, which may be problematic for a highly dynamic scenario, as elaborated in the following.
- Modeling social interaction in RNNs started with the seminal work by Alahi @cite , which proposes a social occupancy grid pooling mechanism, as shown in Fig. . This design is very effective for pedestrian trajectory prediction in a crowd @cite @cite @cite , and is suitable for unstructured environments and modeling collision avoidance behaviors. It is directly applied to vehicle prediction in @cite and @cite . However, for highly dynamic on-road driving, spatial locations should not be the only reference for weighting social effects, as shown in Fig. . Going beyond these methods, our proposed VBIN can learn to weight the interactions automatically based on the relative dynamics of both agents.
- Firstly, the existence of low-rank solutions is used in Burer and Monteiro method @cite . It is based on the factorization @math , where @math is an arbitrary matrix of size @math . This problem is non-convex, though it requires much less computations and performs well in practice. However, finding the minimum rank solution of multiple runs of the algorithm with different @math (one increments @math until resulting point satisfies particular conditions) might be computationally ineffective.
- In our work we propose an efficient first-order algorithm, which performs rank minimization. It starts from the solution of the SDP relaxation. This solution is provided by either CVX interior-point algorithm @cite , @cite or Burer-Monteiro low-rank procedure, implemented in SDPLR @cite .
- An alternative approach to generate behaviors is using data-driven methods that exploit the relationship between body movements and acoustic features (e.g., prosody). Vigot al @cite demonstrated that there is a statistically significant correlation between prosodic features and raw body movements. Graf al @cite showed that there is correlation between prosodic events and behaviors such as eyebrow and head movements. Busso al @cite reported that the correlation between prosodic features and head movements across different emotions are on average more than @math , using (CCA). Speech and gestures also co-occur. The study from McNeill @cite showed that more than 90
- Studies have attempted to combine both approaches creating hybrid frameworks. Stone al @cite designed a hybrid system to generate meaningful behaviors given the text. They jointly segment audio and motion capture recordings into units expressing pre-defined communicative intents. Given an input text, they parse the input into their predefined categories, using dynamic programming to find speech and motion capture segments that have the same communicative goal. The generation with this framework is limited to the stored speech segments. Sadoughi al @cite proposed to constrain a speech-driven model based on the discourse functions of the sentence to generate more meaningful head and eyebrow motion. The study was limited to only two discourse functions: and , where the subjective evaluation of the result showed improvements for the constrained model versus the unconstrained model when the constraint was .
- Sadoughi al @cite proposed a model to generate speech-driven head and eyebrow movements constrained on discourse functions. The preliminary study tested the constrained model on one session of the IEMOCAP corpus, constraining the models on two dialog acts: , and . The subjective evaluation of the models showed that the behaviors from the constrained models are more preferable, natural and appropriate compared to the unconstrained model. In Sadoughi and Busso @cite , we explored the idea of constraining the models using prototypical behaviors such as head nods. The models were trained with gestures directly retrieved from the corpus by providing few examples.
- Many methods that attempt to integrate the nonlinearity of wave scattering have been proposed in the literature. The iterative linearization (IL) method @cite @cite iteratively computes the forward model using the current estimated permittivity, and estimates the permittivity using the field from the previously computed forward model. Hence, each sub-problem at each iteration is a linear problem. Contrast source inversion (CSI) @cite @cite @cite defines an auxiliary variable called the contrast source, which is the product of the permittivity contrast and the field. CSI alternates between estimating the contrast source and the permittivity. Hybrid methods (HM) @cite @cite @cite combine IL and CSI, aiming to benefit from each of their advantages. A comprehensive comparison of these three methods can be found in the review paper @cite . Recently, the idea of neural network unfolding has inspired a class of methods that updates the estimates using error backpropagation @cite @cite @cite @cite @cite . While such methods can in principle model the precise nonlinearity, in practice, the accuracy may be limited by the availability of memory to store the iterates needed to perform unfolding.
- Distributional privacy @cite is a privacy mechanism which says that the released aggregate information only reveals the underlying ground truth distribution and nothing morre. Each data owner is protected by the randomness of the other randomly selected data owners rather than by adding explicit privacy noise to the output. The indistinguishability from the underlying distribution protects individual data owners and is strictly stronger than differential privacy. However, it is computationally inefficient though can work over a large class of queries known as Vapnik-Chervonenkis (VC) dimension.
- Sampling whereby a centralized aggregator randomly discards responses has been previously formulated as a mechanism to amplify privacy @cite @cite @cite @cite @cite . The intuition is that when sampling approximates the original aggregate information, an attacker is unable to distinguish when sampling is performed and which data owners are sampled. These privacy mechanisms range from sampling without a sanitization mechanism, sampling to amplify a differentially private mechanism, sampling that tolerates a bias, and even sampling a weaker privacy notion such as k-anonymity to amplify the privacy guarantees.
- Many previous works have noticed the problem of training an NMT system with lots of parameters. Some of them prefer to use the dropout technique @cite @cite @cite . Another possible choice is to ensemble several models with random starting points @cite @cite @cite . Both techniques could bring more stable and better results. But they are general training techniques of neural networks, which are not specifically targeting the modeling of the translation process like ours. We will make empirical comparison with them in the experiments.
- The word prediction technique has been applied in the research of both statistical machine translation (SMT) @cite @cite @cite @cite and NMT @cite @cite . In these research, word prediction mechanisms are employed to decide the selection of words or constrain the target vocabulary, while in this paper, we use word prediction as a control mechanism for neural model training.
- Recurrent neural network (RNN), especially long short-term memory @cite is the most widely used layer in sequence transduction models. However, the inherent sequential nature of RNN model precludes parallelisation. Google proposed the Transformer @cite architecture to eschew recurrence and only based on attention mechanism which significantly improves the training speed. Facebook also proposed their sequence transduction model architecture @cite which based entirely on convolutional neural networks (CNN) to parallelise computation.
- Generative adversarial network (GAN) @cite has also been employed to language generation task. In @cite @cite @cite , the generator is the sequence transduction model, and the discriminator is to discriminate between machine generated sequences and the human generated ones. Policy gradient update and Monte Carlo search are utilised to solve the generator differentiation problem. Moreover, teacher forcing plays a significant role in improving training stability in the adversarial training structure.
- @cite . FaceNet uses traiplet loss to train the CNN model and mines semi-hard examples to train the triplet loss. It utilized on a dataset with 8M identities and trained on a cluster of cpu for thousands of hours. FaceNet achieves remarkable result on LFW @cite while it's not practical because it relys on such large dataset and need a large mount of time to train
- @cite . This work applys triplet loss on person re-identification problem. It constructs image batch efficiently by sampling @math persons with @math images each. It also proposed batch hard and batch all hard example mining strategies. This work achieves the start-of-the-art in person re-identification.
- Most popular person Re-ID algorithms can be categorized into two classes, feature representation learning and metric learning. For the first category, usually the human identity labels are exploited as the supervision for training a classifier for different identities and can be considered as a classification problem. During recent years, CNN-based feature representation learning has been dominating various research fields because of its excellent performance and is no exception in person Re-ID community. Xiao al @cite propose a joint learning strategy to train a single classifier for multiple domains at the same time, and then fine-tune to adapt to each single domain with a domain guided dropout policy.
- The idea of decomposing behavior into reusable components or abstractions has a long history in the reinforcement learning literature. One question that remains largely unanswered, however, is that of suitable criteria for identifying such abstractions. The option framework itself @cite @cite provides a computational model that allows the implementation of temporal abstractions but does in itself not provide an objective for option induction. This is addressed partially in @cite where long-lasting options are explicitly encouraged.
- A popular means of encouraging specialization is via different forms of information hiding, either by shielding part of the policy from the task goal (e.g. @cite ) or from the task reward (e.g. @cite ). @cite combine information hiding with meta-learning to learn options that are easy to reuse across tasks.
- Information-theoretic regularization terms that encourage mutual information between the option and features of the resulting trajectory (such as the final state) have been used to induce diverse options in an unsupervised or mixed setting (e.g. @cite @cite @cite ), and they can be combined with information hiding (e.g. @cite ). Similar to our objective they can be seen to encourage predictable outcomes of options. @cite have recently proposed a model that directly encourages the predictability of trajectories associated with continuous option embeddings to facilitate mixed model-based and model-free control.
- Finally, another class of approaches that is related to our overall intuition is one that seeks to identify "bottleneck" states and construct goal-directed options to reach them. There are many definitions of bottlenecks, but they are generally understood to be states that connect different parts of an environment, and are hence visited more often by successful trajectories. Such states can be identified through heuristics related to the pattern of state visitation @cite @cite or by looking at between-ness centrality measures on the state-transition graphs. Our objective is based on a similar motivation of finding a small number of states that give rise to a compressed high-level decision problem which is easy to solve. However, our algorithm is very different (and cheaper computationally).
- Edge caching has been proposed as an efficient way to improve the performance of cellular networks in terms of the observed delivery delay @cite @cite @cite @cite @cite @cite , energy consumption @cite , and operating cost @cite . These benefits of edge caching are attributed to the fact that only a small portion of available content accounts for most of the traffic load @cite . Hence, by caching the most popular content at the SBSs, significant resources are saved. In @cite , it is explored how to capture content popularity dynamics among the various caches by using at each SBS the information about its neighboring SBSs. The impact of content request patterns and users' behavior on edge caching-assisted mobile video streaming of real-life data sets is studied in @cite .
- Caching systems can be further improved and offer higher QoE to the end users by exploiting video encoding into multiple layers (layered video) which can be done by scalable coders such as @cite . Specifically, in @cite @cite , the flexibility in caching decisions coming from encoding in multiple layers is used to minimize the delivery delay at the end-users by allowing the video layers to be cached at different SBSs of a network operator. To solve the problem a pseudo-polynomial approximation algorithm is proposed. For a similar setting, another heuristic algorithm is presented in @cite . The work in @cite also examines the case of several network operators collaboratively optimizing their caching strategies. In this setting, SBSs' cache space is split into two parts: one allocated for contents requested by users of the operator who owns the cache and a second which is given for caching video layers of content requested by users who belong to other operators. In @cite , the caching cost, the available cache capacity at the SBSs, and the various social traits of the mobile users are considered to decide which video layers to cache in each SBS.
- In @cite , QoE for streaming of @math videos is studied in terms of: a) perceptual quality, b) presence, c) acceptability, and d) cyber-sickness. In particular, perceptual quality and the acceptability are measured when users watch a @math video both with or without a Head-Mounted Display (HMD). Similarly, in @cite , a number of objective quality metrics, including PSNR, weighted to spherically uniform PSNR (WS-PSNR), spherical PSNR without interpolation (S-PSNR-NN), spherical PSNR with interpolation (S-PSNR-I), and PSNR in Crasters Parabolic Projection (CPP-PSNR) are explored for quantifying the quality of @math video encoding. It is concluded that traditional PSNR is the most appropriate quality metric for measuring end-to-end distortion because of its low complexity.
- Methods in image aesthetics assessment could generally be divided into three distinct categories: classical handcrafted low-level features, generic features based on image descriptors, and the contemporary approach of utilizing deep learning models. @cite proposed visual features based on standard photography and visual design rules to encapsulate aesthetic attributes from low-level image features. @cite proposed to learn aesthetic attributes from textual comments on the photographs using generic image features. Recently, deep learning methods have been applied to image aesthetic assessment @cite @cite @cite and have significantly improved the prediction precision against previous non-deep methods. @cite proposed a query-dependent aesthetic model based on feature representation learned from CNN. @cite proposed to adopt the generic features from the penultimate layer output of AlexNet with spatial pyramid pooling. @cite proposed a CNN modified from AlexNet by stacking seven scene convolutional layers. @cite proposed ILGNet derived from part of the GoogLeNet which contains Inception module.
- Generative Adversarial Networks (GANs), proposed by , have shown great potential for generating or modifying images. Many studies focused on image augmentation using GANs @cite @cite @cite . The application to the medical domain is logical, because it is generally difficult to obtain data there, and all datasets are naturally heavily imbalanced. focuses on brain MRI augmentation using paired image-to-image translation similar to the pix2pix approach @cite .
- Several enhancements to the original gradient descent algorithm have been proposed. These include adding a momentum" term to the update rule @cite , and adaptive gradient" methods such as RMSProp @cite , and Adam @cite , which combines RMSProp and AdaGrad @cite . These methods have seen widespread use in deep neural networks @cite @cite @cite .
- Recently, there has been a lot of work on finding novel ways to adaptively change the learning rate. These have both theoretical @cite and intuitive, empirical @cite @cite backing. These works rely on non-monotonic scheduling of the learning rate. @cite argues for cyclical learning rates. Our proposed method also yields a non-monotonic learning rate, but does not follow any predefined shape.
- Our work is also motivated by recent works that theoretically show that stochastic gradient descent is sufficient to optimize over-parameterized neural networks, making minimal assumptions @cite @cite . Our aim is to mathematically identify an optimal learning rate, rejecting the notion that only small learning rates must be used, and then experimentally show the validity of our claims.
- Most previous work on exploring semantics for statistical machine translation (SMT) studies the usefulness of predicate-argument structure from semantic role labeling @cite @cite @cite @cite . first convert Prolog expressions into graphical meaning representations, leveraging synchronous hyperedge replacement grammar to parse the input graphs while generating the outputs. Their graphical meaning representation is different from AMR under a strict definition, and their experimental data are limited to 880 sentences. We are the first to investigate AMR on a large-scale machine translation task.
- Recently, investigate semantic role labeling (SRL) on neural machine translation (NMT). The predicate-argument structures are encoded via graph convolutional network (GCN) layers @cite , which are laid on top of regular BiRNN or CNN layers. Our work is in line with exploring semantic information, but different in exploiting AMR rather than SRL for NMT @. In addition, we leverage a graph recurrent network (GRN) @cite @cite for modeling AMRs rather than GCN, which is formally consistent with the RNN sentence encoder. Since there is no one-to-one correspondence between AMR nodes and source words, we adopt a doubly-attentive LSTM decoder, which is another major difference from .
- GRNs have recently been used to model graph structures in NLP tasks. In particular, use a GRN model to represent raw sentences by building a graph structure of neighboring words and a sentence-level node, showing that the encoder outperforms BiLSTMs and Transformer @cite on classification and sequence labeling tasks; build a GRN for encoding AMR graphs for text generation, showing that the representation is superior compared to BiLSTM on serialized AMR. We extend by investigating the usefulness of AMR for neural machine translation. To our knowledge, we are the first to use GRN for machine translation.
- In an effort to understand the effects of technology on policing, the HMIC (Her Majesty's Inspectorate of Constabulary) @cite published a report to study the current readiness of police services to effectively deal with cybercrimes and their victims. The main outcomes from the study were related to providing training, awareness, and guidance for all those involved with policing cybercrimes. In addition, they recommended raising the level of capabilities within law enforcement in digital forensics and examining digital devices. Another study by , @cite focused on identifying different technical investigation roles needed for policing cybercrimes. These roles are technical enquirer, network investigator, forensic technician, digital forensic examiner and technical domain expert. They highlight that when complexity and risk of the cybercrime increase, the level of specialist technical skills and knowledge should also increase. Moreover, another study by , @cite focused on identifying the needs of cyber forensic investigators. They conducted a survey of participants from different occupations including cyber forensic students, professors, law enforcement, and practitioners. The results from the survey suggest that participants indicated that the main needs include additional funding, advanced tools, better communication, and revised laws.
- Our modeling framework deviates significantly from the existing literature as we employ a GraphLSTM (Graph-based LSTM) @cite model to learn an arbitrarily structured graph, where each node corresponds to the aggregated demand in a spatial Voronoi partition. To the best of our knowledge, in the context of spatial partitioning, Graph-based RNNs have not been explored in the literature. Another important contribution of this paper lies in understanding the impact of different spatial partitioning schemes on the predictive performance of RNNs. To that end, we perform a comparison of the Geohash-based ConvLSTM and the Voronoi-based GraphLSTM. These features set our work apart from the existing literature. A part of this work was presented as a poster at the NIPS Workshop on Machine Learning in Intelligent Transportation Systems, 2018 @cite .
- Sequential search methods perform a training run with some candidate hyperparameters, and use the results to inform the choice of the next set of hyperparameters for evaluation. BO is a sample efficient global optimisation framework that models performance as a function of the hyperparameters, and is especially suited for sequential search as each training run is expensive. After each training run BO uses the observed performance to update the model in a Bayesian way, which then informs the choice of the next set of hyperparameters for evaluation. Several modifications have been suggested to further reduce the number of evaluations required: input warping @cite to address nonstationary fitness landscapes; freeze-thaw BO @cite to decide whether a new training run should be started and the current one discontinued based on interim performance; transferring knowledge about hyperparameters across similar tasks @cite ; and modelling training time as a function of dataset size @cite . To further speed up the wall clock time, some BO based methods use a hybrid mode wherein batches of hyperparameter settings are evaluated in parallel @cite @cite @cite @cite @cite .
- By contrast, parallel search methods like grid search and random search run multiple training runs with different hyperparameter settings in parallel to reduce wall clock time, but require more parallel computational resources. These methods are easy to implement, and have been shown to perform well @cite @cite .
- Both sequential and parallel search suffer from two key disadvantages. First, they require performing multiple training runs to identify good hyperparameters. Not only is this computationally inefficient, but when applied to RL, also sample inefficient as each run requires fresh interactions with the environment. Second, these methods learn fixed values for the hyperparameters that are used throughout training instead of a schedule, which can lead to suboptimal performance @cite @cite @cite .
- PBT @cite is a hybrid of random search and sequential search, with the added benefit of learning a schedule of hyperparameters. It starts by training a population of hyperparameters which are then updated periodically throughout training to further explore promising hyperparameter settings. However, by requiring multiple training runs, it inherits the sample inefficiency of random search.
- Like HOOF, gradient based methods @cite @cite @cite @cite @cite are highly sample efficient and require only one training run to optimise hyperparameters. They perform gradient descent on some suitably chosen loss function with respect to the hyperparameters. Hence, they are even more restricted than HOOF in the hyperparameters that they can optimise. For example, the approach of optimises only the learning rate. Meta-gradients @cite optimise only the discount rate and TD( @math ) hyperparameters and, unlike HOOF, cannot optimise, e.g., the learning rate and the policy entropy coefficient in the A2C objective function.
- Early approaches towards computational models of visual attention were defined in terms of different theoretical frameworks, including Bayesian @cite and graph-based formulations @cite . A mechanism inspired more by biological than mathematical principles was first implemented and described in the seminal work by . Their model captures center-surround differences at multiple spatial scales with respect to three basic feature channels: color, intensity, and orientation. After normalization of activity levels, the output is fed into a common saliency map depicting local conspicuity in static scenes. This standard cognitive architecture has since been augmented with additional feature channels that capture semantic image content, such as faces and text @cite .
- Later attempts addressed that shortcoming by taking advantage of classification architectures pre-trained on the database @cite . This choice was motivated by the finding that features extracted from CNNs generalize well to other visual tasks @cite . Consequently, @cite and @cite employed a pre-trained classification model to read out salient image locations from a small subset of encoding layers. This is similar to the network by which utilizes the output at three stages of the hierarchy. Related approaches also focused on the potential benefits of incorporating activation from both coarse and fine image resolutions @cite , and recurrent connections to capture long-range spatial dependencies in convolutional feature maps @cite @cite . Our model explicitly combines semantic representations at multiple spatial scales to include con-textual information in the predictive process. For a more complete account of existing saliency architectures, we refer the interested reader to a comprehensive review by .
- The binary @math - @math -insertion channel was first studied in @cite , where a construction of codes correcting @math such errors was given, and asymptotic bounds on the cardinality of optimal codes derived for any fixed @math and @math . Another construction for same model was later given in @cite . Generalizations of the constructions from @cite and @cite for the @math -duplication channel were given in @cite and @cite , respectively. The best known asymptotic bounds for the @math -duplication channel in the regime of fixed @math and @math , were reported in @cite , where the upper bound from @cite (for @math ) was improved for every @math , and the bounds were also generalized to the case @math .
- The capacity of the @math -ary @math -duplication channel was determined in @cite . We should note, however, that for @math , which is the main motivating model in the present paper, the zero-error capacity of the @math -duplication channel is trivially @math , so the mentioned results from @cite are only interesting for @math . The zero-error capacity of a model that can be seen as the version of the @math -duplication channel was determined in @cite .
- Notice that, in all of the above-mentioned works with the exception of @cite , no upper bound on the number of inserted duplicates at each position was assumed, i.e., @math is taken to be @math . We find the assumption @math realistic, especially in the context of the model with synchronization errors that inspired introducing the duplication channel in the first place. Our main results here are a construction of optimal zero-error codes and a characterization of the zero-error capacity of the @math -duplication channel, for any fixed @math and @math . We recover the mentioned results from @cite as a special case when @math , and we also obtain the discrete-time analogs of the results from @cite as a special case when @math . Moreover, and further generalizing our results, we obtain a characterization of the zero-error capacity of the @math - @math -insertion channel.
- In @cite , the author proposed an analysis of packet collision and packet loss probabilities in LoRaWAN, and developed theoretical expressions for both of them. The author showed that his theoretical expressions are more accurate than a Poisson distributed process to describe the collisions.
- In @cite , the authors made a study regarding the CSS modulation technique. They show that some CSS symbols are not orthogonal. Their simulations show that the achievable range of the CSS technique is lower than an ultra-narrowband solution, but the robustness against interference is higher.
- In @cite , the authors provide an analysis and report experimental validation of the various performance metrics of the LoRa technology. The LoRa modulation is based on CSS, which enables low-quality oscillators in the end-device, and faster and more reliable synchronization. Therefore, LoRa seems to be a promising option for implementing communication in many diverse IoT applications. Authors first overviewed the features and analyzed the scalability of LoRa network. Then, they introduced setups of the performance measurements. Their results showed that using the transmit power of 14 dBm and the highest spreading factor of 12, more than 60
- Few works have been done to study the collisions among synchronized signals in LoRa as in the following papers @cite , @cite and @cite .
- In @cite , the authors have proposed two algorithms to decode some cases of collisions of LoRa signals. The first algorithm is used when superposed signals are slightly desynchronized, and the second algorithm is used when superposed signals are completely synchronized. Authors observe that the first algorithm is able to significantly increase the throughput, by decoding many collisions of two signals, and some collisions of three signals. On the other side, the second algorithm has improved the throughput, by decoding both signals when exactly two signals are colliding. The second algorithm requests any of the two colliding transmitters to retransmit its frame. Hence, when one frame is retransmitted, the algorithm is able to decode it, and to deduce the other colliding frame by elimination. The authors have considered the case of only two synchronized signals. In this paper, we are improving this by reducing the amount of data retransmitted, and by considering two or more synchronized signals.
- Generating images from text requires each sentence to be encoded into a fixed length vector. Previous works such as StackGAN used text embedding generated by a pre-trained Convolutional Recurrent Neural Network @cite . The RNN network has been widely applied in modelling natural languages for classification or translation purposes @cite .
- Deep Convolutional GAN (DCGAN) @cite utilized several layers of convolutional neural networks to encode and decode images, in addition to Batch Normalization @cite to stabilize the GAN training.
- On the basis of DCGAN, GAN-CLS @cite generated images based on the corresponding image caption @math in addition to the noise vector @math . @math was sampled from a Gaussian distribution @math , and the text description @math was encoded with a pre-trained text encoder @math to be @math . @math was then concatenated with @math and processed through series of transposed convolution layers to generate the fake image. In the discriminator @math , a series of convolution-batch normalization-leaky ReLU were applied to discriminate between true images and fake images.
- GAWWN @cite proposed an architecture for controllable text-to-image generation that adopted supplementary information such as bounding boxes or part locations of the main object in the image.
- As previous works failed to generate images with higher-resolution than @math , StackGAN @cite employed a 2-stage GAN network to generate photo-realistic @math images from text descriptions. Its architecture consisted of 2 stages: Stage-I generated a low-resolution image (e.g. @math ) based on texts and random noises, Stage-II generated a higher resolution image (e.g. @math ) based on texts and the lower resolution images from Stage-I.
- AttnGAN @cite was an extension of StackGAN, which used an attention mechanism in addition to an image-text matching score. It was able to generate images of better quality and achieve higher inception score.
- For a quite long period of time, the conventional features such as Improved Dense Trajectory Feature (iDTF) @cite were in a dominant position in the field of action recognition. In recent years, thanks to ImageNet dataset @cite , Convolutional Neural Networks (CNN) such VGG @cite , ResNet @cite have gradually been proposed and adapted to perform action recognition, but the performance is still poor since they can only capture appearance information. To model motion information in videos, various two-stream CNNs which take both RGB images and optical flow as input have significantly improve the action recognition performance and surpass the conventional features @cite @cite . To explicitly model spatio-temporal feature directly from raw videos, a 3D CNN architecture called C3D is proposed in @cite .
- A typical framework used in many state-of-the-art temporal action localization systems @cite @cite is detection by classification framework, which is borrowed from object detection task. First, various features are extracted on the action segments pre-determined by action proposals. Then action classifiers are trained on these features to classify these action segments. In order to design a model specific to temporal localization, in @cite , statistical length and language modeling are used to represent temporal and contextual structure. In @cite a sparse learning framework is proposed to retrieve action segment proposals of high recall.
- Weakly supervised object detection has been studied a lot in the past few years @cite . Several works try to adapt these methods to weakly supervised action understanding @cite @cite @cite @cite . These works can be divided into three types according to the used weak supervision. The first type is movie script, which gives uncertain temporal annotations of action instances @cite . The second type is an ordered list of action classes occurring in the videos @cite . The third type weak supervision only contains video labels which contain no any order information of the containing action instances but whether a action category appears in a video @cite @cite @cite . Our weakly supervised FSN falls into the third type.
- Significant accomplishments have been made in computer vision, resulting in increasingly effective state of the art methods for image processing @cite @cite . Early efforts in automatic rooftop segmentation relied on techniques to generate candidate rooftops and subsequent evaluation to accept or reject candidate rooftops. Edge detection, corner detection, and segmentation into homogeneous regions via k-means clustering or support Vector Machines (SVM) have been used to identify candidate rooftops @cite @cite . Discriminative features used to evaluate candidate rooftops include building shadows, geometry and spectral characteristics @cite @cite . Several approaches have used LIDAR alone or in addition to multi-spectral images @cite @cite @cite
- Newer-generation machine learning techniques @cite have also been applied in satellite image classification @cite and in rooftop segmentation specifically @cite @cite @cite . Convolutional neural networks (CNNs) have greatly improved the state of the art in semantic segmentation tasks wherein each pixel in an image is associated with a class label @cite @cite . High-resolution rooftop detection presents a dense prediction problem in which proper pixel-wise labeling is paramount to a produce a product with well-defined rooftops. Recent work in precipitation downscaling uses stacked CNNs to outperform a suite of machine learning methods @cite . Another study uses stacked U-Nets which enhance the results of the previous U-Net @cite . This study found that stacking of just two CNNs outperforms the state of the art method. Introduced in 2015, U-Nets utilize skip connections and an encoder-decoder structure to learn a latent translation from input to output @cite .
- Adversarial training has been demonstrated to improve the labeling accuracy of semantic segmentation models @cite . One study compares the performance of U-Net and adversarial-trained U-Net for semantic segmentation of roads and finds that adversarial training reduces over fitting and improves validation accuracy. @cite . Much work has been done applying adversarial semantic segmentation in medical imaging as well as style transfer @cite @cite @cite . One study focusing on rooftop segmentation uses GANs to overcome missing data, while another uses conditional GANs to refine 3D building shapes @cite @cite . In our study, we build upon progress in semantic segmentation by applying and evaluating progressive training.
- The contour tree @cite , a tree that contracts connected components of to points (formally defined in section:Background ), is closely related to the notion of merge tree, which contracts connected components of to points on simply connected domains. As shown by Tarasov and Vyali @cite and later generalized by @cite in arbitrary dimension, the contour tree can be efficiently computed by combining with a simple linear-time traversal the merge trees of the input function and of its opposite (called the join and split trees, see section:Background ). Due to this tight relation, merge and contour trees have often been investigated jointly in the literature.
- To compute the contour tree, two intermediate data-structures, the join and split trees, need to be into the global output contour tree @cite . Regarding this combination step, the existing parallel methods to contour tree computation use almost directly the reference sequential algorithm @cite . Some parallel attempts for this combination step have been described in @cite @cite @cite , but no experimental result concerning this step has been documented.
- Morozov and Weber @cite @cite and @cite presented three approaches for merge and contour tree-based visualization in a distributed environment, with minimal inter-node communications. However, these approaches focus more on the reduction of the communication between the processes than on the efficient computation on a single shared memory node as we do here with the target of an efficient interactive exploration in mind.
- Network alignment problem is an important research problem, which has been studied in various areas, e.g., protein-protein-interaction network alignment in bioinformatics @cite @cite @cite , chemical compound matching in chemistry @cite , data schemas matching data warehouse @cite , ontology alignment web semantics @cite , graph matching in combinatorial mathematics @cite , and figure matching and merging in computer vision @cite @cite . Network alignment is an important problem for bioinformatics. By studying the cross-species variations of biological networks, network alignment problem can be applied to predict conserved functional modules @cite and infer the functions of proteins @cite . Graemlin @cite conducts pairwise network alignment by maximizing an objective function based on a set of learned parameters. Some works have been done on aligning multiple network in bioinformatics. IsoRank proposed in @cite can align multiple networks greedily based on the pairwise node similarity scores calculated with spectral graph theory. IsoRankN @cite further extends IsoRank by exploiting a spectral clustering scheme.
- Similarity measure based on heterogeneous networks has been widely studied. Sun introduces the concept of in @cite , where a is a path consisting of a sequence of relations. However, the suffers from two disadvantages. On one hand, cannot describe rich semantics effectively. On the other hand, once numerious are defined, it's challenging to assemble them. Some methods to resolve these deficiencies are proposed later. @cite applys meta-graph to similarity measure problem, but entities are constrained to be of the same type. Zhao @cite proposes the concept of and extends the idea to recommendation problems which require that entities belong to different types. However, and are proposed for single non-attribute networks. In our definition, not only regular node types but also attribute types are involved, and it can be applied to the similarity measure across networks.
- Active learning is an effective method for network alignment in the face of lacking labeled links which has been previous studied by @cite @cite . The query strategies proposed by Cort ' e s and Serratosa @cite return a probability matrix for different alignment choices which makes the quantification of network alignment straightforward. However, this kind of strategies totally ignore the existing in online social networks. Therefore, we provide an innovative query strategy considering in . Malmi @cite proposes two relative-query strategies and instead of focusing on absolute-query. However, it may not be less challenging for experts to make comparative judgements in online social networks, because the quantity of cantidates corresponding to one node will be huge.
- Across the aligned networks, various application problems have been studied. Cross-site heterogeneous link prediction problems are studied by @cite by transferring links across partially aligned networks. Besides link prediction problems, Jin and proposes to partition multiple large-scale social networks simultaneously in @cite . The problem of information diffusion across partially aligned networks is studied by in @cite , where the traditional LT diffusion model is extended to the multiple heterogeneous information setting. give a comprehensive survey about the existing works on heterogeneous information networks in @cite , which includes a section talking about network information fusion works and related application problems in detail.
- Geo-replication has become a key feature in cloud storage systems, with data being replicated in multiple data centers spread around the world. The goal of geo-replication is to provide high availability and low latency, by allowing clients to access any nearby replica. To achieve these properties, a number of systems @cite @cite adopt a weak consistency model, where an update can execute in any replica, being propagated asynchronously to other replicas.
- Our work is closer to systems @cite @cite that provide support for both weak and strong consistency. For helping programmers decide which operation should execute under each consistency model, several tools have been proposed @cite @cite @cite @cite @cite . These tools, typically based on static analyses, impose an additional complexity to application development that is often non-trivial. In our approach, the programmer specifies the degree of concurrency allowed and which database constraints should be maintained -- the system enforces the specified concurrency while trying to minimize coordination. Some systems, such as Oracle multi-master replication, allow programmers to specify how to handle conflicting updates. Our approach is more complete, by addressing a wider range of database constraints, which are key for enforcing application correctness.
- AntidoteDB @cite is the backing store for AQL. AntidoteDB provides a key-object interface with support for CRDTs and escrow data types that we used to implement the SQL semantics. AntidoteDB ensures Parallel Snapshot Isolation. A number of systems provide equivalent semantics @cite . AQL parallel-snapshot isolation @cite with integrity invariants, in a similar way as snapshot isolation has been extended with integrity invariants @cite . Our approach for enforcing referential integrity cab be seen as a runtime version of our previous work, IPA @cite , where, following a static analysis process, application operations were modified in a way that guarantees that invariants are preserved when executed under weak consistency. In this work, we apply a similar idea in runtime to SQL code. Our approach can also be seen as an extension of the approach to enforce serializability under snapshot isolation proposed by Cahill et. al. @cite , be executing additional updates to force concurrency detection, and using conflict resolution policies to achieve the intended behavior.
- Multi-fidelity optimization has recently attracted considerable research interests. Techniques such as hierarchical partitioning @cite , hierarchical modeling @cite and ensemble methods @cite , are used to incorporate multiple fidelities cheap approximations of the BOF. Most relevant to this paper is the line of work on Bayesian optimization with multi-fidelity data such as the MF-GP-UCB method in @cite and the multi-fidelity BO (MFBO) algorithm in @cite . Research topics that are close to MFBO in concept include multi-information source optimization @cite @cite , multi-task BO @cite , multi-output GP @cite @cite , meta-learning based BO @cite @cite .
- Achieving exploration by deploying multiple policies has surfaced in the literature in varied contexts. The most closely related is @cite which follows the same framework but does not provide theory or consider an optimal diversity objective. @cite provides a DE algorithm and theory for exploration under a safety model but does not address exploration for policy gradient methods. @cite uses multiple agents with diverse estimates of the MDP to efficiently learn a model of the reward and transition function. @cite adds an explicit divergence regularizer to the policy gradient objective which encourages divergence from a heuristically chosen subset of previously deployed policies.
- Others have studied parameter space noise for exploration in gradient-based evolutionary strategies @cite @cite @cite , but they do not optimize diversity within policy performance constraints. @cite proposes exploration by adding a trainable noise parameter to each network parameter, which incurs significant computational cost.
- Different generalizations of the policy gradient with the common goal of variance reduction in gradient estimates @cite @cite exist but do not address the same exploration issue studied in this work. An alternate line of work reduces variance using control variates @cite .
- For EH relaying with a battery, several resource allocation methods are discussed in literature. An AF relaying network with TS energy harvesting is considered in @cite , where data relaying is realized when sufficient energy is collected through EH. An AF relaying network with PS energy harvesting is considered in @cite , where the remaining energy after data transmission is stored in the battery. The optimal resource allocation that maximizes the energy efficiency in a WSN with DF relaying is considered in @cite . A sum-throughput maximization problem is formulated for DF relay @cite , where the relay node opportunistically switch between modes of total EH and PS based information processing. Resource allocation schemes for EH nodes which harvest energy from renewable sources such as wind or solar are investigated in @cite @cite . All these work assume full CSI at the decision node. The outage performance is analyzed in @cite for a sub-optimal resource allocation scheme based on incremental DF relay protocol.
- Layer-wise optimization of NNs. Several authors have identified the limitations that backpropagation imposes on distributed algorithms. As a result, many approaches have been proposed including ADMM, block-coordinate descent @cite delayed gradients @cite , synthetic gradients @cite , proximal @cite , penalty based methods @cite @cite , and alternating minimization methods @cite . Our method (especially the local correction phase) is related to the synthetic gradient approach in @cite . The major differences between synthetic gradients of @cite and our approach are that we exploit the dynamical-systems view and a multilevel discretization scheme to approximate the co-state (adjoint) variables quickly. We also establish the convergence of our method with weaker conditions than in @cite . In Section we also show that our method outperforms even an improved version of the synthetic gradient method.
- Single image based SR methods downsample a given image to create a LR image and learn the mapping between the original and LR version. The learnt mapping is then applied to the original image to generate a SR image. In @cite HR and LR dictionaries are learned from MRI to generate SR images. These methods depend on learning the dictionaries on external LR-HR images and assume that the test image is a representative of the training data. Since this is not always the case the results are unsatisfactory. These approaches are computationally demanding as the candidate patches have to be searched in the training dataset to find the most suitable HR candidate. Instead, compact and generative models can be learned from the training data to define the mapping between LR and HR patches.
- Parametric generative models, such as coupled-dictionary learning based approaches, have been proposed to upscale MR brain @cite and cardiac @cite images. These methods benefit from sparsity constraint to express the link between LR and HR. Similarly, random forest based non-linear regressors have been proposed to predict HR patches from LR data and have been successfully applied on diffusion tensor images @cite . Recently, convolutional neural network (CNN) models @cite have been put forward to replace the inference step as they have enough capacity to perform complex nonlinear regression tasks. Even by using a shallow network composed of a few layers, @cite achieve superior results over other state-of-the art SR methods. Recent works have proposed image SR methods based on training data free approach using Fourier burst accumulation @cite , CNNs @cite and generative adversarial networks (GANs) @cite @cite .
- @cite @cite used framewise CE training to initialize the LSTM layers of phone-based CTC models. They found that using such pretrained parameters, CTC training is more stable than when using random initialization. @cite trained deep feed-forward sequential memory networks (Deep-FSMN) with CTC and proposed to incorporate CE loss as a regularization term. They argued that CE loss is helpful in stabilizing CTC training and improving the alignments of CTC models, which then lead to significant improvements in WER.
- The construction of conv layers is realized by multi-stage Saab transforms @cite . The Saab transform is a variant of the principal component analysis (PCA) with a constant bias vector to annihilate activation's nonlinearity. The Saab transform can reduce feature redundancy in the spectral domain, yet there still exists correlation among spatial dimensions of the same spectral component. This is especially true in low-frequency spectral components. Thus, a channel-wise PCA (C-PCA) was proposed in @cite to reduce spatial redundancy of Saab coefficients furthermore. Since the construction of conv layers is unsupervised, they can be fully adopted in an SSL system.
- The construction of FC layers is achieved by the cascade of multiple rectified linear least-squared regressors (LSRs) @cite . Let the input and output dimensions of a FC layer be @math and @math (with @math ), respectively. To construct an FC layer, we cluster input samples of dimension @math into @math clusters, and assign pseudo-labels based on clustering results. Next, all samples are transformed into a vector space of dimension @math via LSR, where the index of the output space dimension defines a pseudo-label. In this way, we obtain a supervised LSR building module to serve as one FC layer. It accommodates intra-class variability while enhancing discriminability gradually.
- Within the framework of statistical learning theory, several recent papers have significantly improved previous generalization bounds for deep networks by incorporating more refined attributes of the network structure, aiming to explain the paradoxical effect of improved performance while over-training the network. Using covering number techniques, @cite provide margin based bounds that relate generalization error to the network's Lipschitz constant and matrix norms of the weights. @cite establish similar matrix-norm-based margin bounds using a PAC-Bayes approach. @cite present compression-based results by compressing the weights of a well performing network and bounding the error of the compressed network. Finally, @cite are able, under certain (restrictive) assumptions on matrix norms, to achieve generalization bounds that are completely independent of the network size.
- The value of SSL has been subject to much debate. @cite provide a mathematical framework for the intuitive of @cite , and show that for unlabeled data to be beneficial, some clustering criterion is required (specifically, that the data consists of separated clusters with identical labels within each cluster). Based on a density level set approach, they prove fast rates of convergence in the SSL setting. @cite and @cite ) study SSL within a minimax framework, the former work shows that, under the so-called manifold assumption, optimal minimax rates of convergence may be achieved, while the second work demonstrates a separation between two classes of problems. When the structure of the data manifold is known, fast rates can be achieved, while without such knowledge, convergence cannot be guaranteed. More directly related to our work, and building on the clustering assumption, @cite identify situations in which semi-supervised learning can improve upon supervised learning. Unfortunately, their SSL bound suffers from the curse of dimensionality, and so depends exponentially on the dimension. Our work can be seen as allowing a trade-off between the clustering separation and the dimension, suggesting how to improve the bounds in @cite .
- @cite provide a principled approach to feature representation, and characterize the relation between the information retained by features about the input, and the loss incurred by a classifier based on these features. They suggest the application of their results to SSL, but do not provide explicit conditions or generalization bounds in this setting. Recently, @cite provide such bounds for semi-supervised learning with linear AEs and a joint reconstruction classification loss, using uniform stability arguments. They also provide empirical results that show that nonlinear AEs can indeed contribute to supervised learning. While their results are close in spirit to ours, we are more concerned with understanding how the structure of the data affects SSL, and, in particular, characterizing when, and to what extent, clustering of the input contributes to performance through unsupervised learning. Moreover, we rely on bounds that are specific to neural networks, rather than on the looser stability based bounds.
- Our problem can be viewed as a challenging extension of two complex multi-armed bandit problems. First, relaxing the need for decentralization, i.e. when a central controller is requested to jointly select @math , the problem coincides with a combinatorial bandit problem with semi-bandit feedback, as explained in , where we review the achievable regret in the centralized setting. Second, the particular case of a common utility for all players, i.e. @math for all @math , has been studied extensively, and we review in the achievable regret in this multi-player bandit problem, presenting in particular some inspiring ideas to design algorithms for our more general problem. Finally in we discuss the Game-of-Thrones algorithm of @cite , which is our only competitor in the fully distributed setting considered in this paper.
- In the centralized setting of our problem, a central controller selects at each time step a matching that maps the @math players to different arms in @math . In other words, introducing @math with means @math for @math and @math , the central controller selects a subset of @math elementary arms whose indices form a matching from @math to @math . Then, the utility of each elementary arm is observed and the reward is the sum of those. This is a particular combinatorial bandit problem with semi-bandit feedback, first studied by @cite .
- Various semi-distributed settings of our problem in which some kind of communication is allowed between players have been studied by @cite @cite @cite . In particular, @cite present an algorithm with expected regret that is @math . Our algorithms are the first to reach a logarithmic regret in the fully-distributed setting. One can note however, that compared to the best centralized algorithm, the dependency in @math , @math and @math obtained in Theorems and are a bit worse, suggesting a price for decentralization.
- The only previous paper on the setting we study here is by @cite , who use a very different approach from ours. The Game-of-Thrones algorithm proceeds in epochs, and each epoch is divided into three phases. The first is an exploration phase in which each player select arms at random in order to estimate the mean of each arm. In the second phase (called the GOT phase) the players jointly run a Markov chain (which is called the games of thrones dynamics) whose unique stochastically stable state corresponds to a maximum matching of the empirical means, on which the Markov chain will spend most of its time. The players then enter an exploitation phase in which they keep playing the arm they visited most often during the GOT phase.
- The Game-of-Thrones algorithm is proved to have an expected regret of order @math for any given constant @math , where the constant in the @math depends on @math and on the problem parameters ( @math , @math and @math ) in a way that is not very explicit in Theorem 3 of @cite . Unlike our algorithms, GOT does not need to know the horizon @math . However, (a lower bound on) the value of @math is needed to calibrate the length of the exploration phase. Moreover, the analysis of the Game-of-Thrones algorithm works for small enough @math ,' where @math is a parameter of the GOT phase, but it is not clear how the players must choose @math to achieve the guaranteed regret.
- Before we study the case when @math is a Gabor frame, we include here a short overwiew of the results on the singular values for random frames with independent entries, such as Gaussian matrices. The largest singular value of the analysis matrix @math of a random frame with independent entries can be estimated using Latalas theorem @cite . The following result implies that, with high probability, @math .
- The following optimal estimate of the smallest singular value of the analysis matrix for a random subgaussian frame with independent entries is due to Rudelson and Vershynin @cite .
- To the best of our knowledge, singular values of the analysis matrix @math of a Gabor frame with random window and general @math with @math were not studied before. At the same time, the following bounds on the singular values of the synthesis matrix @math of a Gabor system @math with a Steinhaus window @math and @math have been established in @cite .
- The ability of neural networks to efficiently represent local context features sometimes allows them to make surprisingly good independent decisions for each structured output variable @cite . However, these independent classifiers are often insufficient for structured prediction tasks where there are strong dependencies between the output labels @cite @cite . A natural solution is to use these neural feature representations to parameterize the factors of a conditional random field @cite for joint inference over output variables @cite @cite @cite . However, most previous work restricts the linear-chain CRF states to be the labels themselves---learning no additional output structure.
- The latent dynamic conditional random field (LDCRF) learns additional output structure beyond the labels by employing hidden states (latent variables) with Markov dependencies, each associated with a label; it has been applied to human gesture recognition @cite . The dynamic conditional random field (DCRF) learns a factorized representation of each state @cite . The hidden-state conditional random field (HCRF) also employs a Markov sequence of latent variables, but the latent variables are used to predict a single label rather than a sequence of labels; it has been applied to phoneme recognition @cite and gesture recognition @cite . All these models learn output representations while preserving the ability to perform exact joint inference by belief propagation. While the above use a log-linear parameterization of the potentials over latent variables, the input-output HMM uses a separate neural network for each source state to produce transition probabilities to its destination states.
- Experiments in all of the above parameterizations use only a small hidden state space due to the large numbers of parameters required. In this paper we enable a large number of states by using a low-rank factorization of the transition potentials between latent states, effectively learning distributed embeddings for the states. This is superficially similar to the label embedding model of @cite , but that work learns embeddings only to model similarity between observable output labels, and does not learn a latent output state structure.
- Few approaches were proposed to perform segmentation of the glomeruli. @cite introduced a separate detection and segmentation stage, both based on training a supervised classification model, specifically a linear support vector machine. For detection and segmentation different versions of histogram of oriented gradient descriptors were applied. Detection was performed by means of the sliding window approach. For segmentation, a polygon-fitting technique was utilized to determine the precise outline of the objects by adding a circularity constraint. For learning to distinguish between border region and non-border regions (and not between inside and outside of a glomerulus), this method requires highly precise annotations for training. This method was developed for a specific staining (desmin) which highlights cells in glomeruli.
- To circumvent the need for large annotated training data, @cite proposed an unsupervised segmentation algorithm. The authors aim at segmenting the Bowman's capsule surrounding the glomerulus by applying a color-based segmentation approach. The segmentation method consists of k-means clustering after thresholding including morphological operations. The inherent problem here is, that the Bowman's capsule is not always visible, depending on the staining protocol as well as the cutting plane (Fig. , ). Therefore, this method is not capable of generating a reliable precise segmentation of all objects.
- All further approaches do not focus on a segmentation but on related tasks such as tissue classification or object detection: In the work of @cite the need for large annotated training data was circumvented by a weakly-supervised approach. It was shown that even with a small number of positive training samples, a detection of the glomeruli can be performed. However, a segmentation stage is missing in this work. In a study of @cite various image representations were investigated to determine whether they are appropriate for distinguishing between different regions in renal pathology. The authors found that colour as well as texture representations can be utilized for distinguishing between the investigated texture classes. In a further study domain adaptation methods were investigated for patch-wise classification of renal tissue. It was shown that domain adaptation can help to improve the fit of trained models for image data showing changed properties. However, the performance of feature-based patch-wise classification was not sufficient for a reliable classification or segmentation applying a difficult-to-segment staining.
- @cite applied a CNN for distinguishing between glomerulus and non-glomerulus texture based on previously extracted patches (similar to @cite @cite ). Although this application is distinct from a segmentation, it provides evidence for the principal effectiveness of CNNs for distinguishing between glomerulus and non-glomerulus tissue. Neural networks were also applied by @cite with focus on a regression on WSI level. Specifically, the authors focussed on directly predicting the kidney function based on biopsies from patients suffering from chronic kidney disease without previously segmenting the glomeruli.
- Since the work of @cite , cascades became highly popular, specifically in the field of face detection. Cascades were also combined with state-of-the-art CNN architectures and were not only applied to face detection but also for segmentation applications . @cite proposed a method to improve facial part segmentation performance by first detecting facial landmarks. @cite introduced a cascade for combining segmentation and classification of thyroid nodules in ultrasound images. @cite proposed a CNN cascade including feature sharing to separately address detection, segmentation and classification. In contrast to these multi-task approaches , here the focus is on segmentation only.
- is a @math D convolution of an input volume with a group of @math D kernels that result in extracted features from the input. Each @math D kernel is associated with a bias value. Number channels is the z-axis of input volume. Input volume is called maps and a slice of it is a map. A window is the size of one kernel convolved with part of input volume. Operations in a compute window are independent, hence it is well suited to multi-core processors: GPUs @cite and other designs using ASIC @cite @cite and FPGAs @cite @cite .
- or bypass is used in ResNet models @cite . The output values of a CONV are element-wise added with a previous layer's input. In hardware, we want to add those bypass values as output results are being produced by a CONV layer to save communication cost. Thus we need to keep track of previous input layers and to conditionally issue an extra instruction.
- Memory transfer friendly computation tiling for CNN accelerators was explored in @cite @cite . In @cite , block tiling with x-y axis ordering was used. They store tiles with extra overlap regions, called augmented-tiles, in DRAM to avoid multiple DMA transactions. This work also stores overlapped regions but it tiles at the granularity of row strips with channel major ordering to lower overlapped data replication. This lowers the required memory bandwidth.
- Other domain-specific instructions set for CNN were presented in @cite , can be added into Snowflake's compiler, because they also use vector compute instructions and scratchpad on-chip memory loads instructions. The intermediate representation and the techniques presented can be also be integrated into conventional frameworks @cite , which is left for future work.
- Crowd counting has been tackled in computer vision by a myriad of techniques. Crowd counting via head detections has been tackled by @cite @cite @cite using motion cues and appearance features to train detectors. Recurrent network framework has been used for head detections in crowd scenes by @cite . They use the deep features from Googlenet @cite in an LSTM framework to regress bounding boxes for heads in a crowd scene. However, crowd counting using head detections has limitations as it fails in dense crowds, which are characterized by high inter-occlusion between people.
- In crowd counting from videos, @cite use image features like Tomasi-Kanade features into a motion clustering framework. Video is processed by @cite into a set of trajectories using a KLT tracker. To prevent fragmentation of trajectories, they condition the signal temporally and spatially. Such tracking methods are unlikely to work for single image crowd counting due to lack of temporal information.
- Multi-column CNN used by @cite @cite perform late fusion of features from different CNN columns to regress the density map for a crowd scene. In @cite , shallow CNN columns with varied receptive fields are used to capture the large variation in scale and perspective in crowd scenes. Transfer learning is employed by @cite using a VGG network employing dilated layers complemented by a shallow network with different receptive field and field of view. Both the model fuse the feature maps from the CNN columns by weighted averaging via a 1 @math 1 convolutional layer to predict the density map of the crowd. However, the weighted averaging technique is global in nature and does not take in to account the intra-scene density variation. We build on the performance of multi-column CNN and incorporate a patch based switching architecture in our proposed architecture, Switch-CNN to exploit local crowd density variation within a scene (see Sec for more details of architecture).
- While switching architectures have not been used for counting, expert classifiers have been used by @cite to improve single object image classification across depiction styles using a deep switching mechanism based on depiction style. However unlike @cite , we do not have labels (For eg: Depiction styles like "art" and "photo") to train the switch classifier. To overcome this challenge, we propose a training regime that exploits CNN regressor's architectural differences (See Section )
- The pioneering work of Roberts @cite and Marr @cite exemplify early shape models composed of 3D geometric primitives with recognizable 2D properties. Similarly intuitive properties are still in common use, such as symmetric parts @cite , skeletons @cite and CAD wire-frames @cite .
- Recently, generating distributions based upon the parameters of a complex function learned from data have been utilized to represent shapes without the need for any prior structural knowledge @cite . Recent examples include: GP-LVMs @cite , Convolutional Neural Networks @cite , Recurrent Neural Networks @cite , Deep Belief Networks @cite , Generative Adversarial Networks @cite @cite and Deep Convolutional Auto-encoders @cite . The work that we extend upon in this paper, 3DGAN @cite , falls in this category. Several of these shape models have been shown in practical applications such as interactive editing @cite and reconstructing room geometry @cite @cite @cite and depth @cite from a single image can which act as import cues for understanding scenes at a higher level @cite @cite .
- As one of the most popular tool for the analysis of homologous protein and nucleotide sequences, HMMER attracts many acceleration attempts. The previous version, HMMER2, is based on Viterbi algorithm that has proved to be the computational bottleneck. The initial effort of GPU-based implementation for HMMER2 is ClawHMMER @cite which introduces a streaming version of Viterbi algorithm for GPUs. They also demonstrate the implementation running on a 16-node GPU cluster, each equipped with a Radeon 9800 Pro GPU. Another early GPU-based implementation is proposed by Walters @cite who properly fit the Viterbi algorithm into the CUDA-enabled GPU with several optimizations, like memory coalescing, proper kernel occupancy and shared constant memory usage, which outperforms the ClawHMMER substantially. Yao @cite present a CPU-GPU cooperative pattern to accelerate HMMER2. Ganesan @cite re-design the aligment process of a single sequence across multiple threads to partially break the sequential dependency in computation of Viterbi scores. This helps building a hybrid task and data-level parallelism that eliminates the overhead due to unbalanced sequence lengths.
- However, with the heuristic pipeline, HMMER3 achieves about @math x to @math x speedups over its predecessor @cite , which hence renders any acceleration effort of HMMER2 obsolete. There are only few existing work that aim to accelerate SSV, MSV and P7Viterbi stages of pipeline in HMMER3. Abbas @cite re-writes mathematical formulas of MSV and Viterbi algorithms to expose reduction and prefix scan computation patterns which are fitted into the FPGA architecture. @cite , a speculative method is proposed to reduce the number of global memory access on the GPU, which aims to accelerate the MSV stage. Lin @cite @cite also focus on MSV stage but incorporate SIMD video instructions provied by the CUDA-enabled GPU into their method. Like the strategy of @cite , they assign each thread to handle a whole sequence. A CPU-based implementation of P7Viterb stage is done by Ferreira @cite who propose a cache-oblivious parallel SIMD Viterbi algorithm that offsets cache miss penalties of original HMMER3 work. Neto @cite accelerate the SSV stage via a set of optimizations on the GPU, such as model tiling, outer loop unrolling, coalesced and vectorized memory access.
- @cite and its variants are the most popular approximations. In general, this approach organizes the output vocabulary into a tree where the leaves are words and intermediate nodes are latent variables, or classes. The tree structure could have many levels and there is a unique path from root to each word. The probability of a word is the product of probabilities of each node along its path. In practice, we could use a tree with two layers, where we want to organize words into simple clusters. In this case, the computational complexity reduces from @math to @math . If we use a deeper structure like the Huffman Tree, the computational complexity could be reduced to @math . In general, the hierarchical structure is built on frequency binning @cite @cite or word similarities @cite @cite @cite . In this paper, we propose another word-similarity-based hierarchical structure. But, instead of performing k-means over pre-learned word embeddings, we propose a new approach that learns hierarchical structure based on the model's historical prediction during the language model learning process.
- @cite is inspired by the idea that we could use convolution network to produce word embedding from a character level model. Aside from a big reduction in number of parameters and incorporating morphological knowledge from words, this method can also easily deal with out-of-vocabulary words, and allows parallel training over corpora that have different vocabulary size. But this method does not decrease the computational complexity compared to the standard full softmax @cite .
- Sampling based approaches approximate the normalization in the denominator of the softmax with some other loss that is cheap to compute. However, sampling based approaches are only useful at training time. During inference, the full softmax still needs to be computed to obtain a normalized probability. These approaches have been successfully applied to language modeling @cite , machine translation @cite , and computer vision @cite .
- Noise Contrastive Estimation (NCE) is proposed in @cite @cite as a more stable sampling method than IS. NCE does not try to estimate the probability of a word directly. Instead, it uses an auxiliary loss that works to distinguish the original distribution from a noisy one. @cite showed that good performance can be achieved even without computing the softmax normalization.
- It was first studied by @cite . In term of different visual features and distinctive learning paradigm, the facial attribute analysis has been developed into three categories: (1) the methods @cite of using hand-crafted visual features, such as SIFT @cite and LBP @cite ; (2) the methods of utilizing the recent deep features @cite @cite @cite ; and (3) multi-task methods of learning facial attribute @cite @cite . We here highlight the differences between our architecture and these previous works. Liu @cite cascaded three deep networks pre-trained for facial attribute prediction. In contrast, we show that the tasks of face detection and facial attribute prediction are highly correlated and our jointly learning architecture can improve both tasks. Rudd @cite introduced a mixed objective optimization network which utilizes distribution of attribute labels to learn each task. Abdulnabi @cite proposed a multi-task CNN model sharing of visual knowledge between tasks for facial attribute analysis. Comparing with @cite @cite , we focus on jointly learning the face detection and facial attribute analysis; and our model can predict facial attributes on the images in the wild @cite @cite without cropped and aligned, as illustrated in Fig. and Fig. .
- Our framework can be categorized as multi-task learning @cite @cite , which shares the information and explores the similarity of related tasks on the same data. The multi-task learning can facilitate a wide range of tasks and applications, including but not limited to action recognition @cite , information retrieval @cite , facial landmark detection @cite @cite , and facial attribute prediction @cite @cite @cite @cite @cite . The recent technical report @cite also combine face detection with the tasks of locating face landmarks and recognizing gender. However, unlike our facial attribute prediction on the images in the wild, their work still has to firstly detect face regions for the facial landmarks and predicting gender.
- There is a broad range of methods designed to perform WSD @cite @cite @cite . The most accurate techniques are supervised @cite , but they require annotated training data which is not always available. In order to overcome this limitation, some researchers have proposed various unsupervised or knowledgde-based WSD methods @cite @cite @cite @cite @cite @cite @cite @cite @cite . Since our approach is unsupervised and based on WordNet @cite @cite , our focus is to present related work in the same direction. extend the gloss overlap algorithm of by using WordNet relations. proposed a brute-force algorithm for global WSD by employing the extended Lesk measure @cite @cite to compute the semantic relatedness among senses in a given text. However, their BF approach is not suitable for whole text documents, because of the high computational time. More recently, have proposed and compared three stochastic algorithms for global WSD as alternatives to BF search, namely a Genetic Algorithm, Simulated Annealing, and Ant Colony Optimization. Among these, the authors @cite @cite have found that the Ant Colony Algorithm yields better results than the other two.
- The relation between graphs and deep neural networks have been previously explored, but most contributions do so from a different perspective. While our proposal is to obtain a graph representation of the embedding produced by a CNN when processing an image, most related work focuses on training DNNs for processing graph data. For example, DeepWalk @cite uses random walks in a graph ( Flickr or YouTube networks) to feed a SkipGram model, and then evaluate community detection methods on those graphs. Similarly to DeepWalk, the work of also processes graphs as input, but it uses a probabilistic approach on weighted graphs to feed an autoencoder. In contrast, we are performing community detection on a dataset of images, something that, to the best of our knowledge, had not been done before.
- The Stable Marriage problem is based on a very general notion of stability which corresponds to allocations where no individuals can discern any gain from further trade. It became central in cooperative game theory and found applications in a variety of important real-life scenarios @cite @cite . As a recognition of this large impact, the 2012 Nobel Prize in Economics was awarded to Lloyd S. Shapley and Alvin E. Roth .
- In the domain of education, a large number of U.S. metropolitan areas use some variant of the Gale-Shapley algorithm for high school admission. It all started in 2004, when the New York department of education introduced an applicant-proposing version of the Gale and Shapley algorithm for high school admission. In these systems, total orders are used to express preferences @cite . Remark that they do not use randomness since institutions are legally entailed to perform selection.
- In Artificial Intelligence, several researchers considered the stable marriage problem. In @cite , the authors introduce a local search approach which exploits properties of the problems to reduce the size of a state neighborhood. In @cite , male optimality and uniqueness of stable marriages for partially ordered preferences are studied.
- In the domain of Constraint Programming, @cite explores stability in different problems variations including cases where men women can rank members of their own genders, etc. A constraint encoding of the SM problem with ties and incomplete lists is presented and empirically evaluated in @cite .
- In the context of quantum physics, many tailored basis set have been designed to reduce the number of DOFs to represent spectral projectors (or density matrices in physics terminology). Notable examples include the Gaussian basis set and the atomic orbital basis set @cite @cite @cite . Such basis sets are developed based on physical intuition, and can provide relatively accurate solution with much reduced number of degrees of freedom compared to more conventional basis sets. However, expert knowledge is often required to systematically converge the solution. These basis sets have also been used to enrich'' conventional basis sets to achieve a balance between the small number of DOFs and the systematic convergence property @cite @cite . However, the number of DOFs in the mixed basis representation is often much larger than those using Gaussian orbital or atomic orbital basis sets alone.
- A lot of recent work has focused on constructing large datasets suitable for training neural models. QA datasets have been assembled based on Freebase @cite @cite , Wikipedia articles @cite @cite @cite and web search user queries @cite ; for reading comprehension (RC) based on news @cite @cite , children books @cite and novels @cite , and for recognizing textual entailment based on image captions @cite . We continue this line of work and construct a dataset for science exam QA. Our dataset differs from some of the aforementioned datasets in that it consists of natural language questions produced by people, instead of cloze-style questions. It also differs from prior work in that we aim at the narrower domain of science exams and in that we produce multiple choice questions, which are more difficult to generate.
- Existing models for multiple-choice science exam QA vary in their reasoning framework and training methodology. A set of sub-problems and solution strategies are outlined in . The method described by evaluates the coherence of a scene constructed from the question enriched with background KB information, while train an entailment model that derives the correct answer from background knowledge aligned with a max-margin ranker. Probabilistic reasoning approaches include Markov logic networks @cite and an integer linear program-based model that assembles proof chains over structured knowledge @cite . The Aristo ensemble @cite combines multiple reasoning strategies with shallow statistical methods based on lexical co-occurrence and IR, which by themselves provide surprisingly strong baselines. There has not been much work applying neural networks to this task, likely because of the paucity of training data; this paper is an attempt to address this issue by constructing a much larger dataset than was previously available, and we present results of experiments using state-of-the-art reading comprehension techniques on our datasets.
- Several similarity measures have been employed for selecting answer distractors @cite , including measures derived from WordNet @cite , thesauri @cite and distributional context @cite @cite . Domain-specific ontologies @cite , phonetic or morphological similarity @cite @cite , probability scores for the question context @cite and context-sensitive lexical inference @cite have also been used. In contrast to the aforementioned similarity-based selection strategies, our method uses a feature-based ranker to learn plausible distractors from original questions. Several of the above heuristics are used as features in this ranking model. Feature-based distractor generation models @cite have been used in the past by for creating biology questions. Our model uses a random forest to rank candidates; it is agnostic towards taking cloze or humanly-generated questions, and it is learned specifically to generate distractors that resemble those in real science exam questions.
- The ranking problem is well studied in IR community. There are many methods been proposed, including pointwise @cite , pairwise @cite @cite and listwise @cite @cite algorithms. Experiment results show that listwise methods deliver better performance than pointwise and pairwise methods in general @cite .
- In order to bring the advantages of Semantic Web to the life science community, a number of biomedical KGs have been constructed over the last years, such as Bio2RDF @cite and Chem2Bio2RDF @cite . These datasets make the interconnection and exploration of different biomedical data sources possible. However, there is little patients clinical information within these biomedical KGs. STRIDE2RDF @cite and MCLSS2RDF @cite apply Linked Data Principles to represent patients electronic health records, but the interlinks from clinical data to existing biomedical KGs are still very limited. Hence, none of the existing linked datasets are bridging the gap between clinical and biomedical data.
- In recent years the research interest to online social media have significantly increased, and different studies have explored the Web forums from the point of natural language processing tasks like speech acts classifying @cite . However, we are not aware of any research in the task of post relevance detection, especially from the perspective of semantic relatedness detection of complex linguistic units (like sentences and texts) for the Russian language. But the detection of semantic relatedness itself has a broad amount of resolutions proposed by other researchers; the extensive survey of them is presented at the official web-page of ( https: nlp.stanford.edu projects snli ). For English most of the research of semantic relatedness similarity were proposed as the part of (for example, for the task of textual semantic similarity detection @cite ); there were also some studies in the task of word similarity for Russian language as a part of @cite . In this study we will also use datasets proposed on RuSSE to evaluate the word representations obtained with different WEM.
- While there have been many previous efforts applied to automated heart sound analysis, gauging the success of historical approaches has been somewhat difficult, due to differences in dataset quality, number of recordings available for training and testing algorithms, recorded signal lengths and the environment in which data was collected (e.g. clinical vs. non-clinical settings). Moreover, some existing works have not performed appropriate train-test data splits and have reported results on training or validation data, which is highly likely to produce optimistic results due to overfitting @cite . In this work, we report results from the 2016 PhysioNet Computing in Cardiology Challenge, which evaluated entries on a large test-set that was not made publicly available. To reduce overfitting, no recordings from the same subject were included in both the training and the test set and a variety of both and PCG recordings, which exhibited very poor signal quality, were included to encourage the development of accurate and robust algorithms.
- The work presented in this paper, is one of the first attempts at applying to the task of heart sound data analysis. However, there have been recent efforts to apply deep learning approaches to other types of physiological time series analysis tasks. An early work that applied deep learning to the domain of psychophysiology is described in @cite . They advocate the use of for recognizing affect from physiological inputs such as and within a game-based user study. The authors argue against the use of manual ad-hoc feature extraction and selection in affective modeling, as this limits the creativity of attribute design to the researcher. One difference between the work of @cite and ours is that they perform an initial unsupervised pre-training step using stacked convolutional denoising auto-encoders, whereas our network does not require this step and is instead trained in a supervised fashion .
- Similar deep learning efforts that process physiologic time series have also been applied to the problems of epileptic seizure prediction @cite and human activity recognition @cite .
- Wearable systems for the automatic recognition of human gestures and full-body movements typically rely on inertial information. Accelerometers prove to be the most informative sensor for the task @cite . To comply with end users' constraints related to the impact of the monitoring system on their appearance and freedom of motion, most solutions adopt a single sensing device, either located at the waist @cite or, as it is becoming more and more common, at the wrist @cite .
- Joint image and language modeling enables the generation of semantic descriptions, which provides more intelligible predictions. Image captioning is one typical of application @cite . Recent methods use recurrent neural networks (RNNs) to model natural language conditioned on image information modeled by CNNs @cite @cite @cite @cite . They typically employ pre-trained powerful CNN models, such as GoogLeNet @cite , to provide image features. Semantic image features play a key role in accurate captioning @cite @cite . Many methods focus on learning better alignment from natural language words to provided visual features, such as attention mechanisms @cite @cite @cite , multimodal RNN @cite @cite @cite and so on @cite @cite . However, in the medical image domain, pre-trained universal CNN models are not available. A complete end-to-end trainable model for joint image-sentence modeling is an attractive open question, and it can facilitate multimodal knowledge sharing between the image and language models.
- Image-sentence alignment also encourages visual explanations for network inner workings @cite . Hence, attention mechanisms become particularly necessary @cite . We witness growing interests of its exploration to achieve the network interpretability @cite @cite . The full power of this field has vast potentials to renovate computer-aided medical diagnosis, but a dearth of related work exists. To date, @cite and @cite deal with the problem of generating disease keywords for radiology images.
- Based on the residual network (ResNet) @cite , the new pre-act-ResNet @cite introduces identity mapping skip-connection @cite to address the network training difficulty. Identity mapping gradually becomes an acknowledged strategy to overcome the barrier of training very deep networks @cite @cite @cite @cite . Besides, skip-connection encourages the integration of multi-scale representations for more efficient feature utilization @cite @cite @cite .
- As ASP has been applied to more and more problems, the importance of ASP software development tools has been realized by the community. Some integrated development environment (IDE) tools, e.g., APE @cite , ASPIDE @cite , iGROM @cite and SeaLion @cite have previously been developed. They provide a graphical user interface for users to carry out a sequence of tasks from editing an ASP program to debugging that program, easing the use of ASP significantly. However, the target audience of these tools is experienced software developers. Compared with the existing environments, our environment is online, self contained (i.e., fully independent of the users' local computers) and provides a very simple interface, focusing on teaching only. The interface is operable by any person who is able to use a typical web site and traverse a local file system.
- However, the animate predicate is different from theirs (?? Can someone make the difference very specific in the design section?) and we feel that our design is more natural in modeling animation tasks ( future work is to study these two designs -- goes to future work part). There are a few other major differences between our work and theirs. We use SPARC (e.g., sorts etc make it easier more effective to write correct programs compared with Clingo @cite or DLV cite...). Next, ASPviz requires two programs one for drawing animation while the other is for information needed by the drawing animation. We don't require them to be separate program although we encourage the methodology of organize one program into two parts. Finally, we provide an online environment instead of a standalone application with the advantage: very expensive to install and manage a standalone program in a teaching environment for genera students particularly younger students (e.g., high school ones) @cite ). Our online version allows us to HTML canvas will significantly reduce the development efforts.
- Development of image perturbation algorithms for fooling deep neural networks can be very useful for generating more effective training samples and for finding flaws in trained models @cite . The algorithms can be divided according to the following criteria: [leftmargin=5ex, nolistsep] : One of the most popular works demonstrating the weaknesses of Deep Convolutional Neural Networks (DCNN) is @cite . The authors use Genetic Algorithms to generate images that have no semantic meaning to human observers, but which can make deep CNN classifiers predict classes with high confidence. However since this model generates semantically incoherent images, they are easily identifiable by humans as adversarial. The next class of algorithms transform images by adding visually imperceptible residuals such as in @cite , @cite , @cite , @cite , @cite . The algorithms generate tampered images that match the input image visually, but still fool the classifier. Similar to the later algorithms, the proposed methods in this paper also generate perturbed images that are visually similar to the input images.
- : In @cite , @cite , @cite , @cite etc, the adversarial algorithms need knowledge of the internals of the victim model (white-box), as opposed to @cite , @cite , @cite , @cite , and @cite which are black-box attack models. The proposed methods are able to mount a black-box attack.
- The task of style transfer has been studied in computer vision and graphics communities for a long time, under the name of texture transfer''. Some early works synthesize new images by sampling pixels or image patches according to certain similarity metrics @cite @cite @cite @cite . Another line of works first extract a set of feature maps from an image by applying manually designed filters, and then transform the feature maps of a synthesized image to match that of a style image @cite @cite .
- An important inspiration for the present work is Poisson Image Editing @cite , which aims to transfer a source region in an image to a target region in another image seamlessly. Poisson Image Editing restrains the target regions to have the same Laplacian as the source region, and fixes the boundary pixels to be the original pixels around the target region. As the Laplacian constraint has a big set of solutions, given the additional boundary constraint, a solution still exists. This solution is a transferred region fitted in the background in the target image with smooth transitions.
- @cite and @cite incorporate gradient terms into the patch similarity metric for matching structurally coherent patches and blending a region into another image with higher consistency. Recently, @cite propose to complete missing regions in an image with a Laplacian pyramid, i.e. stacked Laplacian filters in decreasing granularities. This method selects patches that are structurally coherent by exploiting the property that image Laplacians preserve the detail structures.
- @cite propose to use a convolutional neural network (CNN) to extract features for style transfer, namely the Neural Style Transfer method. Different layers of CNNs capture information at different levels of abstraction. Constraints with respect to different levels of features are integrated in a single CNN as different loss functions. In particular, the style to be transferred is captured as Gram correlation matrices of the style image features. In this framework, image synthesis reduces to the optimization within the CNN. This approach elegantly addresses the style transfer problem with high-quality output images. @cite make two extensions to this method: 1) better control the spatial consistency by using a guided Gram loss, and 2) better preserve the color distribution of the content image, by incorporating an extra luminance channel or matching colour histograms.
- @cite propose a CNN for transforming images, named the transformation network. It is trained with the loss function as in @cite , but the parameters to be trained are the CNN weights, instead of an input image. For each style image, a dedicated transformation network is trained. To get a stylized output image, one only needs to feed the content image into this CNN and take a forward pass, which is much faster than @cite . @cite apply the traditional idea of patch-matching to CNN features. They propose a Markov Random Field loss function that drives the CNN feature patches in the synthesized image to approximate their best matches in the style image. This method, namely MRF-CNN, is extended by Champandard @cite , in which user-provided segmentation information is incorporated to improve the local consistency of stylized images.
- More recently, @cite propose a method that can achieve photorealistic style transfer, as a post-processing step of Neural Style Transfer. They first extract locally affine functions using Matting Laplacian This Laplacian' refers to the graph laplacian, which is irrelevant to the image Laplacian used in this work. of @cite , then fine-tune the stylized image to fit these affine functions. In the examples they presented, the stylized images are photorealistic and well retain the detail structures of the content image. On our test images (Section ), this method produces images that are much more consistent to the content images, but their stylishness is greatly reduced.
- Registering, stitching, and aligning images that represent contiguous samples along some axis in space or time is a computer vision problem that has found applications across photography, microscopy, astronomy, and many other areas @cite . Classical approaches make heavy use of image-derived correspondences computed by local correlation measurements or robust interest point detectors @cite , which are used to estimate how overlapping or nearly overlapping images geometrically relate to one another. These estimates can then be used to, for example, define an optimization problem in which images are elastically deformed to maximize pixel-wise consistency @cite . Such approaches have proven highly effective for producing coherent 3d volumes from thousands or millions of individual 2d images that may be subject to various types of deformation, artifacts, and missing data.
- Heinrich and colleagues recently introduced neural network architectures that produce impressive results on the task of generating isotropic 3d volumes from anisotropic source data @cite , which is closely related to the experiments we pursue in Section 5. The novel training methods introduce in this work are likely to be complementary to general advances in neural network architectures for 3d image synthesis.
- During the preparation of this manuscript, we became aware of independent simultaneous work done by Jeong and G "u nt "u rk. They also studied the randomized Kaczmarz method adapted to phase retrieval, and obtained almost the same result that we did (see @cite and Theorem 1.1 therein). In order to prove their guarantee, they use a stopping time argument similar to ours, but replace the ACW condition with a stronger condition called . They prove that measurement systems comprising vectors drawn independently and uniformly from the sphere satisfy this property with high probability, and the main tools they use in their proof are hyperplane tessellations and a net argument together with Lipschitz relaxation of indicator functions.
- The term was first coined by @cite to describe the wiring diagram of the anatomical connectivity of the human brain. The interconnectivity found between parcellated brain regions as described above is considered the macroscale connectome. Meso- and micro- scale connectomes are at the local neuronal circuits and single neuronal cells levels respectively, and they currently require an invasive high resolution scans of dissected brains using microtome machinery. Friston defined functional connectivity as the temporal correlation between spatially remote neurophysiological events @cite . This paper focuses on visualization tasks relevant to macroscale functional and structural connectomes, derived from functional and diffusion-weighted MRI respectively.
- The connectogram @cite , a recent connectome visualization technique, is a node-link diagram in a circular layout (see fig:case2 ). Names of brain regions are positioned along the perimeter of a circle, which is split into two halves according to their hemispheric affiliation. Each hemisphere is further divided into the different brain lobes. The inner sphere contains multiple colored rings, each representing a specific metric. The regions are interconnected within the circle using curved lines. This technique prevents some of the clutter that is found in other network visualizations. However, it is harder to correlate anatomical structures with connectivity using a circular diagram like the connectogram, as the multiple rings may make it difficult for a user to make sense of the data @cite . tab:table1 contains a list of different visualization tools in the field of connectomics and their capabilities. Although previous efforts have surveyed visualization in connectomics, such as @cite and @cite , our survey is the first to comprehensively explore recent connectomics visualization software in terms of their ability to support group studies analysis tasks. Moreover, we catalog these software tools by the primary connectome dataset types they support.
- Extending the user task definitions presented in @cite through a thorough investigation of connectome visualization projects and surveying neuroscientists that work with group studies, we have identified visual analytics tasks for clinical neuroscientists:
- While most commonly used visualization tools are dedicated desktop applications, web-based implementations, such as @cite or @cite , free the user from being attached to a specific operating system @cite . is also a web-based application and runs in any modern browser. The use of stereoscopic techniques can provide a more immersive way to explore brain imaging data @cite , H "a find that healthcare professionals perceive the increased dimensionality provided by stereoscopy as beneficial for understanding depth in the displayed scenery @cite . Moreover, Ware and Mitchell find that the use of stereographic visualizations reduces the perception error rate in graph perception for large graphs with more than 500 nodes @cite , and @cite observed that, when coupled with highlighting, stereoscopic representations of 3D graphs outperformed their non-immersive counterpart. lets the user move between desktop and VR environments for interactively exploring 3D connectomes.
- There is a large body of research that focuses on recognizing and tracking human motion. The latest developments in deep features and convolutional neural network architectures achieve impressive performance; however, these require large amounts of data @cite , @cite , @cite , and @cite . These methods tackle the recognition of actions performed at the center of the camera plane, except for @cite , which uses static cameras to analyze actions. Method @cite allows actions to not be centered on the plane; however, it requires scenes with good illumination and no occlusions. At its current stage of development the DECU framework cannot collect the large number of samples necessary to train a deep network without disrupting the hospital.
- Multi-sensor and multi-camera systems and methods have been applied to smart environments @cite and @cite . The systems require alterations to existing infrastructure making their deployment in a hospital logistically impossible. The methods are not designed to account for illumination variations and occlusions and do not account for non-sequential, subtle motion. Therefore, these systems and methods cannot be used to analyze patient motion in a real ICU where patients have limited or constrained mobility and the scenes have random occlusions and unpredictable illumination.
- Network layer protocol designs for neighbor discovery can be categorized into randomized and deterministic algorithms. The main objective is to optimize random access probability or the transmit schedule of each device such that the system throughput is maximized @cite @cite @cite @cite @cite . Instead of purely avoiding collision, the FlashLinQ technology developed by Qualcomm assigns channels based on signal-to-interference ratios and achieves superior performance over carrier sense multiple access with collision avoidance (CSMA CA) systems @cite @cite . The neighbor discovery algorithm proposed in this paper can be regarded as a physical layer technique, which can be optimized with the network layer protocols to improve the system performance.
- Recently, the idea of codes on graph has been applied in random access @cite @cite . One scheme is named coded slotted ALOHA, where the packets are repeatedly transmitted in different slots and are decoded using successive cancellation. These works assume synchronization transmission and perfect interference cancellation. The asynchrounous model has been studied in @cite @cite @cite , where the asynchronicity is modeled to cause additional interference. However, perfect interference cancellation was still assumed. Rateless codes have been proposed for multiple access in machine-to-machine communications @cite , where the channel gains are assumed to be known. As the number of users increases, the imperfect channel estimatioon is detrimental to the performance of successive cancellation.
- Neighbor discovery can be formulated as multiuser detection from the perspective of physical layer processing @cite . Due to the bursty traffic patterns, the number of active devices is typically orders of magnitude smaller than the total local device population. Based on this crucial observation, low-complexity neighbor discovery algorithms inspired by compressed sensing were proposed @cite @cite @cite @cite @cite . These algorithms can reduce the code length by over 50 Noncoherent neighbor discovery based on energy detection can be formulated as the classical group testing problem @cite . By utilizing the sparsity nature, neighbor discovery can be achieved efficiently using a low transmission overhead. A related study applies sparse graph codes to asynchronous group testing @cite , where the required code length is larger than that for our proposed scheme.
- One of the early techniques for video skimming relies on language understanding along with visual features @cite . However, the reliance on language understanding constrains the applicability to where meaningful and discernible audio is present. Subsequently, EM @cite , and SVD @cite were also applied. A user attention based approach for video summarization @cite models user's attention and then selects the frame or a sequence of frames (clips) that are most likely to attract attention (as predicted by the model). More recently, there is a growing interest in detecting primitives (such as shots @cite , events @cite ) and using that as the basis for summarization. However, the drawback of being able to systematically parameterize summarization while being able to produce summaries of variable lengths remains.
- Sports video summarization in general has seen considerable traction in recent times -- player and ball motion tracking @cite , highlight generation using audio visual cues @cite @cite etc. are being actively pursued in constrained settings. A recent and an interesting approach to generate basketball highlights @cite leverages multimodal context cues for video summarization. While the system asserts good results in indoor basketball settings, replicating same to the outdoor settings would be a challenge in the absence of high quality broadcast data complemented with player and play-by-play stats. Local soccer matches or junior leagues are devoid of such elaborate setups and we only have video data in such settings. We thus propose a generic parameterized approach based on visual motion' cues to generate highlights of the soccer matches.
- The task of producing routes using orthogonal layers of parallel channels and vias at intersections is often tackled heuristically, since an optimal solution does not exist for this NP-complete problem @cite @cite @cite @cite . Previous work in this field @cite @cite @cite @cite @cite @cite mostly rely on explicit rule-based algorithms to tackle parts of this complex task. For instance, Zajc al @cite proposed using hierarchical algorithms for automatic routing of interconnects in two layer channels. Rivest al @cite show a "greedy" heuristic channel router assuming all pins and wiring sit on a common grid. The other class of routers, which have received attention lately, are objective function based. Alpert al @cite proposed a combination of minimum spanning tree (Prim) and shortest path tree (Dijkstra) objectives for performance driven global routing. Constructing routing tree with different objectives was also explored, such as timing objective @cite , buffer insertion @cite and wire sizing objective @cite , congestion and thermal objective @cite @cite . A completely different objective for diagonal routing as opposed to the orthogonal (Manhattan) routing was also proposed @cite .
- There has been a number of studies for the automatic photo adjustment. Several methods focus on the global tonal adjustment @cite , the color enhancement @cite , and the personalized enhancement @cite . Those methods are global adjustment approaches based on hand-crafted low-level features such as the color histogram, the scene brightness, and the highlight clipping. @cite , Kapoor al proposed a method that discovers the clusters of users that have similar preferences of image enhancement for the personalized adjustment. While the concept of our method may be similar to those methods, the main difference is that we aim to discover the retouching presets that vary according to the local semantics.
- Hwang al @cite presented a locally varying photo enhancement method that is based on both low- and high-level contexts. Their method finds an appropriate color mapping from external images using pixel-wise contextual features. The work of Yan al @cite is closely related to our work. The authors combine multiple hand-crafted features including a multi-scale pooling of a scene parsing map for semantics-aware color regression. While the multi-scale pooling features were effective in modelling the semantics-aware photo adjustment, the performance is limited to the quality of the scene parsing map since the features are not trained in an end-to-end manner.
- Our method is also related to various deep learning based semantics-aware image processing methods. Tsai al @cite used a scene parsing deep network to localize a sky region and transfer a different style of sky from external images. @cite , the authors propose a DNN for image harmonization, which is an encoder-to-decoder network to exploit high-level contextual features. The DNN is jointly trained with a scene parsing task to improve the training. In contrast to @cite , our method does not rely on the segmentation mask and rather finds the inherent segmentation masks from the data. Deep learning based colorization methods @cite @cite are also related to our work in that the methods make use of rich contextual features of CNNs to estimate the color of a pixel according to the scene context. Unlike those methods, we do not reconstruct missing color channels, and the color mapping of pixels is consistent in a semantic region.
- Abnormality Detection There is a wealth of literature on abnormality detection @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . Most of the previous work is based on hand-crafted features (e.g., Optical-Flow, Tracklets, etc.) to model the normal activity patterns, whereas our method learns features from raw-pixels using a deep-learning based approach using an end-to-end training protocol. Deep learning has also been investigated for abnormality detection tasks in @cite @cite @cite . Nevertheless, these works mainly use existing Convolutional Neural Network (CNN) models trained for other tasks (e.g., object recognition) which are adapted to the abnormality detection task. For instance, @cite proposed a Binary Quantization Layer, plugged as a final layer on top of a pre-trained CNN, in order to represent patch-based temporal motion patterns. However, the network proposed in @cite is not trained end-to-end and is based on a complex post-processing stage and on a pre-computed codebook of the convolutional feature values. Similarly, in @cite @cite , a fully convolutional neural network is proposed which is a combination of a pre-trained CNN (i.e., AlexNet @cite ) and a new convolutional layer where kernels have been trained from scratch.
- Here we survey some works using visual features obtained from pre-trained deep neural networks for recommendation tasks. @cite introduced an image-based recommendation system based on styles and substitutes for clothing using visual embeddings pre-trained on a large-scale dataset obtained from Amazon.com. Recently, @cite went further in this line of research and introduced a visually-aware matrix factorization approach that incorporates visual signals (from a pre-trained DNN) into predictors of people's opinions. The latest work by @cite deals with visually-aware artistic recommendation, building a model which combines ratings, social signals and visual features. @cite compared visual DNN and explicit (stylistic) visual features for movie recommendation, and they found the explicit visual features more informative than DNN features for their recommendation task.
- The most related work is @cite , which also handles evolving features in streaming data. Different to our setting where there are overlapping periods, @cite handles situations where there is no overlapping period but there are overlapping features. Thus, the technical challenges and solutions are different.
- Birg ' e @cite gives stronger, but less transparent, bounds than Fano's inequality using Fano-type arguments; again, @math is bounded in terms of an average of Kullback--Leibler divergences, but these are used as the argument for a function, rather than directly substituted. Sason and Verd 'u [Section 3] sasonV18arimoto recently derived a generalized Fano's inequality in terms of the Arimoto-R 'enyi conditional entropy. They also obtained upper bounds on @math in terms of the pairwise R 'enyi divergences [Section 4] sasonV18arimoto .
- Using Fano's inequality, Yang and Barron @cite obtained order-optimal minimax risk lower bounds that depend only on global metric entropy features of the underlying function class, without explicitly constructing a packing set. The required metric entropy features (bounds on the packing number and covering number) are available from results in approximation theory for many function classes of interest. Guntuboyina @cite obtained a lower bound on the average error probability in terms of general @math -divergences, and also generalized the metric entropy results of Yang and Barron @cite to certain @math -divergences such as the @math -divergence. An interesting direction for future work would be to obtain a non-asymptotic result analogous to Theorem for the case where only the global metric entropy features are available.
- An important historical remark is that Hayashi and Nagaoka @cite first linked channel coding and binary hypothesis testing, with later work @cite by Hayashi clarifying this approach and Nagaoka @cite using similar ideas to derive strong converse results. The recent work by Vazquez- @cite also provides results characterizing the average error probability of channel coding in terms of the Type I error of a binary hypothesis test. This link with channel coding has been used in other contexts to prove strong converse results, including @cite , which derived strong converse results for the group testing problem.
- An example of users' data protection and privacy policy enforcement on a blockchain is provided in @cite . The authors propose to control access permissions to private data collected by a service (e.g., location from a mobile phone) through a Bitcoin blockchain. Every time a users subscribes to a service a new transaction specifies the access permissions and another contains the hash of the data, which are stored in a off-chain database. Policies encoded in a protocol executed by the blockchain grant or deny access to the data referenced in the chain. Although this solution is in part similar to ours, their proposed policies only specify simple allow deny enforcement (i.e. white blacklisting) without the possibility to express more complex policy conditions. Moreover, scalability is not taken into consideration: issuing minimum two transactions for every subscription of a user application to an online service would easily saturate a Bitcoin network even considering few service providers.
- Applied to the same field but with a more comprehensive approach is MedRec @cite . The authors propose to give patients control over their Electronic Health Records (EHRs) through the use of Ethereum blockchain and smart contracts, which manage authentication, confidentiality, accountability and sharing of the data. The latter are referenced in the chain using their hashes but are stored externally. Miners are rewarded with anonymized aggregated data useful for research. Although the proposed approach addresses many issues, considerations about costs and scalability on a large scale deployment are not mentioned.
- There is a large body of works on the development and validation of early warning scores to predict clinical deterioration and other related outcomes. For instance, the MEWS score @cite and NEWS score @cite are two of the more common scores used to assess overall clinical deterioration. In addition, the SIRS score for systemic inflammatory response syndrome was part of the original clinical definition of sepsis @cite , although other scores designed for sepsis such as SOFA @cite and qSOFA @cite have been more popular in recent years. A more sophisticated regression-based approach called the Rothman Index @cite is also in widespread use for detecting overall deterioration. Finally, @cite used a Cox regression to predict sepsis using clinical time series data, although they do not account for temporal structure since they simply create feature and event-time pairs from raw data.
- There are several related works that also utilized multitask Gaussian processes in modeling multivariate physiological time series. For instance @cite and @cite used a similar model to ours, but instead focused more on forecasting of vitals to predict clinical instability, whereas our task is a binary classification to identify sepsis early. Finally, our end-to-end technique to discriminatively learn both the MGP and classifier parameters builds off of @cite . However, our focus is more applied and the setting is more involved, as our time series are multivariate, of highly variable length, and may contain large amounts of missingness.
- . @cite proposed a people count estimation method combining the foreground segmentation and the head-shoulder detection. A Mosaic Image Difference based foreground segmentation algorithm is performed first to detect active areas, and then a head-shoulder detection algorithm is utilized to detect heads and count the number from the detected foreground areas. @cite presented an object detection system based on coherent motion region detection for counting and locating objects in the presence of high object density and inter-object occlusions. They used the locations of tracked low-level feature points to construct all possible coherent-motion-regions and chose a good disjoint set of coherent motion regions representing individual objects using a greedy algorithm. @cite described an unsupervised data-driven Bayesian clustering algorithm to detect individual entities. The method tracked simple image features and probabilistically group them into clusters representing independently moving entities.
- . @cite used dense SIFT features and Maximum Excess over SubArrays distance as a loss function to train a regression model on randomly selected patches. In order to adapt to the change of the crowd density and perspective, @cite proposed a Multi-column CNN architecture to map the image to its crowd density map. The network structure included three parallel CNN with different sizes to extract features from different scales. The features learned by each column CNN were adaptive to variations in people head size due to perspective effect or image resolution. @cite proposed an end-to-end CNN architecture that directly maps the whole image to the counting result. A pre-trained GoogLeNet model was used to extract high-level deep features and the long-short time memory (LSTM) decoders for the local count and fully connected layers for the final count. A cross-scene crowd counting architecture was proposed by @cite . Firstly, they trained a deep CNN with two related learning objectives, crowd density, and crowd count. And then a data-driven method was introduced to fine-tune the learned CNN to an unseen target scene, where the training samples similar to the target scene were retrieved from the training scenes for fine-tuning.
- Some researchers tried to combine two methods to get a more accurate estimate count. @cite proposed a multi-source multi-scale counting method to compute an estimation of the number of individuals presented in an extremely dense crowd visible in a single image. This method combined the detection approach (low confidence head detection) and the features regression approach (repetition of texture element and frequency-domain analysis) to solve the perspective, occlusion, clutter and few pixels per person problems in a crowd of more than hundreds. Then the spatial consistency constraints are employed by MRF to smooth the counting between adjacent patches. @cite addressed the problem of person detection and tracking in crowded video scenes. They explored constraints imposed by the crowd density and formulate person detection as the optimization of a joint energy function combining crowd density estimation and the localization of individual people.
- Before the present era of deep learning, speech emotion recognition systems prevalently relied on a two-stage training approach, where feature engineering and classifier training were performed separately. Commonly used hand-crafted features include pitch, MFCC, log-Mels and the recommended feature sets from the INTERSPEECH challenges. Support vector machine (SVM) and extreme learning machine (ELM) were two of the most competitive classifiers. For the ease to compare models, @cite summarized the performances by a SVM trained on the INTERSPEECH challenge feature sets over several public corpora. @cite recently proposed a sparse kernel reduced-rank regression (SKRRR) for bimodal emotion recognition from facial expressions and speech, which has achieved one of the state-of-the-art performances on the eNTERFACE'05 @cite corpus.
- @cite employed a multilayer perceptron (MLP) to learn from spliced data frames and took statistics of aggregated frame posteriors as utterance-level features. An MLP-ELM supervised by these utterance features and the corresponding labels has been shown to outperform the MLP-SVM model.
- It has been known that emotion involves temporal variations of mental state. To exploit this fact, W " o @cite and Met @cite conducted experiments at the conversation-level to show that human emotion depends on the context of a long-term temporal relationship using HMM and Bi-directional LSTM (BLSTM). @cite posed speech emotion recognition at the utterance level as a sequence learning problem and trained an LSTM with a connectionist temporal classification objective to align voiced frames with emotion activation.
- Recently, proposed the CLDNN architecture for speech recognition based on the log-Mels @cite and the raw waveform signal @cite , in which both models have been shown to more competitive than a LSTM and a CNN model alone or combined. They also demonstrated that with a sufficient amount of training data (roughly @math hours), a CLDNN trained on the raw waveform signal can match the one trained on the log-Mels. Moreover, they found the raw waveform and the log-Mels in fact provide complementary information. Based on the CLDNN architecture, @cite published a model using the raw waveform signal for continuous emotion tracking. @cite trained a CLDNN model on the log-Mels for speech emotion recognition and quantitatively analyzed the difference in spectrally decorrelating power between the discrete cosine transformation and the convolutional operation. @cite repeated the comparison between CNN, LSTM and CLDNN for speech emotion recognition using spectrograms. @cite applied the CLDNN architecture to classifying depression based on the log-Mels and spectrograms. They employed the full-spectrum temporally convolutional operation on the log-Mels but the temporally-only convolutional operation on the spectrograms.
- On the multi-modal side, @cite fine-tuned the AlexNet on spectrograms and images, separately, for audio-visual emotion recognition but only applied time-averaging for temporal pooling. @cite extended the uni-modal work in @cite to make use of visual cues. They fine-tuned the pre-trained ResNet model @cite for facial expression recognition and then re-trained the concatenated bimodal network with the LSTM layers re-initialized again. Our work is similar to @cite because we both report the benchmarking of convolutional types. However, in addition to the novelty aforementioned, we train our models in an end-to-end fashion on log-Mels and MFCCs depending on their locally spectral-temporal correlation. Moreover, we keep the testing partition speaker-independent of the training parition. @cite also experimented with two types of convolutional operations but they applied them to different features. As a result, it is difficult to draw a fair conclusion from the comparison. This work is also similar to @cite , @cite and @cite , where all adopt the CLDNN architecture for speech emotion recognition tracking but the underlying features and the intended goals are different.
- Two key components are included in image matching, one is extracting proper features from the original image and the other is measuring the distance of the features to describe the similarity of images. At first, hand-craft features such as SIFT @cite and DAISY @cite are cooperated with fixed metric method like Graph Model @cite @cite @cite to match images. It means that the two parts of matching (extracting feature and measuring similarity) are independent when using above methods and the isolation hinders the improvement of performance. To break the isolation, researchers propose learning descriptors or similarity metric in condition of fixed the other part. For example, @cite learns the descriptors by minimizing the classification error and @cite learns the metric by treating it as a linear transformation. Learning descriptors and metric jointly is proposed to make the cooperation of the two parts more powerful. @cite , boosting trick is adopted to learn descriptors and metrics and achieved great performance. The performance of these methods are limited by the hand-crafted features, while the proposed method employs deep features and achieves better performance.
- Analogical reasoning has been studied both on its own and as a component of downstream tasks, using a range of systems. Early work used rule-based systems for world knowledge @cite and syntactic @cite relationships. Supervised models were used for SAT (Scholastic Aptitude Test) analogies @cite , and later for synonymy, antonymy, and some world knowledge @cite @cite . Analogical reasoning has also been used in support of downstream tasks, including word sense disambiguation @cite and morphological analysis @cite @cite @cite .
- Recent work on analogies has largely focused on their use as an intrinsic evaluation of the properties of a VSM. The analogy dataset of , often referred to as the Google dataset, has become a standard evaluation for general-domain word embedding models @cite @cite @cite @cite , and includes both world knowledge and morphosyntactic relations. Other datasets include the MSR analogies @cite , which describe morphological relations only; and BATS @cite , which includes both morphological and semantic relations. The semantic relations from SemEval-2012 Task 2 @cite have also been used to derive analogies; however, as with the lexical Sem-Para dataset of , the semantic relationships tend to be significantly more challenging for embedding-based methods @cite . Additionally, demonstrate that even for some lexical relations where embeddings appear to perform well, they are actually learning prototypicality as opposed to relatedness.
- Generative Adversarial Networks (GANs) have recently gained a lot of popularity due to the relative sharpness of samples generated by these models compared to other approaches. The originally proposed baseline approach @cite has been modified to incorporate deep convolutional networks without destabilizing the training scheme and achieving significant qualitative improvements in image quality @cite @cite . Further improvements were made by @cite by incorporating algorithmic tricks such as mini-batch discrimination which stabilize training and provide better image quality. We incorporate some of these tricks in our work as well.
- Our central idea -- utilizing a mixture model for latent space -- has been suggested in various papers, but mostly in the context of variational inference. For example, @cite , @cite and @cite model the approximate posterior of the inferred latent distribution as a mixture model to represent more complicated distributions. More recently, @cite and @cite propose normalizing flows' to transform the latent probability density through a series of invertible mappings to construct a complex distribution. In the context of GANs, no such approaches exist, to the best of our knowledge.
- Our approach can be viewed as an attempt to modify the latent space to obtain samples in the high probability regions in the latent space. The notion of latent space modification has been explored in some recent works. For example, @cite propose to alternate between training the latent factors and the generator parameters. @cite formulate an MCMC sampling process to sample from high probability regions of a learned latent space in variational or adversarial autoencoders.
- The existing literature on analyzing human activities is extensive. Thorough surveys of earlier work include Gavrila @cite and @cite . Below, we review closely related work in activity recognition, including individual actions, group multi-person activities, and trajectory analysis.
- The incorporation of track-level features as extra cues for interaction modeling was done by @cite and @cite . Recent work has developed more sophisticated deep temporal models for activity analysis. @cite utilize attention models to focus on key players in sports activties. @cite build hieararchical LSTMs to model multiple interacting people over time. In contrast with this, our work learns trajectory features directly from human position inputs.
- Trajectory Data Analytics : There exists significant literature on trajectory analysis focusing on team sports, such as basketball, soccer, and hockey. Applications within sports analytics include analyzing player and team performance, and mining underlying patterns that lead to certain results. Work in this field has included various statistical models to capture the spatio-temporal dynamics in player trajectories. We refer readers to a recent survey @cite on detailed team sports analysis with trajectory data.
- Classic examples in the vision literature include Intille and Bobick @cite who analyzed American football plays based on trajectory inputs. M ' @cite utilized relative positioning between key elements in a scene, such as vehicles and checkpoints, to recognize activities.
- When processing untrimmed videos, actions can either be localized in the temporal domain only @cite @cite @cite @cite @cite @cite @cite @cite , or in the spatio-temporal domain @cite @cite @cite @cite . For the latter, videos are usually constrained to contain only few action instances. While most approaches in this area are fully supervised, @cite propose a weakly supervised method for actor-action segmentation that is based on a multi-task ranking model.
- In the context of fully supervised action detection, most approaches use a sliding window to efficiently segment a video @cite @cite and rely on CNNs or recurrent networks @cite @cite @cite that can not be used if only weak supervision is available. The same holds for @cite , who model context and length information, which is also done in our approach. They show that length and context information significantly improve action segmentation systems, using a Poisson distribution to model action lengths and a language model to incorporate action context information. Other fully supervised methods guided by grammars have been proposed in @cite @cite @cite . Note that in contrast to our task, their length and context model can be easily estimated from the frame-level training annotations. The challenge for our problem formulation, however, is that no annotations that allow a direct estimation of a context or length model are provided.
- Another recently published and related method by Wang al @cite addresses the task of detecting an action in a video with sparse action occurrences. More precisely, for a given action class, they generate action proposals and train a neural network to distinguish instances of this action from background in the video. Being designed to distinguish actions from background in a video, their method is not suited for densely labeled videos containing many difference actions followed by one another, as it is the case in this paper.
- A standard approach to incorporating external information into vector spaces is to pull the representations of similar words closer together. Some models integrate such constraints into the training procedure, modifying the prior or the regularisation @cite @cite @cite @cite , or using a variant of the SGNS-style objective @cite @cite . Another class of models, popularly termed , injects lexical knowledge from available semantic databases (e.g., WordNet, PPDB) into pre-trained word vectors @cite @cite @cite @cite @cite . Morph-fitting falls into the latter category. However, instead of resorting to curated knowledge bases, and experimenting solely with English, we show that the of any language can be exploited as a source of inexpensive supervision for fine-tuning vector spaces, at the same time specialising them to better reflect true semantic similarity, and learning more accurate representations for low-frequency words.
- Other than recent results describing effects of audio distractions @cite , we are unaware of any prior usability studies utilizing a fully automated and unattended physical environment.
- However, some prior work reinforces validity of virtually-attended remote experiments and unattended online surveys, in contrast with same efforts in a traditional lab-based setting. @cite collected psychometric data in: (1) a physically attended experimental lab setting and (2) its virtually attended remote counterpart. No significant differences were found. This is further reinforced by @cite who compared data collected from (1) unattended online, and (2) attended offline, questionnaires. Finally, Lazem and Gracanin @cite replicated two classical social psychology experiments where both the participants and the experimenter were represented by avatars in Second Life See secondlife.com , instead of being physically co-present. Here too, no significant differences were observed.
- For example, Short Authenticated String (SAS) protocols ask the user to compare two strings of about 20 bits each @cite .
- @cite performed the first usability study of Bluetooth pairing techniques using SAS. It determined that the compare-and-confirm" method -- which involves the user comparing two 4-to-6-digit decimal numbers and indicating a match or lack thereof -- was the most accurate and usable approach.
- @cite compiled a comprehensive comparative usability study of eleven major secure device pairing methods. They measured task performance times, completion times, completion rates, perceived usability and perceived security. This led to the identification of most problematic as well as most effective pairing methods, for various device configurations.
- @cite proposed an authentication protocol that used Mad-Lib" style SAS. Each device in this protocol creates a nonsensical phrase based on the protocol outcome, and the user then determine if the two phrases match. This approach was found to be easier for non-specialist users.
- @cite examined usability of device pairing in a group setting. In this setting, up to 6 users tried to connect their devices to one another by participating in a SAS protocol. It was found that group effort decreased the expected rate of security and non-security failures. However, if a single individual was shown a SAS different from that of all others participants, the former often lied about the SAS in order to fit in with the group, demonstrating so-called insecurity of conformity.''
- @cite discovered that subject's performance in secure device pairing could be improved if it were to be scored. In other words, notifying subjects about their performance score resulted in fewer errors.
- O'Malley and Poplawsky @cite argued that sensory noise affects behavioral selectivity. Specifically, while a consistent positive or negative effect on task completion may not occur, a consistent negative effect was observed for tasks that require subjects to react to signals on their periphery. Meanwhile, a consistent positive effect on task completion was observed for tasks that require subjects to react to signals in the center of their field of attention. This leads to the claim that sensory stimulation has the effect of narrowing the subject's area of attention.
- Moreover, visual stimuli were found to be dominating in multi-sensory contexts. Eimer @cite showed that in experiments with tactile, visual, and audio stimuli, subjects overwhelmingly utilized visual queues to localize tactile and auditory events.
- Topic models have been extensively studied for a variety of applications in document modelling and information retrieval. Beyond LDA, significant extensions have sought to capture topic correlations @cite , model temporal dependencies @cite and discover an unbounded number of topics @cite . Topic models have been extended to capture extra context information such as time @cite , authorship @cite , and class labels @cite . Such extensions often require carefully tailored graphical models, and associated inference algorithms, to capture the desired context. Neural models provide a more generic and extendable option and a number of works have sought to leverage these, such as the Replicated Softmax , the Auto-Regressive Document Model , Sigmoid Belief Document Model @cite , Variational Auto-Encoder Document Model (NVDM) @cite and TopicRNN Model @cite . However, these neural works do not explicitly capture topic assignments.
- We argue for the important role that interference characterization plays in evaluating and predicting the network performance in , including both microwave and mmWave bands. Traditionally, mmWave bands are considered for backhaul in cellular systems and for high-volume consumer electronics such as personal area and local area networks, but not for cellular access due to concerns about short-range and non-line-of-sight coverage issues @cite @cite . MmWave has, however, recently been shown to be suitable for cellular communications, provided short cell radius of the order of 100-200 meters and sufficient beamforming gain between communicating nodes @cite . Reducing the cell radius leads to dense base station (BS) deployments. Even under beamforming, these high BS and user densities can drive cellular networks to be more interference rather than noise limited. While large adaptive arrays with narrow beams can boost the received signal power and hence reduce the impact of out-of-cell interference @cite @cite , this interference remains an important performance-limiting factor in dense mmWave networks @cite .
- To evaluate an interference model, we apply it to a cellular network and analyze the system performance. For cellular network analysis, stochastic geometry is shown to capture the main performance trends with analytical tractability . Stochastic geometry is used to develop a tractable model for downlink heterogeneous cellular networks @cite , which is applied to analyze coordinated multipoint beamforming @cite . These stochastic geometry based networks are useful in verifying that the performance of a cellular network matches experimental trends, and can also be used to verify the accuracy of an interference model.
- Besides the aforementioned works, Automatic Ensemble @cite is the most recent work which uses stacking and meta-data to assist model selection and tuning. TPOT @cite is another system that uses genetic programming to find the best model configuration and preprocessing work-flow. Automatic Statistician @cite is similar to the works just described but focuses more on time-series data and interpretation of the models in natural language.
- Different from automation of model selection and tuning where the literature is very rich, only a few works have been proposed to automate feature engineering. The main reason is that feature engineering is both domain and data specific. In fact, it requires a lot of data exploration with deep domain knowledge to search for relevant patterns in the data. However, recent work shows that, for a specific type of problem and data such as provided in relational databases, automation of feature engineering is achievable @cite .
- Data Science Machine (DSM) @cite is the first system that automates feature engineering from a database of multiple tables. This feature engineering approach is based on an assumption that, for a given relational database, data scientists usually search for features via: 1. generating SQL queries to collect data for each example in the training set and 2. transforming the data into features. DSM automates the given two steps via creating an entity graph and performing automatic SQL query generation to join the tables along different paths of the entity graph. It converts the collected results into features using a predefined set of simple aggregation functions.
- * Supervised Segmentation These techniques treat 3D mesh segmentation as a labelling problem and use machine learning to optimise the mapping from features to labels. It requires extensive manual effort to label all data. The recent effort from the community contributed to a large set of segmentation benchmarks (e.g. @cite ).
- Existing supervised techniques rely on local features. The work by @cite proposed a method for mesh segmentation where a large pool of geometric features are ranked using JointBoost so that the best features are used to describe specific segments. Similarly, the work by @cite ranks a large pool of features in order to detect the optimal segment boundaries for a given mesh, and an extreme learning machine was trained to classify labels using one @cite and two layers @cite . However, supervised methods can perform poorly on very complex meshes, due to insufficient training data or large variations within label classes @cite @cite .
- Recently, @cite extended the CNN idea to 3D segmentation. They reshape a large pool of geometric features into a matrix resembling that of a 2D image, fitting the 2D image-based CNN pipeline, and then train a CNN on these images'' using the ground truth labels. The technique shows good performance, however, the reshaping and the use of 2D filters may infer relationships between adjacent rows of features that may have no correlation. As Figure shows, passing a convolutional filter over such an image'' would unavoidably infer relationships between (up to 5) unrelated features regardless of the position of the filter.
- From the literature, we have two further observations. First, existing feature driven techniques use local features developed by the influential work @cite . These features are mostly defined per face (a few are normalized by different geodesic radii for smoothing purposes), as such, there is no spatial scale information included in the architecture. To the best of our knowledge, a feature-based deep learning network architecture that considers multi-scale geometric features derived from a set of local faces has not been fully explored in supervised mesh segmentation. We hypothesise that multi-scale features would be useful because face-based @cite and patch-based techniques @cite have both shown good performance in co-segmentation. Second, whilst deep learning is useful, there is not much analysis as how CNN perform compared to other techniques in the deep learning family.
- @cite , CF after one stage of Laplacian-smoothing, combination of CF0 and CF1, combination of all smoothed CF features, and combination of all CF (including CF0). The chart shows that the new smoothed CF features improve upon the original in most cases, and also further improves when they are combined with the original CF . In this paper, we show that by using multi-scale features, treating them as separate 1D vectors per scale, and applying multi-branch 1D CNN filters through the network, we avoid the parameter tuning problem of reshaping a 2D matrix. Our method clearly out-performs existing work @cite in accuracy. Further, we provide comprehensive evaluation of various deep learning techniques, and show how CNN , though more complex, can improve performance over simpler architectures ( NN , AE ). It is worth noting that we have found existing methods lack reproducibility. Some existing work have not provided any or complete experimental code. Despite using the exact architecture and settings stated, some high performing results are hard to reproduce.
- Traditional linguistic stegosystems are based on modification of an existing cover text, e.g., using synonym substitution @cite @cite and or paraphrase substitution @cite . The idea is to encode the secret information in the transformation of the cover text, ideally without affecting its meaning or grammatical correctness. Of these systems, the most closely related to ours is CoverTweet @cite , a state-of-the-art cover modification stegosystem that uses Twitter as the medium of cover; we compare to it in our preliminary evaluation ( ).
- Cover modification can introduce syntactic and semantic unnaturalness @cite ; to address this, Grovsald and Orgun proposed an alternative stegosystem where a human generates the stegotext manually, thus improving linguistic naturalness at the cost of human effort @cite .
- Matryoshka @cite takes this further: in step 1, it generates candidate stegotext automatically based on an @math -gram model of the English language; in step 2, it presents the candidate stegotext to the human user for polishing, i.e., ideally small edits that improve linguistic naturalness. However, the cost of human effort is still high, because the (automatically generated) candidate stegotext is far from natural language, and, as a result, the human user has to spend significant time and effort manually editing and augmenting it.
- The minimum jerk model is one example of an invariant model, where by finding a model for a task, the dynamics can be simplified. Many models for a controller for cooperative manipulation only have a few changing parameters that include initial and final time and position. They may also include a maximum velocity or other parameter. With just a few parameters, the entire trajectory can be described. One approach to determining these models has been programming by demonstration @cite .
- In the last few years, deep neural network approaches have gained in popularity dealing with NER task. With the advance of computational power, there has been more and more research that applied deep learning methods to improve performances of their NLP systems. LSTM and CNN are prevalent models used in these architectures. Firstly, @cite used a CNN over a sequence of word embeddings with a CRF layer on the top. They nearly achieved state-of-the-art results on some sequence labelling tasks such as POS tagging, chunking, albeit did not work for NER. To improve the accuracy for recognizing named entities, @cite used Bi-LSTM with CRF layer for joint decoding. This model also used hand-crafted features to ameliorate its performance. Recently, @cite proposed a hybrid model that combined Bi-LSTM with CNN to learn both character-level and word-level representations. Instead of using CNN to learn character-level features like @cite , @cite used BI-LSTM to capture both character and word-level features.
- Nonparametric density estimation has been a well studied problem in statistics and machine learning @cite . Unfortunately, nonparametric approaches like kernel density estimation suffer greatly from the curse of dimensionality and do not perform well when data does not have a small number of dimensions ( @math ). To alleviate this, several semiparametric approaches have been explored. Such approaches include forest density estimation @cite , which assumes that the data has a forest (i.e. a collection of trees) structured graph. This assumption leads to a density which factorizes in a first order Markovian fashion through a tree traversal of the graph. Another common semiparametric approach is to use a nonparanormal type model @cite . This approach uses a Gaussian copula with a rank-based transformation and a sparse precision matrix. While both approaches are well-understood theoretically, their strong assumptions often lead to inflexible models.
- In order to provide greater flexibility with semiparametric models, recent work has employed deep learning for density estimation. The use of neural networks for density estimation dates back to early work by @cite and has seen success in areas like speech @cite @cite , music @cite , etc.. Typically such approaches use a network to learn to parameters of a parametric model for data. Recent work has also explored the application of deep learning to build density estimates in image data @cite @cite . However, such approaches are heavily reliant on exploiting structure in neighboring pixels, often reshaping or re-ordering data and using convolutions to take advantage of neighboring correlations. Modern approaches for general density estimation in real-valued data include neural autoregressive density estimation (NADE) @cite @cite and non-linear independent component estimation (NICE) @cite . We briefly describe these approaches below and contrast them to RED models.
- NADE is a restricted-Boltzmann-machine-inspired density estimator that uses probability product rules and a weight-sharing scheme across conditional densities directly on input covariates @cite . As previously mentioned, a direct application of the chain rule on input dimensions will be sensitive to the order used . To overcome the difficulty of finding a good permutation, deep versions of NADE train the model by sampling random permutations, @math , throughout training. I.e. NADE attempts to find parameters that model the data well (on average) for all permutations. Of course, due to the combinatorial nature of permutations, only a minuscule fraction of permutations will have been explored during training time.
- NICE models assume that data is drawn from a latent independent Gaussian space and transformed @cite . The transformation uses several additive coupling'' shifting transformations on the second half of dimensions, using the first half of dimensions. The full transformation is the result of stacking several of these additive coupling layers together followed by a final rescaling operation. The resulting composition of these transformation yields a simple determinant of the Jacobian.
- The first researchers were worried to present to the readers the MOBA phenomena, its basic characteristics and gameplay. In @cite and @cite we can find information about basic MOBA characteristics. A broad discussion of MOBA as an eSport and game design can be found in @cite .
- In @cite the authors give a corresponding result of Theorem for minimally ramified power series, where @math is expressed in terms of the coefficients of the first two non-linear terms. Provided the information from Theorem we can make a corresponding version of Corollary for minimally ramified power series, where the conditions for optimality are expressed in terms of the four lowest degree non-linear terms.
- Here we rehash some of the known results that we will use in this paper, and describe the method of proof from @cite , which is relevant to this paper.
- A result we shall use repeatedly in this paper concerns the transversal fluctuations of the maximizer @math when @math are i.i.d. and @math there are bounds on the probability that the maximizer @math from @math to a point @math deviates more than @math from the straight line @math , @math . This is a result from @cite and cited here as Theorem 3.2. Furthermore, for line-to-point problems, we have detailed control over the distribution of the (random) starting point of @math , see @cite , Lemma 1.1, Lemma 1.2 and also (4.18) in @cite .
- Determining how many input output examples or execution traces are required in order to generalise well is still an open research problem. However, in this paper, we focus attention more on the explanatory power afforded by programs rather than on the broader problems of generalisation in the space of programs. While these characteristics are of course related, we take a view similar to that of @cite , arguing that it is possible to build from locally valid program fragments which provide useful insight into the black-box processes generating the data. By combining gradient descent and A* search the @math -machine is able to learn informative and interpretable high-level LISP-like programs, even just from a single observation trace.
- Domain adaptation techniques address the fundamental problem of @cite between a dataset, used for training and a dataset, used for testing. This issue often arises in machine learning, especially when algorithms are to be deployed on new, possibly unlabeled domains. Domain adaptation strategies can be divided in two classes: and adaptation. The first approach is based on the assumption that a supervised algorithm (usually a classifier) can benefit not only from a fully labeled source dataset, but also from (at least) some labeled data points from the target dataset. Our method belongs instead to the second class of approaches, which assume that no labels are available for the target data. In the following we detail recent methods proposed in literature to cope with this problem.
- A first class of methods aims to learn transformations which somehow align feature representations in the source and target sets. For instance, in @cite auto-encoders are exploited to learn common features. In @cite , a bi-shifting auto-encoder (BSA) is instead intended to shift source domain samples into target ones. @cite @cite approach the problem relying on dictionary learning, to find representations where source and target datasets are aligned. Geodesic methods @cite @cite aim at projecting source and target datasets on a manifold, and connecting the two subspaces with a path. Then, both datasets can be projected along this path. Eventually, @cite @cite propose a transformation to minimize the distance between the covariances of source and target datasets.
- A different deep learning based approach exploited for unsupervised domain adaptation relies on Generative Adversarial Networks. In @cite , the joint distribution between the two domains is learned, while @cite directly finds a way to transform samples from the source distribution into samples from the target distribution (and viceversa). These approaches seem very promising, though not fully tested in all the benchmark datasets.
- Last worth to mention is Batch Normalization @cite , recently revisited under a specific domain adaptation perspective @cite . The idea is quite similar in spirit to the idea of aligning second order statistics. However Batch Normalization tries to compensate for the source's internal covariance shift by normalizing each mini-batch to be zero-mean and unit variance (whitening), while not taking into account target's statistics: layer's activation are decorrelated for the source data, but not for target points.
- One particular issue that has attracted attention among many of these groups is invariance @cite @cite @cite @cite @cite @cite @cite @cite . Invariance is an important issue in any recognition problem from image or video data, since appearance is substantially affected by the imaging level, i.e. the viewing angle, camera parameters, lighting, etc. In action recognition the issue of invariant recognition is compounded with additional complexity introduced by the high degrees of freedom of human body @cite .
- In recent years, a number of DNNs have achieved notable success by reintroducing elements of symbolic computation as peripheral modules. This includes, e.g.: (i) the memory bank, a discrete set of addressed storage registers each holding a neural activation vector @cite @cite @cite ; and (ii) the sequential program, a discrete sequence of steps, each selected from a discrete set of simple, approximately-discrete primitive operations @cite @cite . The discreteness in these peripheral modules is softened by continuous parameters with which they interface with the central controlling DNN; these parameters modulate (i) the writing and reading operations with which information enters and exits a memory bank ( attention' @cite @cite ); and (ii) the extent to which inputs are passed to and outputs retrieved from the set of operations constituting a program @cite . The continuity of these parameters is of course crucial to enabling the overall system to be learnable by gradient-based optimization.
- The present work constitutes a different approach to reintroducing approximately symbolic representations and rule-based processing into neural network computation over continuous distributed representations. In computation with TPRs, the symbols and rules are internal to the DNN; there is no separation between a central network controller and peripheral quasi-discrete modules. Items in memories are distributed representations that are combined by addition superposition rather than by being slotted into external discrete locations. Computation over TPRs is massively parallel @cite .
- Most methods of interpreting the internal representations of DNNs do so through the input and output representations of DNNs which are by necessity interpretable: these are where the DNN must interface with our description of its problem domain. An internal neuron may be interpreted by looking at the (interpretable) input patterns that activate it, or the (interpretable) output patterns that it activates (e.g., @cite ).
- Most recently, a number of methods have been proposed to empower a single network to transfer multiple styles, including a model that conditioned on binary selection units @cite , a network that learns a set of new filters for every new style @cite , and a novel conditional normalization layer that learns normalization parameters for each style @cite . To achieve arbitrary style transfer, @cite first propose to swap the content feature with the closest style feature locally. Meanwhile, inspired by @cite , two following work @cite @cite turn to learn a general mapping from the style image to style parameters. One closest related work @cite directly adjusts the content feature to match the mean and variance of the style feature. However, the generalization ability of the learned models on unseen styles is still limited.
- Different from the existing methods, our approach performs style transfer efficiently in a feed-forward manner while achieving generalization and visual quality on arbitrary styles. Our approach is closely related to @cite , where content feature in a particular (higher) layer is adaptively instance normalized by the mean and variance of style feature. This step can be viewed as a sub-optimal approximation of the WCT operation, thereby leading to less effective results on both training and unseen styles. Moreover, our encoder-decoder network is trained solely based on image reconstruction, while @cite requires learning such a module particularly for stylization task. We evaluate the proposed algorithm with existing approaches extensively on both style transfer and texture synthesis tasks and present in-depth analysis.
- The automatic generation of programs using machine learning techniques is a relatively new field of research and program synthesis in a human-readable format have only been addressed very recently. A recent example is DeepCoder @cite , a system able to generate computer programs by leveraging statistical predictions to augment traditional search techniques. In another work by @cite , the generation of source code is enabled by learning the relationships between input-output examples via differentiable interpreters. Furthermore, @cite recently demonstrated program synthesis from a mixed natural language and structured program specification as input. It is important to note that most of these methods rely on ; computer languages (e.g. markup languages, programming languages, modeling languages) that are designed for a specialized domain but are typically more restrictive than full-featured computer languages. Using DSLs thus limit the complexity of the programming language that needs to be modeled and reduce the size of the search space.
- Although the generation of computer programs is an active research field as suggested by these breakthroughs, program generation from visual inputs is still a nearly unexplored research area. The closest related work is a method developed by @cite to reverse-engineer Android user interfaces from screenshots. However, their method relies entirely on engineered heuristics requiring expert knowledge of the domain to be implemented successfully. Our paper is, to the best of our knowledge, the first work attempting to address the problem of user interface code generation from visual inputs by leveraging machine learning to learn latent variables instead of engineering complex heuristics.
- It has been proven that humans can recognize actions from limited types of input such as point lights and low quality video @cite @cite . Even with such limited information, we are capable to differentiate stylistic action differences, such as the gender @cite @cite and age @cite of a walking person. There has been several recent work on the study of action style variation in computer vision. Wilson and Bobick @cite use a Parameterized-HMM to model spatial pointing gestures by adding a global variation paramters in the output probabilities of the HMM states. In @cite use a bilinear model to separate perceptual content and style parameters. Davis @cite proposed an approach to determine age of people based on variations in relative stride length and stride frequency over various walking speeds. In @cite Davis and Taylor use regularities in walking to classify typical from atypical gaits. @cite presented a three-mode (body pose, time, and style) expressive-feature model for representing and recognizing performance styles of human actions. The application of style analysis in computer animation for generating new animation styles of human motion have also been reported in @cite @cite @cite @cite @cite @cite .
- Like in other problems of human motion analysis, view invariance is an important requirement in gait recognition. However, only a few papers in the literature take into account view invariance . @cite propose an approach that integrates face and gait recognition from multiple views. As in other works that use multiple view data, their approach is limited to the number of views being used and is not truly'' view-invariant. @cite propose a view-invariant method for the case when the person is far from the camera. They synthesize a side view from any other arbitrary view using a single camera, and apply methods based on side view of walking to solve the gait recognition problem.
- Since its introduction by Clarkson and Woodruff @cite , SpEmb has received great attention due to its efficient time complexity. It has been extensively used as a randomization technique to solve numerical linear algebraic problems, here we look at its application on low rank approximation.
- Compared to most randomized algorithms, FD approaches the matrix sketching problem from a different perspective. It is deterministic, space efficient and produces more accurate estimates given a fixed sketch size. It was introduced by Liberty @cite , later extended by @cite with a comprehensive analysis, and a few variations @cite @cite have been developed since then. The algorithm extends the idea of frequent items for item frequency approximation problem to a general matrix. Given an input matrix @math and space parameter @math , at first, the algorithm considers the first @math rows in @math and shrinks its top @math orthogonal vectors by the same amount to obtain an @math matrix; then combines them with the next @math rows in @math for the next iteration, and repeat the procedure. It is illustrated in Fig. . The detailed algorithm is given in Algorithm .
- The VAE is a latent variable model that is usually trained with a very simple prior, , the standard normal prior. In @cite a Dirichlet process prior using a stick-breaking process was proposed, while @cite proposed a nested Chinese Restaurant Process. These priors enrich the generative capabilities of the VAE, however, they require sophisticated learning methods and tricks to be trained successfully. A different approach is to use an autoregressive prior @cite that applies the IAF to random noise. This approach gives very promising results and allows to build rich representations. Nevertheless, the authors of @cite combine their prior with a convolutional encoder and an autoregressive decoder that makes it harder to assess the real contribution of the autoregressive prior to the generative model.
- Concurrently to our work, in @cite a variational VAE with a memory was proposed. This approach shares similarities with the VampPrior in terms of a learnable memory (pseudo-inputs in our case) and a multimodal prior. Nevertheless, there are two main differences. First, our prior is an explicit mixture while they sample components. Second, we showed that the optimal prior requires to be coupled with the variational posterior. In the experiments we showed that the VampPrior improves the generative capabilities of the VAE but in @cite it was noticed that the generative performance is comparable to the standard normal prior. We claim that the success of the VampPrior follows from the utilization of the variational posterior in the prior, a part that is missing in @cite .
- Very recently, the VampPrior was shown to be a perfect match for the information-theoretic approach to learn latent representation @cite . Additionally, the authors of @cite proposed to use a weighted version of the VampPrior: where @math are trainable parameters such that @math and @math . This allows the VampPrior to learn which components (, pseudo-inputs) are meaningful and may prevent from potential overfitting.
- There is strong practical interest in matching clothing seen on the street to an online catalog, prompting methods to overcome the street-to-shop domain shift @cite @cite @cite . Beyond exact matching, recommendation systems require learning when items go well" together @cite @cite @cite and capturing personal taste @cite and occasion relevance @cite . Our task is very different. Rather than recognize or recommend garments, our goal is to forecast the future popularity of styles based on visual trends.
- Descriptive visual attributes are naturally amenable to fashion tasks, since garments are often described by their materials, fit, and patterns (, , ). Attributes are used to recognize articles of clothing @cite @cite , retrieve products @cite @cite , and describe clothing @cite @cite . Relative attributes @cite are explored for interactive image search with applications to shoe shopping @cite @cite . While often an attribute vocabulary is defined manually, useful clothing attributes are discoverable from noisy meta-data on shopping websites @cite or neural activations in a deep network @cite . Unlike prior work, we use inferred visual attributes as a conduit to discover fine-grained fashion styles from unlabeled images.
- Limited work explores representations of visual . Different from recognizing an article of clothing (, ) or its attributes (, ), styles entail the higher-level concept of how clothing comes together to signal a trend. Early methods explore supervised learning to classify people into style categories, e.g., biker, preppy, Goth @cite @cite . Since identity is linked to how a person chooses to dress, clothing can be predictive of occupation @cite or one's social urban tribe" @cite @cite . Other work uses weak supervision from meta-data or co-purchase data to learn a latent space imbued with style cues @cite @cite . In contrast to prior work, we pursue an unsupervised approach for discovering visual styles from data, which has the advantages of i) facilitating large-scale style analysis, ii) avoiding manual definition of style categories, iii) allowing the representation of finer-grained styles , and iv) allowing a single outfit to exhibit multiple styles. Unlike concurrent work @cite that learns styles of outfits, we discover styles for individual garments and, more importantly, predict their popularity in the future.
- Beyond categorizing styles, a few initial studies analyze fashion . A preliminary experiment plots frequency of attributes (floral, pastel, neon) observed over time @cite . Similarly, a visualization shows the frequency of garment meta-data over time in two cities @cite . The system in @cite predicts when an object was made.The collaborative filtering recommendation system of @cite is enhanced by accounting for the temporal dynamics of fashion, with qualitative evidence it can capture popularity changes of items in the past (i.e., Hawaiian shirts gained popularity after 2009). A study in @cite looks for correlation between attributes popular in New York fashion shows versus what is seen later on the street. Whereas all of the above center around analyzing (observed) trend data, we propose to forecast the (unobserved) styles that will emerge. To our knowledge, our work is the first to tackle the problem of visual style forecasting, and we offer objective evaluation on large-scale datasets.
- Text surrounding fashion images can offer valuable side information. Tag and garment type data can serve as weak supervision for style classifiers @cite @cite . Purely textual features (no visual cues) are used to discover the alignment between words for clothing elements and styles on the fashion social website Polyvore @cite . Similarly, extensive tags from experts can help learn a representation to predict customer-item match likelihood for recommendation @cite . Our method can augment its visual model with text, when available. While text improves our forecasting, we find that text alone is inadequate; the visual content is essential.
- Although the methods cited above have good performance, they suffer from drifting and operational mismatch problems @cite . Therefore, performing spatial transform before temporal decomposition was introduced to overcome these drawbacks. However, since complete DWT is shift variant, in order to achieve in-band ME MC (i.e. directly in the wavelet domain), several methods were proposed to tackle this problem by redundancy. Van der Auwera @cite used a bottom-up prediction algorithm for a bottom-up overcomplete discrete wavelet transform (ODWT). Park and Kim @cite proposed a low-band-shift method by constructing the wavelet tree by shifting low-band subband in each level for horizontal, vertical, and diagonal directions for one pixel and performing downsampling. Andreopoulos @cite defined a complete to overcomplete discrete wavelet transform (CODWT), which avoids inverse DWT generally used to obtain ODWT. More recently, Liu and Ngan @cite use partial distortion search and anisotropic double cross search algorithms with the MCTF method in @cite for a fast motion estimation. Amiot @cite perform MCTF for denoising, using dual-tree complex wavelet (DT-CW) coefficients.
- All MCTF methods summarized above perform motion estimation motion compensation either in the temporal domain before DWT, or in the wavelet domain with the help of redundancy (e.g. ODWT, DT-CW, etc.), due to the fact that complete DWT is shift-variant and motion estimation directly on DWT subbands is a challenging task. However, redundancy in these methods leads to high computational complexity @cite . Inspired by the fact that shift variance keeps the perfect reconstruction and nonredundancy properties of wavelets and breaks the coupling between spatial subbands, and that wavelet codecs always operate on complete DWT subbands @cite , we propose a novel in-band ME MC method, which avoids the need of shift invariance, and operates directly on the original DWT coefficients of the input sequences. Since Haar wavelets are widely utilized in MCTF methods due to the coding efficiency based on their short kernel filters @cite , our method is built on Haar subbands. For accurate ME MC, we define the exact relationships between the DWT subbands of input video sequences, which allows us to avoid upsampling, inverse DWT, redundancy, and interpolation for subpixel accuracy.
- Substance use disorder (SUD) encompasses a complex pattern of behaviors. Many studies have been conducted to discover factors interacting with SUD. A growing number of studies have confirmed a strong association between personal traits and substance use. For example, @cite found that smokers have significantly higher openness to experience and lower conscientiousness , a personality trait related to a tendency to show self-discipline, act dutifully, and aim for achievement. @cite examined the links between alcohol consumption and personality and found that alcohol use is correlated positively with sociability and extraversion. @cite conducted a study involving 1102 participants and found a link between drug use and low conscientiousness . @cite revealed risk factors related to addiction such as age, sex, impulsivity, sweet-liking, novelty reactivity, proclivity for exercise, and environmental impoverishment. Additionally, addiction is also linked to environmental and social factors such as neighborhood environment @cite , family environment @cite @cite and social norms @cite @cite .
- There are other ordinal techniques but which do not impose unimodal constraints. The proportional odds model (POM) and its neural network extensions (POMNN, CHNN @cite ) do not suffer from the monotonicity issue due to the utilization of monotonically increasing biases in the calculation of probabilities. The stick-breaking approach by , which is a reformulation of the multinomial logit (softmax), could also be used in the ordinal case as it technically imposes an ordering on classes.
- Furthermore, there are numerous scientific studies that have analyzed security (e. ,g., @cite @cite ) and privacy aspects (e. ,g., @cite ) of popular websites. These studies provide insights about the overall state of privacy and the adoption of security technologies on the web. However, as they typically focus on aggregate statistics, they do not create strong incentives for individual site owners to improve. Moreover, they rarely provide sector-specific insights ( @cite is one of the few exceptions). And finally, as the published data is typically not updated after the publication, the results become outdated rather quickly. This is also true for the 2016 municipality survey @cite of , that inspired us to create PrivacyScore.
- The most similar previous work is @cite . A notable difference is that @cite is designed for non-projective parsing, and retains tokens between the head and dependent after inducing a long arc. Moreover, limited by the sparse feature-based classifier used, direct attachments are only made with up to the top 3 tokens in the stack.
- The literature also considered random fault models. In the network reliability problem, the goal is to compute the probability that the (connected) input network becomes disconnected under random independent edge failures. The reliability of a network is the probability that the network remains connected after this random process. Karger @cite gave a fully polynomial randomized approximation scheme for the network reliability problem. Chechik et. al @cite studied a variant of the task, in which the goal is to compute a sparse sub-network that approximates the reliability of the input network. We, on the other hand, construct a reinforced network that increases the reliability of the input network; note also that our requirements are much stricter than merely preserving connectivity. For a more detailed survey on Fault-Tolerant Logical Network Structures, see @cite .
- Within the large body of research on NER which have been published in the last two decades, we identify two main approaches. The first approach is characterized by the use of well-established sequence labeling models such as conditional random field (CRF), hidden markov model, support vector machine, maximum entropy and so on. The performance of these models is heavily dependent on hand-crafted features. In particular, most of the participants at CoNLL-2003 shared task attempted to use information other than the available training data such as gazetteers and unannotated data. The best system at CoNLL-2003 shared task is the work of @cite which achieved an @math score of 88.76 them by using phrase features extracted from an external database. Moreover, training NER models jointly with related tasks helps improve their performance. For instance, @cite trained a CRF model for joint-learning three tasks, including coreference resolution, entity linking, and NER, and achieved the state-of-the-art result on OntoNotes dataset. With a similar approach, @cite gained the best performance on CoNLL-2003 shared task dataset.
- Our work is a direct extension of the d-vector model presented by @cite . The extension is two-fold: a CNN TDNN structure that emphasizes on temporal-frequency filtering, more resemble to the traditional feature engineering; an experiment on a text-independent task demonstrated that the learned feature is independent of linguistic content and highly speaker sensitive.
- This work is different from most of the existing neural-based ASV methods. For example, the RNN-based utterance-level representation learning @cite is attractive, but the RNN pooling shifts the focus to the entire sentence, rather than frame-level feature learning. The end-to-end neural models proposed by Snyder @cite and Zhang @cite both involve a back-end classifier, which weakens the feature learning component: it is unknown whether the speaker-discriminant information is learned by the classifier or by the feature extractor. Therefore, the features are not necessarily speaker discriminative and are less generalizable, as the feature extractor depends on the classifier.
- In the last few years, we have witnessed a renewed interest in continuous learning, both for supervised classification and reinforcement learning. Several interesting approaches have been proposed such as: Learning without Forgetting @cite , Progressive Neural Networks @cite , Active Long Term Memory Networks @cite , Adaptive Convolutional Neural Network @cite , PathNet @cite , Incremental Regularized Least Squares @cite , Elastic Weight Consolidation @cite , Encoder-based Lifelong Learning @cite , etc.
- In Table we compare datasets benchmarks which, in our opinion, could be used for continuous object recognition. Datasets where temporal coherent sequences are not available (or cannot be generated from static frames) are here excluded. In principle, such datasets could be used for continuous learning as well, but we think that temporally coherent sequences allow a larger number of real-world applications to be addressed (e.g., robotic vision scenario). YouTube-8M @cite provides a huge number of videos acquired in difficult natural settings. However, the classes are quite heterogeneous and acquisition conditions are completely uncontrolled in terms of object distance, pose, lighting, occlusions, etc. In other words, we believe it is too challenging for current continuous learning approaches (still in their infancy).
- Next we must address related work within the scheduling function. There are a number of papers that focus on comparing the queue management and scheduling algorithms within schedulers, including discussion of support for first-come, first-served (FCFS) [ @cite ], fairshare [ @cite ], backfill [ @cite , and gang scheduling [ @cite , @cite ]. These papers includes comparisons of how jobs are prioritized within queues and how jobs are matched to resources to most effectively execute them. These articles make comparisons among the various schedulers available within Apache Hadoop YARN [ @cite ]; within PBS [ @cite ], within the Maui scheduler [ @cite , and among several of the HPC schedulers [ @cite , @cite , @cite ].
- The publications that are covered in the previous paragraphs are all comparisons of actual schedulers as well as the application of queue management and scheduling algorithms on actual workload traces and simulated workload traces based on actual workload characteristics. However, another branch of job scheduler research has approached this topic from a mathematical and statistical operations research vector. Statistical distributions are applied to various aspects of workload characteristics and resource characteristics. First, a taxonomy of workloads, resources platforms, and mapping and scheduling heuristics is presented in @cite . Workload modeling is extensively treated in @cite , while @cite covers a very large set of scheduling algorithms. An extensive study and comparison of mapping and scheduling algorithms was reported in @cite , with refinements in @cite . Further refinements of the underlying statistical models and parameter set generation was published in @cite and @cite .
- Action recognition has been studied extensively in recent years and readers can refer to @cite @cite @cite for good surveys. Here, we only cover the work related to our methods.
- Hand-crafted features . In recent years, researchers have developed many different spatio-temporal feature detectors for video, such as 3D-Harris @cite , 3D-Hessian @cite , Cuboids @cite , Dense Trajectories @cite , Improved Trajectories @cite . Usually, a local 3D-region is extracted around the interest points or trajectories, and a histogram descriptor is computed to capture the appearance and motion information, such as Histogram of Gradient and Histogram of Flow (HOG HOF) @cite , Histogram of Motion Boundary (MBH) @cite , 3D Histogram of Gradient (HOG3D) @cite , Extended SURF (ESURF) @cite , and so on. Then encoding methods are employed to aggregate these local descriptors into a global representation, and typical encoding methods include Bag of Visual Words (BoVW) @cite , Fisher vector (FV) @cite , Vector of Locally Aggregated Descriptors (VLAD) @cite , and Multi-View Super Vector (MVSV) @cite . These local features share the merits of locality and simplicity, but may lack semantic and discriminative capacity.
- To overcome the limitation of local descriptors, several mid-level representations have been proposed for action recognition @cite @cite @cite @cite @cite @cite @cite . Raptis @cite grouped similar trajectories into clusters, each of which was regarded as an action part. Jain @cite extended the idea of discriminative patches into videos and proposed discriminative spatio-temporal patches for representing videos. Zhang @cite proposed to discover a set of mid-level patches in a strongly-supervised manner. Similar to 2-D poselet @cite , they tightly clustered action parts using human joint labeling, dubbed . Wang @cite proposed a data-driven approach to discover those effective parts with high motion salience, known as motionlet . Zhu @cite proposed a two-layer representation for action recognition. The weakly-supervised actons were learned via a max-margin multi-channel multiple instance learning framework. Wang @cite proposed a multiple level representation called as MoFAP by concatenating motion features, atoms, and phrases. Sadanand @cite presented a high-level video representation called as Action Bank by using a set action templates to describe the video content. In summary, these mid-level representations have the merits of representative and discriminative power, but still depends on the low-level hand-crafted features.
- Deeply-learned features . Several works have been trying to learn deep features and design effective ConvNet architectures for action recognition in videos @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . Karpathy @cite first tested ConvNets with deep structures on a large dataset (Sports-1M). Simonyan @cite designed two-stream ConvNets containing spatial and temporal nets by exploiting ImageNet dataset for pre-training and calculating optical flow to explicitly capture motion information. Tran @cite explored 3D ConvNets @cite on the realistic and large-scale video datasets, where they tried to learn spatio-temporal features with the operations of 3D convolution and pooling. Sun @cite proposed a factorized spatio-temporal ConvNets and exploited different ways to decompose 3D convolutional kernels. Wang @cite proposed a hybrid representation by using trajectory-pooled deep-convolutional descriptors (TDD), which share the merits of improved trajectories @cite and two-stream ConvNets @cite . Feichtenhofer @cite further extended the two-stream ConvNets with convolutional fusion of two streams. Several works @cite @cite @cite tried to use recurrent neural networks (RNN), in particular LSTM, to model the temporal evolution of frame features for action recognition in videos.
- Many research works have been devoted to modeling the temporal structure of video for action recognition @cite @cite @cite @cite @cite @cite . Gaidon @cite annotated each atomic action for each video and proposed Actom Sequence Model (ASM) for action detection. Niebles @cite proposed to use latent variables to model the temporal decomposition of complex actions, and resorted to the Latent SVM @cite to learn the model parameters in an iterative approach. Wang @cite and Pirsiavash @cite extended the temporal decomposition of complex action into a hierarchical manner using Latent Hierarchical Model (LHM) and Segmental Grammar Model (SGM), respectively. Wang @cite designed a sequential skeleton model (SSM) to capture the relations among dynamic-poselets, and performed spatio-temporal action detection. Fernando @cite modeled the temporal evolution of BoVW representations for action recognition.
- Comparing the performance of machine learning systems with human subjects has attracted interest because it may give insights on how machine learning systems can be improved. Borji and Itti @cite compare human classification accuracy and the accuracy of several machine learning algorithms on several datasets. The study considers images as well as line drawings and jumbled images. The study, however, does not test distorted images, and does not include deep learning algorithms.
- Fleuret al @cite test human and computer performance on a range of synthetic classification tasks. The tasks are binary classification experiments on synthetic images where the difference between the two classes is determined by the spatial arrangement of the constituent parts. Even though these tasks are simple, machine performance is still well below human performance. Stabinger al @cite test modern deep networks on these same classification tasks, but find that the networks can still not achieve human accuracy. While these tests are interesting, the experiments do not represent real data that is likely to occur in most applications.
- Parikh @cite tested human and computer performance on jumbled images. Jumbled images are constructed by splitting the image into patches and randomly permuting the patches. Global information is lost with jumbling and only local information can be used for classification. The jumbled image can be thought as a distortion of the original image, but it is not a distortion that occurs naturally due to data acquisition or transmission.
- Given that DNN performance now can match or exceed human performance, we can begin to examine more difficult problems. Humans have the capacity to recognize severely distorted images. For example, Torralba al @cite show that human subjects can accurately recognize low resolution images. Similarly, Bachmann @cite show that human subjects can recognize low resolution faces. Chen al @cite show the effect of noise on a face recognition task. Performance is impaired, but subjects are still able to perform the task with reasonable accuracy. Given that humans can recognize to some degree under quality distortions, it is interesting if DNNs can show the same ability.
- However, it has recently been shown that deep networks do not achieve good performance on distorted images @cite . Noise and blur were shown to affect the DNNs the most compared with other distortions.
- It is not surprising that deep networks trained on clean images perform poorly on distorted images. DNNs are optimized using the statistics of a collection of images, and when the statistics in the testing stage are very different, the model cannot generalize well. Most efforts to add robustness involve fine-tuning DNNs on distorted images. Vasiljevic al @cite show that fine-tuning yields improvements for blurred images. Similarly, Zhou al @cite show that this approach also works for images distorted with noise. However, Dodge and Karam @cite show that models fine-tuned on one type of distortion do not easily generalize to other types of distortions. They show how a gating network can be used to determine the distortion type and level of an input, and pass the data to an appropriately trained network. Finally, the dirty pixel approach @cite shows that fine-tuning with an additional pre-processing module can yield a DNN more robust to blur and noise.
- There have been several papers replacing pieces of the traditional speaker recognition system with DNNs. One approach is to train a GMM on bottleneck features extracted from a DNN, and then extract i-vectors @cite . Another DNN-based approach uses an acoustic speech recognition DNN instead of a UBM-GMM to produce frame posteriors for i-vector computation @cite . Ehsan Variani @cite trained DNNs to classify speakers with frame-level acoustic features. The activations of the final hidden layer are averaged over the utterance to create a d-vector'' which replaces the i-vector. These approaches all show improvements upon the traditional i-vector baseline.
- Zwick @cite survey adopts a theoretical stand-point with regards to the exact and approximate shortest paths algorithms. Zwick's survey addresses single-source shortest-path (SSSP), all pairs shortest-path (APSP), spanners (a weighted graph variation), and distance oracles. The survey illustrates the various variations that each category adopts when handling negative and non-negative edge weights as well as directed and undirected graphs. Sen @cite surveys approximate shortest-paths algorithms with a focus on spanners and distance oracles. Sen's survey discusses how spanners and distance oracles algorithms are constructed and their practical applicability over a static all-pairs shortest-paths setting. Sommer @cite surveys query processing algorithms that trade-off the index size and the query time. Sommer's survey also introduce the transportation network class of algorithms, and include algorithms for general graphs as well as planar and complex graphs.
- Demetrescu and Italiano @cite survey algorithms that investigate fully dynamic directed graphs with emphasis on dynamic shortest-paths and dynamic transitive closures. The survey focuses on defining the algebraic and combinatorial properties as well as tools for dynamic techniques. The survey tackles two important questions, namely whether dynamic shortest-paths achieve a space complexity of @math , and whether single-source shortest path algorithms in a fully-dynamic setting be solved efficiently over general graphs. Nannicini and Liberti @cite survey techniques for dynamic graph weights and dynamic graph topology. They list classical and recent techniques for finding trees and shortest-paths in large graphs with dynamic weights. They target two versions of the problem, namely, time-dependence, and what they refer to as cost updates of the weights. Dean's survey @cite focuses on time-dependent techniques in a dynamic setting. It surveys one special case, namely, the First-In-First-Out (FIFO) network as it exposes structural properties that allow for the development of efficient polynomial-time algorithms.
- The field of fast Approximate Nearest Neighbor (ANN) search has been greatly advanced due to the development of hashing technique, especially those based on deep CNN. For the non-deep hashing methods, the hash code generation process has two stages. First, the image is represented by a vector of hand-crafted visual features (such as Gist descriptor). Then, separate projection or quantization step is used to generate hash codes. Unsupervised and supervised hashing are two main streams, such as Spectral Hashing (SH) @cite , Iterative Quantization (ITQ) @cite , Semi-supervised Hashing (SSH) @cite , Minimal Loss Hashing (MLH) @cite , Robust Discrete Spectral Hashing (RDSH) @cite , Zero-shot Hashing (ZSH) @cite and Kernel Supervised Hashing (KSH) @cite . However, hashing methods based on hand-crafted features may not be effective in dealing with the complex semantic structure of images, thus producing sub-optimal hash codes.
- A comprehensive review on argumentation corpora is beyond the scope of this paper; good overviews can be found in e.g. @cite @cite @cite . Here we only review argumentation corpora constructed from customer reviews. @cite built the corpus, consisting of 2.1k hotel reviews posted on Tripadvisor.com. Instead of directly labelling arguments, they annotate and sentiments polarities. A statement is ''. They designed a rule-based tool to segment statements, and employed crowdsourcing to annotate the sentiment and (e.g. location, services, facilities) in each statement. Results suggested that crowdsourcing workers can reliably identify the sentiments of statements (approval rate 72.8 (rejection rate 43.3 view ArguAna as an intermediate resource for building argumentation corpora, because each argument usually contains several statements (e.g. one positive negative statement serving as the claim, and several neutral statements serving as premises). Thus, ArguAna cannot be directly used for training argumentation mining techniques.
- Crowdsourcing has been used to annotate discourse structures. @cite designed a two-stage crowdsourcing mechanism to annotate two levels of discourse relations in Japanese texts crawled from multiple online genres. In their work, discourse relations include contrast, concession, cause-effect, etc., and these relations closely resemble some relations in argumentation structures (e.g. the attack relation between arguments can be viewed as contrast, and the premise-claim relation is closely related to the cause-effect relation). They did not report the IRA of the crowdsourcing workers, but instead, testified the quality of their discourse corpus by training a discourse parser on their corpus. Results suggested that the quality of their corpus is comparable to the state-of-the-art English discourse corpus, indicating that crowdsourcing can be reliably used for annotating relations between clauses.
- Similar to object proposal generation, temporal proposal generation aims to produce class-agnostic proposals efficiently and accurately. Sparse-prop @cite presented a method that use STIPs @cite and dictionary learning for class-independent proposal generation. SCNN-prop @cite presented a method that fine-tunes 3D convolutional network @cite for binary proposal classification. DAPs @cite used LSTM networks to encode a video stream and produce proposals inside the video stream. Gao @cite proposed a method, called TURN, to use unit-level temporal coordinate regression to refine the temporal action boundary.
- is a challenging task for machines, requiring both understanding of natural language and knowledge of the world @cite . Recently many new datasets have been released and in most of these datasets, the questions are generated in a synthetic way. For example, bAbI @cite is a fully synthetic dataset featuring 20 different tasks. released a corpus of cloze style questions by replacing entities with placeholders in abstractive summaries of CNN Daily Mail news articles. claim that the CNN Daily Mail dataset is easier than previously thought, and their system almost reaches the ceiling performance. curated MCTest, in which crowdworker questions are paired with four answer choices. Although MCTest contains challenging natural questions, it is too small for training data-demanding question answering models.
- There exists a large body of literature on overheads and sources of unpredictable performance in cloud applications. For example, several studies have reported on the significant variance of the bandwidth available to tenants in the absence of network virtualization: the bandwidth may very by a factor of five or more @cite , even within the same day. Given the time spent in network activity by these applications, this variability has a non-negligible impact on the application performance, which makes it impossible for tenants to accurately estimate the execution time in advance. Accordingly, over the last years, many network virtualization architectures and prototypes have been proposed, leveraging admission control and bandwidth reservations and enabling tenants to specify absolute guarantees @cite @cite @cite @cite @cite @cite @cite .
- Neural conversation models are a family of neural architectures (generally based on deep convolutional or recurrent networks) used to represent mappings between dialogue contexts (or queries) and possible responses. Compared to previous statistical approaches to dialogue modelling based on Markov processes @cite @cite @cite , one benefit of these neural models is their ability to be estimated from raw dialogue corpora, without having to rely on additional annotation layers for intermediate representations such as state variables or dialogue acts. Rather, neural conversation models automatically derive latent representations of the dialogue state based on the observed utterances.
- Dialogue is a sequential decision-making process where the conversational actions of each participant influence not only the current turn but the long-term evolution of the dialogue @cite . To incorporate the prediction of future outcomes in the generation process, several papers have explored the use of reinforcement learning techniques, using deep neural networks to model the expected future reward @cite @cite . In particular, the Hybrid Code Networks model of @cite demonstrate how a mixture of supervised learning, reinforcement learning and domain-specific knowledge can be used to optimise dialogue strategies from limited amount of training data.
- Before deep learning techniques became mainstream within the field of computer vision @cite , most methods relied on the classical two-stage approach of designing fixed handcrafted features such as SIFT @cite or LBP @cite , and then training unrelated classifiers for recognition @cite @cite @cite @cite @cite @cite @cite . However, similarly to AUNets, the best performing techniques currently available @cite @cite @cite @cite rely on the power of deep convolutional neural networks for joint representation and classification.
- Most recent methods @cite @cite @cite @cite @cite @cite @cite @cite @cite follow the paradigm of first detecting facial landmarks using external approaches such as Active Appearance Models @cite , either to treat these keypoints as anchors for extracting rectangular regions for further analysis, to perform face alignment, or both. The recent method of Li al @cite does not require robust facial keypoint alignment as it is trained taking this issue into account; however, facial alignment is recommended and the method is developed and tested only in a frontal face setup. In contrast, AUNets operate on the whole face and do not require any particular alignment in existing AU detection multi-view benchmarks.
- Pioneering methods for this task used the whole face image as input @cite @cite @cite . However, the trend reversed towards analyzing patches in more local approaches such as @cite @cite @cite @cite @cite @cite . State-of-the-art techniques @cite @cite @cite join AUNets in returning to a holistic face analysis, in particular Li al @cite shares ideas with Zhao al @cite in forcing a CNN-based approach to specifically focusing on specific regions of the face by using a map saliency. Our method is far from these approaches since we train specific networks for each AU which avoids the need of building attention maps.
- Given that groups of action units can co-occur or be mutually exclusive in the human face, several methods @cite @cite @cite @cite have approached the task as multi-label learning. However, as AUs databases are becoming larger and better annotated @cite @cite , these methods need to be completely retrained to integrate new action units. In contrast, AUNets are modular by design, and can naturally evolve towards more general datasets and new action units. Furthermore, by analyzing the whole face, AUNets learn naturally the relevant local face regions for predicting each action unit.
- While most methods operate on single RGB images both at training and testing @cite @cite @cite @cite , some techniques focus on exploiting the temporal information for improved AU detection @cite @cite @cite . In particular, Jaiswal al @cite and Chu al @cite use CNN's and Bidirectional Long Short-Term Memory to model time dependencies. Similarly, Liu al @cite tackle the temporal information by taking advantage of the Optical Flow @cite ; for this purpose, starting from the OF they extract hand-crafted features as histogram oriented @cite in order to train a classifier. However, our method takes advantage of the full resolution of the OF by feeding it into a CNN. Moreover, on behalf of the Optical Flow, we perform simple statistical operations to smooth the temporal flow as a post-processing step.
- A model based on bidirectional LSTM encoding of tweets conditioned on targets has been shown to achieve state-of-the-art on the SemEval-2016 task 6 dataset @cite . However the RumourEval task is different as it addresses conversation threads. and consider the sequential nature of tweet threads in their works. employ Hawkes processes to classify temporal sequences of tweets. They show the importance of using both the textual content and temporal information about the tweets, disregarding the discourse structure. model the conversational structure of source tweets and subsequent replies: as a linear chain and as a tree. They use linear- and tree- versions of a CRF classifier, outperforming the approach by .
- The RS effect has been dealt with in several computer vision problems, such as Perspective-n-Point (PnP) problem that requires 3D point clouds generated by a GS camera. Ait-Aider al estimated the pose and velocity of fast moving objects in a single image with a rolling shutter camera @cite . The nonlinear and linear models were proposed for general and planar objects, respectively. Magerand al extended this work with a polynomial rolling shutter model and the constrained global optimization @cite @cite . Albl al proposed a double linearized rolling shutter for an efficient estimation @cite . This work remarkably increased the accuracy of motion estimation and the number of inliers.
- Such approaches are able to discover homonymous senses of words, e.g., bank'' as slope versus bank'' as organisation @cite . However, as the graphs are usually composed of semantically related words obtained using distributional methods @cite @cite , the resulting clusters by no means can be considered synsets. Namely, (1) they contain words related not only via synonymy relation, but via a mixture of relations such as synonymy, hypernymy, co-hyponymy, antonymy, etc. @cite @cite ; (2) clusters are not unique, i.e., one word can occur in clusters of different ego networks referring to the same sense, while in WordNet a word sense occurs only in a single synset.
- @cite is a clustering algorithm that was used to induce synsets for a Portuguese WordNet from several available synonymy dictionaries. The algorithm starts by adding random noise to edge weights. Then, the approach applies Markov Clustering (see below) of this graph several times to estimate the probability of each word pair being in the same synset. Finally, candidate pairs over a certain threshold are added to output synsets.
- @cite is a clustering algorithm particularly designed for the word sense induction task. In a nutshell, pairs of nodes are grouped if they have a maximal mutual affinity. The algorithm starts by converting the undirected input graph into a directed graph by keeping the maximal affinity nodes of each node. Next, all nodes are marked as root nodes. Finally, for each root node, the following procedure is repeated: all transitive children of this root form a cluster and the root are marked as non-root nodes; a root node together with all its transitive children form a fuzzy cluster.
- (CW) @cite is a clustering algorithm for weighted graphs that can be considered as a special case of MCL with a simplified class update step. At each iteration, the labels of all the nodes are updated according to the majority labels among the neighboring nodes. The algorithm has a meta-parameter that controls graph weights that can be set to three values: (1) sums over the neighborhood's classes; (2) downgrades the influence of a neighboring node by its degree or by (3) of its degree.
- (CPM) @cite is a clustering algorithm for unweighted graphs that builds up clusters from @math -cliques corresponding to fully connected sub-graphs of @math nodes. While this method is only commonly used in social network analysis, we decided to add it to the comparison as synsets are essentially cliques of synonyms, which makes it natural to apply an algorithm based on clique detection.
- This section describes the different sensor fusion and adaptive sensor fusion approaches, from general algorithms tailored for fusing sensor measurements to more specific algorithms used in computer vision available in the literature. This overview covers some of the latest sensor fusion mechanisms mentioned in @cite , computer vision benchmarks such as @cite and the performance evaluation of some vision-based trackers @cite .
- An adaptive fusion approach with a hierarchical architecture was recently proposed that not only adapts but also encodes information from the performance of the sensors @cite . Although this approach is widely used for model regression and classification, training could leave unexplored regions, causing the resulting output to suffer from outlying data. In addition, depending on the selection of experts, the gating network and the inference model, the overall system cannot be applied in real time applications @cite .
- While adaptive data fusion has been well studied and established for multi-sensor measurements in general @cite , researchers in the computer vision community have gone towards machine learning techniques to incorporate multiple image characteristics into tracking algorithms. Methods such as PROST @cite , VTD @cite , CMT @cite , Struck @cite , or the well known TLD @cite and its variants @cite @cite fit this framework. However, the aforementioned algorithms provide limited mechanisms to incorporate multiple and complementary feature extraction methods, thereby restricting their practical applicability.
- This work aims to create a general Bayesian approach for real-time applications in robotic platforms. The proposed method processes the bounding boxes of the trackers detectors as sensor measurements. This framework is founded in the basis of the bank of KFs with some similarities with the mixture of experts aforecited. Furthermore, this scheme addresses some common problems such as data imperfection, outliers and spurious data, measurement delays, static vs. dynamic phenomena, and others discussed in @cite . Additionally, this approach was tested in simulated signals and two different robotics platforms: An UAV system and a pan-tilt system. Both are capable of following a target. While similar approaches have used vision-based trackers to control a small UAV in @cite @cite and @cite . Previous works did not consider the fusion of several methods at a bounding box level to improve reliability over longer time spans.
- From the perspective of the Collective Classification literature @cite , specifically on the inference side, our method is inspired by @cite @cite . This is a type of inference that seeks to identify and exploit the more certain relational information. Such a cautious approach was used in @cite for labeling object superpixels. In @cite , discriminative relations were mined between object regions and discriminative attributes were discovered per relation. In @cite , relations between objects were considered to improve object detection by penalizing out-of-context object hypotheses. During inference, the object hypotheses with highest certainty are classified first and then used to bootstrap the other objects. Based on their experiments, it was concluded that all cases following this cautious iterative approach resulted in better object detection performance than when considering the neighboring objects at once. In this work, we start from the observations of @cite , and explore a cautious counterpart of @cite with the objective of verifying whether the observations made for object detection also hold for the task of object viewpoint estimation. To this end, we first estimate the viewpoint of the objects with higher certainty and then use these objects to predict the viewpoints of the other ones.
- To some extent, the proposed method bears some resemblance to the message-passing algorithms @cite commonly used for inference in graphical models. In the same fashion as the message-passing algorithms, we start from input elements for which information is available, in this case the objects that define the nodes in the graph. Then, we iteratively select a target node, in our case each of the object hypotheses to be re-estimated, and each of the neighboring nodes (objects) casts a vote (or sends a message) indicating the level of agreement they have with the target node taking a specific state. Furthermore, similar to message-passing algorithms, for the case of loopy graphs the proposed method aims at providing an approximate solution to the problem at a relative low computation time. Different from message-passing algorithms, which operate over a graph defined over two types of nodes (variable nodes and check nodes), our method only considers variable nodes.
- We additionally modify the representation of the assembled module networks themselves: where @cite and @cite parameterized individual modules with a fixed embedding supplied by the parser, here we these parameters jointly with network structures using a soft attention mechanism. This parameterization resembles the approach used in the compositional modular network'' architecture @cite for grounding referential expressions. However, the model proposed in @cite is restricted to a fixed layout structure of (subject, relationship, object) for every referential expression, and includes no structure search.
- Learning network architectures. More generally than these dynamic modular approaches, a long line of research focuses on generic methods for automatically discovering neural network architectures from data. Past work includes techniques for optimizing over the space of architectures using evolutionary algorithms @cite @cite , Bayesian methods @cite , and reinforcement learning @cite . The last of these is most closely related to our approach in this paper: both learn a controller RNN to output a network structure, train a neural network with the generated structure, and use the accuracy of the generated network to optimize the controller RNN. A key difference between @cite and the layout policy optimization in our work is that @cite learns a layout (network architecture) that is applied to every instance, while our model learns a layout policy that predicts a specific layout tailored to each individual input example.
- Visual question answering. The visual question answering task @cite is generally motivated as a test to measure the capacity of deep models to about linguistic and visual inputs jointly @cite . Recent years have seen a proliferation of datasets @cite @cite and approaches, including models based on differentiable memory @cite @cite , dynamic prediction of question-specific computations @cite @cite , and core improvements to the implementation of the multi-modal representation and attention mechanism @cite @cite . Together, these approaches have produced substantial gains over the initial baseline results published with the first VQA datasets.
- It has been less clear, however, that these improvements correspond to an improvement in the abilities of models. Recent work has found that it is possible to do quite well on many visual QA problems by simply memorizing statistics about question answer pairs @cite (suggesting that limited visual reasoning is involved), and that models with bag-of-words text representations perform competitively against more sophisticated approaches @cite (suggesting that limited linguistic compositionality is involved). To address this concern, newer visual question answering datasets have focused on exploring specific phenomena in compositionality and generalization; examples include the dataset @cite , the VQAv2 dataset @cite , and the dataset @cite . The last of these appears to present the greatest challenges to standard VQA approaches and the hardest reasoning problems in general.
- Most previous work on this task other than NMN uses a fixed inference structure to answer every question. However, the optimal reasoning procedure may vary greatly from question to question, so it is desirable to have inference structures that are specific to the input question. Concurrent with our work, @cite proposes a similar model to ours. Our model is different from @cite in that we use a set of specialized modules with soft attention mechanism to provide textual parameters for each module, while @cite uses a generic module implementation with textual parameters hard-coded in module instantiation.
- Finally, there are two general lines of research on paraphrasing not focused on using crowds. The first of these is the automatic collection of paraphrases from parallel data sources, such as translations of the same text or captions for the same image @cite @cite @cite @cite . These resources are extremely large, but usually (1) do not provide the strong semantic equivalence we are interested in, and (2) focus on phrases rather than complete sentences. The second line of work explores the creation of lattices that compactly encode hundreds of thousands of paraphrases @cite @cite . Unfortunately, these lattices are typically expensive to produce, taking experts one to three hours per sentence.
- Prior to SNLI, there has been work in cross-lingual textual entailment using parallel corpora @cite and lexical resources @cite , or crowdsourcing for multilingual training data by . We also note two shared tasks, on cross-lingual entailment with five languages @cite and English relatedness and inference @cite .
- SNLI is the first large-scale dataset for NLI in English @cite , two orders of magnitude larger than any predecessor. It was recently expanded with test data for multiple genres of English to allow for cross-domain evaluation. https: www.nyu.edu projects bowman multinli Prior to our work, there have been no SNLI-style cross-lingual methods or evaluations.
- After an exhaustive search, we have not been able to find any work that characterizes performance of hot spots in cellular networks using real data. The only exception could be the work by Nika et al. @cite , where a data set with more than 700K users is analyzed to understand temporal characteristics of data hot spots. Predictability is assessed, but considering only hot spot repetitions at the exact time and location in the forthcoming week. However, we find that the focus of the work is not on performance, but on load, with hot spots being defined on the basis of relative daily cell traffic.
- A few further research papers have been written in an attempt to understand how load evolves in cellular networks. For instance, Paul et al. @cite analyzed the spatio-temporal behavior of network resource usage in a real 3G network. Interestingly, their results indicate that aggregate network usage exhibits periodic behavior, but individual sectors do not exhibit such properties (however, in @cite , Jiang et al. do find such correlations in KPIs). The usage of individual applications in cellular data networks has been studied by @cite . Finally, simulation has been also used to quantify hot spots in wireless cellular networks @cite . None of the previous works considers real performance hot spots nor provides any forecasting methodology.
- In terms of forecasting, we find that, over the past years, there have been some attempts to predict network performance based on historical network measurements. The network weather service @cite was one of the first systems to make predictions of TCP IP throughput and latency. Another example is the Proteus @cite system, which uses regression trees to forecast short-term performance in cellular networks, with the objective to proactively adapt mobile application network patterns. Similarly, Sprout @cite attempts to predict immediate network load in cellular networks to optimize TCP-layer parameters. Finally, in @cite , the authors use gradient boosted trees to forecast hot spots within a data center, in order to avoid stranglers and, therefore, speed up map-reduce computation. In general, all these works focus on very short-term forecasts (in the order of seconds or, at most, minutes) and, consequently, they consider data sets spanning only very short time periods. Further approaches are either not applied to cellular networks or do not consider performance hot spots.
- The energy dissipation rate is a fundamental statistic in experimental and theoretical studies of turbulence, e.g., Sreenivasan @cite , Frisch @cite . In 1968, Saffman @cite , addressing the estimate of energy dissipation rates, @math , wrote that In 1992 Constantin and Doering @cite made a fundamental breakthrough, establishing a direct link between the phenomenology of energy dissipation and that predicted for general weak solutions of shear flows directly from the NSE. This work builds on Busse @cite , Howard @cite (and others) and has developed in many important directions. It has been extended to shear flows in Childress, Kerswell and Gilbert @cite , Kerswell @cite and Wang @cite . For flows driven by body forces extensions include Doering and Foias @cite , Cheskidov, Doering and Petrov @cite (fractal body forces), and @cite (helicity dissipation). Energy dissipation in models and regularizations studied in @cite , @cite , @cite , @cite .
- As for the first strategy, it mainly uses linear function to project the multimedia data into one common space. For example, a straightforward method is to adopt Canonical Correlation Analysis (CCA) @cite , which is a traditional statistical correlation analysis method, to project the features of different media types into a lower-dimensional common space. Given the training pairs, CCA can find matrices for them, which can make the projected training pairs have maximum correlations with the same dimension to obtain the shared representation. Thus, the simple similarity measurement can be adopted to present cross-media retrieval. Some later works attempt to combine semantic categories to extend CCA, such as @cite . Besides, Cross-modal Factor Analysis (CFA) @cite minimizes the Frobenius norm between pairwise data in the transformed domain to learn a common space for different modalities. The joint graph regularized heterogeneous metric learning (JGRHML) @cite is proposed by to construct the joint graph regularization term using the data in the learned metric space, and this work is further improved as the joint representation learning (JRL) @cite by modeling the correlations and semantic information in a unified framework.
- In the second strategy, DNN is used to model the correlation between different media types. The strong ability of DNN shown in feature representation learning significantly contributes to the research of single-media retrieval and classification. Some general models such as Stacked Autoencoders (SAE) @cite , Deep Belief Network (DBN) @cite and Deep Boltzmann Machines (DBM) @cite are proposed with their learning algorithms for the feature representation learning. Inspired by these, researchers attempt to apply DNN to cross-media retrieval. Bimodal Autoencoders (Bimodal AE) @cite proposed by generates the shared representation at the shared code layer taking the speech and visual as input, and it also has a reconstruction layer to reconstruct both two media types. Srivastava and Salakhutdinov propose multimodal DBN @cite and multimodal DBM @cite to capture the inter-media correlation between different media types at the joint layer. The architectures of Deep CCA @cite @cite and Corr-AE @cite are similar, consisting of two linked deep encodings. Deep CCA is a non-linear extension of CCA, while Corr-AE can jointly minimize representation learning error and correlation learning error meanwhile with two extensions, namely Corr-Cross-AE and Corr-Full-AE. Besides, CMDN @cite preforms the shared representation learning with multiple deep networks.
- Imitation learning is primarily concerned with matching the performance of the demonstrator. One popular algorithm, DAGGER @cite , iteratively produces new policies based on polling the expert policy outside its original state space, showing that this leads to no-regret over validation data in the online learning sense. DAGGER requires the expert to be available during training to provide additional feedback to the agent. In addition, it does not combine imitation with reinforcement learning, meaning it can never learn to improve beyond the expert as DQfD can.
- Deeply AggreVaTeD @cite extends DAGGER to work with deep neural networks and continuous action spaces. Not only does it require an always available expert like DAGGER does, the expert must provide a value function in addition to actions. Similar to DAGGER, Deeply AggreVaTeD only does imitation learning and cannot learn to improve upon the expert.
- Another popular paradigm is to setup a zero-sum game where the learner chooses a policy and the adversary chooses a reward function @cite @cite @cite . Demonstrations have also been used for inverse optimal control in high-dimensional, continuous robotic control problems @cite . However, these approaches only do imitation learning and do not allow for learning from task rewards.
- Recently, demonstration data has been shown to help in difficult exploration problems in RL @cite . There has also been recent interest in this combined imitation and RL problem. For example, the HAT algorithm transfers knowledge directly from human policies @cite . Follow-ups to this work showed how expert advice or demonstrations can be used to shape rewards in the RL problem @cite @cite . A different approach is to shape the policy that is used to sample experience @cite , or to use policy iteration from demonstrations @cite @cite .
- AlphaGo @cite takes a similar approach to our work in pre-training from demonstration data before interacting with the real task. AlphaGo first trains a policy network from a dataset of 30 million expert actions, using supervised learning to predict the actions taken by experts. It then uses this as a starting point to apply policy gradient updates during self-play, combined with planning rollouts. Here, we do not have a model available for planning, so we focus on the model-free Q-learning case.
- The motivation is two fold: Combining information encoded in multiple observations has the potential to dramatically increase the discriminative power of the coefficient matrix since we can obtain a consensus, which should be more robust to outliers and noise. We demonstrate this in our later experiments. Proliferation of sensors and reduced cost. Multi-sensor data is becoming increasingly common. For example in remote sensing, imagery is often captured from multiple sensors, each dedicated to particular frequency ranges of the electromagnetic spectrum @cite @cite and in human activity recognition there has been a surge in the use of RGB-Depth cameras @cite @cite . Moreover, single observations can be turned into multiple tertiary observations. For example many image classification and recognition algorithms use many image features is used to represent the original image @cite @cite @cite . Each of these features is actually a new observation or appearance. See Figure for an example.
- The work most closely related and which inspires our work is the so called Multi-Task Low-rank Affinity Pursuit (MLAP) @cite . MLAP adopts the low-rank representation model for each individual observation and fuses them together by seeking sparsity consistent low-rank coefficients. The MLAP objective is the following where @math , @math and @math are observation data, the fitting error and coefficient matrices for the @math th observation respectively, @math is the number of observations, the @math norm is defined as @math , @math and @math is the vectorisation operator such that @math .
- Word representations based on distributional semantics have been common @cite @cite . The distributional methods typically begin by constructing a word-context matrix and then applying dimension reduction techniques such as SVD to obtain high-quality word meaning representations. Although some investigated incremental updating of the word-context matrix @cite @cite , they did not explore the reduced representations. On the other hand, neural word embeddings have recently gained much popularity as an alternative. However, most previous studies have not explored incremental strategies @cite @cite @cite .
- Very recently, proposed an incremental learning method of hierarchical soft-max. Because hierarchical soft-max and negative sampling have different advantages @cite , the incremental SGNS and their method are complementary to each other. Also, their updating method needs to scan not only new but also old training data, and thus is not an incremental algorithm in a strict sense. As a consequence, it potentially incurs the same time complexity as the re-training. Another consequence is that their method has to retain the old training data and thus wastes space, while incremental SGNS can discard old training examples after processing them.
- Reflection symmetry algorithms fall into two different categories depending on whether they detect sparse symmetries (straight lines or curves) @cite @cite @cite @cite @cite @cite or a dense heatmap @cite @cite @cite . The most common sparse approach to detect reflection symmetry is to match up symmetric points or contours in the image to determine midpoint and direction of the symmetry axis @cite @cite @cite @cite @cite @cite . These approaches often use a Hough transform to accumulate the axes of reflection, derived from the matched feature's midpoints and angles, and vote on the dominant symmetries. Atadjanov and Lee @cite extend the Loy and Eklundh @cite algorithm by taking the matched keypoints and then comparing the histogram of curvature around the keypoints. Wang al @cite uses local affine invariant edge correspondences to make the algorithms more resilient to perspective distortion contours. The method does not use a Hough space to vote, opting instead to use an affine invariant pairwise (dis)similarity metric to vote for symmetries.
- Pritts al @cite detect reflection, rotation and translation symmetry using SIFT and MSER features. The symmetries are found through non-linear optimization and RANSAC. Tuytelaars al @cite detect reflection through a Cascade Hough Transform. Kiryati and Gofman @cite define a Symmetry ID function implemented through Gaussian windows to find local reflection symmetry.
- There have been some shallow-network reflection detection approaches (well before the current deep learning craze). Zielke al @cite use a static feed forward method to enhance the symmetric edges for detection. The max operation between the different orientations is similar to other voting systems @cite . Fukushima and Kikuchi @cite @cite present another neural network method for detecting reflection symmetry around the center of an image. They use a 4-layer network to find the symmetry axis. Skeletonization , a related problem to reflection detection, has attracted a lot of attention recently @cite @cite @cite @cite . Shen al @cite use a deep CNN to learn symmetry at multiple scales and fuse the final output together. The network needs object skeleton ground-truth for the particular scale of the objects. The network outputs a skeleton heatmap which is thresholded to produce a binary image denoting the detected skeletons.
- Earlier work on rotation symmetry detection includes the use of autocorrelation @cite @cite and image moments @cite @cite @cite . Loy and Eklundh @cite use a variation on their SIFT feature-based reflection symmetry approach to find rotation symmetry as well. The orientations of matched SIFT feature pairs are used to find a rotation symmetry center. The detected rotation symmetry centers emerge as maxima in the voting space. This algorithm stands out from all others since the authors have made their code publicly available, and the symmetry competition workshops in CVPR 2011 2013 have used it as the baseline algorithm for both reflection and rotation symmetry detection. Thus far, this algorithm is considered to be the best baseline algorithm for reflection and rotation symmetry detection.
- Lee and Liu @cite @cite @cite have developed an algorithm to detect (1) the center of rotation, (2) the number of folds, (3) type of symmetry group (dihedral cyclic O(2)), and (4) the region of support. The first step of their algorithm is rotation symmetry center detection where they use a novel frieze expansion to transform the image at each pixel location into polar coordinates and then search for translation symmetry. The second step applies a Discrete Fourier Transform (DFT) on the frieze expansion to determine (2)-(4) listed above. In our work, for rotation symmetries we only focus on detecting rotation symmetry centers.
- Different from recent efforts in the deep learning CNN community where researchers are seeking networks that are rotation reflection or affine invariant to input images @cite @cite @cite @cite , our work explicitly acknowledges (near) reflection and rotation symmetries in the raw data regardless of the transformations applied on the input images. To the best of our knowledge, there have been no deep learning networks trained on human symmetry labels for automated reflection and rotation symmetry detections.
- As depth can significantly simplify the deblurring problem, the multi-view methods have been proposed to leverage on depth information. Building upon the work of Ezra and Nayar @cite , Li al @cite extended the hybrid camera with an additional low-resolution video camera where two low-resolution cameras form a stereo pair and provide a low-resolution depth map. Tai al @cite used a hybrid camera system to compute a pixel-wise kernel with optical flow. Xu al @cite inferred depth from two blur images captured by a stereo camera and proposed a hierarchical estimation framework to remove motion blur caused by in-plane translation. Just recently, Sellent al @cite proposed a video deblurring technique based on stereo video, where 3D scene flow is estimated from blur images using a piecewise rigid 3D scene flow representation.
- Removing compression artifacts has been addressed in the past. There is a vast literature of image restoration, targeting image compression artifacts. The vast majority of the approaches can be classified as processing based @cite @cite @cite @cite @cite @cite @cite @cite and a few ones are learning based @cite @cite @cite @cite .
- We make the following contributions. We define a deep convolutional residual generative network @cite , that we train with two strategies. Similarly to @cite our network is fully convolutional and is therefore able to restore images of any resolution. Differently from @cite we avoid MSE loss and we use a loss based on SSIM, this improves results perceptually. Nonetheless, as also happening in the super-resolution task, networks trained to optimize the MSE produce overly smoothed images; this behavior unfortunately is also present in our SSIM trained feed-forward network.
- The first formulation of a convolutional network analogy for irregular domains modeled with graphs has been introduced by Bruna al @cite , who looked into both the spatial and the spectral domain of representation for performing localized filtering.
- To cover these important cases, we formulate our filtering approach in the spatial domain, where the limited complexity of evaluation and the localization property is provided by construction. The main challenge here is dealing with weight sharing among local neighborhoods @cite , as the number of vertices adjacent to a particular vertex varies and their ordering is often not well definable.
- The approach of Niepert al @cite introduces a heuristic for linearizing selected graph neighborhoods so that a conventional 1D CNN can be used. We share their goal of capturing structure in neighborhoods but approach it in a different way. Finally, Graph neural networks @cite @cite propagate features across a graph until (near) convergence and exploit edge labels as one of the sources of information as we do. However, their system is quite different from the current multilayer feed-forward architectures, making the reuse of today's common building blocks not straightforward.
- * CNNs on Point Clouds and Meshes. There has been little work on deep learning on point clouds or meshes. Masci al @cite define convolution over patch descriptors around every vertex of a 3D mesh using geodesic distances, formulated in a deep learning architecture. The only way of processing point clouds using deep learning has been to first voxelize them before feeding them to a 3D CNN, be it for classification @cite or segmentation @cite purposes. Instead, we regard point cloud as graphs in Euclidean space in this work.
- The process of modeling a city is complex. The intrinsic complexity of interactions between city entities make it very difficult to map relevant sets of dynamic aspects that are often used to characterize a city. Moreover, these entity interactions, along with the numerous entities and processes, differ from one city to another. Thus, the process of modeling the city is typically use-case centered, where the modeling is performed towards a specified goal. This approach, hence, streamlines the process, identifying which characteristics need to be modeled. The work in @cite proposes a core conceptual model for the Domain Knowledge Model of a Smart City, which originally involves multiple domains and cities. The proposed work aims to support cross-domain and cross-city interoperability by specifying terms from different stakeholders. Ontologies play a big role in enabling cross-city comparison. The Semantic Web has been used in the Open Government Data (OGD) approach to make it possible for cities to share information and knowledge under a common vocabulary. Pushing this further, the GCI (Global City Indicators) Ontology @cite is an effort for the modeling of city entities that covers the concepts used by global indicators using Semantic Web technologies.
- The number of articles published every year keeps increasing @cite @cite and well over 50 million scholarly articles have been published so far @cite . While this repository of human knowledge contains invaluable information, it has become increasingly difficult to take advantage of all available information due to its sheer amount.
- Relation extraction can be seen as a process comprising two steps that can be done jointly @cite or separately: first, entities of interest need to be identified, then the relation among each possible set of entities has to be determined. In this work, we concentrate on the second step (often referred to as relation extraction or classification) and on binary relations, i.e. relations between two entities. Extracted relations can be used for a variety of tasks such as question-answering systems @cite , ontology extension @cite , and clinical trials @cite .
- Supervised methods for relation extraction commonly employ support vector machines @cite @cite @cite @cite , na " ve Bayes @cite , maximum entropy @cite , or conditional random fields @cite . These methods require the practitioner to handcraft features, such as surface features, lexical features, syntactic features @cite or features derived from existing ontologies @cite . The use of kernels based on dependency trees has also been explored @cite @cite @cite .
- More recently, a few studies have investigated the use of artificial neural networks for relation extraction @cite @cite @cite . Our approach follows this line of work.
- A similar problem to self-positioning has been handled in @cite using spatial RAY features. It tries to predict the position of the vehicle on the road from an input video stream. However, they predict at most 3 lanes at a time either on the left or right hand side, whereas we have upto 6 lanes in our database with upto 3 lanes being on each side. In addition, there experiments are spread over only 2 days with similar road conditions. Our experiments are more rigorous, spreading over 5 days and the data includes 6 different road conditions. They have low traffic density whereas we have moderate-to-high traffic density. Thus direct comparison between these two approaches may not be possible.
- Vehicle detection is a subset of a widely-studied problem of object detection. There are a plethora of approaches for general object detection, @cite , or approaches dedicated to vehicle detection which use optical-flow and hidden-markov-model based classification to interpret motion-based clues @cite . HOG and HAAR features have also been used with different classifiers and learning frameworks such as adaboost, SVM and active learning @cite @cite . Though there are innumerable methods developed for general object detection and specifically for vehicle detection, listing them all is beyond the scope of this paper. A good review of vehicle detection methods can be found in @cite . In spite of continuous efforts, there are no reliable vision-based solutions to predicting number of lanes and vehicle self-positioning given a front-facing view.
- The simplest possible approach is to treat the time series as vectors and apply well-known kernels such as a linear or radial basis kernel @cite . While this approach works well in some circumstances, time dependencies and the relationships among multiple attributes in the MTS are not explicitly modeled.
- DTW @cite is one of the most commonly used similarity measures for UTS and has become the state-of-the-art in many practical applications @cite @cite @cite . Several formulations have been proposed to extend DTW to the multidimensional setting @cite @cite . Since DTW does not satisfy the triangle inequality, it is not negative definite and, therefore, one cannot obtain a psd kernel by applying an exponential function to it @cite . Such an indefinite kernel may lead to a non-convex optimization problem (e.g., in an SVM), which hinders the applicability of the model @cite . Several approaches have been proposed to limit this drawback at the cost of more complex and costly computations. In @cite @cite ad hoc spectral transformations were employed to obtain a psd matrix. @cite designed a DTW-based kernel using global alignments (GAK). Marteau and Gibet proposed an approach that combines DTW and edit distances with a recursive regularizing term @cite .
- Conversely, there exists a class of (probabilistic) kernels operating on the configurations of a given parametric model, where the idea is to leverage the way distributions capture similarity. For instance, the Fisher kernel assumes an underlying generative model to explain all observed data @cite . The Fisher kernel maps each time series @math into a feature vector @math , which is the gradient of the log-likelihood of the generative model fit on the dataset. The kernel is defined as @math , where @math is the fisher information matrix. Another example is the probability product kernel @cite , which is evaluated by means of the Bhattacharyya distance in the probability space. A further representative is the marginalized kernel @cite , designed to deal with objects generated from latent variable models. Given two visible variables, @math and @math and two hidden variables, @math and @math , at first, a joint kernel @math is defined over the two combined variables @math and @math . Then, a marginalized kernel for visible data is derived from the expectation with respect to hidden variables: @math . The posterior distributions are in general unknown and are estimated by fitting a parametric model on the data.
- In several cases, the assumption of a single parametric model underlying all the data may be too strong. Additionally, finding the most suitable parametric model is a crucial and often difficult task, which must be repeated every time a new dataset is processed. This issue is addressed by the autoregressive kernel @cite , which evaluates the similarity of two time series on the corresponding likelihood profiles of a vector autoregressive model of a given order, across all possible parameter settings, controlled by a prior. The kernel is then evaluated as the dot product in the parameter space of such profiles, used as sequence representations. The reservoir based kernels @cite , map the time series into a high dimensional, dynamical feature space, where a linear readout is trained to discriminate each signal. These kernels fit reservoir models sharing the same fixed reservoir topology to all time series. Since the reservoir provides a rich pool of dynamical features, it is considered to be generic'' and, contrarily to kernels based on a single parametric model, it is able to represent a wide variety of dynamics for different datasets.
- There are various RvNN models for sentiment classification @cite @cite @cite @cite @cite @cite . All of these models attempt to capture sentence representation in a bottom-up fashion in accordance with a parse tree. In this way, sentence representation can be calculated by learning compositional functions for each phrase.
- Several studies have focused on using a compositional function to improve compositionality @cite @cite @cite @cite . parameterized each word as a matrix--vector combination to denote modification and representation. used a bilinear function enacted by tensor slicing for composition in place of a large matrix--vector parameterization. incorporated a constituent label of each phrase as feature embedding to take account of the different compositionality based on parent or children label combinations. proposed a more robust model than those discussed above in which long short-term memory (LSTM) units were applied to a RvNN, as mentioned in Section . Thanks to the application of LSTM, this model can learn increased numbers of parameters appropriately, unlike the other models.
- Based on psychological studies, the human ability of intuition of attention @cite has been introduced into many computer science fields. The main function of this ability is deciding which part of an input needs to be focused on.
- In natural language processing (NLP), the attention mechanism is utilized for many tasks, including neural machine translation @cite @cite @cite , neural summarization @cite @cite @cite , representation learning @cite , and image captioning @cite .
- Recently, a few noteworthy studies have followed and revisited the network performance analysis of UDNs using more practical assumptions @cite @cite @cite @cite @cite @cite , such as a general multi-piece path loss model with probabilistic line-of-sight (LoS) and non-LoS (NLoS) transmissions, a non-zero BS-to-UE antenna height difference @math , and a non-fully-loaded network with a finite UE density @math .
- The inclusion of these more realistic assumptions significantly changed the previous conclusion on the SINR invariance @cite , indicating that the coverage probability performance of UDNs is with respect to the BS density. In particular, two seemingly contradictory performance behaviors can be observed in @cite and @cite , both considering a general multi-piece path loss model recommended by the 3GPP.
- First, if we consider a practical non-zero BS-to-UE antenna height difference @math , then the coverage probability is shown to crash as the BS density increases in a fully-loaded UDN. This is caused by a severe in UDNs @cite . The intuition of such is that the signal power becomes in UDNs due to the lower-bound on the BS-to-UE distance, as a UE cannot be closer than @math to its serving BS.
- Second, if we consider a practical finite UE density @math , then the coverage probability is shown to take off as the BS density increases. This is caused by a soaring in UDNs @cite . The intuition of such is that the aggregate interference power becomes in UDNs due to the partial activation of a finite density of BSs to serve a finite density of UEs. In more detail, a large number of BSs can switch off their transmission modules in UDNs, entering into idle mode, if there is no active UE within their coverage areas. As a result, the number of interfering BSs and also the SSR are limited by the finite number of UEs.
- A large portion of queries issued in Web search engines target entities or contain semantic resources (such as types, relations and attributes) @cite as a primary intent. Consequently, the identification of entity-centric queries has become of particular concern for commercial search engines serving as a means to narrow the search space and to provide contextual query results @cite . Thus, the traditional task of Ad-hoc Document Retrieval (ADR) @cite is moving towards an entity retrieval task @cite . Hence, instead of top-- @math document retrieval that match a keyword query, the task and therefore the results are increasingly becoming entity-centric.
- Following this direction, @cite proposed a hybrid approach based on query expansion and relevance feedback techniques on top of the BM25 ranking function to build an entity retrieval framework. In contrast to this work, we use the state-of-the-art BM25F @cite @cite to assign varying degrees of importance to different parts of a document. Further, through an offline pre-processing step we are able to infer links between similar entities for the retrieval process. This is particularly important when considering datasets that have less links between entities, a significant feature of the work by @cite . Another advantage of adopting BM25F is penalising documents entities, consisting of long textual literals, in the final ranking @cite . Sindice @cite is another approach focusing on indexing RDF documents. It supports data discovery and integration by taking advantage of DBpedia entities as a source to actively index resources. The process performed by Sindice plays a key role in centralising disparate data sources on the Web. The adoption of entities and foremost entity types (topics) is also supported by @cite in the recommendation of entities in Web search. Our approach can benefit Sindice by indexing documents following a topic-based fashion.
- Zhiltsov and Agichtein @cite propose a learning to rank approach, where they model the relations between entities through a various set of features, such as language models and other query related features (e.g query length). Finally, through tensor matrix factorisation they find latent similarities between entities, later used in their learning to rank model. One major disadvantage of this approach is that it is supervised, hence, unlikely to perform reasonably well on ad-hoc entity search tasks.
- Based on the detected objects and parts, a few recent works focused on the extraction of more discriminative features @cite @cite . For instance, a two-level attention model was proposed in @cite . In addition, several researchers also explored the idea of human interaction based techniques @cite @cite , which requires more manual inputs.
- Privacy : Preserving user privacy is crucial when collecting compensation data. We encountered unique challenges associated with privacy and security while designing our system. Our methodology for addressing these challenges through a combination of techniques such as encryption, access control, de-identification, aggregation, and thresholding is described in @cite . Please refer this paper for a discussion of different privacy techniques (in contrast with our approach), and an empirically study of the tradeoffs between privacy and modeling needs.
- Statistical Smoothing : The idea of Bayesian hierarchical statistical smoothing originates from the smoothing of sparse events (e.g., CTR) in the context of computational advertising @cite @cite , where a natural hierarchy for the combination of ads category and publisher category is used for an (ad, publisher) pair with very little data to borrow strength from its ancestor nodes. However, that is an entirely different context and the models used are hence different.
- Low Rank . Tensor decomposition with low-rank approximation-based methods are commonly used to accelerate deep convolutional networks. For example, in @cite @cite , the authors exploited the redundancy between convolutional filters and used low-rank approximation to compress convolutional weight tensors and fully connected weight matrices. Yang al @cite use an adaptive fastfood transform was used to replace a fully connected layer with a series of simple matrix multiplications, rather than the original dense and large ones. Liu al @cite propose a sparse decomposition to reduce the redundancy in convolutional parameters. @cite @cite , the authors used generalized singular vector decomposition (GSVD) to decompose an original layer to two approximated layers with reduced computation complexity.
- Product Quantization . Some other researchers focus on product quantization to compress and accelerate CNN models. The authors of @cite proposed a framework to accelerate the test phase computation process with the network parameters quantized and learn better quantization with error correction. Han al @cite proposed to use a pruning stage to reduce the connections between neurons, and then fine tuned networks with weight sharing to quantify the number of bits of the convolutional parameters from 32 to 5. In another work @cite , the authors trained neural networks with extremely low precision, and extended success to quantized recurrent neural networks. Zhou al @cite generalized the method of binary neural networks to allow networks with arbitrary bit-width in weights, activations, and gradients.
- Architecture . Some researchers improve the efficiency of networks by carefully designing the structure of neural networks. @cite , a simple model was trained by distilling the knowledge from multiple cumbersome models, which helps to reduce the computation cost while preserving the accuracy. Romero al @cite extended the knowledge distillation approach to train a student network, which is deeper but thinner than the teacher network, by extracting the knowledge of teacher network. In this way, the student network uses less parameters and running time to gain considerable speedup compared with the teacher network. Iandola al @cite proposed a small DNN architecture to achieve similar performance as AlexNet by only using 50x fewer parameters and much less computation time via the same strategy.
- Most of the effort in localization is typically devoted to finding accurate inference methods, assuming the distribution and location of beacons in the environment are given. One setting for these methods is where sensor measurements can be assumed to provide direct, albeit possibly noisy, measurements of relative range or bearing from beacons---an assumption that is typically based on a simple model for signal propagation. Location estimation then proceeds by using these relative range and or bearing estimates and knowledge of beacon locations. For example, acoustic long baseline (LBL) networks are frequently used to localize underwater vehicles @cite @cite , while a number of low-cost systems exist that use RF and ultrasound to measure range @cite @cite .
- Another approach, common for radio frequency (RF) beacon and WiFi-based networks, is to infer location directly from received signal strength (RSS) signatures. One way to do this is by matching against a database of RSS-location pairs @cite . This database is typically generated manually via a site-survey, or organically'' during operation @cite @cite . and @cite adopt a different approach, training neural networks to predict a receiver's location within an existing beacon network based upon received signal strength.
- The above methods deal with optimal ways of inferring location given an existing network of beacons. The decision of how to distribute these beacons, however, is often made manually based on expert intuition. Automated placement methods are used rarely, and for very specific settings, such as RSS fingerprint-based localization @cite . The most common of these is to ensure full coverage--- to ensure that all locations are within an acceptable range'' of at least one beacon, assuming this condition is sufficient to guarantee accurate localization.
- One common instance of optimizing placement for coverage is the standard art-gallery visibility problem @cite that seeks placements that ensure that all locations have line-of-sight to at least one beacon. This problem assumes a polygonal environment and that the beacons have an unlimited field-of-view, subject to occlusions by walls ( cameras). Related, propose a greedy landmark-based method that solves for the placement of the minimum number of beacons (within a @math factor) necessary to cover all but a fraction of a given polygonal environment. Note that these methods treat occlusions as absolute, while in practice, obstructions often only cause partial attenuation---with points that are close but obstructed observing similar signal strengths as those that are farther away. provide an interesting alternative, and like us, use backpropagation to place WiFi access points---but again, only for the objective of maximizing coverage. consider localization accuracy for placing wireless access points to maximize receiver signal-to-noise ratios.
- The above methods address spatial placement but not transmission channel assignments, and associated issues with interference. Automatic channel assignment methods have been considered previously, but only for optimizing communication throughput @cite @cite --- to minimize interference from two beacons in the same channel at any location. Note that this is a very different and simpler objective than one of enabling accurate localization, where the goal is to ensure that there is a unique mapping from every RSS signature (with or without interference) to location.
- Our approach provides a way to trade-off localization accuracy with the number of beacons, similar to the performance-cost trade-offs considered by the more general problem of sensor selection @cite @cite @cite . Some selection strategies are designed with specific inference strategies in mind. and use a greedy entropy-based approach to place sensors, tied to a Gaussian process (GP) model that is used for inference. However, this approach does not model the accuracy of the predictions at the selected locations. choose locations for a fixed sensor network that maximize mutual information using a GP model for phenomena over which inference is performed ( temperature). However, these formulations require that the phenomena be modeled as a GP, and are thus not suitable for the task of beacon-based localization.
- A common approach to localization within existing radio frequency (RF)-based beacon networks is to use the received signal strength (RSS) as a fingerprint that is matched against a database of RSS-location pairs to determine the receiver's location @cite . This database is typically generated manually via a site-survey,
- Many beacon networks provide direct, albeit noisy, measurements of relative range or bearing. For example, acoustic long baseline (LBL) networks are frequently used to localize underwater vehicles @cite @cite , while a number of low-cost systems exist that use RF and ultrasound to measure range @cite @cite . propose an algorithm for estimating location based upon noise-corrupted range measurements, formulating the problem as one of realizing a two-dimensional graph whose structure is consistent with the observed ranges. describe a geometric technique that estimates a robot's location as it navigates a network of fixed beacons using either range or bearing observations. employ spectral methods to localize camera networks using angular measurements, without the need for a global coordinate frame. Alternatively, evaluate the use of feedforward and recurrent neural network architectures to localize a receiver based upon noisy range measurements.
- Meanwhile, beacon allocation traditionally relies on coverage as a heuristic to guide placement in a specific environment. When the environment is polygonal and beacons have an unlimited field-of-view subject to occlusions by walls (e.g., cameras), coverage corresponds to the standard art-gallery visibility problem @cite . Related, propose a greedy landmark-based method that solves for the placement of the minimum number of beacons (within a @math factor) necessary to cover all but a fraction of a given polygonal environment. However, a beacon's sensing geometry is often complex as a result of partial attenuation due to the environment, beacon noise, and signal interference. Interference varies according to the placement of other beacons and channel allocation, which most coverage algorithms do not reason over. These factors are particularly important for RF-based localization, which is sensitive to the effects of aliasing, further complicating the relationship between allocation and inference.
- In many scenarios, beacon allocation includes choosing both the location and channel for each beacon. These problems are typically assumed to be decoupled---the location of each beacon is first determined and then their channels are chosen to minimize interference @cite This can result in a sub-optimal allocation. An exception is , who jointly solve access point placement and channel selection using a local search approximation, however they seek to maximize communication throughput, which is a more straightforward objective than localization and does not require reasoning over inference.
- The sensor selection problem @cite @cite considers related scenarios in which there is a cost associated to querying sensors. The problem is to choose the subset of sensors to utilize at each point in time so as to balance inference accuracy with the corresponding query cost.
- In the current work, we have restricted ourselves to grammatical composition, and in particular pregroup grammar. However, the categorical compositional scheme can be instantiated in a number of ways. The grammar can be changed from pregroup grammar to another categorial grammar, as in @cite , or a compositional scheme that is not grammatically based may be used. Indeed, one of the challenges of this approach is to find a model of composition that accurately reflects human behaviour. One way of doing so would be to use an approach in which the syntactic scheme is generated by the semantics of the universe of discourse. Furthermore, since phrases and sentences are represented as sets equipped with a convex algebra, the model can in future work be extended to include logical composition.
- For the case of fully supervised learning of actions, well-studied deep learning and temporal modeling approaches exist. While the authors of @cite focus on a purely neural network based approach, Tang al @cite propose to learn the latent temporal structure of videos with a hidden Markov model. Combining deep learning and temporal modeling, the authors of @cite use a segmental CNN and a semi-Markov model to represent temporal transitions between actions. However, these methods are not applicable in a weakly supervised setting.
- There is one class of graph partitioning problems that come with size restrictions, namely, the Graph Tree Partitioning Problem @cite , where the objective is to partition a graph into equal size subsets, such that the weight of the spanning tree on each subset is either as high as possible (max-min) or as low as possible (min-max), or, the sum of the weights of the spanning trees are either as high (max-sum), or, as low (min-sum) as possible. Though these objectives are closely related to ours, they are not exactly the same problems.
- In general, blur detection algorithms from a single image can be divided into gradient-based, intensity-based and transform-based algorithms. In @cite , Chakrabarti al propose a sub-band decomposition based approach. They estimate the likelihood function of a given candidate point spread function (PSF) based on local frequency component analysis. Liu al @cite propose a method which employs features such as image color, gradient, and spectrum information to classify blurred images. Shi al @cite propose a method based on different features such as gradient histogram span, kurtosis, and data-driven local filters to differentiate between blurred and unblurred image regions. Shi al @cite propose a method based on utilizing a sparse representation of image patches using a learned dictionary for the detection of slight perceivable blur. In @cite , Su al propose a method based on examining singular value information to measure blurriness. The blur type (motion blur or defocus blur) is then determined based on certain alpha channel constraints. In @cite , Tang al employ the image spectrum residual @cite , and then they use an iterative updating mechanism to refine the blur map from coarse to fine by exploiting the intrinsic relevance of similar neighboring image regions.
- CNNs for Visual Recognition. While, in the past, the problems of feature extraction and classifier training were typically decoupled @cite @cite @cite , the impressive results achieved 5 years ago by AlexNet @cite on the ImageNet recognition challenge have put deep learning at the center of visual recognition. Recent years have seen great progress in this context, with increasingly deeper networks @cite @cite @cite , novel normalization @cite @cite and optimization @cite @cite @cite @cite strategies. All these networks, however, follow the same general strategy of stacking multiple layers, convolutional and fully-connected ones, each of which computes linear combinations of the output of the previous one. Despite the use of nonlinearities and pooling strategies, the resulting operations therefore still essentially extract first-order information, in the sense that they cannot compute higher-order statistics, such as covariances.
- Covariance Descriptors and Learning. To the best of our knowledge, @cite , and its log-Euclidean metric learning extension @cite , can be thought of as the first attempts to learn RCDs. This, however, was achieved by reducing the dimensionality of input RCDs, and thus has limited learning power. In a work concurrent to ours @cite , the framework of @cite was extended to learning multiple transformations of input RCDs. This approach, however, still relied on RCDs as input. By contrast, here, we introduce an end-to-end learning strategy. As discussed later, this requires special care to transition from the convolutional activations to the covariance matrix, and, as evidenced by our experiments, significantly outperforms the approach of @cite .
- Private data-collection systems @cite @cite @cite @cite @cite @cite @cite that use secret-sharing based methods to compute sums over private user data typically (a) provide no robustness guarantees in the face of malicious clients, (b) use expensive NIZKs to prevent client misbehavior, or (c) fail to defend privacy against actively malicious servers @cite .
- Other data-collection systems have clients send their private data to an aggregator through a general-purpose anonymizing network, such as a mix-net @cite @cite @cite @cite or a DC-net @cite @cite @cite @cite @cite . These anonymity systems provide strong privacy properties, but require expensive verifiable mixing'' techniques @cite @cite , or require work at the servers that is quadratic in the number of client messages sent through the system @cite @cite .
- In theory, secure multi-party computation (MPC) protocols @cite @cite @cite @cite @cite allow a set of servers, with some non-collusion assumptions, to privately compute any function over client-provided values. The generality of MPC comes with serious bandwidth and computational costs: evaluating the relatively simple AES circuit in an MPC requires the parties to perform many minutes or even hours of precomputation @cite . Computing a function @math on millions of client inputs, as our five-server deployment can do in tens of minutes, could potentially take an astronomical amount of time in a full MPC. That said, there have been great advances in practical general-purpose MPC protocols of late @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . General-purpose MPC may yet become practical for computing certain aggregation functions that cannot (e.g., exact ), and some special-case MPC protocols @cite @cite @cite are practical today for certain applications.
- Our method is similar in spirit to a recent work @cite , which detects text lines by finding and grouping a sequence of through a CNN coupled with recurrent neural layers. In contrast, we detect oriented segments only using convolutional layers, yielding better flexibility and faster speed. Also, we detect links explicitly using the same strong CNN features for segments, improving the robustness.
- The architecture of our network inherits that of SSD @cite , a recent object detection model. SSD proposed the idea of detecting objects on multiple feature layers with convolutional predictors. Our model also detects segments and links in a very similar way. Despite the model similarity, our detection strategy is drastically different: SSD directly outputs object bounding boxes. We, on the other hand, adopt a bottom-up approach by detecting the two comprising elements of a word or text line and combine them together.
- Researchers have in the past studied negative sentiment on social media @cite . Work has also been done on building classifiers that can identify content that is hateful and antagonistic in nature @cite @cite . Additionally, techniques have at times involved building a hate lexicon to solve the problem. @cite . However, most of these projects focus on content that is openly hateful and not disguised to evade detection. Although related, our endeavour and its novelty is geared more towards finding such instances that seek to fool the system.
- Curriculum learning is widely used in many machine learning approaches. Typically however, the curriculum requires at least some manual specification. A key point about our work is that Alice and Bob devise their own curriculum entirely automatically. Previous automatic approaches, such as @cite , rely on monitoring training error. But since ours is unsupervised, no training labels are required either.
- The closest work to ours is that of @cite , who also have one part of the model that proposes tasks, while another part learns to complete them. As in this work, the policies and cost are parameterized as functions of both state and goal. However, our approach differs in the way tasks are proposed and communicated. In particular, in @cite , the goal space has to be presented in a way that allows explicit partitioning and sampling, whereas in our work, the goals are sampled through Alice's actions. On the other hand, we pay for not having to have such a representation by requiring the environment to be either reversible or resettable.
- Several concurrent works are related: @cite form an implicit curriculum by using internal states as a target. @cite automatically generate a series of increasingly distant start states from a goal. @cite use an adversarial framework to perturb the environment, inducing improved robustness of the agent. @cite propose a scheme related to our random Alice'' strategy In their paper they analyzed our approach, suggesting it was inherently unstable. However, the analysis relied on a sudden jump of Bob policy with respect to Alice's, which is unlikely to happen in practice. .
- Many methods have been proposed that adapt techniques from text topic modeling to vision. Li and Perona @cite propose a Bayesian hierarchical model to learn characteristic intermediate themes in an unsupervised way, for example, while Sivic al @cite and @cite introduce ways to discover hierarchical image structure from unlabeled datasets @cite @cite . However, most of these techniques were developed prior to the prevalence of deep neural network, and are based on hand-tuned features. Moreover, none have studied egocentric imagery, as we do here.
- Finally, other deep learning methods have been introduced for the segmentation of various types of images with different architectures (e.g. U-Net @cite ), deeper networks and different learning methods (e.g. generative adversarial network @cite ). While these latest advances are of high interest and may be considered in future research, they are out of the scope of this paper as we base this work on FCNs.
- Many neural network models have been studied on the SQuAD task. @cite proposed to associate documents and questions and adapted the so-called to determine the positions of the answer text spans. @cite proposed a to extract and rank a set of answer candidates. @cite focused on word representation and presented a fine-grained gating mechanism to dynamically combine word-level and character-level representations based on the properties of words. @cite proposed a (MPCM) model, which matched an encoded document and question from multiple perspectives. @cite proposed a dynamic decoder and so-called to improve the effectiveness of the decoder. The (BIDAF) used the bi-directional attention to obtain a question-aware context representation.
- Recent real-time 3D reconstruction and SLAM approaches, e.g. @cite (Visual-features with RANSAC), @cite (Direct image alignment based on optimization), or @cite (Point cloud alignment based on ICP) can effectively generate dense or semi-dense 3D maps, but they have no understanding of the observed scenes and objects.
- The more complex problem of 3D semantic reconstruction remains an open research problem. Recent approaches can be grouped into two main categories: 3D semantic reconstruction base on 3D template matching @cite @cite @cite , and 3D semantic reconstruction base on 2D semantic segmentation @cite @cite .
- The work most closely related to ours is @cite , which performs dense, 3D semantic mapping ofindoor scenes, using deconvolutional neural networks @cite . Real-time frame-rates of about 25 Hz are achieved CRF optimization post-processing. But most such methods @cite @cite @cite rely on using fully connected CRF @cite optimization as an offline post-processing step, following online 3D reconstruction, i.e. these methods do not actually achieve semantic mapping in real-time. Additionally, these methods are not fully end-to-end trainable, and there is no interaction between classifier learning and CRF learning. The parameters of classifier and CRF cannot be jointly learned in a united framework. Furthermore, all of the above methods are focused on semantic recognition. In contrast, our work is the first method that achieves simultaneous 3D reconstruction with semantic labeling, and we achieve both 3D reconstruction semantic labeling simultaneously in real-time.
- Materials recognition is a challenging research topic due to wide variation in appearance within categories. Previous material recognition research predominantly focused on material classification, and did not achieve pixel-wise material segmentation. Most previous work employed hand-crafted visual features, e.g. reflectance-based edge features @cite , variances of oriented gradients @cite , and pairwise local binary patterns @cite . Recently CNN features @cite @cite @cite have been employed to achieve the state-of-the-art results of material classification in many public material datasets. In addition to the 2D features, @cite combined 3D geometry (surface normals, camera intrinsic and extrinsic parameters) with 2D features (texture and color) to improve material classification.
- For pixel-wise material segmentation, @cite convert patch-based trained CNN classifiers into an efficient fully convolutional framework combined with a fully connected CRF to perform pixel-wise material recognition. @cite combined local appearance with separately recognized global contextual cues including objects and places, which can lead to a superior result. They employed fully convolutional network(FCN) @cite followed by recurrent neural network(RNN) for dense pixel-wise material segmentation. @cite proposed a novel CNN architecture trained on 4D light-field images and employ FCN for per-pixel material recognition. However, in contrast to our work, none of these methods perform material recognition simultaneously with 3D reconstruction.
- If @math is a simple rectilinear polygon (i.e., @math ), then there always exists a rectilinear path that has both the minimum length and the minimum link distance for any two points @math and @math in @math @cite @cite . de Berg @cite built a data structure of @math size in @math time that can find such a path in @math time for any two-point query. The preprocessing time and space were both reduced to @math by Schuierer @cite (with @math query time).
- If @math is a general rectilinear domain with @math , then there may not exist a rectilinear path that is both a minimum-link path and a shortest path @cite . The problems of finding only minimum-link paths or only shortest paths have been studied extensively. Imai and Asano @cite presented an @math time and space algorithm for finding a minimum-link path in @math , and the space was reduced to @math @cite @cite @cite . Recently, @cite proposed an @math time and @math space algorithm for the problem, after @math is triangulated (which can be done in @math time or @math time for any @math @cite ). The algorithms in @cite @cite @cite also construct an @math size data structure that can answer each one-point minimum-link path query in @math time.
- To find a minimum-link path between two points @math and @math in an arbitrary polygonal domain @math , Mitchell @cite gave an @math time algorithm, where @math is the inverse of Ackermann's function and @math is the size of the visibility graph of @math and @math in the worst case. The one-point query problem was also studied in @cite .
- In principle, our DA-RNN framework only requires dense data associations between consecutive frames. It is thus independent of the underlying representation and could be combined with any of the reconstruction techniques described above. Here, we use KinectFusion @cite to achieve a volumetric representation for geometry and semantics.
- Semantic labeling on images classifies each pixel of an input image into one of the predefined semantic classes. The semantic labeling problem has often been tackled with probabilistic graphical models such as Markov Random Fields (MRFs) or Conditional Random Fields (CRFs) @cite @cite , which model the context around pixels. More recently, convolutional neural networks have been applied to semantic labeling @cite @cite @cite @cite , which achieve significant improvement over previous methods. However, all these approaches mainly focus on semantic labeling of a single image. Recurrent neural networks @cite @cite have been applied to semantic video segmentation, which exploit the temporal relationship or information provided by multiple viewpoints of a scene. @cite @cite show how the labels extracted from individual RGB-D frames can be incorporated into a voxel or surfel map, resulting in more stable labeling. Further improvements are achieved by performing MRF or CRF inference in the 3D map. Approaches such as @cite @cite @cite perform labeling by conducting 3D object detection through the 3D reconstruction, thereby potentially incorporating information that is not available in any single view.
- Much work has focused on divergent word usages among people in numerous aspects, such as gender, age, occupation, and region. It is reported that typical male language uses more judgmental adjectives, elliptical sentences, directives, and I'' reference, while typical female language contains more intensive adverbs, references to emotions, uncertainty verbs @cite . People in different age groups appear to choose words differently. reported that, comparing with younger people, older people talk more about family and work, and use less swear words in their language @cite . Jobs affect the language patterns of people as well @cite . In the paper, the authors listed the most used words of several occupations, and their results indicate clear divergences in word usages between people of different jobs. Similarly, different cities may have different preferences in words @cite , which reflect regional differences between people.
- Perceived characteristics of a robot including competence, responsibility, and credibility (and combinations of these), have been proposed to provide means for evaluating the level of trust in a machine @cite @cite . Still, a clear stance on what exactly is being measured is necessary @cite (e.g. trust in terms of competence as in @cite , or trustworthiness' directly, as in @cite ). Measuring trust in human-robot interactions is challenging. A momentary state of trust can be captured in an experiment, but trust is earned over a longer term @cite @cite @cite . Accumulation of small errors might affect trust more than a single large error, and a poor initial performance by a robot could leave a stronger negative impact @cite . Also, trust is not only a situational construct (i.e. depending on the situation), but it is also dispositional (i.e. depends on the individual and its characteristics) @cite . Additionally, the study of human-robot interactions through experiments has many practical and ethical issues and associated limitations, as participants should not be exposed to dangerous situations, nor should they be deceived @cite .
- Other scales measure people's perceptions of robots during an interaction, e.g. in @cite @cite , which studied the effect of people's general attitudes and emotions such as technophobia or social anxiety on the interaction outcomes. Nonetheless, solely using questionnaires and scales make these metrics entirely subjective, not considering factors such as the presence of faults in the robot system. Ideally, both objective and subjective metrics should be added to a study @cite .
- As mentioned in @cite , the ability of the humans (e.g. previous experience and training), their characteristics (e.g. demographics, personality, attitudes), the performance of the robot, and its attributed characteristics (e.g. anthropomorphism), as well as the environment (e.g. communication channels) and task nature (e.g. difficulty), have been studied as factors to contribute to the trust development in human--robot interactions. Also, a segment of research in human-robot interaction has focused on the influence of the robot's design (e.g. anthropomorphism, communication features, gestures) in the development of trust, as robot characteristics influence it the most, compared to environmental and human-related factors @cite . @cite and @cite have studied whether people trust expressive robots more or not, even when they make mistakes. These studies found that co-verbal gestures and more expressiveness caused the robots to be perceived more human-like and likeable, increasing future contact or interaction intentions, even though the mistakes affected a successful performance during the tasks.
- Research in robotic co-workers has focused on the design aspects of such systems, including social and interaction aspects in terms of dialogues, gestures and social cues, or more engineering oriented such as scheduling shared collaborative building tasks and the design of protocols (e.g. turn taking), with the intention to obtain a collaboration between the user and the robot that is productive and efficient (e.g. @cite @cite @cite ). Although trust metrics have been used to evaluate robotic systems in manufacturing tasks (e.g. in @cite for collaborative masonry), research is needed to evaluate how trust is developed in these systems and their environments, compared to more social interactions such as in @cite , or challenging remote interactions such as in @cite @cite @cite @cite .
- Moreover, engineering of a cognitive FD relaying network has been considered in @cite @cite , where various resource allocation algorithms to improve the outage probability have been proposed. In @cite , the authors developed a mathematical model to analyze the outage probability for the proposed FD relay-selection scheme over both independent Nakagami- @math and Rayleigh fading channels. The authors also extended their analysis to two-way FD-based AF relays in underlay cognitive networks @cite where they analyzed various performance metrics such as outage probability, symbol error probability, etc. In addition, @cite proposed a joint routing and distributed resource allocation for FD wireless networks. @cite investigated distributed power allocation for a hybrid FD HD system where all network nodes operate in the HD mode except the access point (AP). These existing results focus on either minimizing the outage probability or analyzing performance for existing algorithms. This paper considers both non-coherent and coherent FU relay nodes and focuses on maximizing SU throughput given interference constraint at the PU and power constraints.
- The remaining of this paper is organized as follows. Section describes the system model. Section studies the dynamic power allocation policies for full-duplex cognitive relay selection to achieve the maximum SU network rate. Sections and consider the non-coherent and coherent scenarios, respectively. Section demonstrates numerical results followed by concluding remarks in Section . A part of this work will be presented at CISS 2017 @cite .
- Unlike traditional passwords that can be matched in their encrypted or hash form with standard ciphers (e.g., AES, RSA, and SHA-3), face templates cannot be simply protected by standard ciphers because of the intra-subject variations in face images @cite @cite . Due to the avalanche effect https: en.wikipedia.org wiki Avalanche @cite of standard ciphers, the face templates protected by standard ciphers need to be decrypted before matching. This introduces another challenge, (decryption) key management. In addition, decrypted face templates can also be gleaned by launching an authentication attempt.
- @cite @cite : Zhmoginov and Sandler @cite learn the reconstruction of face images from templates using a CNN by minimizing the template difference between original and reconstructed images. This requires the gradient information from target template extractor and cannot satisfy our assumption of template extractor. Cole et. al. @cite first estimate the landmarks and textures of face images from the templates, and then combine the estimated landmarks and textures using the differentiable warping to yield the reconstructed images. High-quality face images (e.g., front-facing, neutral-pose) are required to be selected for generating landmarks and textures in @cite for training the reconstruction model. Note that both @cite and @cite does not aim to study vulnerability on deep templates and hence no comparable statistical results based template reconstruction attack were reported.
- With adversarial training, GANs @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite are able to generate photo-realistic (face) images from randomly sampled vectors. It has become one of the most popular methods for generating face images, compared to other methods such as data augmentation @cite and SREFI @cite . GANs typically consist of a generator which produces an image from a randomly sampled vector, and a discriminator which classifies an input image as real or synthesized. The basic idea for training a GAN is to prevent images output by the generator be mistakenly classified as real by co-training a discriminator.
- DCGAN @cite is believed to be the first method that directly generates high-quality images ( @math ) from randomly sampled vectors. PPGN @cite was proposed to conditionally generate high-resolution images with better image quality and sample diversity, but it is rather complicated. Wasserstein GAN @cite @cite was proposed to solve the model collapse problems in GAN @cite . Note that the images generated by Wasserstein GAN @cite @cite are comparable with those output by DCGAN. BEGAN @cite and LSGAN @cite have been proposed to attempt to address the model collapse, and non-convergence problems with GAN. A progressive strategy for training high-resolution GAN is described in @cite .
- In this work, we employed an efficient yet effective method, DCGAN to generate face images. The original DCGAN @cite is easy to collapse and outputs poor quality high-resolution images (e.g., @math in this work). We address the above problems with DCGAN (Section ).
- One of the first approaches for direct RGB-D odometry is KinectFusion @cite , which uses only the depth channel D to estimate the odometry and a dense map and discards the RGB information. As its main limitations, it is restricted to small workspaces and will probably fail if the scene does not contain enough geometric structure.
- Kintinuous @cite builds on KinectFusion and uses a rolling cyclical buffer that shifts the volume as the camera is moving, hence not being restricted to small workspaces. It also includes loop closing and pose graph optimization for global consistency.
- @cite is one of the first approaches that proposes to minimize the photometric error between the current frame and a past frame.
- DVO SLAM @cite @cite models the map as a pose graph. The constraints between keyframes are set from the tracking thread, which is based on dense photometric and geometric error minimization. This system also achieves CPU real time but not at full resolution ( @math pixels).
- @cite shows and compares three different alignment strategies for direct tracking, namely the forward-compositional, the inverse-compositional and the efficient second-order minimization approach. In this paper we use the inverse composition, as it is the most efficient of the three.
- @cite estimates the relative motion between frames by a least-squares optimization that minimizes the 3D geometric error between corresponding RGB salient points. @cite uses the alignment obtained from feature matching as a seed for a joint optimization of the RGB-D point clouds. In both cases these relative transformations form the edges of a pose graph that are optimized using @math @cite and TORO respectively.
- Regarding the weighting of the geometric and the photometric error @cite , @cite , @cite and @cite scale the errors with a heuristic constant. @cite weights both contributions according to their respective covariances. @cite scale each depth error using its squared inverse depth.
- @cite proposes to use the inverse depth in the minimization of the geometric reprojection error. We evaluate this parametrization in our system.
- ElasticFusion @cite is one of the most recent works and the RGB-D SLAM state of the art in terms of accuracy. The tracking thread uses ICP and dense photometric reprojection error. It achieves global consistency in a map-centric manner by a non-rigid deformation of the map structure, instead of using a more standard pose-centric graph optimization.
- Some of these approaches incorporate the multi-view constrains in the tracking thread by weighting the errors with the standard deviation of the depth inverse depth but they lack a multi-view model in the mapping thread. In our approach we also use multi-view constraints in the mapping. We show in our results that a semi-dense photometric error improves the accuracy and the efficiency of the estimation. @cite @cite and @cite have shown the effectiveness of a semi-dense or sparse photometric error in the optimization of the camera pose, but in a monocular setting.
- In the machine learning setting, a wide array of applications consider optimization as a means to perform inference in learning. Among many other applications, these architectures are well-studied for generic classification and structured prediction tasks ; in vision for tasks such as denoising ; and uses unrolled optimization within a network to stabilize the convergence of generative adversarial networks @cite . Indeed, the general idea of solving restricted classes of optimization problem using neural networks goes back many decades @cite @cite , but has seen a number of advances in recent years. These models are often trained by one of the following four methods.
- These methods can be used for tasks like (structured) prediction where the training method shapes the energy function to be low around the observed data manifold and high elsewhere @cite . In recent years, there has been a strong push to further incorporate structured prediction methods like conditional random fields as the last layer'' of a deep network architecture @cite @cite @cite as well as in deeper energy-based architectures @cite @cite @cite . Learning in this context requires observed data, which isn't present in some of the contexts we consider in this paper, and also may suffer from instability issues when combined with deep energy-based architectures as observed in .
- The argmin operation over an unconstrained objective can be approximated by a first-order gradient-based method and unrolled. These architectures typically introduce an optimization procedure such as gradient descent into the inference procedure. This is done in . The optimization procedure is unrolled automatically or manually @cite to obtain derivatives during training that incorporate the effects of these in-the-loop optimization procedures. However, unrolling the computation of a method like gradient descent typically requires a substantially larger network, and adds substantially to the computational complexity of the network.
- Though tremendous efforts have gone to solve , their perturbation problems had received little or no attention in the past. In fact, today there are only a handful articles written on the perturbations of only. For , Cardoso @cite presented a first order perturbation bound for a set of commuting matrices, and the result was later generalized by Russo @cite . For general , using gradient flows, Afsari @cite studied sensitivity via cost functions and obtained first order perturbation bounds for the diagonalizer. Shi and Cai @cite investigated a normalized through a constrained optimization problem, and obtained an upper bound on certain distance between an approximate diagonalizer of a perturbed optimization problem and an exact diagonalizer of the unperturbed optimization problem.
- Over the years, LB codes have been written and optimized for large clusters of commodity CPUs @cite , for application-specific machines @cite @cite @cite and even for FPGAs @cite . More recently work has focused on exploiting the parallelism of powerful traditional many-core processors @cite , and of power-efficient accelerators such as GPUs @cite @cite or Xeon-Phi processors @cite . As diversified HPC architectures emerge, it is becoming more and more important to have robust methodologies to port and maintain codes for several architectures. This need has sparked the development of frameworks, such as the Open Computing Language (OpenCL), allowing to write portable codes, that can be compiled (with varying degrees of efficiency) for several accelerator architectures. OpenCL is a low level approach: it usually obtains high performances at the price of substantial adjustments in the code implying large human efforts and seriously posing a threat to code correctness and maintainability.
- Very recently have described an implementation of a MPI Lattice Boltzmann code with OpenACC @cite ; however portability of code and performances across different architectures have not been analyzed; to the best of our knowledge, this paper is the first work discussing these issues for OpenACC.
- The first DFA attack on PRESENT was published in 2010 by G. Wang and S. Wang @cite . Their attack aimed at a single nibble and they were able to recover the secret key with the computational complexity of @math by using 64 pairs of correct and faulty cipher texts on average.
- @cite proposed a fault-propagation pattern based DFA and demonstrated this technique on PRESENT and PRINT ciphers. The attack on PRESENT-80 and PRESENT-128 uses 8 and 16 faulty cipher texts on average, respectively, and reduces the master key search space to @math and @math .
- @cite used a combination of differential fault analysis and statistical cryptanalysis techniques to attack lightweight block ciphers. They tested their methodology on PRESENT-80 and PRINT-48. The attack on PRESENT is aimed at middle rounds of algorithm, using single random S-box and multiple S-boxes fault attack. The main outcome of the paper is an extension of the fault model from the usual fault model, which aims at ultimate or penultimate rounds, to the other rounds as well.
- @cite presented two DFA attacks on PRESENT, the first one attacks a single bit and the second one attacks a single nibble of the intermediate state. They were able to recover the secret key with 18 faulty cipher texts on average, using the second attack.
- The most efficient attack so far was proposed by @cite . They used a 2-byte random fault model, attacking the algorithm state after round 28. For PRESENT-80, they needed two 2-byte faults and for PRESENT-128 they needed three 2-byte faults and an exhaustive search of @math on average.
- Our research is related to a large body of work in neural-network compression and speed improvement (e.g., @cite @cite @cite @cite @cite ). Specifically, the work of @cite and other similar approaches suggest to reduce the memory-related issues of machine learning models by introducing a low-dimensional "bottleneck" before the output, as we do in this paper. However, to the best of our knowledge, we are the first to investigate the use of these techniques in the context of decision-tree ensembles. We can say that our work is a blend of the interpretation introduced by @cite and recent network compression techniques.
- In the last decade several works in the robotic community addressed the problem of developing robust place recognition @cite @cite @cite @cite @cite and semantic classification @cite @cite @cite approaches using visual data. In particular, focusing on place categorization from monocular images, earlier works adopted a two-step pipeline: first, hand-crafted features, such as GIST @cite , CENTRIST @cite , CRFH @cite or HOUP @cite , are extracted from the query image, and then the image is classified into one of the predefined categories using a previously-trained discriminative model (, Support Vector Machines). Similarly, earlier studies on visual-based place recognition and loop closing also considered hand-crafted feature representations @cite @cite @cite .
- Recent works have also shown that the NBNN can be successfully employed for place recognition and categorization tasks @cite @cite @cite . Kanji @cite introduced a NBNN scene descriptor for cross-seasonal place recognition. In a later work @cite , Kanji extended this approach by integrating CNN-based features and PCA, deriving a PCA-NBNN model for addressing the problem of self-localization in case of images with small view overlap. Kuzborskij @cite proposed a multi-scale parametric version of the NBNN classifier and demonstrated its effectiveness in combination with precomputed CNN descriptors for scene recognition. Our work is inspired by @cite . However, the proposed learning model is based on a fully-convolutional network which can be trained in an end-to-end manner. Therefore, it is significantly faster and more accurate than @cite .
- The work was later extended by @cite @cite where they proposed model inversion attacks on machine learning algorithms by exploiting confidence information revealed by the model. For instance, when applied to facial recognition systems, they show that it is possible to reconstruct images about a particular label known to the adversary.
- Recently, the work of Tram e @cite shows that machine learning models is possible when taking into consideration only the predictions provided by the model. Membership inference attacks were developed by @cite . Here, the adversary is given black-box access to the model and can infer whether a certain record was originally in the training set.
- @cite use deep learning to infer and reveal the identity of subjects behind blurred images. In their work, @cite show that an adversarially crafted input can be fed to deep learning models and make them prone to error, i.e., make the model misclassify the input therefore producing incorrect outputs. For example, a STOP sign on the road can be subtly modified to look the same to human eyes, but that is classified as another sign by a trained model. The work was extended in @cite @cite @cite @cite .
- Collaborative deep learning proposed by Shokri and Shmatikov @cite uses DP to obfuscate shared parameters while @cite propose to apply DP to the parameters during training. DP was used in deep auto-encoders in @cite .
- Covert channels, however, can be used to defeat DP-protected databases as shown in the work of @cite . In general, privacy cannot be guaranteed if auxiliary information (outside the DP model) is accessible to the adversary @cite . At NDSS'16, it was shown by @cite that DP at a certain granularity is not effective in real-life scenarios where data such as social data, mobile data, or medical records have strong correlations with each other. Note that it's a matter of setting DP granularity right and DP is not being violated at all.
- A centralized approach to deep learning forces multiple participants to pool their datasets into a large central training set on which it is possible to train a model. This poses serious privacy threats, as pointed out by Shokri and Shmatikov @cite , and distrustful participants may not be willing to collaborate.
- Considering the security and privacy issues described above, Shokri and Shmatikov @cite introduce a new collaborative learning approach, which allows participants to train their models, without explicitly sharing their training data. They exploit the fact that optimization algorithms, such as Stochastic Gradient Descent (SGD), can be parallelized and executed asynchronously. Their approach includes a selective parameter sharing process combined with local parameter updates during SGD. The participants share only a fraction of their local model gradients through a Parameter Server (PS). Each participant takes turns and uploads and downloads a percentage of the most recent gradients to avoid getting stuck into local minima. This process only works if the participants agree in advance on a network architecture @cite .
- Close to the present paper, @cite derive perturbation bounds to assess the robustness of approximate MCMC algorithms. The assumptions upon which their results rely are: the original chain is uniformly contractive in the total variation norm (this implies uniform ergodicity); and the perturbation is sufficiently small (in the operator norm induced by the total variation norm). The main results of their paper are: the perturbed kernel is uniformly contractive in the total variation norm; the perturbed stationary distribution is close to the original stationary distribution in total variation; explicit bounds on the total variation distance between finite time approximate sampling distributions and the original stationary distribution; explicit bounds on total variation difference between the original stationary distribution and the mixture of finite time approximate sampling distributions; and explicit bounds on the MSE for integral approximation using approximate kernel and the true kernel. The results derived by @cite are applied within the same paper to a wide variety of approximate MCMC problems including low rank approximation to Gaussian processes and subsampling approximations.
- Lastly, Rudolf and Schweizer @cite also use the Wasserstein topology. They focus on approximate MCMC algorithms, with applications to autoregressive processes and stochastic Langevin algorithms for Gibbs random fields. Their results use the following assumptions: the original kernel is Wasserstein ergodic; a Lyapunov drift condition for perturbed kernel is given, with drift function @math ; @math has finite expectation under the initial distribution; and the perturbation operator is uniformly bounded in a @math -normalised Wasserstein norm. Their main results are: explicit bounds on the Wasserstein distance between the original and perturbed finite time sampling distributions; and explicit bounds on the Wasserstein distance between stationary distributions.
- Performance variability due to resource sharing can significantly detract from the suitability of a given architecture for a workload as well as from the overall performance realized by parallel workloads @cite . Over the last decade there have been studies to analyze the sources of performance degradation and several solutions have been proposed. In this section, we first detail some of the existing work that copes with I O congestion and then we present some of the theoretical literature that is similar to our problem.
- While many other studies suggest that I O congestion is one of the main problems for future scale platforms @cite @cite , few papers focus on finding a solution at the platform level. Some paper consider application-side I O management and transformation (using aggregate nodes, compression etc) @cite @cite @cite . We consider those work to be orthogonal to our work and able to work jointly. Recently, numerous works focus on using machine learning for auto tuning and performance studies @cite @cite . However these solution also work at the application level for IO-scheduling and do not have a global view of the I O requirements of the system and they need to be supported by a platform level I O management for better results.
- The study from @cite offers ways of isolating the performance experienced by applications of one operating system from variations in the I O request stream characteristics of applications of other operating systems. While their solution cannot be applied to HPC systems, the study offers a way of controlling the coarse grain allocation of disk time to the different operating system instances as well as determining the fine-grain interleaving of requests from the corresponding operating systems to the storage system.
- Closer to this work, online schedulers for HPC systems were developed such as our previous work @cite , the study by @cite , and a solution proposed by @cite . @cite , the authors investigate the interference of two applications and analyze the benefits of interrupting or delaying either one in order to avoid congestion. Unfortunately their approach cannot be used for more than two applications. Another main difference with our previous work is the light-weight approach of this study where the computation is only done once.
- Our contributions herein relate to three different areas of prior work: consumer modeling from a microeconomics and marketing perspective @cite , time-aware recommender systems @cite @cite @cite @cite , and PU learning @cite @cite @cite @cite @cite @cite . The extensive consumer modeling literature is concerned with descriptive and analytical models of choice rather than prediction or recommendation, but nonetheless forms the basis for our modeling approach. A variety of time-aware recommender systems have been proposed to exploit time information, but none of them explicitly consider the notion of time utility derived from inter-purchase durations in item categories. Much of the PU learning literature is focused on the binary classification problem, e.g. @cite @cite , whereas we are in the collaborative filtering setting. For the papers that do examine collaborative filtering with PU learning or learning with implicit feedback @cite @cite @cite @cite , they mainly focus on media recommendation and overlook users' demands, thus are not suitable for durable goods recommendation.
- The Dynamic Frameskip Deep Q-network [ @cite ] proposes to use multiple time scales of action repetition by augmenting the Deep Q Network (DQN) [ @cite ] with separate streams of the same primitive actions corresponding to each time scale. This way, the time scale of action repetition is dynamically learned. Although this framework leads to a significant improvement in the performance on a few Atari 2600 games, it suffers from not being able to support multiple time scales due to potential explosion of the action space and is restricted to discrete action spaces. @cite also explore learning macro-actions composed using the same action repeated for different time scales. However, their framework is limited to discrete action spaces and performance improvements are not significant.
- The spirit of upsampling feature maps through learnable parameters is known as deconvolution, which is widely applied in different domains @cite @cite @cite @cite . Shelhamer @cite first put forward a novel structure to do pixel-wise semantic segmentation via learnable deconvolution. Recently, Newell @cite proposed an hourglass structure where feature maps are zoomed in and out through stacks for pose estimation. Our model is inspired by these works and yet have distinctions in several ways: First, to the best of our knowledge, our work is the first in using the decovlution structure for object proposal. Second, existing approaches either concatenate all features @cite or use the final feature map for prediction @cite , while we carefully design a network to select features at different locations (thus resolution varies) of a network for objects, , low resolution features for large objects. With such a philosophy in mind, we have each object equipped with suitable features at a proper resolution.
- Researchers are aware of the benefit of using features from different resolution levels. Jie @cite proposed a scale-aware pixel-wise proposal framework where two separate networks are learned to handle large and small objects, respectively. Yang @cite introduced a scale-dependent pooling scheme and exploited appropriate features depending on the scale of candidate proposals. Recently, Liu @cite proposed a fast end-to-end learning detector where different feature layers from the VGG model output individual predictions. Howerver, these works still use the zoom-out network structure and have limitations stated in Section .
- Bounding box regression is widely used in object detection and region proposal @cite @cite @cite @cite . Iterative bounding box regression is recently proved to be effective at the testing stage to improve the localization accuracy in many works @cite @cite @cite . The bounding boxes approach closer to real locations step by step during iterative testing @cite . Our work in bounding box regression involves both training and testing, which is not fully investigated in previous work.
- The analysis of sequential data has important applications in areas like natural language processing, data compression, behavioral modeling or bioinformatics @cite @cite @cite . Considering the focus of this paper, here we limit our review of the relevant literature to works addressing the modeling of (i) sequential data on pathways in graphs, or (ii) time-stamped data on temporal or dynamic graphs.
- The idea of active search for localization is not brand new. To name a few, saccade and fixate" biological pattern were explored in the field of visual attention @cite @cite @cite . In @cite , proposed to estimate pose through cascaded regression steps learnt through gradient descent etc. Latest works on object localization managed to exploit the power of deep learning and achieved more competitive results @cite @cite @cite @cite @cite .
- In @cite , proposed a recurrent neural network (RNN) based localization network that accumulatively finds numbers from the cluttered translated MNIST dataset. In @cite , proposed to explore statistical relations between consecutive windows and based their model on R-CNN @cite for generic object detection. In @cite , proposed AttentionNet" where at each current window, a CNN was trained to predict quantized weak directions for the next step to simulate a gradual attention shift. In @cite @cite , the authors explicitly deployed deep RL and achieved promising performance with much fewer window evaluations than main stream region proposal methods.
- However, none of these works examine the problem of joint active search of multiple objects. In order to exploit beneficial contextual information among differnt objects, we present collaborative multi-agent deep RL. We instantiate our idea with Caicedo and Lazebnik @cite as a single active search model baseline, but our mechanism could be applied to other baseline models with minor adaptation.
- Recently, the field of reinforcement learning revives with the power of deep learning @cite @cite . Equipped with effective ideas such as experience replay etc., conventional methods, e.g. Q-learning, work out very effectively in learning good policies without intermediate supervision for challenging tasks. Our model benefits from these effective ideas in a similar way as recent active methods @cite @cite but with specific novel designs motivated by the joint search problem of interest.
- Multi-agent machine learning and reinforcement learning are not new topics. However, conventional collaborative RL methods mostly explore hand-crafted communication protocols @cite @cite . During the preparation of this work, we realize two interesting work that proposed to facilitate learnable communication protocols for multi-agent deep RL @cite @cite and demonstrate superior performance to non-communication counterparts on control management and game related tasks. In @cite , proposed CommNet" where policy networks are facilitated with learnable communication channels learnt via back-propagation. In @cite , proposed Differentiable Inter-Agent Learning" to effectively learn communication for deep Q-networks.
- Our proposal share the idea of utilizing back-propagation or designing differentiable communication channels but have different cross network structure with gates and a novel joint sampling Q-learning method. Specifically, our cross network structure used explicit gating mechanism to allow a specific agent to be responsible for certain actions. This is motivated by the problem of object search where one agent usually has primary contribution to the policy. Also different from the training of the unfolded RNNs as in @cite , where long range back-propagation may be less effective, our joint sampling design facilitates immediate updates of the parameters and could be easily incorporated into the deep Q-learning algorithm by introducing an auxiliary concept of virtual agent implementation.
- Another interesting connection relates to works of pose estimation, landmark detection etc., where one is assigned the task of localizing the positions of all joints or landmarks potentially highly related due to physical constraints. Explicit learning the relationship of different joints landmarks has been studied in such literatures @cite @cite . However similar ideas were rarely explored in general localization problems where such interactions are relatively implicit. Our work partially fills the gap here and proves the concept is similarly applicable in many object-level localization problems.
- In this section, we briefly introduce the existing works on Android malware analysis. They are categorized into static @cite @cite , dynamic @cite @cite , and hybrid @cite @cite .
- Static analysis techniques perform code disassembling and decompilation without actually running it. This approach is undermined by the use of various code transformation techniques @cite . We may divide static analysis based techniques into the following categories: i) This analysis deals with extracted syntactic pattern features @cite @cite @cite , and create a unique signature matching for a particular malware. However, such signature cannot handle new variants of existing known malware. Moreover, the signature database should be updated to handle new variants. AndroSimilar @cite has been proposed to detect zero-day variants of the known malware. It is an automated statistical feature signature-based method. However, it is sensitive due to code transformation methods. ii) The Manifest file contains important meta-data about the components (i.e., activities, services, receivers, etc.) and required permissions. There are some methods that have been proposed to extract such information and subject it to analysis @cite @cite . iii) Discovering the dangerous permission request is not adequate to proclaim a malware app, but nevertheless, permissions mapping requested and used permissions are an important risk identification technique @cite @cite .
- Dynamic analysis techniques allow us to learn malicious activities. Android app execution is event-based with asynchronous multiple entry points. It is important to trigger those events. Dynamic techniques are divided into the following two categories: i) Some malicious apps may cause Denial of Service (DoS) attacks by over-utilizing the constrained hardware resources. Range of parameters such as CPU usage, memory utilization statistics, network traffic pattern, battery usage and system-calls for benign and malware apps are gathered from the Android subsystem. Automatic analysis techniques along with machine learning techniques are used @cite @cite @cite . ii) It is related to abnormal behaviors such as sensitive data leakage and sending SMS emails @cite @cite @cite @cite .
- Network coding can be viewed as a generalized store-and-forward network routing principle. Messages from different source nodes are combined and regenerated at the intermediate nodes according to algebraic encoding. Besides the well-known advantages of throughput enhancement @cite @cite @cite and data robustness @cite , the recent studies on network coding focus on reliability and security enhancement.
- Network coding can improve the efficiency of data recovery process when storage nodes fail in distributed storage systems. It was proved that the data recovery problem of distributed storage systems is equivalent to the multicasting problem of network routing @cite . The authors of @cite designed a cooperative network coding recovery mechanism for multiple node failures. A proxy-based multiple cloud storage system with the feature of fault tolerance was built based on the network coding storage scheme @cite . A network coding method called Regenerating Code was proposed to improve the repair process of distributed storage systems @cite . Different from erasure coding, the repaired data fragments are mixed in intermediate nodes, thereby reducing the repair bandwidth. The authors of @cite applied network coding to optimize the reliability performance of frequently accessed data in cloud storage systems.
- Another research area of network coding is to prevent data being eavesdropped during transmission. The information-theoretical security problem for an untrusted channel was first discussed in @cite . A network coding system was built to prove that a wiretapper cannot obtain any information from the transmitted message @cite . A weaker type of security issue was investigated in @cite , where a node can decode packets only after receiving sufficient linear independent encoded data. The construction of a secure linear network code for a wiretap network was presented in @cite .
- The secrecy capacity for a network-coded cloud storage system was investigated in @cite @cite , where the secrecy capacity is defined as the maximum amount of data that can be securely stored under the perfect secrecy condition. The perfect secrecy condition ensure the eavesdropper cannot obtain any information of source data. The secrecy capacity for nodes with different storage capacities was derived in @cite . The coding scheme that can achieve the storage upper bound of secrecy capacity was proposed in @cite . The maximum data size being stored under the perfect secrecy condition for any number of eavesdropped nodes was determined in @cite . The authors of @cite considered how to achieve the information-theoretical secrecy when an eavesdropper can access some data in the storage nodes.
- For secure storage over multiple clouds, similar to this work, the authors of @cite proposed a security protection scheme to ensure that no symbols can be decoded by an eavesdropper, which is weaker than perfect secrecy. In @cite , a link eavesdropping problem in a network-coded cloud storage system was investigated. A publicly verifiable protocol for network coded cloud storage was proposed in @cite .
- Consensus algorithms have attracted a lot of attention in areas requiring high-availability and fault tolerance. Mainly because they provide the ability to replicate a service across multiple servers, in a way that allows the service to continue working when some of the servers fail. This scheme of using consensus to achieve a replicated service is called state machine replication'' (SMR) @cite .
- Multiple algorithms fall in the SMR category: the original Paxos @cite , ZooKeeper's @cite consensus algorithm Zab @cite , Raft @cite which aims to simplify Paxos, and Viewstamp replication @cite .
- Others have improved Paxos to be more concurrent, by removing the false dependencies between log entries. For example, Generalized Paxos @cite allows concurrent log entries as long as they commute. Multicoordinated Paxos @cite and Egalitarian Paxos @cite achieve similar improvements.
- DO-RAMBO @cite handles multiple objects. However, it still doesn't support , and it assumes a fixed namespace of the multiple objects it handles. Thus, a general key-value cannot be used directly, since each key must be known as part of the fixed namespace.
- Another system providing consistent key-value like behavior appears in @cite , which doesn't support but it seems that it can be added easily. However, @cite assumes FIFO links, which incur false dependencies between operations. It might be possible to remove this assumption, but it would probably make the failure handling much more complex.
- Lastly, high-throughput systems like @cite @cite assume an external configuration service (i.e., Paxos), while systems like @cite @cite add additional assumptions to the consistency model.
- Finally, trained a long short-term memory model for selecting answers to TREC questions. Their model takes words from question and answer sentences as input and returns a score measuring the relevancy of an answer with respect to a given question. A recent work close to ours is @cite , where the authors build a neural network for solving Task A of SemEval. However, this does not approach the problem as MTL.
- Network-based Intrusion Detection Systems (NIDS) are deployed on various networks segments, typically at the edge of the network, to monitor all incoming and outgoing network traffic @cite @cite . The systems use different strategies to detect malicious communication which can be broadly characterized into systems analyzing payload or payload statistics, systems processing proxy logs or Netflows, systems that build rules or signatures in a separate controlled environment, and systems relying on additional data sources such as DNS records.
- Payload packet inspection has been proposed in the PAYL system @cite , where the fingerprints are represented as statistical distributions of bytes in a payload. The Provex system @cite extends the inspection to encrypted data by maintaining a list of known decryption methods. In BotHunter @cite , the anomalies found by PAYL are combined with anomaly detections from inbound and outbound scans and correlated with intrusion reports based on Snort rules @cite . BotSniffer @cite computes the anomaly scores from peaks in the number of entries for each destination IP and port pair aggregated over a longer time period. The anomaly detectors are reused in the BotMiner system @cite to create clusters of similar host activities (so called A-plane). The A-plane is correlated with the clustering of communication patterns represented by the flow statistics (C-plane) to produce the final report. Despite their effectiveness, the packet analysis NIDS systems cannot always be deployed since the increasing network bandwidth increases the rates of data processing and packet inspection may violate privacy protection imposed by internal local network policies. The proposed system does not use packet inspection or clustering which allows it to achieve fast processing speeds necessary for online deployment.
- Our detection system classifies individual flows and therefore can report the threats immediately as they occur. Compared to clustering approaches, this has the advantage that the system does not need several potential infections that are otherwise needed to initiate the clustering. In addition, the computational costs are much lower @cite @cite .
- Our architecture can easily incorporate new threats by retraining the classifiers. As shown in , the retraining can use a small set of samples which is not possible with clustering approaches that remove them as not interesting @cite @cite @cite @cite . The multi-level design makes it possible to retrain individual levels by identifying false negatives from the analysis of the inputs and outputs at various levels.
- Unsupervised morpheme segmentation traces back to @cite , which falls under the framework of Letter Successor Variety (LSV) which builds on the hypothesis that predictability of successor letters is high within morphemes and low otherwise. The most dominant pieces of work on unsupervised morpheme segmentation, Morfessor @cite @cite @cite and Linguistica @cite adopt the Minimum Description Length (MDL) principle @cite : they aim to minimize describing the lexicon of morphs as well as minimizing the description of an input corpus. Morfessor has a widely used API and has inspired a large body of following work @cite @cite .
- The unsupervised original implementation was later adapted @cite @cite to allow for minimal supervision. Another work on minimally supervised morpheme segmentation is @cite which relies on Adaptor Grammars (AGs) @cite . AGs learn latent tree structures over an input corpus using a nonparametric Bayesian model @cite .
- @cite use Conditional Random Fields (CRF) for morpheme segmentation. In this supervised method, the morpheme segmentation task is modeled as a sequence-to-sequence learning problem, whereby the sequence of labels defines the boundaries of morphemes @cite @cite . In contrast to the previously mentioned generative approaches of MDL and AG, this method takes a discriminative approach and allows for the inclusion of a larger set of features. In this approach, CRF learns a conditional probability of a segmentation given a word @cite @cite .
- All these morpheme segmenters rely solely on orthographic features of morphemes. Semantics were initially introduced to morpheme segmenters by @cite , using LSA to generate word representations and then evaluate if two words are morphologically related based on semantic relatedness, as well as deterministic orthographic methods. Similarly, @cite use edit distance and mutual information as metrics for semantic and orthographic validity of a morphological relation between two words. Recent work in @cite , inspired by the log-linear model in @cite incorporates semantic relatedness into the model via word representations. Other systems such as @cite rely solely on evaluating two words from a semantic standpoint by the use of a two-layer neural network.
- MORSE introduces semantic information into its morpheme segmenters via distributed word representations while also relying on orthographic features. Inspired by the work of @cite , instead of merely evaluating semantic relatedness, we are the first to evaluate the morphological relationship via the difference vector of morphologically related words. Comparing the difference vectors of multiple pairs across the corpus following the same morphological relation, gives MORSE a vocabulary-wide evaluation of morphological relations learned.
- Our work also overlaps with to the area of opinion summarization, which constructs natural language summaries for multiple product reviews @cite . Most previous work extracts opinion words and aspect terms. Typical approaches include association mining of frequent candidate aspects @cite @cite , sequence labeling based methods @cite @cite , as well as topic modeling techniques @cite . Recently, word embeddings and recurrent neural networks are also used to extract aspect terms @cite @cite .
- Recommendation systems suggest to a user new products and services that might be of their interest. There are two main approaches, which are content-based and collaborative-filtering (CF) based @cite @cite , respectively. Most existing social recommendation systems are CF-based, and can be further grouped into model-based CF and neighborhood-based CF @cite @cite . Matrix Factorization (MF) is one of the most popular models for CF. In recent MF-based social recommendation works, user-user social trust information is integrated with user-item feedback history (e.g., ratings, clicks, purchases) to improve the accuracy of traditional recommendation systems, which only factorize user-item feedback data @cite @cite @cite .
- There has been work integrating sentiment analysis and recommendation systems, which use recommendation strategies such as matrix factorization to improve the performance of sentiment analysis @cite @cite . These methods typically use ensemble learning @cite or probabilistic graph models @cite . For example, who proposed a factor graph model to recommend opinion rating scores by using explicit product features as hidden variables.
- The extraction of ancillary information from biometric traits is known as . As defined by Dantcheva al @cite , "[s]oft biometric traits are physical, behavioral, or material accessories, which are associated with an individual, and which can be useful for recognizing an individual."
- Gender is one soft biometric attribute, and gender recognition has been explored using biometric traits such as faces, fingerprints, gait and irises. The earliest work on gender-from-iris @cite used a classifier based on decision trees, and reported an accuracy of about 75 extracted hand-crafted geometric and texture features from log-Gabor-filtered images in a dataset of over 57,000 images. The training and testing sets were not person-disjoint, which typically results in a higher estimated accuracy than can be expected for new persons.
- Later, @cite used a Support Vector Machine (SVM) classifier with features extracted using spot and line detectors and Law's texture measures. They used a dataset of 600 images and a cross-validation protocol with 2, 5 and 10 folds, with person-disjoint partitions. They considered both race-from-iris and gender-from-iris, and their classification accuracy on gender-from-iris ranged from @math 83 In the work of @cite , using an SVM to classify Local Binary Pattern (LBP) features extracted from 3,000 iris images yielded an accuracy of @math 80 20 @math train test @math 72 28 @math 60
- An overview of the techniques and results used so far is presented in Table . None of these works has looked systematically at the effect of cosmetics on accuracy of predicting gender-from-iris. Most of the works do not use a subject-disjoint training and testing, especially those reporting the highest accuracy. And these works report accuracy from a single random split into train-test data, rather than a mean of N random splits. Apart from @cite , no other research employed neural networks for this task.
- We already discussed related work in the area of SMT solving and will therefore focus here on related work in optimization. Optimization problems with logical formulae frequently arise in practice; examples in process systems engineering include: choosing between multiple possible treatment technologies in a waste water treatment plant @cite and designing distillation column configurations @cite . Typically used techniques in process engineering will first translate logical rules into mathematical constraints (e.g., as detailed in @cite ) and then solve the resulting MINLP problem; this process of formulating a mathematical optimization problem based on logical constraints and then transforming the resulting problem into a MINLP problem is known as Generalized Disjunctive Programming (GDP) @cite . Our proposed method of solving MINLP using SMT technology is therefore complementary to GDP; GDP is an excellent formulation technology, but transforming to a MINLP problem and then using traditional optimization methods eliminates the possibility of directly exploiting logical constraints in the original formulation.
- With respect to MINLP solvers based on traditional branch-and-bound approaches @cite @cite @cite @cite @cite , the advantage of the overall approach taken in tool is that -- as in most SMT-based approaches -- there is support for incremental strategies such as push pop. Typical MINLP solvers will fathom, i.e. discard, regions of the tree as soon as they determine that this region cannot include the global solution.
- Most methods for stochastic optimal control focus on dynamic programming @cite @cite or approximate dynamic programming @cite . More closely related to this work, however, are relaxation methods. Unlike approximate dynamic programming, these methods provide bounds on the achievable optimal values. The most closely related methods focus on occupation measures @cite @cite @cite . These works rely on the general insight that a convex relaxation of an optimization problem can be obtained by examining randomized strategies @cite . Such relaxation methods apply to stochastic optimal control problems, but it is not clear that they can be used for analysis problems. In contrast, our relaxation methodology applies to both cases. A more removed relaxation method is studied in @cite @cite . These works use duality to relax the constraint that controllers depend causally on state information. Also related is the work of @cite @cite which uses a combination of moment dynamics and deterministic optimal control to find approximate solutions to stochastic optimal control problems. This work, however, only considers systems with closed moments. Furthermore, this work assumes a fixed parametric form for the controller, and does not obtain a relaxation of the original stochastic control problem.
- The volume-based approach voxelizes a 3D mesh model for a 3D representation. Several CNN networks have been proposed to classify the 3D shapes directly. Examples include the 3D ShapeNet @cite , the VoxNet @cite and the SubVolume supervision method @cite . Although the network architectures of volume-based methods are typically shallow (e.g., the VoxNet model consists of two convolutional layers and one fully connected layer), they strike a balance between the number of available training samples and the network model complexity. Volume-based methods have two drawbacks. First, to control computational complexity, the resolution of a 3D voxel model is much lower than that of its corresponding 2D view-based representation. As a result, high frequency components of the original 3D mesh are sacrificed. Second, there are few pretrained CNN models on 3D data, volume-based networks have to be trained from scratch. Although a volumetric representation preserves the 3D structural information of an object, the performance of classifying 3D mesh models directly is still lower than that of classifying the corresponding view-based 2D models.
- , @cite pursued the medium access methodology and proposed a quality of service based medium access control (MAC) scheduling approach to avoid inter-WBAN interference and introduce a fuzzy inference engine for intra-WBAN scheduling so as to avoid interference within WBANs. Other approaches pursued the resource allocation include @cite , @cite . , @cite proposed a distributed interference detection and mitigation scheme through using adaptive channel hopping for intra- and inter-WBAN interference mitigation. Whilst, , @cite proposed a dynamic resource allocation scheme for intra- and inter- WBAN interference mitigation among multiple coexisting WBANs through using orthogonal channels for high interfering nodes.
- VO algorithms estimate the incremental ego-motion of a camera. A traditional VO algorithm, illustrated in Fig. a., operates by extracting features in an image, matching the features between the current and successive images and then computing the optical flow. The motion can then be computed using the optical flow. The fast semi-direct monocular visual odometry (SVO) algorithm @cite is an example of a state-of-the-art VO algorithm. It is designed to be fast and robust by operating directly on the image patches, not relying on slow feature extraction. Instead, it uses probabilistic depth filters on patches of the image itself. The depth filters are then updated through whole image alignment. This visual odometry algorithm is efficient and runs in real-time on an embedded platform. Its probabilistic formulation, however, makes it difficult to tune and it also requires a bootstrapping procedure to start the process. As expected, its performance depends heavily on the hardware to prevent tracking failures - typically global shutter cameras operating at higher than 50 fps needs to be used to ensure the odometry estimates remain accurate.
- Some deep-learning approaches have been proposed for visual odometry, however, to the best of our knowledge, a neural network approach has never been used in any form for end-to-end monocular visual-inertial odometry. In @cite , a Stereo-VO method is presented where they extract motion by detecting synchronicity'' across the stereo frames. @cite investigated the feasibility of using a CNN to extract ego-motion from optical flow frames. In @cite the feasibility of using a CNN for extracting the homography relationship between frame pairs was shown. Recently, @cite use an RNN to fuse the output of a standard marker-based visual pose system and IMU, thereby eliminating the need for statistical filtering.
- Recently, the hill-climbing strategy has been also explored @cite @cite @cite . Basically, no sophisticated space partitioning is required in the original idea @cite . The NNS starts from a group of random seeds (random location in the vector space). The iterative procedure tranverses over the kNN graph by depth-first search. Guided by kNN graph, the search procedure ascents closer to the true nearest neighbor in each round. It takes only few number of rounds to converge. Since the procedure starts from random position, it is likely to be trapped in local optima. To alleviate this issue, an intervention scheme is introduced in @cite . However, this method turns out to be sub-optimal. Recently, this issue is addressed by indexing the reference set by multiple k-d trees, which is similar as FLANN @cite . When one query comes, the search tranverses over these k-d trees. Potential candidates are collected as the seeds for hill-climbing search. Such that the search process could be faster and the possibility that is trapped in a local optima is lower. Unfortunately, such kind of indexing causes nearly one times of memory overhead to load the k-d trees, which makes it inscalable to large-scale search task.
- Hill-climbing strategy is adopted in our search scheme due to its simplicity and efficiency. Different from @cite @cite , the search procedure is directed by residue vector quantization (RVQ) @cite . Firstly, the candidate vectors are quantized by RVQ. However, different from compressional methods, the quantized codes are used only to build inverted indexing. Candidate vectors sharing the same RVQ codes (indexing key) are organized into one inverted list. In comparison to @cite , only 4 bytes are required to keep the index for one vector. When a query comes, the distance between the query and the RVQ codes are calculated, the candidates reside in the lists of top-ranked codes are taken as the seeds for hill-climbing procedure. The inverted indexing built from RVQ codes plays the similar role as k-d trees in @cite , while requiring very small amount of memory.
- It is worth noting that some studies developed mobile applications to serve their approaches through which users can benefit from the automated leaf classification algorithms. For example, LeafSnap @cite which is developed for iOS, is one of the first released applications of this kind. Subsequently, Pl@ntNet @cite mobile application, a more sophisticated one, has been released. A similar application MobileFlora @cite , which is for flower recognition, is also relevant. We also built a mobile application to showcase our tree identification system. This application can be freely downloaded from Google Play Store https: play.google.com store apps details?id=com.payinekereg.treelogy .
- * Incidence variety compactification. The problem of the compactification of the strata is extensively studied from different approaches in a joint work of Bainbridge, Chen, Gendron, Grushevsky, and Moeller (see @cite and @cite ). Their compactification (called incidence variety compactification ) is slightly different from the one that we use here. We will recall their definitions in since we will make use of some of their results.
- * Moduli space of twisted canonical divisors. @cite , Farkas and Pandharipande proposed another compactification of the strata. Let @math be non-integers such that @math . Let @math be a vector of positive integers of lenght @math and let @math be vector of non-negative integers of length @math that is complete for @math and @math . We recall that @math is the locus of smooth curves such that @math and that we denote by @math its closure in @math . @cite , Farkas and Pandharipande defined the space of twisted canonical divisors denoted by @math . The space of twisted canonical divisors is a singular closed subspace of @math such that @math is one of the irreducible components of @math .
- We assume that @math . In the appendix of @cite , Farkas and Pandharipande defined a class @math in @math (or @math ): this class is a weighted sum over the classes of irreducible components.
- We consider the two following functions: Both @math and @math are polynomials for large values of @math (this result is due to Aaron Pixton, see @cite and @cite ). We denote by @math and @math the asymptotic polynomials. The two following conjectures have been proposed.
- These two conjectures are the analogous for differentials of the formula for the so-called double-ramification cycles (DR cycles): the DR cycle is a natural extension of to @math of the cycle in @math defined as the locus of marked curves @math such that @math for any fixed vector of integers @math such that @math (see @cite ).
- * Compactification via log-geometry. J 'er 'emy Gu 'er 'e constructed a moduli space of rubber'' differentials using log geometry. He proved that this space is endowed with a perfect obstruction theory. Moreover, if @math , this moduli space surjects onto the moduli space of twisted canonical divisors and the class @math is the push-forward of the virtual fundamental cycle (see @cite ).
- If @math has only positive values, Dawei Chen and Qile Chen have also used log geometry to define a compactification of the strata @math (see @cite ).
- * Induction formula for singularities in families. The central result of the present work is the induction formula of . A similar formula has been proved by Kazarian, Lando and Zvonkine for classes of singularities in families of genus 0 stable maps (see @cite ). Their formula contains only the genus 0 part of our induction formula.
- They gave an interpretation of the induction formula in genus 0 as a generalization of the completed cycle formula of Okounkov and Pandharipande (see @cite ). For, now it is not clear if this generalized completed cycle formula has an extension to higher genera.
- Our computation of cohomology classes of strata of differentials could be useful to compute these numerical invariants. This idea is developed for example in @cite and @cite based on the work of Eskin, Kontsevich, and Zorich @cite (see ). This has been explored in the subsequent paper (see @cite )
- One of the first computational frameworks, proposed by Bermingham @cite in 2009, combined social network analysis with sentiment detection tools to study the agenda of a radical YouTube group: the authors examined the topics discussed within the group and their polarity, to model individuals' behavior and spot signs of extremism and intolerance, seemingly more prominent among female users. The detection of extremist content (on the Web) was also the focus of a 2010 work by Qi @cite . The authors applied hierarchical clustering to extremist Web pages to divide them into different pre-imposed categories (religious, anti immigration, etc.).
- Scanlon and Gerber proposed the first method to detect cyber-recruitment efforts in 2014 @cite . They exploited data retrieved from the Dark Web Portal Project @cite , a repository of posts compiled from 28 different online fora on extremist religious discussions (e.g., Jihadist) translated from Arabic to English. After annotating a sample of posts as recruitment efforts or not, the authors use Bayesian criteria and a set of textual features to classify the rest of the corpus, obtaining good accuracy, and highlighted the most predictive terms.
- A few studies explored unconventional data sources: one interesting example is the work by Vergani and Bliuc @cite that uses sentiment analysis (Linguistic Inquiry and Word Count @cite ) to investigate how language evolved across the first 11 Issues of Dabiq, the flagship ISIS propaganda magazine. Their analysis offers some insights about ISIS radicalization motives, emotions and concerns. For example, the authors found that ISIS has become increasingly concerned with females, reflecting their need to attract women to create their utopia society, not revolving around warriors but around families. ISIS also seems to have increased the use of internet jargon, possibly to connect with the identities of young individuals online.
- The generative adversarial networks (GANs) learn two sub-networks: a generator and a discriminator. The discriminator reveals whether a sample is generated or real, while the generator produces samples to cheat the discriminator. The GANs are first proposed by Goodfellow al @cite to generate images and gain insights into neural networks. Then, DCGANs @cite provides some techniques to improve the stability of training. The discriminator of DCGAN can serve as a robust feature extractor. Salimans al @cite achieve a state-of-art result in semi-supervised classification and improves the visual quality of GANs. InfoGAN @cite learns interpretable representations by introducing latent codes. On the other hand, GANs also demonstrate potential in generating images for specific fields. Pathak al @cite propose an encoder-decoder method for image inpainting, where GANs are used as the image generator. Similarly, Yeh al @cite improve the inpainting performance by introducing two loss types. In @cite , 3D object images are generated by a 3D-GAN. In this work, we do not focus on investigating more sophisticated sample generation methods. Instead, we use a basic GAN model @cite to generate unlabeled samples from the training data and show that these samples help improve discriminative learning.
- Semi-supervised learning is a sub-class of supervised learning taking unlabeled data into consideration, especially when the volume of annotated data is small. On the one hand, some research treats unsupervised learning as an auxiliary task to supervised learning. For example, in @cite , Hinton al learn a stack of unsupervised restricted Boltzmann machines to pre-train the model. Ranzato al propose to reconstruct the input at every level of a network to get a compact representation @cite . In @cite , the auxiliary task of ladder networks is to denoise representations at every level of the model. On the other hand, several works assign labels to the unlabeled data. Papandreou al @cite combine strong and weak labels in CNNs using an expectation-maximization (EM) process for image segmentation. In @cite , Lee assigns a pseudo label'' to the unlabeled data in the class that has the maximum predicted probability. In @cite @cite , the samples produced by the generator of the GAN are all taken as one class in the discriminator. Departing from previous semi-supervised works, we adopt a different regularization approach by assigning a uniform label distribution to the generated samples.
- Some pioneering works focus on finding discriminative handcrafted features @cite @cite @cite . Recent progress in person re-ID mainly consists of advancing CNNs. Yi al @cite split a pedestrian image into three horizontal parts and respectively train three part-CNNs to extract features. Similarly, Cheng al @cite split the convolutional map into four parts and fuse the part features with the global feature. In @cite , Li al add a new layer that multiplies the activation of two images in different horizontal stripes. They use this layer to explicitly allow patch matching in the CNN. Later, Ahmed al @cite improve the performance by proposing a new patch matching layer that compares the activation of two images in neighboring pixels. In addition, Varior al @cite combine the CNN with some gate functions, aiming to adaptively focus on the salient parts of input image pairs, this method is limited by computational inefficiency because the input should be image pairs.
- A CNN can be very discriminative by itself without explicit part-matching. Zheng al @cite @cite directly use a conventional fine-tuning approach (called the ID-discriminative embedding, or IDE) on the Market-1501 dataset @cite and its performance exceeds many other recent results. Wu al @cite combine the CNN embedding with hand-crafted features. In @cite , Zheng al combine an identification model with a verification model and improve the fine-tuned CNN performance. In this work, we adopt the IDE model @cite @cite as a baseline, and show that the GAN samples and LSRO effectively improve its performance. Recently, Barbosa al @cite propose synthesizing human images through a photorealistic body generation software. These images are used to pre-train an IDE model before dataset-specific fine-tuning. Our method is different from @cite in both data generation and the training strategy.
- have recently applied a leverage score-based sampling to the least-squares step of the sparse CP decomposition by showing how leverage scores of an unfolded tensor can be estimated by the leverage scores of the factor matrices @cite . This approach is similar to the way that we bound the coherence of Khatri-Rao products. also use randomization within CP-ALS, specifically for the case of rank reduction, where the input to the algorithm is already in CP format @cite . They use randomization to improve the conditioning of the individual least squares problems in order to compute better overall approximations. The randomization makes each iteration of their method more costly, but they observe faster convergence (and overall running time) than ALS for ill-conditioned problems.
- have applied sketching methods to tensors with provable guarantees @cite . show that this sketch can be computed without reading the entire tensor (in sublinear time) under certain conditions @cite .
- An alternative to sketching is to compress the tensor using lossy methods before computation. Zhou and Cichocki examine the effectiveness of performing a CP decomposition on a compressed representation of the data using the lossy Tucker decomposition to produce a smaller problem size @cite . ParCube @cite compresses the original tensor by directly sampling and performs a decomposition on the result.
- The paper @cite considered a model in which the algorithm observes noisy versions of the oracle's response and established lower bounds on the complexity of convex optimization problems under first order as well as gradient-only oracles. In @cite , complexity lower bounds were obtained for convex optimization problems with a stochastic zero-order oracle. The paper @cite studied the complexity of convex optimization problems under a zero-order stochastic oracle in which the optimization algorithm submits two queries at each iteration and the oracle responds to both queries. These results were extended to the case in which the algorithm makes queries about multiple points at each iteration in @cite . In @cite , the complexity of convex optimization problems was studied under an erroneous oracle model wherein the oracle's responses to queries are subject to absolute relative errors.
- The question on the effectiveness of probabilistic countermeasures against memory-safety vulnerabilities has been a matter of discussion in the literature for the past decade. @cite , discuss the effectiveness of Address Space-Layout Randomization. They noted that in 32 bits architectures, the entropy achieved by ASLR only slightly slows down attackers, while causing performance overheads. They also discuss the increase in security by re-randomization, but different from us, they do this informally and only for ASLR.
- @cite propose to treat the effectiveness of program diversification (i.e. using probabilistic transformations) as a probabilistic dynamic type-checking problem. They illustrate their approach on a C-like language and ASLR. Their approach is formal but involves the modeling of low-level details of programs, such as pointers, whereas our approach abstracts away from concrete programs and focus on countermeasures. Also, they do not explicitly give bounds on the probability of an attack for a given program transformation.
- @cite cast the problem of reasoning on the effectiveness of ASLR as full-abstraction problem. Different from , they consider concrete probability bounds on attackers, and abstract away from malicious inputs by considering arbitrary execution contexts. To this end, they also construct exemplary high and low level languages, and limit themselves to the analysis of ASLR.
- @cite propose to randomize the heap layout and allocation strategy for increased resilience against memory management bugs that are triggered accidentally. This line of work has been extended @cite by considering attackers that deliberately exploit vulnerabilities (using for instance heapspray attacks). Probabilistic security bounds are then derived by reasoning on the proposed countermeasures and successful attack events. Such bounds, although mathematically justified, are obtained informally (i.e. without using a formal reasoning language or framework).
- In our examples we have emphazised single vs. multiple key-sampling for randomized countermeasures and discussed its impact on the bounds. Such a fine-grained re-randomization for many countermeasures and memory layouts has been systematically implemented by Curtsinger and Berger @cite .
- In @cite , Zhang note that for irregular LDPC codes, OMS can incur a significant BER degradation compared to SPA decoding. To fight this effect, they suggest having each CN of degree @math use offset @math , rather than a global @math . The authors use a genetic algorithm to choose the offsets for each degree and, interestingly, find that this offsetting scheme not only matches the performance of SPA but can also lower the BER error floor exhibited by some codes.
- The approach given in @cite is similar to NOMS in that it uses more than just one global offset. The main difference between this approach and NOMS is that the offsets found by @cite do not depend on particular paths through the Tanner graph, and thus in general may have difficulty overcoming the effect of short cycles, especially if the code has only a few different check node degrees. For the BCH codes considered in this work, for example, every check node has the same degree, so the approach in @cite would yield a single @math , thus reverting to OMS. NOMS, on the contrary, has the potential to improve performance even for regular codes. Finally, the genetic algorithm used to learn the offsets in @cite is a random heuristic method, which, unlike gradient-based optimization methods, has been found to perform poorly as the size of the optimization problem increases @cite .
- Besides the class of GANs, there exist other models that also attempt to generate natural images. For example, @cite rendered images by matching features in a convolutional network with respect to reference images. @cite used deconvolutional network to render 3D chair models in various styles and viewpoints. @cite introduced a deep recurrent neutral network architecture for image generation with a sequence of variational auto-encoders to iteratively construct complex images.
- Cons-free programs, combined with various limitations on recursion, were introduced by Jones @cite , building on ground-breaking work by Goerdt @cite @cite , and have been studied by a number of authors (see, e.g., @cite @cite @cite @cite ). The main difference with our work is that we consider full recursion with full non-determinism, but impose constraints not present in the previous literature.
- Characterisation of non-deterministic complexity classes via programming languages remains a largely unexplored area. Bellantoni obtained a characterisation of @math in his dissertation @cite using similar approaches as @cite , but at the cost of having a minimisation operator (as in recursion theory), a restriction later removed by Oitavem @cite . A general framework for implicitly characterising a larger hierarchy of non-deterministic classes remains an open problem.
- We define a simple call-by-value programming language with explicit non-deterministic choice. This generalises Jones' toy language in @cite by supporting different types and pattern-matching as well as non-determinism. The more permissive language actually proofs and examples, since we do not need to encode all data as boolean lists, and have fewer special cases.
- The increasing amount of landmark photos has resulted in numerous methods for landmark retrieval @cite @cite @cite @cite . Hays al @cite presented a feature matching approach to return the K nearest neighbors with respect to the query landmark photo where a query photo and photos in database are represented by aggregating a set of low-level features to perform landmark retrieval. Zhu al @cite proposed to learn the landmark feature by combining low-level features while assisted with Support Vector Machine (SVM). In @cite , a region based recognition method is proposed to detect discriminative landmark regions at patch level, which is seen as the feature for landmark retrieval. To augment semantic interpretation on landmark representation, Fang al @cite presented an effective approach, namely GIANT, to discover both discriminative and representative mid-level attributes for landmark retrieval. Zhu al @cite propose a hypergraph @cite model for landmark retrieval.
- However, these approaches are still using a single query photo for landmark retrieval whilst our approach is focusing on mining robust patterns of landmark photos from an expanded multi-query set. Wang al @cite proposed to learn a mid-level pattern representations for landmark retrieval over multi-query set, which applied the KRIMP @cite algorithm and Minimum-description-length (MDL) principle @cite to mine the compact pattern representation. Unlike that, we propose a deep network based high-level features for landmark retrieval. As a related problem, several scene categorization methods @cite @cite via deep feature learning problem is also proposed. Besides the different problem from ours, we also exploited the user information for deep feature learning for landmark retrieval.
- There are abundant related work towards feature learning via dictionary learning @cite . For instance, Zhu al @cite proposed a weakly-supervised cross-domain dictionary learning method is proposed to learn a reconstructive, discriminative and domain adaptive dictionary. To this end, the weakly labeled data is utilized from the source data set to span the coverage of intra-class diversity over the training data to gain discriminative power. However, they have not studied the problem of landmark retrieval.
- These methods are commonly based on the idea that those particular multiple queries are manually selected or simply retrieved from top-k similar items whilst we automatically determine helpful queries by exploring the latent topics of query landmark as well as the informative user communities. Besides, previous query expansion pipelines are not applicable in the context of social media networks, which cannot be addressed by simple variations of methods in literature. To address the limitation, Wang al @cite proposed a multi-query expansion technique to learn a robust mid-level pattern representation. Based on that, we propose to learn high-level deep feature over multi-query set, which is superior to the mid-level representation in @cite .
- Geo-tagging refers to adding geographical identification metadata into various multimedia data such as images @cite in websites, blogs, and photo-sharing web-services @cite @cite @cite . Associating time and location information (latitude longitude) with pictures can facilitate geotagging-enabled information services in terms of finding location-based news, photos, or other resources @cite @cite @cite @cite . There are also a number of approaches @cite trying to learn the location based visual codebook for landmark search. Similar to our method, such research area also explore the user communities. However, this kind of research is apparently different from landmark retrieval studied in this paper.
- It is well known that learning deep network features plays a crucial role for a wide range of applications. Shao al @cite proposed a multi-spectral neural networks (MSNN) is by projecting multiple penultimate layer of the deep network, namely multi-columns deep network, into multi-spectral smooth embedding to achieve the complementary multi-view spectral relationship. To this end, an effective multi-view alignment strategy is proposed. As a by-product, the smooth low-dimensional manifold embedding enables it to be robust to possible noise corruptions in raw feature representations. A multi-objective genetic program with four-layer structure is proposed in @cite to automatically generate domain adaptive global feature descriptors for image classification. A typical yet simple deep network @cite is learned for image classification, by employing PCA to learn multi-stage filter banks, followed by simple binary hashing and block histograms for indexing and pooling. Some multi-view feature fusion methods @cite @cite @cite @cite @cite @cite @cite @cite have also been proposed.
- @math The methods belonging to this category attempt to correlate joints locations. In order to add temporal information to this representation, Yang @cite employed the differences of 3D joint position between the current frame and the preceding frame, and between the current frame and the initial one. Action classification was performed by using the Naive-Bayes nearest neighbor rule in a lower dimensional space constructed by using principal component analysis (PCA). Li @cite employed a bag-of-3D-points graph approach to encode actions based on body silhouette points projected to the three orthogonal Cartesian planes. More complex representation is introduced in @cite , where the relative 3D geometry between different rigid bodies is explicitly estimated. Their relative geometry between @math rigid body parts can be described as special Euclidean group @math using the rotation and translation. Therefore, the entire human skeleton in each frame can be described as a point in @math . A sequence of skeletons is a curve in the Lie group @math .
- The search problem has been extensively studied, e.g. see the survey by Benkoski @cite ; deterministic algorithms for optimal linear search @cite ; incorporating a when a robot changes direction during the search @cite ; when bounds on the distance to the target are known in advance @cite ; and for moving targets or more general linear cost functions @cite . Other approaches include optimal randomized algorithms for the related @cite , and stochastic and game theoretic investigations @cite @cite .
- The search problem has also been studied in environments where search occurs in graphs (see, e.g. @cite ) or along dynamically evolving links of networks @cite @cite . More recently, variants of search using collections of robots have been investigated. The robots can employ either communication (at any distance) or communication, where communication is only possible among co-located robots. For example, the problem of @cite @cite is essentially a search problem where search is completed only when the target is reached by the last robot. Linear group search in the face-to-face communication model has also been studied with robots that either operate at the same speed or with a pair of robots having distinct maximal speeds @cite @cite . Finally, a new direction of research seeks to analyze linear search with multiple robots where some fraction of the robots may exhibit either crash faults @cite or @cite .
- . Network morphism originated from knowledge transferring for convolutional neural networks. Early attempts were only able to transfer partial knowledge of a well-trained network. For example, a series of model compression techniques @cite @cite @cite @cite were proposed to fit a lighter network to predict the output of a heavier network. Pre-training @cite was adopted to pre-initialize certain layers of a deeper network with weights learned from a shallower network. However, network morphism requires the knowledge being fully transferred, and existing work includes Net2Net @cite and NetMorph @cite . Net2Net achieved this goal by padding identity mapping layers into the neural network, while NetMorph decomposed a convolutional layer into two layers by deconvolution. Note that the network morphism operations in @cite @cite are quite primitive and at a micro-scale layer level. In this research, we study the network morphism at a meso-scale module level, and in particular, we investigate its morphing capability.
- . The evolution of convolutional neural networks has been from sequential to modularized. For example, LeNet @cite , AlexNet @cite , and VGG net @cite are sequential networks, and their difference is primarily on the number of layers, which is 5, 8, and up to 19 respectively. However, recently proposed networks, such as GoogLeNet @cite @cite and ResNet @cite , follow a modularized architecture design, and have achieved the state-of-the-art performance. This is why we wish to study network morphism at the module level, so that its operations are able to directly apply to these modularized network architectures.
- User association has been extensively studied for code-division multiple access (CDMA) networks @cite @cite @cite @cite . Results there suggest that joint user association and power control can significantly improve the performance of a CDMA network. Many recent studies focus on system utility maximization in OFDMA HetNets, which often requires solving non-convex integer programs. Game theory has also been used to derive simple distributed scheduling policies (e.g., @cite ). Optimal linear precoder design and base station selection are considered for uplink HetNets in @cite . The authors of @cite studied spectrum sharing by strategic operators in the unlicensed band. While each operator is free to transmit over the entire common spectrum subject to the maximum power constraint, leading to the tragedy of commons, @cite characterized more favorable Nash equilibria of both a one shot game and a repeated game. In contrast to the slow-timescale setting here, the aforementioned studies focus on dynamically updating user and resource allocation on a relatively fast timescale, which depends on the instantaneous channel realizations.
- The stochastic geometry framework way proposed to evaluate and optimize the expected system performance over random topologies and channel conditions @cite @cite . The approach does not apply to the optimization of resource allocation to all possible interference patterns as considered in this paper.
- @cite , a slow-timescale model similar to the model considered in this paper was proposed. User association and spectrum allocation are jointly optimized to maximize the sum rate under proportional fairness constraints. There are two major differences between @cite and our approach here. First, we allow rather realistic stochastic traffic, whereas @cite is limited to backlogged traffic and rate maximization. Second, the proposed algorithm in @cite avoids exponential complexity by limiting to a small number of global patterns a priori . Here, the proposed scalable solution includes all possible patterns as candidates.
- Visual scene understanding often harnesses the statistical patterns of object co-occurrence @cite @cite @cite @cite as well as spatial layout @cite @cite . A series of contextual models based on surrounding pixels and regions have also been developed for perceptual tasks @cite @cite @cite @cite . Recent works @cite @cite exploits more complex structures for relationship prediction. However, these works focus on image-level predictions without detailed visual grounding. Physical relationships, such as support and stability, have been studied in @cite @cite @cite . Lu al @cite directly tackled the semantic relationship detection by combining visual inputs with language priors to cope with the long-tail distribution of real-world relationships. However, their method predicts each relationship independently. We show that our model outperforms theirs with joint inference.
- One of the most popular ways of representing a visual scene is through text descriptions @cite @cite @cite . Although text-based representation has been shown to be helpful for scene classification and retrieval, its power is often limited by ambiguity and lack of expressiveness. In comparison, scene graphs @cite offer explicit grounding of visual concepts, avoiding referential uncertainty in text-based representation. Scene graphs have been used in many downstream tasks such as image retrieval @cite , 3D scene synthesis @cite and understanding @cite , visual question answering @cite , and automatic caption evaluation @cite . However, previous work on scene graphs shied away from the graph generation problem by either using ground-truth annotations @cite @cite , or extracting the graphs from other modalities @cite @cite @cite . Our work addresses the problem of generating scene graphs directly from images.
- Conditional Random Fields (CRF) have been used extensively in graph inference. Johnson al used CRF to infer scene graph grounding distributions for image retrieval @cite . Yatskar al @cite proposed situation-driven object and action prediction using a deep CRF model. Our work is closely related to CRFasRNN @cite and Graph-LSTM @cite in that we also formulate the graph inference problem using an RNN-based model. A key difference is that they focus on node inference while treating edges as pairwise constraints, whereas we enable edge predictions using a novel primal-dual graph inference scheme. We also share the same spirit as Structural RNN @cite . A crucial distinction is that our model iteratively refines its predictions through message passing, whereas the Structural RNN model only makes one-time predictions along the temporal dimension, and thus cannot refine its past predictions.
- Work surrounding generative models for deep learning has mostly been in developing graphical models, autoencoder frameworks, and more recently, generative recurrent neural networks (RNNs). Specific graphical models that have been used to learn generative models of data are Restricted Boltzmann Machines (RBMs), an undirected graphical model with connected stochastic visible and stochastic hidden units, and their generalizations, such as Gaussian RBMs. Srivastava and Salakhutdinov use these basic RBMs to create a Deep Boltzmann Machine (DBM), a multimodal model that learns a probability density over the space of multimodal inputs and can be effectively used for information retrieval and classification tasks @cite . Similar work done by Salakhutdinov and Hinton shows how the learning of a high capacity DBM with multiple hidden layers and millions of parameters can be made more efficient with a layer-by-layer "pre-training" phase that allows for more reasonable weight initializations by incorporating a bottom-up pass @cite . In his thesis, Salakhutdinov also adds to this learning algorithm by incorporating a top-down feedback pass as well as a bottom-up pass, which allows DBMs to better propagate uncertainty about ambiguous inputs @cite .
- Other generative approaches involve using autoencoders. The first ideas regarding the probabilistic interpretation of autoencoders were proposed by ; a more formal interpretation was given by Vincent, who described denoising autoencoders (DAEs) @cite @cite . A DAE takes an input @math and first maps it, with an encoder, to a hidden representation @math through some mapping, @math , where @math is a non-linearity such as a sigmoid. The latent representation @math is then mapped back via a decoder into a reconstruction @math of the same shape as @math , i.e. @math . The parameters, @math , @math , @math , and @math are learned such that the average reconstruction loss between @math and @math is minimized @cite . show an alternate form of the DAE: given some observed input @math and corrupted input @math , where @math has been corrupted based on a conditional distribution @math , we train the DAE to estimate the reverse conditional @math @cite . With this formulation, construct a deeper network of stacked DAEs to learn useful representations of the inputs @cite .
- An alternate model has been posited by , who propose using a recurrent neural network architecture to generate digits. This architecture is a type of variational autoencoder, a recent advanced model that bridges deep learning and variational inference, since it is comprised of an encoder RNN that compresses the real images during training and a decoder RNN that reconstitutes images after receiving codes @cite .
- Finally, another approach for estimating generative models is via generative adversarial nets @cite @cite . In this framework, two models are simultaneously trained: a generative model @math that captures the distribution of the data and a discriminative model @math that estimates the probability that a sample came from the training data rather than @math . @math is trained to maximize the probability that @math makes a mistake.
- In early years of research, it was thought that the morphological features of the face were good indicators of a spontaneous smile. The Facial Action Coding System (FACS) @cite defines Action Units (AU), which are the contraction or relaxation of one or more muscles. It is commonly used to code facial expressions, and can be used to identify emotions. In FACS, a smile corresponds to AU12, which is the contraction of the zygomatic major muscle that raises the lip corners. A genuine smile of joy is thought to include AU6, also known as the Duchenne Marker, which is the contraction of the orbicularis oculi (pars lateralis) muscle that raises the cheek, narrows the eye aperture, and forms wrinkles on the external sides of the eyes. However, recent research casts doubt on the reliability of Duchenne marker in identifying true feelings of enjoyment @cite . Another possible marker of spontaneous smiles is the symmetry of the smile. Initial studies claim that smile symmetry is a factor in identifying spontaneous smiles, where spontaneous smiles are more symmetrical than posed smiles @cite . However, later studies report no significant differences of symmetry @cite @cite .
- Recently, more attention has been paid to dynamical properties of smiles such as the duration, amplitude, speed, and acceleration instead of static features like smile symmetry or the AUs. To analyze these properties, the smile is generally broken up into three different phases - onset, apex, and offset. Spontaneous smiles tend to have a smaller amplitude, a slower onset @cite , and a shorter total duration @cite . The eye region is analyzed as well - the eyebrow raise in posed smiles have a higher maximum speed, larger amplitude and shorter duration than spontaneous ones @cite . Most techniques extract dynamic properties of smiles that are known to be important factors in classifying smiles @cite @cite . Apart from these properties of smiles, facial dynamics can reveal other useful information for the classification of smiles, such as the subject's age.
- On the other hand, existing supervised approaches in the field are mainly based on sequence labeling. Since 2014 the SemEval workshop included a shared task on the topic @cite , which has also encouraged the development of new supervised methods. We find approaches based on CRFs such as and deep learning @cite @cite , @cite .
- With respect to NBM, researches have mainly focused on computation schemes @cite and neighbor selection strategies @cite . also serves as the basis for neighbor selection, thus we concentrate upon in this paper. Generally, there are two main approaches to compute . One introduces different kinds of correlation coefficients as @cite , such as Pearson and Cosine correlations. However, some researchers argue that such kind of methods isolate the relations between two items without leveraging global information. The other approach learns via regression models. @cite @cite introduce a way to learn similarity by minimizing mean squared error between observed ratings and their corresponding estimation. @cite factors similarity matrix via low-rank approximations. @cite presents a weighted error function which gives more weight to the users who rated items most similar to the estimated item. @cite @cite simplify standard neighborhood-based models to a simple linear regression problem for top- @math recommendation based on binary databases.
- A reinforcement learning problem is usually defined using the Markov Decision Process (MDP) model. A standard MDP @cite is defined as a tuple @math where: @math is a finite set of states, @math is a finite set of actions, @math is a transition function with @math being the probability of reaching state @math after executing action @math in state @math , @math is a reward function with @math being the immediate numerical environmental feedback received by the agent after performing action @math in state @math . In this framework, a @math -step history @math is a sequence of state-action: where @math and @math . The value of such a history @math is defined as: where @math is a discount factor. A policy specifies how to choose an action in every state. A deterministic policy @math is a function from the set of states to the set of actions, while a randomized policy @math states the probability @math of choosing an action @math in a state @math .
- In a standard MDP, an optimal policy can be obtained by solving the Bellman's optimality equations: @math Many solution methods can be used @cite to solve this problem exactly: for instance, value iteration, policy iteration, linear programming. Approaches based on approximating the value function for solving large-sized state space have also been proposed @cite .
- Classically, in reinforcement learning (RL), it is assumed that the agent does not know the transition and reward functions. In that case, an optimal policy has to be learned by interacting with the environment. Two main approaches can be distinguished here @cite . The first (called indirect or model-based method), tries to first estimate the transition and reward functions and then use an MDP solving method on the learned environment model (e.g., @cite ). The second (called direct or model-free method), searches for an optimal policy without trying to learn a model of the environment.
- Multiobjective MDP (MOMDP) @cite is an MDP @math where the reward function is redefined as @math with @math being the number of objectives. The value function @math of a policy @math is now vectorial and can be computed as the limit of the vectorial version of ) and ): @math , In MOMDP, the value function of policy @math Pareto-dominates that of another policy @math if in every state @math , @math is not smaller than @math on every objective and @math is greater than @math on at least one objective. By extension, we say that @math Pareto-dominates @math if value function @math Pareto-dominates value function @math . A value function (resp. policy) is Pareto-optimal if it is not Pareto-dominated by any other value function (resp. policy). Due to incomparability of vectorial value functions, there are generally many Pareto-optimal value functions (and therefore policies), which constitutes the main difficulty of the multiobjective setting.
- In multiobjective optimization, four main families of approaches can be distinguished. One first natural approach is to determine the set of all Pareto-optimal solutions (e.g., @cite @cite ). However, in practice, searching for all the Pareto-optimal solutions may not be feasible. Indeed, it is known @cite that this set can be exponential in the size of the state and action spaces. A more practical approach is then to determine an @math -cover of it @cite @cite , which is an approximation of the set of Pareto-optimal solutions. where @math .
- Another approach related to the first one is to consider refinements of Pareto dominance, such as Lorenz dominance (which models a certain notion of fairness) or lexicographic order @cite @cite . In fact, with Lorenz dominance, the set of optimal value functions may still be exponential in the size of the state and action spaces. Again, one may therefore prefer to determine its @math -cover @cite in practice.
- Still another approach to solve multiobjective problems is to assume the existence of a scalarizing function @math , which, given a vector @math , returns a scalar valuation. Two cases can be considered: @math can be either linear @cite or nonlinear @cite @cite @cite .
- A final approach to multiobjective problem assumes an interactive setting where a human expert is present and can provide additional preferential information (i.e., how to trade-off between different objectives). This approach loops between the following two steps until a certain criterion is satisfied (e.g., the expert is satisfied with a proposed solution or there is only one solution left): show potential solutions or ask query to the expert receive a feedback answer from the expert The feedback answer from the expert allows to guide the search for a preferred solution among all Pareto-optimal ones @cite , or elicit unknown parameters of user preference model @cite .
- A preference-based MDP (PBMDP) is an MDP where possibly no reward function is given. Instead, one assumes that a preference relation @math is defined over histories. In the case where the dynamics of the system is not known, this setting is referred to as preference-based reinforcement learning (PBRL) @cite @cite @cite @cite . Due to this ordinal preferential information, it is not possible to directly use the same decision criterion based on expectation like in the standard or multiobjective cases. Most approaches in PBRL @cite @cite @cite relies on comparing policies with probabilistic dominance , which is defined as follows: where @math denotes the probability that policy @math generates a history preferred or equivalent to that generated by policy @math . Probabilistic dominance is related to Condorcet methods (where a candidate is preferred to another if more voters prefers the former than the latter) in social choice theory. This is why the optimal policy for probabilistic dominance is often called a Condorcet winner .
- Theano @cite , Torch @cite , Caffe @cite , MXNet @cite are packages that aim to provide a friendly front end to complex computation back-end that are written in C++. Theano is a python front end to a computational graph compiler, which has been largely superseded by Tensorflow in the compilation speed, flexibility, portability etc, while akid is built on of Tensorflow. MXNet is a competitive competitor to Tensorflow. Torch is similar with theano, but with the front-end language to be Lua, the choice of which is mostly motivated from the fact that it is much easier to interface with C using Lua than Python. It has been widely used before deep learning has reached wide popularity, but is mostly a quick solution to do research in neural networks when the integration with community and general purpose production programming are not pressing. Caffe is written in C++, whose friendly front-end, aka the text network configuration file, loses its affinity when the model goes more than dozens of layer.
- The analysis of coupled queues is a long-standing open problem, and even solving a special case of two interacting queues is challenging @cite . Zhuang @cite modeled multiple interacting queues as a continuous-time Markov chain (CTMC) with fixed BS locations in the network. By minimizing the average packet queuing delay, the optimal spectrum allocation pattern was obtained for each BS. Similarly, Cheng @cite optimized the average queuing delay of network subjected to the BS power constraints. They formulated a Markov decision process (MDP) problem based on the instantaneous channel state information and queue state information, and proposed an adaptive user scheduling and power control policy. However, the state space of the Markov process may become huge as the network scales up, and the analysis would become intractable. Hence, this motivates us to deal with the coupled queue problem with the tool of stochastic geometry to account for the random BS deployment, and derive the mean performance metrics analytically such that some insights can be gained for system design.
- The attacks become harder if the external action is taken after the transaction is committed by the blockchain. Rosenfeld's attack @cite consists of issuing a transaction to a merchant. The attacker then starts solo-mining a longer branch while waiting for @math blocks to be appended so that the merchant takes an external action in response to the commit. The attack success probability depends on the number @math of blocks the merchant waits before taking an external action and the attacker mining power. However, when the attacker has more mining power than the rest of the system, the attack, also called or , is guaranteed successful, regardless of the value @math . To make the attack successful when the attacker owns only a quarter of the mining power, the attacker can incentivize other miners to form a coalition @cite until the coalition owns more than half of the total mining power.
- Without a quarter of the mining power, discarding a committed transaction in Bitcoin requires additional power, like the control over the network. It is well known that delaying network messages can impact Bitcoin @cite @cite @cite @cite @cite . Decker and Wattenhoffer already observed that Bitcoin suffered from block propagation delays @cite . @cite analyzed the effect of propagation delays on Bitcoin using a Markov process. @cite investigated Bitcoin in the synchronous communication setting, however, this setting is often considered too restrictive @cite . extended the analysis for when the bound on message delivery is unknown and showed in their model that the difficulty of Bitcoin's crypto-difficulty has to be adapted depending on the bound on the communication delays @cite . This series of work reveal an important limitation of Bitcoin: delaying propagation of blocks can waste the computational effort of correct nodes by letting them mine blocks unnecessarily at the same index of the chain. In this case, the attacker does not need more mining power than the correct miners, but simply needs to expand its local blockchain faster than the growth of the longest branch of the correct blockchain.
- Ethereum proposed the Ghost protocol to cope with this issue @cite . The idea is simply to account for the blocks proposed by correct miners in the multiple branches of the correct blockchain to select the main branch. As a result, growing a branch the fastest is not sufficient for an attacker of Ethereum to be able to double spend. Even though the propagation strategy of Ethereum differs from the pull strategy of Bitcoin, some network attacks against Bitcoin could affect Ethereum. In the Eclipse attack @cite the attacker forces the victim to connect to 8 of its malicious identities. The Ethereum adaptation would require to forge @math more identities and force as many connections as the default number of clients is 25. @cite proposed a BGP hijacking attack and showed that the number of Internet prefixes that need to be hijacked for the attack to succeed depends on the distribution of the mining power. BGP-hijacking typically requires the control of network operators but is independent from Bitcoin and could potentially be exploited to delay network messages and execute a Balance attack in Ethereum.
- The R3 consortium has been experimenting Ethereum since more than half a year now and our discussion with the R3 consortium indicated that they did not investigate the dependability of the Ghost consensus protocol and that they also worked with Ripple, Axoni, Symbiont. Some work already evoked the danger of using proof-of-work techniques in a consortium context @cite . In particular, experiments demontrated the impossibility of ordering even committed transactions in an Ethereum private chain without exploring the impact of the network delay @cite . As a private blockchain involves typically a known and smaller number of participants than a public blockchain, it is also well-known @cite @cite that many Byzantine Fault Tolerance (BFT) solutions @cite @cite @cite @cite could be used instead. At the time of writing, R3 has just released Corda @cite as a proposed solution for private chains. Corda does not yet recommend a particular consensus protocol but mentions BFT and favors modularity by allowing to plug any consensus protocol instead @cite . Our work confirms that proof-of-work, besides being unnecessary for consortium private chain when the set of participants is known, is not recommended especially for dependability reasons.
- Many research fields focus on the collection of personal information, such as lifelogging, expertise finding, and personal informatics. Personal informatics is a class of tools that help people collect personally relevant information for the purpose of self-reflection and gaining self-knowledge @cite @cite @cite . Various tools have been developed to help people collect and analyze different kinds of personal information, such as location @cite , finances @cite , food @cite , weight @cite @cite , and physical activity @cite . Knowledge Model and Understanding Tree facilitate a new type of personal informatics tool that helps people discover their expertise and deficiencies in a more accurate way, by quantitatively assessing an individual's possession of knowledge.
- Expertise is one's expert skill or knowledge in a particular field. Expertise finding is the use of tools for finding and assessing individual expertise @cite @cite @cite . As an important link of knowledge sharing, expertise finding has been heavily studied in many research communities @cite @cite @cite @cite @cite @cite . Many sources of data have been exploited to assess an individual's expertise, such as one's publications, documents, emails, web search behavior, other people's recommendations, social media etc. Knowledge Model and Understanding Tree provide a new source of data to analyze one's expertise -- one's learning history about a subject, which is more comprehensive and straightforward than other data sources, because one's expertise is mainly obtained through learning (Including Informal Learning", which occurs through the experience of day-to-day situations, such as a casual conversation, play, exploring, etc.)
- Conventional object recognition aims at object category labeling given a test image. Earlier work such as visual word coding @cite uses statistical information of local patches. The deformable part model, now known as DPM @cite uses part relationship and part appearance. Deep learning has recently made significant contributions to object recognition. Representative network architectures include AlexNet @cite , VGG @cite and ResNet @cite . Excellent object detection methods founded on one of these architectures include RCNN @cite and Faster RCNN @cite . Object segmentation can be considered as a special holistic object recognition task @cite @cite which cuts the object from the background.
- Object parts layout has been used to provide sub-object level information. The specific problem closely related to object parts detection is human pose estimation @cite @cite @cite where different human parts (e.g., head, body, hands, legs) need to be localized. @cite , a separate representation was respectively proposed for holistic object and body parts, and a fully connected model was used to optimize their arrangement. The model was applied to the six animal categories and achieved a better object representation performance. @cite , to segment object parts, a mixture of compositional models was used to represent the object boundary and the boundaries of the semantic parts. This compositional model incorporates edge, appearance, and semantic parts. The above methods localize parts only, but not in-depth explore semantics on them.
- Our part state can be considered as a caption" on the associated part region. Here, we survey a number of works on image captioning. @cite , a generative model was presented that is based on a deep recurrent architecture. Combining the recent advanced machine translation techniques, the model was trained to maximize the likelihood of the target description sentence on the training images. @cite , inter-modal correspondences were proposed between language and visual data. To some degree image captioning explores high-level image semantics. However, image captions vary from person to person and are difficult to be objectively measured.
- Visual tracking in sports has been receiving great attention over the years and it has been tackled in many different ways. Due to its simplicity and robustness to deal with more complex models, methods based on particle filters became popular @cite @cite @cite . Another significant approach relies on the fact that often the background is static and, therefore, tracking may be performed by using background subtraction methods @cite . Other authors explore the use of multiple cameras to obtain more reliable results @cite .
- One challenging condition frequently found in sports scenes is occlusion. Many previous works focused on modeling it explicitly to handle these difficult situations @cite @cite . Zhang al @cite tackle this issue in sports by using a structured sparse model for each person. This approach builds on the robustness of sparse models by assuming that the occlusion model is usually not sparse, but rather a structured connected area. This allows using better models which are able to ignore features from large occlusion areas, one player occluding another one. In a related topic, Soomro al @cite propose using structural properties encoded in graphs to associate tracks when the videos of football games are cut, which causes occlusion of players as well as abrupt motion. This problem is formulated as a graph matching between different frames. In order to solve the ambiguity problem for the association, the authors use knowledge about the previously learned team formation. Therefore, the model requires some additional external information in order to successfully recover tracking.
- The use of structural information for multi-object tracking has also been incorporated into the SPOT tracker @cite . The authors use a model-free approach that learns the appearance and structural relations between the tracked objects online. In the first frame, manual annotations are provided and used to initially train a Histogram of Oriented Gradients (HOG) detector @cite for finding the object in the next frames, their approach is also based on tracking by detection. The structural relations are also learned from the first frame by training a structured Support Vector Machine (SVM). The models are then updated while the tracking is being performed, using a gradient descent approach. The candidate graphs are evaluated using the information obtained from the HOG detectors as well as the distances between any two objects.
- Although similar, their proposal differs from the one presented in this paper in the following aspects: (1) their structural model only computes the difference between the observed distance and an ideal value that comes from the online training. Our model considers both distance and angle information obtained from a more general probability density function. (2) Although they use the structure to improve tracking and to deal with occlusion, it is not used to guide the detection process, which could lead to improved performance by restricting the search space. Our approach, inspired by @cite , uses some objects as supports for the structural model to generate candidates of where the target is likely to be found after tracking loss. (3) Their method of tracking by detection does not consider motion models, while we rely on particle filters.
- Another important issue that must be dealt with during tracking is abrupt motion. Perhaps, the simplest way to deal with it is by generating some additional target location hypotheses around the previous location to try to cover a broader region, as explored in @cite . Another proposal is to solve the same problem using spatial position information for finding better candidates @cite @cite . This is done by dividing the image into several smaller regions and using the information obtained from the density of states of each one to find new locations. More recently, Su al @cite propose relying on visual saliency information to guide the tracking and restore the target. It is important to note that, although tracking-by-detection methods should be able to deal with abrupt motion, most of them do not behave well in this situation because their association function usually enforces the temporal stability constraint. As previously mentioned, we use a different approach that relies on the structural relations between the objects to find the most likely new locations of a lost target.
- Specifically, for convex problems, @cite proposed basic proximal gradient (PG) method and Nesterov's accelerated proximal gradient (APG) method. They proved that PG has the convergence rate @math , and APG has the convergence rate @math , where @math is the number of iterations. For non-convex problems, @cite considered that only @math could be non-convex, and proved that the convergence rate of APG method is @math . @cite considered that both of @math and @math could be non-convex, and proved the convergence of PG method. @cite considered that both of @math and @math could be non-convex, and proved that the APG algorithm can converge in a finite number of iterations, in a linear rate or a sublinear rate (, @math ) at different conditions. We summarize these exact proximal gradient methods in Table . In addition to the above batch exact proximal gradient methods, there are the stochastic and online proximal gradient methods . Because they are beyond the scope of this paper, we do not review them in this paper.
- Mapping of Urban Crimes. Spicer @cite describe a theoretical framework capable of mapping urban crimes into related locations in a city. Their work discusses the pros and cons of mapping methods. To represent a city, they use street networks and GIS software, describing methods for mapping georeferenced elements into a complex network. However, their methods associate a crime with the nearest node or edge based on address geocoding, not having any relation with graph measures. Moreover, all methods are superficially introduced, not presenting any formal fundaments.
- Shinode and Shinode @cite aim at introducing a search-window method to analyze urban crimes in street networks. They represent a search window as a subarea that concentrates a high number of crimes in small regions; besides that, the crimes are used to identify clusters spatially and temporally. The authors validate their methods through an empirical case study. However, their criminal data derive from 911-calls made in 1996; that is, they used a dataset that carries few details and that is outdated, especially when one considers the conclusions claimed by the authors. Such information implies a superficial analysis of the crime behavior and the ways to prevent it.
- Crime Analysis. White @cite represent criminal activities via a georeferenced complex network, through which they compare criminal demographics and identify criminal communities. The demographic regions are created by clustering the urban areas using socioeconomic borders, whereas the crime network is formed by linking crime locations that are close according to the Euclidean distance. To identify the criminal communities, the authors employ a label propagation technique. Their results show that the crime network can be constructed without requiring information about individual crimes. However, their crime network does not take the urban topology into account, so their comparison is based only on socioeconomic information.
- Rey @cite employ spatiotemporal techniques to represent and analyze urban crimes. Their approach provides a way to quantify neighborhood criminality and is able to identify patterns from the data. Moreover, they discuss spatial crime analyses as a way to determine the influence of a crime on its surroundings, analyzing an entire city and providing an evaluation of its crimes. However, their work is based on outdated data, derived from a police district in the period from 2005 to 2009. Besides, their methods rely on black-box GIS technologies, used to summarize criminal areas by geocoding each crime into a quarter-mile grid cell. Furthermore, they analyze only city cells having residential units.
- Other works include the proposals of Galbrun @cite , who focus on crime mapping to estimate the probability of a crime occurrence within a street segment; of Fitterer @cite , who use a statistical model to predict crimes in the city of Vancouver; and of Bogomolov @cite , who predict in London by using mobile data. Our work differs from theirs because (i) we consider crimes within a more complete and up-to-date period; (ii) we map crimes directly, there is no space summarization in the crime mapping phase; and (iii) we benefit from complex networks, which are a reliable toolset to design georeferenced data.
- Pal, Singh, and Kumar @cite and D'Hondt and Panangaden @cite dealt with @math and the GHZ-state sharing problem in a different setting, where pre-shared entanglement is assumed but only classical communication is allowed. The relation between several network models that differ in available quantum resources has been discussed by Gavoille, Kosowski, and Markiewicz @cite . Recently, @cite proved that quantum communication cannot substantially speed up algorithms for some fundamental problems, such as the minimum spanning tree, compared to the classical setting. For fault-tolerant distributed quantum computing, the Byzantine agreement problem and the consensus problem were studied by Ben-Or and Hassidim @cite and Chlebus, Kowalski, and Strojnowski @cite , respectively. In the cryptographic context where there are cheating parties, Refs. @cite @cite devises quantum algorithms that elect a unique leader with a small bias. Some quantum distributed protocols were experimentally demonstrated by @cite and @cite .
- See the surveys @cite @cite @cite and the references therein for more work on distributed quantum computing.
- Large Margin Nearest Neighbors (LMNN, @cite ) is one of the early attempts to learn a Mahalanobis distance metric as a convex optimization problem over the set of PSD matrices. The loss function is composed of the linear combination of two terms @math and @math . The first term aims at penalizing large distances between an observation and other observations sharing the same label, while the second term objective is to penalize small distances between observations from different classes. The loss function is then casted as a semidefinite program. Note that there no regularization term in the objective function so that LMNN is prone to overfitting. Another well known problem, related to the proposed approach, is that the selection of neighbors is initially made using Euclidean distance, which may not be adapted.
- Finally, the authors, in @cite , propose a boosting-like algorithm for metric learning. While the idea of using the principle of boosting is similar to our proposition, they are using it to decompose the learning process to a linear positive combination of rank-one matrices, instead of setting local weights to individuals as in our approach. Consequently, the two methods do not focus on the same points, and may even be used together to produce an even more fast metric learning algorithm.
- In @cite and @cite , the authors, relying on the theory of learning with good (dis-)similarity functions @cite , propose some other heuristics to select examples. In particular, in @cite , their approach relies on AdaBoost, where several learners are trained on different subsets. In @cite , the authors propose a selection that is promoting diversity. In practice, sample points are selected so that the average similarity between samples belonging to different classes is small. In other terms, it is maximizing the between-class variance of a subset of the data. In both cases, the objective is to provide a classifier, so that it is not the same objective as our proposition.
- Our approach also differs from usual instance selection for nearest neighbor algorithms @cite , where a weight is given to the distance between two samples, whereas we propose to set a weight on samples.
- Finally, note that we restrict here to the description of global metric learning algorithms: a unique metric matrix @math is learned for the entire data set. Some other approaches propose to learn a matrix by class @cite , or multi-task metrics @cite . Naturally, the proposition could also be used with non linear (i.e. kernelized or local metric learning, see e.g. @cite , @cite ) methods.
- Over the years, many attempts were made to estimate the 3D surface of a face appearing in a single view. Before listing them, it is important to mention recent multi image methods which use image sets for reconstruction (e.g., @cite @cite @cite @cite @cite ). Although these methods produce accurate 3D reconstructions, they require many images from multiple sources to produce a single 3D face shape whereas we reconstruct faces from single images.
- Statistical shape representations , such as the widely popular 3DMM @cite @cite @cite @cite @cite @cite @cite , use many aligned 3D face shapes to learn a distribution of 3D faces, represented as a high dimensional subspace. Each point on this subspace is a parameter vector representing facial geometry and sometimes expression and texture. Reconstruction is performed by searching for a point on this subspace that represents a face similar to the one in the input image. These methods do not attempt to produce discriminative facial geometries and indeed, as mentioned earlier, were only used for face recognition under controlled settings.
- The very recent method of @cite also uses a CNN to regress 3DMM parameters for face photos. They too recognize absence of training data as a major concern. Contrary to us, they propose synthesizing training faces with known geometry by sampling from the 3DMM distribution. This approach produces synthetic looking photos which can easily cause overfitting problems when training large networks @cite . They were therefore able to train only a shallow residual network (seven layers compared to our 101) and their estimated shapes were not shown to be more robust or discriminative than other methods.
- Scene assumption methods. In order to obtain correct reconstructions, some make strong assumptions on the scene and the viewing conditions in the input image. Shape from shading methods @cite , for example, make assumptions on the light sources, facial reflectance and more. Others instead use facial symmetry @cite . The assumptions they and others make often do not hold in practice, limiting the application of these methods to controlled settings.
- Example based methods , beginning from the work of @cite and more recently @cite @cite , modify the 3D surface of example face shapes, fitting them to the face appearing in input photo. These methods favor robustness to challenging viewing conditions over detailed reconstructions. They were thus only used for face recognition to synthesize new views from unseen poses.
- Learning knowledge graphs @cite @cite @cite and using graphs for visual reasoning @cite @cite has recently been of interest to the vision community. For reasoning on graphs, several approaches have been studied. For example, @cite collects a knowledge base and then queries this knowledge base to do first-order probabilistic reasoning to predict affordances. @cite builds a graph of exemplars for different categories and uses the spatial relationships to perform contextual reasoning. Approaches such as @cite use random walks on the graphs to learn patterns of edges while performing the walk and predict new edges in the knowledge graph. There has also been some work using a knowledge base for image retrieval @cite or answering visual queries @cite , but these works are focused on building and then querying knowledge bases rather than using existing knowledge bases as side information for some vision task.
- Li and Zemel present Graph Gated Neural Networks (GGNN) @cite which uses neural networks on graph structured data. This paper (an extension of Graph Neural Networks @cite ) serves as the foundation for our Graph Search Neural Network (GSNN). Several papers have found success using variants of Graph Neural Networks applied to various simple domains such as quantitative structure-property relationship (QSPR) analysis in chemistry @cite and subgraph matching and other graph problems on toy datasets @cite . GGNN is a fully end-to-end network that takes as input a directed graph and outputs either a classification over the entire graph or an output for each node. For instance, for the problem of graph reachability, GGNN is given a graph, a start node and end node, and the GGNN will have to output whether the end node is reachable from the start node. They show results for logical tasks on graphs and more complex tasks such as program verification.
- There is also a substantial amount of work on various types of kernels defined for graphs @cite such as diffusion kernels @cite , graphlet kernels @cite , Weisfeiler-Lehman graph kernels @cite , deep graph kernels @cite , graph invariant kernels @cite and shortest-path kernels @cite . The methods have various ways of exploiting common graph structures, however, these approaches are only helpful for kernel-based approaches such as SVMs which do not compare well with neural network architectures in vision.
- Gatys al @cite formulates style transfer as an optimization problem that combines texture synthesis with content reconstruction. Their formulation involves additive loss functions placed on multiple layers of a pretrained CNN, with some loss functions synthesizing the textures of the style image and some loss functions reconstructing the content image. Gradients are computed by backpropagation and gradient-based optimization is carried out to solve for the stylized image. An alternative approach uses patch-based similarity matching between the content and style images @cite @cite @cite . In particular, Li and Wand @cite constructs patch-based loss functions, where each synthetic patch has a nearest neighbour target patch that it must match. This type of patch-matching loss function is then combined additively with Gatys al's formulation. While these approaches allow arbitrary style images, the optimization frameworks used by these methods makes it slow to generate the stylized image. This is particularly relevant for the case of video where we want to style a huge number of frames.
- As mentioned previously, it is possible to train a neural network that approximates the optimum of Gatys al's loss function for one or more fixed styles @cite @cite @cite @cite . This yields a much faster method, but these methods need to be re-trained for each new style.
- Ruder al @cite introduces a temporal loss function that, when used additively with Gatys al's loss functions, can perform style transfer for videos with temporal consistency. Their loss function relies on optical flow algorithms for gluing the style in place across nearby frames. This results in an order of maginitude slowdown compared to frame-by-frame application of style transfer.
- In comparison to existing style transfer approaches, we propose a method for directly constructing the target activations for a single layer in a pretrained CNN. Like Li and Wand @cite , we make use of a criteria for finding best matching patches in the activation space. However, our method is able to directly construct the entire activation target. This deterministic procedure allows us to easily adapt to video, without the need to rely on optical flow for fixing consistency issues. In addition to optimization, we propose a feed-forward style transfer procedure by inverting the pretrained CNN. Unlike existing feed-forward style transfer approaches, our method is not limited to specifically trained styles and can easily adapt to arbitary content and style images. Unlike existing CNN inversion methods, our method of training does not use a pixel-level loss, but instead uses a loss on the activations. By using a particular training setup (see ), this inverse network is even able to invert activations that are outside the usual range of the CNN activiations.
- There is a large amount of literature on asymptotically flat black hole spacetimes; we only give a brief account here. The seminal papers by Wald @cite and Kay--Wald @cite proved the boundedness of linear scalar waves on the Schwarzschild spacetime; Dafermos and Rodnianski @cite gave a more robust proof, exploiting the red-shift effect, and established polynomial decay. Polynomial decay for linear scalar waves on Kerr was proved by Andersson--Blue @cite and in the aforementioned @cite ; see also @cite . The precise decay rates (Price's law @cite ) were obtained by Tataru @cite , see also @cite , as well as @cite in a spherically symmetric but non-linear context. Strichartz estimates were proved by Tataru and Tohaneanu @cite , also in collaboration with Marzuola and Metcalfe @cite . For non-linear results, we refer to Luk's work on Kerr @cite , Ionescu--Klainerman @cite and Stogin @cite for a wave map equation, and Dafermos--Holzegel--Rodnianski @cite for a scattering construction of dynamical Kerr black holes. Many of these works rely on the very influential vector field method' developed by Klainerman @cite ; see also @cite for a discussion of recent developments.
- Recently, caching strategy optimization is extended to wireless heterogeneous networks. The combination of the optimal caching and the network heterogeneity brings more gains in network capacity. Utilizing the tool of stochastic geometry, the works @cite @cite investigate the optimal probabilistic caching at helper stations while assuming deterministic caching at macro stations to maximize the successful transmission probability in a two-tier HetNet. Based on @cite , the work @cite considers different types of BSs with different cache capacities. The cache optimization problem for the first type of BSs is solved by assuming that the placement strategy for other types of BSs is given. The joint probabilistic caching optimization problem for all types of BSs is yet not considered, and little analytical insight on the cache design and system performance is available. In general, the joint optimization for probabilistic cache placements in different tiers of a HetNet is very challenging due to the different tier association probabilities brought by the content diversity as well as the complicated interference distribution by the nature of network heterogeneity.
- Furthermore, a tradeoff between the small BS density and total storage size is firstly presented in @cite , where each small BS caches the most popular contents. Using the optimal caching scheme, @cite shows that the helper density can be traded by the cache size to achieve a target area spectral efficiency. Note that the tradeoff studies in @cite @cite are conducted numerically only without theoretical analysis. Deriving and analyzing the tradeoff theoretically has not been solved. In @cite , the authors address the question that how much caching is needed to achieve the linear capacity scaling in the dense wireless network based on scaling law method.
- Recent work has identified programmers using abstract syntax trees with a surprisingly high success rate despite obfuscation of code @cite . We do not attempt to identify programmers based on code, but we use code of the same programmer to support the reconstruction of obfuscated code. We use only small parts of the abstract syntax tree and compute also aggregate (i.e, pattern) statistics.
- Obfuscation is also (ab)used by malware @cite to circumvent intrusion detection systems by using polymorphic techniques. Though obfuscation is typically associated with source or machine code, obfuscation techniques have also been applied to data @cite . Obfuscation has also been leverage to to construct an abstract state machine that enables computation on encrypted and non-encrypted data @cite .
- Circuit Privacy: Traditional program obfuscation attempts disguise the computation (of a circuit) in one way or another. For instance, Indistinguishability obfuscation requires that given any two equivalent circuits @math and @math of similar size, the obfuscations of @math and @math should be computationally indistinguishable @cite . In functional encryption @cite , ciphertexts encrypt inputs @math , and keys are issued for circuits @math . Using a key to decrypt a ciphertext @math , yields the result of the circuit @math evaluated for @math but does not reveal anything else about @math . Furthermore, no collusion of secret key holders should be able to learn anything more than the union of what they can each learn individually. Since the introduction of these concepts @cite , significant progress has been made (see @cite for an overview). We argue that from a practical perspective, encryption at the source (or byte) code level is more meaningful than for Boolean circuits due to performance reasons. Given a processor supports @math commands each implemented using circuits of size @math , it suffices to hide the command type rather than the circuit.
- Keeping the circuit private while changing any wire value in the circuit has also been investigated @cite . An attack can be detected and data can then be erased.
- In his thesis Gentry @cite gave a fully homomorphic encryption (FHE) scheme. He also discussed how to ensure circuit privacy by adding a large random error vector.
- Many methods have been proposed to represent the relationship between the appearance of two viewpoints. Seitz and Baker @cite model image transformations using a space-variant linear filter, similar to a convolution but varying per-pixel. They highlight that a linear transformation of a vectorized representation of all the pixels in an image is very general; it can represent all standard parametric transformations, such as similarity, affine, perspective, and more. More recently, Jaderberg al @cite describe an end-to-end learnable module for neural networks, the spatial transformer, which allows explicit spatial transformations ( scaling, cropping, rotation, non-rigid deformation) of feature maps within the network that are conditioned on individual data samples. Practically, including a spatial transformer allows a network to select regions of interest from an input and transform them to a canonical pose. Similarly, Tinghui al @cite address the problem of novel view synthesis. They observe that the visual appearance of different views is highly correlated and propose a CNN architecture for estimating appearance flows, a representation of which pixels in the input image can be used for reconstruction.
- Several methods have been recently proposed to jointly reason about co-located aerial and ground image pairs. Luo al @cite demonstrate that aerial imagery can aid in recognizing the visual content of a geo-tagged ground image. M 'a ttyus al @cite perform joint inference over monocular aerial imagery and stereo ground images for fine-grained road segmentation. Wegner al @cite build a map of street trees. Given the horizon line and the camera intrinsics, Ghouaiel and Lef e vre @cite transform geo-tagged ground-level panoramas to a top-down view to enable comparisons with aerial imagery for the task of change detection. Recent work on cross-view image geolocalization @cite @cite @cite @cite has shown that convolutional neural networks are capable of extracting features from aerial imagery that can be matched to features extracted from ground imagery. Vo al @cite extend this line of work, demonstrating improved geolocalization performance by applying an auxiliary loss function to regress the ground-level camera orientation with respect to the aerial image. To our knowledge, our work is the first work to explore predicting the semantic layout of a ground image from an aerial image.
- There is a long tradition of using computer vision techniques for aerial and satellite image understanding @cite @cite @cite . Historically these two domains were distinct. Satellite imagery was typically lower-resolution, from a strictly top-down view, and with a diversity of spectral bands. Aerial imagery was typically higher-resolution, with a greater diversity of viewing angles, but with only RGB and NIR sensors. Recently these two domains have converged; we will use the term aerial imagery as we are primarily working with high-resolution RGB imagery. However, our approach could be applied to many types of aerial and satellite imagery. Kluckner al @cite address the task of semantic segmentation using a random forest to combine color and height information. More recent work has explored the use of CNNs for aerial image understanding. Mnih and Hinton propose a CNN for detecting roads in aerial imagery @cite using GIS data as ground truth. They extend their approach to handle omission noise and misregistration between the imagery and the labels @cite . These approaches require either extensive pixel-level manual annotation or existing GIS data. Our work is the first to demonstrate the ability to transfer a dense pixel-level labeling of ground imagery to aerial imagery.
- Domain adaptation addresses the misalignment of source and target domains @cite . A significant amount of work has explored domain adaptation for visual recognition @cite . Jhuo al @cite propose a low-rank reconstruction approach where the source features are transformed to an intermediate representation in which they can be linearly reconstructed by the target samples. Our work is most similar to that of Sun al @cite , who propose a method for transferring scene categorizations and attributes from ground images to aerial imagery. Similar to our approach, they learn a transformation matrix which minimizes the distance between a source feature and the target feature. Our work differs in several ways: 1) we carry out the linear transformation not only in the semantic dimensions but also in the spatial dimensions, 2) we constrain the transformation matrix such that the semantic meaning of the source feature and the target feature remains the same, 3) our transformation matrix is input dependent, and 4) we learn the transformation matrix as well as the source feature at the same time, in an end-by-end manner, which simplifies training.
- Iqbal and Bailey @cite define as a visual, auditory, or tactile designed to attract attention. In daily language, the word notification may be used to describe the alert as well as a visual representation that is typically found in a pop-up or a notification center (see Figure ). In this paper, we will use the word to .
- Notification-Management Strategies In a recent survey @cite , the majority of respondents considered themselves to typically receive 20-50 or 50-100 notifications per day. In an log study on mobile phone notifications @cite , participants received a medium number of 63.5 of notifications per day. Both results reveal that, on average, people deal with dozens of notification alerts every day. To manage this volume of notifications, Chang and Tang @cite found that the ringer mode is a frequently-used mechanism to manage attentiveness to notifications on mobile phones. Lopez-Tovar @cite argue that users desire more fine-grained control over how notifications are presented in different contexts. However, Westermann @cite report that only a small fraction (10 , people typically remain exposed to the majority of the notifications alerts that they receive.
- Notifications & Engagement @cite showed that notifications often trigger engagement with the mobile phone: in their data set, the majority (79 shown by Mark @cite , when information workers are without (the interruption of) emails, they switch less between tasks. Similarly, Iqbal and Horvitz @cite found that disabling email notifications leads to less frequent opportunistic email checking. Yet, not all interruptions are notification-triggered: in 2009, Jin and Dabbish @cite found that in the case of information workers, 50 addition, Oulasvirta @cite report that people frequently check their phones even if there are no notifications. We hypothesize that disabling notifications will reduce the engagement with the mobile phone, but not eliminate it.
- Distraction & Productivity Impairment Since people receive plenty of notifications, by sheer probability, notifications are bound to appear from time to time while the receiver is busy with other tasks. Since people usually attend to notifications within minutes, notifications may sometimes interrupt those other tasks. Such interruptions can have negative effects: previous work in the context of information workers has found that notifications negatively affect work efficiency when delivered in the middle of a work task @cite @cite @cite @cite @cite @cite @cite , and the effect is more pronounced when the task is cognitively demanding @cite @cite . As found by Stothart @cite , this is even true when the notification is not attended, as tested in a controlled exam setting. Hence, previous work consistently highlights the disruptive effects of notifications in work settings. Mobile phone users also expressed to frequently feel interrupted by notifications, even outside of work settings @cite . We hypothesize that the absence of notifications will have positive effects on productivity.
- Notifications & Stress In addition to negative effects on work efficiency, interruptions can also affect people emotionally. Interruptions in the workplace have been linked to frustration @cite and stress @cite . In the context of notifications, information workers felt significantly less stressed without email @cite , without email notifications @cite , or when checking work email was restricted to 3 times per day @cite . However, work emails no longer reach us at only work. Our mobile devices may notify us about incoming emails at any time, which blurs the boundaries between work and private life @cite @cite . Stress levels were found to positively correlate with the number of mobile phone notifications from, in particular, email clients @cite , which indicates that email notifications are particularly problematic. Mobile phone notifications in general have been linked to inattention and hyperactivity @cite . On the basis of this related work, we hypothesize that the absence of notifications will reduce stress and other negative emotions.
- Notifications & Availability On mobile phones, the largest chunk of notifications originate from messaging applications @cite @cite @cite , such as SMS, WhatsApp, or Facebook Messenger. On such communication channels, people are assumed to be constantly co-present, and thus, constantly available for conversation @cite . On average, notifications from messengers are attended within minutes @cite @cite @cite @cite @cite , and people maintain this levels of attentiveness for large parts of their wake time @cite . Consequently, notification-enabled computed mediated communication plays a crucial role [..] in the fragmentation of the working day @cite . We hypothesize that the absence of notifications will affect the participants' ability to maintain the usual level of attentiveness.
- Suppressing Notifications Two previous studies applied the methodology of depriving participants from notifications: Iqbal and Horvitz @cite asked 20 information workers to turn off email notifications on their work computers for one week. Compared to a baseline week, some participants checked emails more frequently as a result, but for the majority of the participants, it reduced the frequency of opportunistic email checking. While the participants were aware that notifications are disruptive, they valued the awareness they provide. After the study, none of the participants kept notifications disabled.
- Kushlev @cite conducted a study in which for one week, 221 participants were asked to maximize interruptions through their phone (enabling alerts, keeping phone in reach) and compared this to a baseline condition, where the same participants were asked to minimize interruptions (disabling alerts, keeping phone out of sight). The results show that with maximized interruptions, participants reported higher levels of inattention and hyperactivity -- symptoms associated with ADHD.
- The success of deep learning methods in object recognition has also translated to the problem of material recognition, the classification and segmentation of material categories in arbitrary images. Bell al., achieve per-pixel material category labeling by retraining the then state-of-the-art object recognition network @cite on a large dataset of material appearance @cite . This method relies on large image patches that include object and scene context to recognize materials. In contrast, Schwartz and Nishino @cite @cite learn material appearance models from small image patches extracted inside object boundaries to decouple contextual information from material appearance. To achieve accurate local material recognition, they introduced intermediate material appearance representations based on their intrinsic properties (e.g., smooth'' and met allic'').
- The spatio-temporal relationships across cameras @cite @cite @cite @cite @cite or prior knowledge about topology has also been used for human re-identification. Chen al @cite make use of prior knowledge about camera topology to adaptively learn appearance and spatio-temporal relationships between cameras, while Mazzon al @cite use prior knowledge about relative locations of cameras to limit potential paths people can follow. Javed al @cite presented a two-phase approach where transition times and exit entrance relationships are learned first, which are later used to improve object correspondences. Fleuret @cite predicted occlusions with a generative model and a probabilistic occupancy map. Dick and Brooks @cite used a stochastic transition matrix to model patterns of motion within and across cameras. These methods have been evaluated on non-crowded scenarios, where observations are sparse and appearance is distinctive. In crowded scenes, hundreds of people enter a camera simultaneously within a small window of few seconds, which makes learning transition times during an unsupervised training period virtually impossible. Furthermore, since it is not always possible to obtain camera topology information, our approach is applicable whether or not the camera topology is available.
- have been used for improving tracking performance @cite @cite @cite . Pellegrini al @cite were the first to use social force models for tracking. They modeled collision avoidance, desired speed and destination and showed its application for tracking. Yamaguchi al @cite proposed a similar approach using a more sophisticated model that tries to predict destinations and groups based on features and classifiers trained on annotated sequences. Both methods use agent-based models and predict future locations using techniques similar to crowd simulations. They are not applicable to re-identification, as our goal is not to predict but to associate hypotheses. Therefore, we use social and contextual constraints for re-identification in an offline manner. Furthermore, both these methods require observations to be in metric coordinates, which for many real scenarios might be impractical.
- The proposed problem is closely related to product recommender systems that are able to separate substitues and complements @cite @cite . @cite first propose to incorporate the concepts of substitutes and complements into recommendation systems by analyzing navigation logs. More specifically, predicting complementary relations is pioneered by @cite . They utilize topic models and customer purchase information (e.g., the products in the items also viewed'' section and the items also bought'' section of a product page) to predict category-level substitutes and complements. However, we observe that purchase information generated by the unknown algorithm from Amazon.com tends to be noisy and inaccurate for complementary entities since co-purchased products may not be complementary to each other. We demonstrate that their predictions are non-complementary entities for the products that we use for experiments in Section . Also, category-level predictions are not good enough for specific pairs of products (i.e., and are not complements). Furthermore, their predictions do not provide information about incompatible entities, which are valuable buying warnings for customers. Thus, fine-grained extraction of complementary entities from reviews that express firsthand user experience is important. To the best of our knowledge, the linguistic patterns of complementary relations are not studied in computer science.
- The proposed problem is closely related to aspect extraction @cite @cite @cite @cite , which is to extract product features from reviews. More specifically, extracting comparable products (i.e, one type of substitutes, or products that can replace each other) from reviews is studied by Jindal and Liu @cite . Recently, dependency paths @cite are used for aspect extraction @cite @cite . @cite use unsupervised graph labeling method to identify entities from opinion targets. However, since aspects are mostly context independent and the same aspect may appear multiple times, aspect extraction in general does not need to extract each occurrence of an aspect (as long as the same aspect can be extracted at least once). In contrast, the CER problem is context dependent and many complementary entities are infrequent (i.e., is infrequent than the aspect ). We use dependency paths to accurately identify each occurrence of complementary entities. Since extracting each complementary entity can be inaccurate, we further utilize domain knowledge to improve the precision.
- CER is closely related to Named Entity Recognition (NER) @cite and relation extraction @cite . NER methods utilize annotated data to train a sequential tagger @cite @cite @cite . However, our task is totally different from NER since we care about the context of a complementary entity and many complementary entities are not named entities (e.g., ). CER is also different from relation extraction @cite @cite @cite @cite , which assumes that two entities are identified in advance. In reviews, the target entity is unfortunately missing in many cases (i.e., Works with my phone''). The proposed method only cares about the relation context of a complementary entity rather than a full relation.
- Our concepts of are similar with the work of . For lexicalized grammars such as Combinatory Categorial Grammar (CCG), Tree-Adjoining Grammar (TAG) and Head-Driven Phrase Structure Grammar (HPSG), each word in the input sentence is assigned one or more super tags, which are used to identify the syntactic role of the word for constraint parsing @cite @cite @cite @cite @cite . For a lexicalized grammar, can benefit the parsing in both accuracy and efficiency by offering information. In particular, defined the concept in TAG, which is similar to our . However, there are three differences. First, the is defined to describe the main syntactic tree structure with a series of unary projections, while is defined to describe how words can start or end hierarchical constituents (it is possible to be empty if the word cannot start or end constituents). Second, are extracted from gold trees and used to prune the search space of parsing as hard constraints. In contrast, we use constituent hierarchies as soft features. Third, use to prune a chart parsing, while we use to improve a linear shift-reduce parser.
- Under the lexicalized grammar, this can benefit the parsing with more accuracy and efficiency as @cite . Recently, the works on obtaining the super tags appear. proposed the efficient methods to obtain super tags for HPSG parsing using dependency information. and turn to design recursive neural network for for CCG parsing. In contrast, our models predict the constituent hierarchy instead of single super tag for each word in the input sentence, which are also likely regarded as the member of multiple ordered labels prediction family.
- Our constituent hierarchy predictor is also related to sequence-to-sequence learning @cite , which is successful in neural machine translation @cite . The neural model encodes the source-side sentence into dense vectors, and then uses them to generate target-side word by word. There has also been work that directly use sequence-to-sequence model for constituent parsing, which generates bracketed constituency tree given raw sentences @cite @cite . Compared to , who predicts a full parser tree from input, our predictors tackle a much simpler task, by predicting the constituent hierarchies of each word separately. In addition, the outputs of the predictors are used for soft lookahead features in bottom-up parsing, rather than being taken as output structures directly.
- By integrating the neural constituent hierarchy predictor, our parser is related to neural network models for parsing, which has given competitive accuracies for both constituency parsing @cite @cite and dependency parsing @cite @cite @cite . In particular, our parser is more closely related to neural models that integrates discrete manual features @cite @cite . use neural features to rerank a sparse baseline parser; Durrett and Klein directly integrate sparse features into neural layers in a chart parser. In contrast, we integrate neural information into sparse features in the form of lookahead features.
- The task of efficient query execution in database systems is similar to the task of execution optimization using runtime approximations in LD frameworks. Efficient and scalable data management has been of central importance in database systems @cite . Over the past few years, there has been an extensive work on query optimization in databases that is based on statistical information about relations and intermediate results @cite . The author of @cite gives an analytic overview regarding the procedure of query optimization and the different approaches used at each step of the process.
- Another set of approaches in the field of query optimization have focused on creating dynamic execution plans. Dynamic planning is based on the idea that the execution engine of a framework knows more than the planner itself. Therefore, information generated by the execution engine is used to re-evaluate the plans generated by the optimizer. There has been a vast amount of approaches towards dynamic query optimization such as query scrambling for initial delays @cite , dynamic planning in compile-time @cite , adaptive query operators @cite and re-ordering of operators @cite .
- Discrete Congestion Games. As the first seminal work regarding the computational complexity of equilibrium computation in congestions games, @cite showed that the problem of computing a pure Nash equilibrium is PLS-complete for network congestion games. @cite strengthened this result to hold even for network congestion games with linear cost functions. On the other hand, there are polynomial algorithms for symmetric network congestion games (cf. @cite ), for matroid congestion games with player-specific cost functions ( @cite @cite ), for polymatroid congestion games with player-specific cost functions and polynomially bounded demands ( @cite @cite ) and for so-called total unimodular congestion games (see Del @cite ). Further results regarding the computation of approximate equilibria in congestion games can be found in @cite @cite , Chien and Sinclair @cite and Skopalik and V "ocking @cite .
- Atomic Splittable Congestion Games. Atomic splittable network congestion games with player-independent cost functions have been studied (seemingly independent) by @cite and Haurie and Marcotte @cite and Marcotte @cite . Both lines of research mentioned that Rosens' existence result for concave games on compact strategy spaces implies the existence of pure Nash equilibria via Kakutani's fixed-point theorem. @cite presented the first upper bounds on the price of anarchy in atomic splittable congestion games. These were later improved by Harks @cite and finally shown to be tight by Schoppmann and Roughgarden @cite .
- For the computation of equilibria, Marcotte @cite proposed four numerical algorithms and showed local convergence results. Meunier and Pradeau @cite developed a pivoting-algorithm (similar to Lemkes algorithm) for nonatomic network congestion games with affine player-specific cost functions. Polynomial running time was, however, not shown and seems unlikely to hold. @cite considered nonatomic routing games on parallel links with affine player-specific cost functions. They developed a convex potential function that can be minimized within arbitrary precision in polynomial time. @cite considered general concave games with compact action spaces and investigated algorithms computing an approximate equilibrium. Roughly speaking, they discretized the compact strategy space and use the Lipschitz constants of utility functions to show that only a finite number of representative strategy profiles need to be considered for obtaining an approximate equilibrium (see also @cite for a similar approach). The running time of the algorithm, however, depends on an upper bound of the norm of strategy vectors, thus, implying only a pseudo-polynomial algorithm for our setting.
- . Some of the first widely noted successes of deep sequence-to-sequence learning models were for the task of machine translation @cite @cite @cite @cite @cite @cite . In several respects, this is actually a similar task to video caption generation, just with a rather different input modality. What they share in common is that both require bridging different representations, and that often an encoder-decoder paradigm is used with a Recurrent Neural Network (RNN) decoder to generate sentences in the target language. Many techniques for video captioning are inspired by neural machine translation ones, including soft attention mechanisms to focus on different parts of the input when generating the target sentence word by word @cite .
- Image Captioning . Image captioning can be regarded as a greatly simplified case of video captioning, with videos consisting of just a single frame. Recurrent architectures are often used here as well @cite @cite @cite @cite @cite . Spatial attention mechanisms allow for focusing on different areas of an image @cite . Recently, image captioning incorporating semantic concepts have achieved inspiring results. A semantic attention approach has been proposed @cite to selectively attend to semantic concept proposals and fuse them into hidden states and outputs of RNNs, but their model is difficult to extend for multiple channels. Overall, none of these methods for image captioning need to account for temporal and motion aspects.
- Video captioning . For video captioning, many works utilize a recurrent neural architecture to generate video descriptions, conditioned on either an average-pooling @cite or recurrent encoding @cite @cite @cite @cite of frame-level features, or on a dynamic linear combination of context vectors obtained via temporal attention @cite . Recently, hierarchical recurrent neural encoders (HRNE) with attention mechanism have been proposed to encode video @cite . A recent paper @cite additionally exploits several kinds of visual attention and relies on a multimodal layer to combine them. In our work, we present a novel attention model with more effective multimodal layers that jointly models multiple heterogeneous signals, including semantic attributes, and experimentally show the benefits of this approach over previous work.
- Both the two-stage approach for SOCPs @cite and consensus ADMM for QCQPs @cite deal with general optimization frameworks, but require a large number of auxiliary variables and hence lift the original problem into a much higher dimension space. Customized algorithms for specific problems to enable efficient or parallel computing are developed in @cite @cite by exploiting the structures of the problems. Specifically, in @cite , the authors consider the robust coordinated beamforming in multi-cell networks. By using SDR approximation and S-Procedure methods, the original non-convex beamforming problem is first reformulated as a tractable convex SDP. Then an ADMM-based distributed algorithm is proposed, which is provably able to converge to the global optimum of the centralized SDP problem. The ADMM update in each BS, however, still relies on general-purpose solvers. In @cite , the joint BS activation and beamforming design for power minimization problem in HetNets is first reformulated as an SOCP using a sparsity regularizer. An efficient algorithm based on ADMM is developed to solve the SOCP, in which each updating step is in closed form and can be carried out distributively among multiple BSs.
- Pioneering works on saliency prediction were based on the Feature Integration Theory proposed by Treisman al @cite in the eighties. Itti al @cite defined the first computational model to predict saliency on images: this work, inspired by Koch and Ullman @cite , computed a set of individual topographical maps representing low-level cues such as color, intensity and orientation and combined them into a global saliency map. After this seminal work, a large variety of methods explored the same idea of combining complementary low-level features @cite @cite @cite and often included additional center-surround cues @cite @cite . Other methods enriched predictions exploiting semantic classifiers for detecting higher level concepts such as faces, persons, cars and horizons @cite @cite @cite @cite @cite . Related research efforts have also been done in the compressed domain, as in @cite @cite .
- It is well known that deep learning approaches strongly depend on the availability of sufficiently large datasets. The publication of a large-scale eye-fixation dataset, SALICON @cite , indeed contributed to a big progress of deep saliency prediction models. Huang al @cite introduced an architecture consisting of a deep neural network applied at two different image scales. They compared different standard CNN architectures such as AlexNet @cite , VGG-16 @cite and GoogleNet @cite , in particular showing the effectiveness of the VGG network.
- In this work, instead, we model the center bias present in human gazes using multiple learned prior maps. This is different from the approaches of @cite and @cite , as we let the network learn a set of Gaussian parameters, keeping it trainable end-to-end without predefined information.
- Recently, Pan al @cite introduced , a deep network for saliency prediction trained with adversarial examples. As all other Generative Adversarial Networks, it is composed by two modules, a generator and a discriminator, which combine efforts to produce saliency maps.
- In this work, we also employ the ResNet @cite model to extract feature maps from the input image. The only other saliency model that exploits this network is proposed by Liu al @cite and called . This model simultaneously incorporates global and scene contexts to infer image saliency thanks to a deep spatial contextual LSTM which scans the image both horizontally and vertically.
- A related line of research is that of explaining activations of a neural model by means of techniques based on backpropagation @cite . It is worthwhile to notice that this research line is very different from that of saliency prediction, as it does not aim to replicate human fixations.
- Salient object detection is slightly related to the topic of this work, even though it is a significantly different task. Salient object detection consists, indeed, in identifying a binary map indicating the presence of salient objects @cite @cite @cite @cite . On the contrary, in saliency prediction the objective is to predict a density map of eye fixations.
- A saliency detection approach which is in some aspects related to our work is that of Kuen al @cite , in which a recurrent (non convolutional) network provides salient object detection. At each timestep, their recurrent network outputs the parameters of a spatial transformation which is used to focus on a particular location of the image, and builds the binary prediction for that location. Our recurrent network is, instead, convolutional, and is used to process saliency features by iteratively refining the prediction.
- Bitouk al @cite , for example, automatically substituted an input face by another face selected from a large database of images based on the similarity of appearance and pose. The method replaces the eyes, nose, and mouth of the face and further makes color and illumination adjustments in order to blend the two faces. This design has two major limitations which we address in this paper: there is no control over the output identity and the expression of the input face is altered.
- A more difficult problem was addressed by Dale al @cite . Their work focused on the replacement of faces in videos, where video footage of two subjects performing similar roles are available. Compared to static images, sequential data poses extra difficulties of temporal alignment, tracking facial performance and ensuring temporal consistency of the resulting footage. The resulting system is complex and still requires a substantial amount of time and user guidance.
- One notable approach trying to solve the related problem of pupeteering -- that is, controlling the expression of one face with another face -- was presented by Suwajanakorn al @cite . The core idea is to build a 3D model of both the input and the replacement face from a large number of images. That is, it only works well where a few hundred images are available but cannot be applied to single images.
- An alternative to the expensive optimization approach was proposed by Ulyanov al @cite and Johnson al @cite . They trained feed-forward neural networks to transform any image into its stylized version, thus moving costly computations to the training stage of the network. At test time, stylization requires a single forward pass through the network, which can be done in real time. The price of this improvement is that a separate network has to be trained per style.
- While achieving remarkable results on transferring the style of many artworks, the neural style transfer method is less suited for photorealistic transfer. The reason appears to be that the Gram matrices used to represent the style do not capture enough information about the spatial layout of the image. This introduces unnatural distortions which go unnoticed in artistic images but not in real images. Li and Wand @cite alleviated this problem by replacing the correlation-based style loss with a patch-based loss preserving the local structures better. Their results were the first to suggest that photo-realistic and controlled modifications of photographs of faces may be possible using style transfer techniques. However, this direction was left fairly unexplored and like the work of Gatys al @cite , the approach depended on expensive optimization. Later applications of the patch-based loss to feed-forward neural networks only explored texture synthesis and artistic style transfer @cite .
- Since our approach to face replacement is rather unique, the results look different from those obtained with more classical computer vision techniques @cite @cite @cite or using image editing software (compare Figures and ). While it is difficult to compete with an artist specializing in this task, our results suggest that achieving human-level performance may be possible with a fast and automated approach.
- Voxel space reasoning. Another line of work completes and labels 3D scenes, but with separate modules for feature extraction and context modeling. Zheng al @cite predict the unobserved voxels by physical reasoning. Kim al @cite train a Voxel-CRF model from labeled floor plans to optimize the semantic labeling and reconstruction for indoor scenes. Hane al @cite and Blaha al @cite use joint optimization for multi-view reconstruction and segmentation for outdoor scenes. However, this line of work uses predefined features, and separates the feature learning from the context modeling, and it is expensive for CRF-based models to encode long-range contextual information. In contrast, our model is able to jointly learn the low-level feature representation and high-level contextual information end-to-end from large-scale 3D scene data, directly modeling long-range contextual cues though big receptive field.
- Previously published material. In our previously published paper @cite , we presented a research proposal with the goal of analyzing natural language NFRs taken from industrial requirements specifications to better understand their nature. Our study reported here, relies on and extends our previous study design. We present the results in full detail, and provide a comprehensive discussion on the implications on software engineering disciplines.
- NFR classifications. There exist several classification schemes for NFRs in literature (e.g., @cite @cite @cite @cite @cite @cite ). One example for such a classification, which is based on a quality model, is the @cite . It defines external and internal quality of a software system and derives several quality characteristics (e.g., Functionality--Security or Portability--Installability ). Sommerville further provides a classification scheme based on a distinction between process requirements , product requirements , and external requirements @cite . We base our distinction of NFR classes on the classification. Furthermore, we exclude process requirements from our study, as they do not describe properties of the system itself.
- Pohl @cite discusses the misleading use of the term non-functional'' and argues to use quality requirements'' for product-related NFRs that are not constraints. Glinz @cite performs a comprehensive review on the existing definitions of NFRs, analyzes problems with these definitions, and proposes a definition on his own. He highlights three different problems with the current definitions: a definition problem, i.e., NFR definitions have discrepancies in the used terminology and concepts, a classification problem, i.e., the definitions provide very different sub-classifications of NFRs, and finally a representation problem, i.e., the notion of NFRs is representation-dependent. In our study, we faced all of the three problems: we motivate our study based on the definition and classification problem and during the execution of our study, we faced the representation problem (see also our discussion on threats to validity in ). Although we agree on the critique about the obsolete and misleading notion of the term non-functional'', it still dominates the way requirements are handled in practice, as reflected in our data.
- @cite perform a literature review on NFRs, investigating the notion of NFRs in the software engineering literature to increase the understanding of this complex and multifaceted phenomenon. Amongst others, they found about 114 different NFR classes. As a result of a frequency analysis, they found that the five most frequently mentioned NFR classes in literature are performance , reliability , usability , security , and maintainability (in that order). In our study, we got similar results: we found that the five most frequently used NFR classes in our industrial specifications are security , reliability , usability , efficiency , and portability (in that order). We excluded functionality from this list, as it is not a classical NFR class . While performed their analysis on available literature, our study analyzes NFRs documented in industrial projects.
- Although the exact research question has not been addressed in any study as far as the author's knowledge is concerned. A closest work which evaluate the RGB-NIR images for semantic segmentation is reported in @cite . The authors used a CRF based object detection framework and found that it might be partially useful for some classes. It differs from the work reported here, in the choice of method as well as the type of the inquisitive query. In addition to this, the closest work would be the semantic segmentation studies which utilize CNN and predict pixel-wise probabilities. Notably, SegNet reported in @cite could be one of the recent state-of-the-art in addition to FCN @cite which has already been explained in detail in the previous section. However, since FCN is more recent and has already claimed to be better performing therefore, SegNet has not been included in the experimentation.
- The concept of code smells was, to the best of our knowledge, first proposed by Fowler and Beck @cite to answer the question at which point the quality of code is so low that it must be refactored. According to Fowler and Beck, the answer cannot be objectively measured, but we can look for certain concrete, visible symptoms, such as duplicated code @cite as an indicator for bad maintainability @cite . This concept of smells, as well as the list that Fowler and Beck proposed, led to a large field of research. @cite provide an in-depth analysis of the state of the art in code smells. The metaphor of smells as concrete symptoms has since then been transferred to quality of other artifacts including (unit) test smells @cite and smells for system tests in natural language @cite . @cite , further characterize different defects of use cases through the term use case smell. In our work, we extend the notion of smells to the broader context of requirements engineering and introduce a concrete definition for the term .
- Various authors have worked on QA for software requirements by applying manual techniques. Some put their focus on the classification of quality into characteristics @cite , others develop comprehensive checklists @cite @cite @cite @cite @cite . Regarding QA, some develop constructive QA approaches, such as creating new RE languages, e.g. @cite , to prevent issues up front, others develop approaches to make analytic QA, such as reviews, more effective @cite . In a recent empirical study on analytical QA, @cite manually investigate the presence of defects in use cases. To sum it up, these works on manual QA provide analytical and constructive methods, as well as (varying) lists for defects. They strengthen our confidence that today's requirements artifacts are vulnerable to quality defects.
- One specific area of QA is avoiding redundancy and cloning. Whereas @cite use ConQAT to search for syntactic identity resulting from a copy-and-paste reuse, @cite aim at detecting similar content, therefore using methods from information retrieval (such as Latent Semantic Analysis @cite ). @cite extend this work specifically for use cases. Their tool, ReqAlign, classifies each step with a semantic abstraction of the step. These publications analyze the performance of their approaches, and depending on the artifact and methods achieve precision and recall close to 1 (see Table ).
- The remaining approaches listed in Table aim at detecting ambiguities in unconstraint natural language. Since the quality defects detected by the approaches by @cite , Kof @cite , HeRA by @cite @cite , @cite , RESI by K " @cite @cite @cite , and Alpino by @cite are not the ones discussed in ISO 29148 and since we could not find an evaluation of precision and recall of these approaches, we omit discussing these approaches in-depth here. An analysis of what these approaches focus on in detail as well as their evaluation can be found in short in Table and in full length in our supplementary material online @cite . In the following, we first report on those publications that focus on criteria different from ISO 29148, but which report precision or recall. Afterwards, we describe publications that aim at detecting quality violations of ISO 29148 (see Table ).
- First, @cite target the specific grammatical issue of coordination ambiguity (detecting problems of ambiguous references between parts of a sentence), mostly through statistical methods, such as occurrence and co-occurrence of words. In a case study, they report on a precision of their approach mostly between 54 , Krisch and Houdek @cite , focus on the detection of passive voice and so-called weak words. They present their dictionary- and POS-based approach to practitioners and find many false positives, similar to our RQ 3. In average, a precision of 12 approaches focus on very related, but not identical quality violations or smells.
- In summary, in our contribution, we extend the current state of reported evidence on automatic QA for requirements artifacts via systematic studies in terms of distribution, precision, recall, and relevance, as well as by means of a systematic evaluation with practitioners under realistic conditions. We perform this on both existing, as well as new quality defects taken from the ISO 29148. Therefore, we extend our previously published first empirical steps @cite to close these gaps by thorough empirical evaluation.
- Mobile-edge computing has been recently introduced @cite as a way to move the cloud'', i.e., the servers processing mobile traffic, closer to users, thus reducing the latency and load of networks. Network Function Virtualization is widely regarded to as an enabling technology for MEC (see, e.g., @cite ). Recent works have studied the radio techniques needed to enable MEC @cite , its relationship to the Internet-of-things @cite and context-aware, next-generation networks @cite . Closer to our scenario, the authors of @cite study how caches and servers should be placed in the network as its load changes over time. With regard to MEC and caching, a prominent application is mobile video streaming. As an example, @cite @cite account for layered video coding techniques, and address the problem of placing the right layers at the right cache -- with @cite also accounting for cooperation between operators. Other works @cite @cite aim at foreseeing the content demand, in order to proactively populate caches or serve users.
- Not many works exist that combine real-world traces and mobile edge computing. Among the most recent ones, @cite studies the price (in terms of additional infrastructure) of deploying caches within the cellular core network. Compared to this study, @cite only focuses on caching and vehicular traffic, and it only considers the dataset for the city of Los Angeles.
- There is a wide variety of quality models. @cite differentiate between quality definition models and quality assessment models. The former is a specification of what constitutes quality in a software system, the latter describes how a software system's quality can be assessed according to specific rules. In the area of software security, security pattern collections are an example of quality definition models, e.g., @cite .
- Common describe quality by measurable concepts that imply strong assumptions. While for some quality attributes, those assumptions are stable for others, such as security, the assumptions are changing fast @cite . Due to their single-value representation, metrics often do not explain how system properties influence the quality related activities that are performed with the system @cite . Hence, metrics are not well established for security @cite and unstable due to fast variation of the security underlying physics'' (i.e., the IT system) @cite .
- @cite propose an security assessment method that has similarities to the method in this paper. It also defines metrics and aggregates them to quality attributes. This method, however, uses -ilities'' similar to ISO 9126 that have several well-known problems. Moreover, they concentrate on the architecture of the software (white-box view) whereas this paper focuses on testing by application scanners (black-box view).
- @cite use Dynamic Bayesian Networks to investigate the security of networked systems. Their focus is more on the combined effects of different vulnerabilities as opposed to a complete quality statement for the system incorporating scan results.
- Deep generative models (DGMs) are good at discovering the underlying structures in the input data, but the training of the model parameters and inference of the posterior distribution are highly nontrivial tasks. Recently, significant progress has been made on enriching the representative power of variational inference and Markov chain Monte Carlo methods for posterior inference, such as variational Autoencoders (VAEs) @cite @cite and neural adaptive MCMC @cite . VAEs @cite @cite build a recognition model to infer the posterior of latent variables and the parameters are trained to optimize a variational bound of the data likelihood. Neural adaptive MCMC @cite employs a similar recognition model as the proposal distribution for importance sampling to estimate the gradient of log-posterior and hence can perform approximate Bayesian inference of DGMs.
- Some recent advances @cite @cite @cite @cite @cite have been made on extending DGMs to deal with partially observed data. For example, the conditional VAEs @cite treat labels as conditions of DGMs to describe input data; they perform posterior inference of labels given unlabeled data and can generate a specific class of images. ADGM @cite introduces auxiliary latent variables to DGMs to make the variational distribution more expressive and does well in semi-supervised learning. Cat-GAN @cite generalizes GANs with a categorical discriminative network and an objective function that includes the mutual information between the input data and the prediction of the discriminative network. @cite proposes feature mapping, virtual batch normalization and other techniques to improve the performance of GANs on semi-supervised learning and image generation. The Ladder Network @cite achieves excellent classification results in semi-supervised learning by employing lateral connections between autoencoders to reduce the competition between the invariant feature extraction and the reconstruction of object details.
- Our work is complimentary to the above progress in the sense that we investigate a new criterion (i.e., max-margin learning) for DGMs in both supervised and semi-supervised settings. Some preliminary results on the fully supervised mmDGMs were published in @cite , while the semi-supervised extensions are novel.
- The performance of vertex classification highly depends on the quality of vertex features. Traditional methods usually employ hand-crafted features that relevant to the category systems (e.g., age @cite , gender @cite , profession @cite , personality @cite et. al). However, these methods usually need many human efforts and lack of capability to other real-world scenarios.
- To address these challenges, researchers propose NRL to construct effective features automatically @cite @cite . Moreover, the low-dimensional representations in NRL models also overcome the sparsity issue and are more efficient for computation. It's also worth noting that many semi-supervised NRL models are proposed to improve the particular vertex classification task, including MMDW @cite , DDRW @cite , Planetoid @cite , and GCN @cite . These methods can incorporate labeling information during the learning process, and obtain discriminative representations of vertices.
- Traditional methods use hand-crafted similarity measurements such as common neighbors @cite , Salton index @cite , Jaccard index and resource allocation index @cite for link prediction. Besides these topology-based methods, link propagation methods @cite @cite are based on graph regularization, which is a common approach in semi-supervised learning. Matrix factorization methods @cite predict links by adapting matrix completion technic to the adjacency matrix. Further, people employ tensor to develop temporal model @cite for link prediction on dynamic networks. Other methods improve the effect of link prediction by taking information like community affinity @cite and vertex attribute @cite into account.
- Detecting communities from networks is a critical research filed in social science. In terms of community detection, traditional methods focus on partitioning the vertices into different groups, i.e., detecting non-overlapping communities. Existing non-overlapping community detection works mainly include clustering-based methods @cite , modularity based methods @cite @cite @cite , spectral algorithms @cite , stochastic block models @cite @cite and so on. The major drawback of these traditional methods is that they cannot detect overlapping communities, which may not accord with real-world scenarios. To address this problem, CPM @cite is proposed to generate overlapping communities by merging overlapping @math -cliques. Ahn @cite proposes link clustering for overlapping community detection by employing non-overlapping community detection methods to partition the links instead of vertices and then assigning a single vertex to corresponding groups of its links.
- In recent years, community affiliation based algorithms show their effectiveness on overlapping community detection @cite @cite @cite @cite . Community affiliation based algorithms predefine the number of communities and learn a vertex-community strength vector for each vertex and assign communities to vertices according to the vector. For example, Yang @cite proposes Non-negative Matrix Factorization (NMF) method, which approximates adjacency matrix @math by @math where matrix @math is vertex-community affinity matrix. Then the algorithm learns non-negative vertex embeddings and converts each dimension of the embeddings into a community. These community affiliation based algorithms try to approximate the adjacency matrix in value and design different objective functions for it. Our model follows the idea to represent the community relationship with a non-negative vector, but we don't explicitly set hard community membership for the vertices.
- Representation learning has shown its effectiveness in computer vision @cite and natural language processing @cite . Representation learning is also becoming an important technique for network analysis in recent years. Current methods @cite @cite @cite @cite embed each vertex into a real-valued vector space based on modeling local information and take the representations as features in further evaluation tasks.
- More specifically, social dimension @cite computes the top- @math eigenvectors of adjacency matrix and takes them as @math -dimensional representations. By first generating random walks from a network, DeepWalk @cite employs word2vec @cite , a widely-used word representation learning algorithm, to learn vertex embeddings from random walk sequences. Comparing with DeepWalk, node2vec @cite designs an effective random walk strategy for sequence generation based on BFS and DFS. LINE @cite characterizes the probability of first-order and second-order proximities between two vertices. GraRep @cite models @math -order proximity between two vertices through matrix factorization. Struc2vec @cite constructs a multilayer context graph to encode structural similarities between vertices and generate structural context for them. Some works employ deep neural networks to learn vertex representations, such as deep auto-encoders @cite @cite , convolutional neural networks @cite @cite and deep generation models @cite .
- Vertices in real-world networks usually accompany with heterogeneous information, such as text contents and labels. There are a variety of works that attempt to incorporate this information into NRL. TADW @cite and CANE @cite extend existing NRL models to take advantage of the accompanying text information. By utilizing labeling information of vertices, MMDW @cite employs max-margin principle to learn discriminative network representations. Chang @cite design a deep embedding architecture for capturing complex heterogeneous data in a network. TransNet @cite employs translation mechanism to model the interactions among vertices and labels on edges. GCN @cite and GraphSAGE @cite takes additional features of vertices as inputs and generate different levels of vertex representations layer-by-layer.
- Traditional re-id approaches typically use low-level or mid-level features like colors, shapes and attributes to describe the appearance of a person and learn a good distance metric. Lots of research work falls into this category @cite @cite @cite @cite @cite @cite @cite . Driven by rapid development of deep learning, deep convolutional neural network (dCNN) is used to extract features from raw images and various methods are proposed to embed dCNN feature to re-id search space, which are regarded as deep metric learning here. The siamese network structure @cite @cite @cite @cite @cite @cite is popular for its incorporation of deep feature extraction and discrimination into a unified framework. Recently, Varior al @cite proposed a subnetwork acting as gate to selectively enhance similar pieces in the whole feature map. Varior al @cite proposed to divide the image of a person into several rows and feed horizontal clips to an RNN based LSTM, which is followed by the siamese loss. McLaughlin al @cite used the similar LSTM and siamese loss on video-based re-id and achieved the state-of-the-art performance on several video re-id datasets. Siamese approach treats re-id as a classification problem @cite .
- @cite aims to learn a set of hash functions as Eq. . Such functions can preserve the ordinal relations between @math and @math , where @math is the dissimilarity between the @math -th and the @math -th item. Its goal is to make sure the ordinal relation can be preserved in the produced Hamming space: @math To embed such ordinal relations, OEH first constructs a directed unweighted ordinal graph @math , where each node @math is the dissimilar degree @math , and each directed edge is defined via @math . Then the objective function is to minimize the inconsistency between the given ordinal relation graph and the ones generated from the corresponding hash codes. At last, by using the landmark-based ordinal graph, the quartic ordinal relation is transformed to the triplet ordinal relation, which transforms the target of OEH to @math where @math and @math are the landmark points.
- Object recognition is one of the most important and challenging problems in computer vision. The ability to classify objects plays a crucial role in scene understanding, and is a key requirement for autonomous robots operating in both indoor and outdoor environments. Recently, computer vision has witnessed significant progress, leading to impressive performance in various detection and recognition tasks @cite @cite @cite . On the one hand, this is partly due to the recent advancements in machine learning techniques such as deep learning, fueled by a great interest from the research community as well as a boost in hardware performance. On the other hand, publicly-available datasets have been a great resource for bootstrapping, testing, and comparing these techniques.
- Examples of popular image datasets include ImageNet, CIFAR, COCO and PASCAL, covering a wide range of categories including people, animals, everyday objects, and much more @cite @cite @cite @cite . Other datasets are tailored towards specific domains such as house numbers extracted from Google Street View @cite , face recognition @cite , scene understanding and place recognition @cite @cite , as well as object recognition, manipulation and pose estimation for robots @cite @cite @cite @cite .
- On low spatial resolution: There are various methods proposed to address the low spatial resolution issue in MLA-based light field cameras. One main approach is to apply super-resolution image restoration to light field sub-aperture images. Super-resolution in a Bayesian framework is commonly used, for example, in @cite with Lambertian and textural priors, in @cite with a Gaussian mixture model, and in @cite with a variational formulation. Learning-based methods are adopted as well, including dictionary-based learning @cite and deep convolutional neural networks @cite @cite . In addition to spatial domain super-resolution restoration, Fourier-domain techniques @cite @cite and wave optics based 3D deconvolution methods @cite @cite @cite @cite have also been utilized.
- Alternative to the standard MLA-based light field camera design @cite , where the MLA is placed at the image plane of the main lens and the sensor is placed at the focal length of the lenslets, there is another design approach where the MLA is placed to relay image from the intermediate image plane of the main lens to the sensor @cite . This design is known as focused plenoptic camera.'' As in the case of the standard light field camera approach, super-resolution restoration for focused plenoptic cameras is also possible @cite .
- All single-sensor light field imaging systems are fundamentally limited by the spatial-angular resolution trade-off, and the above-mentioned restoration methods have performance limitations in addition to the computational costs. Another approach for improving spatial resolution is to use a hybrid two camera system, including a light field camera and a high-resolution camera, and merge the images to improve spatial resolution @cite @cite @cite . Dictionary-learning based techniques are adopted @cite @cite in this problem as well: High-resolution image patches from the regular camera are extracted and stored as a high-resolution patch dictionary. These high-resolution patches are downsampled; and from the downsampled patches, low-resolution features are extracted to form a low-resolution patch dictionary. During super-resolution reconstruction, a low-resolution image patch is enhanced through determining (based on feature matching) and using the corresponding high-resolution patches in the dictionary. In @cite , high-resolution image is decomposed with complex steerable pyramid filters; the depth map from the light field is upsampled using joint bilateral upsampling; perspective shift amounts are estimated from the upsampled depth map, and these shift amounts are used to modify the phase of the decomposed high-resolution image; with the modified phases, pyramid reconstruction is applied to obtain high-resolution light field.
- On narrow baseline: One of the most important features of light field cameras is the ability to estimate depth. However, it is known that depth accuracy and range is limited in MLA-based light field cameras due to narrow baseline. The relation between baseline and depth estimation accuracy in a stereo system has been studied in @cite . In a stereo system with focal length @math and baseline @math , the depth @math of a point with disparity @math is obtained through triangulation as @math . With a disparity estimation error of @math , the depth estimation error @math becomes @cite :
- For an MLA-based light field camera, the maximum baseline is less than the size of the main lens aperture, making depth estimation very challenging. There are methods specifically proposed for depth estimation in MLA-based light field cameras. For example, in @cite , the problem is formulated as a constrained labeling problem on epipolar plane images in a variational framework. In @cite , ray geometry of 3D line segments is imposed as constraints on light field triangulation and stereo matching. In @cite , defocus and shading cues are used to improve the disparity estimation accuracy.
- As reported in @cite , there are two learning systems instantiated in mammalians:'' 1) the neocortex gradually acquires sophisticated knowledge representation, and 2) the hippocampus quickly learns specifics of individual experiences. CNNs are typically trained using big data, and contain rich appearance patterns of objects. If one compares CNNs to the neocortex, then the fast retrieval of latent patterns related to a semantic part can be compared to the short-term learning in hippocampus.
- In order to explore the hidden semantics in the CNN, many studies have focused on the visualization of CNN units @cite @cite @cite @cite @cite and analyzed their statistical features @cite @cite @cite @cite . @cite extracted and visualized a subspace of CNN features.
- Going beyond passive'' visualization, some studies actively'' extracted CNN units with certain semantics for different applications. Zhou @cite @cite discovered latent scene'' semantics from CNN feature maps. Simon discovered objects @cite in an unsupervised manner from CNN feature maps, and learned semantic parts in a supervised fashion @cite . In our study, given very few part annotations, we mine CNN patterns that are related to the semantic part. Obtaining clear semantics makes it easier to transfer CNN patterns to other part-based tasks.
- Transferring hidden patterns in the CNN to other tasks is important for neural networks. Typical research includes end-to-end fine-tuning and transferring CNN knowledge between different categories @cite @cite and or datasets @cite . In contrast, we believe that a good explanation and transparent representation of part knowledge will creates a new possibility of transferring part knowledge. As in @cite @cite , the AOG is suitable to represent the semantic hierarchy, which enables semantic-level interactions between human and neural networks.
- '' in un- weakly-supervised learning: Generally speaking, in terms of un- weakly-supervised learning, modeling parts is usually more challenging than modeling entire objects. Given image-level labels (without object bounding boxes), object discovery @cite @cite @cite can be achieved by identifying common foreground patterns from noisy background. Closed boundaries and common object structure are also strong prior knowledge for object discovery.
- In contrast to objects, semantic parts are hardly distinguishable from other common foreground patterns in an unsupervised manner. Some parts ( the abdomen) do not have shape boundaries to determine their shape extent. Inspired by graph mining @cite @cite @cite , we mine common patterns from CNN activation maps in conv-layers to explain the part.
- Part localization detection is an important task in computer vision @cite @cite @cite @cite . There are two key points to differentiate our study from conventional part-detection approaches. First, most methods for detection, such as the CNN and the DPM @cite @cite @cite , limit their attention to the classification problem. In contrast, our effort is to clarify semantic meanings of implicit CNN patterns. Second, instead of summarizing knowledge from massive annotations, our method mines CNN semantics with very limited supervision.
- . Our work is related to the so-called debating' technologies at IBM @cite . Their work is strongly based on machine learning methods to track correlated text passages that may be spread over a corpus. Going beyond the observational hypotheses that are more on our sight and are typical of social media, the goal is to enable eliciting all kind of textual information that may be relevant as evidence to decide on a claim. An example is: (topic) ''; (claim) ''; (evidence) .'' To date, as far as we know this is yet a fully NLP project, which does not consider multimedia data for grounding on material evidence.
- @cite . The validation of natural language claims on arbitrary properties does not seem to suit well sentiment analysis techniques. We show why by means of an example from our collected web claims. The sentence '' receives positive sentiment, while the author clearly does not appreciate his perceived Neymar's behavior. So claim polarity detection taken as sentiment detection is misleading here.
- That is related with irony, which is a particular challenge for sentiment analysis that requires advanced techniques. In fact, the problem of detecting claims on a specific property for an entity is a somewhat different one, which is endowed with a pre-defined conceptual structure (see ) and really seems to be better addressed directly by dependency parse trees. The preference of the latter over sentiment analysis techniques for the claim parsing and polarity detection task is discussed by @cite .
- . Propositional (yes no) questions are an important class of questions that are addressed by Q &A systems @cite , but our understanding of hypotheses as a specific kind of propositional questions is an important distinction in the enormous scope of Q &A systems. Our focus is not on factoid-style questions, e.g., Was X born in year Y?.'' By hypotheses' we mean the more subjective questions that are too intriguing to be freely answered, requiring some compelling (material) evidence.
- Since the inception of the Web, our research community has been interested in studying human navigational click data on the Web---e.g., see @cite . In this line of research, a variety of models has been proposed including the well-known Markov chain model utilized in this work @cite @cite @cite @cite , or models such as decentralized search @cite @cite motivated by small-world navigation @cite .
- Insights have been utilized to infer missing links @cite , to predict break-ups of the navigation process @cite , for recommendations @cite , or to improve the link structure of a website @cite . For the latter, @cite highlighted the importance of improving hyperlink structures based on their usage due to the large amount of unused links. Consequently, they proposed an algorithm for suggesting useful links and estimate their success based on clickthrough rates.
- Based on this wide range of studies aiming at understanding human Web navigation, a series of navigational regularities, patterns and strategies have been suggested. For example, West and Leskovec @cite found trade-offs between similarity and degree in navigational behavior suggesting different phases in user sessions, namely an exploration (orientation) and an exploitation (goal-seeking) phase @cite . Subsequent research has suggested that humans prefer to navigate between semantically similar websites @cite @cite @cite , have preferences for choosing links at the beginning of pages @cite @cite @cite , and that navigational patterns exhibit regularities with respect to underlying network characteristics @cite @cite @cite @cite .
- Our work is also broadly connected to research studying click data in other contexts, e.g., characterizing user behavior and sybil detection in online social networks @cite @cite , improving search engine ranking functions @cite @cite @cite @cite @cite , and marketing and next purchase prediction @cite @cite @cite @cite . However, these do not cover the specifics of large-scale information networks such as Wikipedia.
- A major step forward in speech recognition technology has arisen through the emergence of the Kaldi toolkit. Table shows results from @cite comparing Kaldi against other major ASR toolkits, with the performance of Kaldi with the DNN approach having a third of the error rate in comparison to the other software.
- Designing multicell beamformers under the CS CB mode has attracted a lot of research attention, such as @cite @cite @cite @cite @cite @cite @cite @cite . Uplink-downlink duality and iterative fixed-point iteration have been successfully exploited in @cite @cite @cite @cite @cite to obtain optimal beamformers to either minimize the sum transmit power at the BSs or maximize the minimum SINR at the MSs. Different to these previous studies, part of this work examines the application of uplink-downlink duality to optimize the multicell beamformers under the DPS mode.
- Several works have been devoted to the numerical computation of the fixation probability on specific classes of graphs, such as instances of bipartite graphs and Erd "os-R 'enyi random graphs @cite @cite @cite @cite .
- Several birth-and-death processes on graphs have been studied, beside the Moran process, which vary in the order in which the birth and death events occur in a step, and on whether the selection bias introduced by fitness is applied in the choice of the vertex that reproduces or the one that dies. The Voter model @cite is a well-known such process. It has been observe that small differences in these processes have significant impact on the fixation probability @cite @cite @cite . In particular, for dynamics where, unlike in Moran process, the vertex to die is chosen first, it is known that simple strong suppressors exits, such as the star @math with fixation probability @math , while there are no strong amplifiers @cite . Although not elaborated in the current paper, our results carry over to the process where the vertex to reproduce is chosen uniformly at random, and its offspring replaces a neighbor chosen with probability inversely proportional to its fitness. This corresponds to a setting where advantageous mutants do not reproduce more often, but they live longer.
- More involved population dynamics have been studied in evolutionary game theory in the context of well-mixed populations @cite @cite , and more recently in populations on graphs @cite @cite . The fitness of an individual is no longer fixed (determined by its type: mutant or non-mutant), but is determined by the expected payoff for the individual when playing a two-player game against a randomly chosen neighbor. In this game mutants play with one strategy and non-mutants with another.
- Finally, similar stochastic processes have been use to model dynamics other than evolutionary ones, such as the spread of influence in social networks @cite , the spread of epidemics @cite , the emergence of monopolies @cite , or interacting particle systems @cite .
- One group of methods is based on the bag of words model using algorithms such as tf-idf to adjust weights for terms in a representation of texts as vectors (cf. the introduction in ). Lee2008 proposed an unsupervised keyword extraction method by using a tf-idf model with some heuristics @cite . Our approach uses similarity measures for finding a perfect match for each dataset reference in an article by comparing titles of datasets in a registry to sentences in articles. Dice, Jaccard and Cosine can be applied to a vector representation of a text easily (cf. ). The accuracy of algorithms based on such similarity measures can be improved by making them semantics-aware, e.g., representing a set of synonyms as a single vector space dimension.
- Corpus and Web based methods often use information about the co-occurrence of two texts in documents, and are used for measuring texts' semantic similarity. Turney2001 introduced a simple unsupervised learning algorithm for detecting synonyms @cite , which searches queries through an online search engine and analyses the results. The quality of the algorithm depends on the number of search results returned.
- sighal2013 proposed an approach to extract dataset names from articles @cite . They employed the normalized Google distance algorithm (NGD), which estimates both the probability of two terms existing separately in a document, as well as of their co-occurrence. In this formula, @math is the number of all web pages searched. @math means the number of returned pages for @math as a query term and @math represents the number of pages for the intersection of @math and @math . They used two academic search engines -- Google Scholar and Microsoft Academic Search -- instead of a local corpus.
- Schaefer2014 proposed the Normalized Relevance Distance (NRD) @cite . This metric measures the semantic relatedness of terms. NRD is based on the co-occurrence of terms in documents, and extends NGD by using relevance weights of terms. The quality of these methods depends on the size of the corpus used.
- Sahami2006 suggest a similarity function based on query expansion @cite . Their algorithm determines the degree of semantic similarity between two phrases. Each of these phrases is searched by an online search engine and then expanded by using returned documents. Afterwards, the new phrases are used for computing similarity.
- The problem that we aim to solve involves the two subtasks of 1. identifying dataset references in an article, and then 2. finding at least one correct match for each of these identified references. Literature citation mining is the process of determining the number of citations that a specific article receives. It constructs a literature citation network, which can be used for detecting the impact of an article @cite . Citation mining can usually be handled by three subtasks. First, literature references should be extracted from the bibliography section of a document, and afterwards, metadata extraction should be applied on the references extracted in the first phase. Finally, each reference should be linked to the cited article by using the metadata extracted in the second step @cite .
- proposed a rule-based citation mining technique @cite . Their approach detects literature references from each document and then extracts citation metadata from each of them, such as title, authors, and venue. Based on the venue, it then extracts all related titles from the DBLP computer science bibliography, which contains more than three million articles. Finally, it tries to link the title of each extracted literature reference and titles found in DBLP.
- Many different machine learning approaches have been employed for extracting metadata, and in a few cases also for detecting dataset references. For example, Zhang2006 @cite and Han2003 @cite proposed keyword extraction methods based on support vector machines (SVM).
- Kaur2010 conducted a survey on several effective keyword extraction techniques, such as selection based on informative features, position weight, and conditional random field (CRF) algorithms @cite . Extracting keywords from an article can be considered as a labeling task. CRF classifiers can assign labels to sequences of input, and, for instance, define which parts in an article can be assumed to be keywords .
- Boland2012 proposed a pattern induction method for extracting dataset references from documents in order to overcome the necessity of such a large training set @cite . Their algorithm starts with either the name of a dataset or with an abbreviation of this name, and then drives patterns of all phrases that contain that name or abbreviation in articles. The patterns are applied to articles in order to extract more dataset names and abbreviations. This process repeats with new abbreviations and names until no more datasets can be detected in articles. It derives patterns of phrases that contain dataset references iteratively by using a bootstrapping approach.
- Sketch-based algorithms were also proposed for multiple streams @cite @cite @cite . An estimation of @math , namely, the cardinality of @math , can be found using any min max sketch estimator for the cardinality estimation problem @cite . An estimation of @math can then be found using the inclusion-exclusion principle @cite . In @cite @cite @cite it is proposed to estimate the Jaccard similarity and then use it to estimate the intersection cardinality. In @cite @cite the estimators are generalized to set expressions between more than two streams.
- Although sampling techniques provide greater scalability, they also make it more difficult to infer the characteristics of the original stream. One of the first relevant works is the Good-Turing frequency estimation, a statistical technique proposed by Alan Turing and his assistant I.J. Good, for estimating the probability of encountering a hitherto unseen element in a stream, given a set of past samples. For a recent paper on the Good-Turing technique, see @cite .
- Many early works in the database literature tried to address the problem of estimating the cardinality from small samples; until the mid 1990s, this was the prevalent approach @cite . A sample of the data was collected and sophisticated estimators applied on the distributions of the values (see @cite @cite for relevant references). However, because these estimators were sensitive to the order of the elements and their repetition pattern, they failed to provide accurate estimates (see pages 19-21 in @cite ).
- In @cite @cite , the authors present an estimator for the cardinality and entropy of a stream using @math samples. Their main idea is to create a frequency histogram fingerprint'' of all sampled elements, and then run a linear program that approximates the real distribution in the full stream. However, creating a fingerprint requires exact mapping and counting of all the distinct elements in the given sample, whose length is @math . This becomes difficult in most real-world applications, as the number of distinct elements in the sample can be very large. The algorithm proposed in the present paper requires significantly less processing of only a small portion of the sampled stream.
- Various approaches for non-functional requirements have been proposed. The standard IEEE Std 830-1998 @cite considers requirements specifications in general. It concentrates strongly on functional issues and quality requirements play only a minor role. Ebert discusses in @cite an approach for managing non-functional requirements. He classifies them in and that gives them a first structure. However, this is still too coarse-grained to be applied fruitfully. Also more general approaches such as @cite @cite do not impose a sufficient structure on the quality requirements that foster elicitation or assurance. More structure is provided by the UMD approach @cite . However, it focuses mainly on that should be avoided and hence do not provide enough connection to the stakeholders. Finally, @cite included the use of quality models in their approach to non-functional requirements. However, the used quality models themselves provide no direct connection to the stakeholders.
- For unlabeled data, difficulties arise because the categorical distribution is not reparameterizable. @cite approach this by marginalizing out @math over all classes, so that for unlabeled data, inference is still on @math for each @math . The lower bound on unlabeled data is:
- The success of word embedding @cite encourages researchers to focus on machine-learned representation instead of heavy feature engineering in NLP. Using word embedding as the typical feature representation for words, NNs become competitive to traditional approaches in NER. Many NLP tasks, such as NER, chunking and part-of-speech (POS) tagging can be formulated as sequence labeling tasks. In @cite , deep convolutional neural networks (CNN) and conditional random fields (CRF) are used to infer NER labels at a sentence level, where they still use many hand-crafted features to improve performance, such as capitalization features explicitly defined based on first-letter capital, non-initial capital and so on.
- In this Section, we present related work and the novel contributions of the study presented in this paper in context to existing work. analyze research papers published in MSR (Mining Software Repositories) series of conferences from @math to @math (a period of @math years) @cite . conduct a bibliometric study consisting of mining @math papers in Requirements Engineering (RE) series of conference of @cite . analyze @math years of RE papers published from the year @math to @math . They study several aspects such as: authorship numbers and scholarly productivity of various countries or regions, interdisciplinarity, topic modeling and categorization, collaboration (university and industry, internal and external) and public and proprietary dataset @cite . conduct a research study on gender imbalance and low participation of women in Computer Science Research (CSR) @cite . They conduct several empirical and statistical analysis consisting of mining thousands of bibliometric entries in DBLP http: dblp.uni-trier.de bibliography data @cite . Their findings reveal that in the broad field of Computer Science, there is a gender balance wherein only @math 740 @math 2001 @math 2010 @math 2010 @math 2015$) through arXiv open access https: arxiv.org .
- Most work modeling 802.11 performance relies on the decoupling assumption, initially proposed by Bianchi in @cite . Bianchi proposes a model for single contention domains, using a discrete-time Markov chain. Under the decoupling assumption, the collision probability experienced by all stations is time-invariant and can be found via a fixed-point equation that depends on the parameters of the protocol. @cite examine the backoff process of 802.11 using the same assumptions and renewal theory. The authors also extract a fixed-point equation for the collision probability. The decoupling assumption has later been examined in @cite , @cite (analytically and experimentally) and found to be valid for 802.11.
- @cite study 802.11 without the decoupling assumption. They analyze an @math -dimensional chain ( @math being the number of backoff stages) that describes the number of stations at each backoff stage. capture the expected change of the number of stations at each backoff stage between two consecutive time slots, and their equilibrium point yields the average number of stations at each backoff stage in steady state. Similarly to @cite , we also use drift equations to obtain an accurate model for 1901.
- There are a few works analyzing the backoff mechanism of 1901 that rely on the decoupling assumption. First, @cite introduce a model using a discrete-time Markov chain similar to Bianchi's model for 802.11 @cite . The additional state required to capture the effect of the deferral counter @math significantly increases the complexity of the Markov chain.
- Our works of @cite , @cite propose a simplification to the model of @cite , reducing the Markov chain to a single fixed-point equation; by applying a similar theoretical technique to @cite , these papers also prove that this fixed-point equation admits a unique solution. Being less accurate but simpler than the model introduced here, the model of @cite enables us to optimize the performance of the protocol towards high throughput.
- As noted by many authors, the PDP problem generalizes the classical TSP problem and is thus NP complete for @math . Guan @cite has shown that the PDP problem is NP complete for @math and @math and Guan also shows how to solve this version in linear time if we allow temporarily dropping objects. @cite present asymptotically optimal algorithms for @math (SCP) and @math where the origins are picked i.i.d. and the destinations are picked i.i.d. from separate distributions.
- Stein @cite also conducts a probabilistic analysis but he looks at the variant where @math and @math and where @math origins and @math destinations are picked independently using a uniform distribution on some planar region. Stein shows that the value of the optimal solution divided by @math converges almost surely to a constant times the square root of the area of the region. Stein also shows how to solve the problem he considers by concatenating two TSP tours on the origins and destinations respectively. This way of solving the problem yields a solution which is roughly @math O(n^2) @math 4 @math d=2 @math c=+ @math d=2 @math c = O( n) @math d @math c$ @cite .
- Transfer learning is widely studied within computer vision such as transferring knowledge for object detection @cite @cite and segmentation @cite , however transferring from vision to other modalities are only possible recently with the emergence of high performance visual models @cite @cite . Our method builds upon teacher-student models @cite @cite and dark knowledge transfer @cite . In @cite @cite the basic idea is to compress (i.e. transfer) discriminative knowledge from a well-trained complex model to a simpler model without loosing considerable accuracy. In @cite and @cite both the teacher and the student are in the same modality, whereas in our approach the teacher operates on vision to train the student model in sound. @cite also transfer visual supervision into depth models. Our approach is broadly inspired by efforts to model cross-modal relations @cite @cite @cite @cite and works that leverage large amounts of unlabeled video @cite @cite @cite @cite @cite . In this work, we leverage the natural synchronization between vision and sound to learn a deep representation of natural sounds without ground truth sound labels.
- Regarding @math , it is a non-linear function that sets to zero all negative values after the convolutional layer. This filtering is supposed to facilitate the exploitation of discriminative information by de-noising filter detections. @cite introduced the @math as an extension of @math . It multiplies negative values with a learnable coefficient instead of setting them to zero. This method enables to filter less information at activation layer while keeping the non linear property of @math although at the cost of additional learnable parameters.
- According to a recent survey of urban sensing research @cite , local event analysis is a key area of mobile phone data analysis. Local events are usually defined as unusual gatherings or movements of massive amounts of people ( , protests, emergencies, sports, natural events, etc.) @cite @cite @cite @cite . Hence, the unit of analysis is a single event with time and space constraints. Our method, instead, works at the city level with less prescribed time and location from those above. One thing in common between our work and the cited references is that all analysis have been performed ex-post, which makes it possible to create spatio-temporal signatures of places @cite . Even though these approaches allow us to analyze and understand the city, they do not allow measurement of city-scale phenomena due to their assumptions of locality.
- Augmented Reality and Location-based Games The effects of augmented reality games and applications within the city have been anticipated for more than one decade @cite @cite . However, the limits in mainstream hardware have impeded their general implementation adoption at different times. Most studies about the impact of those games have been small-scale only @cite . Even though smartphone technology has allowed location-based augmented reality games to become more commonplace in the last few years, until the launch of Pokmon Go, they still lacked the cultural impact needed to have a considerable effect on the city. As Frank Lantz is quoted in @cite , in relation to the game PacManhattan: @cite . Since as we discussed above, Pokmon is one of the most successful media franchises in the world @cite , it enables the unique opportunity to study both, the impact of a location-based augmented reality game; and the effect of an intervention at the city scale when it comes to population mobility.
- Transfer learning is a widely known technique that was generally inspired by the ability of a human being to detect and to use previously gained knowledge in one area for efficient learning in another. In general, the definition of transfer learning was given in @cite as: There are three types of transfer learning: (1) supervised or inductive transfer learning (when labeled samples are available in target domain but there can be no labeled instances in the source one); (2) semi-supervised or transductive transfer learning (labeled samples are available only for the source learning task); (3) unsupervised transfer learning (no labeled data both in source and target learning tasks).
- The first attempt of sentiment analysis on text was initiated by who pioneered this field by using bag-of-word features. This work mostly hinged on feature engineering; since then, many kinds of feature learning methods had been introduced to increase the performance @cite @cite @cite @cite @cite . Aside from pure machine learning approaches, lexicon based approaches had been another trend, which relied on the manual or algorithmic creation of word sentiment scores @cite @cite @cite @cite .
- Since the emergence of the Convolutional Neural Networks (CNN; ), conventional methods have become gradually obsolete because of the outstanding performance from the CNN variants. CNN based models are distinguished from earlier methods because they do not rely on laborious feature engineering. The first success of CNN in sentiment analysis was triggered by document classification research @cite , where CNN showed state-of-the-art results in numerous document classification datasets. This success has engendered an upsurge in deep neural network research for sentiment analysis. Various modified models have been proposed in the literature. One of the famous deep learning methods that models a document is the generalized phrase proposed by , which represents a sentence using element-wise addition, multiplication, or recursive auto-encoder.
- Attention based methods have been successful in many application domains, such as image classification @cite , image caption generation @cite , machine translation @cite @cite @cite , and question answering @cite @cite @cite . However, in the field of sentiment analysis, the attention is applied to only aspect-based sentiment classification @cite . To the best knowledge of ours, there is no attention-based model for a general sentiment analysis task.
- There have been studies in how to mine anonymized data. @cite give a top-down specialization method (TDS) for anonymization so that the anonymized data allows accurate decision trees. @cite and @cite address the frequent itemset mining problem. proposes a clustering algorithm that handles the anonymized data as uncertain information @cite . propose a hierarchical density based clustering method using fuzzy objects @cite . discusses the problem of distance calculation for uncertain objects @cite . Nearest Neighbor classification using generalized data is investigated by Martin @cite . studies Naive Bayes using partially specified data @cite . studies decision tree classifier with random substitutions @cite . proposes a new Support Vector classification and Nearest Neighbor classification that deals with anonymized data @cite . They extend the generalization based anonymization scheme to keep all necessary statistics and use these statistics to build effective classifiers. investigate learning C4.5 decision trees from datasets that satisfy differential privacy @cite . propose a tree classifier based on random forests that is built from differentially private data @cite .
- Another approach related to mining anatomized data in the client-server database model is vertically partitioned data mining (e.g., @cite @cite ). Vertically partitioned data mining makes strong assumptions about data partitions and what data can be shared, and typically assume that each party holding data has significant computational power. For example, some decision tree techniques assume that two tuples of two partitions can be linked directly to each other, that the tuples are ordered in the same way; and that the class labels are known for both partitions. In one sense, our problem is easier, as one party (client) can see all the data. However, we assume this party (again client) does not have the resources to even store all the data, much less to build the tree.
- There are several recent attempts to study degeneration of abelian differentials from the classical viewpoint without using log structures, i.e., compactifying the strata in the Hodge bundle over @math (resp. in @math ) using twisted differentials (resp. twisted canonical divisors). Gendron ( @cite ) proved the smoothability of twisted differentials when they have zero residues at all nodes. The first author ( @cite ) studied twisted canonical divisors on curves of pseudo-compact type. Farkas and Pandharipande ( @cite ) studied systematically the space of twisted canonical divisors and showed that it is in general reducible, which contains, besides the closure of the stratum, a number of boundary components that have dimension one less than the dimension of the stratum. Finally in @cite a crucial global residue condition was found and used to isolate the closure of the stratum. As mentioned before, the moduli space @math of log twisted differentials maps onto the Farkas-Pandharipande space of twisted canonical divisors, hence it also contains extra boundary components. Nevertheless, there is a way to implant the global residue condition under the log setting to isolate the main component, which will be addressed in our future work.
- We mention that the log twisted differential considered in this paper is closely related to the strata compactification studied by Gu ' e r ' e ( @cite ), whose construction also relies on the theory of stable log maps, but is motivated from the viewpoint of Gromov-Witten theory.
- Another recent effort similar to ResearchDoom is Unreal CV @cite . This engine can also extract metadata similar to ResearchDoom and, by building on the Unreal engine, can potentially be applicable to a huge variety of modern video games. ResearchDoom is much simpler than Unreal CV as it applies to a single and relatively old game. While Doom incorporates several limitations, such as the fact that the camera can only rotate around the vertical axis, the data is nevertheless fairly complex. Furthermore, Doom provides a relatively restricted and consistent world form which data can be extracted for experiments. We leverage the latter fact to define and provide abundant pre-computed data with detailed metadata as well as with annotations using the Microsoft Coco format.
- Backward-Shifted Coding (BSC) @cite and @cite are two schemes which leverage on time redundancy mechanisms for scalable video coding. In both schemes, base layer or enhancement layers of any number of future segments of a video can be delivered in advance.
- The analysis reported in @cite provides a complete characterization of BSC's performance with respect to a set of key metrics, namely the initial buffering delay, the playback interruption, and the mean video quality, which are responsible for the users' quality of experience (QoE). Also, a cost function is proposed in order to evaluate the QoE. However, the analysis proposed in @cite does not account for the concurrent effect of bitrate adaptation. The scheme, combines video streaming and video downloading. Users are able to download optional enhancement layers for a tagged video. But, video streaming leverages on the base layer only. @cite , numerical simulations demonstrate that system renders higher average video data rate than standard streaming schemes (HAS); the effect of adaptation part is neglected.
- Robotic motor skill learning has shown considerable promise for enabling robots to autonomously learn complex motion skills @cite @cite @cite @cite . However, most successes in robotic motor skill learning have involved significant manual design of representations in order to enable policies to generalize effectively. For example, the well-known dynamic movement primitive representation @cite has been widely used to generalize learned skills by adapting the goal state, but it inherently restricts the learning process to trajectory-centric behaviors.
- Enabling robotic learning with more expressive policy classes that can represent more complex strategies has the potential of eliminating the need for the manual design of representations. Recent years have seen improvement in the generalizability of passive perception systems, in domains such as computer vision, natural language processing, and speech recognition through the use of deep learning techniques @cite . These methods combine deep neural networks with large datasets to achieve remarkable results on a diverse range of real-world tasks. However, the requirement of large labeled datasets has limited the application of such methods to robotic learning problems. While several works have extended deep learning methods to simulated @cite @cite and real-world @cite @cite robotic tasks, the kind of generalization exhibited by deep learning in passive perception domains has not yet been demonstrated for robotic skill learning. This may be due to the fact that robotic learning experiments tend to use relatively small amounts of data in constrained domains, with a few hours of experience collected from a single robot in each experiment.
- A central motivation behind our work is the ability to apply deep learning to robotic manipulation by making it feasible to collect large amounts of on-policy experience with real physical platforms. While this may seem impractical for small-scale laboratory experiments, it becomes much more realistic when we consider a possible future where robots are deployed in the real-world to perform a wide variety of skills. The challenges of asynchrony, utilization, and parallelism, which we aim to address in this work, are central for such real-world deployments. The ability of robotic systems to learn more quickly and effectively by pooling their collective experience has long been recognized in the domain of cloud robotics, where it is typically referred to as collective robotic learning @cite @cite @cite @cite . Our work therefore represents a step toward more practical and powerful collective learning with distributed, asynchronous data collection.
- Distributed systems have long been an important subject in deep learning @cite . While distributed asynchronous architectures have previously been used to optimize controllers for simulated characters @cite , our work is, to the best of our knowledge, the first to experimentally explore distributed asynchronous training of deep neural network policies for real-world robotic control. In our work, we parallelize both data collection and neural network policy training across multiple machines.
- Several methods have been proposed to improve the performance of VAE. @cite extends the variational auto-encoders to semi-supervised learning with class labels, @cite proposes a variety of attribute-conditioned deep variational auto-encoders, and demonstrates that they are capable of generating realistic faces with diverse appearance, Deep Recurrent Attentive Writer (DRAW) @cite combines spatial attention mechanism with a sequential variational auto-encoding framework that allows iterative generation of images. Considering the shortcoming of pixel-by-pixel loss, @cite replaces pixel-by-pixel loss with multi-scale structural-similarity score (MS-SSIM) and demonstrates that it can better measure human perceptual judgments of image quality. @cite proposes to enhence the objective function with discriminative regularization. Another approach @cite tries to combine VAE and generative adversarial network (GAN) @cite @cite , and use the learned feature representation in the GAN discriminator as basis for the VAE reconstruction objective.
- Several recent papers successfully generate images by optimizing perceptual loss, which is based on the high-level features extracted from pretrained deep CNNs. Neural style transfer @cite and texture synthesis @cite tries to jointly minimize high-level feature reconstruction loss and style reconstruction loss by optimization. Additionally images can be also generated by maximizing classification scores or individual features @cite @cite . Other works try to train a feed-forward network for real-time style transfer @cite @cite @cite and super-resolution @cite based on feature perceptual loss. In this paper, we train a deep convolutional variational autoencoder for image generation by replacing pixel-by-pixel reconstruction loss with feature perceptual loss based on a pre-trained deep CNN.
- Object recognition from RGB-D data traditionally relied on hand-crafted features such as SIFT @cite and spin images @cite , combined together through vector quantization in a Bag-of-Words encoding @cite . This heuristic approach has been surpassed by end-to-end feature learning architectures, able to define suitable features in a data-driven fashion @cite @cite @cite . All these methods have been designed to cope with a limited amount of training data (of the order of @math depth images), thus they are able to only partially exploit the generalization abilities of deep learning as feature extractors experienced in the computer vision community @cite @cite , where databases of @math RGB images like ImageNet @cite or Places @cite are available.
- All these works build on top of CNNs pre-trained over ImageNet, for all modal channels. Thus, the very same filters are used to extract features from all of them. As empirically successful as this might be, it is a questionable strategy, as RGB and depth images are perceptually very different, and as such they would benefit from approaches able to learn data-specific features (figure ). Our method matches this challenge, learning RGB features from RGB data and depth features from synthetically generated data, within a deep learning framework. The use of realistic synthetic data in conjunction with deep learning architectures is a promising emerging trend @cite @cite @cite . We are not aware of previous work attempting to use synthetic data to learn depth representations, with or without deep learning techniques.
- In recent years, knowledge-based topic models have been proposed, which ask human users to provide some prior domain knowledge to guide the model to produce better topics instead of purely relying on how often words co-occur in different contexts. For example, Chen and Liu encode the Must-Links (meaning that two words should be in the same topic) and Cannot-Links (meaning that two words should not be in the same topic) between words over the topic-word multinomials @cite . Besides, two recently proposed models, i.e., a quadratic regularized topic model based on semi-collapsed Gibbs sampler @cite and a Markov Random Field regularized Latent Dirichlet Allocation model based on Variational Inference @cite , share the idea of incorporate the correlation between words. All these models only deal with long texts, and perform poorly on short texts.
- The earliest works on short text topic models mainly focused on exploiting external knowledge to enrich the representation of short texts. For instance, @cite first found the related long texts for each short text, and learned topics over short texts and their related long texts using LDA. @cite learned the topics on another large-scale dataset using a conventional topic model such as PLSA and LDA for short text classification. However, these models are only effective when the additional data are closely related to the original data. Furthermore, finding such additional data may be expensive or even impossible.
- Recently, some works found that even through the assumption that each text is generated by one topic does not fit long texts, it can work well for short texts @cite @cite . Therefore, many topic models adopted this assumption to discover the latent topics in short texts. @cite empirically compared the data with traditional news media, and proposed a Twitter-LDA model by assuming that one tweet is generated from one topic. Yin and Wang @cite also adopted this assumption for topic inference based on Gibbs sampling. However, these models failed to solve the problem of very limited word co-occurrence information in short texts. Therefore, motivated by the results that prior domain knowledge is useful for long text topic models @cite @cite , we will propose a novel method for short texts by incorporating the external word correlation knowledge provided by word embeddings to improve the quality of topic modeling.
- The convex RPCA ) has been thoroughly studied @cite . To exploit the example-wise sparsity of the sparse component, @math norm has been adopted @cite @cite : where @math is defined to be the sum of @math norms of column vectors of a matrix. When a matrix has large singular values, the nuclear norm may be far from an accurate approximation of the rank. Non-convex rank approximations have been considered in a number of applications, such as subspace clustering @cite @cite . A non-convex rank approximation has also been used in RPCA @cite , which replaces the nuclear norm in ) by a non-convex rank approximation @math , with @math , and @math being the @math -th largest singular value of @math . The above approaches usually need to solve SVDs. When the matrix involved is large, the computation of SVD, in general, is intensive. To reduce the complexity of RPCA, several approaches have been attempted. For example, AlgProj @cite uses non-convex alternating minimization techniques in RPCA and admits @math complexity. @cite uses a factorization approach: which solves SVDs of thin matrices and hence admits scalability, where @math is identity matrix with proper size.
- The probabilistic idea behind EBP was extended in the BinaryConnect algorithm of . In BinaryConnect, the real-valued version of the weights is saved and used as a key reference for the binarization process. The binarization noise is independent between different weights, either by construction (by using stochastic quantization) or by assumption (a common simplification; see spang1962reduction ). The noise would have little effect on the next neuron's input because the input is a summation over many weighted neurons. Thus, the real-valued version could be updated using the back propagated error by simply ignoring the binarization noise in the update. With this method, were the first to binarize weights in CNNs and achieved near state-of-the-art performance on several datasets. They also argued that noisy weights provide a form of regularization, which could help to improve generalization, as previously shown by @cite . This method binarized weights while still maintaining full precision neurons.
- propose a framework based on a task-specific externally defined similarity metric between individuals, seeking to achieve fairness through the goal that similar people [be] treated similarly'' @cite . They strive towards individual fairness, which is a stronger notion of fairness than the definitions we use; however, our approach shares some of the underlying motivation (though not the specifics) in that our balance conditions for the positive and negative classes also reflect the notion that similar people should be treated similarly.
- An important problem is to represent or approximate a function by a sparse or finite sum of exponential terms, removing the constraints on the frequencies. This problem has a long history and many applications, in particular in signal processing @cite , @cite .
- Hankel matrices are central in the theory of Pad 'e approximants for functions of one variable. Here also a large literature exists for univariate Pad 'e approximants: see e.g. @cite for approximation properties, @cite for numerical stability problems, @cite , @cite for algorithmic aspects. The extension to multivariate functions is much less developed @cite , @cite .
- This type of approaches is also used in sparse interpolation of black box polynomials. In the methods developed in @cite , @cite , further improved in @cite , the sparse polynomial is evaluated at points of the form @math where @math are prime numbers or primitive roots of unity of co-prime order. The sparse decomposition of the black box polynomial is computed from its values by a univariate Prony-like method.
- Hankel matrices and their kernels also play an important role in error correcting codes. Reed-Solomon codes, obtained by evaluation of a polynomial at a set of points and convolution by a given polynomial, can be decoded from their syndrome sequence by computing the error locator polynomial @cite [chap. 9]. This is a linear recurrence relation between the syndrome coefficients, which corresponds to a non-zero element in the kernel of a Hankel matrix. Berlekamp @cite and Massey @cite proposed an efficient algorithm to compute such polynomials. Sakata extended the approach to compute Gr "o bner bases of polynomials in the kernel of a multivariate Hankel matrix @cite . The computation of multivariate linear recurrence relations have been further investigated, e.g. in @cite and more recently in @cite .
- A completely different approach, known as compressive sensing , has been developed over the last decades to compute sparse decompositions of functions (see e.g. @cite ). In this approach, a (large) dictionary of functions is chosen and a sparse combination with few non-zero coefficients is computed from some observations. This boils to find a sparse solution @math of an underdetermined linear system @math . Such a solution, which minimizes the @math norm'' can be computed by @math minimization, under some hypothesis.
- For the sparse reconstruction problem from a discrete set of frequencies, it is shown in @cite that the @math minimization provides a solution, for enough Fourier coefficients (at least @math ) chosen at random. As shown in @cite , this problem can also be solved by a Prony-like approach, using only @math Fourier coefficients.
- In the work of the second author with Saloff-Coste @cite , an upper bound similar to Theorem was proved. More precisely, suppose @math is a symmetric probability measure of finite second moment on @math such that @math for some @math , then by [Theorem 1.7] Saloff-Coste2014 [ H_ (n) (n ^ 1+ n )^ 1- for any >0. ] In particular, this bound implies the Liouville property of @math for @math . Our results in this paper imply a sharper upper bound @math under the same assumption, and are applicable to a larger class of decay lower bounds.
- By far our most interesting contribution that uses our new data set is vehicle counting. Most contemporary counting methods can be broadly categorized as a density estimator @cite @cite @cite or, detection instance counter @cite @cite @cite . Density estimators try to create an estimation of the density of a countable object and then integrate over that density. They tend not to require many training samples, but are usually constrained to the same scene on which it was trained. Detection counters work in the more intuitive fashion of localizing each car uniquely and then counting the localizations. This can have the downside that the entire image needs to be inspected pixel by pixel to create the localizations. Also, occlusions and overlapping objects can create a special challenge since a detector may merge overlapping objects. Another approach tries to count large crowds of people by taking a fusion over many kinds of feature counts using a Markov random field constraint @cite and seems like a synthesis of the density and detection approaches. However, it uses object-specific localizers such as a head detector so it is unclear how well it would generalize to other objects.
- Our method uses another approach. We teach a deep learning neural network to recognize the number of cars in an extended patch. It is trained only to count the number of objects as a class and is not given information about location or expected features. Then we count all the cars in a scene using a very large stride by counting them in groups at each stride location. This allows us to take one look at a large location and count by appearance. It has recently been demonstrated that one-look methods can excel at both speed and accuracy @cite for recognition and localization. The idea of using a one-look network counter to learn to count has recently been demonstrated on synthetic data patches @cite and by regression on subsampled crowd patches @cite . Here we utilize a more robust network, and demonstrate that a large strided scan can be used to quickly count a very large scene with reasonable accuracy. Additionally, we are not constrained by scene or location. Cars can be automatically counted anywhere in the world, even if they are not on roads or moving.
- Floating point representations. For nearly two decades now, SIFT @cite is very likely the most widely used local image descriptor. It and the representations that followed (e.g., SURF @cite ) represent the region around an image pixel using a vector of typically 128 floating point values. This vector is often a histogram of measurements extracted from the image, most commonly various functions of the local intensity gradients.
- One of the first binary descriptors was the Binary Robust Independent Elementary Features (BRIEF) @cite , soon followed by the Oriented fast and Rotated BRIEF (ORB) descriptor @cite which added rotation invariance, the Binary Robust Invariant Scalable Keypoints (BRISK) @cite which used a more effective pixel sampling pattern, and the Fast REtinA Keypoint descriptor (FREAK) @cite which sampled intensities using a pattern similar to the one found in human retinas. The Accelerated-KAZE (A-KAZE) was suggested in @cite . It builds on the earlier Local Difference Binary (LDB) descriptor @cite @cite by computing the binary descriptor values from mean image intensities over a range of patch sizes. The binary online learned descriptor (BOLD) @cite improve accuracy yet retain high processing speeds. Finally and very recently, the LATCH binary descriptors were proposed in @cite . We defer discussion of LATCH to Sec. .
- Hybrid binary floating-point methods were also suggested. One example is LDA-Hash @cite which extracts SIFTs, projects them to a discriminative space and applies a threshold to obtain binary descriptors. DBRIEF @cite instead uses patch intensities directly, BinBoost @cite @cite learns a set of hash functions corresponding to each bit in the final descriptor and PR-proj @cite uses learning and dimensionality reduction to produce compact binary representations. The computational effort required to extract these descriptors is similar to (if not greater than) floating point descriptors. The representations, however, are short binary vectors and so matching and storing them is relatively efficient.
- Computing local descriptors on the GPU. Of course, we are not the first to propose porting local feature extraction to the GPU. To our knowledge, nearly all these efforts used the GPU to aid in extracting floating point descriptors , including GPU-SIFT (see, e.g., @cite @cite @cite @cite ) and GPU-SURF @cite . These methods all used GPUs in portions of the extraction process. For example, @cite used the GPU only to compute convolutions, all other stages performed on the CPU. In addition, and more importantly, the gain in performance reported by these methods are modest and do not approach the speeds of contemporary binary descriptors, let alone our CLATCH.
- Deep features. Following the remarkable success of deep learning in computer vision, it is no surprise that these methods are also being applied to feature point description. Convolutional Neural Networks (CNN) were used in a number of previous attempts to learn local descriptor representations @cite @cite @cite @cite .
- In most cases, a Siamese deep network is trained with hinge loss @cite @cite @cite . The training set used in these cases consists of positive and negative labeled patch pairs. Metric learning is then used to improve matching. Finally, @cite proposed an efficient CNN design, bringing processing speeds down substantially. As we later show, their run time is still slower than our proposed approach.
- The widespread use of IEEE 802.11 infrastructure Wireless Local Area Networks (WLANs) has enabled mobile users to seamlessly transfer huge volumes of data. While IEEE 802.11 infrastructure WLANs provide mobility, and are a cheap alternative to cellular networks, they are well known to display several performance anomalies @cite @cite @cite , for example, multi-rate unfairness, uplink-downlink unfairness, hidden and exposed node problems. Thus, there is a need to study such performance anomalies, and provide better performance management solutions for these infrastructure WLANs.
- In @cite , introduce MIDAS, a management framework that addresses under-served links by throttling traffic to the interfering links. Based on the notion of Activity Share'' that they introduce, the authors propose a method to assess the potential throughput gain that the under-served link can experience, when hindering transmissions are rate-limited. MIDAS limits the rate at which traffic is sent to the interfering links, so that the under-served link's activity share improves. While MIDAS improves the air time'' of under-served links, there are no flow-level system objectives that drive the target activity share. Such flow-level system objectives are indispensable in networks with Quality-of-Service (QoS) guarantees.
- SMARTA @cite is a centralized controller that sets and dynamically adjusts the operating parameters of enterprise WLAN Access Points (e.g., channel frequency and power allocation) to optimize a predefined objective. In @cite , the authors describe several simple tests to estimate the interference environment and obtain a . Based on this conflict-graph, the controller executes channel assignment and power control algorithms that aim to minimize the number of conflicting transmissions. We note that SMARTA focuses on system configuration, and that there are no mechanisms for controlling the rate of traffic on the links. Also, SMARTA does not incorporate the capacity of the wide-area-network (WAN)-WLAN links into its design. Therefore, SMARTA may not perform efficiently in networks with WAN-WLAN traffic. Further, we would like to note that SMARTA is evaluated via simulations and not on a real testbed.
- CENTAUR @cite is motivated by the observation that when the traffic pattern is dominated by downloads, the IEEE 802.11 distributed coordination function (DCF) leads to wasted airtime in networks with hidden and exposed terminals. To tackle this, the authors in @cite propose a centralized controller for hidden and exposed terminal links; the objective is to ensure that transmissions to hidden nodes do not happen simultaneously, while transmissions to exposed nodes overlap, in time, as much as possible. Links that are not associated with hidden or exposed nodes use the IEEE 802.11 DCF to access the medium. Handling exposed terminals requires carrier sensing to be effectively disabled. To achieve this, the authors in @cite use fixed backoffs at the access points (APs) --- this requires modifications to the AP firmware. Since most firmwares are not open-source, it is difficult to incorporate such modifications.
- In our earlier work @cite , we had introduced ADWISER (vanced iFi nternet ervice nhance); a device for WLAN performance management. ADWISER is located between the WLAN and LAN (see Fig. ) so that all packets to and from the wireless STAs pass through it. ADWISER uses virtual servers and queues to pull into itself the queues from two bottleneck resources: the WLAN medium and the Internet access link. As a result, ADWISER was able to mitigate infrastructure WLAN performance anomalies, such as throughput inefficiency due to multi-rate association, unfairness between downlink and uplink TCP controlled file transfers, and throughput unfairness between intranet and Internet downloads.
- Convolutional Neural Networks (CNN) have recently advanced general object detection substantially @cite @cite @cite . A common strategy is to generate a number of object proposals by employing inexpensive low-level features, and then a strong CNN classifier is applied to further classify and refine the generated proposals. Selective Search (SS) @cite which generates class-agnostic object proposals, is one of the most popular methods applied in recent leading object detection systems, such as Region CNN (R-CNN) @cite and its extensions @cite . Recently, Ren @cite proposed a Faster R-CNN system for object detection. They proposed a Region Proposal Network (RPN) that generates high-quality class-agnostic object proposals directly from the convolutional feature maps. The RPN is fast by sharing convolutional computation. However, the RPN proposals are not discriminative, and require a further refinement and classification by an additional costly CNN model, e.g., the Fast R-CNN model @cite . More importantly, text is different significantly from general objects, making it difficult to directly apply general object detection system to this highly domain-specific task.
- A variety of computational approaches have been employed for predicting links in single layer networks, including supervised classifiers, statistical relational learning, matrix factorization, metric learning, and probabilistic graphical models (see surveys by @cite @cite @cite for a more comprehensive description). Regardless of the computational framework, topological network measures are commonly used as features to describe node pairs and can be combined in a supervised or unsupervised fashion to do link prediction. @cite . In this paper, we aggregate several of these metrics (listed in the next section), but our framework can be easily generalized to include other types of features.
- The primary focus of this paper is leveraging cross-layer information to improve link prediction in multiplex networks, although we also introduce our own single layer link prediction technique. This process of using cross-layer information can be treated as a transfer learning problem where information is learned from a source network and applied to improve prediction performance the target network. @cite introduced a transfer-based factor graph (TranFG) model which incorporates social theories into a semi supervised learning framework. This model is then used to transfer supervised information from a source network to infer social ties in the target network.
- Another strategy is to create more general versions of the topological measures that capture activity patterns in multiplex networks. @cite introduced a probabilistically weighted extension of the Adamic Adar measure for these networks. Weights are calculated by doing a triad census to estimate the probability of different link type combinations. The extended Adamic Adar metric is then used, along with other unsupervised link predictors, as input for a supervised classifier. Similarly, @cite extend the definition of network neighborhood by considering the union of neighbors across all layers. These multilayer features are then combined in a supervised model to do link prediction. One weakness with the above mentioned models is their inability to use temporal information accrued over many snapshots, rather than relying on a single previous snapshot. In this paper, we evaluate two versions of our framework, a version that only uses topological metrics calculated from one time slice vs. multiple snapshots.
- Conversely, there are a number of approaches that ignore cross-layer network dependencies, while using the history of changes between snapshots to predict future network dynamics. We have experimented with two types of techniques: time series forecasting @cite @cite and decay models @cite . Soares and Prud ^ e ncio @cite investigated the use of time series within both supervised and unsupervised link prediction frameworks. The core concept of their approach is that it is possible to predict the future values of topological metrics with time series; these values can either be used in an unsupervised fashion or combined in a supervised way with a classifier. In previous work, we introduced a rate prediction model @cite that uses time series to predict the rate of link formation. Our proposed framework, , both models the rate of link formation in each layer and uses a decay model to account for changes in the topological metrics over time. In our results, we compare the improvements achieved by temporal vs. cross-layer modeling.
- Several systems and frameworks for modeling the dynamics of knowledge and the flow of information have been developed. These systems are obviously related to reactive multi-context systems. In this section we provide a comparison with those approaches we consider most relevant, stressing differences as well as commonalities with . Note that we focus entirely on dynamic approaches and do not include systems which handle heterogeneous information in a static setting. An interesting example of the latter type are Lierler and Truszczy 'nski's abstract modular systems @cite . In a nutshell, modular systems realize the communication between different reasoning modules through joint vocabularies rather than bridge rules. These systems are best viewed as alternatives to classical MCSs. It is an open question how to adapt them to a dynamic setting.
- * LARS The Logic-based framework for Analyzing Reasoning over Streams (LARS) @cite aims at providing a formal declarative logical language for reasoning with streams. LARS is a rule-based formalism, whose language features not only provide different means to refer to or abstract from time, but also a novel window operator, thus providing a flexible mechanism to represent and reason with views on streaming data. Next, we give an overview of this language and show how to run LARS within an .
- The semantics of LARS is based on the FLP semantics of logic programs @cite . Given an input stream @math and time point @math , each LARS program @math is associated with a set of streams, the of @math for @math at @math . Since the semantics of a LARS program is defined for a fixed input data stream and for a particular time point, it is in fact mainly static.
- The idea of EVOLP is that programs can update their own rules thus describing their possible self-evolution. Each self-evolution can be represented by a sequence of programs, each program corresponding to a state, and these sequences of programs can be treated as in Dynamic Logic Programs (DLPs) @cite . Dynamic logic programs are sequences @math of generalized logic programs, whose semantics is based on the causal rejection principle. The idea is that the most recent rules are put in force, (where @math is to be seen as the most recent set of rules), and the previous rules are valid as far as possible, i.e., they are only kept if they do not conflict with more recent rules. Here, these intuitions about DLPs are sufficient, and we point to @cite for more details.
- Closely related to EVOLP are the two frameworks of Reactive ASP, one implemented as a solver @cite and one described in @cite . The system extends an ASP solver for handling external modules provided at runtime by a controller. The output of these external modules can be seen as the observations of EVOLP. Unlike the observations in EVOLP, which can be rules, external modules in are restricted to produce atoms so the evolving capabilities are very restricted. On the other hand, permits committing to a specific answer set at each state for the sake of efficiency, a feature that is not part of EVOLP. Reactive ASP as described in @cite can be seen as a more straightforward generalization of EVOLP where operations other than @math for self-updating a program are permitted. Since EVOLP can be captured by , and since permit several (evolution) operations in the head of bridge rules, it should be clear that Reactive ASP as described in @cite can be captured by .
- @cite propose a probabilistic GAN and cast it into an energy-based density estimator by using the Gibbs distribution. Quite unlike EBGAN, this proposed framework doesn't get rid of the computational challenging partition function, so the choice of the energy function is required to be integratable.
- @cite presented a Live TV study for a large IPTV service that measures user holding time (i.e. the time it takes from the moment the user starting a content to the moment he starts watching a different content). When channel surfing, they found that on average users take 10 seconds to change channel and when actually viewing a content they watch it for 10 minutes on average. They also evaluated the popularity of program categories and found that it changes throughout the day. For example, kids programs have a greater share during the morning period. @cite present a user study for an IPTV provider that offers both Live TV and VOD services. They also measure the user holding time. It was found that the holding time for VOD is significantly longer than Live TV. When channel surfing is discounted, Live TV and VOD present similar holding times. They measure the transition probabilities among Live TV, VOD and not watching TV, and found out that the users are equally likely to transition to any other mode.
- Some authors performed dataset characterizations that helped in the design of their recommendation systems. @cite presented a high-level dataset characterization and showed that on average more video content is consumed in the weekend. The average number of views for a video decreases exponentially as the time passes. The authors attribute this result to the users viewing behavior and the user interface of the Catch-up TV portal.
- Several papers studied data-structures for storing colored trees with support for various queries @cite @cite @cite @cite @cite @cite @cite . In particular, the problem of finding the nearest ancestor with color @math was considered in @cite @cite @cite @cite @cite . In order to solve the nearest colored node problem, we combine techniques from the papers above and from @cite .
- Another related problem is to find an approximate nearest node with color @math . This problem has been studied in general graphs @cite @cite @cite and planar graphs @cite @cite @cite .
- The general study of optimal strategies in XOR games was initiated by Tsirelson, who shows @cite that for any XOR game with @math and @math inputs per party there is an optimal strategy that uses a maximally entangled state of dimension at most @math , where @math is the largest integer that satisfies @math and @math . To establish this Tsirelson first proves that to each player's input in the game can be associated a real @math -dimensional unit vector, @math for Alice and @math for Bob, such that the correlations @math achieve the optimal quantum bias in the game. Tsirelson then uses a clever construction, based on a representation of the Clifford algebra, to show that these vectors can be mapped to observables and a maximally entangled state in dimension @math that achieve precisely the same correlations. Slofstra @cite shows that Tsirelson's bound is tight for a slight variant of the @math game.
- These results characterize the dimension of exactly optimal strategies in any XOR game. To the best of our knowledge, even if one considers arbitrary two-player games the @math game remains the most efficient (in terms of total number of inputs and outputs per party) test for @math -qubit maximally entangled states. In particular, although there is strong indication that certain Bell inequalities, such as the @math inequality, have a quantum bound that may only be achieved in the limit of infinite dimensions @cite , no such result has been rigorously proven. There are examples of two-player one round games which provably require infinite-dimensional entanglement in order to be played optimally @cite @cite , but these require the exchange of quantum messages between the referee and the players. Recently Slofstra @cite showed the existence of a game for which a value @math can be attained using infinite-dimensional commuting-operator strategies, but it is not known if there exists a tensor product strategy achieving this value; in particular there is no optimal entangled state'' for this game.
- The first category is non-adaptive influence maximization: we must find a set of influential customers all at once in advance subject to a budget constraint. first formalized and studied this problem under two diffusion models, namely Independent Cascade model and Linear Threshold model. study influence maximization problem under various extended models. The second category is adaptive influence maximization, which is closely related to adaptive stochastic submodular maximization @cite @cite @cite @cite . Existing studies mainly adopt full-feedback model, assuming that we can observe the full status of the previous cascade before selecting the next seed. We relax this assumption by incorporating partial-feedback and develop a novel @math -greedy adaptive policy that achieves the first bounded approximation ratio under partial-feedback model.
- @cite provide an early survey on graphs in information visualization, focusing on layout algorithms for both the general case and special subclasses (e.g., planar graphs, trees) as well as on techniques for interactive navigation, in particular focus+context and clustering. The state of the art report by von @cite offers an updated and extensive review of the field; it has a particular focus on issues and solutions for large scale graphs, but contains sections on dynamic graphs as well as interactions. @cite explore, and outline a structure of, the design space of dynamic graph visualization. @cite also review the field, discussing in particular multivariate and temporal aspects of networks. A comprehensive survey on visualizing dynamic graphs is found in @cite .
- Given the high variability in the importance of the mental map (depending on tasks, user preferences, and possibly other factors), some scholars have proposed an interactive control of the layout stability, to let the user fine-tune it @cite , @cite . According to the taxonomy of interaction by @cite , intaractive control of stability can be understood as a interaction, corresponding to the user's intent: Show me a different spatial arrangement''. It falls into the class of user-controlled adjustments of the layout, which are very common visual-level interactions for graphs @cite . @cite evaluated this stability slider in the context of a specific technique (GraphDiaries) featuring staged animations of node-link diagrams, but they did not consider it as an independent factor in the study design. @cite also evaluated a graph visualization featuring a stability slider, but without a specific focus on it.
- Besides the importance of preserving the mental map in node-link diagrams, another issue which attracts the interest of scholars is the comparison between animation approaches and timeline approaches, the latter being usually based on small multiples in a juxtaposition arrangement. The controlled experiment by Farrugia and Quigley @cite found that static drawings outperform animation in terms of task completion time. @cite , in an analogous user study, found that small multiples are generally faster, but more error-prone for certain tasks; moreover, mental map preservation has little influence on both response time and error rate. @cite also conducted a comparative evaluation of animation versus small multiples in the context of flow maps. They found that with animation there were more findings of changes in adjacent time slices, where with small multiples there were more findings about longer time periods. Moreover, they suggest that switching from one view to the other might lead to an increase in the numbers of findings; we see this consideration in analogy with the mental map case, where the introduction of a stability slider might allow the user to adapt the layout to a particular task and possibly increase the overall visualization effectiveness.
- A profound understanding of analytical tasks is a necessary prerequisite to design novel visualization techniques as well as evaluate existing ones. @cite propose a task taxonomy for dynamic graphs along three different axes: graph entities, temporal features, and properties (structural and domain-specific). According to @cite , each task can be understood as a question containing references to two dimensions and requiring an answer in the third one. In this way, they distinguish between topological tasks, temporal tasks, and behavioural tasks. Archambault and Purchase @cite structure their taxonomy along two dimension, mostly aiming at assessing the importance of the mental map. They distinguish between local and global tasks, and between distinguishable and undistinguishable tasks (depending on whether graph entities need to be distinguished from each other or can be aggregated). A task taxonomy for multivariate networks can be found in @cite .
- , discuss an approach to assess coordinated human activity on the basis of trajectories @cite . They considered basketball and developed a probabilistic method to detect key events.
- The FINA09 diving dataset was introduced in @cite . They carry out an initial investigation and conclude that temporal information, implicitly present in videos, is very important and that HOG features can be used for pose estimation. , addressed the issue of quality assessment @cite . They developed an approach in which the system is trained on an action dataset which has scores, for quality, marked by humans. Once trained, the system can assess sample and output a score for quality between 0 and 100. They tested their system on Olympic diving dataset in which the scores were given by judges.
- In an edge-based scheme, public information is associated with each pair @math where @math from which @math can be extracted with knowledge of @math . Thus, informally, we might define @math to be @math , where @math is some symmetric encryption algorithm with key @math contained in @math . An edge-based scheme can be used for arbitrary posets but requires public information @cite .
- The significant open problem with prior work on chain-based schemes is the assumption that the chain partition is part of the input to the algorithm: there may be many such partitions and it is not immediately obvious how one should select a specific partition in order to optimize characteristics of the corresponding enforcement scheme (an example being to minimize the number of secrets issued). Hence, it seems very natural to ask how difficult it is to compute a good'' chain partition, given that Our recent work @cite shows that it is possible to compute a minimal chain partition in polynomial time using a minimum cost network flow algorithm.
- * Visual Question Answering Services Researchers spanning communities as diverse as human computer interaction, machine learning, computational linguistics, and computer vision have proposed a variety of ways to answer questions about images @cite @cite @cite @cite . Yet a commonality across these communities is they adopt a one-size-fits-all approach when deciding the number of answers for any visual question. For example, crowd-powered systems aim to supply a pre-specified, fixed number of answers per visual question @cite and automated systems return a single answer for every visual question @cite @cite @cite . Inspired by the observation that there can be multiple plausible answers per visual question, we propose a richer representation of visual question answering that accounts for whether different people would agree on a single answer. We propose a system that automatically predicts whether humans will disagree. We demonstrate the predictive advantage of our system over relying on the uncertainty of a VQA algorithm in its predicted answer @cite .
- * Answer Collection from a Crowd Our work relates to methods that propose how to employ crowd workers to answer questions about images. Such approaches aim to collect a pre-specified, fixed number of answers per visual question. For those systems that treat response time as a first priority, a variable number of answers may arise but this is due to varying crowdsourcing conditions such as the available supply of workers @cite @cite . Other systems ensure a fixed number of answers are collected per visual question @cite @cite . Unlike prior work, our goal is to collect answers in a way that is both economical complete in capturing the diversity of plausible answers for all visual questions. To our knowledge, our work is the first to predict the number of answers to collect for a visual question. Experiments demonstrate that our disagreement predictions are useful to significantly reduce human effort for capturing the diversity of valid answers for 121,512 visual questions.
- * Analyses of Crowd Disagreement More broadly, our work relates to efforts to account for crowd disagreement. For example, researchers have suggested ways to resolve crowd disagreement due to task difficulty @cite and ambiguity specificity @cite @cite . Some methods demonstrate which workers to trust most when aggregating multiple responses into a final, single response @cite @cite . Other methods leverage context to automatically disambiguate which of multiple outcomes is the desired outcome @cite . Unlike prior work, we focus on the task of visual question answering. Moreover, while prior work focuses on resolving specific sources of crowd disagreement (e.g., task difficulty or ambiguity), we instead propose a single, integrated system that jointly detects various sources of crowd disagreement that arise for visual question answering. The advantage of this approach is to separate easy to answer" instances from all instances that would require additional effort to resolve the disagreement; e.g., collect multiple answers for ambiguous and subjective tasks or apply an aggregation scheme to produce a single answer from multiple answers when crowd workers are unreliable.
- * High Quality Work with Fixed Human Budget Our work aligns with methods that actively allocate a limited human budget to where it will best contribute to improving the quality of results. For example, one method distributes a budget between three different levels of human effort when deciding how to segment images @cite . Another method spends a budget between less costly crowd workers and more costly expert efforts to improve outcomes for biomedical citation screening @cite . Another method predicts when to employ algorithms versus crowd workers to segment images @cite . To our knowledge, our work is the first towards deciding how to spend a budget for the task of visual question answering, which is distinct from prior work which focused on spending a budget for image analysis or language analysis alone. Furthermore, our aim is to spend a budget to capture the of all valid results for every task rather than to collect a result for every task.
- * Minimizing Human Labeling Our aim to actively decide how to allocate human effort to improve results is also somewhat related to active learning @cite . Specifically, active learners try to use as little human effort as possible to train accurate prediction models. Some methods iteratively supplement a training dataset with the most informative images for training a classifier @cite @cite . Other methods solicit redundant labels to prevent incorrect noisy labels from teaching prediction models to make mistakes @cite @cite . While active learners aim to minimize human input to improve the accuracy of a prediction model, our method aims to minimize human input while still exhaustively capturing all plausible answers to all visual questions.
- Providing incentives to human agents to return truthful responses is one of the central challenges for crowdsourcing algorithms and applications @cite .
- Prediction markets are models with a goal of obtaining predictions about events of interest from experts. After experts provide predictions, a system assigns a reward based on a scoring rule to every expert. Proper scoring rules ensure that the highest reward is achieved by reporting the true probability distribution @cite @cite @cite . An assumption of the scoring rules is that the future outcome must be observable. This assumption prevents crowdsourcing systems to scale to large crowds as obtaining the correct answer for each event or task is prohibitively expensive.
- The model presented in @cite relaxes this assumption. The proposed scoring rule evaluates experts by comparing them to each other. The model assigns a higher score for an expert if her predictions are in agreement with predictions of other experts. Work @cite belongs to the class of peer prediction methods. Peer prediction methods is wide class of models for providing incentives @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . Such methods elicit truthful answers by analyzing the consensus between workers in one form or another. Peer prediction methods encourage cooperation between workers and, as a result, promote uninformative equilibria. The study in @cite shows that for the scoring rules proposed in the peer-prediction method @cite , a strategy that always outputs good'' or bad'' answer is a Nash equilibrium with a higher payoff than the truthful strategy. Works by @cite @cite show that the existence of such equilibria is inevitable. In contrast, hierarchical incentive schemes we propose make the truthful strategy the only Nash equilibrium.
- The model described in @cite considers a scenario of rational buyers who report on the quality of products of different types. In the developed payment mechanism the strategy of honest reporting is the only Nash equilibrium. However, the model requires that the prior distribution over product types and condition distributions of qualities is the common knowledge. This requirement is a strong assumption.
- The Bayesian Truth Serum scoring method proposed in @cite elicits truthful subjective answers on multiple choice questions. The author shows that the truthful reporting is a Nash equilibrium with the highest payoff. The model is different from other approaches in that besides the answers, workers need to provide predictions on the final distribution of answers. Workers receive a high score if their answer is surprisingly'' common - the actual percentage of their answer is larger than the predicted fraction. Similarly, incentive mechanisms in @cite @cite @cite @cite @cite require workers provide belief reports along with answers on tasks. Truthful mechanisms in @cite @cite @cite requires knowledge about the distribution from which answers are drawn. Our mechanisms do not rely on worker's beliefs on other workers' responses nor require knowledge about the global answer distribution.
- The work in @cite studies the problem of incentives for truthfulness in a setting where persons vote other persons for a position. The analysis derives a randomized approximation technique to obtain the higher voted persons. The technique is strategyproof, that is, voters (which are also candidates) cannot game the system for their own benefit. The setting of this analysis is significantly different from ours, as the limiting assumption is that the sets of voters and votees are identical. Also, the study focuses on obtaining the top- @math voted items, while in our setting we do not necessarily rank items.
- The PeerRank method proposed in @cite obtains the final grades of students using a fixed point equation similar to the PageRank method. However, while it encourages precision, it does not provide a strategyproof method for the scenario that students collude to game the system without making the effort to grade truthfully.
- Recently, understanding the mechanism of the human visual system (HVS) has been found to be useful in building the statistical assumptions by mimicking the human built-in ability of color constancy @cite @cite . One limitation of these approaches is that we are still far from fully understanding the mechanism of the HVS. In this regard, we believe our deep learning approach which train HVS inspired complex model (CNN) with large data to simulate color information processing on the human brain could make a breakthrough for the problem of the computational color constancy because we do not need to know how it works specifically.
- Combinatorial methods find the best combination of the statistics-based methods for an input image based on the scene contents. Various scene characteristics, including scene semantics @cite , indoor outdoor classification @cite , 3D geometry @cite , low-level visual properties @cite , and natural image statistics @cite are used to find the best combination. Refer to the survey paper @cite for more information.
- Direct methods build their own estimation model and estimate the illumination by learning the model from the training data. Gamut-based methods @cite @cite @cite find the canonical gamut from the training data and estimate the illumination by mapping the gamut of the input image into the canonical gamut. Distributions of the pixel intensity and the chromaticity are used as the key features for estimating the illumination in the correlation framework @cite , the neural network based method @cite , the support vector regression @cite , and the Bayesian framework @cite @cite . @cite @cite , the derivative structure and the spatial distribution of the image are used for the illumination estimation.
- Traffic phase scheduling: Design and development of traffic phase scheduling algorithms have a long history; more than 50 years @cite . Thus, there is huge literature in the area, especially on the design of optimal pre-timed policies @cite @cite @cite , which activate traffic phases according to a time-periodic pre-defined schedule. These policies do not meet expectations under changing arrival times, which require adaptive control @cite . The adaptive control mechanisms including @cite , @cite , @cite , @cite , @cite and @cite , optimize control variables, such as traffic phases, based on traffic measures, and apply them on short term.
- Our previous work @cite investigates the impact of the blocking problem at intersections, characterizes the waiting times, and develops a shortest delay routing algorithm in transportation systems. As compared to this work, in this paper, we develop a connectivity-aware traffic phase scheduling algorithm by taking into account heterogeneous communications.
- The task of camera pose estimation from line correspondences has been receiving attention for more than a quarter of century. Some of the earliest works are those by @cite and @cite . They introduce two different ways to deal with the PnL problem -- algebraic and iterative approaches -- both of which have different properties and thus also different uses. A specific subset of algebraic approaches are the methods based on linear formulation of the PnL problem.
- The iterative approaches consider pose estimation as a nonlinear least squares problem by iteratively minimizing specific error function, which usually has a geometrical meaning. In the early work of @cite , the authors attempted to estimate the camera position and orientation separately developing a method called R . Later on, @cite introduced a method called R for simultaneous estimation of camera position and orientation, and proved its superior performance to R . Recently, @cite proposed two modifications of the R algorithm exploiting the uncertainty properties of line segment endpoints. Several other iterative methods are also capable of estimation of pose parameters and line correspondences, e. ,g. @cite @cite . They pose an orthogonal approach to the common RANSAC-based correspondence filtering and consecutive separate pose estimation.
- Among the earliest efforts in this field are those of @cite and @cite . Both methods solve the minimal problem of pose estimation from 3 line correspondences in a closed form. Solutions of the P3L problem are multiple: up to 8 solutions may exist . Unfortunately, neither method is able to exploit more measurements to remove the ambiguity, and both methods are sensitive to presence of image noise.
- The second recent improvement is the Robust PnL () algorithm of @cite . Their method works with 4 or more lines and is more accurate and robust than the method of Mirzaei and Roumeliotis. An intermediate model coordinate system is used in the method of , which is aligned with a 3D line of longest projection. The lines are divided into triples, for each of which a P3L polynomial is formed. The optimal solution of the polynomial system is selected from the roots of its derivative in terms of a least squares residual.
- The RPnL algorithm was later modified by @cite into the Accurate Subset based PnL () algorithm, which acts more accurately on small line sets. However, it is very sensitive to outliers, limiting its performance on real-world data. This algorithm is compared to other state-of-the-art methods in . A drawback of both RPnL and ASPnL is that their computational time increases strongly for higher number of lines -- from 8 ,ms , ,10 lines to 630 -- 880 ,ms , ,1000 lines.
- The first DLT method for solving PnL is the method of [ ] hartley2004multiple . Following the terminology of @cite , we call the method . It does not act directly on 3D lines, but rather on 3D lying on 3D lines (for example line endpoints). It exploits the fact that if a 3D line and a 3D point coincide, their projections also must coincide. The DLT-Lines method requires at least 6 line correspondences.
- Recently, @cite developed a DLT method, which acts on 3D lines directly. The lines are parameterized using Plcker coordinates, hence the name of the method is . The method yields more accurate estimates of camera orientation than DLT-Lines at the cost of a bit larger reprojection error and slightly lower computational efficiency. Also, the minimum number of lines required is 9.
- PCA finds dimensions with maximum covariance in an unsupervised way @cite . It's mathematically defined as an orthogonal linear transformation to convert possibly correlated variables into values of linearly uncorrelated variables. The optimization problem for PCA can be summarized as follows:
- LDA tries to seek the best linear discriminant function in a supervised way @cite . Compared to PCA, LDA explicitly attempts to model the difference between the classes of data. LDA projects original data onto subspace which has sufficient discriminant power. The dimension of the subspace is restricted to be less than the number of classes. In particular, the dimension of subspace for binary classification problem is 1.
- There are several statistical tests to select features @cite , such as rank sum test @cite , t-test @cite and so on. In the paper these methods are called feature selection methods. Here we take t-test as a representative to study the performance of FS methods derived from statistical test. The t-test assesses whether the means of two groups are statistically different from each other. Normalized distance between two classes of samples can be obtained using the sample mean values: @math , @math and the sample variances @math and @math :
- In @cite and @cite , RP has been proposed to address the burdensome computation time in dimensionality reduction. In RP, transformation matrix @math is randomly generated. Experiment results @cite @cite have reveals that RP is computationally less expensive with a little distortion of original data. On the other hand, RP may not capture intrinsic structure underlying original data because @math is generated without considering the structure of data.
- There also exists some work on optimizing baseband processing power in C-RANs. By virtualization, the baseband processing resources in the BBU pool are dynamically shared among all RRHs. In @cite , a dynamic programming scheme is proposed to minimize the baseband processing power consumption in the case of dynamic cell traffic load. However, the computational complexity of the proposed scheme is exponential. In order to reduce the computational complexity, the authors in @cite propose a BBU virtualization scheme that minimizes the baseband processing power consumption with a linear computational complexity order.
- In late 2015 @cite released their work on model compression. They introduced deep compression, a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35 to 49 times without affecting their accuracy. In their method, the network is first pruned by learning only the important connections. Next, the weights are quantized to enforce weight sharing, finally, the Huffman coding is applied. After the first two steps they retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9 to 13 times; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, their method reduced the storage required by AlexNet by 35 times, from 240MB to 6.9MB, without loss of accuracy.
- In this paper, the same question is being addressed. However, we address it in an information-theoretic manner. We will show that if the function @math (as a random variable instantiated after observing the sample @math ) carries little about any individual observation @math , then the difference between @math and @math will be small with a high probability. The measure of information used here is given by the notion of @math between the random variables @math and @math , where @math is the mutual stability introduced in @cite . Variational information is an instance of the class of measures using @math -divergences, for which an axiomatic basis has been proposed .
- As a consequence of our main theorem, concentration bounds for a given learning algorithm can be immediately deduced once we recognize that the algorithm generalizes uniformly in expectation. Examples of when this holds include having (1) a finite average description length of the hypothesis, (2) a finite VC dimension of the , (3) differential privacy, (4) robust generalization, (5) typical generalization, (6) bounded mutual information, and (7) finite domains. We briefly describe these settings that have been previously studied in the literature and prove their connections to uniform generalization in Appendix . We also present connections between uniform generalization and learnability in Appendix . A second consequence of our work is establishing the equivalence between the notion of uniform generalization studied by @cite and the notion of considered more recently by @cite .
- Besides deriving a concentration bound, we also establish that our bound is tight. This tightness result is inspired by the work of @cite (Lemma 7.4) and @cite (Example 3), where similar results are established for differential privacy and learnability respectively. In Section , we combine techniques from both works to show that our concentration bound is indeed tight.
- Moreover, Convolutional Neural Network (CNN) has been applied on IQA problem recently. AlexNet @cite , a breakthrough in computer vision using CNN, greatly outperformed the state-of-the-art methods which used handcrafted features before 2012. In these years, many variations of network structures have been presented, such as GoogLeNet @cite , VGG @cite and Network-In-Network @cite . These models achieve great performance in ILSVRC, which focuses on image classification, object recognition and localization. However, the purpose of image aesthetic learning is very different from object detection. The difference between them leads to the performance gap for a model applied on distinct domains. To deal with this problem, specific models should be designed for aesthetic learning. In @cite @cite , Lu et al also notice that general architectures are not suitable for image quality classification which depends on both local and global information. They conduct experiments on network architectures by adjusting the number of layers. Eventually, they construct a CNN RAPID net @cite with four convolutional layers followed by three fully-connected layers for aesthetic learning specially. This structure is leveraged for advanced methods in this paper.
- Since a good sentiment lexicon is crucial to identifying sentiment expressions, such as SMS messages and tweets @cite , various methods have been proposed over the past few years to generate lexicons. For example, sentiment strengths can be propagated with synonymous relationships between words. The relationship can be synsets in WordNet @cite @cite , syntactic patterns in corpora @cite , and contextual information in sentences @cite . Moreover, building domain-specific lexicons @cite and leveraging crowdsourced annotations @cite have been studied in previous work. Other methods directly model sentence sentiments @cite @cite .
- Our work is also related to generating lexical databases. The most commonly used English lexical dictionary is WordNet @cite , where words, meanings, relationships are well compiled in the structured database. Similar words are clustered together as different synsets. As mentioned before, WordNet can be used for generating a sentiment dictionary. A sentiment word database called SentiWordNet @cite was developed by classifying the sentiment strength of WordNet synsets. Commonly used sentiment dictionaries also include Harvard Inquirer http: www.wjh.harvard.edu inquirer spreadsheet .htm , Micro-WNOp @cite , MPQA @cite , LIWC @cite , VADER @cite , the lexicon used in @cite https: www.cs.uic.edu liub FBS sentiment-analysis.html , etc. Other methods focus on structures and compositions of sentences @cite @cite .
- Existing efforts on identifying and measuring sentiment slang words mainly focus on specific corpus, by leveraging the context of slang words. For example, LOL'' is found to appear frequently around funny'', so LOL'' probably means causing amusement @cite . obtain a seed set of slang words from UD to help generate representation for words in Twitter @cite . These methods are limited by the completeness of the corpus, which usually generate domain-specific lexicons and require retraining on more data to cater to a new task. To the best of our knowledge, we are the first to leverage Urban Dictionary to build an extensive slang word sentiment dictionary for analyzing sentiment in short and informal user-generated content. Urban Dictionary has been used to analyze slang words. The vocabulary can help identify slang words in online reviews @cite . further identify sentiment expressions by using UD as a resource of slang word vocabulary @cite . Our work is different from theirs, since we further leverage different resources to estimate the sentiment strength for slang words from UD, instead of merely using the vocabulary.
- Deep neural networks have exhibited their great powers when the processed data own a Euclidean data structure. In many contexts, however, one may be faced with data defined in non-Euclidean domains. To tackle graph-structured data, @cite presented a spectral formulation of convolutional networks by exploiting a notion of non shift-invariant convolution that depends on the analogy between the classical Fourier transform and the Laplace-Beltrami eigenbasis. Following @cite , a localized spectral network was proposed in @cite to non-Euclidean domains by generalizing the windowed Fourier transform to manifolds to extract the local behavior of some dense intrinsic descriptor. Similarly, @cite proposed a geodesic convolution' on non-Euclidean local geodesic system of coordinates to extract local patches' on shape manifolds. The convolutions in this approach were performed by sliding a window over the shape manifolds.
- Stochastic gradient descent (SGD) has been the workhorse for optimizing deep neural networks. As an application of the chain rule, backpropagation is commonly employed to compute Euclidean gradients of objective functions, which is the key operation of SGD. Recently, the two works @cite @cite extended backpropagation directly on matrices. For example, @cite formulated matrix backpropagation as a generalized chain rule mechanism for computing derivatives of composed matrix functions with respect to matrix inputs. Besides, the other family of network optimization algorithms exploits Riemannian gradients to handle weight space symmetries in neural networks. For instance, recent works @cite @cite @cite @cite developed several optimization algorithms by building Riemannian metrics on the activity and parameter space of neural networks, treated as Riemannian manifolds.
- This work is related to hashing for multimedia retrieval, known as cross-modal hashing, which has been an increasingly popular research topic in machine learning, computer vision, and multimedia retrieval communities @cite @cite @cite @cite @cite @cite @cite @cite @cite . Please refer to @cite for a comprehensive survey.
- Previous cross-modal hashing methods can be organized into unsupervised methods and supervised methods. Unsupervised methods learn hash functions that encode input data points to binary codes only using unlabeled training data. Typical learning criteria include reconstruction error minimization @cite , neighborhood preserving in graph-based hashing @cite @cite , and quantization error minimization in correlation quantization @cite @cite . Supervised methods explore supervised information (e.g. pairwise similarity or relevance feedback) to learn compact hash codes. Typical learning criteria include metric learning @cite , neural network @cite , and correlation learning @cite @cite . As supervised methods can explore the semantic relationship to bridge modalities and reduce the semantic gap @cite , they can achieve superior accuracy than unsupervised methods for cross-modal retrieval.
- Most of previous cross-modal hashing methods based on shallow architectures cannot effectively exploit the heterogeneous relationship across different modalities. Latest deep models for multimodal embedding @cite @cite @cite @cite have shown that deep learning can bridge heterogeneous modalities more effectively for image captioning, but it remains unclear how to explore these deep models to cross-modal hashing. Recent deep hashing methods @cite @cite @cite have given state of the art results on many datasets, but they can only be used for single-modal retrieval. To the best of our knowledge, DCMH @cite is the only cross-modal deep hashing method that uses deep convolutional networks @cite for image representation and multilayer perceptrons @cite for text representation. However, DCMH can only address traditional cross-modal retrieval where heterogeneous relationship between query and database is available for hash learning, which is very restricted for real applications. To this end, we propose a novel transitive hashing network (THN) method to address cross-modal retrieval where heterogeneous relationship is not available between query and database, which leverages an auxiliary cross-modal dataset from a different domain and builds transitivity to bridge different modalities.
- Basu @cite developed an algorithm for automated shellcode generation targeting the x86 architecture. The Metasploit project provides the msfvenom utility, which can turn arbitrary x86 programs into alphanumeric x86 code. Both UPX See http: upx.sf.net . and msfvenom can generate self-decrypting ARM executables, yet neither provide alphanumeric encodings for this platform.
- Unsupervised video segmentation is usually based on motion to a certain degree. In motion segmentation, motion is the only feature for localizing objects. Ochs al @cite and Keuper @cite cluster long term point trajectories. Papazoglou and Ferrari @cite use optical flow to compute so-called inside-outside maps, partitioning the frames into foreground and background. Yang al @cite use motion to detect disoccluded areas and assign them to the correct object. The common drawback of pure motion segmentation is the need for distinct motion of the objects and the background. Lee al @cite employ object proposals @cite to enhance motion cues with a set of static features. A sequence of min-cuts generates the figure ground segments in the work of Li al @cite . Multiple paths connecting the segments are extracted and post-processed, resulting in a set of multiple possible segmentations. Dong al @cite enforce the temporal consistency of object proposals via optical flow. Wang &Wang @cite discover reoccurring objects in the video, from which they estimate a holistic model. Yang al @cite estimate the appearance and the segmentation simultaneously by adding auxiliary nodes to the Markov random field model.
- The work of Prest al @cite uses point trajectories like Ochs al @cite and Keuper @cite to identify objects. To assign class labels to the object regions, they jointly optimize over videos with the same class label. Hartmann al @cite and Tang al @cite also use the video tag to train a classifier for frame wise segments.
- In Zhang al @cite , frame-by-frame detections and segmentation proposals are combined to a temporally consistent semantic segmentation. The combination of a detector and a video segmentation approach is similar in spirit to our work, but technically, the approach is very different. We directly compare to Zhang al on the YouTube dataset. Also in the recent work of Seguin al @cite , object tracking (either manual or via detection) guides a multiple instance segmentation. However, they do not make use of motion information, thus ignoring a powerful bottom-up cue in videos.
- Whereas in the pioneering work of Viola and Jones @cite stages are distinct and essentially trained with hard negative mining, the soft cascades of Bourdev and Brandt @cite are trained as a single boosted classifier where each weak learner has a cumulative score rejection threshold. We are motivated by the general idea of stages building successively on each other and realize it in the context of DNNs. Zehnder al @cite share stages among several class-specific cascades for multi-class detection, but the stages itself are independent. In deep learning, the sharing idea of Faster R-CNN @cite comes probably the closest to our method. However, their training is less principled than ours, using a '4-step training algorithm to learn shared features via alternating optimization'. Moreover, our architecture is more flexible, as subsequent stages can also add new feature channels besides new layers.
- We also note that the term 'cascades' is overloaded in the literature. Several authors @cite @cite @cite speak of cascades to describe a sequence of stages, evaluated as whole, where one stage receives the output of another and further refines it. Our cascades aim for early rejection of negatives.
- Algorithms for solving kernel machines. Several optimization algorithms have been proposed to solve kernel machines @cite @cite . Among them, decomposition methods @cite @cite have become widely-used in software packages such as LIBSVM and SVMLight. Parallelizing kernel SVM has also been studied in the literature. Cascade-SVM @cite proposed to randomly divide the problem into subproblems, but they still need to solve a global kernel SVM problem in the final stage. Several parallel algorithms have been proposed to solve the global kernel SVM problem, such as PSVM @cite (incomplete Cholesky factorization + interior point method) and P-pack SVM @cite (distributed SGD).
- Kernel approximation approaches. Another line of approach aims to approximate the kernel matrix and then solve the reduced size problem. Nystr " o m approximation @cite and random Fourier features @cite can be used to form low-rank approximation of kernel matrices, and then the problem can be reduced to a linear SVM or logistic regression problem. To parallelize these algorithms, a distributed linear SVM or logistic regression solver is needed. The comparisons are in Figure . Using random Fourier features in kernel machine is also applied in @cite and @cite . Comparing with our proposed method, (1) they only consider solving linear systems, and it is nontrivial for them to solve kernel SVM and logistic regression problems; (2) they cannot obtain the exact solution of kernel machines, since they only use the approximate kernel.
- Distributed Block Coordinate Descent. The main idea of our algorithm is similar to a class of Distributed Block Coordinate Descent (DBCD) parallel computing methods. It was discussed recently in many literature @cite @cite @cite , where each of them have different ways to solve subproblems in each machine, and synchronize the results.
- Our Innovations. In contrast to the above algorithms, ours is the first paper that develops a block-partition based coordinate descent algorithm to kernel machines; previous work either focused on linear SVM @cite @cite @cite or primal ERM problems @cite . These approaches proposed to maintain and synchronize the vector @math or @math , where @math is the direction and @math is the data matrix. Unfortunately, in kernel methods this strategy does not work since each sample may have infinite dimension after the nonlinear mapping. We overcome the challenges by synchronizing the @math vector ( @math ) and developing an efficient line search procedure. We further show that a better partition (obtained by kmeans) can speedup the convergence of the algorithm.
- On the theoretical front: previous papers @cite @cite can only show linear convergence when the objective function is @math and @math is strongly convex. @cite considered some non-strongly convex functions, but they assume each subproblem is solved exactly. In this paper, we prove a global linear convergence even when @math is not strongly convex and each subproblem is solved approximately. Our proof covers general DBCD algorithms for some non-strongly convex functions, for which previous analysis can only show sub-linear convergence.
- There are three primary related research areas relevant to our article: mobile Clouds, Fog and Peer-to-Peer (P2P) computing, and query processing in sensor networks. @cite has grown as a research area that lies at the intersection of mobile devices such as smart phones and the Cloud. The key idea is to use these personal devices as a thin client to access rich services hosted on Clouds, forming a variation of a client-server model. In addition, the concept of Cloudlets has been proposed as an additional layer that sits between the edge and the Cloud to help reduce latencies while offering superior computing power than the edge alone @cite . This is conceptually the computing equivalent of Content Distribution Networks that move data closer to the edge. Both these paradigms conceive of interactions between a single client and a remote Cloud Cloudlet, which is in contrast to our approach of leveraging the collective capabilities of distributed edge devices and the Cloud.
- There are a plethora of research and industry products on virtual reality devices and interactions. However, an ability to provide precise and intuitive input signal is still identified as an issue among the VR users @cite . In this section, we address how prior studies approached to solve selection tasks, especially with the menu interface, and text-entry.
- Bowman @cite characterized the four universal interaction tasks of virtual environment systems: selection, navigation, manipulation, and system control. Although simple selection would be adequate for static menu layouts, delicate manipulation is needed for complex UI where buttons are smaller and packed.
- The Hoeffding bound has been used for propositional machine learning tasks on data streams, such as learning decision trees @cite and decision rules @cite , and clustering @cite . However, its usage for learning relational models is limited. One reason is that it requires independence of observations, which cannot always be ensured in relational domains, due to dependencies in the data @cite @cite @cite @cite . An ILP approach that uses the Hoeffding bound for relational learning is @cite , an extension of the system for learning first-order decision trees @cite . These are decision trees where each internal node consists of a conjunction of literals and each leaf is a propositional predicate representing a class. constructs trees by testing conjunctions of literals at each node, using an ILP refinement operator to generate the conjunctions and information gain as the guiding heuristic. extends by using the Hoeffding bound to perform these internal tests on a subset of the training data. To ensure independence of observations, learns from interpretations @cite , a setting, used also by , where each training instance is assumed a disconnected part of the dataset.
- In @cite an algorithm is presented that generates the vertices of @math for an arbitrary interval @math such that any two consecutive vertices have Hamming distance 1 or 2, where the value 2 appears only between vertices on level @math and @math , but the Hamming distance between the first and last vertex is arbitrary, possibly @math . The running time of that algorithm is @math per generated vertex. In addition, this paper presents a loopless algorithm, achieving @math time per generated vertex, to generate all vertices in @math level by level, using only distance-2 steps in each level. In particular, these enumerations are not cycles in @math , and they are not tight.
- In @cite the authors present algorithms for enumerating all vertices of @math for an arbitrary interval @math such that any two consecutive bitstrings have Levenshtein distance at most 2 and Hamming distance at most 4. The Levenshtein distance is the minimum number of bit insertions, deletions, and bitflips necessary to transform one bitstring into another. Again, these enumerations are not cycles in @math and they are not tight. However, the corresponding generation algorithms are loopless, so they are very simple and fast. Improving on this, as a byproduct of the results mentioned in the previous section we obtain a simple loopless algorithm to generate all vertices of @math for an arbitrary interval @math such that any two consecutive bitstrings have Hamming distance and Levenshtein distance at most 2.
- @cite studied throughput and average time for packet delivery in a non-cognitive network for two cooperation scenarios-- forced cooperation where best relay retransmits a failed packet and voluntary cooperation where a user may act as relay to get higher access probability in return. In our paper, due to presence of PU, relaying capability of SUs is affected by sensing errors and interference by other users. In @cite , authors modelled packet transmission process of automatic repeat request (ARQ) mechanism as Markov chain assuming that at most one retransmission per packet is allowed. We model transmission process of proposed protocols in a similar way but the signal flow graph approach employed for throughput analysis puts no restriction on number of retransmissions.
- Efficient algorithms for constructing @math -spanners were also devised in @cite @cite @cite @cite @cite . These algorithms are based, however, on different approaches than that of the current paper. The latter is based on @cite . Specifically, the approach of @cite @cite is based on @cite @cite construction of pairwise covers and hopsets, i.e., the algorithm works top-down. It recurses in small clusters, and eliminates large ones. The approach of @cite @cite @cite is based on @cite collection of trees, used originally for distance oracles. Streaming algorithms for constructing multiplicative spanners were given in @cite @cite @cite , and near-additive spanners in @cite @cite . Spanners and emulators with sublinear error were given in @cite @cite . Spanners with purely additive error and lower bounds concerning them were given in @cite @cite @cite @cite @cite @cite @cite @cite .
- Isotropic smoothing methods are the earliest smoothing algorithms. These algorithms have low complexity but suffer from severe shrinkage and further feature blurring @cite . @cite introduced an implicit smoothing algorithm that produces stable results against irregular meshes and avoids the shrinkage by volume preservation. Later, the concept of the differential coordinates was introduced by Alexa @cite as a local shape descriptor of a geometry. exploited the differential coordinates concept for mesh denoising by computing the mean of the differential coordinates at each vertex and then computes a smooth surface according to the mean differential coordinates @cite . This method produces less shrinkage but is unable to preserve shallow features. The differential coordinates framework has been extended for a variety of mesh processing algorithms by Sorkine @cite . In general, isotropic smoothing methods are prone to shrink volumes and blur features, but effective in noise removal.
- A multistage denoising framework was applied in recent methods @cite and @cite where feature identification is done by the eigenanalysis of the NVT (normal voting tensor) @cite , @cite . Then, the whole geometry is divided into different clusters based on the features and then smoothing is applied on different clusters independently. Later, the guided mesh normals were computed based on the consistent normal orientation and bilateral filtering was applied @cite . Recently, Wei et. al. @cite exploited both vertex and face normal information for feature classification and surface smoothing. In continuation, researchers detected features on the noisy surface using quadratic optimization and then remove noise using @math -optimization while preserving features @cite . Multistage denoising algorithms produce effective results against different levels of noise but have higher algorithm complexity because of the different stages. In our method, the face normal smoothing is motivated by the NVT-based algorithms. Noise and features are decoupled using the eigenanalysis of the ENVT and noise is removed by the multiplication of the ENVT to the corresponding face normal.
- Salient object detection. Salient object detection aims at detecting dominant objects in a scene. Given a test image, some methods generate a saliency map that highlights the overall region of salient objects; other methods produce bounding boxes for localization. Ideally, if a salient object detection method can well localize each salient object, then the number of objects can be simply inferred by counting the detection windows. However, many existing salient object detection methods assume the existence of salient objects, and they are mainly tested and optimized for images that contain a single dominant object . Therefore, salient object detection methods often generate undesirable results on background images, and are prone to fail on images with multiple objects and complex background. Recently, @cite proposed a salient object detection method for unconstrained images. Although this method can handle complex images to some extent, we will show that the counting-by-detection approach is less effective than our subitizing method in predicting the number of salient objects.
- Detecting the existence of salient objects. Only a few works address the problem of detecting the existence of salient objects in an image. @cite use a global feature based on several saliency maps to determine the existence of salient objects in thumbnail images. Their method assumes that an image either contains a single salient object or none. @cite use saliency histogram features to detect the existence of interesting objects for robot vision. It is worth noting that the testing images handled by the methods of @cite and @cite are substantially simplified compared to ours, and these methods cannot predict the number of salient objects.
- Droste @cite defined a model for the OneMax problem, in which during every fitness evaluation of the bit-string @math , there was exactly one uniformly-chosen random bit mis-read with probability @math . Hence the measured noisy fitness values are equivalent to replacing the Gaussian noise term in by an appropriate discrete random variable taking values from @math . Under this scheme, Droste showed that (1+1)-EA with mutation probability @math could optimise a @math -bit OneMax problem corrupted by , with high probability, in polynomial time if @math is @math .
- Variants of probabilistic graphical models have been introduced to capture the underlying data distribution. Undirected graphical models with latent variables, such as restricted Boltzmann machines (RBMs) and deep Boltzmann machines (DBMs) @cite provided the underlying discipline for pre-training deep neural networks. Deep belief networks (DBNs) @cite and its variants are hybrid models in which pre-trained DBMs and sigmoid belief networks are layer-wise mixed. DBNs can reproduce the input from multiple hidden layers, but they are restricted to a simple dataset due to the computationally costly step of Markov chain Monte Carlo (MCMC) methods.
- Similar to the DRAW, a recurrent adversarial network @cite adds generated images from multiple generators sequentially and puts the sigmoid function at the end. Adding the images based on RGB channels results in intermixing of pixels. Our model uses the additional alpha channel to avoid this issue.
- Some of the variants of GAN, such as LAPGAN @cite , DCGAN @cite , and recurrent adversarial network @cite , improved the quality of generated images. VAE GAN @cite replaced pixel-wise loss function of VAE with feature-wise loss function where the features come from the discriminator of GAN. @cite has used two GAN: the Structure-GAN generates structures; the Style-GAN puts styles on the structures.
- Driven by the off-chip bottleneck, a major research focus are lossless trace compression schemes. Program trace compression available in commercial solutions typically requires 1 to 4 bit per executed instruction @cite @cite , while solutions proposed in academia claim compression ratios down to @math bit per instruction @cite . Even though data traces contain in general no redundancy, in practice compression rates of about 4:1 have been achieved @cite .
- Visually-aware recommender systems. In recent years, there have been some works that investigate parsing or retrieving visually similar clothing images (e.g. @cite @cite @cite ), although they do not focus on learning user preferences. Recent works have introduced visually-aware recommender systems where the visual rating dimensions of users are uncovered on top of the visual signals in the system---product images. Such visual dimensions have been demonstrated to be successful at link prediction tasks useful for recommending alternative (e.g. two similar t-shirts) and complementary (e.g. a t-shirt and a matching pair of pants) items. This thread of work has also extended standard Matrix Factorization with visual dimensions to facilitate item recommendation tasks @cite @cite . A following work @cite further considers the long-term temporal dynamics associated with the visual preference of the communities, i.e., fashion evolution. However, none of above works focuses on predicting session-level user actions, a key recommendation task that requires the ability to model data.
- Modeling Temporal Dynamics. Apart from the visual domain, in the machine learning community there have also been earlier efforts that investigate temporally-evolving data with algorithms such as SVMs @cite , decision trees @cite , instance-based learning @cite , etc. Similar to ours are Collaborative Filtering models that take into account temporal dynamics. In particular, early works are based on similarity-oriented Collaborative Filtering, where a time weighting scheme can be used to assign previously-rated items with decaying weights when computing similarities (e.g. @cite ). In contrast, recent works are mostly based on the well-known Matrix Factorization technique @cite . For instance, Koren @cite showed state-of-the-art results on data using Matrix Factorization to model the underlying temporal dynamics. However, these works do not consider handling the complex semantics of visual and social signals.
- Sequential recommendation. Markov chains are a powerful tool to model stochastic transitions between different states.' In sequential recommendation domains, Markov chains have been studied by several earlier works, from investigating their strength at uncovering sequential patterns (e.g. @cite @cite ), to directly modeling decision processes with Markov transitions @cite . In a more recent paper @cite , Rendle proposed to combine the strength of Markov chains at modeling the smoothness of subsequent actions and the power of Matrix Factorization at modeling preferences for sequential recommendation. The resulting model, called FPMC, has shown superior prediction accuracy by benefiting from both simultaneously. Our work extends the basic idea mainly by modeling complex visual and social dynamics and further considering Markov chains with higher orders.
- Notably, there is another line of work that employs (personalized) probabilistic Markov embeddings to model sequential data like playlists and POIs (e.g. @cite @cite @cite ). These works differ from ours in terms of both the signals being modeled and the types of models. Moreover, none of them has shown to be able to handle datasets on the same scale with to the best of our knowledge.
- Social recommendation. In the recommender systems literature, there has been a large body of work that models social networks for mitigating cold-start issues in recommender systems, e.g. @cite @cite @cite @cite @cite . The type of social signals they usually benefit from are so-called trust' relations amongst different users, which are different from the those we are interested in. In contrast, we focus on modeling the signal, i.e., the interactions between a viewer and the creator of an item, which brings a unique set of challenges.
- There are a lot recent work on deep learning for autonomous driving or Advanced Driver Assistance Systems (ADAS) using camera sensor data as inputs, e.g., @cite @cite @cite . However, the primary purpose of these studies is not to characterize human driver's driving styles. More importantly, unlike our approach, they are technically about solving computer vision problems rather than learning from GPS records.
- A common method for predicting the output of PV systems is a model-based approach @cite @cite , which uses a model, composed of the design characteristics of the PV array and the irradiance measurements, to estimate PV generation. In order to understand this approach, and how the proposed methodology in this paper offers a significant improvement for the purpose of real-time estimation of PV generation, we briefly discuss its limiting factors. The critical input to these models is the plane-of-array (POA) irradiance. The POA irradiance on a tilted plane, @math , whose tilt is @math degrees from the horizontal plane can be calculated using Equation @cite
- The critical slowdown picture of Glauber dynamics for the 2D Ising model is by now fairly well understood. For @math , the dynamics on an @math torus has @math via the work of Martinelli and Olivieri @cite @cite and Martinelli, Olivieri and Schonmann @cite , showing that, in this regime, there is a uniform bound on the inverse gap (in fact under arbitrary boundary conditions; see [ .7] Martinelli97 ). That this dynamics has @math at any @math for some @math was shown by Chayes, Chayes and Schonmann @cite , and thereafter with the sharp @math by Cesi @cite . Finally, a polynomial upper bound on @math at @math was given in the aforementioned paper @cite ; establishing the correct (believed to be universal and approximately @math ; cf. @cite and its references) remains a challenging open problem.
- As for Swendsen--Wang, comparison estimates due to Ullrich @cite @cite imply that its inverse gap is at most that of Glauber dynamics on any graph and at any temperature (see Theorem ); thus for @math on @math it also has @math for all @math and for all @math thanks to duality, and similarly at @math it has @math .
- For all other @math , Glauber dynamics for the Potts model on @math is again known to have @math for all @math by combining the following results: Alexander @cite related exponential decay of connection probabilities in the FK model on @math to an analogous spatial mixing property in the Potts model on a finite box; Beffara and Duminil-Copin @cite proved the exponential decay of correlations in the FK model for all @math ; and the works of Martinelli @cite @cite @cite translate the aforementioned spatial mixing property to an @math bound on the inverse gap. In contrast, Potts Glauber dynamics on @math is always expected to be exponentially slow for @math : as mentioned before, this is known for @math , and was proved for large enough @math in @cite @cite .
- In the only two cases so far where the dynamical critical behavior on @math has been addressed---the case @math in @cite and the case of integer @math large enough in @cite @cite ---through the comparison inequalities of Ullrich, the results apply to all Markov chains discussed above (each has @math at @math and @math at @math large enough). Note that the results of @cite @cite are applicable to every dimension @math , while requiring that @math be sufficiently large as a function of @math .
- The dynamics for critical 2D Potts FK models under boundary conditions takes after Glauber dynamics for the low temperature Ising model under plus boundary conditions. Improving on the original work of Martinelli @cite , a delicate multi-scale analysis due to Martinelli and Toninelli @cite , based on censoring inequalities (see ), yielded an upper bound of @math for the Ising model with plus boundary conditions. This was followed by an @math bound in @cite via this approach, extended to all @math . Our proof of is based on this method.
- The first approach computes eigenpairs of the normal equations matrix @math (or @math if @math ). The eigenpairs of @math correspond to the squares of the singular values and the right singular vectors of @math , @math . If @math , the corresponding left singular vectors are obtained as @math . Although extreme Hermitian eigenvalue problems can be solved very efficiently, @math can be ill-conditioned. Assume @math is a computed singular triplet corresponding to the exact triplet @math with residual satisfying @math . Then the relative errors satisfy the relations @cite , where @math is the closest singular value to @math , @math is a constant that depends on the dimensions of @math , and @math is the machine precision. Since all singular values (except the largest @math ) will lose accuracy, this approach is typically followed by a second stage of iterative refinement. The singular triplets can be refined one by one @cite @cite @cite or collectively as an initial subspace for an iterative eigensolver @cite . This is especially needed when seeking smallest singular values.
- SVDPACK @cite and PROPACK @cite implement variants of Lanczos or LBD methods. In addition, PROPACK implements an implicitly restarted LBD method. Both methods work well for computing a few largest, well separated singular values, which in most cases is an easy problem. The computation of smallest singular triplets is not supported in SVDPACK and it is not efficient in PROPACK which only implements the Rayleigh-Ritz projection @cite . In addition, neither library can use preconditioning or support message passing parallelism, although PROPACK does support shared memory multithreading. These are severe limitations for large-scale problems that need to run on supercomputers and that often converge too slowly without preconditioning.
- SLEPc offers an LBD method with thick restarting @cite , which has similar algorithmic limitations to PROPACK for computing smallest singular values. In addition, this particular SVD solver cannot be directly used with preconditioning. However, the SLEPc LBD has an efficient parallel implementation.
- Despite accuracy limitations, eigenvalue iterative methods based on @math are widely used for computing the largest eigenpairs where the loss of accuracy is limited (see )) and even for low accuracy computations of the smallest singular values. For example, two popular packages in machine learning, scikit-learn http: scikit-learn.org stable modules generated sklearn.decomposition.TruncatedSVD.html and Spark's library MLib https: spark.apache.org docs 1.2.1 mllib-dimensionality-reduction.html , use a wrapper for the popular package ARPACK (implicit restarting Arnoldi method) @cite . Other solvers for standard Hermitian eigenvalue problems can also be used. Table lists the most widely used eigensolver libraries with high-performance computing implementations.
- @cite @cite study the concept of cake-cutting. Their model is the standard cake-cutting model in which the cake is divided among (and not among families as in our model). They define a group-envy-free division as a division in which no coalition of individuals can take the pieces allocated to another coalition with the same number of individuals and re-divide the pieces among its members such that all members are weakly better-off. Coalitions are also studied by @cite @cite . In our setting, the families are pre-determined and the agents do not form coalitions on-the-fly. In an alternative model, in which agents allowed to form coalitions based on their preferences, the family-cake-cutting problem becomes easier. For instance, it is easy to achieve a division with connected pieces between two coalitions: ask each agent to mark its median line, find the median of all medians, then divide the agents to two coalitions according to whether their median line is to the left or to the right of the median-of-medians.
- Besides land division, family preferences are important in matching markets, too. For example, when matching doctors to hospitals, usually a husband and a wife want to be matched to the same hospital. This issue poses a substantial challenge to stable-matching mechanisms @cite @cite @cite .
- For most natural language processing tasks, the conventional approach to developing a system is to use supervised learning algorithms trained on a set of annotated data. However, this approach is inappropriate for low-resource languages due to the lack of annotated data. An alternative approach is to harness different source of information aside from annotated text. Knowledge-bases such as dictionaries are one such source, which can be used to inform or constrain models, such as limiting the search space for POS tagging @cite @cite @cite .
- Parallel bilingual corpora provide another important source of information. These corpora are often plentiful even for many low-resource languages in the form of multilingual government documents, book translations, multilingual websites, etc. Word alignments can provide a bridge to project information from a resource-rich source language to a resource-poor target language. For example, parallel data has been used for named entity recognition @cite based on the observation that named entities are most often preserved in translation and also in syntactic tasks such as POS tagging @cite @cite and dependency parsing @cite . Clues from related languages can also compensate for the lack of annotated data, as we expect there to be information shared between closely related languages in terms of the lexical items, morphology and syntactic structure. Some successful applications using language relatedness information are dependency parsing @cite and POS tagging @cite . However, these approaches are limited to closely related languages such as Czech and Russian, or Telugu and Kannada, and it is unclear whether these techniques will work well in situations where parallel data only exists for less-related languages, as is often the case in practice.
- To summarize, for all these mentioned tasks, lexical resources are valuable sources of knowledge, but are also costly to build. Language relatedness information is applicable for closely related languages, but it is often the case that a given low-resource language does not have a closely-related, resource-rich language. Parallel data therefore appears to be the most realistic additional source of information for developing NLP systems for low-resource languages @cite @cite @cite , and here we primarily investigate methods to exploit parallel texts.
- Until recently the dominant paradigm in object localization and detection has been the exhaustive sliding windows'' approach (e.g., @cite ). In the last several years, more selective approaches to creating category-independent object proposals have become popular (e.g., @cite ). However, in order to make object detection more efficient and accurate, many groups have looked at how to use contextual features.
- The term context'' takes on different meanings in different papers @cite . In some cases it refers simply to co-occurrence of objects, possibly in the same vicinity. For example, @cite uses co-occurrence statistics to re-score candidate bounding boxes ( sheep'' and cow'' are more likely to co-occur in an image than "sheep" and "bus"). Several groups (e.g., @cite @cite @cite @cite @cite ) use graphical models to capture statistics related to co-occurrence well as some spatial relationships among objects in an image.
- Other approaches use Gist'' and other global image features to relate overall scene statistics or categories to object locations, sometimes combining these features with co-occurrence statistics and spatial relationships between objects (e.g., @cite @cite @cite @cite ). Still other methods learn statistical relationships between objects and features of pixel regions that surround the objects (e.g., @cite @cite ) or objects ( things'') and homogeneous image regions ( stuff'') @cite . Going further, @cite describes a method for learning joint distributions over scene categories (e.g., street,'' office,'' kitchen'') and context-specific appearance and layout of objects in those scenes.
- More recently, several groups have combined object-proposal methods such as R-CNN @cite @cite with graphical models (e.g., @cite ) or other CNN architectures (e.g., @cite ) to combine object-to-object context with whole scene context in order to improve object detection.
- The context-based methods described above are only a sample of the literature on incorporating context into object detection methods, but they give the general flavor of techniques that have been explored. Our work shares several goals with these approaches, but differs in at least four major aspects. One is our focus on images that are instances of abstract visual situations such as Dog-Walking,'' in which situational'' context from relevant objects helps very significantly in object localization, as was shown in Sec. . In contrast, the approaches described above have been designed and tested using image datasets in which the role of semantic or situational'' context is limited. This means that context of the kinds described above most often do not result in large improvements in localization accuracy @cite .
- Depth estimation is a rich field of study and we discuss only the monocular methods. A key strategy in early works for handling depth ambiguity was to use strong assumptions and prior knowledge. For example, Saxena al @cite @cite devised a multi-scale MRF, but assumed that all scenes were horizontally aligned with the ground plane. Hoiem al @cite , instead of predicting depth explicitly, estimated geometric structures for major image regions and composed simple 3D models to represent the scene.
- Once RGB-D data could be collected from laser or depth cameras on a large scale, it became feasible to apply data-driven learning-based approaches @cite @cite @cite @cite @cite . Karsch al @cite proposed a non-parametric method to transfer depth from aligned exemplars and formulated depth estimation as an optimization problem with smoothness constraints. Liu al @cite modelled image regions as super-pixels and used discrete-continuous optimization for depth estimation and later integrated mid-level region features and global scene layout @cite . Others tried to improve depth estimations by exploiting semantic labels @cite @cite @cite . With hand-crafted features, however, the inferred depth maps are coarse and only approximate the global layout of a scene. Furthermore, they lack the finer details necessary for many applications in computer vision and graphics.
- More recent methods @cite @cite @cite have the harnessed the power of pre-trained CNNs in the form of fully convolutional networks @cite . The convolutional layers from networks such as VGG @cite and ResNet @cite are fine-tuned, while the fully connected layers are re-learned from scratch to encode a spatial feature mapping of the scene. The learned map, however, is at a much lower resolution than the original input. To recover a high-resolution depth image, the feature mapping is then up-sampled @cite @cite or passed through up-convolution blocks @cite . Our network architecture follows a similar fully convolutional approach, and increases resolution via up-sampling. In addition, we add skip connections between the up-sampling blocks to better leverage intermediate outputs.
- One of the most successful yet less applied techniques for optimal designing of LQR controllers is PSO. The superiority of PSO over GA in finding optimal weighting matrices has been experimentally shown in some studies @cite @cite . In @cite PSO, GA and trial-and-error methods are utilized to adjust LQR weighting matrices which is applied to controlling an aircraft landing flare system. It is concluded that LQR design based on PSO is more efficient and robust compared to other methods. In @cite authors compared the performance of the ordinary LQR, the LQR with prescribed degree of stability (LQRPDS) and the PSO-based LQR in controlling distribution static compensator and showed that PSO-based design results in best performance under different operating conditions.
- In @cite a method is proposed to determine the weighting matrices using PSO with pole region constraint for controlling a flexible-link manipulator. In @cite it is suggested that PSO-based LQR produces better result compared to trial-and-error approach for the active suspension system. In @cite authors applied ordinary LQR, PSO-based LQR, AWPSO-based LQR and ICA-based LQR for optimal load frequency control and concluded that AWPSO-based LQR outperforms other approaches in terms of settling-time and maximum overshoot. A PSO-based optimal real-time LQR controller for stabilizing an inverted pendulum system is proposed in @cite . In @cite wavelet-PSO is proposed for tuning LQR controllers and is applied to an optimal structural control. In @cite it is concluded that contrary to trial-and-error approach, a PSO-based state feedback controller does not have sub-optimal performance in case of partial state feedback. @cite proposed a quantum-behaved PSO algorithm for tuning a given LQR controller. They exploited a conventional weighted aggregation of control objectives which can only provide the designer with one solution. More studies on PSO-based LQR design can be found in @cite @cite @cite @cite .
- They are a few studies that exploit multi-objective design for tuning LQR controllers. In @cite a multi-objective evolutionary algorithm is presented to control a double inverted pendulum system. They concluded that multi-objective approach results in shorter adjusting time and smaller amplitude value deviating from steady-state in comparison with the pole assignment design approach. @cite also applies a multi-objective evolutionary algorithm to stabilize a double inverted pendulum system. Their results suggest that multi-objective approach results in shorter adjusting time and smaller amplitude value deviating from steady-state in comparison to non-dominated sorting GA. As far as the authors' knowledge is concerned, it is the first time that a multi-objective derivative of the PSO algorithm is applied for optimal design of LQR controllers.
- Products and services traded on carding forums can generally be classified as credit card information, bank account information, credentials or online payment services @cite . In 2011, @cite analysed the records of six closed forums and identified online payments, game-related accounts, credit cards and financial accounts as being the items most traded. Stone- analysed the Spamdot forum, studying the tightly connected community of buyers and sellers that were active on it @cite . showed that cybercriminals active on such forums actively look for free samples of stolen credentials, and assess their quality before making a purchase @cite .
- Credit card information is generally divided into three groups: , , and @cite . Credit card numbers (also known as ) include at least the information printed on the card, that is, actual credit card number, cardholder name, expiration date and security code on the back of the card (not to be confused with CVV), and sometimes, the billing address and phone number. Dumps denote information from the tracks on the magnetic stripe of a card. These data are required to clone physical credit cards. Fullz provide further information on the cardholder including, for example, date of birth or social security number @cite .
- Sood and Enbody @cite provide a more detailed estimation of rates charged per credit card number. Numbers from the USA cost @math 10 on average, from Canada @math 7 and from the UK @math 8. Classified according to credit card types, a classic or standard credit card number from the USA or Canada costs @math 10, a gold card @math 20 and an Amex @math 10. Classic and Amex cards are the cheapest in the listing of Sood and Enbody @cite . Nevertheless, they are still more expensive than the lower limits of their quoted price range ( @math 5). However, it is not ideal that these rates have not been observed but estimated and it remains unclear on which basis they have been calculated.
- Reasons for price differences include the types of cards and countries of origin, as already mentioned, in addition to the rarity and the quantity of the products to be purchased @cite . Discounts on purchases of large card quantities lower the price per item. Furthermore, cards with more personal information available, with high balances and extended expiration dates and freshly acquired cards tend to be more expensive @cite .
- Generally, there are several types of participants on the forums: sellers, buyers, intermediaries, mules, administrators, and others. These roles are not mutually exclusive; sellers may simultaneously be buyers. Although the total number of participants is in unclear, @cite argue that, based on expert interviews and literature review, the total number of participants on the forums is likely to rise. The increasing spread of different marketplaces and forums would facilitate access to one of them. At least from a historic perspective, Christin @cite confirmed this growth of participants on underground platforms as he observed a linear increase of sellers during his half-year analysis of Silk Road, a large underground marketplace. In the aftermath of Silk Road's take-down in 2013, the number of sellers on competitor - and newcomer-marketplaces has substantially increased, surpassing the original number of sellers on Silk Road @cite .
- In terms of seller prolificacy, @cite analysed the records of 6 closed forums and concluded that 10 It is common ground among crime scientists that crime is distributed neither randomly nor evenly @cite . That implies a small group accounts for more offenses than its expected share would be. As earlier stated, studies on marketplaces suggest the presence of some highly prolific users. We hypothesise ( 2 ) that on active carding forums, a small number of traders are responsible for a large proportion of traffic.
- Looking at the products sold per seller, several studies found evidence of specialisation amongst sellers. Derived from literature review and expert interviews, Kraemer- @cite , for example, promote an ecosystem perspective to understand the actions of underground traders. Comparable to the legitimate business community, underground ecosystems includes actors that compete against each other, targeting competitive advantage. They try to reach this advantage by specialising in a particular type of product @cite .
- By applying a framework of social organisation, Holt @cite identified specialisation on underground forums too. While one third of sellers offered various products, two thirds focused on only one product category. As the Symantec report @cite illustrates, there are perpetrators specialising in writing viruses, in distributing malware or in monetising credit cards, for example. In recent years, Symantec has observed an increasing professionalisation in all aspects in the underground economy. Their findings are supported by research literature. Sood and Enbody @cite also identified specialisation as a trend in underground markets. They argue that these markets are increasingly accessible to people with various technical skills. Hence, there is a division of labour due to differing skills. While analysing seller characteristics on black marketplaces, Soska and Christin @cite discovered numerous specialised sellers, though there was a notable number of vendors selling different products as well.
- However, the effort to establish baseline reputation appears to be laborious. Before half of traders receive their first positive feedback, for example, they write approximately sixty posts @cite . In this case, the reputation process is intrinsically peer-driven. Sellers are dependent on recommendations by buyers. Sometimes, forum administrators provide a vetting process, often in addition to the peer-driven process and often with intransparent criteria. In those cases, entry costs are relatively high and access to higher tiers is tight @cite .
- As discussed, the efforts needed for gaining trust are extensive. A consequence might be that sellers concentrate on establishing reputation on one specific forum instead of several forums. It is therefore not expected that sellers are present on multiple forums. By assuming this, we support Motoyama' @cite expectation of non-existing multiple accounts. In contrast, we disagree with @cite who argue without providing any reasons that sellers would advertise on multiple marketplaces. We hypothesise ( 6 ), therefore, that the vast majority of actors are not operating on more than one forum.
- The related work is largely centered on the notion of neural language models @cite , which improve generalization of the classic @math -gram language models by using continuous variables to represent words in a vector space. This idea of distributed word representations has spurred many successful applications in natural language processing. More recently, the concept of distributed representations has been extended beyond pure language words to a number of applications, including modeling of phrases @cite , sentences and paragraphs @cite , relational entities @cite @cite , general text-based attributes @cite , descriptive text of images @cite , online search sessions @cite , smartphone apps @cite , and nodes in a network @cite .
- Our system is related to the RNN tracker of , which reported near state-of-the art results on the DSTC2 dataset and introduced the first incremental system which was able to update the dialogue state word-by-word with such accuracy. In contrast to work of , we use no abstraction of slot values. Instead, we add the additional features as described in . The first system which used a neural network for dialogue state tracking @cite used a feed-forward network and more than 10 manually engineered features across different levels of abstraction of the user input, including the outputs of the spoken language understanding component (SLU). In our work, we focus on simplifying the architecture, hence we used only features which were explicitly given by the dialogue history word representation and the database.
- The system of achieves the state-of-the-art results and, similarly to our system, it predicts the dialogue state from words by employing a RNN. On the other hand, their system heavily relies on the user input abstraction. Another dialogue state tracker with LSTM was used in the reinforcement setting but the authors also used information from the SLU pipeline @cite .
- It is worth noting that there are first attempts to train an end-to-end dialogue system even without explicitly modeling the dialogue state @cite , which further simplifies the architecture of a dialogue system. However, the reported end-to-end model was evaluated only on artificial dataset and cannot be compared to DSTC2 dataset directly.
- Table summarizes related embedding models for link prediction and KB completion. The models differ in the score functions @math and the algorithms used to optimize the margin-based objective function, e.g., SGD, AdaGrad @cite , AdaDelta @cite and L-BFGS @cite .
- DISTMULT @cite is based on a Bilinear model @cite @cite @cite where each relation is represented by a diagonal rather than a full matrix. The neural tensor network (NTN) model @cite uses a bilinear tensor operator to represent each relation while ProjE could be viewed as a simplified version of NTN with diagonal matrices. Similar quadratic forms are used to model entities and relations in KG2E , ComplEx , TATEC and RSTE . In addition, HolE uses circular correlation---a compositional operator---which could be interpreted as a compression of the tensor product.
- The TransH model @cite associates each relation with a relation-specific hyperplane and uses a projection vector to project entity vectors onto that hyperplane. TransD @cite and TransR CTransR @cite extend the TransH model using two projection vectors and a matrix to project entity vectors into a relation-specific space, respectively. TransD learns a relation-role specific mapping just as STransE, but represents this mapping by projection vectors rather than full matrices, as in STransE. The lppTransD model extends TransD to additionally use two projection vectors for representing each relation. In fact, our STransE model and TranSparse can be viewed as direct extensions of the TransR model, where head and tail entities are associated with their own projection matrices, rather than using the same matrix for both, as in TransR and CTransR.
- The type of search problem investigated in our work was first seen sixty years ago when Beck @cite and Bellman @cite asked an important, yet simplistic question tied to the minimization of distance. Motivated from this, several different natural search problems have been studied including the use of a fixed @cite @cite or mobile target @cite , the tools searchers have access to, the number of searchers, the communication restrictions and many more. Often, the essential part of the robot activity is the recognition and or mapping of the terrain. In the case of a known structure, the main objective of the search is to minimize the time to find the treasure. Searching for a motionless target has been studied in the cow-path problem @cite , lost in a forest problem @cite @cite and plane searching problem @cite @cite .
- Search by multiple robots with communication capabilities has been considered in @cite @cite , while @cite @cite study the evacuation of @math robots searching for an exit located on the perimeter of a disk. The problem of finding trajectories for obstacle avoidance in both known and unknown terrains has been considered in several papers including @cite @cite @cite .
- Web environment provides content publishers with a means to track user behavior in much greater detail than in an offline settings, including capturing user's registered information and activity logs of user's clicks, page views, searches, website visits, social activities, and interactions with ads. This allows for targeting of users based on their behavior, which is typically referred to as ad targeting @cite . With the rise of big data applications and platforms, machine learning approaches are heavily leveraged to automate the ad targeting process. Within the past several years, there has been a plethora of research papers that explored different aspects of online advertising, each with the goal of maximizing the benefits for advertisers, content publishers, and users.
- For any machine learning technique, features used to train a model typically have a critical influence on the performance of the deployed algorithm. Features derived from user events collected by publishers are often used in predicting the user's propensity to click or purchase @cite . However, these features represent only a weak proxy to what publishers and advertisers are actually interested in, namely, user's purchase intent. On the other hand, commercial e-mails in the form of promotions and purchase receipts convey a strong, very direct purchase intent signal that can enable advertisers to reach a high-quality audience. According to the Direct Marketing Association's National Client E-mail Report" www.powerprodirect.com statistics, accessed June 2015 , an e-mail user has been identified as having over $10 recent work @cite was among the first attempts to investigate the value of commercial e-mail data. The authors applied Sparse Principal Component Analysis (SPCA) on the counts of received e-mails in order to cluster commercial domains, demonstrating significant promise of e-mail data source.
- Outside of the e-mail domain, information about user's purchase history is extensively used by e-commerce websites to recommend relevant products to their users @cite . Recommendation systems predict which products a user will most likely be interested in either by exploiting purchase behavior of users with similar interests (referred to as collaborative filtering @cite ) or by using user's historical interaction with other products (i.e., context-based recommendation @cite ). Unlike these studies, we are not limited to the data from a single website, as purchases extracted from e-mails enable us to gather information from hundreds of different e-commerce websites that can be exploited to learn better product predictions. To the best of our knowledge, this work represents the first study that offers a comprehensive empirical analysis and evaluation of product predictors using e-mail data of such scale and nature.
- Neural language models have been proposed to address these issues, inducing low-dimensional, distributed embeddings of words by means of neural networks @cite @cite @cite . Such approaches take advantage of the word order in text documents, explicitly modeling the assumption that closer words in the word sequence are statistically more dependent. Historically, inefficient training of the neural network-based models has been an obstacle to their wider applicability, given that the vocabulary size may grow to several millions in practical tasks. However, this issue has been successfully addressed by recent advances in the field, particularly with the development of highly scalable continuous bag-of-words (CBOW) and skip-gram (SG) language models @cite @cite for learning word representations. These powerful, efficient models have shown very promising results in capturing both syntactic and semantic relationships between words in large-scale text corpora, obtaining state-of-the-art results on a plethora of NLP tasks.
- More recently, the concept of distributed representations has been extended beyond word representations to sentences and paragraphs @cite @cite , relational entities @cite @cite , general text-based attributes @cite , descriptive text of images @cite , nodes in graph structure @cite , and other applications.
- proposed two methods to utilize distributed representations for the WSI task. The first method learned centroid vectors by clustering all pre-computed context vectors of each target word. The other method simply adopted @cite and changed context vector calculation from the average of all context word vectors to weighted average. Our work has further contributions. First, we clearly point out the two advantages of sense embedding methods: 1) joint learning under the mutli-task learning framework, 2) better knowledge representation by discriminative training, and verify them by experiments. In addition, we adopt various sense embedding methods to show that sense embedding methods are generally promising for WSI, not just one method is better than other methods. Finally, we compare our methods with recent state-of-the-art WSI methods on both supervised and unsupervised metrics.
- Face detection has been a well studied area of computer vision. One of the first well performing approaches to the problem was the Viola-Jones face detector @cite . It was capable of performing real time face detection using a cascade of boosted simple Haar classifiers. The concepts of boosting and using simple features has been the basis for many different approaches @cite since the Viola-Jones face detector. These early detectors tended to work well on frontal face images but not very well on faces in different poses. As time has passed, many of these methods have been able to deal with off-angle face detection by utilizing multiple models for the various poses of the face. This increases the model size but does afford more practical uses of the methods. Some approaches have moved away from the idea of simple features but continued to use the boosted learning framework. Li and Zhang @cite used SURF cascades for general object detection but also showed good results on face detection.
- This section examines preceding academic and industrial work related to secure file sharing on clouds. @cite proposed a trusted data sharing method over untrusted cloud storage providers. This method can be described as follows:
- Yin et.al @cite proposed another efficient secure data storage scheme using Elliptic Curve Cryptography (ECC) based PKI. Before using this solution, users need to authenticate to a Certificate Authority (CA) and register for a certificate. When users want to share a file with other authenticated users, the user first encrypts the data with his symmetric key and encrypts the symmetric key with the private key of his her registered certificate. After the encryption process, authenticated users can obtain the public key and decrypt the encrypted key to get the symmetric key. The symmetric key, then, will be used to decrypt the file. The main advantage of this solution is the efficiency of preferring ECC over other algorithms like RSA. Because of the small key length, this solution requires less computation and communication cost.
- Image segmentation is often used in microscopy images that are currently segmented to obtain features such as number of cells @cite , separation of overlapped particles @cite and skull-stripping of mouse brain @cite .
- There are a number of commercial and free software available for these processing @cite ; however, these tools are focused on some images, and cannot be extended to analysis of other ones @cite @cite . Furthermore, segmentation of nontrivial images is one of the most challenging tasks in image processing: its accuracy determines the success of computational analysis procedures @cite .
- Existing work in stochastic optimal control includes jump diffusion stochastic differential equations (SDE) @cite @cite which focuses on controlling the SDEs with the jump term driven by Poisson processes not for Hawkes processes. Inspired by the opinion dynamics model proposed in @cite , the authors in @cite proposes a multivariate jump diffusion process framework for modeling opinion dynamics over networks and determining the control over such networks.
- Embedding of probability distributions in RKHS with the Mean Map has been successfully used for a large variety of tasks, such as two-sample test @cite , classification @cite or even performing algebraic operations on distributions @cite . In @cite , the estimation of a mixture model with respect to the metric of the RKHS is considered with a greedy algorithm. The proposed algorithm is however designed to approximate the target distribution by a large mixture with many components, resulting in an approximation error that decreases as the number of components increases, while our approach considers as a sparse'' combination of a fixed, limited number of components which we aim at identifying. Furthermore, unlike our method that uses RFFs to obtain an efficient algorithm, the algorithm proposed in @cite does not seem to be directly implementable.
- DeepFace @cite is one of these outstanding networks that contains a nine-layer deep CNN model with two convolutional layers and more than 120 million parameters trained on four million facial images from over 4,000 identities. This method, through alignment of images based on a 3D model and use of an ensemble of CNNs, could achieve accuracies of 97.35 FaceNet @cite is a deep CNN based on GoogLeNet @cite and the network proposed in @cite and trained on a face dataset with 100 to 200 million images of around eight million identities. This algorithm uses triplets of roughly aligned faces obtained from an online triplet mining approach and directly learns to map face images to a compact Euclidean space to measure face similarity. FaceNet has been evaluated on the LFW and YTF datasets and has achieved accuracies of 99.63
- Although the sieve is linear, the information objective that is optimized is nonlinear so the sieve substantially differs from methods like PCA. Superficially, the sieve might seem related to methods like Canonical Correlation Analysis (CCA) that seek to find a @math that makes @math and @math independent, but that method requires some set of labels, @math . One possibility would be to make @math a copy of @math , so that @math is reducing dependence between @math and a copy of itself @cite . However, this objective differs from common information as can be seen by considering the case where @math consists of independent variables. In that case the common information within @math is zero, but @math and its copy still have dependence. The concept of common information'' has largely remained restricted to information-theoretic contexts @cite @cite @cite @cite @cite . The common information in @math that is some variable, @math , is called intersection information and is also an active area of research @cite .
- Insofar as the sieve reduces the dependence in the data, it can be seen as an alternate approach to independent component analysis @cite that is more directly comparable to least dependent component analysis'' @cite . As an information theoretic learning framework, the sieve could be compared to the information bottleneck @cite , which also has an interesting Gaussian counterpart @cite . The bottleneck requires labeled data to define its objective. In contrast, the sieve relies on an unsupervised objective that fits more closely into a recent program for decomposing information in high-dimensional data @cite @cite @cite , except that work focused on discrete latent factors.
- The sieve could be viewed as a new objective for projection pursuit @cite based on common information. The sieve stands out from standard pursuit algorithms in two ways. First, an information based orthogonality'' criteria for subsequent projections naturally emerges and, second, new factors may depend on factors learned at previous layers (note that in Fig. each learned latent factor is included in the remainder information that is optimized over in the next step). More broadly, the sieve can be viewed as a new approach to unsupervised deep representation learning @cite @cite . In particular, our setup can be directly viewed as an auto-encoder with a novel objective @cite . From that point of view, it is clear that the sieve can also be directly leveraged for unsupervised density estimation @cite .
- In HyperNEAT @cite , CPPNs are used as indirect compressed encodings of the weights of a larger neural network. The inputs to the CPPN are the coordinates of the presynaptic and postsynaptic neuron, and the output is the weight joining those two neurons. If a single CPPN must encode multiple layers of a deeper neural network then there are two possibilities, either an extra input is given signaling which layer of weights the CPPN is outputting @cite , or the CPPN is constrained to always have multiple output nodes, with a specific node outputting the weight for its assigned layer @cite .
- A limitation of the CPPN approach is that weights are evolved rather than being learned by gradient-based methods that utilize backpropagation. Such methods scale better than evolutionary methods with respect to the number of parameters in the model. They are able to optimize millions of parameters at once, e.g. for convolutional neural networks for performing object classification on ImageNet @cite . CPPNs and convolutional neural networks have previously been studied with CPPNs being used to evolve adversarial examples for convolutional network classifiers on ImageNet @cite . However, in that work the CPPN is not modified by gradient descent.
- It has also been shown that even greater improvements in the compression of neural network weights should be possible - even after removing most of the weights from the filters of a trained convolutional neural network, it is possible to predict the missing weights with high accuracy @cite . This allows compression of the weights of convolutional neural networks in order to make them computationally more efficient @cite @cite . It is of interest whether the appropriate simplifying structures can be discovered rather than designed, much like how evolution stumbled upon such structure for the mammalian visual system.
- Recent work applied CPPNs in the HyperNEAT framework to evolve the weights of the 5 layer LeNet-5 convolutional neural network for MNIST character recognition @cite . Classification performance with HyperNEAT alone used to optimize the weights of this network was very poor after 2500 generations, with only 50 Previous work exists in evolving the topology of neural networks for learning. For example evolved the topology of the cells of an LSTM (Long Short Term Memory) recurrent neural network for sequence learning @cite , and more recently explored the topology of LSTMs and GRUs improving on both @cite .
- Our problem can be thought of as a special instance of cortical alignment, where the main goal becomes accurate prediction of a particular region's location, rather than finding a complete correspondence between entire brain volumes. By comparison, virtually all extant alignment methods ( @cite @cite @cite @cite @cite @cite @cite @cite ) define transformations across full cortical volumes and subsequently check whether specific regions of interest (ROIs) are accurately matched between subjects.
- A recent method incorporates functional connectivity constraints in the mapping @cite and shows improved ability to align intertwined networks in the brain (i.e. default mode network ). However, many functional areas are not usually a strong part of these networks and thus receive little benefit from this approach.
- The FIT IoT-LAB @cite is a testbed equipped with thousands of wireless nodes located in six different sites across France. It allows users to evaluate and test their novel ideas ranging from low level protocols to advanced Analytic and services in a very large scale wireless IoT environment. Major services offered by IoT-LAB include: 1) Remote access to sensors and gateways: the testbed provides users with APIs to flash any firmware, design, build, and compile applications; 2) Access to the serial ports of reserved IoT devices; 3) Internet access for nodes with end-to-end IP connection using IPv6 and 6LoWPAN; 4) Power consumption monitoring per device; 5) and robots to test and improve real-time decision making in IoT context.
- SmartSantander @cite is a project of the Future Internet Research and Experimentation initiative of the European Commission. It uniquely offers a city-scale experimental research facility with support of services of a smart city. The testbed comprises a large number of Internet of Things devices deployed in several urban locations mainly in Santander city. SmartSantander has conceived a 3-tiered architecture as follows: 1) IoT nodes: Responsible for sensing the environment parameter such as temperature and noise; 2) Repeaters: These nodes are placed between sensors and gateways, in order to behave as forwarding nodes; and 3) Gateways: IoT nodes and repeaters are configured to send all captured data via 802.15.4 protocol to gateways.
- Since traditional wireless sensor networks and IoT simulators do not focus on modeling of large scale deployments, @cite proposed a simulation methodology for IoT systems with a large number of interconnected devices. It is designed to study low-level networking aspects. In summary, the main advantages of their approach are 1) simulation of IoT systems with geographically distributed devices; 2) simulation of are IoT devices with multiple network interfaces and protocols, as well as different mobility, network, and energy consumption models.
- The OASIS standard Devices Profile for Web Services (DPWS) aims at enabling the deployment of web services on constrained devices. To accelerate the development of DPWS enabled applications, proposed DPWSim @cite , a simulation toolkit that allows developers to design, develop, and test service-based IoT applications using the DPWS technology without the presence of physical sensors and gateways.
- Probably the work that is closest to phraseNet is the recently proposed Neural Generative QA (genQA) @cite , where a set of triples are stored in a QA memory, and a neural network queries this memory for words to use in generating the answer. More specifically, phraseNet @math has the same gating strategy as in genQA. Still, phraseNet is different from that in several important ways: 1) phraseNet can handle multiple phrases in one sentence, and 2) phraseNet can generate multi-word expression.
- The softmax with multiple modes in phraseNet @math is very similar to the recently proposed CopyNet @cite . But the generative mode in CopyNet still follows a strict word-by-word fashion and therefore a soft-decision between modes has to be made for each mode. In a similar way, phraseNet is related to @cite and @cite .
- In the bottom-up merging segmentation approaches @cite @cite , it is beneficial to use contour cues learned from human annotations. Then rather than treating human segmentations as only static samples, what other stuff can we learn? In @cite , it is noticed that although human segmentations are of the high quality, there are surprisingly many boundary mistakes, even in the renown BSDS500 dataset, see Fig. .a-b. Easy to see, these mistakes are made intentionally instead of by casual operations.
- However, it is not well-investigated in @cite how to accommodate the multilevel cues in the merging criteria, not to mention the emerging dense semantic predictions. The Fully Convolutional Neural Networks (FCNN) are proposed in @cite , then widely used in semantic segmentation tasks. The FCNNs take the pixels as the input, and follow the basic structures of the standard convolutional neural networks which succeed in image classification challenges. Furthermore, the FCNNs can deal with images of arbitrary sizes, and gain flexible granularities using multiple processing streams @cite . The corresponding semantic segmentation methods can produce pixel-wise semantic labels, to say, maps of the probabilities of belonging to specific categories, see Fig. . They deal with the images without any preprocessing stages, and the results can be improved with the existing tools such as conditional random fields @cite @cite @cite . The FCNN-based methods obtain the state-of-the-art on several popular challenges such as the Pascal 2012 @cite and the MS COCO @cite .
- @cite are neural networks which directly regress the parameters of a finite parametric mixture model. When combined with a recurrent neural network this yields impressive generative models of handwritten text @cite .
- @cite and @cite perform a factorization of the output using a predefined and somewhat arbitrary ordering of output dimensions. The resulting model samples one variable at a time conditioning on the entire history of past variables. These models provide tractable likelihood evaluations and compelling results but it is unclear how to select the factorization order in many applications .
- (NCE) @cite is a method that estimates the parameters of unnormalized probabilistic models by performing non-linear logistic regression to discriminate the data from artificially generated noise. NCE can be viewed as a special case of GAN where the discriminator is constrained to a specific form that depends on the model (logistic regression classifier) and the generator (kept fixed) is providing the artificially generated noise (see supplementary material).
- The generative neural sampler models of @cite and @cite did not provide satisfactory learning methods; @cite used importance sampling and @cite expectation maximization. The main difference to GAN and to our work really is in the learning objective, which is effective and computationally inexpensive.
- As an alternative to the GAN training objective the work @cite and independently @cite considered the use of the (MMD) @cite @cite as a training objective for probabilistic models. This objective is simpler to train compared to GAN models because there is no explicitly represented variational function. However, it requires the choice of a kernel function and the reported results so far seem slightly inferior compared to GAN. MMD is a particular instance of a larger class of probability metrics @cite which all take the form @math , where the function class @math is chosen in a manner specific to the divergence. Beyond MMD other popular metrics of this form are the total variation metric (also an @math -divergence), the Wasserstein distance, and the Kolmogorov distance.
- In @cite a generalisation of the GAN objective is proposed by using an that mimics an interpolation between the KL and the reverse KL divergence and has Jensen-Shannon as its mid-point. It can be shown that with @math close to @math and @math it leads to a behavior similar the objectives resulting from the KL and reverse KL divergences (see supplementary material).
- Inverse classification can be seen as a form of sensitivity analysis, the process of examining the input features' effects on the target output. While there are many forms of sensitivity analysis @cite @cite , inverse classification is most similar to local sensitivity analysis and variable perturbation method. Later on (Section III), we propose a benchmark method that is based on these.
- Past works on inverse classification can be looked at from three perspectives: the manner in which the algorithm operates, the type of data the algorithm operates on, and the framework that guides the process of obtaining recommendations. Algorithm operation, which represents the optimization method employed, can be broken down into two groups: @cite @cite @cite @cite and @cite @cite . Greedy methods tend to focus on extreme objectives, which may not be realistic in the real world, while nongreedy methods tend to focus on more moderate objectives. This work uses the latter.
- Algorithmic data types, which refers to the type of data a particular optimization algorithm has the capability of operating on, also fall into two categories: @cite @cite @cite and @cite @cite @cite . Discrete data types lead to coarse-grained recommendations, while continuous data types provide those that are more fine-grained. In this work, we focus on the latter, as precision recommendations are the goal.
- Framework refers to the constraints that govern recommendation feasibility. These are manifested in the literature as either @cite @cite @cite or @cite @cite @cite . Unconstrained problems lead to unrealistic recommendations that may also be very extreme (e.g., reduce your age by 30 years'). Constrained frameworks lead to more moderate and realistic recommendations. However, while @cite @cite focus on moderate objectives, they do not consider (1) what can cannot be changed, (2) how hard it might be to change and (3), cumulatively, how willing someone may be to make changes. In @cite the authors consider (2), but do not consider (1) and (3). Additionally, in @cite , the formulation of relies on data points which lie exactly on the separating hyperplane; there is not guarantee that such points exist in practice. In this work we propose a framework that considers (1), (2) and (3).
- Projection of residuals on random directions @cite in SRM has been applied too in the JPEG domain and is denoted as JPSRM features. With this set of features, the authors obtained detection errors of 0.43 and 0.13 for payloads of 0.1 and 0.4 bpp respectively, with J-UNIWARD and a JPEG Quality Factor set to 0.75. DCTR features @cite have been introduced as an alternative to projection in random directions. In the features calculus, projections are achieved with DCT bases. This change of bases allows to reduce the size of the feature set while preserving its accuracy. When applied on J-UNIWARD with 0.75 JPEG Quality Factor, detection errors are 0.44 and 0.13 for payloads of 0.1 and 0.4 bpp respectively. To the best of our knowledge, the most trustworthy feature set is PHARM @cite . It is a continuation of the DCTR features in the sense that it still takes the position of the residual into the DCT grid. Compared to this previous set, PHARM only considers small prediction kernels to compute residuals. With 0.75 JPEG Quality Factor, detection errors are 0.42 and 0.12 for payloads 0.1 and 0.45 with PHARM features.
- The design of steganalyzers based on a deep learning network, a powerful machine learning technique that has become a breakthrough technology as noted in the previous section, has recently been investigated. More precisely, the use of a convolutional neural network (a deep learning approach matching exactly the underlying two-step process in classical steganalysers) to fulfill the steganalysis task, has outperformed conventional methods like the ones described in the previous paragraph. Thereafter, we will present works that have investigated the use of such networks to detect if an image embeds a secret message when using various steganographic algorithms, like HUGO @cite , WOW @cite , and S-UNIWARD @cite in @cite , or only the last one in @cite . For the experiments, both works considered databases of @math pixels grey-level images, mainly from BOSS database, but also in @cite using an homemade database called LIRMMBase, which has been obtained by mixing Columbia, Dresden, Photex, and Raise databases.
- First of all, in 2015 Qian @cite proposed a CNN consisting of 5 convolutional layers finally producing 256 features followed by three fully connected layers (for the classification part): two hidden layers of 128 ReLU (for Rectified Linear Unit) neurons each and 2 softmax neurons in the output layer (a more detailed overview of CNNs architecture is given in the next section). Besides, this CNN does not process an input image directly, but rather works on a @math high-pass filtered image issued by a @math kernel denoted @math . The experiments showed that the designed CNN was only slightly outperformed by state-of-the-art SRM+EC-based steganalyzers. In fact, even if they obtained a lower detection accuracy (a few percent larger errors: 3 Later, Pibre @cite investigated this former work further and improved the detection performance when embedding key is reused for different images. They obtained a CNN with a different shape: fewer but larger layers, able to reduce the detection error by more than 16
- As phrases have long since been used as the basic translation units in SMT, bilingual phrase embeddings attract increasing interests. Since translation equivalents share the same semantic meaning, embeddings of source target phrases can be learned with information from their counterparts. Along this line, a variety of neural models are explored: multi-layer perceptron @cite , RNN encoder-decoder @cite and recursive autoencoders @cite @cite .
- Our model combines a deep network with reinforcement learning @cite @cite @cite . Several recent works have applied these methods to multi-agent domains, such as Go @cite @cite and Atari games @cite , but they assume full visibility of the environment and lack communication. There is a rich literature on multi-agent reinforcement learning (MARL) @cite , particularly in the robotics domain @cite @cite @cite @cite @cite . Amongst fully cooperative algorithms, many approaches @cite @cite @cite avoid the need for communication by making strong assumptions about visibility of other agents and the environment. Others use communication, but with a pre-determined protocol @cite @cite @cite @cite .
- A few notable approaches involve learning to communicate between agents under partial visibility: Kasai al @cite and Varshavskaya al @cite , both use distributed tabular-RL approaches for simulated tasks. Giles & Jim @cite use an evolutionary algorithm, rather than reinforcement learning. Guestrin al @cite use a single large MDP to control a collection of agents, via a factored message passing framework where the messages are learned. In contrast to these approaches, our model uses a deep network for both agent control and communication.
- From a MARL perspective, the closest approach to ours is the concurrent work of Foerster al @cite . This also uses a deep reinforcement learning in multi-agent partially observable tasks, specifically two riddle problems (similar in spirit to our levers task) which necessitate multi-agent communication. Like our approach, the communication is learned rather than being pre-determined. However, the agents communicate in a discrete manner through their actions. This contrasts with our model where multiple continuous communication cycles are used at each time step to decide the actions of all agents. Furthermore, our approach is amenable to dynamic variation in the number of agents.
- The Neural GPU @cite has similarities to our model but differs in that a 1-D ordering on the input is assumed and it employs convolution, as opposed to the global pooling in our approach (thus permitting unstructured inputs). Our model can be regarded as an instantiation of the GNN construction of Scarselli al @cite , as expanded on by Li al @cite . In particular, in @cite , the output of the model is the fixed point of iterating equations and to convergence, using recurrent models. In @cite , these recurrence equations are unrolled a fixed number of steps and the model trained via backprop through time. In this work, we do not require the model to be recurrent, neither do we aim to reach steady state. Additionally, we regard Eqn. as a pooling operation, conceptually making our model a single feed-forward network with local connections.
- Many studies have used diverse data sources to look into human mobility patterns in the perspective of both individual behavior and collective dynamics. @cite show that human trajectories present a high degree of temporal and spatial regularity. More recently, a dichotomy in individual mobility was revealed in @cite and @cite . Those studies suggest that two mobility profiles, called returners and explorers, govern people movements based on preferential returns and explorations of new places. In relation to collective dynamics, @cite analyze large-scale collective behavior from aggregated call detail records. Their work reveal that the spatio-temporal fluctuations of individuals in a city strongly depend on the activity patterns and routines. Yet, @cite propose an approach for modeling how people move in different metropolitan areas.
- In this work we explore group meetings to enable opportunistic routing in multihop D2D networks. Previous efforts explore D2D in several ways to provide different applications. @cite propose a D2D protocol for public safety use cases, such as natural disasters with massive people concentration. The protocol includes device discovery through beacon broadcast using encryption keys to set up secure D2D communication links. Some works have shown the potential of using LTE-D2D to provide communication in infrastruture-less settings, such as in wearable networks @cite and in the internet of things @cite . Vehicular networks may also benefit from the adoption of D2D. @cite , the authors propose a traffic flow model based on a dynamic resource allocation algorithm to deal with the problem of vehicular communication, sharing the same licensed frequency spectrum as D2D communications, in a single LTE cell. @cite , the combination of LTE-D2D with MIMO is proposed to achieve low latency and better reliability in vehicular communication.
- The question of what properties can be verified using a constant verification time was studied in @cite , and several complexity classes were presented, including LD---local decision---which includes all properties that can be decided using constant number of rounds and no additional information, and NLD---non-deterministic local decision---which includes all properties that can be decided in a constant number of rounds with additional information in the form of a certificate given to each vertex. While NLD and PLS are closely related, they differ in that NLD certificates are independent of vertex identifiers. Since PLS labels may depend on vertex identifiers, there is a PLS for every sequentially decidable property on ID based networks, while not all sequentially decidable properties are in NLD. Our lower bounds in Subsections and allow labels to depend on unique vertex identifiers, so our arguments give identical lower bounds for certificate sizes in the weaker NLD model. Nonetheless, the schemes for @math in Subsections and do not require unique identifiers.
- Proof-of-work has been previously compared to Byzantine fault tolerant protocols @cite @cite . Some of this research @cite focuses on comparing experimentally Bitcoin against PBFT @cite . The Bitcoin blockchain and the PBFT consensus protocol were evaluated with nodes scattered at 8 locations around the world. As one could expect given the difficulty of the crypto puzzle of Bitcoin, the experiments showed that PBFT achieves a lower latency and a higher throughput than Bitcoin in serving transactions. However, PBFT suffers from scalability limitations and the authors recommend using sharding to avoid having to scale to hundreds of nodes.
- Another part of this research @cite discusses the probabilistic guarantees of proof-of-work systems and the deterministic guarantees of Byzantine fault tolerance. The proof-of-work consensus is compared to Byzantine agreement protocols along two axes, scalability and performance, where proof-of-work consensus protocols are considered as scalable but inefficient while Byzantine agreement protocols are considered as efficient but not scalable. For example, Bitcoin scales beyond 1000 nodes while achieving a performance lower than 100 transactions per second with a high latency, whereas standard Byzantine fault tolerant protocols achieve more than 10,000 transactions per second but scale only to tens of nodes.
- Some solutions immune to the Blockchain anomaly also exist. PeerCensus @cite was proposed as an algorithm with two components: one to execute a Byzantine agreement protocol on top of Bitcoin with a simple voting system and another to minimise the effect of Sybil attacks during these votes. The latter component makes it difficult for an attacker to create multiple identities so as to outnumber the votes with its own votes. Using this technique PeerCensus strengthens the guarantees of Bitcoin and resolves immediately the forks, hence avoiding the Blockchain anomaly.
- Tendermint http: tendermint.com . is a blockchain system building upon proof-of-stake. It is known to favour consistency over availability, taking the opposite view of Casper, the proof-of-stake alternative to the Ghost protocol. The Tendermint consensus protocol builds upon the Byzantine agreement protocol with authentication @cite and requires strictly more than two third of correct processes to ensure agreement and validity deterministically and to guarantee termination when the network stabilises and messages between non-faulty nodes get delivered.
- The problem of matrix completion is a well studied one and several solutions have been proposed during the past years, see for example @cite @cite @cite . The online setup has its roots on the so--called subspace tracking problem, e.g., @cite , in which the columns of a matrix are revealed sequentially one per iteration step and the goal is the identification of the underlying subspace. Extensions of these works, which deal with the presence of missing entries and or outliers have been studied in @cite @cite @cite @cite @cite @cite . The batch version of the matrix completion on graphs problem was originally presented in @cite and extended to its robust version, which deals with the presence of outliers, in @cite .
- Some limitations in DMTD can be addressed by replacing certain analog processing steps with digital implementations. The TIC can be dramatically redesigned @cite with much higher effective @math . One group @cite replaced the TIC by digitizing the mixed and filtered signals at @math and later eliminated the mixers with high-speed direct sampling of the input signals @cite . Others have replaced mixer-based spectral down-conversion with aliasing through under-sampling @cite . Early consideration of a direct-sampling system @cite very similar to the present work showed plausible limits due to quantization effects alone can be @math for averaging intervals @math . While high-speed samples can be processed entirely in software @cite or with custom hardware @cite , this work explores oscillator metrology using an inexpensive, commercially available, unmodified software defined radio (SDR). We note a similar approach for characterizing ADCs @cite . We employ an ADC noise cancelation technique in the time domain, which perhaps is analogous to cross-spectral analysis @cite in the frequency domain.
- A recent interest in learning to synthesize views for more challenging objects under diverse view variations has been driven by the ability of Convolutional Neural Networks (CNNs) @cite @cite to function as image decoders. Dosovitiskiy al @cite learned a CNN capable of functioning as a renderer: given an input graphics code containing identity, pose, lighting their model could render the corresponding image of a chair. Yang al @cite and Tatarchenko al @cite built on this work using the insight that the graphics code, instead of being presented explicitly, can be implicitly captured by an example source image along with the desired transformation. Yang al @cite learned a decoder to obtain implicit pose and identity units from the input source image, applied the desired transformation to the pose units, and used a decoder CNN to render the desired view. Concurrently, Tatarchenko al @cite followed a similar approach without the explicit decoupling of identity and pose to obtain similar results. A common module in these approaches is the use of a decoder CNN to generate the pixels corresponding to the transformed view from an implicit explicit graphics code. Our work demonstrates that predicting appearance flows instead of pixels leads to significant improvements.
- Currently, many studies have applied deep learning to attributes learning @cite @cite . Shankar @cite propose a deep-carving neural net to learn attributes for natural scene images. Chen @cite use a double-path deep domain adaptation network to get the fine-grained clothing attributes. Our work differs from them in the aspects of motivation and methodology. We are motivated by how to learn attributes of the human cropped from surveillance videos from a small set of data labeled with attributes. Our semi-supervised learning framework consistently boosts the discriminative power of dCNN and attributes for person ReID.
- Inspired by the promising performance of deep learning, some researchers begin to use deep learning to learn visual features and distance metrics for person ReID @cite @cite @cite @cite . In @cite , Li use a deep filter pairing neural network for person ReID, where two paired filters of two cameras are used to automatically learn optimal features. In @cite , Yi present a siamese'' convolutional network for deep distance metric learning. In @cite , Ahmed devise a deep neural network structure to transform person re-identification into a problem of binary classification, which judges whether a pair of images from two cameras is the same person. In @cite , Ding present a scalable distance learning framework based on the deep neural network with the triplet loss. Despite of their efforts to find better visual features and distance metrics, the above mentioned works are designed specifically for certain datasets and are dependent on their camera settings. Differently, we use deep learning to acquire general camera-independent mid-level representations. As a result, our algorithm shows better flexibility, , it could handle person ReID tasks on datasets containing different number of cameras.
- Some recent works also use triplet loss for person ReID @cite @cite . Our work uses attributes triplet loss for dCNN fine-tuning. This differs from the goals in these works, , learning distance metric among low-level features. Therefore, these works also suffer from the low flexility and the quadratic complexity.
- The terrain guarding problem is closely related to the well known Art Gallery Problem @cite of finding the minimum set of positions to guard a polygon. The first result was obtained by Chv 'atal: @math guards are always sufficient and sometimes necessary to guard a polygon of n vertices. Art Gallery Problem was shown to be NP-hard: on simple polygons @cite , on simple orthogonal polygons @cite , and on monotone polygons @cite . Moreover, it was shown to be APX-hard on simple polygons @cite .
- Terrain Guarding Problem for general 1.5D terrains is shown to be NP-hard by a reduction from @cite . Ben-Moshe . @cite gave the first @math -approximation algorithm. Elbassioni . @cite gave an improvement by showing that LP rounding results in a 4-approximation for TGP @math if @math (a 5-approximation otherwise). A local search based PTAS is also proposed for TGP @cite @cite .
- For orthogonal terrains, Katz and Roisman @cite gave a 2-approximation algorithm that runs in O( @math ) time, by computing a minimum clique cover in chordal graphs. Recently, Durocher . @cite studied the orthogonal terrain guarding problem under where two vertices @math are considered to see each other only if the interior of the segment @math is strictly above the terrain. Under this restricted definition, no reflex vertex of the input terrain @math can see convex vertices both on its left and right side. This property simplifies the problem, and leads to a linear time greedy exact algorithm. Under standard visibility, Durocher . @cite also observed that the hardness result for TGP in @cite does not apply for orthogonal terrains, leaving the complexity of OTGP open.
- Two of the most used algorithms in bioinformatics literature for protein property prediction are PSIPRED @cite and Jpred @cite . PSIPRED 3.2, which uses a two layer MLP approach, claims a 3-class per-position accuracy (Q @math ) score of 81.6 similar structure of a two layer MLP network. However, Jpred considers more features and uses a jury based approach with multiple models @cite . Jpred claims an 81.5 , and also predicts relative solvent accessibility. @cite uses a deep MLP architecture with multitask learning and achieves 81.7 @cite created a generative stochastic network to predict secondary structure from the same data we used, for a Q @math of 66.4 Q @math , the Q @math accuracy tries to distinguish between more classes.
- The state-of-the-art protein sequence classification system is SSpro, which obtains 91.74 different unfiltered PDB dataset @cite . However, this system exploits additional information via sequence similarity, and their reported accuracies were only 80 result in even better accuracies.
- Recently, work has also been done on the model side, particularly in natural language processing and image recognition tasks. @cite created a similar algorithm in the natural language processing domain, where they labeled word properties, such as part of speech or category of a named entity, on text data. If we consider each protein chain to be a sentence and each amino acid to be a word, the techniques transfer easily. @cite used both a windowed approach and a sentence level approach with a convolutional network, though their network was shallow and only outputed predictions for one position at a time. Long-short term memory networks have been used very successfully in sequence learning, machine translation @cite @cite and language modeling @cite . We note that machine translation is a much more general sequence to sequence task where the input and output sizes are not matched. Language modeling tries to guess future words based on past words, while protein sequences has no innate direction.
- In the image domain, @cite has beaten the state-of-the-art on image classification by a large percentage through using a deep multilayer convolutional network in the ImageNet Large-Scale Visual Recognition Challenge. Scene labeling is the task of labeling each pixel of an image with one of several classes, a 2D analogue of protein property prediction. @cite uses a recurrent neural network to obtain state-of-the-art results on scene labeling without any feature engineering. @cite designs fully convolutional networks for dense pixel prediction by running several convolutional networks on different scales. @cite increases the resolution of a bounding box based image classifier by introducing the shift-and-stitch technique, which we use on sequences instead of images and on the entire model instead of only on the last layer.
- Another line of research aims to incorporate feedback from contact sensors into manipulator control policies. This approach have been successfully used to optimize the quality of a grasp @cite or servo towards a desired contact sensor observation @cite @cite . It is also possible to directly learn a feedback policy that is robust to uncertainty @cite @cite . These approaches have achieved impressive real-time performance, but require a higher-level planner to specify the goal.
- One common approach is to plan a sequence of move-until-touch actions that localize an object, then execute an open-loop trajectory to complete the task @cite @cite @cite . Other approaches, like our own, formulate the problem as a POMDP @cite @cite and find a policy that only takes information-gathering actions when they are necessary to achieve the goal @cite @cite @cite .
- Unfortunately, most of this work assumes that the end-effector can move freely in the workspace and that objects do not significantly move when touched. More recent approaches have relaxed the latter assumption by incorporating a stochastic physics model into the POMDP @cite @cite and using SARSOP @cite , an offline point-based @cite POMDP solver, to find a near-optimal policy for manipulating an object relative to the hand. Unfortunately, hand-relative policies often fail when executed on a manipulator due to kinematic constraints or collision with obstacles. We use a hand-relative policy to guide DESPOT @cite , an online POMDP solver @cite , in Lat-POMDP, a mixed-observable model @cite that includes these constraints.
- The first noteworthy work in action recognition proposed the use of STIP (Laptev, 2005). This idea extends the Harris corner detector into the time @math domain, and improvements using expanded feature representations were proposed in @cite , @cite , and @cite . As another way of representing spatiotemporal features, @cite adopted a Gaussian mixture model to capture the frequency of each feature. In that approach method, primitive features are grouped separately when classifying activities. However, thus far, the best approach for action recognition is arguably the DT approach (, 2013), which is based on descriptions of the trajectories of tracked feature points, which are densely sampled. When obtaining these trajectories, the following spatiotemporal features are used: the trajectory histograms of oriented gradients (HOG; Dalal and Triggs, 2005), histograms of optical flow (HOF; , 2008), and the motion boundary histograms (MBH; , 2006)].
- Lai and Robbins @cite wrote one of the earliest papers on the stochastic MAB problem and provided an asymptotic lower bound of @math on the expected regret for any bandit algorithm. In @cite , sample-mean based upper confidence bound (UCB) policies are presented that achieve the logarithmic regret asymptotically. In @cite , several variants of the UCB based policies, including UCB1, are presented and are proved to achieve the logarithmic regret bound uniformly over time for arm-distributions with finite support.
- Recently, contextual bandits with budget and time constraints have been studied in @cite @cite . Resourceful contextual bandits from @cite consider a general setting with random costs associated with the arms, a continuous context-space and multiple budget constraints. An algorithm called Mixture that achieves @math regret is proposed for this problem. A simplified model with fixed costs, discrete and finite contexts, exactly one time constraint and one budget constraint is considered in @cite . For this model, an algorithm called UCB achieving @math regret is proposed. In these problem formulations, time and budget constraints also affect the regular exploration-exploitation trade-off.
- Work by Donalek et. al. @cite uses uses Unity3D and Oculus VR headsets to visualize astronomical datasets, but has key differences in terms of capabilities and constraints. The rendering in @cite occurs on a personal computer as opposed to a mobile device, and therefore is less resource bound than in our context. Furthermore, the underlying system that we build upon is designed for collaborative full-room VR, allowing users to interact with a social network in much the same way they would a real object.
- With , Heer et. al. @cite provided a precedent for social network visualization using force layouts. We provide much of the same basic functionality in VR, but at a much larger scale. Although not the focus of this paper, a Three.js webGL frontend that in many respects resembles a 3D version of Vister is additionally provided by the analysis server (discussed below). One contrast between our work and Vister is that the latter focuses on active layouts, while our layouts are computed ahead of time, due to the larger network size.
- The work of Munzner @cite offers a comprehensive exposition of large network visualization, covering many types of layout techniques.
- For simple random walks, the range of possible values of the cover time is well understood. Aleliunas al @cite showed that for any connected graph, @math . That work also introduced the spanning tree argument that is used also in establishing other upper bounds cited below, and in fact provides upper bounds on @math and not just on @math . For regular graphs the upper bound can be improved to @math , as shown by Kahn al in @cite . A more refined connection between regularity and cyclic cover time was provided by Coppersmith al @cite who proved that for any connected graph @math ,
- Action recognition has been widely studied in recent years. Early approaches extracted local spatio-time descriptors from input video and encoded these descriptors with Bag of Visual Words or its variants for classification. Laptev @cite proposed spatio-time interest points by extending Harris corner into spatio-time dimension. Wang et al. @cite further exploited trajectorires to model temporal relationship of continuous frames. Furthermore, Kviatkovsky et al. @cite proposed a covariance descriptor to realize online action recognition. Popular local descriptors for video representation include HOG @cite , HOF @cite , MBH @cite and TBC @cite . And feature encoding techniques include hard quantization @cite , VLAD @cite , and Fisher Vector @cite . @cite @cite @cite exploited mid-level representations by proposing MoFAP and Motionlets.
- A large number of specific protocols have been proposed for efficient routing in mobile networks @cite . These algorithms are based on the available topological knowledge of the network communication graph, or sometimes on the relative coordinates of each node. For the latter set of geometric" routing algorithms, it has been shown that greedy (i.e., local) forwarding strategies may lead into dead-ends, while the optimal delivery strategy has only been guaranteed for the static case @cite because it involves a preprocessing stage (see also @cite @cite @cite @cite @cite for further developments in this area). Considering only topology-based routing algorithms, reactive protocols similar to DSR @cite and AODV @cite make sense in a highly mobile environment. An alternative approach FRESH @cite takes mobility into account, but, arguably for the worse, it views mobility as a resource rather than a handicap. There also exist hybrid protocols, such as IZR @cite . Nevertheless, none of these protocols take into account both a potential topological change at every single iteration, and simultaneously a worst-case perspective.
- In addition, there have long been controversies on predictive power of social media aiming at different fields @cite @cite . In the field of finance, found that public mood on Twitter could predict the Dow Jones Industrial Average @cite . The public mood dimensions of Calm and Happiness seemed to have a predictive effect. However, the tweets they collected were associated with whole social status, not just the stock market in America, which could not represent online investors' sentiment. also showed stock micro-blog sentiments did have predictive power for market-adjusted returns. Instead of emotion on social media @cite , some researchers examined textual representations in financial news articles for stock prediction @cite @cite . proposed a deep learning method for event-driven stock market prediction on large-scale financial news dataset @cite . Besides, showed that daily trading volumes of stocks traded in NASDAQ100 were correlated with daily volumes of queries related to the same stocks @cite .
- Another related area is that of kernels for structured representations, which allow the application of techniques such as Support Vector Machines to structured data. Typically, kernels for graphs are based on the idea of finding common substructures between two graphs. For example, @cite present a kernel for graphs based on random walks. @cite also studied how to encapsulate their similarity measure for Description Logics into a kernel. For a survey on kernels for structured data the reader is referred to @cite .
- Similarity measures for specific application domains, such as molecular structures in domains of biology or chemistry have also been studied @cite , and they are typically grouped into three classes @cite : sequence similarities (e.g., for DNA fragments), fingerprint-based (transform each molecule into a sequence of binary features representing whether a particular molecule exhibits particular properties or contains certain substructures) and graph-based (based on maximum common sub-graphs). The latter is a computationally expensive process, and thus there are a number of strategies to simplify the computations (e.g., @cite ).
- A more recent paper @cite in 2015 focused on comparing 13 hand pose estimation algorithms from a single depth frame and evaluated their performance on various publicly available dataset. Furthermore, @cite created a new hand training dataset that is more diverse and complex to existing one. @cite focus primarily on comparing the quality of each of the 13 algorithms using a common training dataset, this paper focus on reviewing the latest state-of-the-art hand pose estimation algorithms.
- Nuclear norm minimization-based matrix completion @cite is formulated as where @math is the optimal estimation of @math . The truncated nuclear norm minimization problem @cite is defined as: where @math is the sum of @math minimum singular values. It is widely known that the TV norm @cite is an efficient smoothness regularization that accumulates all the gradients of a given image @math : where @math and @math denote the vertical and horizontal positions of @math , respectively. Since the TV norm accumulates gradient modules of the entire image, minimizing the TV norm can result in a smooth estimation:
- Since the @math is isotropic and not differentiable, an anisotropic version is proposed in @cite that is easier to minimize:
- Additionally, in @cite , a modified linear total variation was defined as: which leads to a smooth low-rank matrix completion problem:
- An ADMM-like optimization scheme @cite can be adopted to solve Fcn.. However, the traditional TV norm only can guarantee an estimation presenting a locally smooth visualization, when in reality a natural image should be smoothed at every scale. In the next section, we propose a multi-scale DCT norm in the frequency domain.
- The femtocaching network model is proposed in @cite @cite and the capacity improvement for single-hop communication is computed. In @cite , the authors considered a femtocaching D2D network with multihop relaying of information from the helper to the UTs. They proposed a solution based on index coding in which the helper is utilizing the side information in the UTs to create index codes which are to be multicasted to the UTs. This way, they reduce bandwidth utilization by grouping multiple unicast transmissions into multicast transmission. However, that paper does not consider the case of coded side information and also it assumes that the relayed message from the helper cannot be changed based on the information in the relaying UTs.
- Caching has been a subject of recent interest to many researchers. The fundamental limits of caching is studied in @cite . The results in @cite has been extended to include decentralized coded caching strategies in @cite @cite @cite @cite . Other researchers studied the problem of caching in wireless and D2D networks. Among them are the works of authors in @cite @cite @cite . The authors in @cite have studied the capacity of wireless D2D networks with caching in certain regimes. Our work is essentially different from all of these works in the sense that the UT always request the content from helper (femtocache) while in these papers, a wireless ad hoc network is considered where UTs' requests can be satisfied by any of the nodes in the network. Clearly, such network model requires significant overhead to locate the nearest UT with the requested content while in our approach, the request always is sent toward the helper.
- Coded caching has been previously suggested @cite @cite as an efficient caching technique for devices with small storage capacity. Our results demonstrate that apart from the practical importance of coded caching in small storage systems, it can be useful in increasing the capacity of cached networks.
- More recently, several works have developed and demonstrated attacks. These attacks exploit protocol vulnerabilities across various layers in the stack to achieve high jamming gain and energy efficiency, and a low probability of detection @cite . For instance, @cite shows that the energy consumption of a smart jamming attack can be four orders of magnitude lower than continuous jamming. The works in @cite @cite show that several Wi-Fi bit rate adaptation algorithms, such as SampleRate, ONOE, AMRR, and RARF, are vulnerable to smart jamming. However, both conventional and smart jamming attacks are usually non-protocol compliant. Moreover, they require physical proximity. These limitations can be used to identify and locate the jammer.
- The work in @cite @cite show that local coupling due to interferences can have global effects on wireless networks. Thus, @cite proposes a queuing-theoretic analysis and approximation to predict the probability of a packet collision in a multi-hop network with hidden nodes. It shows that the sequence of the packet collision probabilities in a linear network converges to a fixed point. The work in @cite evaluates the impact of rate adaption and finds out that traffic increase at a single node can congest an entire network, and points out the existence of a phase transition.
- While some PPLs do have compilers, to our knowledge SIMPL is the first compiler that aims to make it easy to add new inference algorithms. For example, Bher @cite compiles Church, but only supports MCMC sampling.
- The R2 system @cite propagates evidence backwards through a probabilistic program to reject samples as early as possible. While this has the goal of making inference faster, the optimization is orthogonal to ours---it reduces the number of calls to the random number generator (by removing some sampling steps), whereas we reduce overhead without affecting the number of calls to the random number generator. In fact, we could first use the R2 optimization to generate a new, better probabilistic program, and then use our system to optimize inference over the new model. This also holds for other optimizations such as program slicing @cite .
- PE has been studied extensively, and an overview can be found in @cite . One recent approach to automatic PE is multi-stage programming languages such as MetaML @cite . However, multi-stage programming requires a statically computed binding-time analysis, which precludes operations like (value node) which are sometimes evaluated and sometimes generate residual code.
- Another approach to automatic PE is Lightweight Modular Staging @cite . LMS is similar to our approach in that it is value-driven, and we could probably obtain similar results using it, but this would require more effort from the inference algorithm writer. SIMPL provides a simpler interface to the programmer through annotations.
- The Delite framework @cite allows developers to create DSLs and automatically get benefits of common optimizations. It may be feasible to implement our DSL in Delite, but we do not think the primitive operations provided by Delite (which focus on collections) are well suited for inference.
- Programmer controlled compilation has been added to the JIT in Lancet @cite , which exposes an API for the JIT compiler to the programmer, so that the programmer can control compilation. Our work is similar, but since we target only inference algorithms, we provide higher-level optimizations that are more accessible for an algorithm writer.
- Broadway @cite uses annotations to provide semantic information about a library in order to optimize it. It replaces general library calls with manually written specialized versions, whereas we want to specialize the code automatically.
- Few theoretical results on transfer and policy advice have been achieved. Closest to this work is that in torrey2013teaching , where the authors only provide empirical validations to their approach without drawing on any theoretical analysis. Given the theoretical derivations in this paper, we in fact note that the method @cite is a special case of ours considering only one-teacher advice models.
- Another well-studied parameter would be @math which is defined as the largest absolute value of a subdeterminant of the constraint matrix @math associated to a polyhedron. @cite strengthened and extended the Dyer and Frieze upper bound @cite holding for totally unimodular case; i.e., when @math . Complexity analyses based on @math for the shadow vertex algorithm and the primal-simplex based Tardos' algorithm were proposed by Dadush and H " a hnle @cite , and Mizuno, Sukegawa, and Deza @cite @cite , respectively.
- We also note that there are studies that attempt to understand the behavior of @math when the number of facets is sufficiently large. Gallagher and Kim @cite provided an upper bound on the diameter of a normal simplicial complex and showed the tail-polynomiality ; more specifically, they showed that the diameter is bounded from above by a polynomial in @math when @math is sufficiently large. An alternative simpler proof for such tail-polynomial upper bounds can be found in Mizuno and Sukegawa @cite . In contrast, in this paper, we assume that @math is large, and try to utilize this assumption to strengthen the previous results.
- Literature on the convolutional neural networks: Since @cite introduced convolutional neural netoworks (CNN) in 1990, CNN has been used in various applications in computer vision such as object classification @cite @cite , object detection @cite @cite @cite @cite , action recognition @cite @cite , event recognition @cite @cite @cite @cite , image segementation @cite @cite and so on. Convolutional layers have been widely used in deep neural networks because they can make the network deeper without keeping the number of parameters significantly large. In general, the deeper the network is the better representation it can provide.
- Besides the benefit of keeping the number of parameters relatively small, the convolutional layers also provide additional advantages. Unlike the fully connected layers with fixed input and output dimensions, the convolutional layer allows the structure to be flexible by taking input and output of variable sizes depending on the given tasks. @cite introduced spatial pyramid pooling'' which constructs a multi-scale pyramid of feature maps in order to eliminate the requirement that input of CNN is fixed-sized. @cite replaced the fully connected layers from @cite with convolutional layers for semantic segmentation, called a fully convolutional network (FCN). @cite also implemented the FCN for object localization. Moreover, the output of the convolutional layers (i.e., feature maps) preserves local spatial information to a certain degree relative to the original input image. Figure 6 in Mahendran and Vedaldi @cite showing reconstructed images from the output of each layer of @cite illustrates the spatial configuration of an input image cannot be recovered after @math layer. This finding supports our argument that exploiting the sub-windows of the feature map from the @math layer along with expert units of fully connected layers is highly efficient for object localization.
- Several approaches are introduced to address the above issue and apply the DCNN for the object detection problem. @cite used a scanning window strategy and apply DCNN to each window in order to localize the object. @cite adapts the last fully connected layer to handle a number of local scanning windows to achieve the localization of objects of interest. @cite apply DCNN to 2000 windows with distinctive objectness characteristics for every test image, which is refered as to RCNN''. However, repeated applications of DCNN greatly increase computational complexity. Selective search to extract object-like windows in the image used in RCNN also requires about two seconds per an image. In contrast to the above two approaches, the proposed DCNN is much faster because the convolutional stage is applied only once for the entire image instead of repeatedly applying it for each local scanning window.
- Human activity recognition is a computer vision area with a great amount of attention @cite . There not only have been studies to recognize activities from YouTube-style videos @cite or surveillance videos, but also first-person videos from wearable cameras @cite @cite @cite @cite @cite and robots @cite . However, they only focused on developing features methods for more reliable recognition, without any privacy aspect consideration.
- On the other hand, as mentioned in the introduction, there are research works whose goal is to specifically address privacy concerns regarding unwanted video taking. designed a method to automatically detect locations where the cameras should be turned off. @cite was similar. studied human activity recognition from extreme low resolution videos, different from conventional activity recognition literature that were focusing mostly on methods for images and videos with sufficient resolutions. Although their work only focused on recognition from 3rd-person videos captured with static cameras, they showed the potential that computer vision features and classifiers can also work with very low resolution videos. However, they followed the conventional paradigm' described in Figure (a), simply resizing original training videos to make them low resolution. @cite studied privacy protection for convolutional neural networks (CNNs), but they consider the privacy protection only at the training phase and not at the testing phase, unlike ours.
- in 2014 performed a large scale analysis of the security of embedded firmware. They presented a correlation technique that allows for propagation of existing vulnerability information to similar firmware that has previously not been identified as vulnerable. One downside of their approach is that it is rather granular and cannot identify previously unknown firmware modification attacks @cite . present a general discussion of firmware modification attacks and then focus their work on HP's LaserJet printers, identifying an issue allowing an adversary with print permissions to update the affected printer using modified firmware @cite . Coppola in 2013 studied the firmware update procedure of Withings' wireless scale. He combined hardware and software reverse engineering to exploit a critical flaw in the update process, allowing him to upload arbitrary firmware to the scale @cite .
- Providing incentives to human agents to return thuthful responses is a central challenge of crowdsourcing algorithms and applications @cite .
- The model presented in @cite relaxes this assumption. The proposed scoring rule evaluates experts by comparing them to each other. The model assigns a higher score for an expert if her predictions are in agreement with predictions of other experts.
- The peer-prediction method @cite uses proper scoring rules to reward experts depending on how good their input for predicting other experts' reports. Similarly, the model described in @cite evaluates experts depending on how good their reports are in predicting the consensus of other workers.
- The model described in @cite considers a scenario of rational buyers who report on the quality of products of different types. In the developed payment mechanism the strategy of honest reporting is the only Nash equilibrium. However, the model requires that the prior distribution over product types and condition distributions of qualities is the common knowledge. Such assumptions do not hold in our peer review setting.
- The PeerRank method proposed in @cite obtains the final grades of students using a fixed point equation similar to the PageRank method. However, while it encourages precision, it does not provide a strategyproof method for the scenario that students collude to game the system without making the effort to grade truthfully.
- Membership inference can be considered as a user profiling problem, where missing attributes are inferred from known attributes and network structure. Several works are built on the idea of propagation. ( backstrom2010 ) proposed an algorithm specialized to locations. ( mislove2010 ) infer user profiles in a university by applying a graph clustering algorithm. However, members of the detected cluster do not necessarily share the same attribute. Pennacchiotti and Popescu ( pennacchiotti2011 ) proposed a hybrid algorithm to infer binary attributes, by combining machine learning methods and propagation on networks. More complicated models are proposed to capture the correlation between different types of attributes @cite , and the relationship between node and edge @cite . Limitations of the user profiling problem were discussed in @cite .
- Given that we are processing a network with millions of nodes and hundreds of groups, we need an inference algorithm that is scalable to both the size of network and the number of groups. We notice that little attention has been paid to social tie strength in user profiling. We leverage the techniques developed for supervised random walk @cite @cite to learn the strength. It turns to be reasonably efficient and effective.
- @cite proposed the problem of semi-supervised learning with structured outputs. Moreover, a novel discriminative approach was also proposed to use the manifold of input features of both labeled and unlabeled data points. This approach is based on the semi-supervised maximum-margin formulation. It is an inductive algorithm, and it can be easily extended to new-coming test data points.
- Brefeld and Scheffer @cite proposed to solve the problem of semi-supervised structured output prediction by learning in the space of input-output space, and using co-training method. This method is based on the assumption that the multiple structured output predictors should be consent with each other. Based on this assumption, the structural support vector machine is extended to the argued input-output space.
- @cite proposed a hybrid method to solve the problem of semi-supervised structured output learning. This method combines both the generative and discriminative methods. The objective of this method is composed of log-linear forms of both discriminative structured predictor and generative model. The generative model is used to incorporate unlabeled data points. The discriminant functions is enhanced by the unlabeled data points provided by the generative model.
- @cite proposed to regularize the structured outputs by the manifold constructed from the input space directly. This method constructs a nearest neighbor graph from the input features, and use it to represent the manifold. Then the manifold is used to regularize the learning of the missing outputs of the unlabeled data points. The outputs and the predictor are learned simultaneously, and they regularize each other in the learning process.
- The basic estimation question studied in this paper takes a different hue depending on whether the underlying distribution is discrete or continuous. In the discrete setting, significant understanding of the minimax rate-optimal estimation of functionals, including entropy and mutual information, of an unknown probability mass function is attained via recent works @cite @cite @cite @cite @cite . The continuous setting is significantly different, bringing to fore the interplay of geometry of the Euclidean space as well as the role of dimensionality of the domain in terms of estimating the information theoretic quantities; this setting is the focus of this paper. This fundamental question has been of longstanding interest in the theoretical statistics community where it is a canonical question of estimating a functional of the (unknown) density @cite but also in the machine learning @cite @cite , information theory @cite @cite @cite @cite , and theoretical computer science @cite @cite @cite communities. The popularity of mutual information and other information theoretic quantities comes from their wide use as basic features in several downstream applications @cite @cite @cite @cite .
- A conceptually straightforward way to estimate the differential entropy and mutual information is to use a kernel density estimator (KDE) @cite @cite @cite @cite @cite @cite : the densities @math are separately estimated from samples and the estimated densities are then used to calculate the entropy and mutual information via the resubstitution estimator. A typical approach to avoid overfitting is to conduct data splitting (DS): split the samples and use one part for KDE and the other for the resubstitution.
- In some cases, the parametric rate of convergence of @math of @math error is achieved: of particular interest is the result of @cite where the parametric rate is achieved for differential entropy estimation via KDE of density followed by the resubstitution estimator when the dimension is no more than 6. Numerical evidence suggests the hypothesis that the lower bounds derived in Theorem below could perhaps be improved when the dimension is more than 4 and estimators constrained to only use fixed @math -NN distances. Under certain very strong conditions on the density class (that are relevant in certain applications on graphical model selection @cite ), exponential rate of convergence can be demonstrated @cite @cite . Recent works @cite @cite have studied the performance of the leave-one-out (LOO) approach where all but the sample of resubstitution are used for KDE, involving techniques such as von Mises expansion methods.
- Alternative methods involve estimation of the entropies using spacings @cite @cite , the Edgeworth expansion @cite , and convex optimization @cite . Among the @math -NN methods, there are two variants: either @math is chosen to grow with the sample size @math or @math is fixed. There is a large literature on the former, where the classical result is the possibility of consistent estimation of the density from @math -NN distances @cite @cite , including recent sharper consistency characterizations @cite @cite . Several works have applied this basic insight towards the estimation of the specific case of information theoretic quantities @cite @cite and extensions to generalized NN graphs @cite . For fixed @math -NN methods, apart from the works referred to in the main text, detailed experimental comparisons are in @cite and local Gaussian approaches studied in @cite @cite @cite bringing together local likelihood density estimation methods @cite @cite with @math -NN driven choices of kernel bandwidth.
- In this paper we have considered the smoothness of the class of pdfs studied via bounded Hessians. In nonparametric estimation, a standard feature is to consider whole families of smooth pdfs as defined by how the differences of derivatives relate to the differences of the samples @cite . Of specific interest is the H " o lder family: @math , i.e., for any tuple @math , define @math . Then for any @math such that @math , where @math is the largest integer smaller than @math , we have: for any @math . The rate of convergence of various nonparametric estimators depends on the parameter @math of the H " o lder family under consideration, cf. @cite @cite for recent work on convergence rate characterization of information theoretic quantities via KDE and resubstitution estimators as a function of the smoothness parameter @math . It is natural to ask if such smoothness considerations could lead to a refined understanding of the rates of convergence of the fixed @math -NN KL and KSG estimators studied here.
- Pedestrian detection. Recent pedestrian detection works feature the proposal+CNN'' approach. Pedestrian detection usually employs weak pedestrian detectors as proposals, which allows achieving relatively high recall using very few proposals @cite @cite @cite @cite . Despite the impressive recent progress in pedestrian detection, it has been rarely considered with person re-ID as an application. This paper attempts to determine how detection can help re-ID and provide insights in assessing detector performance.
- 6.2pt Person re-ID. Recent progress in person re-ID mainly consists in deep learning. Several works @cite @cite @cite @cite @cite focus on learning features and metrics through the CNN framework. Formulating person re-ID as a ranking task, image pairs @cite @cite @cite or triplets @cite are fed into CNN. It is also shown in @cite that deep learning using the identification model @cite @cite @cite yields even higher accuracy than the siamese model. With a sufficient amount of training data per ID, we thus adopt the identification model to learn an CNN embedding in the pedestrian subspace. We refer readers to our recent works @cite @cite for details.
- Detection and re-ID. In our knowledge, two previous works focus on such end-to-end systems. In @cite , persons in photo albums are detected using poselets @cite and recognition is performed using face and global signatures. However, the setting in @cite is not typical for person re-ID where pedestrians are observed by surveillance cameras and faces are not clear enough. In a work closer to ours, Xu @cite jointly model pedestrian commonness and uniqueness, and calculate the similarity between query and each sliding window in a brute-force manner. While @cite works on datasets consisting of no more than 214 video frames, it may have efficiency issues with large datasets. Departing from both works, this paper sets up a large-scale benchmark system to jointly analyze detection and re-ID performance.
- Finally, we would like to refer readers to @cite , concurrent to ours and published in the same conference, which also releases a large dataset for end-to-end person re-ID.
- Object detection is a fundamental and heavily-researched task in computer vision. Until recently, the sliding window paradigm was dominant @cite @cite , especially for face and pedestrian detection. Deformable part models @cite followed this framework but allowed for more object variability and thus found success across general object categories; likewise, @cite @cite showcased the use of CNNs for general object detection in a sliding window fashion. More recent detectors follow the region-proposal paradigm established by @cite in which a CNN is used to classify regions generated by an object proposal algorithm @cite . Many recent detectors follow this setup @cite @cite @cite @cite @cite @cite , including our own work, which uses the Fast R-CNN detector as its staring point @cite . We next discuss in more detail specific innovations in this paradigm and how they relate to our approach.
- : Context is known to play an important role in visual recognition @cite . Numerous ideas for exploiting context in CNNs have been proposed. @cite used two contextual regions centered on each object for pedestrian detection. @cite , in addition to region specific features, features from the entire image are used to improve region classification. @cite implement context in a more implicit way by aggregating CNN features prior to classification using different sized pooling regions. More recently, @cite proposed to use ten contextual regions around each object with different crops. Our approach is most related to @cite , however, we use just four contextual regions organized in a foveal structure and importantly our classifier is trained jointly end-to-end.
- : @cite proposed to use a multi-stage' classifier that used features at many convolutional layers for pedestrian detection, showing improved results. Such skip' architectures have recently become popular for semantic segmentation @cite @cite . Concurrently with our work, @cite proposed to revisit skip connections for general object detection. Our own implementation of skip connections closely resembles @cite .
- : When originally introduced, object proposals were based on low-level grouping cues, edges, and superpixels @cite @cite @cite @cite @cite . More recently, large gains in proposal quality have been achieved through use of CNNs @cite @cite @cite @cite . In this work we use DeepMask segmentation proposals @cite . Specifically, we used an early version of the improved variant of DeepMask described in @cite that includes top-down refinement but is based on the VGG-A architecture @cite , not the later ResNet architecture presented in @cite . Overall, we obtain substantial improvements in detection accuracy on COCO by using DeepMask in place of the Selective Search @cite proposals used in the original work on Fast R-CNN @cite .
- : The CNN used for classification forms an integral part of the detection pipeline and is key in determining final detector accuracy. The field has witnessed rapid progress in CNNs in recent years. The introduction of AlexNet @cite reinvigorated the use of deep learning for visual recognition. The much deeper VGG @cite and GoogleNet @cite models further pushed accuracy. In our work we use variants of the VGG network @cite , specifically VGG-A for DeepMask and VGG-D for our MultiPath network. In concurrent work, He at al. @cite introduced the even deeper Residual Networks (ResNet) that have greatly improved the state of the art and have also proven effective for object detection. We expect that integration of ResNet into our system could further boost accuracy.
- To encode the faces into numerical vectors, many successful features were proposed Gabor @cite and Local Binary Patterns (LBP) @cite , fiducial points based descriptors @cite . They handled videos by either aggregating features over all frames, using average or max-pooling @cite @cite , or extending features to be spatio-temporal 3D Gabor @cite and LBPTOP @cite . Facial Action Units, represent movement of facial muscle(s) @cite , were automatically detected and used as high level features for video prediction @cite @cite .
- Nguyen al @cite proposed a latent SVM based algorithm for classifying and localizing events in a time-series. They later proposed a fully supervised structured SVM for predicting Action Unit segments in video sequences @cite . Our algorithm differs from @cite , while they use simple MIL, we detect multiple prototypical segments and further learn their temporal ordering. MIL based algorithm has also been used for predicting pain @cite . In recent works, MIL has been used with HMM @cite and also to learn embedding for multiple concepts @cite for predicting facial expressions. Rudovic al @cite proposed a CRF based model that accounted for ordinal relationships between expression intensities. Our work differs from this work in handling weakly labeled data and modeling the ordinal sequence between sub-events (see ).
- Since Rosenthal introduced the congestion game @cite , it has been applied to problems of congestion externalities in several areas such as transportation and communication networks @cite . In this game, players choose a combination of resources and the players' payoffs depend on the congestion level, i.e., the number of players using the same resources. The congestion game is a subclass of potential games, which feature the finite improvement property (FIP). This property guarantees that if each player updates his strategy in response to other players by turns, it will reduce his private cost and improve the common potential function, which eventually reaches a local minimum, the pure Nash equilibrium (pNE), where each player has a deterministic strategy. Because of this property, potential and congestion games have been well studied and have wide applications @cite .
- According to a review of sharing studies in transportation areas @cite , there are few game theory studies on externalities and coordination of players' moves in vehicle sharing problems. Traditional studies on sharing include the optimization of vehicle routes for picking up all passengers (the Dial-a-ride problem ), problems of splitting passengers' fares according to their riding distances, optimization of locations of carpool stations, and problems of relocation of carpool vehicles among stations. Those studies mainly focus on optimization problems under given moves (origins and destinations) of passengers and fixed drivers of vehicles, and therefore, do not analyze the PoA when passengers strategically choose their moves in response to the moves of other passengers and vehicles. The review states that there are no studies on coordination of players' moves to improve the efficiency of sharing or reduce the PoA.
- Object proposals @cite @cite @cite @cite considerably reduce the computation compared with sliding window methods @cite @cite in detection framework. These methods can be classified into two general approaches: traditional methods and deep learning based methods. Traditional methods attempt to generate region proposals by merging multiple segments or by scoring windows that are likely be included in objects. These methods unusually adopt cues like superpixels @cite , edges @cite @cite , saliency @cite and shapes @cite @cite as features . Recently, some researchers are using CNN to generate region proposals. Deepbox @cite is trained with a slight ConvNet model that learns to re-rank region proposals generated by EdgeBoxes @cite . RPN @cite has joined region proposal generator with classifier in one stage or two stages. Both region proposal generation and detection results are promising. In DeepProposal @cite , a coarse-to-fine cascade on multiple layers of CNN features is designed for generating region proposals.
- There is a huge wealth of literature on dealing with visual odometry or camera motion tracking. However, we will only focus on the direct approaches which can track and reconstruct a dense and textured 3-D model in real-time. Motion tracking using active sensor @cite @cite is independent to lighting but leaves surfaces un-textured. Approaches combining appearance and depth @cite @cite @cite @cite for camera tracking are the most relevant approaches. In all these approaches, it is assumed that brightness of consecutive frames is constant which is likely to fail when video flickering happens. In addition, @cite introduce a simple color blending method but as shown in our experiments, it is inadequate to deal with large exposure changes. Kerl @cite introduce a key frame based approach by taking the rolling shutter effect into account. The approach relies on local brightness constancy when tracking live frames with a key frame, it is capable of producing sharp super-resolution frames involves no exposure compensation.
- Maxime @cite propose one of the first work in real-time 3-D HDR texture capturing. We follow the same approach of transforming raw RGB into radiance domain and tracking using radiance. It mainly focuses on re-lighting virtual specular objects. The differences between @cite and this paper are two-fold. First, in @cite , a gamma function is adopted to approximate inverse camera response function (CRF). Gamma function may introduce error when radiance is high and the resulting radiance is not directly proportional to scene luminance (Fig. ). Second, in @cite , the exposure is estimated jointly with camera pose, but we find that the shape of error function when tracking using exposure compensated radiance bears shallow global minima even when exposure has been compensated for and, therefore, not as robust as normalized radiance based object function we proposed.(Fig. ) Lastly, mosaic artefacts are clearly visible from the synthetic HDR mode which indicate inadequate exposure estimation (Fig. (d)).
- Normalized cross correlation (NCC) has been widely applied in visual tracking @cite to deal with challenging lighting condition but its computational cost grows exponentially with the size of patch. Small patches are sensitive to image noise and bring many local minimum (Fig. ). In addition, the 3-D HDR texture capturing is not addressed in the paper. HDR video capture using high-end stereo rig @cite @cite is also relevant to the topic since it involves estimating disparity between binocular views so that LDR frames captured by both frame can be integrated into a single stream of HDR video @cite , but the high quality HDR video is the main focus of the group of approach rather than a full 3-D model.
- There has been a recent line of work on end-to-end character-based neural models which achieve good results for part-of-speech tagging @cite @cite , dependency parsing @cite , text classification @cite , speech recognition @cite @cite , and language modeling @cite @cite . However, success has not been shown for cross-lingual tasks such as machine translation. Recently, attempt character-level NMT; however, the experimental evidence is weak. The authors demonstrate only small improvements over word-level baselines and acknowledge that there are no differences of significance. Furthermore, only small datasets were used without comparable results from past NMT work. propose to segment words into smaller units and translate just like at the word level, which does not learn to understand relationships among words.
- Our work takes inspiration from @cite and @cite . Similar to the former, we build representations for rare words on-the-fly from subword units. However, we utilize recurrent neural networks with characters as the basic units; whereas use recursive neural networks with morphemes as units, which requires existence of a morphological analyzer. In comparison with @cite , our hybrid architecture is also a hierarchical sequence-to-sequence model, but operates at a different granularity level, word-character. In contrast, build hierarchical models at the sentence-word level for paragraphs and documents.
- Since the introduction of semantic attributes @cite @cite @cite , they have gained increasing attention from the computer vision community. They represent an intermediate layer of semantics and facilitate various tasks in visual recognition like zero-shot learning @cite @cite @cite , aiding object classification and localization @cite , relative attribute comparison @cite and detection of unfamiliar classes @cite . However, in the prevailing direction the attributes are learned in a manner from all classes available in the source @cite @cite @cite @cite . Such methods cannot cope with the high variations in each of the attributes. Some approaches jointly model objects, attributes and their correlations @cite @cite to handle such variations. Nonetheless, these correlations cannot be learned for unseen classes since there is no training data.
- Our approach is related to the work on learning class-specific attributes. In @cite a set of attributes are learned per class as an intermediate step for feature selection in order to reduce attribute correlations. Yu al @cite propose to learn data-driven attributes at the category-level to better discriminate the classes. However, data-driven attributes usually carry no semantic meaning; thus, their approach requires user interaction when performing zero-shot learning. In @cite the concepts in ImageNet are augmented with a set of semantic and data-driven attributes. These are used along with the hierarchy to learn a better similarity metric for content-based image retrieval. Correspondingly, we propose to explicitly model the intra-attribute variations at different abstraction levels. That is, rather than just using class-specific attributes, we expand the notion of the attribute from the most abstract to the most specific driven by the embedded relations between the categories.
- Additionally, hierarchies represent an attractive structure for knowledge transfer and they have been exploited in various ways: parameter transfer @cite @cite , representation transfer @cite and bounding box annotations propagation @cite . Of particular relevance to our work is the joint modeling of hierarchy and attributes. In @cite , a ranking classifier is trained using attributes for label embedding; showing that using hierarchical labels along attributes as side information improved the zero-shot performance. In the recent work of @cite , a hierarchy and exclusion graph is learned over the various object categories. The graph models binary relations among the classes like mutual exclusion and overlap. They also model similar relations between objects and attributes and use it for zero-shot classification.
- Some pioneers try to extract skeletons from the gradient intensity maps computed on natural images. The gradient intensity map is generally obtained by applying directional derivative operators to a gray-scale image smoothed by a Gaussian kernel. For instance, in @cite , the author provides an automatic mechanism to determine the best size of the Gaussian kernel used for gradient computation, and he propose to detect skeletons as the pixels for which the gradient intensity assumes a local maximum (minimum) in the direction of the main principal curvature. Jang and Hong @cite extract the skeleton from the pseudo-distance map which is obtained by iteratively minimizing an object function defined on the gradient intensity map. Yu and Bajaj @cite propose to trace the ridges of the skeleton intensity map calculated from the diffused vector field of the gradient intensity map, which can remove the undesirable biased skeletons. Due to the lack of object prior, these methods are only able to handle the images with simple scenes.
- Consider a partial differential equation on a bounded, simply-connected domain @math , subject to the Dirichlet and Neumann boundary conditions @math denotes a linear differential operator, @math and @math are disjoint sets of the boundary with @math , and @math denotes the outward normal to @math . A finite element method, or more generally a weighted-residual method @cite , introduces a set of test functions (a.k.a. weight functions) @math , and require the residual @math to be orthogonal to @math , i.e., Eq. ) and ) are the and of the PDE, respectively. In finite element methods, @math is typically (weakly) differentiate and has a local support. In addition, @math typically forms a partition of unity, i.e., @math , so that the weighted-residual formulation is global conservative in that @math
- The isoparametric FEM in general uses the same-degree polynomial basis uniformly. The @math -adaptive FEM generalizes it by allowing the degrees of the basis functions to vary from element to element @cite @cite . The @math -adaptivity may be further combined with @math -adaptivity, leading to the @math -adaptive FEM or @math -FEM @cite @cite . In these methods, inter-element continuity is typically enforced as constraints @cite . The @math -FEM can achieve high-order convergence, with error decreasing exponentially in the degree of basis functions, provided that the mesh is nearly optimal @cite . Similar to isoparametric FEM, the @math -FEM also requires curved elements for curved geometries, requires good element shapes for accuracy, and is limited to only moderately high orders due to the use of equally spaced nodes.
- Another class of high-order FEM is the discontinuous Galerkin (DG) methods. Unlike in isoparametric or spectral element methods, the basis functions in DG are not continuous between elements. Instead, a numerical flux is used at the element boundaries. DG is most conveniently applied to hyperbolic PDEs, such as the Navier-Stokes and compressible Euler equations @cite @cite , while extensions to parabolic and elliptic problems have also been developed @cite . DG can achieve high-order accuracy and can be used on complex geometries @cite , but high-quality curved elements are required for curved boundaries. Other methods related to DG include the interior penalty methods for elliptical problems, which can be unified with DG under one framework @cite , the compact DG @cite , which has a more compact stencil than local DG methods, and hybridized DG @cite , which has a lower number of degrees of freedom.
- To overcome mesh dependence, meshless or meshfree methods have been proposed @cite @cite . Like IGA, these methods avoid the use the piecewise polynomials as basis functions. Some meshless methods are Galerkin methods with non-Lagrange basis functions, such as radial basis functions or Shepherd basis functions, which require special quadrature rules and cause complexities in enforcing Dirichlet boundary conditions. Another class of meshless methods is the generalized finite difference (GFD) methods @cite , which are weighted residual methods using the Dirac delta functions at the nodes as test functions. When high-degree polynomials are used as basis functions, GFD can achieve high-order accuracy. However, unlike FEM, GFD methods cannot use integration by parts to reduce the order of derivatives, and are not globally conservative because the test functions do not form a partition of unity.
- @cite proposed to invert the pooling process in a CNN to generate progressively higher resolution input images by storing the switch' variables from the pooling operation. Deconv networks have recently been applied successfully to semantic segmentation @cite . Deconv layers share similarities with our refinement module, however, switches' are communicated instead of the feature values, which limits the information that can be transferred. Finally, @cite proposed to progressively increase the resolution of an optical flow map. This can be seen as a special case of our refinement approach where: (1) the features' for refinement are set to be the flow field itself, (2) no feature transform is applied to the bottom-up features, and (3) the approach is applied monolithically to the entire image. Restricting our method in any of these ways would cause it to fail in our setting as discussed in .
- With the reappearance of convolutional neural nets, part detectors became more reliable, leading to a significant improvement in accuracy @cite @cite @cite @cite @cite @cite . The works of @cite @cite focus on multi-scale representation of body parts in order to infer the location of keypoints. In @cite , the authors deal with simultaneous detection and pose estimation of multiple people. Recent works @cite @cite use an iterative scheme to refine pose estimation. As in our approach, in @cite an image dependent binary term is learned. They, however, learn the binary terms explicitly while in our model it arises naturally from the voting scheme.
- Sliding window and Hough Transform are two major frameworks used in many visual object detection approaches. Owing to the significant work of @cite (i.e., the ISM approach), the Hough Transform framework becomes more and more popular in detecting irregular-shaped and articulated objects @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . The ISM approach extracts local features from objects in training images. The spacial relationship between each local feature and its corresponding object center is recorded as a voting vector. All the extracted local features are clustered, and the obtained cluster centers form an appearance codebook. Any local feature in a test image is matched to the codebook. The recorded voting vectors corresponding to the activated codebook entries are used to cast Hough votes. The partISM approach @cite and voting line-based approach @cite also build up a codebook by clustering local features. In @cite , the parts of a pedestrian are detected independently. The spacial relationship between each part and the center of a pedestrian is learned to predict the locations of pedestrians in test images.
- However, the clustering step for building up a codebook in ISM and partISM is time-consuming. Moreover, in the clustering step, a distance threshold is used to determine whether a local feature should merge with a cluster. The threshold value can significantly influence the effectiveness of the derived codebook. In addition, the weights assigned to all Hough votes cast from a codebook entry are identical in the ISM approach. To address these issues, a number of improvements have been made to the ISM approach in recent years. Instead of unsupervised clustering, the Hough forests @cite @cite and DGHT @cite approaches implement a supervised clustering step to construct random forests. The leaf nodes of a tree in a random forest are considered to form a discriminative codebook. Given a local feature in a test image, both its corresponding voting vectors and the probability that the feature belongs to foreground can be obtained from the leaf nodes of a tree. The probability is considered as the weight of any Hough vote cast by the feature. The MMHT @cite approach adapts the probabilistic framework in the ISM approach and learns the weights of Hough votes in a discriminative max-margin framework.
- As for handling object scale variations, the ISM @cite , 4D-ISM @cite , partISM @cite , IRD @cite , Fast PRISM @cite and MMHT @cite approaches use local feature descriptors to estimate the scales of local features and cast Hough votes in a scale space. The positions where the voting points are most concentrated in the scale space are considered as the locations of object hypotheses. The Hough forests @cite , PSCG @cite and latent Hough forest @cite approaches, as well as the iterative multi-object extraction framework @cite , simply rescale a test image to form an image pyramid and perform object detection at each level of the pyramid. In the derived Hough image pyramid, 3D local maxima indicate the estimated positions and scales of object hypotheses @cite . In @cite , local scale estimation is considered to be unreliable. Therefore, to solve this problem, voting points are extended to voting lines in @cite . A voting line consists of the voting points cast by a local feature at all scales in a scale space. The position of an intersection point of voting lines in the scale space indicates the location and scale of an object hypothesis.
- The Partial Least Squares technique @cite is a supervised dimensional reduction tool, which is usually followed by a feature selection step to discard noisy and redundant features. For instance, @cite and @cite use PLS followed by a feature selection strategy named Ordered Predictors Selection to detect humans and vehicles on challenging datasets, respectively. Recently, a novel pedestrian detection approach @cite performs feature selection by using an Adaboost algorithm to select spatially pooled covariance matrix features and LBP features. By intensively inspecting the experimental design of the detector, the approach proposed in @cite obtains an impressive performance in pedestrian detection. @cite employs PLS in a multi-stage framework to perform data-driven object detection. Furthermore, a non-linear variant of PLS in Reproducing Kernel Hilbert Space, i.e. Kernel Partial Least Squares @cite , is employed in @cite @cite @cite to improve the performance of PLS in the tasks of head pose estimation, monocular 3D pose estimation and human age estimation, respectively.
- The second category, where our proposed method also belongs to, uses information from the context. Notable works are @cite and @cite . In particular, applying to machine translation task, @cite learns to point some words in source sentence and copy it to the target sentence, similarly to our method. However, it does not use attention mechanism, and by having fixed sized softmax output over the relative pointing range (e.g., -7, , -1, 0, 1, , 7), their model (the Positional All model) has a limitation in applying to more general problems such as summarization and question answering, where, unlike machine translation, the length of the context and the pointing locations in the context can vary dramatically. In question answering setting, @cite have used placeholders on named entities in the context. However, the placeholder id is directly predicted in the softmax output rather than predicting its location in the context.
- The third category of the approaches changes the unit of input output itself from words to a smaller resolution such as characters @cite or bytecodes @cite @cite . Although this approach has the main advantage that it could suffer less from the rare unknown word problem, the training usually becomes much harder because the length of sequences significantly increases.
- One can interpret the most biased coin problem as an infinite armed bandit problem where each coin is an arm. In that setting, @cite , @cite and @cite prove and refine bounds on the expected cumulative regret of the player, whereas @cite focus on the pure exploration setting. All of this work relies on the assumption that the distribution of the means is parametric and known (though @cite describes a method to estimate the relevant parameters first). Our setting relies on a different parameterization of the means (i.e. @math where @math is a Dirac delta located at @math ), and we focus on settings in which the relevant parameters are unknown.
- HRIS @cite is the first and the only work to infer network-constrained trajectories from partial observations. In this section, we outline how InferTra is different.
- More recently, a technique @cite was designed to study the trajectory inference problem in a setting where trajectories are not constrained by a network. Due to the focus on network-free trajectories, @cite is not applicable to our problem.
- Considering secure distributed computation, also known as secure multi-party computation, from an information theoretic (i.e., Shannon) perspective is still in its infancy. To the best of the authors' knowledge there exist only some very recent results. For instance, introduce a new Shannon theoretic multiuser source model in @cite and @cite and characterize when a function is securely computable. In this context, they provide necessary and sufficient conditions for the existence of protocols that achieve this. Within the standard secure multi-party computation model of @cite , Lee and Abbe determine in @cite the least amount of randomness needed for securely computing a given function. This provides a novel notion of the complexity of a function for its secure computation. In the second part of that paper, the considerations are extended to a probabilistic source model for which the decoding error probability is required to vanish asymptotically in the block length.
- In @cite , take a distributed source coding approach to the problem of securely computing the modulo-2 sum of two distributed binary sources. Similarly to @cite , they assume the data to be drawn from some joint memoryless source and derive bounds on the amount of randomness and communication needed to asymptotically achieve secrecy. In @cite , the results are extended to arbitrary functions.
- Time-aware user profiles are constructed based on the assumption that the degree of user interests declines as time passes. The decline of user interests is modeled by a decay function. In the past, the decay functions sliding window @cite and exponential decay @cite @cite have been employed for user profiling. However, they have not been compared so far like we do in this work.
- Optimal exact regenerating codes at the MBR point for all values of @math and optimal exact regenerating codes at the MSR point when @math are constructed in @cite . The asymptotic achievability of the MSR point for all values of @math is shown in @cite .
- Constructions of exact regenerating codes between the MBR and MSR points exceeding the time-sharing line are studied in @cite @cite @cite @cite . Results in @cite and @cite are combined in @cite . However, in contrast to these constructions we are not trying to find codes that perform as well as possible. Instead, our main purpose is to show that LRCs can be used as exact regenerating codes and, in many cases, they have quite good performance. However, when compared to the other constructions, LRCs when used as exact regenerating codes do not usually perform that well. The established connection also enables further analysis on LRCs in terms of storage space, repair bandwidth, .
- From the opposite point of view, nontrivial upper bounds for exact regenerating codes are studied for example in @cite and @cite . In the case @math , the capacity curve for all possible values of @math was solved in @cite . In the nonasymptotic case, it was shown in @cite that in most points between MBR and MSR, there do not exist exact regenerating codes that would achieve the capacity of functionally repairing regenerating codes.
- Locally repairable codes for several parameter sets @math are constructed in @cite , @cite , @cite , @cite , @cite , @cite . In @cite it was proved that bound is not achievable for all parameters @math . Codes with local regeneration are studied in @cite . In a completely different direction, connections between regenerating codes and LRCs have been made in @cite , where local repairability is exploited to select helpers in a regenerating code.
- Due to the complexity of the field equation, there are not many rigorous results on dynamical aspects of the Skyrme model. In @cite , small data global well-posedness and scattering is proved and @cite establishes large-data global well-posedness. There is also some recent activity on the related but simpler Adkins-Nappi model, see e.g. @cite @cite @cite . From a numerical point of view, the linear stability of the Skyrmion is addressed in @cite and @cite studies the nonlinear stability. As far as the method of proof is concerned, we note that our approach is in parts inspired by @cite .
- One of the most popular techniques is the instruction level energy model @cite . Various test patterns of instructions are executed on a processor and their power empirically measured, leading to a model of per instruction energy costs and the dynamic cost of switching between different instructions. Simulating an instruction sequence, or interpreting a trace of an execution, can then be combined with this energy model to produce a cost value for the execution. @cite extend this model to include the costs of circuit switching in instruction operands. These costs include the amount of switching occurring on data buses supplying input operands to an instruction, and the switching on the output when a result is written back to the register file.
- Further modelling techniques for dynamic power go beyond the core part of the processor, such as analysing flash memory @cite , caches @cite and DRAMs @cite . High performance processors feature hardware-provided counters that record metrics such as cache hit rates, which can be used by appropriately parametrised energy models @cite . The energy consumption on the buses to these components can also be influenced by data values and can be modelled accordingly.
- WCEC is a form of energy estimate, where the aim is to find the maximum amount of energy that a piece of software will consume, without needing to execute that software. The problem is thus made of two parts: modelling the energy consumption of the software under test, and searching for the execution of it that will lead to the greatest amount of energy consumed. This problem is defined in a similar way to the Worst Case Execution Time (WCET) problem @cite where the execution time of software is modelled, and then the longest possible path found. However, the techniques required to obtain a solution have a number of differences.
- The first publication to provide a technique for computing the WCEC of software was by @cite , where upper bounds on the energy consumption of several programs were inferred using energy models of software basic blocks and an ILP solver to find a maximal path through the program. The authors additionally debunk the suggestion that the execution path consuming the most time is always the path that also consumes the most energy. With regards to dynamic power, the authors assume maximal circuit switching on every clock cycle but model power management techniques within the processor such as clock gating to create a realistic energy model. The dynamic power of switching due to operand values is not specifically considered, and indeed the authors show that that its contribution of dynamic power to overall energy is low, thus their approximation does not introduce significant imprecision. We address the contribution of operand values to dynamic power in sec:xcoresw .
- Resource analysis techniques that extract cost relations from programs have been employed to analyse energy consumption bounds @cite @cite . The costs used in these analyses represent energy consumption and are based on models that provide a single energy cost per instruction, obtained by averaging the energy measured from processing random data, constrained to yield valid operands for the respective instruction @cite . However, bounds obtained in this way cannot be considered safe, as executions would exist where the energy from operand values exceeds the average case.
- More recently, @cite have presented techniques for estimating over and under approximations of WCEC through implicit path enumeration and genetic algorithms, respectively. They do not, however, comment on dynamic power at all: their absolute instruction energy model appears to assume maximum switching for each instruction cost. Their relative energy model does not consider real energy costs, instead estimating the difference in energy consumption between instructions, again with no explicit consideration of dynamic power.
- Switching between instructions is a notable contributor to energy consumption, which can be controlled through the order in which instructions are executed. Techniques have been developed to reduce consumption through instruction scheduling @cite , but this is known to be an NP-hard problem. Instruction scheduling uses pre-computed costs of switching between instructions to determine an optimal static schedule. It does not consider the operands to instructions or any cost that does not have a fixed value.
- Formally, following the presentation of @cite , define @math to be a set of literals, and @math a set of disjunctive form clauses: where each @math is a Boolean variable. A truth assignment defines each @math or its negation to be true. A clause is deemed to be satisfied if at least one literal in the clause is assigned true. A MAXSAT problem is a set of literals and set of clauses @math , such that the solution is the truth assignment that causes the maximal number of clauses to be satisfied.
- Despite this, there are scenarios where WCET is subject to timing anomalies @cite . These arise when a seemingly shorter execution path later results in an overall increase in time. A cache miss may cause the worst execution time locally, but this state may later preclude a subsequent scenario that, with a global view, would in fact be the worst-case.
- In real-time embedded systems, timing-predictable processors execute instructions within a fixed number of clock cycles, irrespective of the data the operation works on. This is particularly beneficial to WCET analysis, which can then focus on identifying the worst-case execution path which is determined by the control flow, rather than by the data flow of the computation. More advanced micro-architectural features, such as early-out of operations, or cache hierarchies, provide higher average performance at the cost of predictability. This makes WCET analysis far more challenging, as tight bounds firmly rely on timing predictability of the target architecture @cite . However, even operations that have a variable execution time, such as serialized integer multiply and divide, or floating point operations, can be quantized by the processor's clock period into a tractable number of discrete possibilities. The range may be in the order of tens, hundreds, or thousands of cycles, depending on the type of operation. This extends into other architectural features, such as caches and branch predictors, which although more complex to analyse, can still be quantized.
- For the techniques that are used in WCET to be directly transferable to WCEC, a set amount of energy per operation would need to be specified and realised in hardware, similar to specifying and ensuring, through timing analysis at design time, that each operation fits into a fixed number of clock cycles. Consider the converse: A processor that presents a similar WCET analysis difficulty would be an asynchronous design, where the precise execution time is a non-trivial function of an operation's input data. Such devices may have an average delay, but actual performance or tight bounds for a given use case may be harder to determine @cite .
- Recently hashing using deep learning has shown great promise. The authors of @cite @cite learn hash bits such that multilevel semantic similarities are kept, taking raw pixels as input and training a deep CNN. This has the effect of simultaneously learning an image feature representation (in the early layers of the network) and the hash bits, which are obtained by thresholding the outputs of the last network layer, or at 0.5. Note that these methods suffer from huge computation complexity introduced by the triplet ranking loss for hashing. In contrast, our proposed method is much more efficient in training, as shown in our experiments.
- Several works in the literature of power systems, operational research and more recently machine learning offer approaches for solving sequential stochastic problems using dynamic programming. Of which, the majority of these works focus on energy storage @cite @cite @cite @cite , unit commitment @cite @cite @cite , and energy market bidding strategies @cite @cite @cite . To our knowledge, no work has been done to use MDPs for assessing the reliability in power grids.
- For our proxy abstraction devise a hierarchical model. Hierarchical models, offer several benefits over flat models when appropriate. They can improve exploration, enable learning from fewer trials, and allow faster learning for new problems by reusing subtasks learned on previous problems @cite . Standard approaches for hierarchical models include: planning with options (often referred to as skills) @cite , task hierarchy @cite and hierarchy of abstract machines @cite . These models include levels of decision making that share the same state-space and a termination condition to switch between controllers. This structure does not fit our problem well where two separate decision makers run on state-spaces and resolutions.
- The first neural network approach to style transfer is @cite , using so-called Gram Matrices'' to represent global statistics about the image based on output from convolution layers. These statistics are computed by taking the inner product of intermediate activations---a tensor operation that results in a (N N ) matrix for each layer of (N ) channels. During this operation, all local information about pixels is lost, and only correlations between the different channel activations remain. When glitches occur, it's most often due to these global statistics being imposed onto the target image regardless of its own statistical distribution, and without any understanding of local pixel context.
- Thankfully, recent CNN architectures are capable of providing such semantic context, typically by performing pixel @cite . These models rely primarily on convolutional layers to extract high-level patterns, then use deconvolution to label the individual pixels. However, such insights are not yet used for synthesis---despite benefits shown by non-neural approaches.
- The state-of-the-art approaches to style transfer exploit semantic information to great effect, performing color transfer on photo portraits using specifically crafted image segmentation @cite . In particular, facial features are extracted to create masks for the image, then masked segments are processed independently and colors can be transferred between each corresponding part (e.g. background, clothes, mouth, eyes, etc.) Thanks to the additional semantic information, even simpler histogram matching algorithms may be used to transfer colors successfully.
- Neural Networks have attracted little attention in the CF community. In a preliminary work, @cite tackled the Netflix challenge using Restricted Boltzmann Machines but little published work had follow @cite . While Deep Learning has tremendous success in image and speech recognition @cite , sparse data has received less attention and remains a challenging problem for Neural Networks.
- Nevertheless, Neural Networks are able to discover non-linear latent variables with heterogeneous data @cite which makes them a promising tool for CF. @cite @cite @cite directly train Autoencoders to provide the best predicted ratings. Those methods report excellent results in the general case. However, the cold start initialization problem is ignored. For instance, AutoRec @cite replaces unpredictable ratings by an arbitrary selected score. In our case, we apply a training loss designed for rating inputs and we integrate side information to lessen the cold start effect.
- Other contributions deal with this cold start problem by using Neural Networks properties for CBF: Neural Networks are first trained to learn a feature representation from the item which is then processed by a CF approach such as Probabilistic Matrix Factorization @cite to provide the final rating. For instance, @cite @cite respectively auto-encode bag-of-words from restaurant reviews and movie plots, @cite auto-encode heterogeneous side information from users and items. Finally, @cite @cite use Convolutional Networks on music samples. In our case, side information and ratings are used together without any unsupervised pretreatment.
- A large selection of relevant work are trained in the strong supervision paradigm with detailed annotated ground truth in the form of bounding boxes @cite , @cite , object masks @cite , @cite , @cite and 3D object appearance cues @cite , @cite . The requirement of rich annotations curb the application of these methods in data-sets and modalities where training data is limited to weaker forms of labeling. Weak supervision for object detection tries to work around this limitation by learning localization cues from large collection of data with in-expensive annotations.
- Oquab et.al. @cite has proposed a weakly supervised object localization system which learns from training samples with objects in composite scenes by explicitly searching over candidate object locations and scales during the training phase. While this method performs well on data-sets with complex scenes, the extent of localization is limited with respect to estimating one point in the test image. The extent of the object is not estimated and detecting multiple instances of the same object class is not considered. In our proposed approach, we estimate both the location and extent of objects and are capable of estimating multiple instances of objects in the test image. Also, we use pre-existing classification networks for localization where as @cite proposes training custom adaptation layers.
- Characterizing the demographics of Twitter users has been studied by @cite who infer geography, gender, and race of the users based on self-reported locations and the names of the users. They find large deviances from the demographic distribution of the overall population. @cite provide a more extensive demographic comparison of five social media platforms based on telephone interviews. @cite look into the demographics and behavior of web users, whereas @cite study the same for search-engine users.
- The demographic prediction based on user's apps has been previously studied by @cite who predict the users' gender. In their previous work @cite , they also predict language, country, relationship status, and whether the user is a parent, but instead of predicting these attributes directly, they first predict which apps are associated with the attributes and then check whether a user has apps corresponding to a given demographic attribute. We extend these works by studying new demographics (age, race, and income), showing that increasing the training dataset size drastically improves the prediction accuracy, and comparing various dimensionality reduction methods for the app data.
- Others have studied demographic prediction, e.g., based on website visits @cite @cite , social network features @cite @cite , call patterns @cite , Twitter followers @cite and profiles @cite , and location data @cite . Related to demographic prediction, @cite investigate the predictability of personality traits based on apps and other smartphone usage features. There also was an app for predicting personality based on installed apps http: www. idigitaltimes.com what-do-your -apps-say -about-you-new-app-iphone-here-tell -you -410883 .
- Since the great success of AlexNet @cite on the ImageNet Large Scale Visual Recognition Challenge (ILSCRC-2010), a number of attempts have been made to improve the architectures of CNN in order to achieve better accuracy. We divide these methods into the following three categories.
- (1) . Some researchers paid attention to the parameters of CNN, such as the sizes of convolutional filters, the strides of filters, the number of feature channels in each layer, and the number of convolutional layers. They tried to adjust the parameters to improve the performance of CNN through exhaustive experiments. Zeiler and Fergus @cite visualized the trained CNN model and found that large filter size and large stride of the first convolutional layer could cause aliasing artifacts. Therefore, they used smaller receptive window size and smaller stride. Sermanet @cite utilized smaller strides in the first convolution, larger number of feature maps, and larger number of layers. They achieved better results than the AlexNet. The VGG network @cite pushes the depth of CNN to up to 19 convolutional layers by using very small convolutional filters and gains a significant improvement. These above efforts can be viewed as preliminary explorations on how to construct networks with better performance.
- We agree that both width and depth of networks are important for the tasks of visual recognition. However, larger capacity does not guarantee higher accuracy. Given a dataset with limited samples, when a network reaches its peak performance, it is difficult to further improve performance by simply adding more feature maps or stacking more convolutional layers. This means that the discriminability of networks does not increase infinitely with the size of networks. The performance comparison with ResNet110 @cite and ResNet1202 @cite on the CIFAR10 dataset supports our viewpoint. ResNet110 is a 110-layer CNN with 1.7M parameters, and ResNet1202 is a 1202-layer CNN with up to 19.4M parameters. However, ResNet110 achieves a test error of $ 6.43
- Recent studies show multi-hop scheme has a lower power consumption in comparison to one-hop scheme. Using relays reduces the whole WBAN interference and its power consumption @cite @cite @cite . On the other hand, other works prove that TDMA scheme is an attractive solution to avoid interference within an intra-WBAN. These schemes require time synchronization which is infeasible when large number of WBANs coexist @cite @cite @cite . Some efforts focus on co-channel interference analysis of non-overlapping WBANs. They develop a model for efficient network planning and resource management by using geometrical probability approach @cite . Other research works assume one desired and multiple coexisting interferers WBANs. The coordinator of the desired WBAN calculates SINR periodically and afterwards commands its nodes to select an appropriate interferece mitigation scheme @cite .
- Other studies as in @cite analyze performance parameters of a reference WBAN in the presence of another interfering network. Moreover, these parameters have been improved by adoption of an optimized time hopping code assignment strategy. Works in @cite consider a WBAN where the coordinator periodically polls its sensors. The nodes however calculate SINR then select the modulation scheme according to the experienced channel quality. Authors of @cite propose a single-relay cooperative scheme. Where a set of relays computes individually the required transmission power to participate in the cooperative communication. Eventually, the best relay is selected in a distributed fashion.
- Furthermore, authors of @cite explore the problem of interference among multiple coexisting WBANs. The proposed scheme enables two or three WBANs to agree on a common TDMA schedule. This scheme reduces the whole interference amongst them. The work in @cite investigates the problem of coexisting WBANs. It adopts a TDMA scheme within a WBAN and a carrier sensing mechanism to deal with inter-WBAN interference. The coordinator checks the channel whether free before polling the sensor to avoid inter-WBANs interference.
- Authors of @cite consider a sensor network with slot based transmission scheme. This work provides an analytical framework to optimize the size of relay-zone around each source node. Authors of @cite propose a resource allocation scheme for interference mitigation among multiple coexisting WBANs. Each pair of interfering WBANs form an interference region. However, the nodes belonging to this region are later allocated orthogonal sub-channels. Whilst, other nodes that do not belong to this region transmit using the same time slot.
- Given an @math -vertex planar graph @math and a point location for each vertex in @math , Pach and Wenger @cite showed that @math admits a planar polyline drawing with the given vertex locations, where each edge has at most @math bends. They also showed that @math bends are sometimes necessary. @cite and Gordon @cite independently improved the bend complexity to @math . Consequently, for @math , these constructions can be used to draw @math on @math layers with at most @math bends per edge.
- In @cite David Harel and Amir Pnueli distinguish between ''transformational'' and ''reactive'' systems. They define: "A transformational system accepts input, performs transformation on them and produces outputs'' - which is consistent with the definition of a system. They further define ''Reactive systems ... are repeatedly prompted by the outside world and their role is to continuously respond to external inputs ... A reactive system, in general, does not compute or perform a function, but is supposed to maintain a certain ongoing relationship, so to speak, with its environment.''
- In my opinion it is false to conclude from the non-functional relation of the reactive systems to their interaction partners to their somehow non-functional functioning - or ''mode of operation''. Even reactive systems work in a step wise mode and at least for technical systems the simple fact holds that for being constructable, there has to be a function to be implemented - the system function. The main difference between ''transformational'' and ''reactive'' systems in the sense of David Harel and Amir Pnueli is not their functioning but - as they correctly point out - their relation to their environment. For ''transformational'' systems their interfaces towards the interaction provides access to the full system function and thereby provides deterministic interactions. ''transformational'' systems are composed hierarchically by composition according to the definition of sub super systems . By contrast, as I proposed already in @cite , the interfaces of ''reactive'' systems provide access to at least two different projections of the systems, which I call ''roles'' and thereby only provide nondeterministic interactions.
- Manfred Broy describes his component model in several articles (e.g. @cite @cite @cite ). The behavior of a system is given by a set of ''input'' and ''output'' processes, whereby a process in his sense is a finite or infinite set of discrete events. The event concept is not entirely clear, as on the one hand, an event may represent (among others) a state change or some action, but on the other hand he states that an event represents a point in time and thus has no time duration. Also, it remains unclear, whether event sequences like @math and @math are semantically equivalent or not.
- BIP stands for Behavior, Interaction, and Priority @cite @cite @cite . It is a general framework to describe reactive systems from certain parts with a set of rules. In my view, BIP is a way to describe computational systems in the sense of this article: a BIP-component provides states and a transition function. By its priority mechanism, together with a given port, there is supposed to be a unique transition to be selected.
- In his excellent book ''Principles of Cyber-Physical Systems'' @cite Rajeev Alur distinguishes between functional and reactive components in the tradition of David Harel and Amir Pnueli.
- Very substantial effort has been devoted to acoustic modelling in the hidden Markov model (HMM) speech synthesis framework. Amongst the many proposed techniques, we highlight just a few of the most influential. @cite , a minimum generation error training criterion was proposed to address an inconsistency between training and generation criteria, and the lack of interaction between static and dynamic features during training. @cite , the so-called trajectory HMM was proposed to explicitly model the relationships between static and dynamic features. As a complement to improving the acoustic model itself, enhancement techniques such as global variance @cite and modulation spectrum enhancement @cite aim to mitigate the lack of variation in generated parameter trajectories that results from using an incorrect acoustic model. Although such enhancement techniques do not reduce objective error (e.g., lower spectral distortion w.r.t. a natural speech reference), significant improvements in subjective naturalness are obtained. However, none of the above techniques address what is perhaps the most fundamental problem of HMM-based speech synthesis: across-context averaging via decision tree clustering, which has been identified as a major contributing factor to reduced naturalness @cite .
- One way to model contextual constraints is proposed in @cite : a bidirectional long short-term memory (LSTM)-based recurrent neural network (RNN) to map a sequence of linguistic features to the corresponding sequence of acoustic features. An LSTM with a recurrent output layer is proposed in @cite to smooth acoustic features across consecutive frames. A systematic investigation on the architectures of gated recurrent neural network can be found in @cite . These studies formulate speech synthesis as a sequence-to-sequence mapping problem and provide evidence that a better model of speech parameter results in better synthetic speech.
- There are several methods proposed recently that deal with video keyword and description extraction @cite @cite @cite @cite @cite @cite @cite . For instance, Rohrbach al @cite proposed to generate a rich semantic representation of the visual content such as object and activity labels. They employed the Conditional Random Field (CRF) to model all the input visual components. @cite , they extended their work to a three-level-of-detail video description scheme. Then they applied a machine translation framework to generate the natural language using the semantic representation as sources. Unfortunately, this model cannot be used to address our problem due to the extensive manual labelling work required.
- To the best of our knowledge, there is only one work that specifically targets automatic video description problems in surveillance videos. Xu al @cite develop a novel distributed multiple-scene global understanding framework that clusters surveillance scenes by their ability to explain each others behaviours. However, their work only focuses on multiple-scene case and again, utilizes hierarchical probabilistic models.
- The development of career ladders is vital to achieving a prosocial future for crowd work @cite , but currently long-term advancement in crowd work is difficult. One barrier to career development is reputation, since reputation is one of the foremost concerns of workers @cite . Since the availability of higher wage tasks depends on prior ratings, workers focus on maintaining a good reputation @cite . This focus can discourage workers from attempting new tasks that extend beyond their comfort zone and could lower their ratings.
- The elimination process has been found in multiple competitive situations such as multiple round contests and buying decisions @cite . of distractors has been long studied in the psychological literature, since the pioneer work of Tversky @cite . These backward elimination models may offer better explanation than the forward selection when eliminating aspects are available @cite . However, existing studies are mostly on selecting a single best choice. Multiple sequential eliminations are much less studied @cite . Second, most prior work has been evaluated on a handful of items with several attributes, whereas we consider hundreds of thousands of items with thousands of attributes. Third, the cross-field connection with data mining has not been made. The link between choice models and Random Utility Theory has been well-studied since Thurstone in the 1920s, and is still an active topic @cite @cite @cite . Deep neural networks for L2R have been studied in the last two years @cite @cite @cite @cite @cite . Our work contributes a formal reasoning of human choices together with a newly introduced highway networks which are validated on large-scale public datasets against state-of-the-art methods.
- @cite consider a weight function @math on the edges of the graph @math instead of a matrix @math of interferences between colors, defining the interference at a vertex to be the sum of weights of incident monochromatic edges. Although they bounds are similar to ours, using the same techniques, we further show tightness and follow a different, more detailed, scheme. Many other works impose conditions to the colors of the endpoints of any edge. Most of them can be framed into @cite , where vertices at distance @math must get colors at distance @math . Particular instances include @cite , where no two vertices at distance @math can have the same color, @cite , where adjacent vertices must get colors @math apart and vertices at distance @math must get different colors, as well as @cite , where adjacent vertices must get colors @math apart and vertices at distance @math must get colors @math apart.
- For a different setting, in a @cite a distance @math is associated to each particular edge @math , forcing its endpoints to get colors @math apart. A similar flavor has the @cite , where adjacent vertices must get colors whose distance is not in a prescribed set @math , and the @cite , where the vertices have to be partitioned into sets with prescribed pairwise distances.
- Of independent interest are the works on the Frequency Assignment Problem, which use different models to fit the characteristics of specific applications. The bibliography about this problem is too extensive to include here even a selection. Instead, we prefer referring the reader to the comprehensive survey by @cite .
- Many load balancing methods have been suggested to handle this problem in P2P systems. The first work has been done by Chord @cite . They diminish load of overlay nodes by using the concept of virtual servers. They allocate @math virtual servers per physical node and suppose that all overlay nodes are similar. However, their approach does not practically resolve the load balancing problem.
- @cite introduce the novel design to perform fair load distribution in the context of content and resource management in unstructured P2P systems. They collect load objects by the meta-data and after that they compute a reassignment of objects by using that information.
- Karger and Ruhl @cite present dynamic load balancing algorithms without using virtual servers. In their algorithms, lightly loaded nodes should be neighbors of heavily loaded nodes in order to reassign their load. They maximize utilization of load in nodes but they do not completely consider different node capacities. Moreover, It is not clear whether their algorithms are practical or not.
- Many efforts have been made to learn the Granger causality of point processes @cite . For general random processes, a kernel independence test is developed in @cite . Focusing on 1-D point process with simple piecewise constant conditional intensity, a model for capturing temporal dependencies between event types is proposed in @cite . @cite @cite , the inherent grouping structure is considered when learning the Granger casuality on networks from discrete transition process. @cite proposed a continuous-time diffusion network inference method based on parametric cascade generative process. In more general cases, a class of graphical models of marked point processes is proposed in @cite to capture the local independence over various marks. Specializing the work for Hawkes processes, @cite firstly connects Granger causality with impact functions. However, although applying lasso or its variants to capture the intra-structure of nodes @cite is a common strategy, less work has been done on learning causality graph of Hawkes process with sparse-group-lasso as we do, which leads them to be sensitive to noisy and insufficient data.
- Neural language models @cite are an approach to learning embeddings which use a word's representation to its surrounding context. This relies on the fact that words with similar meanings have similar contexts (the distributional hypothesis of language @cite ), which forces their representations to be similar. Intriguingly, it was observed @cite @cite that the geometry of the resulting space preserved between terms. An example is a consistent offset vector existing between Berlin' and Germany', and Dublin' and Ireland', seemingly representing the relationship capital city of country . This property has been exploited to perform knowledge-base completion, for example @cite @cite @cite , however these approaches have restricted their attention to edge-discovery a knowledge graph. To such a graph we therefore developed a model @cite which can combine structured and unstructured data sources while explicitly modelling the types of relationships present in the structured data.
- There has been significant research on network diagnosis methods. capture TCP variables at the CDN to localize performance bottlenecks @cite ; they require OS kernel changes in the critical path, since they require TCP instrumentation outside of the tcp structure. WhyHigh @cite and LatLong @cite further discover client clusters with performance problems and diagnose user-to-CDN path problems at an aggregate level (instead of per-session). @cite and @cite diagnose datacenter network performance using detailed instrumentation (e.g., socket logs and packet traces). At large serving rates, such logging may be infeasible. Network tomography techniques @cite localize bad performance to network interfaces; they assume that the path between two hosts is known -- uncommon in datacenter networks. MonitorRank @cite uses similar tomography-based localization.
- Service and network log mining are common diagnosis methods. Distalyzer compares anomalous logs with known baseline logs @cite for diagnosis. Spectroscope compares two trace logs to understand differences between them @cite . mine log features @cite . Syslogs have been used to study network-specific failures in datacenters @cite @cite @cite , but not for root cause analysis. Prior work has not explored causal discovery for log analysis -- this becomes particularly necessary when looking for a small number of cascading problems in large log volumes.
- The most popular methods for classical phase retrieval---Fienup's algorithm @cite and many related approaches @cite @cite @cite @cite @cite ---are based on alternating projections onto the sets @math (or @math ) and onto the set @math . However, the nonconvexity of @math makes the projection not uniquely defined and possibly hard to compute. The success of such projection-based methods hinges critically on precise prior knowledge (which, in general, will not be available in practice) and on the choice of a projection operator onto @math . Ultimately, convergence to @math (up to global phase) is in general not guaranteed and these methods often fail in practice.
- Further algorithmic techniques to tackle include two different semidefinite relaxation approaches, PhaseLift @cite and PhaseCut @cite . PhaseLift lifts'' into the space of (complex) positive semidefinite rank- @math matrices via the variable transformation @math . Then, the nonlinear constraints @math are equivalent to constraints with respect to the matrix variable @math . By suitably relaxing the immediate but intractable rank-minimization objective, one obtains a convex semidefinite program (SDP). Similarly, PhaseCut introduces a separate variable @math for the phase, allowing to eliminate @math , and then lifts @math to obtain an equivalent problem with a rank- @math -constraint, which can be dropped to obtain a different SDP relaxation of . Despite some guarantees on when these relaxations are tight, i.e., allow for correctly recovering the solution to (again up to a global phase factor), their practical applicability is limited due to the dimension of the SDP that grows quadratically with the problem dimension.
- A recent method that works in the original variable space is the so-called algorithm @cite . Here, is recast as the optimization problem which is approximately solved by a gradient descent algorithm. Note that in the case of complex variables, the concept of a gradient is not well-defined, but as shown in @cite , a strongly related expression termed the Wirtinger derivative'' can be used instead and indeed reduces to the actual gradient in the real case. For the case of i.i.d. Gaussian random measurements, local convergence with high probability can be proven for the method, and a certain spectral initialization provides sufficiently accurate signal estimates for these results to be applicable. Further variants of the Wirtinger Flow (WF) method that have been investigated are the Truncated WF @cite , which involves improving search directions by a statistically motivated technique to filter out components that bear too much'' influence, and Thresholded WF @cite , which allows for improved reconstruction of signals (i.e., ones with only a few significant components or nonzero elements), in particular when the measurements are corrupted by noise.
- Signal sparsity (or compressability) can also be beneficially exploited in phase retrieval methods, cf. @cite @cite @cite @cite @cite . However, to the best of our knowledge, existing methods assume that the signal is sparse itself or sparse with respect to a fixed pre-defined dictionary. This motivates the development of new algorithms and formulations to learn suitable dictionaries and reconstruct input signals from nonlinear measurements.
- The area of static computing resource management has been well studied in the context of grids, clouds, and even multi-clouds @cite . However, the field of dynamic resource management in response to continuously varying workloads, which is especially important for web facing applications @cite , is still in its infancy. Horizontal autoscaling policies are the predominant approach for dynamic resource management and thus they have gained significant attention in recent years.
- Lorido- classify autoscaling policies as and or @cite . The most widely adopted approaches are based on threshold rules for performance metrics (e.g. CPU and RAM utilisation). For each such characteristic the system administrator provides a lower and upper threshold values. Resources are provisioned whenever an upper threshold is exceeded. Similarly, if a lower threshold is reached resources are released. How much resources are acquired or released when a threshold is reached is specified in user defined autoscaling rules. There are different flavours'' of threshold based approaches. For example in Amazon Auto Scaling @cite one would typically use the average metrics from the virtual server farm, while RightScale @cite provides a voting scheme, where thresholds are considered per VM and an autoscaling action is taken if the majority of the VMs agree'' on it. Combinations and extensions of both of these techniques have also been proposed @cite @cite @cite . or approaches try to predict demand changes in order to allocate or deallocate resources. Multiple methods using approaches like reinforcement learning @cite @cite , queuing theory @cite and Kalman filters @cite to name a few have been proposed.
- propose a system for autoscaling web applications in clouds @cite . They monitor the performance of different VM types to infer their capacities. Our approach to this is different, as we inspect the available to each VM CPU capacity and measure the amount of stolen'' CPU instructions by the hypervisor from within the VM itself. This allows us to normalise the VMs' resource capacities to a common scale, which we use to compare them and for further analysis. Furthermore, their approach relies on a workload predictor, while ours is usable even in the case of purely reactive autoscaling.
- use k-means clustering to analyse the workload mix (i.e. the different type of sessions) and then use a queueing model to determine each server's suitability @cite . However, they do not consider the performance variability of virtual machines, which we take into account. Also, they do not select the type of resource (e.g. VM) to provision and assume there is only one type, while this is precisely the focus of our work.
- A part of our work is concerned with automated detection of application behaviour changes through a Hierarchical Temporal Memory (HTM) model. Similar work has been carried out by @cite , who propose a regression based anomaly detection approach. However, they analyse only the CPU utilisation. Moreover they consider that a set of user transactions' types is known beforehand. In contrast, our approach considers RAM as well and does not require application specific information like transaction types. propose the PREPARE performance anomaly detection system @cite . However, their approach can not be used by a cloud client, as it is built on top of the Xen virtual machine manager to which external clients have no access.
- A more cost-effective, less labor intensive and time consuming method is to develop an automatic approach to evaluate the meaningfulness of the set of discovered attributes. In their work, Parikh and Grauman @cite assumed that there is shared structured among meaningful attributes. They proposed an active learning approach that uses probabilistic principal component analyzers (MPPCA) @cite to predict how likely the visual division created by an attribute is nameable. Note that, when an attribute is nameable, it is assumed to be meaningful. Nevertheless, it is not clear how to use their approach to perform a quantitative measurement, as the predictor only decides whether an attribute is nameable or not. In addition, their method is semi-automatic as human intervention is required in the process. Thus, their method is not suitable for addressing our goal (i.e., to automatically evaluate the meaningfulness of attribute sets).
- @cite , a selection method is proposed for attribute discovery methods to assist attribute-based keywords generation for video description from surveillance systems. Their selection method is based on the shared structure assumption. However, this work did not consider quantitative analysis of the meaningfulness of the discovered attributes ( how much meaningful content contained is in a set of attributes). Moreover distances proposed in @cite might not be sufficient to capture all the characteristics of the meaningfulness of attributes as it does not reflect the direct correlation between each meaningful attribute with the discovered ones.
- There exist many studies to develop learning algorithms for object localization based on the weakly labeled dataset. Most previous works can be interpreted as the same framework, which use candidate regions extracted from an image and then select the correct localization among those regions @cite @cite @cite @cite . In this work, recent methods based on CNN are considered since they have shown a promising performance on weakly supervised object localization @cite @cite @cite .
- In a weakly supervised object localization task, we should find common features within a set of intra-class images, discriminate those intra-class features with each other, and define the most probable region in terms of target class. Transfer learning based on well pre-trained networks (preliminarily trained on different-but-similar datasets) helps to perform those challenging tasks, since pre-trained CNN properly defines ROIs based on discriminative convolutional filters already learned from semantically similar datasets @cite @cite @cite .
- @cite , convolutional feature extraction layers and adaptation layers are used for object localization. The convolutional feature extraction layers are pre-trained from the ImageNet dataset @cite , so it appropriately extracts class-specific features from semantically similar datasets. From those feature maps, adaptation layers build up the class number of score maps. Adaptation layers consist of additional convolutional and global max pooling layers. The convolutional layer in the adaptation layers generates per-class score maps, and the most probable positions (with the highest activation) with respect to each class are pooled at each iteration in order to compute and backpropagate errors in a training phase. Since all the layers in this architecture are convolutional or pooling layers, rescaled input images can generate corresponding sizes of score maps. Each score map stands for the confidence level of existence of each object, i.e. per-class localization maps.
- Inferring segmentation map is more challenging compared to object localization, since it should infer the class label per each pixel. @cite , the authors propose the weakly supervised segmentation framework which uses datasets only with image-level labels in a training phase. A convolutional network pre-trained on the ImageNet dataset @cite is used to extract feature maps from an input image. The class number of extracted feature maps () are aggregated into a single -dimensional vector in the proposed aggregation layer to be compared with the true label. Compared to @cite , the proposed aggregation layer adopts a pooling method which is a smooth version of the max pooling in order to fairly explore the entire feature maps. In an inference stage, the class number of feature maps extracted from an input image are used as initial maps for segmentation. By adding some segmentation priors (image-level prior and smoothing prior), they achieve good segmentation results compared to previous works related to weakly supervised object segmentation.
- Global hand-crafted features describe an image as a whole in terms of color, texture and shape distributions @cite . Some notable examples of global features are color histograms @cite , spatial histogram @cite , Gabor filters @cite , co-occurrence matrices @cite @cite , Local Binary Patterns (LBP) @cite , Color and Edge Directivity Descriptor (CEDD) @cite , Histogram of Oriented Gradients (HOG) @cite , morphological operators like granulometries information @cite @cite @cite , Dual Tree Complex Wavelet Transform (DT-CWT) @cite @cite and GIST @cite . Readers who would wish to deepen the subject can refer to the following papers @cite @cite @cite @cite .
- Local hand-crafted descriptors like Scale Invariant Feature Transform (SIFT) @cite @cite provide a way to describe salient patches around properly chosen key points within the images. The dimension of the feature vector depends on the number of chosen key points in the image. A great number of key points can generate large feature vectors that can be difficult to be handled in the case of a large-scale image retrieval system. The most common approach to reduce the size of feature vectors is the (BoVW) @cite @cite . This approach has shown excellent performance not only in image retrieval applications @cite but also in object recognition @cite , image classification @cite and annotation @cite . The idea underlying is to quantize by clustering local descriptors into . Words are then defined as the centers of the learned clusters and are representative of several similar local regions. Given an image, for each key point the corresponding local descriptor is mapped to the most similar visual word. The final feature vector of the image is represented by the histogram of the its visual words.
- A CNN that has been trained for solving a given task can be also adapted to solve a different task. In practice, very few people train an entire CNN from scratch, because it is relatively rare to have a dataset of sufficient size. Instead, it is common to take a CNN that is pre-trained on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories @cite ), and then use it either as an initialization or as a fixed feature extractor for the task of interest @cite @cite . In the latter case, given an input image, the pre-trained CNN performs all the multilayered operations and the corresponding feature vector is the output of one of the fully connected layers @cite . This use of CNNs have demonstrated to be very effective in many pattern recognition applications @cite .
- A basic retrieval scheme takes as input the visual descriptor corresponding to the query image perfomed by the user and it computes the similarity between such a descriptor and all the visual descriptors of the database of features. As a result of the search, a ranked list of images is returned to the user. The list is ordered by a degree of similarity, that can be calculated in several ways @cite : Euclidean distance (that is the most used), Cosine similarity, Manhattan distance, @math -square distance, etc. @cite .
- A recursive neural network has been applied to sentence classification earlier (see, e.g., @cite .) In this approach, a composition function is defined and recursively applied at each node of the parse tree of an input sentence to eventually extract a feature vector of the sentence. This model family is heavily dependent on an external parser, unlike all the other models such as the ConvRec proposed here as well as other related models described above. It is also not trivial to apply the recursive neural network to documents which consist of multiple sentences. We do not consider this family of recursive neural networks directly related to the proposed model.
- To date, there are not many solutions for traffic that make full use of the intelligent agent concept. However, the multi-agent system approach has become recognized as a convenient approach for modelling and simulating complex systems @cite . Also, it has grown enormously not only applied to traffic but also to transportation in general terms @cite .
- Regarding this simulation tools some examples of multi-agent system approaches for traffic lights control can be seen in @cite , @cite , @cite and @cite . Simulators used in these works were Aimsun, ITSUMO, VisSim and ITSUMO, respectively.
- Begelfor and Werman @cite popularized the use of Grassmannian subspaces as an affine invariant. Their work focused on developing clustering algorithms on the Grassmann manifold, but they did not utilize this invariance to solve the correspondence problem. A method that did try to address this correspondence problem through subspace invariance was @cite . The robustness of their method was never evaluated, and, moreover, their approach of using QR factorizations of rank-deficient orthogonal projection matrices is quite different from our proposed two-stage approach. Finally, have shown the effectiveness of Grassmannian representations for object recognition @cite and their use has expanded to other vision domains @cite .
- Algorithms that address non-rigid transformations inherently have an advantage over strictly affine methods, here we highlight some notable non-rigid methods that can be used to address the affine correspondence problem. @cite form an equi-affine (volume preserving) invariant Laplacian for surfaces, with applications in shape matching. Their method, however, requires explicit metric tensor calculations on mesh surfaces which can lead to further complications of singular points on the surfaces. We avoid surfaces parameterization issues, offering an approach that works directly on point sets. Popular non-rigid matching algorithms include CPD @cite , gmmreg @cite and TPS-RPM @cite . Graph matching methods have also been quite popular recently. Zhou and de la Torre @cite @cite presented the factorized graph matching (FGM) algorithm in which the affinity matrix is factorized as a Kronecker product of smaller matrices. Although the factorization of affinity matrices makes large graph matching problems more tractable, the method is still computationally expensive.
- In this work, our goal is to understand FIFO queues in wireless networks and develop efficient flow control and scheduling policies for such a setup. In the seminal paper @cite , the authors analyze FIFO queues in an input queued switch. They show that the use of FIFO queues in that context limits the throughput to approximately 58 Backpressure routing and scheduling framework has emer-ged from the pioneering work @cite @cite , which has generated a lot of research interest @cite ; especially for wireless ad-hoc networks @cite @cite @cite @cite @cite @cite . Furthermore, it has been shown that backpressure can be combined with flow control to provide utility-optimal operation guarantee @cite @cite . Such previous work mainly considered per-flow queues. However, FIFO queueing structure, which is the focus of this paper, is not compatible with the per-flow queueing requirements of these routing and scheduling schemes.
- The strengths of backpressure-based network control have recently received increasing interest in terms of practical implementation. Multi-path TCP scheme is implemented over wireless mesh networks in @cite for routing and scheduling packets using a backpressure based heuristic. At the link layer, @cite @cite @cite propose, analyze, and evaluate link layer backpressure-based implementations with queue prioritization and congestion window size adjustment. Backpressure is implemented over sensor networks @cite and wireless multi-hop networks @cite . In these schemes, either last-in, first-out queueing is employed @cite or link layer FIFO queues are strictly controlled @cite to reduce the number of packets in the FIFO queues, hence HOL blocking.
- In backpressure, each node constructs per-flow queues. There is some work in the literature to stretch this necessity. For example, @cite , @cite propose using real per-link and virtual per-flow queues. Such a method reduces the number of queues required in each node, and reduces the delay, but it still needs to construct per-link queues. Similarly, @cite constructs per-link queues in the link layer, and schedule packets according to FIFO rule from these queues. Such a setup is different than ours as per-link queues do not introduce HOL blocking. The main differences in our work are: (i) we consider FIFO queues shared by multiple flows where HOL blocking occurs as each flow is transmitted over a possibly different wireless link, (ii) we characterize the stability region of a general scenario where an arbitrary number of FIFO queues, which are served by a wireless medium, are shared by an arbitrary number of flows, and (iii) we develop efficient resource allocation schemes to exploit achievable rate in such a setup.
- Recurrent neural network (RNNs) have been successfully applied to various sequence modeling and sequence-to-sequence transduction tasks. The latter have assumed several guises in the literature such as machine translation @cite , sentence compression @cite , and reading comprehension @cite . A key contributing factor to their success has been the ability to handle well-known problems with exploding or vanishing gradients @cite , leading to models with gated activation functions @cite @cite , and more advanced architectures that enhance the information flow within the network @cite @cite @cite . A remaining practical bottleneck for RNNs is memory compression @cite : since the inputs are recursively combined into a single memory representation which is typically too small in terms of parameters, it becomes difficult to accurately memorize sequences @cite . In the encoder-decoder architecture, this problem can be sidestepped with an attention mechanism which learns soft alignments the decoding states and the encoded memories @cite . In our model, memory and attention are added a sequence encoder allowing the network to uncover lexical relations between tokens.
- Several results related to the visualization of pseudoline arrangements are known. In , pseudolines are drawn on parallel horizontal lines, with crossings on short line segments that connect pairs of horizontal lines @cite . Using a method based on compaction of wiring diagrams, the graphs of pseudoline arrangements may be given straight line drawings in small grids @cite . The planar dual graph of a weak pseudoline arrangement may be characterized as having drawings in which each bounded face is a centrally symmetric polygon @cite . The pseudoline arrangements in which each pseudoline is a translated quadrant can be used to visualize , representing the possible states of knowledge of a human learner @cite . Researchers in graph drawing have also studied force-directed methods for schematizing systems of curves representing metro maps by replacing each curve by a spline; these curves are not necessarily pseudolines, but they typically have few crossings @cite .
- This paper is highly related to studies in 3D object detection for robotic manipulation and assembly and the literature review concentrates on the perception aspect. For general studies on robotic grasping, manipulation, and assembly, refer to @cite , @cite , and @cite . The literature review emphasizes on model-based approaches since the paper is motivated by next-generation mechanical assembly and is about the industrial applications where precision is crucial and object models are available. For model-less studies, refer to @cite and @cite . For appearance-based studies, refer to @cite @cite , and @cite . Moreover, learning approaches are no reviewed since they are not precise. Refer to @cite and @cite if interested.
- Some of the most common choices of features include corner features @cite applied in @cite and @cite , line features applied in @cite , @cite and @cite , cylinder features applied in @cite and @cite , and SIFT features @cite applied in @cite and @cite . Especially, @cite stated clearly the two stages of model-based detection using RGB images: (1) The modeling stage where the textured 3D model is recovered from a sequence of images; (2) The detection stage where features are extracted and matched against those of the 3D models. The modeling stage is based on the algorithms in @cite . The detection stage is open to different features, different polynomial solving algorithms, and some optimizations like Levenberg-Marquardt @cite and Mean-shift @cite , etc. @cite compared the different algorithms in the second stage.
- Some other work uses visual tracking to correct the noises caused by fast motions and improve the precision of initial matches. The fused modals include the RGB image modal and the motion modal where the later one could be either estimated using image sequences or third-part sensors like Global Positioning System (GPS) or gyros. @cite is one representative work which fuses model motion (model-based tracking) and model detection in RGB images to refine object poses. @cite fuses gyro data and line features of RGB images to reinforce the pose estimation for head-mounted displays. @cite uses gyro data, point descriptors, and line descriptors together to improve the performance of pose estimation for outdoor applications.
- The problem of sparsity is well known in the Machine Learning field. For word alignment, the instance of the rare word problem is studied in @cite and @cite . In these papers, rare words act as garbage collectors'' that tend to align to too many target words.
- To deal with rare word problems, many researches utilized the linguistic information. One of the earliest works is @cite which used an external dictionary to improve the word alignment models. Experiments show that this method also solves the problem of rare words. Another approach utilized the information provided by morphological analysis. Some of them are @cite , @cite , @cite . These works do not treat word as the smallest unit of translation. Instead, they computed statistics on morphemes, which are smaller parts of constructing words. This helps reducing sparsity by having better statistics of the rare words which are composed of popular morphemes as popular words. However, when a word is really rare, in which its morphemes are rare as well, these methods are no longer applicable. Furthermore, the dependencies on the languages also limits the scope of these methods.
- The problem of rare words is also reduced with methods involving word class. IBM Model 4 ( @cite ) constrains word distortion on the class of source words. Distortion indicates how likely two words are translations of each other based on theirs positions. A better distortion estimation would result in a better alignment. Another work from @cite utilized the word classes of both source words and target words. It estimated the translation probability of pairs of classes as well as the word translation probability. This class translation probability is usually more reliable than the word translation probability. Aligning will be better, especially in the case of rare words when it encourages alignments to follow the class translation probability. Word classes may be part-of-speech tags which is usually obtained by running part-of-speech tagging software (such as the Stanford tagger in @cite ) or more usually the classes obtained by running a clustering algorithm which is language independent as in @cite .
- Smoothing is a popular technique to solve the problem of sparsity. Language Model, which is another problem of Machine Translation, has a variety of smoothing methods. The classic paper @cite gives a very good study of this solution for the Language Model problem. A large number of smoothing methods with extensive comparisons amongst them will be analyzed carefully in the paper.
- However, there is still lack of studies for applying smoothing techniques to the word alignment problem. The work of this paper is mostly an extended work of @cite which is the earliest study for this matter. In that work, Moore has an intensive study of many improvements to IBM Model 1. Additive smoothing is the first of the three improvements, which directly attempts to solve the sparsity problem. In spite of the simplicity, the method has good performance. The smoothing technique Moore applied is additive smoothing. For every pair of word @math , it is assumed that @math and @math were paired in a constant number of times before the training data is observed. Therefore, in each estimation, this assumption will be taken into account and the prior number of times, which they are paired, will be added appropriately. The constant prior count is tuned manually or learnt from some additional annotated data.
- Basic CNN architectures have been applied to texture recognition such as @cite , in which a simple four layers network was used in the early stage of deep learning to classify the Brodatz database. More recently, Hafemann @cite applied CNN to a forest species classification, similar to a texture classification problem. While more complex and more accurate than @cite , this approach still does not take the characteristics of texture images (statistical properties and repeated patterns) into consideration as it is a simple application of a standard CNN to a texture dataset. @cite demonstrated the relevance of densely extracting texture descriptors from a CNN with the FV-CNN. They obtain impressive results on both texture recognition and texture recognition in clutter datasets. This approach is well suited to region recognition as it requires computing the convolution layer output once and pooling regions with FV separately. However, the CNN part of the FV-CNN does not learn from the texture dataset. A pre-trained network extracts the outputs of the convolution layers without being finetuned on the texture dataset. The fully connected layers are replaced with FV encoding and SVM classification, also making this approach computationally expensive.
- In related contexts, MCMC has been applied to fit continuous-time stochastic volatility models to financial time series, where the price is a diffusion process whose volatility is a latent mean reverting jump process or the sum of a number of such processes (called a superposition model). In this line of research a missing data methodology is employed whereby the observed process is augmented with one or more latent marked Poisson processes and the MCMC procedure generates posterior samples in this high dimensional augmented state space. Examples include @cite , @cite and @cite . Since energy prices additionally exhibit jumps directly in their paths, MCMC has been applied to extensions of these models in which a diffusion process with stochastic volatility is superposed with a jump process, see in the context of electricity and for gas prices. Technically the latter two papers estimate a discrete approximation of the models whereas in this study we pursue inference for continuous time models.
- A lot of research has been done into individual wireless mesh network building blocks. Among them are routing @cite @cite @cite , security @cite , and analyses of topologies, performance, mobility @cite @cite @cite @cite @cite . But research into community network management solutions and best practices still remains scarce. This is why most of the related work in this area comes from the individual community networks which have each developed its own solutions, practices and philosophy.
- Generally, pruning is not considered when constructing random forests as overfitting is avoided by constructing an ensemble of trees. The ensemble approach is a strong approach to avoiding overfitting, however test-time budget constraint problems require consideration of both cost and accuracy. Kulkarni and Sinha @cite provide a survey of methods to prune random forests in order to reduce ensemble size. However, these methods do not explicitly account for feature costs.
- Thorup @cite also gave an oracle for planar which for polynomially bounded edge weights achieves @math space and close to @math query time. Exact oracles for planar digraphs with tradeoff between space and query time have been studied but require near-quadratic space for constant or near-constant query time @cite @cite .
- For a general undirected @math -vertex graph @math , Thorup and Zwick showed that for any parameter @math , there is a @math -approximate distance oracle for @math with @math space and @math query time which is believed to be essentially optimal due to a girth conjecture of Erd o s @cite . Variations and slight improvements have since been presented; see, e.g., @cite @cite @cite @cite @cite .
- The problem of training neural networks that generate images is studied in @cite . proposed a convolutional network mapping shape, pose and transformation labels to images for generating chairs. It is able to control these factors of variation and generate high-quality renderings. We also generate chair renderings in this paper, but our model adds several additional features: a deep encoder network (so that we can generalize to novel images, rather than only decode), distributed representations for appearance and pose, and recurrent structure for long-term prediction.
- A related line of work to ours is disentangling the latent factors of variation that generate natural images. Bilinear models for separating style and content are developed in @cite , and are shown to be capable of separating handwriting style and character identity, and also separating face identity and pose. The disentangling Boltzmann Machine (disBM) @cite applies this idea to augment the Restricted Boltzmann Machine by partitioning its hidden state into distinct factors of variation and modeling their higher-order interaction. The multi-view perceptron @cite employs a stochastic feedforward network to disentangle the identity and pose factors of face images in order to achieve view-invariant recognition. The encoder network for IGN is also trained to learn a disentangled representation of images by extracting a graphics code for each factor. @cite , the (potentially unknown) latent factors of variation are both discovered and disentangled using a novel hidden unit regularizer. Our work is also loosely related to the DeepStereo" algorithm @cite that synthesizes novel views of scenes from multiple images using deep convolutional networks.
- Random Walks and Semantic Networks. The motivation to address random walk centrality comes from works that have used the notion of random walks and related it with semantics. The work of @cite compares the functioning of the human mind when searching for memories with a random walk in a semantic network, as both present a similar behavior. They conclude that these results can help clarify the possible mechanisms that could account for PageRank predicting the prominence of words in semantic memory. @cite use random walks to determine the semantic relatedness of two elements. In particular, they use a personalized PageRank algorithm with a custom teleport vector. They conclude that random walk combined with personalized PageRank is a feasible and potentially fruitful mean of computing semantic relatedness for words and texts. @cite introduce a new measure of lexical relatedness based on the divergence of the stationary distributions computed using random walks over graphs extracted from WordNet. All these works have been valuable sources of inspiration for our method.
- There is considerable prior work on the application of tensor models to temporal data, e.g., EEG data, and overviews can be found in @cite and @cite . In that work, prediction is typically not in focus, but instead one attempts to understand essential underlying temporal processes by analysing the derived latent representations.
- Our approach can also be related to the neural probabilistic language model @cite , which coined the term . It can be considered an event model where the occurrence of a word is predicted based on most recent observed words using a neural network model with word representations as inputs. In our approach we consider that several events might be observed at a time instance and we consider a richer family of latent factor representations.
- There is considerable recent work on dynamic graphs @cite @cite @cite @cite with a strong focus on the Web graph and social graphs. That work is not immediately applicable to KGs but we plan to explore potential links as part of our future work.
- There have been a few attempts in accelerating the test-phase computation of convolutional networks, and many are inspired from the low-rank decomposition. @cite presented a series of low-rank decomposition designs for convolutional kernels. Similarly, CP-decomposition was adopted in @cite to transform a convolutional layer into multiple layers with lower complexity. @cite @cite considered the subsequent nonlinear units while learning the low-rank decomposition. @cite applied group-wise pruning to the convolutional tensor to decompose it into the multiplications of thinned dense matrices. Recently, fixed-point based approaches are explored in @cite @cite . By representing the connection weights (or even network activations) with fixed-point numbers, the computation can greatly benefit from hardware acceleration.
- Another parallel research trend is to compress parameters in fully-connected layers. @cite randomly remove connection to reduce network parameters. Matrix factorization was adopted in @cite @cite to decompose the weighting matrix into two low-rank matrices, which demonstrated that significant redundancy did exist in network parameters. @cite proposed to use dark knowledge (the response of a well-trained network) to guide the training of a much smaller network, which was superior than directly training. By exploring the similarity among neurons, @cite proposed a systematic way to remove redundant neurons instead of network connections. In @cite , multiple fully-connected layers were replaced by a single Fastfood'' layer, which can be trained in an end-to-end style with convolutional layers. @cite randomly grouped connection weights into hash buckets, and then fine-tuned the network with back-propagation. @cite combined pruning, quantization, and Huffman coding to achieve higher compression rate. @cite adopted vector quantization to compress the weighing matrix, which was actually a special case of our approach (apply Q-CNN without error correction to fully-connected layers only).
- Goel and Mehta @cite introduced the random vertex-arrival model. In this model, the adversary may choose the worst instance of a graph, but the online vertices arrive in a random order. The greedy algorithm is already @math -competitive for this problem, as the analysis reduces to @cite . Later works @cite @cite showed that the ranking algorithm has a competitive ratio of at least @math , beating the bounds for adversarial vertex arrival model. There is still a gap between known upper and lower bounds, and closing this gap remains an open problem.
- In the edge arrival model, a fixed bipartite graph is chosen by an adversary and its edges are revealed one by one to an online algorithm that is trying to find a maximum matching. If the edge arrival is adversarial, this problem captures the adversarial vertex arrival model as a special case: constraint the edges incident to a vertex to appear together. The greedy algorithm has a competitive ratio of half and a natural open question is whether we can beat half. The current best hardness result for adversarial edge arrival is @math , even when the algorithm is allowed to drop edges (see @cite ).
- On the other hand, for uniformly random edge arrival Konrad, Magniez, and Mathieu @cite gave the first single pass algorithm that obtains a @math -competitive ratio for bipartite matching in the semi-streaming setting. Their algorithm crucially used the ability to revoke earlier decisions. One of the contributions in this paper is to show that a variant of the greedy algorithm, which appears simple in hindsight, achieves a competitive ratio better than half in the more restrictive online model.
- It is particularly worthy of comparing our method with the work in @cite @cite , where temporary branches including classifiers are attached to intermediate layers, and helps to propagate the supervision information to lower layers with shortcuts. However, such multi-loss mechanism neglects information reduction due to long-term propagation, and the adverse effect of less relevant information for lower layers. Different from it, our method can effectively preserve relevant information and meanwhile restrain the adverse effect of less relevant information, thus obtain a model with better performance. In @cite @cite , powerful networks are obtained by adopting new structures, i.e., Inception module and Residual block, which are concurrent with our work, and also attend the ILSVRC 2015 Challenge. The two structures implement shortcut connections in different ways, however long-term propagation still exists when training a deeper network. Therefore, our contribution is orthogonal to these work, and network performance can be further improved benefitting from our method.
- Variational integrators in mechanical systems start by considering mechanics from a variational point of view, following remarkable works of Lagrange and Hamilton @cite . The Hamilton's principle or the least action principle allows to cast the Newton's framework into a geometric viewpoint in which the path followed by the physical system in the configuration space has optimal geometric properties analogously to the notion of geodesics on curved surfaces @cite . Therefore, we can design numerical integrators that exploit the geometric structure behind mechanical systems, which are named geometric integrators @cite @cite . A special class of geometric integrators, called variational integrators, discretized the variational formulation of mechanics generating iterative schemes to compute an approximation for the path of the physical system with any order of accuracy. Besides, this discrete geometric framework can handle constraints, external and dissipative forces making variational integrators both versatile and powerful @cite @cite @cite .
- The key elements in variational time integrators are the discrete action sum, the discrete Euler-Lagrange equations and the discrete Noether's theorem that were clearly understood due to early works (see @cite and references therein). Numerical aspects and convergence properties were specifically considered in @cite @cite .
- On the other hand, traditional SPH formulations for fluids rely on standard conservation equations and a particle framework to discretize the corresponding Navier-Stokes equations, generating models that treat the continuum fluid as a system of particles and recover continuous fields by using interpolation kernels @cite . Variational formulations of SPH for fluid applications have been proposed, where the constitutive equation of the fluid is given by an internal energy term which is a function of the density @cite @cite . These formulations provide a basis to discuss momentum preserving properties of SPH approaches. In this paper, they are used to derive variational time integrators for SPH by computing the discrete action and its stationary point, as we shall see in the next sections.
- In @cite , the inventors of GSN address the confidence issue, by proposing to split a traditional safety case in two pieces. The first is the safety argument, showing all evidences, and the second is a confidence argument that addresses confidence in evidences, contexts, and individual inferences. This confidence argument is also represented with GSN. It starts by adding to the safety case some possible uncertainty sources, which are called Assurance Claim Points (ACP), that are attached to inferences (the arrows connecting claims), contexts (explanatory information), or solutions. Then, for each ACP, an argumentation mainly focuses on demonstrating that the ACP is trustworthy and appropriate, which is built using GSN. Another proposal @cite , is based on the ACP but only focuses on Context and Solution elements. The authors propose to use a map (Common Characteristic Map) as a check list to identify sources of uncertainties, with recursive dependencies. For instance, if a safety case includes a solution which is a "Process result", they propose the generic uncertainties related to "the use of a language", "the use of a tool", "the use of a mechanism", "the involved artifacts", etc. All those characteristics are then refined, with possible recursive dependencies.
- For strongly convex regularizers, current state-of-the-art for empirical loss minimization is randomized coordinate ascent on the dual (SDCA) @cite and its accelerated variants, e.g., @cite . In contrast to primal stochastic gradient descent (SGD) methods, the SDCA family is often preferred as it is free of learning-rate parameters and has faster (geometric) convergence guarantees. Interestingly, a similar trend in coordinate solvers has been observed in recent Lasso literature, but with the roles of primal and dual reversed. For those problems, coordinate descent methods on the primal have become state-of-the-art, as in @cite and extensions @cite ; see, e.g., the overview in @cite . However, primal-dual convergence rates for unmodified coordinate algorithms have to our knowledge been obtained only for strongly convex regularizers to date @cite @cite .
- Coordinate descent on @math -regularized problems with @math can be interpreted as the iterative minimization of a quadratic approximation of the smooth part of the objective (as in a one-dimensional Newton step), followed by a shrinkage step resulting from the @math part. In the single-coordinate update case, this is at the core of @cite @cite , and widely used in, e.g., solvers based on the primal formulation of @math -regularized objectives @cite @cite @cite @cite @cite . When changing more than one coordinate at a time, again employing a quadratic upper bound on the smooth part, this results in a two-loop method as in @cite for the special case of logistic regression. This idea is crucial for the distributed setting.
- ADMM @cite , proximal gradient descent, and quasi-Newton methods such as L-BFGS and are also often used in distributed environments because of their relatively low communication requirements. However, they require at least a full (distributed) batch gradient computation at each round, and therefore do not allow the gradual trade-off between communication and computation provided by . The works of @cite and @cite have obtained encouraging results for distributed systems employing coordinate descent variants on @math -problems. The latter approach distributes both columns and rows of the data matrix and can be extended to Lasso. However it only provides asymptotic improvement per step, and no convergence rate. We include experimental comparisons with ADMM, prox-GD, and orthant-wise limited memory quasi-Newton (OWL-QN) @cite , an L-BFGS variant that can handle @math regularization @cite , but which has no convergence rate.
- The current state of the art for empirical loss minimization with strongly convex regularizers is randomized coordinate ascent on the dual objective --- Stochastic Dual Coordinate Ascent (SDCA) @cite . In contrast to primal SGD methods, the SDCA algorithm family is often preferred as it is free of learning-rate parameters, and has faster (geometric) convergence guarantees. This algorithm and its variants are increasingly used in practice @cite @cite . On the other hand, primal-only methods apply to a larger problem class, not only of form that enables formation of dual problem as considered here.
- * Distributed Batch Solvers. With traditional batch gradient solvers not being competitive for the problem class , improved batch methods have also received much research attention recently, in the single machine case as well as in the distributed setting. In distributed environments, often used methods are the alternating direction method of multipliers (ADMM) @cite as well as quasi-Newton methods such as L-BFGS, which can be attractive because of their relatively low communication requirements. Namely, communication is in the order of a constant number of vectors (the batch gradient information) per full pass through the data.
- ADMM also comes with an additional penalty parameter balancing between the equality constraint on the primal variable vector @math and the original optimization objective @cite , which is typically hard to tune in many applications. Nevertheless, the method has been used for distributed SVM training in, e.g., @cite . The known convergence rates for ADMM are weaker than the more problem-tailored methods mentioned we study here, and the choice of the penalty parameter is often unclear in practice.
- Standard ADMM and quasi-Newton methods do not allow a gradual trade-off between communication and computation available here. An exception is the approach of Zhang, Lee and Shin @cite , which is similar to our approach in spirit, albeit based on ADMM, in that they allow for the subproblems to be solved inexactly. However, this work focuses on L2-regularized problems and a few selected loss functions, and offers no complexity results.
- By making use of the primal-dual structure in the line of work of @cite @cite @cite @cite @cite , the and frameworks proposed here are the first to allow the use of any local solver --- of weak local approximation quality --- in each round. Furthermore, the approach here also allows more control over the aggregation of updates between machines. The practical variant of the DisDCA Algorithm of @cite , called DisDCA-p, also allows additive updates but is restricted to coordinate decent (CD) being the local solver, and was initially proposed without convergence guarantees. The work of @cite has provided the first theoretical convergence analysis for an ideal case, when the distributed data parts are all orthogonal to each other --- an unrealistic setting in practice. DisDCA-p can be recovered as a special case of the framework when using CD as a local solver, if @math and when using the conservative bound @math , see also @cite @cite . The convergence theory presented here therefore also covers that method, and extends it to arbitrary local solvers.
- There are some recent works that propose to exploit the FD communications for MAC-level channel access in multi-user wireless networks @cite -- @cite . In @cite , the authors develop a centralized MAC protocol to support asymmetric data traffic where network nodes may transmit data packets of different lengths, and they propose to mitigate the hidden node problem by employing a busy tone. To overcome this hidden node problem, propose to adapt the standard 802.11 MAC protocol with the RTS CTS handshake in @cite . Moreover, in @cite extend this study to consider interference between two nodes due to their concurrent transmissions. Different from conventional wireless networks, designing MAC protocols in CRNs is more challenging because the spectrum sensing function must be efficiently integrated into the MAC protocol. In addition, the self-interference must be carefully addressed in the simultaneous spectrum sensing and access to mitigate its negative impacts on the sensing and throughput performance.
- The FD technology has been employed for more efficient spectrum access design in cognitive radio networks @cite -- @cite where SUs can perform sensing and transmission simultaneously. In @cite , a FD MAC protocol is developed which allows simultaneous spectrum access of the SU and PU networks where both PUs and SUs are assumed to employ the @math -persistent MAC protocol for channel contention resolution and access. This design is, therefore, not applicable to the hierarchical spectrum access in the CRNs where PUs should have higher spectrum access priority compared to SUs.
- In our previous work @cite , we propose the FD MAC protocol by using the standard backoff mechanism as in the 802.11 MAC protocol where we employ concurrent FD sensing and access during data transmission as well as frame fragmentation. Moreover, engineering of a cognitive FD relaying network is considered in @cite @cite , where various resource allocation algorithms to improve the outage probability are proposed. In addition, the authors in @cite develop the joint routing and distributed resource allocation for FD wireless networks. In @cite , study the distributed power allocation for a hybrid FD HD system where all network nodes operate in the HD mode but the access point (AP) communicates by using the FD mode. In practice, it would be desirable to design an adaptable MAC protocol, which can be configured to operate in an optimal fashion depending on specific channel and network conditions. This design will be pursued in our current work.
- Various approaches have been proposed to exploit nanophotonics for on-chip communication networks. Development of efficient optical data buffers is still a challenging problem, which makes it quite hard to construct a fully optical packet-switched network. As a result, @cite propose a hybrid approach, in which large data packets are transferred using an optical circuit-switched network whereas the optical network is controlled by an electronic packet-switched network. @cite consider the architecture proposed in @cite and propose a predictive switching and reservation technique to reduce its path setup latency. In some other hybrid architectures such as @cite , local communications are carried out by an electrical network, whereas long distance communications utilize optical links. @cite exploit non-blocking photonic switches as well as light-weight electronic routers to decrease the latency and power overheads of hybrid architectures. Garc ' i a- @cite propose a set of policies to manage hybrid networks consisting of ring-based photonic and electrical mesh sub-networks. The proposed policies use different criteria, such as message size, distance, and photonic ring availability, to decide which sub-network to be used for each message.
- ATAC @cite is another hybrid architecture in which a baseline electrical 2D mesh is used for close-range point-to-point communications, whereas a ring-like optical network is used for long-distance and collective communications. The optical interconnect functions in a similar way as a broadcast bus, and contention is resolved by assigning unique wavelengths to senders. SUOR, proposed by @cite , uses a circuit-switched ring-based optical NoC with a control sub-system responsible for arbitration and flow control. The control sub-system sets up the path from source to destination based on the requests received from nodes. SOUR also takes advantage of channel segmentation by dividing one waveguide into multiple non-overlapping sections that can support multiple transactions simultaneously.
- @cite use 4-port optical switches to build a photonic routing structure called @math , which provides contention-free communication among cores through wavelength routing techniques. Each pair of cores communicate through fixed and predefined wavelengths routed passively by the @math -router. is the architecture proposed by @cite , where all-optical switches are used to implement contention-free wavelength-based passive routing of optical streams. Contention-free communication is carried out by assigning each node with a unique wavelength for data reception. This way, contention is confined to the end-points and is resolved by an electrical arbitration scheme. A scalable wavelength-routed optical NoC based on the Spidergon topology is presented in @cite . It uses per-receiver wavelengths in the data network to prevent network contention, and adopts per-sender wavelengths in the control network to avoid end-point contention. @cite also propose an all-optical design exploiting a mesh-like topology.
- The architecture proposed by K @cite is an instance of an optical crossbar where several Single Write, Multiple Read (SWMR) busses provide full connection among nodes. Each SWMR bus is dedicated exclusively to one node for sending data to others, whereas all other nodes can read data from all busses. A comparison study of worst-case optical losses for various crossbar implementations is presented in @cite . @cite introduce , which is a hybrid crossbar with nodes partitioned into clusters. While intra-cluster communication is done using smaller electrical crossbars, inter-cluster communication is realized by a SWMR optical crossbar. Unlike the architecture proposed in @cite , optical packets are not broadcast to all nodes, but rather, the intended receiver is selected by auxiliary prior to each communication.
- is an all-optical crossbar topology, proposed by @cite , which implements a crossbar consisting of Multiple Write, Single Read (MWSR) shared busses. Each bus is dedicated exclusively to one node for receiving data, whereas all nodes can write data on all busses. Corona takes advantage of an optical token-based arbitration mechanism to resolve contention among nodes for sending data on the same bus. To address the fairness issues, a fair token slot mechanism is presented in @cite . @cite employ MWSR token-ring busses of Corona to implement rows and columns of an optical 2D torus, and modified the token-based arbitration scheme so as to support virtual channel flow control.
- Furthermore, @cite derived backward views for a series of increasingly complex forward views. The derivation of the true online TD( @math ) equations in Appendix is similar to those derivations.
- In a second line of work, Sabato and Shalev-Schwartz suggest ranking categorical features according to their generalization error. However, their approach is limited to considering K-way splits (i.e., a leaf for every categorical value), as opposed to binary splits which group categories, which characterize CART and other tree methods commonly used in statistics. Their appraoch, which is also solely applicable for classification trees, offers strong theoretical results and low computational complexity (as it does not perform actual splitting), albeit in the K-way scenario which is significantly simpler theoretically than the binary split case we are considering, which requires optimizing over @math possible splits. Thus our goals are similar to theirs, but since we are dealing with a more complex problem we cannot adopt either their computational approach or their theoretical results. In our classification examples we compare our approach to theirs, demonstrating the advantage of binary splitting for attaining better predictive performance in practice. The approach of @cite subsumes and improves on various previous lines of work in the machine learning literature on using cross validation to select variable in K-way splitting scenarios .
- In particular, if @math is Hermitian, the result of @cite states that eq:chen.jia (x, v) 1 + ^2 |B^ -1 |^2 sep ( ,G)^2 (x, K), where @math , @math , and @math denotes an orthogonal projector onto @math . The quantity @math describes separation of the targeted eigenvalue @math from the harmonic Ritz values different from the one associated with @math , which are the eigenvalues of @math ; see @cite for the precise definition of @math .
- In the mean time, the past few years have seen a resurgence of research in the design of deep neutral networks, and impressive progresses have been made on learning image features from raw data . To address human action recognition from videos, developed a novel deep architecture of convolutional networks, where they extracted features from both spatial and temporal dimensions. @cite proposed to incorporate a new Switchable Restricted Boltzmann Machine (SRBM) to explicitly model the complex mixture of visual appearance for pedestrian detection, and train their model using an EM-type interative algorithm. Amer and Todorovic applied Sum Product Networks (SPNs) to model human activities based on variable primitive actions. Our deep model is partially motivated by these works, and we target on an more flexible and powerful solution by jointly considering the latent structure embedding, feature learning, and radius-margin classification.
- Recently, recurrent neural networks (RNN) has been used for activity recognition due to its capability in modeling complex temporal dynamics. @cite presented a long-term recurrent convolutional network (LRCN) architecture to integrates CNN and RNN into an unified model, and achieved promising results in a number of vision tasks. @cite further improved LRCN by adding a pooling layer and had shown its potentials in video description. The main difference between RNN models and ours is that their models exploit several types of neural gates and memory cells to learn temporal dynamics implicitly, while our deep structured model explicitly accounts for temporal variations of human activities by inferring latent variables. Speficially, compared with these RNN models, our model has the following advantages. First, the temporal composition is explicitly captured by our model, giving rise to a better interpretability, i.e. the semantic correspondence of video segments and sub-activities. Second, as some recent works report @cite , the RNN models may have problems on using common dropout tricks and this limitation would influence the performances. Moreover, the integration with explicit regularization approaches (e.g. the radius-margin bound) is also an important superiority of our model.
- In the task of emotion recognition from faces, Tang's @cite sets the state-of-the-art on the Facial Expression Recognition Challenge (FERC) dataset. This is achieved by implementing a two stage network: a convolutional network trained in a supervised manner on the first stage, and a Support Vector Machine as the second stage trained on the output of the first stage. Recent work by in @cite successfully demonstrates a multi-modal deep learning based framework for emotion recognition in videos.
- Though a decent amount of work has been done in multiclass SVM and parallel distributed binary SVM individually, the research of distributed multiclass SVM needs more exploration by the research community. There is a continuous attention on SVM because it was proved as the best method in several applications even though it is computationally expensive @cite .
- In 1998 Han. discussed the model of coupling the estimation of class probabilities for each pair of classes @cite . The used classifiers include linear discriminant, nearest neighbors and SVM.
- In 2005 discussed the association between the symptoms and the treatment of a patient @cite . their discussion also includes the significance of handling the huge amount with the help of data mining.
- In 2011 designed a hybrid ensemble model for credit risk which combines both clustering and classification @cite . In this SVM classifiers are the members in the ensemble model.
- A multiclass classification approach for large data sets is discussed by using SVM,enclosing ball(MEB) method @cite . Solving a single optimization problem for the multiclasses is very expensive in terms of the time. A distributed parallel training approach is discussed for single-machine problem in @cite .
- In 2014 Aruna. proposed a binary tree based support vector machine @cite which reduces the number of binary classifiers when compared with OVO and OVA approaches. This algorithm is implemented in a distributed manner under HADOOP framework.
- Extensive work over the past several decades has produced a rich collection of algorithms for fast retrieval of @math -nearest neighbours. Space partitioning forms the basis of the majority of these algorithms. Early approaches store points in deterministic tree-based data structures, such as @math -d trees @cite , R-trees @cite and X-trees @cite @cite , which effectively partition the vector space into a hierarchy of half-spaces, hyper-rectangles or Voronoi polygons. These methods achieve query times that are logarithmic in the number of data points and work very well for low-dimensional data. Unfortunately, their query times grow exponentially in ambient dimensionality because the number of leaves in the tree that need to be searched increases exponentially in ambient dimensionality; as a result, for high-dimensional data, these algorithms become slower than exhaustive search. More recent methods like spill trees @cite , RP trees @cite and virtual spill trees @cite extend these approaches by randomizing the dividing hyperplane at each node. Unfortunately, the number of points in the leaves increases exponentially in the intrinsic dimensionality of the dataset.
- One class of methods @cite @cite @cite @cite that does not rely on space partitioning uses a local search strategy. Starting from a random data point, these methods iteratively find a new data point that is closer to the query than the previous data point. Unfortunately, the performance of these methods deteriorates in the presence of significant variations in data density, since it may take very long to navigate a dense region of the space, even if it is very far from the query. Other methods like navigating nets @cite , cover trees @cite and rank cover trees @cite adopt a coarse-to-fine strategy. These methods work by maintaining coarse subsets of points at varying scales and progressively searching the neighbourhood of the query with decreasing radii at increasingly finer scales. Sadly, the running times of these methods again exhibit exponential dependence on the intrinsic dimensionality.
- Content analysis of mass media has an established tradition in the social sciences, particularly in the study of effects of media messages, encompassing topics as diverse as those addressed in seminal studies of newspaper editorials @cite , media agenda-setting @cite , or the uses of political rhetoric @cite , among many others. By 1997, Riffe and Freitag @cite , reported an increase in the use of content analysis in communication research and suggested that digital text and computerized means for its extraction and analysis would reinforce such trend. Their expectation has been fulfilled: the use of automated content analysis has by now surpassed the use of hand coding @cite . The increase in the digital sources of text, on the one hand, and current advances in computation power and design, on the other, are making this development both necessary and possible, while also raising awareness about the inferential pitfalls involved @cite @cite .
- The majority of the work using social media data applied to political scenarios consist in ad hoc studies where researchers collect data from a given social network during a pre-defined period of time and produce their specific analysis or predictions. The availability of open source research platforms in this area is scarse. Usually researchers use specific APIs and software modules to produce their studies. One research platform is Trendminer @cite , an open source platform for real time analysis of Twitter data. It is the most similar work to POPmine although it only focus on Twitter and does not include sentiment analysis module, while POPmine allows cross media data collection and analysis, including sentiment.
- Differential privacy was defined in @cite and the relaxation to approximate differential privacy is from @cite . Most related to our work is the work on private learning and its sample complexity @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite and the early work on sanitization @cite . That many natural'' learning tasks can be performed privately was shown in the early work of @cite and @cite . A characterization for the sample complexity of pure-private learners was given in @cite , in terms of a new combinatorial measure -- the Representation Dimension , that is, given a class @math , the number of samples needed and sufficient for privately learning @math is @math . Building on @cite , Feldman and Xiao @cite showed an equivalence between the representation dimension of a concept @math and the randomized one-way communication complexity of the evaluation problem for concepts from @math . Using this equivalence they separated the sample complexity of pure-private learners from that of non-private ones.
- Cache attacks. Covert and side channels using the CPU cache exploit the fact that cache hits are faster than cache misses. The methods Prime+Probe @cite @cite @cite and Flush+Reload @cite @cite @cite have been presented to either build covert or side channels. These two methods work at a different granularity: Prime+Probe can spy on cache sets, while Flush+Reload has the finer granularity of a cache line but requires shared memory, such as shared libraries or memory deduplication.
- Attacks targeting the last-level cache are cross-core, but require the sender and receiver to run on the same physical CPU. Gruss al @cite implemented cross-core covert channels using Prime+Probe and Flush+Reload as well as a new one, Flush+Flush, with the same protocol to normalize the results. The covert channel using Prime+Probe achieves 536 ,Kbps, Flush+Reload 2.3 ,Mbps, and Flush+Flush 3.8 ,Mbps. The most recent cache attack by Irazoqui al @cite exploits cache coherency mechanisms and work across processors. It however requires shared memory.
- An undocumented function maps physical addresses to the slices of the last-level cache. However, this function has been reverse engineered in previous work @cite @cite @cite , enhancing existing attacks and enabling attacks in new environments.
- Memory and memory bus. Xiao al @cite presented a covert channel that exploits memory deduplication. In order to save memory, the hypervisor searches for identical pages in physical memory and merges them across VMs to a single read-only physical page. Writing to this page triggers a copy-on-write page fault, incurring a significantly higher latency than a regular write access. The authors built a covert channel that achieves up to 90 ,bps, and 40 ,bps on a system under memory pressure. Wu al @cite proposed a bus-contention-based covert channel, that uses atomic memory operations locking the memory bus. This covert channel achieves a raw bandwidth of 38 ,Kbps between two VMs, with an effective capacity of 747 ,bps with error correction.
- Independently of our work, Hassan al @cite also proposed algorithms to reverse engineer DRAM functions based on timing differences. However, their approach requires customized hardware performance-monitoring units. Thus, they tested their approach only in a simulated environment and not on real systems. Concurrently to our work, Xiao al @cite proposed a method to reverse engineer DRAM functions based on the timing differences caused by row conflicts. Although their method is similar to ours, their focus is different, as they used the functions to then perform Rowhammer attacks across VMs.
- The increasing DRAM density has led to physically smaller cells, which can thus store smaller charges. As a result, the cells have a lower noise margin and the level of parasitic electrical interaction is potentially higher, resulting in the so-called Rowhammer bug @cite @cite @cite .
- This bug results in corruption of data, not in rows that are directly accessed, but rather in adjacent ones. When performing random memory accesses, the probability for such faults is virtually zero. However, it rises drastically when performing accesses in a certain pattern. Namely, flips can be caused by frequent activation () of adjacent rows. As data needs to be served from DRAM and not the cache, an attack needs to either flush data from the cache using the clflush instruction in native environments @cite , or using cache eviction in other more restrictive environments, JavaScript @cite .
- The idea of sequentially processing an image by exploiting structure is not just relevant to object localization. Sequential processing has also been explored for video event detection, where running a multitude of detectors at all spatio-temporal scales is very expensive. Amer al @cite propose an explore-exploit strategy that schedules processes of top-down inference using activity context and bottom-up inference using activity parts. They use a Q-learning algorithm to learn the optimal actions to perform at a state. However, the learning algorithm needs the specification of a reward function which is difficult to obtain in many domains. We use an imitation learning algorithm that alleviates the problem of choosing a reward function.
- A natural combination of CNNs and CRFs is to use the CNN as unary potential and combine it with a CRF that also includes pairwise or higher order factors. For instance @cite @cite observed large improvements in pixel accuracy when combining a DenseCRF @cite with a CNN. The mean-field steps of the DenseCRF can be learned and back-propagated as noted by @cite and implemented by @cite @cite @cite @cite for semantic segmentation and @cite for human pose estimation. The works of @cite @cite @cite use CNNs also in pairwise and higher order factors for more expressiveness. The recent work of @cite replaced the costly DenseCRF with a faster domain transform performing smoothing filtering while predicting the image edge maps at the same time. Our work was inspired by DenseCRF approaches but with the aim to replace the expensive mean-field inference. Instead of propagating information across unaries obtained by a CNN, we aim to do the edge-aware information propagation across representations of the CNN. Experiments on different datasets indicate that the proposed approach generally gives better results in comparison to DenseCRF while being faster.
- A contribution of this work is to define convolutions over superpixels by defining connectivity among them. @cite , a method to use superpixels inside CNNs has been proposed by re-arranging superpixels based on their features. The technique proposed here is more generic and alleviates the need for rearranging superpixels. A method to filter irregularly sampled data has been developed in @cite which may be applicable to superpixel convolutions. The difference being that their method requires a pre-defined graph structure for every example image separately while our approach directly works on superpixels. We experimented with Isomap embeddings @cite of superpixels but for speed reasons opted for the more efficient kernels presented in this paper. The work of @cite extracted multi-scale features at each superpixel and perform semantic segmentation by classifying each superpixel independently. In contrast, we propagate information across superpixels by using bilateral filters with learned feature spaces.
- Another core contribution of this work is the end-to-end trained bilateral filtering module. Several recent works on bilateral filtering @cite @cite @cite @cite back-propagate through permutohedral lattice approximation @cite , to either learn the filter parameters @cite @cite or do optimization in the bilateral space @cite @cite . Most of the existing works on bilateral filtering use pre-defined feature spaces. @cite , the feature spaces for bilateral filtering are obtained via a non-parametric embedding into an Euclidean space. In contrast, by explicitly computing the bilateral filter kernel, we are able to back-propagate through features, thereby learning the task-specific feature spaces for bilateral filters through integration into end-to-end trainable CNNs.
- Our work is related to research on modeling image transformations with neural-network-based approaches. These often involve multiplicative interactions, for example gated RBMs @cite , gated autoencoder @cite or Disentangling Boltzmann Machines @cite . These approaches typically do not scale to large images, although they potentially could by making use of architectures similar to convolutional DBNs @cite . They are also typically only applicable to small transformations.
- The multi-view perceptron @cite is a network that takes a face image and a random vector as input and generates a random view of this face together with the corresponding viewpoint. In contrast, our model can generate directly the desired view without the need for random sampling. @cite trained a variant of a variational autoencoder with factored hidden representations, where certain dimensions are constrained to correspond to specific factors of variations in the input data, such as viewpoint and lighting. This method is conceptually interesting and it allows to generate previously unseen views of objects, but the quality of predictions made by our network is significantly better, as we show in the experimental section.
- A simplified version of unseen view prediction is predicting HOG descriptors @cite instead of images. @cite pose the problem as tensor completion. @cite find object parts similar to those of a given object in a large dataset of 3D models and interpolate between the desired views of these. These methods do not learn a 3D representation of the object class but approximate unseen views by linear combinations of models from a fixed dataset.
- @cite trained an 'up-convolutional' network to generate an image of a chair given the chair type and a viewpoint. This method is restricted to generating images of objects from the training set or interpolating between them. Applying the method to a new test image requires re-training the network, which takes several days. While the decoder part of our network is similar to the architecture of , our network also includes an encoder part which infers the high-level representation from a given input image. Hence, at test time we can generate unseen views and depth maps of new objects by simply forward propagating an image of the object through the network. Our approach also yields more accurate predictions.
- @cite proposed an approach for aligning 3D models of objects with images of these objects. The method makes use of discriminative part detectors and works on complicated real scenes. On the downside, this is a nearest-neighbor kind of method: it selects the best fitting 3D models from a fixed set of models. This limits the generalization capability of the method and makes it proportionally slower if the model collection grows in size.
- @cite reconstruct 3D models from single images of objects by jointly analyzing large collections of images and 3D models of objects of the same kind. The method yields impressive results. However, it jointly processes large collections of images and models with a nearest neighbor approach and hence cannot be applied to a new image at test time that is different from all models in the dataset.
- @cite trained convolutional networks to predict depth from single images of indoor scenes. This is very different from our work in that we predict depth maps not only for the current viewpoint, but also for all other viewpoints. @cite trained 3D Convolutional Deep Belief Networks capable of generating a volumetric representation of an object from a single depth map. This method requires a depth map as input, while our networks only take a single RGB image.
- The large margin nearest neighbours (LMNN) @cite algorithm employs semidefinite programming to optimise a loss function composed of two terms. Specifically, there are two evaluations of Equation . One term draws very similar instances together, and the other encourages a margin to be formed between dissimilar instances. As the name suggests, the motivation for the development of this algorithm was to improve the accuracy of @math -Nearest Neighbours ( @math -NN) classifiers.
- Neighbourhood Components Analysis (NCA) @cite is another method developed to be used in conjunction with @math -NN classifiers. This technique attempts to find the matrix, @math , by minimising a differentiable loss function that approximates the behaviour of @math -NN in the transformed feature space.
- The first application of neural networks to metric learning was with the introduction of Siamese networks @cite . The original authors initially applied these models to signature verification, however others have since used this technique for many other domains such as face recognition and verification @cite . Siamese networks are composed of two standard feed forward neural networks that have the same topology and share the same parameters. The output of these subnetworks is then compared with the cosine similarity function to produce the final output of the network, indicating whether the instances propagated through the two subnetworks are similar or not. During training the network is presented with pairs of instances labelled either positive or negative. For positive pairs the cosine similarity is maximised, whereas for negative pairs it is minimised.
- Following on from this, @cite developed a variant of Siamese networks that compares the output of the subnetworks using Euclidean distance. This method was then further improved by @cite , resulting in the contrastive loss function for Siamese networks, as given in Equation . This function is then averaged for all training pairs to create an objective function for training the network.
- A further generalisation of Siamese networks are the class of methods that consider three instances per loss function evaluation, so called triplet'' loss functions @cite @cite . These methods attempt to minimise the distance between a target instance and another positive example, while simultaneously maximising the distance between the same target instance and a negative example. Some variants of this approach allow one to define the ground truth labels in terms of relative similarity. That is, rather than having hard constraints specifying similarity or dissimilarity, similarity is defined as an instance being more similar to one instance than another.
- There is also an extension of NCA to nonlinear transformations of the input data @cite . This method can be viewed as a probabilistic variant of Siamese networks. The nonlinear transformation models used in the original exposition of this method were stacked Restricted Boltzmann Machines, initialised using unsupervised pretraining and subsequently fine-tuned using the NCA loss function.
- A common theme that unifies all the approaches described thus far is the need to train on pairs (or triples) of instances. This increases the effective size of the training set quadratically, greatly slowing down the training time. Our proposal to decouple the process of learning the embeddings from learning the functions that performs the embedding is not entirely new. The work of @cite utilises a similar two step process for learning a hashing function. In their work the embeddings are in fact bit strings, and the function used to generate the hash codes takes the form of boosted decision trees. They also use a greedy discrete optimisation procedure to find the target hash codes, rather than a numerical optimisation method to find real valued vectors.
- Bromley . @cite paved the way on deep metric learning and trained Siamese networks for signature verification. Chopra . @cite trained the network discriminatively for face verification. Chechik . @cite learn ranking function using triplet @cite loss. Qian . @cite uses precomputed @cite activation features and learns a feature embedding via distance metric for classification.
- Bell . @cite learn embedding for visual search in interior design using contrastive @cite embedding, FaceNet @cite uses triplet @cite embedding to learn embedding on faces for face verification and recognition. Li . @cite learn a joint embedding shared by both 3D shapes and 2D images of objects. In contrast to the existing approaches above, our method computes a novel structured loss and the gradient on the lifted dense pairwise distance matrix to take full advantage of batches in SGD.
- Surface properties of materials, and the relationship between perception of material and object categories are analyzed in @cite @cite . They first proposed a well-designed benchmark dataset called FMD. Then, they designed descriptors to extract hand-crafted features for representation of various surface properties such as color, texture and shape for material recognition in @cite . Moreover, they analyzed the relationship between object and material recognition for accurate and fast perception of materials in @cite .
- In @cite , a filter bank based method was developed using CNNs for texture recognition. The authors achieved state-of-the-art performance on several benchmark datasets for texture recognition and material recognition, including 82.4
- Segmental labeling problems have been widely studied. A widely used approach to a segmental labeling problems with neural networks is the connectionist temporal classification (CTC) objective and decoding rule of @cite . CTC reduces the segmental'' sequence label problem to a classical sequence labeling problem in which every position in an input sequence @math is explicitly labeled by interpreting repetitions of input labels---or input labels followed by a special blank'' output symbol---as being a single label with a longer duration. During training, the marginal likelihood of the set of labelings compatible (according to the CTC interpretation rules) with the reference label @math is maximized. CTC has demonstrated impressive success in various fully discriminative end-to-end speech recognition models [] graves:2014,maas:2015,hannun:2014 .
- Several alternatives to CTC have been approached, such as using various attention mechanisms in place of marginalization . These have been applied to end-to-end discriminative speech recognition problem. A more direct alternative to our method---indeed it was proposed to solve several of the same problems we identified---is due to @cite . However, a crucial difference is that our model explicitly constructs representations of segments which are used to label the segment while that model relies on a marginalized frame-level labeling with a null symbol.
- Our work mainly relies on RKA-JL algorithm @cite and RKA-Block algorithm @cite . We will review these algorithms below.
- Before introducing a greedy algorithm proposed by @cite , we have to note a key property shared by all Kaczmarz related algorithms. Recall that Kaczmarz algorithm iteratively update the current estimate to a new one by projecting on to one hyperplane. It is easy to observe that the Euclidean distance between the current estimate and the solution is monotonically decreased, which means that @math This is obvious because Thus, if we can find a way to maximize @math at each iteration, we may achieve better convergence rate at last.
- The exact greedy idea has been highlighted in @cite when they proposed RKA-JL algorithm. The idea is quite simple but reasonable. At the @math -th updating step, we can choose the hyperplane @math to project on where @math and update the estimate as Eq). But, after thinking of the practical issue, we may find that it is unaffordable to sweep through all rows of @math to pick the best one at each iteration. Thus, @cite proposed a procedure to approximate the best hyperplane before performing each updating step, which is the key component of RKA-JL algorithm.
- Instead of sweeping through the whole data and comparing the update @math , RKA-JL algorithm @cite selects @math rows from @math with probability @math , utilizes Johnson-Lindenstrauss lemma to approximate the distance @math and then choose the maximized one as the row to determine the projecting hyperplane. Besides, a testing step is launched to ensure that the chosen hyperplane will not be worse than that of classical.
- Another direction of researches related with Kaczmarz lies on the utilization of multiple rows of @math to update at each updating step. In @cite @cite block versions of Kaczmarz algorithm are proposed. Instead of just using one hyperplane at each updating step, block Kaczmarz uses multiple hyperplanes. To be specific, when updating @math , we may project the old estimate @math using @math which is a submatrix in @math and its corresponding @math via @math .
- After the introduction of the , many improvements that addressed its fundamental drawbacks were proposed. In @cite , Zhang proposed a new index that is suitable for evaluating highly cited scientists and comparing groups of scientists with an identical . present in @cite a comprehensive review on the and related indicators. They studied their main advantages, drawbacks, and main applications. In @cite , present a study of 37 different variants of . They show a high correlation between the and most of its variants.
- The complicated relationship between the scientist and his or her co-authors is one of the problems of the . Hirsch proposes in @cite a new version of the that takes into account the effect of multiple co-authors and solves a well-known problem with the so-called Hirsch core. A similar problem is solved and a new variant of the is introduced by in @cite . In @cite , apply social network analysis on ego co-authorship network. They show that the highest can be achieved by working with many co-authors.
- Analysis of the relation between the and the behavior of citing authors (citers) is also suitable for a better understanding how a community of citing authors influences . Brooks study complex citer motivations in @cite . Seven citer motives are analyzed, and more than 70
- In recommendation we are interested in the co-occurence of pairs @math of users @math and items @math . Depending on the application, @math may be interpreted as , , etc. In , @cite @cite (and similarly in their extension @cite ) we model @math as random variable with distribution P [u,i] = k=1 ^K ; P [u|k] ; P [i|k]. Hence, @math is expressed as a inner product of two vectors: @math and @math . This is reminiscent of where we expressed the probability of the event in terms of the inner product between @math and @math . Therefore, one half of the inner product agrees with the identity as both @math and @math are probability distributions. However, aspect models and NNMs disagree on the other half (i.e., @math ) because @math is constrained through the existence of @math such that @math . More importantly however, the difference between aspect models and NNMs lies in the different interpretations of @math and @math on the one hand and @math and @math on the other hand.
- Similarly, more general NMF-based models @cite agree with NNM-based models in that they make prediction in terms of inner products of nonnegative vectors but they differ from NNM-based models through different interpretations and regularizations of those nonnegative vectors.
- These new interpretations of @math and @math allow us to deal with situations where @math is a categorical random variable and we can extract hierarchical structures from NNMs in a straightforward manner. Aspect models can deal with both of these tasks too. However, we think that dealing with these tasks in terms of aspect models is less natural then dealing with these tasks with NNMs. For instance, to model multiple outcomes like ( @math ) we need to first decide on a particular graphical model (see section 2.3 in @cite ). Moreover, to extract hierarchical classifications of topics, we need to imagine generative processes like the nested Chinese restaurant process @cite , or we need to employ nested hierarchical Dirichlet processes @cite .
- The evaluation of NNMs in the extreme multi-label setting @cite @cite @cite @cite is still outstanding.
- Other algorithms have been proposed for the task of time series subsequence clustering @cite @cite . Though this task and the one of finding motifs with support have common aspects, they are conceptually different. Time series subsequence clustering aims at obtaining a good and compact representation of the whole time series while, as shown in @cite , ignoring some data. However, motif discovery aims at finding characteristic patterns (with support) that do not necessarily need to reliably and compactly represent the full time series. In fact, the segments used to derive the motifs can actually represent a tiny part of the time series (i.e., @math , as mentioned in the main text).
- : One possible solution is to use range sensors, such as laser range finders, infrared sensors, or RGB-Depth sensors. @cite presented a state estimation method using an on-board laser range finder and inertial measurement unit and showed aggressive flight in GPS-denied environments. @cite used one ultrasonic sensor and four infrared sensors and showed fully autonomous flight, i.e., collision avoidance. The range sensor, however, is not practical to most of publicly available quadcopters as the on-board device is often too heavy for MAVs and consume lots of power. Our work is based on only a monocular camera, which consumes a low power and is built in to most of quadcopters.
- : Using range sensors or visual sensors, a 3-D map of unknown indoor environments can be inferred, while simultaneously estimating its position in the map ( @cite , @cite , @cite ). @cite used a laser rangefinder sensor for a high-level SLAM implementation and exploring unknown indoor environments. @cite presented autonomous indoor navigation based on SLAM using monocular vision. However, SLAM is computationally expensive due to the 3-D reconstruction. This causes unacceptable delay between perception and action. Also, SLAM shows low accuracy when it is applied to indoor environments, like walls, which contain insufficient feature points that can be tracked frame to frame. Our system does not perform path-planning. Thus our approach is closely related to minimizing the delay by reacting fast to its currently faced situation. Our system also shows robust performance on detecting and avoiding walls.
- : Accurate depth estimation and relative position estimation are possible using stereo cameras ( @cite , @cite ). However, stereo vision algorithms suffer when they are used in texture-less regions, as it is hard to match features in one image to the corresponding features in the other image. The additional fact that most of publicly available quadcopters have only one built-in camera makes the solution not practical to a public. Our system shows robust performance in texture-less environments.
- : Other approach uses vanishing points. @cite used a monocular camera and found vanishing points. The points were used to fly in corridor environments. For staircase environments, they found center of the staircase. A front-facing short-range sensor, however, was additionally used to avoid collisions in corners and unknown environments. Our approach does not require the additional range sensor and can successfully perform collision avoidance with a monocular camera. Another approaches that are most closely related to our approach are approaches that learn control policies from input data. The ALVINN project @cite showed how the 3-layer artificial neural networks imitated a human drivers response on road and performed autonomous vehicle driving. @cite applied a novel imitation learning strategy, the DAgger Algorithm, and learned a controller policy that imitated human pilots choice of action from demonstrations of the desired behavior. The system demonstrated a fast autonomous flight in natural forest environments. We extend these learning approaches and employ an advanced classifier, ConvNets, which learns to autonomously fly and finds a target based on purely visual input.
- GCRFs were first introduced in @cite by modeling the parameters of the conditional distribution of output given input as a function of the input image. The precision matrix associated with each image patch was modeled as a linear combination of twelve derivative filter-based matrices. The combination weights were chosen as a parametric function of the responses of the input image to a set of oriented edge and bar filters, and the parameters were learned using discriminative training. This GCRF model was extended to Regression Tree Fields (RTFs) in @cite , where regression trees were used for selecting the parameters of Gaussians defined over image patches. These regression trees used responses of the input image to various hand-chosen filters for selecting an appropriate leaf node for each image patch. This RTF-based model was trained by iteratively growing the regression trees and optimizing the Gaussian parameters at leaf nodes. Recently, a cascade of RTFs @cite has also been used for image restoration tasks. In contrast to the RTF-based approaches, all the components of our network are differentiable, and hence it can be trained end-to-end using standard gradient-based techniques.
- Recently, @cite proposed a cascade of shrinkage fields for image restoration tasks. They learned a separate filter bank and shrinkage function for each stage of their cascade using discriminative training. Though this model can also be seen as a cascade of GCRFs, the filter banks and shrinkage functions used in the cascade do not depend on the noisy input image during test time. In contrast to this, the pairwise potential functions used in our GCRF model are generated by our PgNets based on the noisy input image.
- Variational upper bounds on MAP and the partition function, along with algorithms for providing fast, convergent optimization, have been widely studied in the last decade. In MAP, dual decomposition and linear programming methods have become a dominating approach, with numerous optimization techniques @cite @cite @cite @cite @cite @cite @cite , and methods to tighten the approximations @cite @cite .
- Thus, most algorithms do not satisfy all the desirable properties listed in the introduction. For example, many works have developed convergent message-passing algorithms for convex free energies [e.g.,][] hazan08, hazan10 . However, by optimizing the dual they do not provide a bound until convergence, and the representation and constraints on the counting numbers do not facilitate optimizing the bound over these parameters. To optimize counting numbers, @cite adopt a more restrictive free energy form requiring positive counting numbers on the entropies; but this cannot represent marginal MAP, whose free energy involves conditional entropies (equivalent to the difference between two entropy terms).
- On the other hand, working in the primal domain ensures a bound, but usually at the cost of enumerating a large number of trees. @cite heuristically select a small number of trees to avoid being too inefficient, while @cite focus on trying to speed up the updates on a given collection of trees. Another primal bound is weighted mini-bucket (WMB, @cite ), which can represent a large collection of trees compactly and is easily applied to marginal MAP using the weighted log partition function viewpoint @cite @cite ; however, existing optimization algorithms for WMB are non-monotonic, and often fail to converge, especially on marginal MAP tasks.
- While our focus is on variational bounds @cite @cite , there are many non-variational approaches for marginal MAP as well. provide upper bounds on marginal MAP by reordering the order in which variables are eliminated, and using exact inference in the reordered join-tree; however, this is exponential in the size of the (unconstrained) treewidth, and can easily become intractable. give an approximation closely related to mini-bucket to bound the marginal MAP; however, unlike (weighted) mini-bucket, these bounds cannot be improved iteratively. The same is true for the algorithm of , which also has a strong dependence on treewidth. Other examples of marginal MAP algorithms include local search [e.g.,][] park2004complexity and Markov chain Monte Carlo methods [e.g.,][] doucet02,yuan04 .
- In the past, there has been much research on the task of saliency detection in 2D images. Some of the earlier work employs bottom-up cues, such as color, brightness, and contrast to predict saliency in images @cite @cite @cite @cite . Additionally, several methods demonstrate the importance of shape cues for saliency detection task @cite @cite . Finally, some of the more recent work employ object-proposal methods to aid this task @cite @cite @cite .
- In the recent work, several methods employed egocentric (first-person view) cameras for the tasks such as video summarization @cite @cite , video stabilization @cite , object recognition @cite @cite , and action and activity recognition @cite @cite @cite @cite .
- Unlike other methods, which rely on object detectors @cite , or hand and skin segmentation @cite @cite , we propose EgoObject representation that is based solely on shape, location, size and depth cues in an egocentric RGBD images. We demonstrate that we can use our representation successfully to predict 3D saliency in egocentric RGBD images.
- The regenerating code can be divided into functional regeneration and exact regeneration. In the functional regeneration, the replacement node regenerates a new component that can functionally replace the failed component instead of being the same as the original stored component. @cite formulated the data regeneration as a multicast network coding problem and constructed functional regenerating codes. @cite implemented a random linear regenerating codes for distributed storage systems. @cite proved that by allowing data exchange among the replacement nodes, a better tradeoff between repair bandwidth @math and per node storage @math can be achieved. In the exact regeneration, the replacement node regenerates the exact symbols of a failed node. @cite proposed to reduce the regeneration bandwidth through algebraic alignment. @cite provided a code structure for exact regeneration using interference alignment technique. @cite presented optimal exact constructions of MBR codes and MSR codes under product-matrix framework. This is the first work that allows independent selection of the nodes number @math in the network.
- Recent years have witnessed the evolution of robotics and great industrial academic efforts. As the robots interact with the physical and social environments, considerable research contributions have been devoted to robotic sensing, cognition, motion path planning and control @cite . A multi-robot system aims at achieving challenging tasks or significantly improving mission performance compared with a single robot, which demands consensus and cooperation among robots @cite . Therefore, maintaining the connectivity quality for information exchange among robots becomes vital. As mobile robots are less likely to be connected via wires, the wireless communications and networking among robots and the infrastructure would play an crucial role and the wireless-connected robotic networks (WCRNs) are very likely to be incorporated into the next-generation communication networks.
- Different sparse matrices have different sparsity patterns, and different architectures have different strengths and weaknesses. In order to achieve the best SpMV performance for the target sparse matrix on the target platform, an autotuning approach has long been considered to be beneficial. The first autotuning approaches attempted to tune parameters of specific sparse matrix storage formats. Towards this direction, the Optimized Sparse Kernel Interface (OSKI) library @cite was developed as a collection of high performance sparse matrix operation primitives on single core processors. It relies on the SPARSITY framework @cite to tune the SpMV kernel, by applying multiple optimizations, including register blocking and cache blocking. Autotuning has also been used to find the best block and slice sizes of the input sparse matrix on modern CMPs and GPUs @cite .
- The design of rate and power control algorithms in DSRC is one of most critical problems in ITS. Error Model Based Adaptive Rate Control (EMBARC) @cite is a recent rate control protocol which integrates several existing rate control algorithms including the Linear Integrated Message Rate Control (LIMERIC) @cite , Periodically Updated Load Sensitive Adaptive Rate control (PULSAR) @cite , and the InterVechicle Transmission Rate Control (IVTRC) @cite . LIMERIC allocates the wireless channel equally among all vehicles that share the same bottleneck link while guaranteeing the channel load is below a given threshold. IVTRC generates messages and adapts transmission probabilities based on the Suspected Tracking Error (STE) calculated based on vehicle dynamics to avoid collisions. In EMBARC, the message rates are controlled by LIMERIC and are further modified to satisfy the STE requirement.
- A parallel work @cite introduced a network utility maximization (NUM) formulation on the rate control problem when specified to safety-awareness. A distributed algorithm was proposed to adjust the rate with the objective to maximize the utility function. Similarly, @cite also provided a NUM formulation on the rate control problem and proposed a fair adaptive beaconing rate for intervehicular communications (FABRIC) algorithm, which essentially is a particular scaled gradient projection algorithm to solve the dual of the NUM problem.
- Other related work includes the database approach proposed in @cite , where the optimal broadcast rates and transmission power are calculated offline based on the network configurations. Also, @cite proposed an environment and context-aware distributed congestion control (DCC) algorithm, which jointly control the rate and power to improve cooperative awareness by adapting to both specific propagation environments (such as urban intersections, open highways, suburban roads) as well as application requirements (e.g., different target cooperative awareness range). However, the stability and convergence of the algorithm are not proved mathematically. Besides the rate control algorithm IVTRC, the authors also proposed range control algorithms in @cite @cite @cite where the objective is to adapt the transmission ranges to achieve a specific threshold. The motivation of limiting channel loads below the threshold is to control channel congestion to maximize effective channel throughput. However, fair resource allocation among vehicles to increase the safety awareness of all vehicles are not considered, and the stability of the algorithms is subject to certain conditions @cite .
- The generic title transfer learning'' encompasses quite a few different paradigms. As noted by Levy and Markovitch @cite , such paradigms are motivated by (implicit or explicit) modeling or process assumptions. For example, some paradigms, such as feature transfer'', are motivated by assumptions on the linkage between source and target domains (e.g., features at the target obtained by certain mappings applied on the source features). The survey by @cite identifies the following settings, which are not mutually exclusive.
- For a comprehensive review of these fields the reader is referred to the works of Pan and Yang @cite and Jiang @cite .
- Most early transfer learning methods were based on neural networks, and while SVMs and ensemble techniques have become prominent in this field, DT models are still under-explored in this setting. Of the few tree-based techniques researched, only the work by operates in our model transfer setting. Specifically, proposed a simple technique to update an existing tree trained only on source samples using target samples @cite . Their approach resembled a batch iterative learning technique which relies solely on iterative expansion steps. This technique does not consider any refitting of numeric feature thresholds.
- Finally, in MTL, presented a variation of AdaBoost with decision stumps fitted to multiple tasks @cite . The same authors later applied boosting with DTs while using a modified information gain (IG) criterion @cite . Another approach builds an ensemble by combining multiple random DTs, where task-driven splits are added in each tree, in addition to ordinary feature splits @cite . In this manner, the trees may contain branches uniquely dedicated to particular subsets of tasks.
- Although there is significant literature on the integration of PONs with wireless transmission media, e.g., WOBAN @cite and FiWi @cite @cite , there is a dearth of literature on the integration of PONs with copper transmission media.
- Two physical-layer systems to bridge VDSL signals over a fiber access network were proposed in @cite . Individual VDSL signals are converted to be spectrally stacked into a composite signal that modulates an optical carrier. In the first system the optical carrier is supplied by a laser at the ONU and in the second system the optical carrier is supplied by a laser in the OLT that is reflected and modulated by a Reflective Semiconductor Optical Amplifier (RSOA) at the ONU. The optical carrier provides 1 GHz of spectral width accommodating 40 VDSL lines without guard bands and 25 VDSL lines with guard bands. Although, this approach to a hybrid PON xDSL allows the drop-point device to avoid buffering as well as contain simple logic by pulling the DSLAM functionality into the OLT, the design requires the PON to carry the full bandwidth of each VDSL line even when idle. Designs that operate at the link layer rather than physical layer can avoid transmission of idle data on the PON thereby increasing the number of subscribers that can be supported by capitalizing on statistical multiplexing gains.
- The coaxial copper cable deployed by cable companies represents another existing copper technology that can be used in conjunction with PONs to create a hybrid access network. Such a hybrid access network combining an Ethernet PON with an Ethernet over Coax (EoC) network was proposed in @cite . The proposed network uses EPON protocols on the EoC segment in isolation from the EPON segment without any coordination between the segments. A similar network was examined in @cite in terms of the blocking probability and delay for a video-on-demand service. None of these studies discussed the design of the drop-point device or explored DBA algorithms for these types of networks.
- In November 2011, the IEEE 802.3 working group initiated the creation of a study to extend the EPON protocol over hybrid fiber-coax cable television networks; the developing standard is referred to as EPON Protocol over Coax (EPoC) @cite . Developing bandwidth allocation schemes for EPoC has received little research attention to date. In particular, a DBA algorithm that increases channel utilization in spite of increased propagation delays due to the coaxial copper network was designed in @cite . Mechanisms to map Ethernet frame transmissions to from the time division multiplexed channel of the PON to the time and frequency division multiplexed coaxial network have been studied in @cite @cite .
- Recently, the Deep Q-Network (DQN) has been shown to be able to successfully play Atari games @cite @cite @cite . Trained with a variant of Q-learning @cite , the DQN learns control strategies using deep neural networks. The main idea is to use deep learning to automatically generate informative features to represent the internal states of the environment where the software agent lives, and subsequently approximate a non-linear control police function for the learning agent to take actions. In addition to playing video games, employing reinforcement learning to learn control policies from text has also be investigated. Applications include interpreting user manuals @cite , navigating directions @cite @cite @cite @cite and playing text-based games @cite @cite @cite . Also, DQN has recently been employed to learn memory access patterns and rearrange a set of given words @cite . Unlike the above works, our research here aims to decode natural text with DQN. In addition, we employ an encoder-decoder LSTM network to not only generalize informative features from text to represent the states of DQN, but also create a list of potential actions from the text for the DQN.
- Starting with HCI, there are a number of systems that were designed with the goal of aiding people in decision making. As there is a large body of work on general decision support systems @cite , we only discuss work pertaining to search and recommendation @cite . Ruetsalo @cite propose a system for information retrieval tasks where a user model gets adapted during the search process, allowing the user to update feature weights after each query. We, in contrast, do not ask for explicit feedback in any form, but assume users are rational enough to only shortlist items that have relatively high utility. Also in information retrieval, Jia and Niu @cite propose an interface that helps people know when to stop exploring. Drucker @cite present a visual way of supporting movie selection in groups -- an interesting scenario we would like to like to study in the future. In contrast to their work, we propose shortlists not as an entire system to solve an end-to-end task, but rather as a component that provides digital memory. Hence, our approach can be seen as complementary to these systems -- one can imagine adding shortlists to them as an additional component.
- Lastly, there is a growing interest in recommendation and decision making on a session-based level. Cremonisi @cite study decision making in recommendation systems for hotel search. The authors perform a user study where decision making happens either with the help of a recommender system or without. Interestingly, they found that the number of examined items as well as the time-to-decision increased when users employed a recommender system. This again shows the need to consider both interface design and feedback elicitation at the same time. On the algorithmic side, several approaches are designed to learn from session-based data @cite @cite @cite . The work done by Jannach @cite also adopts a session-based approach to recommendation, but, in contrast to us, assumes that a long term interest profile is available. A challenge that we had to face was also to learn from multiple levels of implicit feedback. Most work assumes that one has access to enough users so that graded relevance labels can just be integrated into standard collaborative filtering models @cite @cite . Our approach did not assume that feedback was available from other users since we adopted a cold-start session-based scenario.
- Currently research on natural language process for social media is still at the starting stage, in the 2006 SIGHAN Chinese word segmentation competition, approaches based on sequence annotation has been widely used. Microsoft used conditional random field and the size of feature window equals one. @cite It turns out that features have been simplified, but the performance was still very good. DaLian Science and Technology University built two models, one is based on the use of the word CRF model, the other is based on MMSM model. During our experiments, we used CRF algorithm as well, and it plays the key role when we build the model. Our work mainly depends on the improvement of conditional random field.
- Conditional random field (CRF) is a statistical sequence modeling framework which is first introduced into language processing by Lafferty (2001). @cite Previous research showed that CRF has a good performance on word segmentation accuracy in the pipeline method. Tseng (2005) and John (2001) introduced a conditional random field sequence model in conjunction with character identity features, morphological features and character reduplication features. @cite @cite The study that is closely related to ours is @cite , which also used assistant algorithm and added external lexicon, while they just add the output of assistant algorithm to the feature templates. Different from that work, we take not only the relevance between the character and its MMSEG output tag, but also the context feature of these MMSEG output tags as well.
- Our approach is inspired from combining classifiers techniques @cite in machine learning, which have been shown to boost the strengths of single classifiers. Several fusion techniques have been successfully used in different areas of computer vision, like face detection @cite , multi-label image annotation @cite , object tracking @cite , and character recognition @cite . However, the constituent classifiers and the mechanisms for combining them are quite different from our framework and the other techniques are only demonstrated on small datasets.
- Here, we propose to take advantage of the composite structure of the objective function and of an efficient solver of that partially linearized problem. Note that a result similar to Lemma , denoted as the surrogate duality gap in @cite , exists for the conditional gradient: @math Using the convexity of @math one can see that @math This means that the bound expressed in Lemma is at least as good that the one provided by the classical CG. In addition when @math is strictly convex, our bond is strictly better which suggests that our approach provides a better control of the convergence along the iterations.
- Finally our approach is also closely related to projected gradient descent and its spectral variant @cite . As discussed more in detail in section , when @math is an euclidean distance, the solving problem boils down to a projection onto the convex @math . In practice the method is more general since it can be used for any problem as long as problem can be efficiently solved.
- Part (ii) of our Theorem is implied by [Theorem 10.8] HMT11 , but our specific supporting estimates are more compact, cover the case of any @math (whereas @cite assumes that @math ), and we deduce them by using a shorter proof (see Remark ). Our approach, our results listed in Section , some of our techniques, e.g., expan -sion compres -sion in Section , and even the concept of factor-Gaussian matrices are new, and so are our families of multipliers and policies of their generation, combination, and application in Sections and as well. Moreover, our progress can be extended to a variety of important matrix computations. In Section (Conclusions) we outline such novel extensions to the highly popular and much studied computations for Least Squares Regression, the Fast Multipole Method and the Conjugate Gradient Algorithms. Hereafter we use the acronyms LSR" for Least Squares Regression", FMM" for Fast Multipole Method", and CG " for Conjugate Gradient". The extensions provide new insights and new opportunities and should motivate further effort and further progress.
- While model-based recognition and pose estimation of objects has been an active area of research for decades in the computer vision community @cite @cite @cite , the proliferation of low-cost depth sensors such as the Microsoft Kinect has introduced a plethora of opportunities and challenges. We describe approaches in vogue for object recognition and localization from 3D sensor data, their limitations, inspirations from early research in vision that motivate our work, and the potential role of contemporary learning-based systems.
- The second, recognition systems employ a single-shot process for identifying object type and pose jointly. Global feature descriptors encode the notion of an object and capture shape and viewpoint information jointly in the descriptor. These approaches employ a training phase to build a library of global descriptors corresponding to different observed instances (e.g., each object viewed from different viewpoints) and attempt to match the descriptor computed at observation time to the closest one in the library. Additionally, global methods unlike the local ones, require points in the observed scene to be segmented into different clusters, so that descriptors can be computed on each object cluster separately. Some of the global recognition systems include Viewpoint Feature Histogram (VFH) @cite , Clustered Viewpoint Feature Histogram (CVFH) @cite , OUR-CVFH @cite , Ensemble of Shape Functions (ESF) @cite , and Global Radius-based Surface Descriptors (GRSD) @cite . Other approaches to estimating object pose include local voting schemes @cite or template matching @cite to first detect objects, and then using global descriptor matching or ICP for pose refinement.
- The idea of using search to explain' scenes was popular in the early years of 2D computer vision: Goad @cite promoted the idea of treating feature matching between the observed scene and 3D model as a constrained search while Lowe @cite developed and implemented a viewpoint-constrained feature matching system. Grimson @cite introduced the Interpretation Tree to systematically search for geometrically-consistent matches between scene and model features, while using various heuristics to speed up search. Our work is also based on a search system, but it differs from the aforementioned works in that the search is over the space of full hypothesized rendered scenes and not feature correspondences. In fact, our proposed algorithm does not employ feature descriptors at all.
- The philosophy of the Render, Match and Refine (RMR) approach proposed by Stevens and Beveridge @cite motivates our work. RMR explicitly models interaction between objects by rendering the scene and uses occlusion data to inform measurement of similarity between the rendered and observed scenes. It then uses a global optimization procedure to iteratively improve the rendered scene to match the observed one. PERCH, our proposed algorithm, operates on a similar philosophy but differs in several details. The explanation cost' we use to compare the rendered and observed scene is based purely on 3D sensor data, as opposed to the 2D edge-feature and per-pixel depth differences used in RMR that make it vulnerable to offset errors between the rendered and observed 2D scenes. Moreover, the explanation cost we propose can be decomposed over the objects in the scene, thereby obviating the need for exhaustive search over the joint object poses.
- Finally, an emerging trend for object recognition and pose estimation in RGB-D data is the use of deep neural networks trained on synthetic data generated using 3D models @cite @cite . As promising as deep learning methods are, they would require sufficient training data to capture invariances to multi-object interaction and occlusion, the generation of which is a combinatorial problem by itself. On the other hand, these methods could be incorporated in PERCH as heuristics for guiding deliberative search as discussed in Sec. .
- Formal methods have been used for SELinux policy analysis. Gokyo @cite is a policy analysis tool designed to identify and resolve conflicting policy specifications. Usage of the HRU security model @cite has been proposed as an approach to SELinux policy analysis @cite . Information flow analysis has been applied to SELinux policies @cite . These analysis methods are not SELinux-specific, and can be easily adapted to SEAndroid.
- Some researchers have applied information visualization techniques to SELinux policy analysis @cite , also in combination with clustering @cite . These techniques are also system-agnostic, and we may use them in future SEAndroid tools.
- In the literature of content-based image retrieval, many methods have been proposed to describe images. A query and dataset images are converted to some form of vector representation to evaluate their similarity. A typical form is the bag-of-features (BoF) representation @cite , in which local features such as SIFT @cite are extracted from an image and quantized into a sparse histogram. The distance between a query histogram and a dataset histogram can be computed efficiently using an inverted index structure. The BoF form has been studied widely because it achieved successful image description with fast computation.
- In the manga feature description task, we compared the proposed method with (1) BoF, (2) large-window FV as the state-of-the-art of BoF-based methods @cite , and (3) compact oriented chamfer matching (Compact OCM) as the state-of-the-art of chamfer-based methods @cite .
- Note that deep-learning techniques are becoming quite popular in image recognition because of their superior performance. These technologies are being applied to the sketch processing. However, the effectiveness of such methods has not yet been discussed sufficiently. For example, Yu and colleagues @cite reported that traditional methods such as FV are still competitive for a sketch recognition task, and therefore such deep learning-based features are beyond the scope of this paper.
- How to query multimedia data has been an important problem in content-based multimedia retrieval @cite . Although many approaches have been proposed, including keywords, images, and spatial image group predicates @cite , we focus on sketches drawn by users. Sketch-based interaction is the most natural way for humans to describe visual objects, and is widely adapted not only for image retrieval, but also in many applications such as shape modeling @cite , image montage @cite , texture design @cite , and as a querying tool for recognizing objects @cite @cite .
- From an image-processing perspective, manga has a distinctive visual nature compared with natural images. Several applications for manga images have been proposed: colorization @cite @cite @cite , vectorization @cite , layout recognition @cite , layout generation @cite @cite , element composition @cite , manga-like rendering @cite , speech balloon detection @cite , segmentation @cite , face tailored features @cite , screen tone separation @cite , retargeting @cite , and manga-like summarization for video @cite @cite @cite . The research group at Universit 'e de La Rochelle constructed a comic database @cite , which we mention in sec:db , and analyzed the structure of visual elements in a page to understand the comics content semantically @cite .
- Approaches to automatic note-level transcription from audio recordings date back as far as 1977 @cite and are extensively reviewed by @cite and @cite . Most systems described in these reviews provide a generic transcription framework covering a wide range of instruments and musical genres. As mentioned in @cite , specific instruments exhibit particular characteristics which might not be captured by a generic transcription system.
- Addressing the more complex task of singing transcription from polyphonic recordings @cite , a multiple fundamental frequency estimator with an accent signal in order to extract the dominant pitch trajectory. The segmentation stage relies on a probabilistic note event model, using a Hidden Markov Model (HMM) trained on manual transcriptions. In an extension to this approach @cite , note transition probabilities were incorporated in the computational model. Recently, this note segmentation method was implemented in the computer-aided note transcription tool tony @cite . It should be mentioned that such probabilistic methods require a large amount of ground truth data during the training stage. This involves time-consuming manual annotations in particular for improvisational and strongly ornamented singing performances, as it is the case for flamenco music.
- Nevertheless, the authors report mistakenly transcribed guitar notes as a main source of error and the inaccurate vocal detection a the major limitation of the system performance. The reported note transcription accuracies reported in @cite with a note f-measure of slightly below @math furthermore indicate the difficulty of transcribing flamenco singing. Significantly higher performance is achieved when evaluating the same transcription algorithm on a dataset containing pop- and jazz singing excerpts. We therefore identify a need for improving the state of the art on flamenco singing transcription and design an algorithm robust towards the particular characteristics of the genre which is furthermore suitable for the analysis of accompanied flamenco singing recordings.
- The @cite simulates a highway toll system for motor vehicles with variable tolling, and was meant to compare Data Stream Management Systems (DSMS) with Database Management Systems (DBMS). The input to the benchmark is from a traffic model that has variable number of vehicles, but each emitting tuples at a uniform rate and with the same type. So while the input rate is variable, each message is of a fixed, small size. The metrics for evaluation are the response time and accuracy of the query, and the maximum sustained rate within a specified response time, but it does not consider resource utilization. While not developed for distributed stream processing systems, which deal with opaque messages and user logic, it can be adopted to validate DSPS. Our proposed benchmark is particularly tuned for DSPS and simulates the behavior of a real-world eGovernance workload, with variable message sizes and resource efficiency as an additional metric.
- In literature, several works deal with behavioral targeting in Online Ad Systems, highlighting their potential privacy threats @cite . The novelty of the current work is that it presents and maliciously exploits a feature of the Google advertising system, which allows a remote attacker to infer user personal information, as interests and navigation behavior. In the following we review the main research works related to privacy issues and online advertisement.
- As mentioned earlier, transfer learning approaches could deal with transferring policies or value functions. For example, @cite describe a method for transferring value functions by constructing a Game tree . Similarly, @cite use the value function from a source task as the initial estimate of the value function in the target task.
- The two recent works that are very relevant to the proposed architecture are discussed in @cite and @cite . @cite explore transfer learning in RL across Atari games by trying to learn a multi-task network over the source tasks available and directly fine-tune the learned multi-task network on the target task. However, fine-tuning as a transfer paradigm cannot address the issue of negative transfer which they do observe in many of their experiments. @cite try to address the negative transfer issue by proposing a sequential learning mechanism where the filters of the network being learned for an ongoing task are dependent through lateral connections on the lower level filters of the networks learned already for the previous tasks. The idea is to ensure that dependencies that characterize similarity across tasks could be learned through these lateral connections. Even though they do observe better transfer results than direct fine-tuning, they are still not able to avoid negative transfer in some of their experiments.
- Considering the data transformation process, which was especially critical in our work, in @cite , a discretization process was also applied to data set in order to find a more adequate set of clusters. Resources are mapped to n+2 dimensional Euclidean space, n being the customers attributes, one dimension for time and another for transactions. Customers transactions are projected according to time, accumulating transactions and its frequency to create a histogram. Clusters are created based on segments of the histogram. Analyses of local and global correlations are then applied to detect suspicious patterns. A good way to analyze individual behavior and or group behavior is to examine their operations to detect suspicious behaviors related to abnormal peaks on the histogram. However, when it is necessary to analyze a big number of clients and transactions over a long period of time, it may become difficult to detect suspicious cases, since there might be few peaks or none at all in the histogram.
- In @cite , the authors propose an extension of the support vector machine (SVM) @cite to detect customers abnormal behavior. A combination of an RBF kernel (radial basis function) is presented with improvements @cite as a definition for distinct distances @cite and supervised and unsupervised SVM algorithms. An SVM class @cite is a supervised way of learning used to detect value anomalies in a group of data without classes. The advantage of this approach is that you can deal with heterogeneous datasets.
- A combination of clustering and MLP (multilayer perceptron) was proposed by @cite . A simple center-based clustering technique is used to detect suspicious cases of money laundering. This technique is based on two main characteristics, which are then used as an MLP creation process entry. The preliminary results show that this approach is efficient. However, the number of characteristics and training patterns is too small and that could affect precision.
- In @cite , the authors present a case study corresponding to the application of a knowledge base solution that combines data mining techniques, clustering, neural networks and genetic algorithms to detect money-laundering patterns.
- We show below that the ISO Problem is an example of a decentralized control system with non-classical observation patterns in which signaling can successfully result in globally optimum performance. The agents need not reveal their observations, state values, their individual system dynamics or their individual cost-functions. We construct concrete signaling schemes which encode-decode the information required in order to recover the same performance as that of centralized control. From the economics side, this work is an extension of general equilibrium theory @cite . To the authors' knowledge there does not appear to be any similar result for coordinating multiple LQG systems or the efficiency of the simplified signaling.
- In the setting of the energy market, @cite @cite discuss a general framework for the operation of an electicity grid via the establishment of a marketplace in which the electricity purchases are done on the basis of spot prices. Thus, electrical energy is treated as a commodity which can be bought, sold, and traded, taking into account its time-and space-varying values and costs, and a framework is presented for the establishment of an energy marketplace. Our results can be viewed as providing guidelines for the operation of this marketplace via simple bidding schemes when the agents are nonlinear stochastic dynamical systems. Our scheme maximize the net social utlity.
- Viewed from the power system end, there have been many efforts since the deregulation of the electricity sector on a market-based framework to clear the system. @cite have proposed a two-layered approach that internalizes individual constraints of market participants while allowing the ISO to manage the spatial complexity. The approximated MPC algorithm is shown to perform well in many realistic applications. In order to analyze the strategic interactions between the ISO and market participants, game theoretical approaches have been proposed, e.g., @cite uses a Stackelberg game framework for studying economic dispatch with demand response. @cite pose the problem as a noncooperative game, and prove the existence of a Nash equilibrium under some assumtpions.
- On the algorithmic front, using multiple classifiers for increasing learning accuracy is an active area of research @cite @cite @cite @cite . A class of work in ensemble based learning @cite @cite @cite exploit the idea that different classifiers can offer complementary information about patterns to be classified which can be used to improve the effectiveness of the overall recognition process. The main motivation behind such algorithmic techniques is to obtain an improvement in accuracy. However, using them to reduce energy and runtime has received attention only in the recent past @cite . The use of multiple classifiers in our proposed methodology is entirely driven by energy-efficiency and reduced computational complexity.
- Past research in building energy-efficient neuromorphic systems have considered application-specific solutions @cite @cite . In @cite , the authors have proposed a scalable effort classification framework consisting of a cascaded chain of biased classifiers with increasing complexity that can dynamically adjust the compute effort depending on the input data. The concensus between the classifier's outputs is used to decide if classification can be terminated at an earlier stage. The methodology that we propose in this paper is complementary to the concept of cascading classifiers. However, the novelty of our work arises from the fact that we leverage the semantic information in the input data to obtain efficiency and reduced testing complexity. Please note that though our training method draws inspiration from @cite , our method has different focus, design and evalutaion strategies.
- Other popular approaches that have been explored to lower the compute effort of the network is based on approximate computing @cite @cite . Exploiting the inherent error resilience of a system, a variety of approximate hardware @cite @cite and software @cite @cite techniques have been proposed to achieve reduced computational complexity. However, these techniques provide an explicit trade-off between efficiency and quality of results @cite . Our approach, on the other hand, provides energy savings, while maintaining classification accuracy.
- In this paper, we study the problem of computing 4- and 5-node motifs' frequencies for , which is much different from the problem of computing the number of subgraph patterns appearing in studied in @cite . Recently, a lot of efforts has been devoted to design sampling methods for computing a large graph's motif concentrations @cite @cite @cite @cite @cite @cite . However, these methods fail to compute motif frequencies, which is more fundamental than motif concentrations. @cite propose the color-coding method to reduce the computational cost of counting subgraphs. Color coding reduces the computations by coloring nodes randomly and enumerating only colorful CISes (i.e., CISes that are consisted of nodes with distinct colors), but @cite reaveals that the color-coding method is not scalable and is hindered by the sheer number of colorful CISes. @cite @cite @cite @cite develop sampling methods to estimate the number of triangles of static and dynamic graphs. @cite develop sampling methods to estimate 4-node undirected motifs' frequencies. However their methods are edge centric methods, which cannot be easily applied to current vertex centric graph computing systems such as GraphLab @cite and GraphChi @cite . Moreover, their methods fail to sample and count 5-node motifs.
- Various community detection algorithms have been developed in the past decade. And most of the algorithms fall into the category of global approach. One stream of global algorithms attempt to find communities by optimizing an objective function. For example, OSLOM @cite is based on the optimization of a fitness function, which expresses the statistical significance of clusters with respect to random fluctuations (i.e., the random graph generated by the configuration model @cite during community expansion). However, the communities identified by mathematical construction may structurally diverge from real communities as pointed out in @cite . Another main stream of research adopts the label propagation approach @cite , which defines rules that simulate the spread of labels of vertices in the network. The DEMON algorithm @cite , for example, democratically lets each vertex vote for the communities it sees surrounding it in its limited view of the global system using a label propagation algorithm, and then merges the local communities into a global collection. Other approaches such as Link Community (LC) @cite partitions the graph by first building a hierarchical link dendrogram according to the link similarity and then cutting the dendrogram at some threshold to yield link communities.
- As noted in the preceding section, among the divergent approaches, random walks tend to reveal communities that bear the closest resemblance to the ground truth communities @cite . In the following, we briefly review some methods that have adopted the random walk technique in finding communities. Speaking of methods that focus on the global structure, Pons @cite proposed a hierarchical agglomerative algorithm, , that quantified the similarity between vertices using random walks and then partitioned the network into non-overlapping communities. Meil @math @cite presented a clustering approach by viewing the pairwise similarities as edge flows in a random walk and studied the eigenvectors and values of the resulting transition matrix. A later successful algorithm, , proposed by Rosvall & Bergstrom @cite enabled uncovering hierarchical structures in networks by compressing a description of a random walker as a proxy for real flow on networks. Variant of this technique such as biased random walk @cite has also been employed in community finding.
- One of the most popular scan registration methods, the iterative closest point (ICP) @cite @cite @cite , relies on point-to-point correspondences to estimate the relative transformation of scans by minimizing the Euclidean distance error metric. The original ICP algorithm assumes that there exists a correspondence between each point of the source and model data-sets. This assumption is often violated with partially overlapping scans. Some modifications to the ICP algorithm have included the maximum error cutoff metric @cite to account for false correspondences, and did not require every point to be matched. One of the key problems with scan registration is that the sparsely sampled corresponding points in two different scans often do not correspond to the same point in the 3D environment, but ICP assumes that they do. In addition, the quality of the ICP solution depends heavily on the availability of good initial estimates of the transformation @cite .
- introduced the Sample Consensus-Initial Alignment (SAC-IA) algorithm @cite @cite using 16-dimensional fast point feature histograms (FPFH) that describe the local surface structure. Experimental results showing the robustness of these features to outliers and invariance to pose, sampling density, and measurement noise are lacking in the literature. Various heuristics based on false correspondence rejection and re-weighting have tried to improve the robustness but the convergence is not guaranteed. In addition, these features require extensive computational steps and the resulting transformation is only an approximation due to the compact representation of a 3D surface (in metric space) as a feature point in feature space.
- The Harris corner point detector initially proposed for 2D images @cite has previously been extended for key-point detection on 3D surfaces @cite @cite where it makes use of surface normals instead of using image gradients. 3D normals calculated in noisy regions or around points at depth-discontinuities are frequently incorrect.
- The spin-image algorithm for object recognition and pose estimation first proposed by for 2D images @cite was later extended to work with 3D laser scans @cite . Surface around the key-point is represented as a gray-scale image where darker areas correspond to regions of high point density. Spin-image based descriptors do not explicitly make use of the information outside of the object boundaries, making them less reliable for pose estimation @cite . In addition, spin image descriptors produce ambiguous correspondence matches in cluttered scenes and noisy environments.
- Frequency-domain based approaches decouple the problem of finding rotation and translation transformation parameters and attempt to find a suitable registration in the transformed domain @cite @cite @cite . Phase correlation is typically employed for matching which is robust to the effects of noise and occlusions, while fast Fourier transforms (FFT) used to compute cross-correlations makes this approach computationally efficient. However, the Fourier transform can only retrieve the global frequency content of the signal and provides a dense representation of the underlying signal.
- Another transformation found in the literature relies on finding a translation invariant Fourier transform on two Extended Gaussian Images (EGI) @cite of laser scans, where the surface normals of an object are mapped onto the unit sphere. However, this approach can only be applied to smooth surfaces and fails to match surfaces with constant EGI (such as a sphere). @cite proposed another approach to scan matching that projects the two scans into the Hough Radon domain @cite defined on the unit sphere. Similar to the work of , a translation invariant spectrum is computed to find the rotation and cross-correlations are used to find the translation. Both EGI approaches and the ones based on transformation to Hough Radon domain are sensitive to the measurement noise and sampling density during the calculation of normals.
- Unlike the shape-fixed rectangles in the frequency domain of conventional FFT, multi-scale transforms such as the discrete wavelet transform (DWT) use dilated shape varying rectangles to find directional elements such as edges and ridge features in the laser scans. However, many wavelet coefficients are needed to account for singularities along lines or curves. To overcome this problem, other directional wavelets such as wedgelets @cite , beamlets @cite , contourlets @cite , surfacelets @cite , etc. have been proposed, however the detected features are less prominent especially at the edges. In order to account for curve-singularities, the curvelet transform has previously been proposed which generates a sparse representation of the points within the scan and employs angled polar wedges in the frequency domain to find directional features. Previously, applied the curvelet transform for the problem of image fusion @cite where only an approximate sub-band of the coefficients was used for registration. The algorithm relied on the assumption that the curvelet coefficients were normally distributed.
- Visual concepts have been widely used in visual recognition tasks @cite @cite @cite . For example, @cite addresses the problem of describing objects with pre-defined attributes. @cite propose to recognize complex visual composites by defining visual phrases. For video analysis, people commonly use predefined pools of concepts (e.g. , ) to help classify and describe high-level activities or events (e.g. ) @cite . However, their concept vocabularies are usually manually selected.
- Visual concepts can be categorized @cite and organized as a hierarchy where the leaves are the most specific and the root is the most general. For example, ImageNet concepts @cite are organized following the rule-based WordNet @cite hierarchy. Similar structure also exists for actions @cite . Since concept classification is not always reliable, @cite propose a method to allow accuracy-specificity trade-off of object concepts on WordNet. As WordNet synsets do not always correspond to how people name the concepts, @cite study the problem of entry-level category prediction by collecting from humans.
- Our research is closely related to the recent work on visual data collection from web images @cite @cite @cite @cite or weakly annotated videos @cite . Their goal is to collect training images from the Internet with minimum human supervision, but for pre-defined concepts. In particular, NEIL @cite starts with a few exemplar images per concept, and iteratively refines its concept detectors using image search results. LEVAN @cite explores the sub-categories of a given concept by mining bigrams from large text corpus and using the bigrams to retrieve training images from image search engines. Recently, @cite use noisily tagged Flickr images to train concept detectors, but do not consider the semantic similarity among different tags. Our VCD framework is able to generate the concept vocabulary for them to learn detectors.
- Image descriptions can be generated by detection or retrieval. The detection based approach usually defines a set of visual concepts (e.g. and ), learns concept detectors and use the top detected concepts to generate sentences. The sentences can be generated using templates @cite @cite @cite or language models @cite @cite . The performance of detection is often limited by missing concepts and inaccurate concept detectors. Retrieval-based sentence generation @cite @cite @cite works by retrieving sentences or sentence components from an existing pool of sentence and image pairs, and use them for description. The retrieval criteria is usually based on the visual similarity of image features. To allow bidirectional retrieval of sentences and images, several work @cite @cite @cite embed image and text raw features into a common latent space using methods like Kernel Canonical Component Analysis @cite . There is also a trend to embed sentences with recurrent neural networks (RNN) @cite @cite @cite @cite @cite , which achieves the state-of-the-art performance in sentence retrieval and generation tasks.
- In the recently proposed Exemplar SVM (ESVM) work, Malisiewicz al @cite propose to learn discriminative templates for each object instance of the training set independently and then combine their calibrated outputs on test images as a post-processing step. In contrast, we work at a part level and use all templates together during both training and testing. More recently, Yan al @cite proposed a 2-level approach for image representation. Similar to our approach it involves sampling image regions, but while they vector quantize the region descriptors, we propose a mechanism to select discriminative regions and build discriminative part based models from them.
- Works have also been reported using features which exploit motion for recognizing and localizing human actions in videos @cite @cite @cite @cite @cite @cite . Wang and Schmid @cite use trajectories, Jain al use tubelets @cite while Simonyan al @cite propose a two-stream convolutional network. Here, we are interested in human action and attribute recognition, but only from still images and hence do not have motion information.
- In mixture of components models, the training images are usually assigned to a single component (see Fig. for an illustration) and thus contribute to training one of the templates only. Such clustering like property limits their capability to generate novel articulations, as sub-articulation in different components cannot be combined. Such clustering and averaging are a form of regularization and involve manually setting the number of parts and components. In comparison, the proposed EPM does not enforce similar averaging, nor does it forbid it by definition. It can have a large number of parts (up to the order of the number of training images) if found necessary despite sufficient regularization. Part-based deformable models initialize the parts either with heuristics ( regions with high average energy @cite ) or use annotations @cite , while EPM systematically explores parts at a large number of locations, scales and atomicities and selects the ones best suited for the task.
- Many methods have also been proposed to reconstruct images using patches, Similarity by Composition by Boiman and Irani @cite , Implicit Shape Models by Leibe al @cite , Naive Bayes Nearest Neighbors (NBNN) by Boiman al @cite , and Collaborative Representation by Zhu al @cite . Similarly sparse representation has been also used for action recognition in videos @cite . However, while such approaches are generative and are generally based on minimizing the reconstruction error, EPM aims to mine out good patches and learn corresponding discriminative templates with the direct aim of achieving good classification.
- In addition to the works mention above, we also refer the reader to Guo and Lai @cite , for a survey of the general literature for the task of human action recognition from still images.
- Present classification algorithms used for Moderate-resolution Imaging Spectroradiometer (MODIS)(500-m) @cite or Landsat(30-m) based land cover maps like NLCD @cite produce accuracies of 75
- A key computation in our work is the evaluation of the Connes--Karoubi character @math on Steinberg symbols @math . This is known to equal the joint torsion of Toeplitz operators [Prop. 1] Carey-Pincus:1999 , thanks to a combination of recent results of Kaad @cite and Migler @cite that enable the joint torsion, in this case, to be matched with the Connes--Karoubi character. (For details, see the remark at the end of .) However, since our work does not concern joint torsion, we choose to execute a direct, and considerably shorter, computation of @math , which employs the determinant theory of Helton and Howe @cite , but is otherwise fairly elementary. The validity of the reconstruction formula for the full @math -group is, to the best of our knowledge, a new result.
- Generally speaking, object tracking is the temporal estimation of the object's kinematic state. In the context of image-based tracking, the object state is typically a parametrization of its localization in the (2D) image plane. In computer vision, object tracking has been thoroughly investigated @cite . Objects of interest could be people, faces, hands, vehicles, etc. According to the considered number of objects to be tracked, tracking can be classified into single-object tracking, fixed-number multi-object tracking, and varying-number multi-object tracking.
- Methods for single object tracking consider only one object and usually involve an initialization step, a state update step, and a reinitialization step allowing to recover from failures. Practical initialization steps are based on generic object detectors allowing to scan the input image in order to find the object of interest @cite @cite . Object detectors can be used for the reinitialization step as well. However, using generic object detectors is problematic when other objects of the same kind than the tracked object are present in the visual scene. In order to resolve such ambiguities, different complementary appearance models have been proposed, such as object templates, color appearance models, edges (image gradients) and texture, (e.g. Gabor features and histogram of gradient orientations). Regarding the update step, the current state can be estimated from previous states and observations with either deterministic @cite or probabilistic @cite methods.
- Even if it is still a challenging problem, tracking a single object is very limited in scope. Rapidly, the computer vision community drove its attention towards fixed-number multi-object tracking @cite . Additional difficulties are encountered when tracking multiple objects. Firstly, there is an increase of the tracking state dimensionality as the multi-object tracking state dimensionality is the single object state dimensionality multiplied by the number of tracked objects. Secondly, associations between observations and objects are required. Since the observation-to-object association problem is combinatorial @cite @cite , it must be carefully addressed when the number of objects and of observations are large. Thirdly, because of the presence of multiple targets, tracking methods have to be robust also to mutual occlusions.
- Our work is an adaptation of these classes of policies for MAB Problem with known trend reward function. Our work is most related to the study of dynamic versions of the MAB where either the set of arms or their expected reward may change over time. There are several applications, including active learning, music and interface recommendation, where the rewards are far from being stationary random sequences. A solution to cope with non-stationary is to drop the stochastic reward assumption and assume the reward sequences to be chosen by an adversary. Even with this adversarial formulation of the MAB problem, a randomized strategy like EXP3 provides the guarantee of a minimal regret @cite @cite .
- Another work done in @cite considers the situation where the distributions of rewards remain constant over epochs and change at unknown time instants. They analyze two algorithms: the discounted UCB and the sliding-window UCB and they establish for these two algorithms an upper-bound for the expected regret by upper-bounding the expectation of the number of times a suboptimal arm is played. They establish a lower-bound for the regret in presence of abrupt changes in the arms reward distributions.
- Similar to @cite , authors in @cite propose a Thompson Sampling strategy equipped with a Bayesian change point mechanism to tackle this problem. They develop algorithms for a variety of cases with constant switching rate: when switching occurs all arms change (Global Switching), switching occurs independently for each arm (Per-Arm Switching), when the switching rate is known and when it must be inferred from data.
- Motivated by task scheduling, the author in @cite proposed a policy where only the state of the arm currently selected can change in a given step, and proved its optimality for time discounting. This result gave rise to a rich line of work. For example, @cite @cite studied the restless bandits, where the states of all arms can change in each step according to an arbitrary stochastic transition function.
- In @cite author study specific classes of drifting restless bandits selected for their relevance to modelling an online website optimization process. The contribution was a feasible weighted least squares technique capable of utilizing contextual arm parameters while considering the parameter space drifting non-stationary within reasonable bounds.
- Golder and Macy @cite studied collective mood in Twitter across countries from 509 million Twitter posts by 2.4 million users over a 2-year period. Despite having different cultures, geographies, and religions, all countries (USA, Canada, UK, Australia, India, and English-speaking Africa) in their study showed similar mood rhythms---people tended to be more positive on weekends and early in the morning. @cite examined the variation of Twitter users' emoticon usage patterns in cross cultures. They used Hofstede's national culture scores of 78 countries and found that collectivist cultures favor vertical and eye-oriented emoticons, where people within individualistic cultures favor horizontal and mouth-oriented emoticons. Hofstede's cultural dimensions have also been used to study whether culture of a country is associated with the way people use Twitter @cite . In another study on cross-country Twitter communication, showed that cultural variables such as Hofstede's indices, language and intolerance have an impact on Twitter communication volume @cite .
- @cite used food and drink check-ins in Foursquare to identify cultural boundaries and similarities across populations. They showed that online footprints of foods and drinks are good indicators of cultural similarities between users, e.g., lunch time is the perfect time for Brazilians to go for slow food places more often, whereas Americans and English people go for slow foods more at dinner time. Extracted features like these allow them to apply simple clustering algorithms such as K-means to draw cultural boundaries across the countries.
- Quercia @cite used tests and measured happiness of 32,787 Facebook users from 12 countries (Australia, Canada, France, Germany, Ireland, Italy, New Zealand, Norway, Singapore, Sweden, UK, USA ). He found that despite comparative economic status, country-level happiness significantly varies across the countries and that it strongly correlates with official well-being scores.
- @cite used about 1.5 million Doodle polls from 211 countries and territories and studied the influence of national culture on people's scheduling behavior. Using Hofstede's cultural dimensions, they found that Doodle poll participants from collectivist countries find more consensus than those from predominantly individualist societies.
- Due to the complexity of nodes' counting, numerous research works focus on estimating the cardinality and mean edges degree in a graph. The authors in @cite @cite state a method for determining the total number of nodes in a graph using data flooding and random walks. They present an algorithm for node estimation in large-scale wireless sensor networks using random walks that travel through the network based on a predefined probability distribution.
- @cite @cite propose a scheme for estimating the network parameters in directed graphs using random walks. Their algorithm precisely predicts the out-degree distributions of a variety of real-world graphs. Similarly, @cite study the problem in a context of robust visual object recognition. The authors in @cite present two algorithms for estimating the total number of online users in social networks by using sampling from graphs assumed to have a stationary distribution.
- The authors in @cite @cite propose a model for estimating the network parameters using random walks in graphs. In particular, by sampling from the graph, they propose a method to determine the average edge degree rather than the individual node degree. The problem of estimating the mean degree of a graph is first suggested by @cite . The authors in @cite present a sampling method for node degree estimations in a sampled network and the authors in @cite show a way to obtain content properties by testing a small set of vertices in the graph.
- While the authors in @cite propose a model for distributed cardinality estimation in anonymous networks using statistical inference methods, the authors in @cite present a scheme for estimating the number of reachable neighbours for a given node and a size of the transitive closure. They present an @math time complexity algorithm based on Monte Carlo that estimates, with a small error, the sizes of all reachability sets and the transitive closure of a graph.
- Keyword queries facilitate the user with the ease of freely forming queries by using only keywords. Approaches that evaluate keyword queries are currently very popular especially in the web where numerous sources contribute data often with unknown structure and where end users with no specialized skills need to find useful information. However, the imprecision of keyword queries results often in low precision and or recall of the search systems. Some approaches (a) combine structural constraints @cite with keyword search while others that (b) try to infer useful structural information implied by simple keyword queries @cite @cite by exploiting statistical information of the queries and the underlying datasets. These approaches require a minimum knowledge of the dataset or a heavy dataset preprocessing in order to be able to accurately assess candidate keyword query results.
- The structural properties of various power grids (e.g., in North America, some European countries, and Iran) were studied in @cite @cite @cite @cite @cite @cite . Most of these studies considered one or two properties (e.g., average degree, degree distribution, average path length, and clustering coefficient) and computed it in a given power grid. In some cases (e.g., @cite @cite @cite @cite @cite @cite @cite ) a certain class of graphs was suggested as a good representative of a power grid network, based on one or two structural properties. For example, Watts and Strogatz @cite suggested the small-world graph as a good representative, based on the shortest path lengths between nodes and the clustering coefficient of the nodes. Barab ' a si and Albert @cite showed that scale-free graphs are better representatives based on the degree distribution. However, by comparing the WI with these models, Cotilla-Sanchez, et al @cite showed that none of them can represent the WI properly.
- More detailed models that are specifically tailored to the power grid characteristics were proposed in @cite @cite but they did not consider the nodes' and the length distribution of the lines. The spatial distribution of the nodes is correlated with the length of the lines, and as mentioned above, it is important to consider line lengths when designing a method for synthetic power grid generation. While there are several models for generating spatial networks @cite @cite @cite , most of them were not designed to generate networks with properties similar to power grid networks. To the best of our knowledge, this paper is the first to consider the spatial distribution of the nodes in power grids and its importance in generating synthetic networks with similar structural properties.
- Deep neural networks have recently brought about significant advancements in various natural language processing tasks, such as language modeling @cite @cite , sentiment analysis @cite @cite @cite , syntactic parsing @cite @cite @cite and machine translation @cite @cite @cite . Models applied in these tasks exhibit significant architectural differences, ranging from recurrent neural networks @cite @cite to recursive models @cite @cite , and including convolutional neural nets @cite @cite @cite @cite @cite @cite .
- @cite consider the case where each item is of a different colour and can be fractionally (arbitrarily) divided between bins, bins have different sizes and the total weight of items exactly equals the total weight of bins. They show how to compute an allocation that is asymptotically optimal for each colour. By contrast, we relax the assumption that we must exactly fill all the bins, and consider the case of indivisible allocations. In this setting, the problem is much more interesting: it is impossible to get arbitrarily good @math -approximate allocations in general. Thus, these relaxed packings have a tradeoff between bin stretch and colour stretch, with polynomial-time approximations. We also consider for the first time the case where items arrive online. However, the case of heterogenous bins is open for our setting.
- There are some other variants of bin packing problems with colours, for example the so called Colored Bin Packing that has the restriction that items of a same colour cannot be packed next to each other on a same bin. Approximation algorithms for the online version of the problem were presented by B "o @cite and Dosa and Epstein @cite . A generalization where the objective is to pack a graph @math into another graph @math where nodes into @math have capacities and nodes in @math corresponds to items of given size, was studied by Bujt 'a in @cite
- The class-constrained bin packing problem', studied by @cite , @cite , @cite and @cite is a coloured bin packing problem. The aim is to minimize the number of bins used, subject to the constraint that each bin contains items from at most @math different colours (and subject to its capacity constraints). This problem has applications in developing algorithms for data placement on parallel disk arrays. Again, optimal solutions to this problem may be arbitrarily far from good if we wish to minimize colour stretch.
- As demonstrated in , Theorem is a straightforward consequence of Lemma and Lemma . The authors of @cite and @cite independently obtained ) for OMP. To the best of the authors' knowledge, the second ERC was first obtained simultaneously in @cite and @cite while ) was initially published in @cite , both ERC being derived for OMP.
- Regarding older works, it is also worth pointing out that the ERC @math , first obtained in [Theorem 5.2] liu2012orthogonal for OMP, has been shown to remain valid for SOMP in [Corollary 1] ding2012robustness . Thereby, the authors of @cite also proved that the older ERC @math , initially derived in [Theorem 3.1] davenport2010analysis for OMP, remains correct for SOMP as @math is implied by @math . Very recently, ) was extended to SOMP in [Remark 1] xu2015perturbation . However, the extension to SOMP of both ) and ) is a novel result. In @cite , the author has derived the ERC @math , which is sharper than ). Combining the ideas developed in @cite and our paper could possibly extend this ERC to SOMP.
- We address the general challenge of , aiming at both recognition and analysis of sentiment and emotions in visual data, but from a multilingual and cross-cultural perspective. Our work is closely related to Multimedia and Vision research that focus on visual aesthetics @cite , interestingness @cite , popularity @cite , and creativity @cite . Our work also relates to research in Cognitive and Social Psychology, especially emotion and culture research @cite @cite @cite , but also neuroaesthetics @cite , visual preference @cite @cite , and social interaction @cite .
- Other work in cross-lingual research comes from text sentiment analysis and music information retrieval. In @cite and @cite , they developed multilingual methods for international text sentiment analysis in online blogs and news articles, respectively. In @cite and @cite , they presented approaches to indexing digital music libraries with music from multiple languages. Specific to emotion, @cite tried to highlight differences between languages by building models for predicting the musical mood and then cross-predicting in other languages. Unlike these works, we propose a multimedia-driven approach for cross-cultural visual sentiment analysis in the context of online image collections.
- It is important to distinguish our work from that of on VSO @cite and its associated detector bank, SentiBank @cite . Their mid-level representation approach has recently proven effective in a wide range of applications in emotion prediction @cite @cite , social media commenting @cite , etc. However, in addition to lack of multilingual support, there are several technical challenges with VSO @cite @cite that we seek to improve on via (1) detection of adjectives and nouns with language-specific part-of-speech taggers, as opposed to a fixed list of adjectives and nouns, (2) automatic discovery of adjective-noun pairs correlated with emotions, as opposed to constructed'' pairs from top frequent adjectives and nouns, and (3) stronger selection criterion based on image tag frequency, linguistic and semantic filters and crowdsource validation.. Our proposed MVSO discovery method can be easily extended to any language, while achieving greater coverage and diversity than VSO.
- Randomly subsampling subgraphs by including each edge independently in the sample with probability @math has been studied extensively in works concerned with cuts and flows (e.g., @cite ). More recently, sampling subgraphs has been used to find independent sets @cite (the main sampling technique used, e.g., the layers model is not independent-it introduces dependencies between sampled vertices). The effect of subsampling variables in mathematical relaxations of constraint satisfaction problems on the value of these relaxations was studied in @cite . Edge-percolated graphs have been also used to construct hard-instance for specific algorithms. For example, @cite proved that the well known greedy coloring algorithm performs poorly on the complete @math -partite graph in which every edge is removed independently with probability @math and @math for @math . Namely, for this graph @math , even if vertices are considered in a random order by the greedy algorithm, with high probability @math colors are used to color the percolated graph whereas @math .
- When @math is sufficiently small, algorithms for random graphs and random formulas can be proven to find the optimal solution (with high probability) for percolated instance. For example, for graph coloring, it is known that for @math with some positive constant @math , with high probability @math is @math -degenerate, and hence can be 3-colored in polynomial time @cite . Since the property of being 2-degenerate is monotone, and as @math -colorability can be decided in polynomial time, it follows that for every @math -vertex graph and @math , one can find in polynomial time with high probability a coloring of the edge percolated graph with the minimum number of colors.
- Similar reasoning applies to @math formulas. It is well known that there exists @math such that a random @math formula in which each possible clause is added independently with probability @math can be solved with high probability using the pure literal heuristic @cite . As observed in @cite , if this heuristic fails in finding a satisfying assignment for a formula @math it will still fail to find a satisfying assignment if clauses are added to @math . This implies that for any @math -variable @math formula @math , if @math , then the clause-percolated formula is satisfiable and furthermore a satisfying assignment can be found in polynomial time using the pure literal heuristic.
- Conventional approaches to NLG typically divide the task into sentence planning, and surface realisation. Sentence planning maps input semantic symbols into an intermediary tree-like or template structure representing the utterance, then surface realisation converts the intermediate structure into the final text @cite @cite @cite . As noted above, one of the first statistical NLG methods that requires almost no handcrafting or semantic alignments was an n-gram based approach by . later addressed the limitations of n-gram LMs in the over-generation phase by using a more sophisticated generator based on a syntactic dependency tree.
- Statistical approaches have also been studied for sentence planning, for example, generating the most likely context-free derivations given a corpus @cite or maximising the expected reward using reinforcement learning @cite . train a set of log-linear models to predict individual generation decisions given the previous ones, using only domain-independent features. Along similar lines, by casting NLG as a template extraction and reranking problem, show that outputs produced by an SVM reranker are comparable to human-authored texts.
- A specific requirement of NLG for dialogue systems is that the concepts encoded in the abstract system dialogue act must be conveyed accurately by the generated surface utterance, and simple unconstrained RNNLMs which rely on embedding at the word level @cite @cite are rather poor at this. As a consequence, new methods have been investigated to learn distributed representations for phrases and even sentences by training models using different structures @cite @cite . Convolutional Neural Networks (CNNs) were first studied in computer vision for object recognition @cite . By stacking several convolutional-pooling layers followed by a fully connected feed-forward network, CNNs are claimed to be able to extract several levels of translational-invariant features that are useful in classification tasks. The convolutional sentence model @cite @cite adopts the same methodology but collapses the two dimensional convolution and pooling process into a single dimension. The resulting model is claimed to represent the state-of-the-art for many speech and NLP related tasks @cite @cite .
- While Galaxy is widely used, especially by the biomedical community, the data management capabilities are quite restricted for large data and necessitate data transfers between single jobs of a workflow to the server hosting the back-end of Galaxy. However, Galaxy can map to the highly parallelized enactments of Swift @cite . Another workflow system well established in the biomedical community is Taverna but the workflow canvas is only available as a workbench. The workflows can be shared via the social website myExperiment @cite .
- Security in Location-based Services. Location-based services face various threats, ranging from rogue users reporting fake GPS @cite @cite , to malicious parties compromising user privacy @cite @cite @cite . A related study on Waze @cite demonstrated that small-scale attacks can create traffic jams or track user icons, with up to 15 mobile emulators. Our work differs in two key aspects. First, we show that it's possible to reverse engineer its APIs, enabling light-weight Sybil devices (simple scripts) to replace full-stack emulators. This increase the scale of potential attacks by orders of magnitude, to thousands of Waze clients per commodity laptop. The impact of thousands of virtual vehicles is qualitatively different from 10-15 mobile simulators. Second , as possible defenses, @cite cites known tools such as phone number IP verification, or location authentication with cellular towers, which have limited applicability (see ). In contrast, we propose a novel proximity graph approach to detect and constrain the impact of virtual devices.
- Researchers have proposed to preserve user location privacy against map services such as Waze and Google. Earlier studies apply location cloaking by adding noise to the GPS reports @cite . Recent work use zero-knowledge @cite and differential privacy @cite to preserve the location privacy of individual users, while maintaining user accountability and the accuracy of aggregated statistics. Our work differs by focusing on the attacks against the map services.
- Mobile Location Authentication. Defending against forged GPS is challenging. One direction is to authenticate user locations using wireless infrastructures: WiFi APs @cite @cite , cellular base stations @cite @cite and femtocells @cite . Devices must come into physical proximity to these infrastructures to be authenticated. But it requires cooperation among a wide range of infrastructures (also modifications to their software hardware), which is impractical for large-scale services like Waze. Our work only uses a small number of trusted infrastructures to bootstrap, and relies on peer-based trust propagation to achieve coverage. Other researchers have proposed peer-based'' methods to authenticate collocated mobile devices @cite @cite @cite @cite @cite . Different from existing work, we use peer-based collocation authentication to build proximity graphs for Sybil detection, instead of directly authenticating a device's physical location.
- Sybil Detection. Sybil detection has been studied in P2P networks @cite and online social networks @cite @cite @cite . The most popular approach is graph-based where the key assumption is that Sybils have difficulty to connect to real users @cite @cite @cite @cite @cite . Thus Sybils would form a well-connected subgraph that has a small quotient-cut from the non-Sybil region. Our work constructs a proximity graph that holds the same assumption, and applies Sybil detection algorithm to locate ghost riders in Waze. We differ from @cite in the graph used and the attack models.
- The development of energy harvesting devices has attracted significant attention in recent years with many potential applications in communication networks for green and self-sustainable communications @cite . In order to make efficient use of harvested energy, both offline and online solutions have been investigated for designing the optimal transmission policy. Offline solutions are possible in highly predictable environments where the energy and data arrivals in a sufficiently distant future (for communication purpose) can be accurately estimated @cite @cite @cite @cite @cite @cite . On the other hand, online solutions typically reduce the dependence on the future knowledge of the energy and data arrival processes, and hence are more applicable in practice. The online algorithms can be roughly categorized into two frameworks as follows:
- The second framework uses parameter-independent methodologies. @cite , the energy and data arrival processes were formulated as time-homogeneous Markov chains without knowing the transition matrix, and Q-learning was applied to perform online optimization on the transmission policy. By Lyapunov optimization technique combined with the idea of weight perturbation, @cite proposed a generic utility-maximization policy, under the assumption that the amount of harvested energy in each time slot is independent and identically distributed (i.i.d.) but its statistical parameters are totally unknown. Although these parameter-independent policies require less knowledge on the energy and data arrival processes, the stochastic models of the energy and data arrival processes still need to be known exactly.
- Our paper is mostly related to the recent work in @cite @cite @cite , which considered the similar assumptions that the future energy arrival is unknown. @cite @cite @cite , the competitive analysis @cite was employed to minimize the gap between online and offline performances. However, minimizing this gap cannot directly guarantee a certain system performance (e.g., optimal worst-case performance) of online algorithms. Additionally, these recent studies still considered time-slotted systems, and hence, the transmission protocol is updated in every time slot, regardless of the change in the amount of energy available.
- Predictability as presented in this paper was introduced by Genc and Lafortune @cite . Their approach was however only Boolean: they addressed the question can the fault be predicted before it occurs?'' They presented an exponential space algorithm using a structure similar to our optimal predictor. They also announced the existence of a polytime algorithm, similar to the twin plant used for diagnosability and formally presented in an extension of their work @cite .
- Together with J 'eron and Marchand, they proposed an additional improvement to lower the complexity down to quadratic @cite . We claim here that their algorithm is not quite quadratic (we discuss this question at the end of this section). Their approach is very similar to the approach presented in the previous section: They construct a twin plant and verify predictability by checking whether there exists a pair @math such that @math and @math .
- Brand 'an Briones and Madalinski presented the notions of @math -predictability and @math -predictability @cite . @math -predictability is similar to our definition of @math -predictability meaning that the fault is predicted at least @math observations before the fault occurs. @math -predictability is the equivalent of our property of @math -predictability, meaning that it is possible to predict the fault occurrence before it occurs but when at most @math observations are still possible before the fault (in other words, the fault prediction is not too early).
- @PARASPLIT While this is a minor issue, we provide an example and a comprehensive discussion that illustrate the complexity error from J ' @cite . Consider the example of Figure a. This DES includes @math states and @math transitions. The single observable event is @math and the single unobservable event is @math (this example does not feature any faulty event). The twin plant then consists in @math states and @math transitions (details in Table ). The @math -reduction, presented on Figure b, contains one state fewer than the original DES but @math transitions. As a consequence, the number of states in the twin plant reduces down to @math but the number of transitions shoots up to @math (details in Table ).
- A first category of related work considers applying signal strength measurement at the receiver as a way to estimate the distance variation between two nodes, having as ultimate goal providing a way to build more robust paths. For instance, the @cite relies on the variation of the received signal strength by a particular node to predict link breaks. @cite have also applied signal strength measurement as a way to build more robust paths. applied the notion of received signal strength to track the distance variation between two nodes, in a specific time period @cite . Their distance change tracking approach is a desirable feature as it can capture movement of nodes and the impact on links. However, and similarly to the other work in this category, it does not track movement patterns.
- A second category of work that tries to make routing more sensitive to node mobility relates to throughput variation measurement as a way to determine node mobility. For instance, Suyang and Evans have used the slope of change of throughput in a link vs. the link load to estimate topology changes @cite . Based on history, through throughput monitoring, a decrease, according to Suyang and Evans, means that there is a true change in the physical topology. They have attributed the changes to increase in the node distance and increase in interference. Even with good attributes of avoiding interference and detecting mobility collectively, node mobility individually has attributes that have not been addressed. Nodes may exhibit movement that does not necessarily impact the route stability.
- A fourth category of work relies on as a measure of improving routing in terms of mobility sensitivity. @cite have proposed whose basic idea is to detect link changes in a quicker way, by increasing the HELLO sending rate. Albeit interesting, such rate only assists in understanding that some nodes may be on the move, but not exactly which.
- Other researchers have recognized the need for algorithms with low per-iteration costs that scale well for large-scale, low-rank spectral optimization problems. Notable efforts include @cite , @cite , and @cite , who advocate variations of the Frank-Wolfe (FW) method to solve some version of the problem where @math is a differentiable function. For example, the choice @math yields a problem related to . The asymmetric version of the problem with @math is easily accommodated by simply replacing the above constraints. For simplicity, here we focus on the symmetric case, though our approach applies equally to the asymmetric case. The main benefit of using FW for this problem is that each iteration requires only a rightmost eigenvalue of the gradient @math , and therefore has the same per-iteration cost of the method that we consider, which requires a rightmost eigenvalue of the same-sized matrix @math . The same Krylov-based eigensolvers apply in both cases.
- There are at least two issues that need to be addressed when comparing the FW algorithm to the approach we take here. First, as @cite make clear, even in cases where low-rank solutions are expected, it is not possible to anticipate the rank of early iterates @math generated by the FW method. In particular, they observe that the rank of @math quickly increases during early iterations, and only slowly starts to decrease as the solution is approached. This motivates their development of algorithmic devices that attenuate rank growth in intermediate iterates. Any implementation, however, must be prepared to increase storage for the factors of @math during intermediate iterates. In contrast, a subgradient method applied to the gauge dual problem can be implemented with constant storage. Second, although in principle there exists a parameter @math that causes the optimization problems and to share the same solution, this parameter is not generally known in advance. One way around this is to solve a sequence of problems for varying parameters @math using, for example, a level-set procedure described by * 2016arXiv160201506A .
- As an alternative to applying the FW algorithm to , we might instead consider applying a variation of the FW method directly to the gauge dual problem . Because the gauge dual objective is not differentiable and the feasible set is not compact if @math , some modification to the standard FW method is required. @cite , @cite , and @cite propose variations of FW that involve smoothing the objective. These smoothing approaches are typically based on infimal convolution with a smooth kernel, which may lead to a function whose gradient is expensive to compute. For example, the soft-max'' smooth approximation of @math is the function @math . Forming the gradient of this smooth function requires computing all eigenvalues of an @math -by- @math Hermitian matrix.
- The stochastic block model was first introduced within the social science literature in @cite . Around the same time, it was studied within theoretical computer science @cite @cite , under the name of planted partition model. '
- A large part of the literature has focused on the problem of of the community (cluster) structure. A long series of papers @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite , establishes sufficient conditions on the gap between @math and @math that guarantee exact recovery of the vertex labels with high probability. A sharp threshold for exact recovery was obtained in @cite @cite , showing that for @math , @math , @math , exact recovery is solvable (and efficiently so) if and only if @math . Efficient algorithms for this problem were also developed in @cite @cite @cite . For the SBM with arbitrarily many communities, necessary and sufficient conditions for exact recovery were recently obtained in @cite . The resulting sharp threshold is efficiently achievable and is stated in terms of a CH-divergence.
- A parallel line of work studied the problem. In this case, the estimated community structure is only required to be asymptotically positively correlated with the ground truth. For this requirement, two independent groups @cite @cite proved that detection is solvable (and so efficiently) if and only if @math , when @math , @math . This settles a conjecture made in @cite and improves on earlier work @cite . Results for detection with more than two communities were recently obtained in @cite @cite @cite @cite . A variant of community detection with a single hidden community in a sparse graph was studied in @cite .
- Let us finally mentioned that the result obtained in this paper are likely to extend to more general SBMs, with multiple communities, to the Censored Block Model studied in @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite , the Labeled Block Model @cite @cite , and other variants of block models. In particular, it would be interesting to understand which estimation-theoretic quantities appear for these models, and whether a general result stands behind the case of this paper.
- While this paper was in preparation, Lesieur, Krzakala and Zdborov 'a @cite studied estimation of low-rank matrices observed through noisy memoryless channels. They conjectured that the resulting minimal estimation error is universal across a variety of channel models. Our proof (see Section below) establishes universality across two such models: the Gaussian and the binary output channels. We expect that similar techniques can be useful to prove universality for other models as well.
- Query auto-completion is the second main direction for instant response to queries in the typing, by which some top query completions are presented to the user (see for example @cite @cite @cite and the references therein). This is done either by following a predictive approach, or by pre-computing completion candidates and storing them in trie structures. Probably the best known example today is the one of Google's instant search, which provides both query predictions (in the search box) and results for the top prediction. Query suggestion goes one step further by proposing alternative queries, which are not necessarily completions of the input one (see for instance @cite @cite ). In comparison, our work does not focus on queries as first-class citizens, but on instant results to incomplete queries.
- Person (or people) search represents another facet of social search'', related to this paper, as the task of finding highly relevant persons for a given seeker and keywords. Usually, the approach used in this type of application is to identify the most relevant users, and then to filter them by the query keywords @cite @cite . In this area, @cite describes the main aspects of the Unicorn system for search over the Facebook graph, including a typeahead feature for user search. A similar search problem, finding a sub-graph of the social network that connects two or more persons, is considered under the instant search paradigm in @cite .
- Saveski @cite perform joint factorisation of the user-item and item-feature matrices by using the same item latent feature matrix in both decompositions; the parameters are optimised by minimising a weighted sum of both matrices' reproduction loss functions. A weight hyperparameter governs the relative importance of accuracy in decomposing the collaborative and content matrices. A similar approach is used by McAuley @cite for jointly modelling ratings and product reviews. Here, LightFM has the advantage of simplicity as its single optimisation objective is to factorise the user-item matrix.
- Shmueli @cite represent items as linear combinations of their features' latent factors to recommend news articles; like LightFM, they use a single-objective approach and minimise the user-item matrix reproduction loss. They show their approach to be successful in a modified cold-start setting, where both metadata and data on other users who have commented on a given article is available. However, their approach does not extend to modelling user features and does not provide evidence on model performance in warm-start scenario.
- Recently, there have been several distributed optimization algorithms proposed for problem . We list several of them, including a distributed implementation of the accelerated gradient method (Accel Grad) by Nesterov @cite This is the accelerated gradient method by Nesterov @cite except that the data points are distributed in @math machines to parallelize the computation of @math . , in Table and present their rounds and runtime for a clear comparison. The algorithms proposed in this paper are DSVRG and DASVRG.
- Let @math be the computation cost of the gradient of @math . For typical problems such as logistic regression, we have @math . Let @math be the cost of solving a linear system. We have @math if exact matrix inversion is used and have @math if an @math -approximate inverse is found by an accelerated gradient method @cite .
- The distributed dual coordinate ascent method, including DisDCA @cite , CoCoA @cite and CoCoA+ @cite , is a class of distributed coordinate optimization algorithms which can be applied to the conjugate dual formulation of . In these methods, each machine only updates @math dual variables contained in a local problem defined on the @math local data points. Any optimization algorithm can be used as a subroutine in each machine as long as it reduces the optimality gap of the local problem by a constant factor. According to @cite @cite , CoCoA+ requires @math rounds of communication to find an @math -optimal solution CoCoA+ has a better theoretical performance than CoCoA. According to @cite , CoCoA+ is equivalent to DisDCA with practical updates'' @cite under certain choices of parameters. . If the accelerated SDCA method @cite @cite is used as the subroutine in each machine, the total runtime for CoCoA+ is @math . Therefore, both DSVRG and DASVRG have lower runtime and communication than CoCoA+, and the other distributed dual coordinate ascent variants.
- Assuming the problem has the form of with @math 's i.i.d. sampled from a distribution @math (denoted by @math ), the DANE @cite and DISCO @cite algorithms require @math and @math rounds of communication, respectively. Hence, DSVRG uses fewer rounds of communication than DANE and fewer than DISCO when @math . DASVRG always uses fewer rounds of communication than DISCO and DANE. Note that, for these four algorithms, the rounds can be very small in the big data'' case of large @math . Indeed, as @math increases, all four methods require only @math rounds which is independent of the condition number @math .
- We also make the connection to the recent lower bounds @cite for the rounds of communication needed by distributed optimization. Arjevani and Shamir @cite prove that, for a class of @math -related functions (see @cite for the definition) and, for a class of algorithms, the rounds of communication achieved by DISCO is optimal. However, as mentioned above, DASVRG needs fewer rounds than DISCO. This is not a contradiction since DASVRG does not fall into the class of algorithms subject to the lower bound in @cite . In particular, the algorithms concerned by @cite can only use the @math local data points from the initial partition to update the local solutions while DASVRG samples and utilizes a second set of data points in each machine in addition to those @math data points.
- Building on the work of @cite , we prove a new lower bound showing that any distributed first-order algorithm requires @math rounds. This lower bound combined with the convergence analysis of DASVRG shows that DASVRG is optimal in the number of rounds of communication.
- In Table , we compare the rounds and runtime of DSVRG, DASVRG and DISCO in the case where @math , which is a typical setting for ERM problem as justified in @cite @cite @cite @cite . We only compare our methods against DISCO, since it uses the fewest rounds of communication among other related algorithms. Let us consider the case where @math , which is true in almost any reasonable distributed computing scenario. We can see from Table that the rounds needed by DSVRG is lower than that of DISCO. In fact, both DSVRG and DASVRG use @math rounds of communication, which is almost a constant for many practical machine learning applications Typically @math , so @math is always less than @math . . .
- In @cite -[6], the fairness in resource allocation is considered as a more important issue. The fairness QoS constraints have been set in the optimal resource allocation in @cite and @cite . In @cite , the authors provided an opportunistic power scheduling scheme for "multi-server" wireless systems while meeting the minimum QoS for each user. A stochastic process has been used to present each user's performance value in @cite and they proposed an opportunistic transmission-scheduling policy to maximize the average system performance. In @cite , Multi-channel Fair Scheduler (MFS) has been introduced and analyzed to guarantees both long-term deterministic (MFS-D) and probabilistic (MFSP) fairness over multiple wireless channels. They provided a framework that maximizes total system throughput for opportunistic scheduling over multiple wireless channels.
- Later in @cite , the study introduced a novel approach for power allocation in the cellular network where the user's utility function has been modeled as sigmoidal-like function. In this work, the power allocation optimization problem is formulated as a product of the utilities of all users with utility proportional fairness policy. A priority has been assigned to users with lower modulation schemes, at the same time giving non-zero power allocation to users using higher modulation schemes. A similar method was used to allocate optimal rates in @cite , @cite
- The MCS selection has been widely studied. Work in @cite - @cite has proposed adaptive modulation and coding (AMC) to enhance the system throughput according to the channel quality. The CQI, the only feedback to BS, corresponds to a resource block (RB) or multiple RB's in the form of MCS index @cite . And CQI value provides important information in link adaption. The study in @cite developed several MCS selection schemes for downlink transmission in LTE systems by using the effective packed-level SINR. Thresholds were set to the SINR values with the Block Error Rate (BLER) smaller than 10 This paper is organized as follows. We first introduce the system model set up in Section . Then we review the CQI with more details in Section . In Section , we describe the solution for mapping the CQI values to the utility function. Section that briefly describes the optimal power allocation algorithm that was proposed in @cite . In Section , we discuss the simulation set up and the results along with a discussion. Finally, Section concludes this paper.
- Device pairing by shaking was first presented in Smart-Its Friends'' @cite . The drawback of this technique was lacking of authentication of the involved devices in the interaction. Another closely related work included accelerometer based analysis to determine whether the devices are carried by the same person or not @cite . The feature extraction algorithms used in @cite includes coherence measure, which was originally introduced in @cite , and quantized FFT coefficients. In @cite , a similar approach in which acceleration signal was used for key generation was presented with a difference in using time domain acceleration features contrary to @cite where only frequency domain features were used. They claim that the same key would rather likely be generated for different shaking processes if the key generation was based on a frequency-based technique. They concluded that the frequency domain is not suitable for key generation.
- Data integration and exchange (e.g., @cite ) are the mostly studied areas in semantic integration. The main task of data integration and exchange is to answer queries posed in terms of the global schema given source databases. The standard semantics of global query answering is to return the tuples in every possible database that is consistent with the global schema constraints and the mapping, i.e., the set of certain answers .
- Heterogeneous DDM @cite also handles the semantic heterogeneity between the global and local schemas, in particular, those containing attributes with different granularities called Attribute Value Taxonomy (AVT). Heterogeneous DDM requires local data resources and their mappings to the global schema to translate the statistics of queries. However, KT does not require data or statistics from either the source or the target. Instead, KT uses mappings to translate the generated mined knowledge from the source directly.
- Most transfer learning work focuses on the homogeneous case in which the source and target domain have identical attributes. The main exceptions are heterogeneous transfer learning @cite and relational transfer learning (e.g., TAMAR @cite , deep transfer @cite ). Heterogeneous transfer learning deals with different representations of the data (e.g., text and images of an object). While it uses an implicit mapping of two feature spaces (e.g., through Flickr), KT uses an explicit mapping via FOL formulas. Relational transfer learning deals with two analogous domains (e.g., in movie and university domains, directors correspond to professors ). In contrast, KT focuses on a single domain with two different representations . Moreover, relational transfer learning only handles deterministic one-to-one matchings which can be inferred by using a small amount of target data, while KT does not use any target data, and relies on the provided explicit FOL mapping.
- Deductive knowledge translation @cite essentially tries to solve the same problem, but it only considers deterministic knowledge and mappings. Our KT work can handle knowledge and mappings with uncertainty, which is more general than the deterministic scenario deductive knowledge translation @cite can handle.
- Roboticists have employed RFID to great effect. The unique identifier has proved useful for object recognition @cite , as a high-confidence landmark in SLAM implementations @cite , for waypoint navigation @cite , and as a complementary sensing modality for multi-sensor fusion @cite . Many of these systems rely on low-frequency (LF) and high-frequency (HF) RFID tags, which have very short read ranges (5-10 @math cm ). In contrast, ultra-high frequency (UHF) tags can be used for both short-range operation @cite as well as long-range operation out to several meters @cite . In robotics, UHF RFID tags have been used for robot localization @cite , to locate tagged objects @cite , for medication delivery @cite , and for manipulation @cite .
- In the context of agricultural sensing, remote crop monitoring (usually via camera imaging) has already proved useful @cite , and it's even feasible to have UAVs obtain direct measurements for applications such as water quality monitoring @cite . Inexpensive sensorized UHF RFID tags have the potential to augment or supplement existing remote-sensing systems to provide precise, direct measurements (eg. for calibration), and could even be used by existing farm equipment to opportunistically obtain relevant agricultural data. Direct sensing can make agricultural systems safer, more efficient, more accurate, and more productive @cite @cite . However, agricultural applications frequently involve large, expansive areas where wiring for communication and power is undesirable or impracticable. Wireless sensor networks have been used collect agriculture data by distributing sensors throughout a field, and transmitting information back to a base station @cite , sometimes using more-advanced mesh networking techniques @cite . These sensor networks have been shown to provide actionable data that can improve growing conditions and irrigation schedules @cite , but they still suffer from battery and cost constraints. UHF RFID sensor tags are a compelling approach to provide direct measurements that enhance other remote sensing techniques.
- Prior works have investigated the performance of web browser on desktop and smartphones from different perspectives such as caching, protocols and webpage content @cite @cite , webpage complexity @cite and energy @cite . In contrast, our major focus is, (a) to find out the web browsing performance issues on an underpowered device such as Glass and, (b) how today's webpage complexity and design contribute to such issues.
- In the context of network computing, oracles and advice commonly appear in the form of @cite @cite . A typical example is a , which is a labelling of the nodes so that the distance between any pair of nodes can be computed or approximated based on the labels. Other examples are that label the nodes with information that helps in finding a short path between any given source and destination. For graph problems, one could of course encode the entire solution in the advice string---hence the key question is whether a very small amount of advice helps with solving a given problem.
- Exploration is an intensely studied area of reinforcement learning. Many of the pioneering algorithms in this area, such as @math @cite and @math @cite , achieve efficient exploration that scales polynomially with the number of parameters in the agent's state space (see also @cite @cite ). However, as the size of state spaces increases, these methods quickly become intractable. A number of prior methods also examine various techniques for using models and prediction to incentivize exploration @cite @cite @cite @cite . However, such methods typically operate directly on the transition matrix of a discrete MDP, and do not provide for a straightforward extension to very large or continuous spaces, where function approximation is required. A number of prior methods have also been proposed to incorporate domain-specific factors to improve exploration. Doshi- @cite proposed incorporating priors into policy optimization, while @cite developed a method specific to relational domains. Finally, have developed a curiosity driven approach to exploration which uses model predictors to aid in control @cite .
- Several exploration techniques have been proposed that can extend more readily to large state spaces. Among these, methods such as C-PACE @cite and metric- @math @cite require a good metric on the state space that satisfies the assumptions of the algorithm. The corresponding representation learning issue has some parallels to the representation problem that we address by using an autoecoder, but it is unclear how the appropriate metric for the prior methods can be acquired automatically on tasks with raw sensory input, such as the Atari games in our experimental evaluation. Methods based on Monte-Carlo tree search can also scale gracefully to complex domains @cite , and indeed previous work has applied such techniques to the task of playing Atari games from screen images @cite . However, this approach is computationally very intensive, and requires access to a generative model of the system in order to perform the tree search, which is not always available in online reinforcement learning. On the other hand, our method readily integrates into any online reinforcement learning algorithm.
- We study algorithms for similarity join in the , which has been widely adopted in the literature (see, e.g., the survey by Vitter @cite ). The external memory model consists of an internal memory of @math words and an external memory of unbounded size. The processor can only access data stored in the internal memory and move data between the two memories in blocks of size @math . For simplicity we will here measure block and internal memory size in units of points from @math , such that they can contain @math points and @math points, respectively.
- Big data attract great attention, appealing many research efforts on big data benchmarking. @cite develop a comprehensive big data benchmark suite--BigDataBench, but it consists of too many workloads resulting in expensive overhead for conventional simulation-based methods. In comparison, @cite propose CloudSuite, consisting of seven scale-out data center applications, but they only select a few workloads according to the so-called popularity, leading to partial or biased observations, which is confirmed in . @cite measure the microarchitectural characteristics of search engine. @cite provide insight into performance and job characteristics via analyzing Hadoop traces derived from a 2000-node production Hadoop cluster in TaoBao. @cite develop a benchmark suite for unstructured data processing, and present four benchmarks which capture data access patterns of core operations in a wide spectrum of unstructured data processing applications. @cite introduce HiBench, evaluating a specific big data platform---Hadoop with a set of Hadoop programs.
- Much research work focuses on workload analysis. @cite characterize data analysis workloads in data centers. They conclude that inherent characteristics exist in many data analysis applications, different from desktop, HPC, traditional server workloads, and scale-out service workloads. @cite study the impact of sharing resources on five datacenter applications, including web search, bigtable, content analyzer, image stitcher and protocol buffer. They find that a sizable benefit and potential degradation exist from resource sharing effects. @cite propose a task classification methodology, in consideration of workloads dimensions, clustering and break points of qualitative coordinates, and apply it to the Google Cloud Backend. @cite evaluate the microarchitectural characteristics of big data workloads---BigDataBench. They find that software stacks e.g. MapReduce v.s. Spark have significant impact on user-observed performance and micro-architectural characteristics. Our work confirms this work again such that we develop an open source workload characterization tool which can automatically reduce comprehensive workloads to a subset of representative workloads.
- Treasure hunt, network exploration and rendezvous in networks are interrelated problems that have received much attention in recent literature. Treasure hunt has been investigated in the line @cite @cite , in the plane @cite and in other terrains @cite . Treasure hunt in anonymous networks (without any information about the network) has been studied in @cite @cite with the goal of minimizing cost.
- The problem of rendezvous has been studied both under randomized and deterministic scenarios. In the framework of networks, it is usually assumed that the nodes do not have distinct identities. An extensive survey of randomized rendezvous in various models can be found in @cite , cf. also @cite @cite @cite @cite . Deterministic rendezvous in networks has been surveyed in @cite . Several authors considered geometric scenarios (rendezvous in an interval of the real line, e.g., @cite @cite , or in the plane, e.g., @cite @cite ). Gathering more than two agents was studied, e.g., in @cite .
- For the deterministic setting, many authors studied the feasibility and time complexity of rendezvous of synchronous agents, i.e., agents that move in rounds. In @cite the authors studied tradeoffs between the time of rendezvous and the number of edge traversals by both agents. In @cite , the authors presented a rendezvous algorithm whose running time is polynomial in the size of the graph, the length of the shorter label and the delay between the starting times of the agents. In @cite @cite , rendezvous time is polynomial in the first two of these parameters and independent of the delay. The amount of memory required by the agents to achieve deterministic rendezvous was studied in @cite for general graphs. The amount of memory needed for randomized rendezvous in the ring was discussed, e.g., in @cite . Several authors investigated asynchronous rendezvous in the plane @cite @cite and in network environments @cite @cite @cite @cite .
- Providing nodes or agents with information of arbitrary type that can be used to perform network tasks more efficiently has been proposed in @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . This approach was referred to as algorithms with advice . The advice is given either to nodes of the network or to mobile agents performing some network task. Several of the authors cited above studied the minimum size of advice required to solve the respective network problem in an efficient way.
- In this paper, we study the problem of super-resolving the mixture model when there are two modalities, i.e. @math . The methodology in this paper can be extended straightforwardly to the analysis of the case @math and is left for future work. We start by recognizing that in the Fourier domain, the observed signal can be regarded as a linear combination of two spectrally-sparse signals, each composed of a small number of distinct complex sinusoids. The atomic norm @cite @cite of spectrally-sparse signals is developed and proposed as an efficient convex optimization framework to motivate parsimonious structures @cite @cite @cite @cite @cite recently, which can be computed efficiently via semidefinite programming. We then separate and recover the two signals by motivating their spectral structures using atomic norm minimization, in addition to satisfying the observation constraints. The proposed algorithm, denoted by AtomicDemix , is reminiscent of the algorithms for sparse error correction @cite , robust principal component analysis @cite , demixing of sines and spikes @cite @cite , and source separation @cite , where one aims to separate two low-dimensional signals with incoherent structures via convex optimization.
- Meta path first proposed by @cite has become a powerful tool, which can be applied in either in link prediction problems @cite @cite or clustering problems @cite @cite . @cite propose to predict co-author relationship in heterogeneous bibliographic networks based on meta path. extend the link prediction model to relationship prediction model based on meta path in @cite . @cite propose to calculate the similarity scores among users based on meta path in bibliographical network. @cite also apply meta path in clustering problem of heterogeneous information networks with incomplete attributes.
- Tensor has been widely used in social networks studies. @cite propose to apply extended tensor factorization model for personalized prediction of review helpfulness. @cite present a tensor-based framework for integrating heterogeneous multi-view data in the context of spectral clustering. A more detailed tutorial about tensor decomposition and applications is available in @cite .
- Class imbalance problems in classification can be very common in real-world applications. @cite propose a technique for over-sampling the minority class with generated new synthetic minority instances. @cite propose to address the class imbalance problems with under sampling of the majority cases in the training set. A systematic study of the is available in @cite .
- College admission problem @cite and stable marriage problem @cite have been studied for many years and lots of works have been done in the last century. In recent years, some new papers have come out in these areas. @cite propose to analyze the stability of the equilibrium outcomes in the admission games induced by stable matching rules. Ma @cite analyzes the truncation in stable matching and the small core in nash equilibrium in college admission problems. Flor ' e @cite propose to study the almost stable matching by truncating the Gale-Shapley algorithm.
- Gribonval have performed an analysis of SOMP for a problem similar to ours in @cite . They were interested in providing a lower bound on the probability of correct support recovery when the signal to be estimated is sparse and its non-zero entries are statistically independent mean-zero Gaussian random variables exhibiting possibly different variances.
- While our statistical analysis considers the additive measurement noise as a random variable and the sparse signals to be recovered as deterministic quantities, the results obtained in @cite use the opposite approach, , the sparse signals are random and the noise is deterministic. Thus, the problem addressed in our paper differs from that presented in @cite but both papers use similar mathematical tools and the criteria to ensure full support recovery with high probability share analogous properties. This last remark will be further discussed in .
- To automate the layer separation process, more recent techniques use multiple images, either from a fixed viewpoint with varying camera settings (such as flash, focus, and polarization), or from multiple viewpoints through the use of a hand-held camera @cite @cite @cite @cite @cite @cite @cite . In the case of the fixed viewpoint approach, @cite @cite @cite exploit the effect of reflection under different rotation angles of a polarizer. Agrawal @cite show how a flash no-flash image pair can be used to remove both reflections and highlights through gradient filtering and integration. Schechner @cite propose to vary the focus of the camera for eliminating reflection artifacts. The use of different modes of capture is complementary to our technique.
- Methods for separating layers using multiple-viewpoint images are based on the intuition that the transmitted layer and reflection undergo different motions under changing views. Szeliski @cite propose to separate the two layers by estimating global and local motions. Gai @cite study the statistics of natural images to extract both the motion of the two layer motions and their mixing coefficients. In a similar vein, Tsin @cite assume locally planar motion and require dense image capture to estimate both the depth and appearance of each layer through EPI analysis. Sinha @cite speed up the process by adopting piecewise planar scene models and extends the semi-global matching @cite for reliable layer separation. More recently, Guo @cite correlate all images through homography and then conduct low-rank decomposition to effectively separate the reflection layer from the transmitted layer. Although these techniques are effective, the requirement of capturing multiple and often many images of scene from different viewpoints, and hence time instances, significantly limits their applicability. Further, there is an implicit assumption that the scene is mostly planar and can be rectified via a homography.
- The work is an extension of the similar-pair method proposed in @cite . In this approach, the probabilities of low-frequency words are enhanced and new words are supported by adding new FST transitions, both referring to the transitions of the similar and high-frequency words. Compared to the other approaches mentioned above, this method is more flexible, which supports any words instead of words limited in some pre-defined classes.
- The extensions we made in this paper for the work in @cite are two-fold: firstly, the similar-pair algorithm is extended to allow multiple predicting words, which enables multiple information engaged and thus better enhancement; second, the similar-pair approach is employed to deal with OOL words, which demonstrated that similar pairs can be cross-lingual.
- Unfortunately, these arguments employed in the Markov setting do not seem to carry over easily when the lookahead window is taken into account. While our setting can still be cast as an MDP by incorporating the lookahead window into the state space, the structure of the state space is now considerably more complex (and increasingly so, as @math ), and it is not so clear as to whether any monotonicity property continues to hold. Our proof techniques circumvent this complexity by focusing on the macroscopic'' sample-path characteristics of the system, instead of the more refined details of the Bellman equations. As a trade-off, our analysis is more coarse'' by nature, and it provides neither a characterization of the multiplicative in the delay scaling, nor a concrete diversion policy that achieves the lower bound of the necessary amount of future information (which, fortunately, has already been given in @cite ).
- Our work is also similar in spirit to the techniques of information relaxation and path-wise optimization for MDPs ( @cite @cite @cite ). In this case, one considers an relaxed version of the original MDP, where the decision maker has access to realizations of the future input sample paths. This relaxed problem is often simpler to solve and simulate than the original stochastic optimization problem, and hence can be used, for instance, as a performance benchmark for evaluating heuristic policies. Our work is different from this literature in several aspects. Most notably, we focus on rigorously understanding the stochastic dynamics involved in the relaxed problem with future information, and how performance scales with respect to the length of the lookahead window, as opposed to using the relaxed problem to approximate the performance of an optimal online policy, which is well understood in our setting.
- For instance, @cite extracted 31 features from each stroke and trained a classifier for a user to distinguish her touch behavior from other users'. These features include the direction of the end-to-end line, average velocity, start locations, and end locations of a stroke. For a complete list of the features, please refer to @cite . More recently, Sae- @cite studied a canonical set of 22 multitouch gestures for authentication on mobile devices, and they found a desirable alignment of usability and security, i.e., gestures that are more secure are rated by users as more usable. @cite proposed to use free gestures as a static authenticator to unlock mobile devices and they further studied the memorability of user generated gestures. @cite verified that touch-based authentication is a promising authenticator via conducting experiments with around 30 users for one month in the wild.
- Common-behavior attacks Serwadda and Phoha @cite showed that a Lego robot can be programmed to swipe the screen of a mobile device, and the stroke recorded by the screen sensors is as desired. The attacker needs to know the defining parameters (e.g., start and stop locations) of the stroke that is to be forged.
- Mining concept-drifting data streams @cite for classification and pattern mining has been studied extensively. Yet considering temporal changes for recommendation is gaining some attention recently. @cite @cite proposed to have a time-dependent bias in matrix factorization for movie music ratings. But the proposed method is not applicable for one-class collaborative filtering @cite problem. Moreover, it aims to minimize the root mean squared error (which is differentiable) rather than ranking metrics for top- @math recommendation. On the other hand, @cite formulate temporal collaborative filtering as a tensor factorization by treating time as one additional dimension. @cite @cite consider the time gap between purchases and propose an opportunity model to identify not only the items to recommend, but also the best timing to recommend a particular product. Meanwhile, improving temporal diversity of recommendation across time @cite @cite is also considered.
- Another related domain is learning to rank @cite , which is initially motivated for the problem of information retrieval given queries. Making recommendations by learning to rank has attracted lots of attentions recently @cite @cite @cite . EigenRank @cite extends memory-based (or similarity-based) methods by considering the ranking (rather than rating) of items in computing user similarities. Matrix factorization has been extended to optimize for ranking-oriented loss as well. But most ranking-related metrics are non-smooth or non-convex. Hence, majority of the methods either approximate the loss via a smooth function or find a smooth lower upper bound for the loss function. For instance, CofiRank @cite extends matrix factorization to optimize ranking measures like NDCG instead of rating measures. Because NDCG is non-convex, the authors propose a couple of steps to find a a convex upper-bound for the non-convex problem and adopt bundle method for optimization. CLiMF @cite instead optimizes a lower bound of smooth reciprocal rank. Our proposed method differs because we explicitly optimizes for the exact ranking measure. This is viable because we are learning only biases, rather than latent factors, with the ranking loss.
- Our proposed bias learning method in collaborative filtering is partly inspired from the thresholding problem in multi-class label classification @cite @cite . For large-scale multi-class label classification problem, one-vs-rest is still widely used. That is, for each class we construct a binary classifier by treating the class as positive, and the remaining classes as negative. Since each binary classifier is constructed independently, researchers propose to learns a threshold (bias) for each class mainly to optimize classification accuracy, precision recall or F-measure. However, in top- @math recommendation, the score difference and ranking of items matter, making all the items dependent on each other. Also, the motivation of this work is mainly to capture temporal dynamics rather than calibrating the classifier prediction scores.
- We review the related works in two following categories. * Human Action Recognition and Retrieval Modeling temporal structure of video sequences is one of the most challenging problems in human action recognition and has attracted intensive research. Many of the existing approaches focus on extracting the local spatio-temporal features and do not explicitly model the temporal patterns of the action sequence. Most of these works are histogram-based and adopting the bag-of-words framework. For example, in @cite , the bag-of-3D-points from the depth maps are sampled and clustered to model the dynamics of human actions. Similar ideas are presented in @cite , where the Histogram of Gradient (HoG) features are exacted from the depth motion maps to classify human actions. Histogram of 3D joints (HOJ3D) @cite and histogram of visual words @cite are also employed to describe the action sequences by using the joint features. Unfortunately, these histogram-based methods do not preserve the temporal order of the primitive postures of the action and may lead to the poor performance on distinguishing between actions composed of the similar postures but in different temporal orders.
- * Hashing Algorithms Hashing algorithms are widely adopted in the approximate nearest neighbor search problem @cite @cite . There are plenty of works aiming at achieving a higher retrieval rate with shorter code length. We categorize these existing hashing algorithms into two families -- the space-partitioning methods and the ranking-based methods.
- The space-partitioning methods, such as Locality Sensitive Hashing (LSH) @cite and Compressed Hashing @cite normally partition the whole feature space into a sequence of half spaces and quantize the original features into binary bits by these half spaces. In order to preserve the Euclidean distances between high dimensional vectors with sufficient precision, long codes are usually required for these methods. To overcome this drawback, Multi-Probe LSH @cite and entropy-based LSH @cite are proposed to reduce the storage burden at the cost of increasing the query time. On the other hand, different paradigms of hashing algorithms have been developed to approximate distance metrics other than the Euclidean distance, such as the @math -norm distance @cite and the Mahalanobis distance @cite . Spherical LSH @cite works for hashing a set of points on the hypersphere of an input space. There also exist several works kernelizing the LSH approaches by considering the Reproducing Kernel Hilbert Space (RKHS) @cite @cite .
- Unlike space-partitioning methods, the ranking-based hashing methods encode the ordinal relation between the original features rather than their magnitudes. For example, Min Hashing @cite approximates the Jaccard similarity coefficient between two sets by encoding a set with the minimal value of a hash function over its members. Recently, the Winner-Take-All (WTA) Hash @cite has been proposed to encode the magnitude orders of randomly permutated features. The resultant hash codes are scale-invariant, and are often more resilient against the noises. Some other ranking-based hashing algorithms, like the Rank-Sensitive Hash @cite , can be regarded as a special case of the WTA when the window size of ordinal comparison is @math .
- However, to the best of our knowledge, there exists no hashing algorithm aiming at encoding the temporal structure of entire sequences of varied length. @cite introduce a supervised hashing algorithm for video sequences. However, it still performs hashing on individual frames, rather than on the entire video. Then the video similarity has to be computed by the average Hamming distance between each pair of frames. In contrast, we attempt to expand the scope of ranking-based hashing methods to directly explore the temporal order of actions on the video level. This can yield much compact hash codes for videos, and the similarity between videos can be directly computed by the Hamming distance between the video-level hash codes.
- Luo and Tseng @cite are among the first to establish asymptotic linear convergence for a non-strongly convex problem under the local error bound property. They consider a class of feasible descent methods (which includes e.g. the cyclic coordinate descent method). The error bound measures how close the current solution is to the optimal solution set with respect to the projected gradient. Recently, @cite proved that the feasible descent method enjoys a linear convergence rate (from the beginning, rather than only locally) under the global error bound property. Considering the class of smooth constrained optimization problems with the global error bound property, @cite @cite showed a linear convergence rate for the parallel version of the stochastic coordinate descent method. In @cite the authors analyzed the asynchronous stochastic coordinate descent method (SCDM) under the weak strong convexity assumption. Very recently, @cite showed that, if the objective function is smooth, then the class of problems with the global error bound property is a subset of the class of problems with the weak strong convexity property.
- One of the simplest and most successful 2-way models is @cite . In that model, relationships are represented as translations in the embedding space: if @math holds, then the embedding of the @math should be close to the embedding of @math plus some vector that depends on the @math . This is a natural approach to model hierarchical and asymmetric relationships, which are common in knowledge bases such as Freebase . Several modifications to have been proposed recently, @cite and @cite . In , the embeddings of the entities are projected onto a hyperplane that depends on @math before the translation. The second algorithm, , follows the same idea, except that the projection operator is a matrix that is more general than an orthogonal projection to a hyperplane. As we shall see in the next section, corresponds to our model with additional constraints on the parameters.
- While 2-way models were shown to have very good performances on some KB datasets, they have limited expressiveness and they can fail dramatically on harder datasets. In contrast, 3-way models perform some form of low-rank tensor factorization, and in that respect can have extremely high expressiveness depending on the rank constraints. In the context of link prediction for multi-relational data, @cite follows natural modeling assumptions. Similarly to , learns one low-dimensional embedding for each entity. However, relationships are represented as a bilinear operator in the embedding space, i.e. each relationship corresponds to a matrix. The training objective of is the Frobenius norm between the original data tensor and its low-rank reconstruction, whereas uses the margin ranking criterion of . Another related 3-way model is @cite . The parameterization of is a constrained version of , and also uses a ranking criterion as training objective.
- While there has been a lot of focus recently on algorithms purely based on learning embeddings for entities and or relationships, many earlier alternatives had been proposed. We discuss works carried ou in the Bayesian clustering framework, as well as approaches that explicitly use the graph structure of the data. The Infinite Relational Model of @cite , which is a nonparametric extension of the Stochastic Blockmodel @cite , is a Bayesian clustering approach that learns clusters of entities of the same kind, i.e. groups of entities that can have similar relationships with other entities. This work was followed by @cite , a 3-way tensor factorization model based on Bayesian clustering in which entities within a cluster share the same distribution of embeddings.
- The last line of work we discuss here are the approaches that use the structure of the graph in a symbolic way. In that line of work, the Path Ranking Algorithm ( PRA ) @cite estimates the probability of an unobserved fact as a function of the different paths that go from the subject to the object in the multi-relational graph; learning consists in finding, for each relationship, a weight associated to a kind of path (represented as a sequence of relationships) linking two entities. The PRA is used in the Knowledge Vault project @cite in conjunction with an embedding approach. Thus, even though we do not consider these symbolic approaches here, they could also be combined with our embedding model if desired.
- Our classifier is trained using accounts obtained from honeypots, somewhat similar to previous work in the context of spam in MySpace and Twitter @cite @cite . Our work uses accounts attracted by Facebook pages actively engaging like farms and, unlike @cite @cite , leverages timeline-based features for the detection. @cite study the human involvement in Weibo's reputation manipulation services, showing that simple evasion attacks (e.g., workers modifying their behavior) as well as poisoning attacks (e.g., administrators tampering with the training set) can severely affect the effectiveness of machine learning algorithms to detect malicious crowd-sourcing workers. Partially informed by their work, we do not only cluster like activity performed by users but also build on lexical and non-lexical features.
- Finally, we stress that our prior work @cite only presents an exploratory measurement study of like farms, based on the characteristics of the accounts that liked a few honeypot pages. Specifically, @cite analyzes the geographic and demographic distribution of garnered likes, the temporal patterns observed for each campaign, as well as the social graph induced by the likers. Whereas, in this paper, we take a significant step further: although we re-use the honeypot campaigns to build a corpus of like farm users, (i) we demonstrate that temporal and social graph analysis can only be used to detect naive farms, and (ii) we introduce a timeline-based classifier that achieves a remarkably high degree of accuracy.
- Our proposal is related to different techniques. On one hand we may find work on multiple instance learning related to similar concepts. Multiple instance learning @cite is a variation of supervised learning where instead of having individually labeled examples, instances comes in bags. These bags contain multiple instances. A bag is labeled positive if there is at least one example with the concept of interest, or labeled negative otherwise. The positive bag can be regarded as a set of attracting instances and the negative one as a set of repulsive instances. The classifier tries to learn a common concept across all the instances of the positive bag. As we commented in the introduction this approach is commonly found in large scale computer vision under the name of weakly supervised learning.
- Weakly supervised learning has been since widely used in computer vision since its inception and popularization @cite , @cite . Early works used this in an standard instantiation of the MIL framework for inferring difficult to describe classes such as in @cite where photometric, geometric, and topological features are recognized. More recently, several works, such as @cite , explore the capacity of this technique for simultaneous localization and recognition. In multiple instance learning we find another approach closely related to our work, count-based multiple instance learning @cite . In count-based MIL the positive bag is composed of instances where the concept appears within the range of an interval. For example, the positive bag may contain images with 5 to 10 appearances of pedestrians. The main difference between this scenario and our proposal lies in the fact that even in count-based multiple instance learning the problem is casted as a binary problem. Either one belongs to the positive bag or to the negative one. In our case this distinction does not apply and the precise number of instances is the important value. This means that we do not explicitly define any set of negative samples.
- Because we are casting the problem of object description as learning to count, it is worthy to check the similarities with counting strategies in literature. From this point of view, counting has been tackled in different domains, such as cell counting @cite or pedestrian counting @cite @cite . The most common approach for counting firstly segments the objects of interest and then appropriate features are manually defined, automatically extracted, and then a supervised learner infers the final counting value @cite , @cite . Contrary to other approaches where bounding boxes or prototypes of the object of interest are used, in @cite the annotation tasks is reduced to dotting (pointing) and the counting process is casted as a problem of estimating image densities. From the point of view of counting approaches and to the best of our knowledge the proposed approach is the first one where the counting problem is handled by learning deep features. Additionally, different from previous approaches we are not giving any hint on the object we are counting besides the occurrence multiplicity.
- In the previous works, robotic network connectivity control problem is often addressed either in a totally centralized way @cite @cite @cite @cite , or completely decentralized way @cite @cite @cite . The centralized framework yields an optimal network but with low resiliency, since responses to failures in a global network may not be instantaneous. The network resulting from the decentralized framework is resilient; however, it is difficult to achieve the team solution with limited coordination. Our framework stands in between these two frameworks and thus leads to balanced features in terms of resiliency and optimality.
- A relevant thread of work which also uses bounding box annotations is that of tracking-by-detection. It has a long and rich history in computer vision and the reader is referred to @cite for a survey. The tracking-by-detection algorithms start with bounding box annotation(s) of the object(s) to track the object(s) over a long period of time. The underlying assumption is that negative data can be sampled from around the object @cite @cite @cite @cite @cite @cite @cite to distinguish between the object and background. This is not valid in the case of videos because the unmarked regions may contain more instances of the same object, rather than background.
- Multi-object tracking-by-detection approaches also focus on solving the problem of - given object locations across multiple frames, determine which locations belong to the same object. Data association is critical for long term tracking, and is also very challenging @cite . In contrast, our goal is not to track over long periods, but to get short and reliable tracking. Moreover, we do not require these short tracklets to be associated with each other, and thus have minimal need for data association.
- SECRET @cite a model called SECRET is proposed to analyze the execution behavior of different stream processing engines (SPEs) from a practical point of view. The authors found that even the outcome of identical, simple queries vary significantly due to the different underlying processing models. There, the focus is on understanding, comparing and predicting the . In contrast, we want to provide means that allow for a similar analytical study for the of stream reasoning formalisms and engines. The two approaches are thus orthogonal and can be used together to compare stream reasoning engines based on different input feeding modes as well as different reasoning expressiveness.
- StreamLog Another logic-based approach towards stream reasoning is StreamLog @cite . It makes use of Datalog and introduces whose first arguments are timestamps. By introducing which have syntactical restrictions on temporal rules, StreamLog defines (for which Closed World Assumption can be safely applied) that can be used in recursive rules in a stream setting. Since sequential programs are locally stratified, they have efficiently computable perfect (i.e., unique) models. Similar to capturing a fragment of Reactive ASP, we can capture StreamLog by converting temporal atoms @math to expressions @math and employing safety conditions to rules to simulate non-blocking negation. Moreover, we plan for having weaker notions of negation that might block rules but just for a bounded number of time points to the future.
- ETALIS The ETALIS system @cite aims at adding expressiveness to Complex Event Processing (CEP). It provides a rule-based language for pattern matching over event streams with . Simultaneous events are not allowed and windows are not regarded as first-class objects in the semantics, but they are available at the system implementation level. Tuple-based windows are also not directly supported. Furthermore, nesting of windows is not possible within the language, but it can be emulated with multiple rules as in CQL. On the other hand, ETALIS models complex events with time intervals and has operators to express temporal relationships between events.
- The work by @cite is the first to reconstruct images from features. They used SIFT descriptors @cite with geometric information for reconstruction. Because they sampled descriptors at keypoints, the geometric information includes the size, orientation, and position of the image patch from which the descriptor is extracted. They construct a database of local descriptors. Their corresponding image patches are taken from an external image database in advance. To reconstruct an image, they retrieve the most resemblant image descriptor in the database for each local descriptor, transform their corresponding image patches according to their geometric information, and blend them by Poisson Blending @cite . They assumed that geometric information of local descriptors is available, which is rarely true in realistic systems.
- d' @cite proposed a method to convert BREAF descriptors @cite and FREAK descriptors @cite into image patches. Their method generates an optimal image patch for the input descriptor analytically. Although the generated images are clear, their method is applicable only to descriptors of certain kinds.
- @cite reconstructed images from HOG features @cite . They proposed four methods and concluded that an approach that learns pairs of an image features and the corresponding image patch by Sparse Coding is effective. Their method is applicable to any feature, in principle.
- With more than hundred methods, we refer the reader to @cite for an overview over the literature of fingerprint segmentation methods. For image segmentation in general, there is a plethora of approaches to solve this problem. These are based e.g. on the intensity of pixels @cite , @cite , @cite , or the evolution of curves for piecewise smooth regions in images @cite , @cite , @cite , @cite . Texture segmentation, however, is still an open problem, because intensity values are inadequate, e.g. for segmenting fingerprint patterns. Methods based on texture descriptors @cite , @cite or finding other meaningful features in an observed image for classification have been suggested.
- To allow for geometric and spatial reasoning without direct 3D reconstruction, we use the spatial orderings of the scene points, obtained by rank aggregation. Rank aggregation is the problem of finding a full ranking that agrees with multiple (full or partial) rankings, i.e., a consensus of rankings. It was traditionally studied in the context of social choice and voting theory @cite , but was used also for biological sequence alignment @cite and web page ranking @cite . Recently, it was used to temporally order a collection of images of a dynamic scene @cite @cite . The common rank aggregation problem is known to be NP hard @cite . In our method, we use the Markov chain approximation for rank aggregation, which was proven to be quite effective if the power iteration method is used @cite .
- The camera guidance problem seemingly resides in the field of "active vision" @cite , where the goal is to change the pose of controlled cameras, e.g., cameras mounted on robots or PTZ cameras, to allow the optimization of some objective. For example, when the objective is object tracking, fixation on objects over time is maintained through control of the camera pose. A general approach for the simultaneous tracking of multiple moving targets using a generic active stereo setup is studied by @cite . As far as we know, none of the methods in the active vision field considered a set of casually taken photographs as in our scenario; moreover, the environment is usually strictly controlled in advance and then manipulated @cite . Another drawback of these methods is the computational time, where full 3D reconstruction of the scene is usually given in advance, or exhaustively calculated. Our method may be also applied to robot collaboration.
- Optimal motion planning has been a topic of renewed activity in robotics largely due to the advent of sampling-based motion planners that are proven to be asymptotically optimal @cite . But the community has had a long history of interest in optimal motions. Numerical trajectory optimization techniques @cite @cite @cite @cite have been long studied, but have several drawbacks. First, they are prone to falling into local minima, and second they typically require differentiable constraint and cost representations, which are often hard to produce for complex obstacles. Grid-based planners @cite @cite @cite are often fast in low dimensional spaces but suffer from the curse of dimensionality,'' with performance degrading rapidly in spaces of higher dimension @cite . Sampling-based planners were originally developed to overcome many of these challenges, and have been shown to have excellent empirical performance in finding feasible paths in high-dimensional spaces, both without @cite and with dynamic constraints @cite @cite . However, they tend to produce jerky paths that are far from optimal. Some hybrid approaches have combined sampling-based planning with local optimization to produce better paths @cite @cite .
- Several authors have extended RRT* to dynamically-constrained systems. It is relatively easy to apply RRT* to dynamically-constrained systems if a steering function is available @cite . Proving convergence is harder, requiring analysis of small-time controllability conditions @cite . Other authors have extended RRT* to systems whose dynamics and costs are (or can be approximated) by linear and quadratic functions, respectively, by definition of a suitable steering function based on the LQR principle @cite @cite . When more complex differential constraints are involved, it may not be possible to devise a suitable steering functions. One method generated complex maneuvers using RRT* and performed each rewiring step by numerically solving a two-point boundary value problems @cite . This adds greatly to computational expense. A similar method performed rewiring using a spline-based trajectory representation that is optimized via a nonlinear program solver @cite .
- The prior work with closest relation to ours in terms of generality of applicability is the Sparse-Stable-RRT planner @cite . Like our work, it avoids the use of a steering function entirely and samples directly in control space. Approximate rewiring is performed by allowing connections to points that are near enough'' according to a state-space distance metric. This scheme was proven to satisfy asymptotic near-optimality, which is the property of converging toward a path with bounded suboptimality @cite . In a more recent paper, the same authors have extended it to an asymptotically-optimal planner, SST*, by progressively shrinking nearness threshold parameters @cite . However, Sparse-Stable-RRT and SST* and have many parameters to tune, and our experiments suggest that AO- @math planners in general outperform both planners.
- The top three prefetchers submitted to the First Data Prefetching Championship (DPC-1) @cite , titled AMPM (Access Map Pattern Matching), GHB-LDB (Global History Buffer - Local Delta Buffer), and PDFCM (Prefetching base on a Differential Finite Context Machine), utilized significantly more sophisticated and novel approaches to the prefetching problem.
- The AMPM prefetcher @cite divided memory into hot zones that were tracked using a 2-bit vectors to locate the stride zone. Our prefetcher instead tracks the last stride with a 6-bit integer.
- The GHB-LDB prefetcher @cite improved upon existing prefetcher models by including global correlation and common stride prefetching. Our prefetcher in contrast simply tracks variable strides and streams. The PDFCM prefetcher @cite utilized hash-based History Tables and Delta Tables to prefetch arbitrary degrees and distances. Our prefetcher utilizes a simpler direct look-up solution that makes prefetching decisions based on the frequency of instruction usage and misses.
- There are several existing methods, including KD-trees @cite , Geometric Near-neighbor Access Trees @cite , and Nearest Vector trees @cite that build tree search structures on large datasets (see @cite for an extensive bibliography). These algorithms typically need batch access to the entire dataset before constructing their trees, in which case they may outperform the BF, however we are interested in an online setting. Two well known tree-based algorithms that allow online insertion are cover trees @cite and ball trees. The ball tree online insertion algorithm @cite is rather costly, requiring a volume minimizing step at each addition. The cover tree, on the other, has a cheap online insertion algorithm, and it comes with guarantees of query time scaling as @math where N is the amount of data and c the so-called expansion constant, which is related to the intrinsic dimensionality of the data. We will compare to cover trees below. Note that @math in fact depends on @math as it is defined as a worse case computation over the data set. It can also diverge from adding a single point.
- To further improve streaming performance, the last category of research jointly adapts video data rate and multicast link rate. This category is the most related to our work. @cite investigate the utility optimization problem of scalable video multicast and prove that this problem is NP-Hard. A greedy algorithm is then proposed to schedule the transmissions of layers and determine the corresponding modulation and coding scheme (MCS) assigned for each transmission. @cite suggest a pseudo-polynomial algorithm with dynamic programming to solve the optimization problem. Most recently, MuVi @cite has been designed to investigate the optimal multicast scheduling problem for videos encoded with I, P, and B frames. As the computational complexity of the suggested algorithm @cite grows quadratically with the number of available time slots, it fails to efficiently solve the optimization problem with multiple multicast sessions. To reduce the computational complexity especially for the case of multiple sessions, a fully polynomial time approximation algorithm is presented @cite . The approximation factor, however, linearly decreases with the number of multicast sessions.
- While the question of necessity of instructor's intervention in online learning and MOOCs is being investigated @cite @cite , technologies to enable timely and appropriate intervention are also required. The pedagogy community has recognized the importance of instructor intervention in online learning prior to the MOOC era (e.g., @cite ). Taking into consideration the pedagogical rationale for effective intervention, they also proposed strategic instructor postings: to guide discussions, to wrap-up the discussion by responding to unanswered questions, with Socrates-style'' follow-up questions to stimulate further discussions, or with a mixture of questions and answers @cite . However, these strategies must be revisited when being applied to the scale of typical MOOC class sizes.
- Among works on forum information retrieval, we focus on those that focus on forum moderation as their purpose is similar to the instructor's role in a course forum. While early work focused on automated spam filtering, recent works shifted focus towards curating large volumes of posts on social media platforms @cite to distil the relevant few. Specifically, those that strive to identify thread solvedness @cite @cite and completeness @cite are similar to our problem.
- Further, strategies for thread recommendation for students such as @cite may not apply in recommending for instructors. This difference is partially due to scale: while the number of students and threads are large, there are few instructors per course. In this case, reliance on collaborative filtering using a user--item matrix is not effective. Learning from previous human moderation decisions @cite , therefore, becomes the most feasible approach. Prior work on MOOC forums propose categorisation of posts @cite @cite @cite to help instructors identify threads to intervene. Chaturvedi @cite , the closest related work to ours, show each of the four states of their sequence models to predict instructor intervention to be distributed over four post categories they infer. In this paper, we use their results for comparison.
- The first and most widely-used interference management policies @cite @cite @cite @cite @cite @cite @cite are based on power control. In these policies, all the UEs in the network transmit at a @math power at all time (provided that the system parameters remain the same) in the entire frequency band Although some power control policies @cite go through a transient period of adjusting the power levels before converging to the optimal power levels, the users maintain the constant power levels after the convergence. . When the cross channel gains among BSs and UEs are high, simultaneous transmissions at the same time and in the same frequency band will cause significant interference among cells. Such strong interference is common in macrocells underlaid with femtocells. For example, in @cite it is shown that that interference from MUEs near the FBS severely affects the uplink transmissions of FUEs. Also, in offices and apartments, where FBSs are installed close to each other, inter-cell interference is particularly strong. In contrast, our proposed solutions mitigate the strong interference by letting only a subset of non-interfering UEs to transmit at the same time.
- Some existing works mitigate strong interference by letting different subsets of UEs to transmit in different time slots (spatial time reuse) @cite @cite @cite @cite @cite @cite @cite @cite @cite or in different frequency channels (spatial frequency reuse) @cite @cite @cite @cite @cite @cite @cite . Specifically, they partition UEs into disjoint subsets such that the UEs in the same subset do not interfere with each other. Given the same partition of the UEs, the policies based on spatial time reuse and those based on spatial frequency reuse are equivalent. Hence, we focus on policies based on spatial time reuse hereafter.
- Some policies based on spatial time reuse, partition the UEs based on the coloring of the interference graph h @cite @cite @cite @cite @cite , which is not efficient. In general, a set of UEs with the same color (i.e. the UEs who can transmit simultaneously) may not be maximal (See Fig. 1 a), in the sense that there may be UEs who do not interfere but have different colors (we will also show this in the motivating example in Subsection ). In this case, it is more efficient to also let those non-interfering UEs to transmit simultaneously, although they have different colors. Hence, the partitioning based on coloring the interference graph is not efficient, because the average number of active UEs (i.e. the average cardinality of the subsets of UEs with the same color) is low.
- Some policies based on spatial time reuse @cite @cite @cite @cite partition the UEs based on the MISs of the interference graph, which is more efficient, because we cannot add any more UEs to an MIS without creating strong interference. However, they are still inefficient compared to our proposed policies for delay-sensitive applications. Specifically, they schedule different MISs in a cyclic and (weighted) round-robin manner, in which each UE transmits at a fixed position in each cycle. For delay-sensitive applications, earlier positions in the cycle are more desirable because they enhance the chances of transmitting prior to delay deadlines. Hence, a cyclic schedule is not fair to the UEs allocated to later positions. In contrast, our proposed policies schedule the MISs in an efficient, nonstationary manner for delay-sensitive applications.
- Besides the above two categories, there are several other related works. For instance in @cite @cite , the authors propose reinforcement learning and evolutionary learning techniques for the femtocells to learn efficient interference management policies. In @cite , the femtocells learn the fixed transmit power levels, while in @cite , the femtocells learn to randomize over different transmit power levels. However, the interference management policies in @cite and @cite cannot provide minimum throughput guarantees for the UEs. In constrast, we provide rigorous minimum throughput guarantees for the UEs. In both @cite @cite the femtocell UEs need to limit their transmission powers in every time slot such that the SINR of the macrocell UE is sufficiently high. If there is strong interference between some femtocells and the macrocell, the femtocell UEs will always transmit at lower power levels, leading to a low sum throughput for them.
- Another method to mitigate interference is to deploy coordinated beam scheduling @cite @cite . In @cite and @cite , the authors schedule a subset of beams to maximize the total reward associated with the scheduled subset, where the reward per beam reflects the channel quality and traffic. The first difference from our work is that the proposed approach schedules a fixed subset of beams and leaves the other UEs inactive. Hence, some UEs have no throughput, which means the minimum throughput as well as the delay-sensitivity of the UEs is not satisfied. Second, we rigorously prove that our proposed policy achieves good performance with low (polynomial-time) complexity , while @cite @cite do not. Third, the schemes in @cite @cite are proposed for a specific network performance criterion and may not be flexible enough for other network performance criteria (such as the sum throughput). Finally, @cite @cite do not consider delay sensitivity of the UEs.
- The second area of research focuses on designing appliance or home level power reduction --- while considering the comfort and ease of home owners @cite @cite @cite @cite . Both Yupik @cite and n-Plug @cite propose adding smart plugs to a few (deferable) appliances, whose usages patterns are monitored to present appropriate slack when a grid-stress event (demand peak) is indicated, using prices in Yupik and frequency in n-Plug. Smartcap @cite uses programmable switches or smart-appliances to control background appliances using a least-slack-first algorithm to flatten any peak. @cite evaluate how peaks can be flattened if the elastic component is allowed to be programmatically controlled. All these work seek to flatten load such that user prefernces are minimally affected; they fail to leverage (as they do not consider the problem domain) the experience of consumers in highly stressed grid that consequently face full blackouts for @math 12hrs a day @cite . Furthermore, these are all primarily peak-shifting algorithms; for a highly-stressed grid there is .
- Recently there have been dramatic progress in deep neural networks for natural language and computer vision. For natural language, Recurrent Neural Networks (RNNs @cite @cite ) and Long-Short Term Memories (LSTMs @cite ) achieve the state-of-the-art performance for many NLP tasks such as machine translation @cite @cite @cite and speech recognition @cite . For computer vision, deep Convolutional Neural Networks (CNN @cite ) outperform previous methods by a large margin for the tasks of object classification @cite @cite and detection @cite @cite @cite . The success of these methods for language and vision motivate their use for multimodal learning tasks (e.g. image captioning and sentence-image retrieval).
- For zero-shot learning, the task is to associate dense word vectors or attributes with image features @cite @cite @cite @cite @cite . The dense word vectors in these papers are pre-trained from a large amount of text corpus and the word semantic representation is captured from co-occurrence with other words @cite . @cite developed this idea by only showing the novel words a few times. In addition, @cite adopted auto-encoders with attribute representations to learn new class labels and @cite proposed a method that scales to large datasets using label embeddings.
- Weighted counterpart of Boolean nested automata. Weighted nested automata have been considered in @cite in the context of finite words, where the weights are given over semirings. It is further required that the semirings of all master and slave automata coincide, while in our case, the value functions may differ. Since the semirings of master and slave automata must coincide for @cite , it can be interpreted as defining weighted counterpart of Boolean nested automata over finite words. Adding nesting structure to words and trees have been extensively studied for non-weighted automata in @cite @cite and also applied to software model checking @cite . The work of @cite defines a weighted counterpart of nesting of finite words, whereas we consider nesting of various weighted automata to express properties of infinite behaviors of systems. Properties such as long-run average response time cannot be expressed in the framework of @cite .
- Register automata. Another related model for specifying quantitative properties are @cite , which are parametrized by cost functions. The main differences between @cite and nested weighted automata are as follows: (i) register automata are over finite words, whereas we consider infinite words, and (ii) we consider nested control of automata, whereas register automaton are non-nested. As a result, both in terms of expressiveness and decidability results nested weighted automata are very different from register automata. For example, the emptiness of register automata with max and sum value functions is decidable, while we show emptiness to be undecidable for deterministic nested weighted automata with these value functions.
- Coreference resolution in general is a difficult natural language processing (NLP) task and typically requires sophisticated inferentially-based knowledge-intensive models @cite . Extensive work in the literature focuses on the problem of entity coreference resolution and many techniques have been developed, including rule-based deterministic models (e.g. , , ) that traverse over mentions in certain orderings and make deterministic coreference decisions based on all available information at the time; supervised learning-based models (e.g. , , ) that make use of rich linguistic features and the annotated corpora to learn more powerful coreference functions; and finally, unsupervised models (e.g. , Haghighi and Klein (2007, 2010)) haghighi2007unsupervised haghighi2010coreference that successfully apply generative modeling to the coreference resolution problem.
- Event coreference resolution is a more complex task than entity coreference resolution @cite and also has been relatively less studied. Existing work has adapted similar ideas to those used in entity coreference. first proposed a deterministic clustering mechanism to group event mentions of pre-specified types based on hard constraints. Later approaches @cite @cite applied learning-based pairwise classification decisions using event-specific features to infer event clustering. Bejan and Harabagiu proposed several unsupervised generative models for event mention clustering based on the hierarchical Dirichlet process (HDP) @cite . Our approach is related to both supervised clustering and generative clustering approaches. It is a nonparametric Bayesian model in nature but encodes rich linguistic features in clustering priors. More recent work modeled both entity and event information in event coreference. showed that iteratively merging entity and event clusters can boost the clustering performance. demonstrated the benefits of propagating information between event arguments and event mentions during a post-processing step. Other work modeled event coreference as a predicate argument alignment problem between pairs of sentences, and trained classifiers for making alignment decisions @cite @cite . Our model also leverages event argument information into the decisions of event coreference but incorporates it into Bayesian clustering priors.
- Most existing coreference models, both for events and entities, focus on solving the within-document coreference problem. Cross-document coreference has attracted less attention due to lack of annotated corpora and the requirement for larger model capacity. Hierarchical models @cite @cite @cite have been popular choices for cross-document coreference as they can capture coreference at multiple levels of granularities. Our model is also hierarchical, capturing both within- and cross-document coreference.
- Our model is also closely related to the distance-dependent Chinese Restaurant Process (DDCRP) @cite . The DDCRP is an infinite clustering model that can account for data dependencies @cite @cite . But it is a flat clustering model and thus cannot capture hierarchical structure that usually exists in large data collections. Very little work has explored the use of DDCRP in hierarchical clustering models. combined a DDCRP with a standard CRP in a two-level hierarchy analogous to the HDP with restricted distance functions. proposed a two-level DDCRP with data-dependent distance-based priors at both levels. Our model is also a two-level DDCRP model but differs in that its distance function is learned using a feature-rich log-linear model. We also derive an effective Gibbs sampler for posterior inference.
- In this paper, we study the problem of super-resolving when there're two channels, i.e. @math . We start by recognizing that in the frequency domain, the observed signal can be regarded as a linear combination of two spectrally-sparse signals, each composed of a small number of complex sinusoids. We then separate and recover the two signals by motivating their spectral structures using atomic norm minimization, which has been recently shown as an efficient convex optimization framework to motivate parsimonious structures @cite @cite @cite , as well as satisfying the observation constraints.
- Like the development of non-asymptotic theory of traditional random matrices has found multitude of applications in areas including statistics, geometric functional analysis, and compressed sensing @cite , we believe that the growth of a non-asymptotic theory of random kernel matrices will help in better understanding of many machine learning applications that utilize kernel techniques.
- The goal of is to release global, statistical properties of a database while protecting the privacy of the individuals whose information the database contains. Differential privacy @cite is a formal notion of privacy tailored to private data analysis. Differential privacy requires, roughly, that any single individual's data have little effect on the outcome of the analysis. A lot of recent research has gone in developing differentially private algorithms for various applications, including kernel methods @cite . A typical objective here is to release as accurate an approximation as possible to some function @math evaluated on a database @math .
- Part based models have played a huge role in the fields of object detection and (human) pose estimation within the last years. Based on the fundamental work Fischler and Elschlager @cite , these models represent an object through multiple parts which are connected via deformation terms, often visualized as springs, allowing for matching them in a flexible configuration. Various manifestations of this basic notion have been developed through the years, kicked of by @cite with their deformable part models for object detection. Different refinements have been proposed specifically for human pose estimation, e.g. by enriching a model with additional parts to compensate for the flexibility of the human body @cite or by allowing rotation of parts @cite .
- While effective implementations of part detectors have been proposed for characteristic body parts like head and torso, part templates for extremities are usually weak. This issue has been addressed by @cite , who argue that person and body part templates should be pose specific rather than generally trained. @cite also follow this notion by proposing the concept of poselets, generic rigid part detectors based on Histograms of Oriented Gradients (HoG) @cite as a generalization of specific body part detectors. Poselets lift the spatial limitation of parts being connected to an actual body part and encode generic parts of the body. @cite recently utilized poselets for training discriminative classifiers to specifically differentiate between arm configurations of a person.
- In the context of key-frame selection in videos, poselets have been used for human activity recognition. @cite proposed a framework based on poselet activations for selecting key-frames that represent key-states in an action sequence. An additional classifier trained on pairwise terms for all activations then decides if a specific action sequence occurred. @cite select action specific postures by matching shape information from individual frames in order to recognize specific tennis strokes in game footage.
- The analysis of human gait probably plays the biggest role in the field of periodic motion research. A big focus lies on the identification of a person via his her intrinsic gait signature, for example by determining and tracking the body shape @cite or through fusion of multiple gait cycles @cite . More general approaches strive to recover the human body pose @cite in order to retrieve a full set of gait parameters. Periodic motion in images was examined by Cutler & Davis @cite , who use self similarity and frequency transformations to obtain the frequency of the periodic motion of human and animal gait.
- Most work researching the tracking of people in aquatic environments has focused on drowning detection @cite , localization of athletes in swimming competitions @cite and motion analysis for video based swimming style recognition @cite . A Kalman filter framework is presented in @cite to explicitly model the kinematics of cyclic motions of humans in order to estimate the joint trajectories of backstroke swimmers. @cite use Gaussian features for detecting a specific pose of a swimmer in a pool with the intention of initializing his her pose. The method closest to our approach is presented in @cite , who divide swimming cycles into intervals and train object detectors for each interval. The stroke rate is computed by counting the occurrences of the intervals. However, they show that arbitrary poses cannot be detected with their approach.
- The main part of Theorem is the continuity of @math around @math , which is the zero temperature asymptotic result for the free energy. This type of problems does not seem to attract much interest in the discrete time setting since in some cases the answers are simple. For instance, consider the (nearest-neighbor) simple random walk model with an i.i.d. random environment with @math . Then it is easy to see that as @math , the free energy is asymptotic to @math times the time constant of the directed last passage percolation. However, if @math is Bernoulli distributed and we send @math , the situation is not so simple. As we mentioned at the beginning, the existence of @math proved in @cite is already highly nontrivial and the continuity as @math remains an open question at the moment.
- Another continuous time polymer model is Brownian directed polymer in Poissonian environment introduced by Comets--Yoshida @cite . The @math limit was studied in the same paper, as well as @math for @math with a specific choice of the other parameters. It is possible to show by a block argument that the finite volume free energy stays bounded as @math in general but, to the best of our knowledge, the existence of the limit at @math is not known. Later in @cite , the asymptotics as the density of the Poisson point process tends to @math was also studied but only for bounded @math , in contrast to Theorem here.
- Finally, we mention that some have been found recently, see, e.g., Moriarty--O'Connell @cite , Amir--Corwin--Quastel @cite and Sepp "a l "a inen @cite . In these models the free energy can be explicitly computed, thus allowing to study various asymptotics. But we refrain from explaining the details of these results since such examples have been found only in @math -dimension so far and also the techniques employed are quite different from ours.
- Many efforts have been devoted to scale up kernel methods. The random feature approach @cite @cite approximates the kernel function with explicit random feature mappings and solves the problem in primal form, thus circumventing the quadratic computational complexity. It has been applied to various kernel methods @cite @cite @cite , among which most related to our work is Randomized Component Analysis @cite . One drawback of Randomized Component Analysis is that their theoretical guarantees are only for kernel matrix approximation: it does not say anything about how close the solution obtained from randomized PCA is to the true solution. In contrast, we provide a finite time convergence rate of how our solution approaches the true solution. In addition, even though a moderate size of random features can work well for tens of thousands of data points, datasets with tens of millions of data points require many more random features. Our online approach allows the number of random features, hence the flexibility of the function class, to grow with the number of data points. This makes our method suitable for data streaming setting, which is not possible for previous approaches.
- Online algorithms for PCA have a long history. Oja proposed two stochastic update rules for approximating the first eigenvector and provided convergence proof in @cite @cite , respectively. These rules have been extended to the generalized Hebbian update rules @cite @cite @cite that compute the top @math eigenvectors (the subspace case). Similar ones have also been derived from the perspective of optimization and stochastic gradient descent @cite @cite . They are further generalized to the kernel case @cite @cite @cite . However, online kernel PCA needs to store all the training data, which is impractical for large datasets. Our doubly stochastic method avoids this problem by using random features and keeping only a small program for regenerating previously used random features according to pre-specified seeds. As a result, it can scale up to tens of millions of data points.
- For finite time convergence rate, @cite proved the @math rate for the top eigenvector in linear PCA using Oja's rule. For the same task, @cite proposed a noise reduced PCA with linear convergence rate, where the rate is in terms of epochs, , number of passes over the whole dataset. The noisy power method presented in @cite provided linear convergence for a subspace, although it only converges linearly to a constant error level. In addition, the updates require explicit orthogonalization, which is impractical for kernel methods. In comparison, our method converges in @math for a subspace, without the need for orthogonalization.
- The conventional approach to compare patches is to use descriptors and a squared euclidean distance. Most feature descriptors are hand-crafted as SIFT @cite or DAISY @cite . Recently, methods for learning a descriptor have been proposed @cite ( , DAISY-like descriptors learn pooling regions and dimensionality reduction @cite ). Simonyan al @cite proposed a convex procedure for training on both tasks.
- Zbontar and LeCun in @cite have recently proposed a CNN-based approach to compare patches for computing cost in small baseline stereo problem and shown the best performance in KITTI dataset. However, the focus of that work was only on comparing pairs that consist of very small patches like the ones in narrow baseline stereo. In contrast, here we aim for a similarity function that can account for a broader set of appearance changes and can be used in a much wider and more challenging set of applications, including, , wide baseline stereo, feature matching and image retrieval.
- The majority of the presented models have studied a single epidemic in a single topology whereas in later work @cite , multiple virus models are introduced. All of them require that the network is fair-play, which means that all nodes have exactly the same behavior towards the viruses. In @cite , an @math model was used with two competing viruses that infect nodes of arbitrary topologies where the nodes are mutually immune. The main result is that for any topology the stronger virus (above threshold) survives and wipes out the weaker one ("winner takes all"). In later work, the condition for mutual immunity is removed and the focus is on the conditions where nodes are infected from both viruses @cite . In both cases, the model used is SIS-like whereas in @cite the authors provide a generalized model that includes the majority of known epidemiological models with appropriate parameter definition in discrete time. All the aforementioned models, are applied to simple networks whereas recent work @cite , refers to competing viruses in a composite network and conjectures that the stronger one will prevail over the weaker. Once again, the problem is formulated by a non-linear dynamical system.
- Ecology, is another interesting application field for such propagation models. The principle of states that two species that compete for the exact same resources cannot stably coexist which resembles the aforementioned phenomenon of "winner takes all". This principle has been studied intensively using propagation models such as SIS, SIRS, Lotka-Volterra etc. but there has been no analytical solution presented so far @cite , @cite , @cite .
- The requirement to collect person-specific training data during a calibration step is a key limitation of both model-based and appearance-based methods. To address this limitation, several previous works used interaction events, such as mouse clicks or key presses, as a proxy for the user's on-screen gaze position @cite @cite . Alternatively, visual saliency maps @cite @cite or pre-recorded human gaze patterns of the presented visual stimuli @cite were used as bottom-up, probabilistic training data to learn the estimation function. However, all of these approaches rely on observations of a specific person and environment, which limits their applicability.
- Purely data-driven approaches leverage large amounts of training data to learn gaze estimators that generalise to arbitrary users without the need for person-specific calibration @cite @cite @cite settings. These methods have significant potential to bring gaze estimation to new settings, including mobile devices, public displays, and egocentric cameras. However, the generalization capability of learning-based methods has not been examined yet. Moreover, prior work used 3D input for head pose information @cite @cite , while we are the first to evaluate the whole pipeline for fully automatic monocular appearance-based gaze estimation for person-independent training scenario.
- Because most existing gaze estimation datasets are designed for coarse gaze estimation, the sampling density of gaze and head pose space is not sufficient to train appearance-based gaze estimators @cite @cite @cite @cite (see Table for an overview of existing datasets). More comparable to , the Eyediap dataset contains 94 video sequences of 16 participants looking at three different targets (discrete and continuous markers displayed on a monitor, and floating physical targets) under both static and free head motion @cite . The UT Multiview dataset also contains dense gaze samples of 50 participants as well as 3D reconstructions of eye regions that can be used to synthesise images for arbitrary head poses @cite . However, as discussed before, both datasets have the significant limitation that they were recorded under controlled laboratory settings. Although the Eyediap dataset includes two different illumination conditions, recordings under the second condition were provided only for a subset of the participants.
- . First of all, we classify data center benchmarks according to their targeted systems. We consider three popular camps of systems in today's data centers: (1) : the great prosperity of the Hadoop-centric systems in industry brings a wide diversity of systems (e.g. Spark @cite , HBase @cite , Hive @cite and Impala @cite ) on top of Hadoop MapReduce and HDFS @cite as well as a wide range of benchmarks specifically designed for these systems. (2) : parallel DBMSs (e.g. MySQL @cite and Oracle @cite ) and NoSQL data stores (e.g. Amazon Dynamo @cite , Cassandra @cite and Linkedin Voldemort @cite ) also widely exist in data centers. (3) : long-running web services such as Nutch search engine @cite and multi-tier cloud applications @cite are another important type of data center applications. These services usually have stringent response time requirement @cite and their request processing is distributed into a large number of service components for parallel processing, thus the service latency is determined by the tail latency of these components @cite @cite .
- The main concept underlying compute-and-forward is that the superposition property of the wireless medium can be exploited for network coding @cite @cite @cite . This phenomenon was independently and concurrently discovered by @cite @cite @cite , with the latter coining the phrase Subsequent efforts @cite @cite @cite developed lattice coding strategies for communicating the sum of messages to a single receiver. This lead to the compute-and-forward framework @cite for multiple receivers that recover linear combinations of the messages (albeit with equal power constraints, unlike the single receiver framework of @cite ).
- As shown by Feng al @cite , any compute-and-forward scheme based on nested lattice codes can be connected to network coding over a finite commutative ring. From this algebraic perspective, the compute-and-forward framework of @cite can be viewed as a special case that connects nested lattice codes generated via Construction A to network coding over a prime-sized finite field. Another important special case is the recent work of Tunali al @cite that develops a compute-and-forward scheme based on nested lattices over Eisenstein integers. For complex-valued channels, this scheme can offer higher computation rates on average (e.g., for Rayleigh fading) since the Eisenstein integers are a better covering of the complex plane than the Gaussian integers employed by @cite . Several recent papers have also used the algebraic perspective of @cite to propose practical codes and constellations for compute-and-forward @cite @cite @cite @cite .
- In recent, independent work, Zhu and Gastpar @cite proposed a compute-and-forward scheme for unequal powers based on the scheme of @cite . They also showed how to use this scheme to reach the two-user multiple-access sum capacity (if the channel strength lies above a small constant). However, their scheme does not retain the connection to a finite field.
- The original motivation for the compute-and-forward strategy was the possibility of relaying in a multi-hop network while avoiding the harmful effects of interference between users (by decoding linear combinations) as well as noise accumulation (by decoding at every relay). Several works have investigated and improved upon the performance of the original compute-and-forward framework in the context of multi-hop relaying @cite @cite @cite @cite @cite @cite . Our expanded framework can improve performance further by permitting relays to employ unequal powers and decode multiple linear combinations when appropriate. Following the conference publication of this work, Tan al @cite noted that our proposed message representation may be inefficient for multi-hop relaying if each relay naively treats a linear combination over @math as @math information symbols for the next hop. They proposed a lattice-based solution to this issue. It is also possible to resolve this issue directly over the message representation by having each relay only use some of its @math received symbols as information symbols for the next hop. See [Section III.F] ncncarxiv for details.
- All the aforementioned studies that are related to gathering in graphs take place in a synchronous scenario i.e., a scenario in which the agents traverse the edges in synchronous rounds. Some efforts have been also dedicated to the scenario in which the agents move asynchronously: the speed of agents may then vary and is controlled by the adversary. For more details about rendezvous under such a context, the reader is referred to @cite @cite @cite @cite for rendezvous in finite graphs and @cite @cite for rendezvous in infinite grids.
- Media bias Our work relates to an extensive body of literature---spanning across political science, economics and communication studies---that gives theoretical and empirical accounts of media bias and its effects. We refer the reader to a recent comprehensive survey of media bias @cite , and focus here on the studies that are most relevant to our approach.
- Selection patterns Several small-scale studies investigate subjects that media outlets select to cover by relying on hand annotated slant labels. For instance, by tracing the media coverage of 32 hand-picked scandal stories, it was shown that outlets with a slant are more likely to cover scandals involving politicians, and vice-versa @cite . Another study @cite focuses on the choices that five online news outlets make with respect to which stories to display in their top news section, and reports that outlets show signs of partisan filtering. In contrast, by relying on an unsupervised methodology, our work explores selection patterns in data involving orders of magnitude more selector agents and items to be selected. Closer to our approach are methods that show political polarization starting from linking patterns in blogs @cite or from the structure of the retweet graph in Twitter @cite . These approaches operate on a predefined - dimension, and assume available political labels. Furthermore, the structure they exploit does not directly apply to the setting of news media articles.
- Language and ideology Recently, natural language processing techniques were applied to identify ideologies in a variety of large scale text collections, including congressional debates @cite @cite , presidential debates @cite , academic papers @cite , books @cite , and Twitter posts @cite @cite @cite . All this work operates on a predefined dimension of -- political ideology using known slant labels; in the news media domain slant is seldom declared or proven with certainty and thus we need to resort to an unsupervised methodology.
- Quote tracking Recent work has focused quoting practices @cite and on the task of efficiently tracking and matching quote snippets as they evolve, both over a set period of time @cite , as well as over an longer, variable period of time @cite . We adapt this task in order to news article quotes with presidential speech segments and build our outlet-to-quote graph.
- Some of the very early work on neural networks by Steinbuch and Piske @cite and Taylor @cite considered a memory that performed nearest-neighbor operations on stored input vectors and then fit parametric models to the retrieved sets. This has similarities to a single layer version of our model.
- Subsequent work in the 1990's explored other types of memory @cite @cite @cite . For example, Das al @cite and Mozer al @cite introduced an explicit stack with push and pop operations which has been revisited recently by @cite in the context of an RNN model.
- Our model is also related to Bahdanau al @cite . In that work, a bidirectional RNN based encoder and gated RNN based decoder were used for machine translation. The decoder uses an attention model that finds which hidden states from the encoding are most useful for outputting the next translated word; the attention model uses a small neural network that takes as input a concatenation of the current hidden state of the decoder and each of the encoders hidden states. A similar attention model is also used in Xu al @cite for generating image captions. Our memory'' is analogous to their attention mechanism, although @cite is only over a single sentence rather than many, as in our case. Furthermore, our model makes several hops on the memory before making an output; we will see below that this is important for good performance. There are also differences in the architecture of the small network used to score the memories compared to our scoring approach; we use a simple linear layer, whereas they use a more sophisticated gated architecture.
- In some applications, the robustness of a graph is related to the centrality measures such as the degree, betweenness, closeness, and eigenvector centralities. Loosely speaking, the centrality measures capture the relative importances of the nodes in a graph. Detailed reviews on the centrality measures and their applications can be found in @cite @cite and the references therein. Typically, the perturbations applied to the nodes with higher centrality scores have a stronger impact on the overall system (e.g., @cite @cite @cite @cite ). Hence, graphs with unbalanced centrality distributions are usually vulnerable to such worst-case perturbations.
- In the literature, there are many works related to the construction of robust interaction graphs. Some of these works consider how a robustness measure can be improved via some modifications to the graph topology. For instance, such improvement can be achieved by rewiring a small percentage of the existing edges (e.g., @cite ) or adding a small number of edges to the graph (e.g., @cite @cite ).
- Another group of studies consider the explicit construction of expanders. Expanders can be constructed via graph operations such as zig-zag product (e.g., @cite @cite ), or derandomized graph squaring @cite . Furthermore, for any @math such that @math is a prime power, an explicit algebraic construction method for a family of @math -regular expanders, i.e. Ramanujan graphs, was presented in @cite . In @cite , Watts-Strogatz small-word networks are transformed into quasi Ramanujan graphs by rewiring some of the edges.
- A random @math -regular graph with @math nodes can be constructed by generating @math copies for each node, picking a uniform random perfect matching on the @math copies, and connecting any two nodes if the matching contains an edge between their copies (e.g., @cite @cite ). In @cite , the authors present a distributed scheme for incrementally building random @math -regular multi-graphs with @math Hamiltonian cycles. Alternatively, some graph processes may be designed to transform an initial @math -regular graph into a random @math -regular graph by inducing a Markov chain with a uniform limiting distribution over the set of @math -regular graphs (e.g., @cite @cite ). The method in this paper is also based on designing a graph process with a uniform limiting distribution over the set of @math -regular graphs. Compared to the similar works in the literature, the proposed scheme is applicable to the most generic case, and it is decentralized. The initial graph is not required to satisfy some strong properties such as being regular or having an integer average degree. Furthermore, the global transformation is achieved via only some local graph operations.
- @cite introduces a network model of public goods, and studies different features of its Nash equilibria. This model is equivalent to a total effort game with linear investment costs and a general interdependence graph. The authors show that these games always have a Nash equilibrium; i.e., one in which users are either specialists exerting full effort (equivalent to main investors in our terminology), or free-riders. They show that such equilibria correspond to maximal independent sets of the graph, and that specialized equilibira may lead to higher welfare compared to other (distributed) Nash equilibria. Similarly, @cite studies the Nash equilibrium of a linear quadratic interdependence model, and relates the equilibrium effort levels to the nodes' Bonacich centrality in a suitably defined matrix of local complementarities. The work in @cite generalizes these results by studying existence, uniqueness, and closed form of the Nash equilibrium in a broader class of games for which best-responses are linear in other players' actions. All the aforementioned work focuses on the Nash equilibrium in public good provision environments.
- Finally, in the context of security games, our work in Section is most related to @cite @cite . The weighted total effort risk model is a generalization of the total effort model in @cite , and is similar to the effective investment model in @cite and the linear influence network game in @cite . The linear influence models in @cite have been proposed to study properties of the interdependence matrix affecting the existence and uniqueness of the Nash equilibrium. The effective investment model in @cite has been considered to determine a bound on the price of anarchy gap, i.e. the gap between the socially optimal and Nash equilibrium investments, in security games. Our work on the above model complements this literature, by considering the effect of users' interdependence on the performance of incentive mechanisms.
- Varilets fit into the larger context of computational topology and data analysis @cite @cite . Although there may exist mathematical connections to persistent homology @cite and discrete Morse theory @cite , this paper focuses instead on the monotone-light factorization @cite , which for piecewise monotone functions may be seen as a decorated version of the Reeb graph @cite . Sometimes called the contour tree @cite , and within persistence theory the merge tree @cite , the Reeb graph has been often used for simplification of scalar fields @cite @cite and has been exploited throughout computational topology.
- Several researchers have studied information flow, community building and similar processes using Social Networking sites as a reference @cite @cite @cite @cite . However, the great majority focused on network-related features without taking into account the actual content spreading within the network @cite . A hybrid approach focusing on both product characteristics and network related features is presented in @cite : the authors study the effect of passive-broadcast and active-personalized notifications embedded in an application aimed at fostering word of mouth.
- Recently, the relation between content characteristics and virality has begun to be investigated, especially with regard to textual content. @cite , for instance, features derived from sentiment analysis of comments are used to predict the popularity of stories. The relevant work in @cite measures a different form of content spreading, by analyzing which are the features of a movie quote that make it memorable" online. Another approach to content virality, somehow complementary to the previous one, is presented in @cite , where the authors investigate which modification dynamics make a meme spread from one person to another (as compared with movie quotes which spread remaining exactly the same). Louis and Nenkova @cite focused on influential scientific articles in newspapers, considering characteristics such as readability, description vividness, use of unusual words and affective content, comparing high quality articles (NYT articles appearing in The Best American Science Writing" anthology) against typical NYT articles.
- Moreover, the work presented in @cite investigates how differences in textual description affect the spread of content-controlled videos. @cite , the authors focus on the act of resubmissions (i.e., content that is submitted multiple times with multiple titles to multiple different online communities) to understand the extent to which each factor influences the success of a content. @cite it is investigated how content spreads in an online community by pinpointing the effect of wording in terms of content informativeness, generality and affect. Finally, @cite developed a model that can predict the success of requests for a free pizza gifted from the Reddit community, using high level textual features such as politeness, reciprocity, narrative and gratitude.
- More recently, some works have tried to investigate how different textual contents give rise to different reactions in the audience: the work presented in @cite correlates several viral phenomena with the wording of a post, while in @cite it is shown that specific content features variations (like the readability level of an abstract) differentiate among virality level of downloads, bookmarking, and citations. Similarly, @cite studied scientific articles in terms of downloads, Twitter mentions, and early citations in the scholarly records, trying to understand how these virality indices correlate among them. Finally, also in the realm of visual content it has been shown that different image characteristics can give rise to different viral phenomena @cite . Following this line of research, we study the effects of emotions considering several audience reactions to find out whether there are peculiar emotional characteristics of a news article that give rise to different viral reactions.
- . @cite have shown, via a massive experiment on Facebook, that emotional states can be transferred to others via emotional contagion, leading people to experience the same emotions without their awareness. The experiment included reducing the amount of emotional content in the News Feed of a user: when positive expressions were reduced, people produced fewer positive posts and more negative posts; when negative expressions were reduced, the opposite pattern occurred. While in @cite the authors manipulated News Feed content, in our study we simply analyze existing and publicly available content voluntarly annotated by readers.
- @cite , the authors focused on the role of emotions in the context of word-of-mouth marketing, using a dataset of Google+ posts: their analyses show consistency with @cite , with increase in linked to higher likelihood of reshares, while the opposite trend holds for . @cite looked at diffusion patterns on the very popular chinese platform Weibo, which shares many features with Twitter, and again found a similar result: angry posts appear to spread at a significantly faster rate, while sad posts do so at a significant lower rate.
- Furthermore, the work presented in @cite comes closer to the focus of the present paper by hypothesizing that negative news content is more likely to be retweeted, while for non-news tweets positive sentiments support virality. To test this hypothesis the authors analyze three corpora and give evidence that negative sentiment enhances virality in the news segment, but not in the non-news segment. Their conclusion is that the relation between affect and virality is more complex than expected based on the findings of @cite .
- Finally, the work presented in @cite uses articles to examine the relationship between emotions evoked by the content and virality, using semi-automated sentiment analysis to quantify the affectivity and emotionality of each article. Results suggest a strong relationship between affect and virality; still, the virality metric considered is interesting but very limited: it only consists of how many people emailed the article. We consider this work as a starting point for our research and for comparison of results: we will discuss it throughout the paper.
- Other approaches cluster users on the basis of similarity between their semantics profile. Approaches of this kind of systems includes GridVine @cite , the semantic overlay networks @cite and p2pDating @cite . They build a semantic P2P overlay infrastructure that relies on a logical layer storing data.
- They make use of heterogeneous but semantically related information sources whereas our approach does not rely on any kind of semantic interpretation. It, in principle, enables a broader exploitation of more heterogeneous data sources. Related with our proposal is Tribler @cite , a P2P television recommender system. In contrast with our approach, neighbor lists can be directly filled in by the user herself using an interface. No topology or affinity property is considered. We propose a gossip system that construct and maintain in rest groups of dynamic users based on their past activities, without needing their direct intervention.
- Feedback control over noisy channels has been a popular research topic in the past two decades. Most of the early contributions focus on stabilization of unstable dynamical systems using feeback control over band-limited communication channels. A very partial list of papers in this context is @cite @cite @cite @cite @cite @cite @cite . This research direction naturally leads to trade-off studies between the achievable control performance and the required capacity of the sensor-controller communication channel. If the communication rate is finite, larger block length (achieving high resolution) is not necessarily preferred since the resulting delay leads to the loss of control performance @cite . LQG control performance subject to capacity constraints is considered in @cite , where a certain separation principle" between control design and communication design is reported. The authors of @cite consider a fundamental performance limitation of the finite horizon minimum-variance control (MVC) over noisy communication channels in the LQG regime. More comprehensive literature surveys on control designs over communication channels are available in @cite @cite @cite @cite .
- However, the majority of the existing work in this context assume sensor models and or channel models , and are different from ). A few exceptions include @cite and @cite , where sensor-controller joint design problems are considered. However, these works are concerned with sensor power constraints rather than information constraints, and are different from ). Our problem formulation ) falls into a general class of sensor optimization problems considered in @cite , where several results are derived regarding the convexity of the problem and the existence of an optimal solution under different choices of topologies in the space of sensors. However, no structural results on specific problems appear there.
- Broadly, the term is used to refer to the limited ability of decision makers (human or robot) to acquire and process information. The model introduced by @cite in the economics literature characterizes bounded rationality using the idea of Shannon's channel capacity. Inspired by this model, recently @cite considered an information-constrained LQG control problem, which is similar to ). In this paper, we remove the somewhat restrictive assumptions made in @cite , including that a controller there is a time invariant function of the current state only. Furthermore, our SDP-based approach is powerful in handling multi-dimensional systems, while @cite is currently restricted to scalar systems.
- Embedding of objects from triplet or paired distance comparisons goes back to the work of Shepard and Kruskal and studied extensively @cite @cite @cite @cite @cite recently.
- Spectral methods have been successfully used for clustering tasks @cite arising in many different areas such as VLSI @cite , machine learning, data analysis @cite and computer vision @cite @cite . They are usually obtained by formulating the clustering task as a combinatorial optimization problem (such as sparsest normalized cuts @cite ), then solving the corresponding basic SDP relaxation, whose solution is often given by @math extremal eigenvectors of an associated matrix.
- One of the first spectral clustering algorithms with worst case guarantees was given in @cite for the graph partitioning problem assuming certain conditions on the internal versus external conductance. The problem of finding a @math -partition so as to minimize the spectral norm was first introduced by @cite in the context of learning mixtures of Gaussians. The best known approximation factor is @math due to @cite .
- Perhaps the simplest case of is when there is a gap between the @math smallest eigenvalue of Laplacian matrix, @math , and @math of the form ( k+1 1 . ) One might think of this as a stability criteria: It implies that all @math -partitions with maximum expansion @math are @math -close to each other. To put it in another way, approximating the optimum @math -partition is at least as easy as finding a @math -partition with minimum possible expansion among all its clusters. For the case of @math , it is trivial to show that thresholding the second smallest eigenvector of Laplacian yields @math -close partition to the optimal one. On the other hand, when @math , the best prior result is due to @cite , which can find a @math -partition that is @math -close to the optimal one. In other words, when @math , there is no algorithm known to find a non-trivial approximation of the optimum @math -partition.
- Video description generation has been investigated and studied in other work, such as @cite @cite @cite . Most of these examples have, however, constrained the domain of videos as well as the activities and objects embedded in the video clips. Furthermore, they tend to rely on hand-crafted visual representations of the video, to which template-based or shallow statistical machine translation approaches were applied. In contrast, the approach we take and propose in this paper aims at open-domain video description generation with deep trainable models starting from low-level video representations, including raw pixel intensities (see Sec. ) and local motion features (see Sec. ).
- In this sense, the approach we use here is more closely related to the recently introduced static image caption generation approaches based mainly on neural networks @cite @cite @cite @cite @cite . A neural approach to static image caption generation has recently been applied to video description generation by @cite . However, their direct adaptation of the underlying static image caption generation mechanism to the videos is limited by the fact that the model tends to ignore the temporal structure of the underlying video. Such structure has demonstrated to be helpful in the context of event and action classification @cite @cite @cite , and is explored in this paper. Other recent work @cite has explored the use of DVS annotated video for video description research and has underscored the observation that DVS descriptions are typically much more relevant and accurate descriptions of the visual content of a video compared to movie scripts. They present results using both DVS and script based annotations as well as cooking activities.
- Markov Chain model which properties are studied in @cite @cite is used for computing the similarity and ranking @cite @cite . @cite use a stochastic random-walk model to compute similarities between nodes of a graph for recommendation. @cite introduce an N-star model, and demonstrate it in ranking conference and journal problems. Finally, @cite do a thorough review on state-of-the-art and trends of content-based recommender systems.
- Until recently the majority of papers that address performance evaluation in visual tracking were concerned with multi-target tracking scenarios @cite @cite @cite @cite @cite @cite @cite @cite @cite . Single-target tracking is, at least theoretically, a special case of multi-target tracking, however, because of the nature of the target domain, there is a crucial difference in the focus of the evaluation. In multi-target tracking, the focus is on the correctness of target identity assignments for a varying number of targets as well as the accuracy of these detections. The algorithms are often focused on a particular tracking domain, which is typically people or vehicle tracking for surveillance @cite @cite @cite , animal groups tracking @cite or sports tracking @cite , to name a few, which means that tracking in multi-object scenarios involves a lot of domain-specific prior knowledge. A well known PETS workshop (e.g. @cite ) has also been organized yearly for more than a decade with the main focus on performance evaluation of surveillance and activity recognition algorithms.
- Finally, evaluation of tracker performance without ground-truth annotations has been investigated by @cite , where the authors propose to use time-reversible nature of physical motion. As noted by @cite , this approach is not suitable for longer sequences. They propose to extend the approach using failure detection based on the uncertainty of the tracker. The problem is that the method has to be adapted to each tracker specifically and is useful only for investigative, but not for comparative purposes. An interesting approach to tracker comparison has also been recently proposed by Pang and Habin @cite . They aggregate existing experiments, published in various articles, in a page-rank fashion to form a ranking of trackers. The authors acknowledge that their meta-analysis approach is not appropriate for ranking recently published trackers. Furthermore, their approach does not remove bias that comes from correlation of multiple performance measures, which is one of the goals of our work.
- The sparse graph regime studied in the present paper was also recently considered in a series of papers that analyzes community detection problems using ideas from statistical physics @cite @cite @cite . The focus of these papers is on a setting whereby the graph @math contains @math non-overlapping communities, each of equal size @math . Using our notation, vertices within the same community are connected with probability @math and vertices belonging to different communities are connected with probability @math . Interestingly, the results of @cite point at a similar phenomenon as the one studied here for @math . Namely, for a range of parameters the community structure can be identified by exhaustive search, but low complexity algorithms appear to fail.
- Let us mention that the very same phase transition structure arises in other inference problem, for instance in decoding sparse graph error correcting codes, or solving planted constraint satisfaction problems @cite @cite @cite @cite . A unified formalism for all of these problems is adopted in @cite . All of these problems present a regime of model parameters whereby a large gap separates the optimal estimation accuracy, from the optimal accuracy achieved by known polynomial time algorithms. Establishing that such a gap cannot be closed under standard complexity-theoretic assumptions is an outstanding challenge. (See @cite for partial evidence in this direction --albeit in a different regime.) One can nevertheless gain useful insight by studying classes of algorithms with increasing sophistication. This suggests that the principal eigenvector of @math should be localized on the set @math . Indeed this approach succeeds in the dense case (degree of order @math ), allowing to reconstruct @math with high probability @cite .
- In the sparse graph setting considered here, the approach fails because the operator norm @math is unbounded as @math . Concretely, the sparse graph @math has large eigenvalues of order @math localized on the vertices of largest degree. This point was already discussed in several related problems @cite @cite @cite @cite @cite . Several techniques have been proposed to address this problem, the crudest one being to remove high-degree vertices.
- We do not expect spectral techniques to overcome the limitations of local algorithms in the present problem, even in their advanced forms that take into account degree heterogeneity. Evidence for this claim is provided by studying the dense graph case, in which degree heterogeneity does not pose problems. In that case spectral techniques are known to fail for @math @cite @cite , and hence are strictly inferior to (local) message passing algorithms that succeed Note that the definition of @math in the present paper correspond to @math in @cite @cite . for any @math . [Semidefinite relaxations.] Convex relaxations provide a natural class of polynomial time algorithms that are more powerful than spectral approaches. Feige and Krauthgamer @cite @cite studied the Lov 'asz-Schrijver hierarchy of semidefinite programming (SDP) relaxations for the hidden clique problem. In that setting, each round of the hierarchy yields a constant factor improvement in clique size, at the price of increasing complexity. It would be interesting to extend their analysis to the sparse regime. It is unclear whether SDP hierarchies are more powerful than simple local algorithms in this case.
- Let us finally mention that the probability measure ) can be interpreted as the Boltzmann distribution for a system of @math particles on the graph @math , with fugacity @math , and interacting attractively (for @math ). Statistical mechanics analogies were previously exploited in @cite @cite . (See also @cite for the general community detection problem.)
- Early approaches to MER @cite @cite assumed that the perceived emotion of a music piece can be represented as a in the VA space, in which the valence and arousal values are considered as independent numerical values. The ground-truth VA values of a music piece is obtained by averaging the annotations of a number of human subjects, without considering the covariance of the annotations. To predict the VA values of a music piece, a regression model can be applied. Given @math inputs @math , @math , where @math is a @math -dimensional feature vector of the @math -th input segment, @math the number of feature descriptors, and @math the valence or arousal value, a regression model is learned by algorithms such as support vector regression (SVR) @cite that minimize the mismatch (e.g. mean squared loss) between the predicted and the ground-truth VA values.
- As emotion perception is rarely dependent on a single music factor but a combination of them @cite @cite , algorithms used feature descriptors that characterize the loudness, timbre, pitch, rhythm, melody, harmony or lyrics of music @cite @cite @cite @cite . In particular, while it is usually easier to predict arousal using, for example, loudness and timbre features, the prediction of valence has been found more challenging @cite @cite @cite . Cross cultural aspects of emotion perception have also been studied @cite . To exploit the temporal continuity of emotion variation within a piece of music, techniques such as system identification @cite , conditional random fields @cite @cite , hidden Markov models @cite , deep recurrent neural networks @cite , or dynamic probabilistic model @cite have also been proposed. Various approaches and features for MER have been evaluated and compared using benchmarking datasets comprising over 1,000 Creative Commons licensed music pieces from the Free Music Archive, in the 2013 and 2014 MediaEval Emotion in Music' tasks @cite @cite .
- Recent years have witnessed growing attempts to model the emotion of a music piece as a probability distribution in the VA space @cite @cite @cite @cite to better account for the subjective nature of emotion perception. For instance, Figure shows the VA values applied by different annotators to four music pieces. To characterize the distribution of the emotion annotations for each clip, a typical way is to use a bivariate Gaussian distribution, where the mean vector presents the most possible VA values and the covariance matrix indicates its uncertainty. For a clip with highly subjective affective content, the determinant of the covariance matrix would be larger.
- A different methodology to address the subjectivity is to call for a user-dependent model trained on annotations of a specific user to personalize the emotion prediction @cite @cite @cite . In @cite , two personalization methods are proposed; the first trains a MER system for each individual specifically, whereas the second groups users according to some personal factors (e.g. gender, music experience, and personality) and then trains MER system for each user group. Another personalization scheme has also been studied @cite : the first stage estimates the general perception of a music piece, whereas the second one predicts the difference between the general perception and the personal one of the target user.
- We also note that most existing work focuses on the aspect of music emotion research, namely MER. Little work has been made to the aspect -- the development of emotion-based music retrieval systems @cite . In what follows, we present the AEG model and its applications to the both of these two aspects.
- Information Retrieval (IR) is one area where accuracy measurement is paramount. Much work has been done in the area of IR, and IR accuracy measurement techniques have previously been applied to forensic text sting searching @cite , document classification @cite , and even fragmented document analysis in digital forensics @cite . The focus, however, has been on the accuracy measurement of particular techniques or tools within the digital examination process, and not for the examination process itself.
- In this work, we show how a plan for a multi-robot system can be addressed in a BT fashion, gaining all the advantages aforementioned in addition to the scalability that distinguishes a general multiagent system. Many existing works @cite @cite @cite stressed the problem of defining local tasks to achieve a global specification emphasizing the advantages of having a team of robots working towards a global goal. Moreover, @cite @cite introduce the concept of task delegation among agents in a multi-agent system, dividing task specification in , where goal and plan are predefined, and where either only the goal is specified while the plan can be chosen by the agent, or the specified plan describes abstractly what actions have to be taken, giving to the agent some freedom in terms of how to perform the delegated task. In @cite , it is shown how verifying the truth of preconditions on single agents becomes equivalent to checking the fulfillment of a global robot network through recursive calls, using a tree structure called .
- Recent works present some advantages of implementing BTs in robotics applications @cite @cite making comparisons with the CHSs, highlighting their modularity and reusability. In @cite , BTs were used to perform autonomous robotic grasping. In particular, it was shown how BTs enabled the easy composition of primitive actions into complex and robust manipulation programs. Finally, in @cite performance measures, such as success failure probabilities and execution times, were estimated for plans using BTs. However, BTs are mostly used to design single agent behavior and, to the best of our knowledge, there is no rigorous framework in academia using the classical formulation of BTs for multi-robot systems.
- Due to the significance of the PC algorithm in the constraint based causal discovery approach, there have been several methods aiming to modify the procedure of the PC algorithm directly to improve the efficiency and or the accuracy of the algorithm. Steck and Tresp @cite used the necessary path condition to reduce the number of CI tests in the PC algorithm. They have proved that when testing the edge between @math to @math , we do not need to condition on the nodes that are not in a path from @math to @math . This modification may help reduce the chance for making errors by conducting fewer tests. However, it requires more running time for finding the nodes that are not in the path. Other researchers @cite @cite proposed to replace the CI test, which is normally based on the Chi-square statistical test, with the Bayesian statistical tests. They aim to reduce the error rates made by the CI tests. However, there is still no clear evidence in real world datasets about the impacts of this replacement on the efficiency improvement of the PC algorithm.
- Meanwhile, the other constraint based methods @cite infer causal relationships by performing CI tests and searching for specific causal structures. For instance, Cooper @cite proposed the LCD method to learn only a small portion of edges rather than the complete network. Specifically, the LCD algorithm searches for the CCC structures which consist of three pairs of correlated variables: ( @math , @math ), ( @math , @math ) and ( @math , @math ). If @math and @math become independent when conditioned on @math , we may infer that one of the following causal relations exists between @math , @math , and @math : @math ; @math ; and @math . @cite proposed the similar method that focuses on the CCU structures. The CCU method searches for two correlated pairs @math and @math and an independent pair @math . When conditioning on @math , if @math becomes dependent then the causal relationships @math are concluded. Compared to the PC algorithm, such specific structure finding methods are more efficient, but the causal discoveries are incomplete.
- In another direction, @cite proposed an efficient method for testing the persistent associations between the target variable and its neighbours. The method is based on partial association tests and association rule mining framework to remove the spurious associations. In a similar fashion, @cite @cite integrated the ideas of retrospective cohort study with the association rule mining framework. They proposed to divide the samples into two groups of individuals, who share common characteristics but differ in regard to a certain factor of interest. The newly-designed dataset is called fair dataset" and is used to infer the level of influence that the factor of interest affects the target variable. These works were extended in @cite , and the software tools were also provided. However, these methods are only applicable for binary datasets with a fixed target variable, and thus may not be suitable for high dimensional datasets, e.g. gene expression data.
- This paper presents a novel model for cardinality relations in visual recognition, in particular for the analysis of video sequences. Existing video analysis methods generally focus on structured spatio-temporal models, complementary to our proposed approach. For instance, pioneering work was done by @cite in analyzing structured videos by creating storyline'' models populated from AND-OR graph representations. Related models have proven effective at analyzing scenes of human activity more broadly in work by @cite . A series of recent papers has focused on the problem of group activity recognition, inferring an activity that is performed by a set of people in a scene. @cite @cite , @cite , and @cite devised models for spatial and temporal relations between the individuals involved in a putative interaction. @cite consider contextual relations between humans and objects in a scene to detect interactions of interest. The structural relations exploited by these methods are a key component of activity understanding, but present different information from the cardinality relations we study.
- Analogous approaches have been studied for unconstrained'' internet video analysis. Methods to capture the temporal structure of high-level events need to be robust to the presence of irrelevant frames. Successful models include @cite and @cite , who extend latent variable models in the temporal domain. @cite develop hidden Markov models with variable duration states to account for the temporal length of action segments. @cite compose a test video with a set of kernel matches to training videos. @cite effectively combine informative subsets of features extracted from videos to improve event detection. @cite label videos with sequences of low-level actions. Pirsiavash and Ramanan @cite develop stochastic grammars for understanding structured events. @cite propose a feature fusion method based on utilizing related exemplars for event detection. @cite apply multiple instance learning to video event detection by representing a video as multi-granular temporal video segments. Our work is similar in spirit, but contributes richer cardinality relations and more powerful kernel representations; empirically we show these can deliver superior performance.
- For example, @cite formulated a prior on the number of positive instances in a bag, and used an iterative cutting plane algorithm with heuristics to approximate the resultant learning problem. @cite proposed @math SVM for learning from instance proportions, and showed promising results on video event recognition @cite . Our work improves on this approach by permitting more general cardinality relations with an efficient and exact training scheme.
- Our approach models a bag of instances with a probabilistic model with a cardinality-based clique potential between the instance labels. This cardinality potential facilitates defining any cardinality relations between the instance labels and efficient and exact solutions for both maximum a posteriori (MAP) and sum-product inference @cite @cite . For example, @cite used cardinality-based models to embed different ratio-based multiple instance assumptions. Here we extend these lines of work by developing a novel kernel-based learning algorithm that enhances classification performance.
- Kernel methods for multiple instance learning include G "a 's @cite MI-Kernel, which is obtained by summing up the instance kernels between all instance pairs of two bags. Hence, all instances of a bag contribute to bag classification equally, although they are not equally important in practice. To alleviate this problem, Kwok and Cheung @cite proposed marginalized MI-Kernel. This kernel specifies the importance of an instance pair of two bags according to the consistency of their probabilistic instance labels. In our work, we also use the idea of marginalizing joint kernels, but we propose a unified framework to combine instance label inference and bag classification within a probabilistic graph-structured kernel.
- To the best of our knowledge, few recent works have actually looked into instance-specific weight learning @cite @cite . Some of the most promising results are reported in @cite . The basic idea in this work is to propagate fusion weights of labeled instances to the individual unlabeled instances along a graph built on low-level features. The method has been shown to outperform other fusion methods on a variety of datasets. However, although the learned weights are instance specific, the method not only still requires a held-out set for which labels are known, it also requires knowledge of the low-level features of instances. On the other hand, our method does not require held-out data. Moreover, our solution is a meta algorithm that requires no knowledge of the low-level features of the instances. Another issue with @cite is that the weights learned for different test instances are not disjoint from each other. This has the undesirable aspect that newer test instances cannot be independently introduced into the set.
- Metric learning is an active research field with many algorithms, generally divided into linear @cite which learn a Mahalanobis distance, non-linear @cite that learn a nonlinear transformation and use @math distance on the transformed space, and local which learn a metric per datum. The LMNN and MLMM @cite algorithm are considered the leading metric learning method. For a recent comprehensive survey that covers linear, non-linear and local methods see @cite .
- The exemplar-SVM algorithm @cite can be seen as a local similarity measure. This is obtained by maximizing margins, with a linear model, and is weakly supervised as our work. Unlike exemplar-SVM, we learn a Mahalanobis matrix and can learn an invariant metric. Another related work is PMLM @cite , which also finds a local Mahalanobis metric for each data point. However, this method uses global constraints, and therefore cannot work with weakly supervised data, i.e. a single positive example. All the techniques above do not learn local invariant metrics.
- The most common way to achieve invariance, or at least insensitivity, to a transformation in computer vision applications is by using hand-crafted descriptors such as SIFT @cite or HOG @cite . Another way, used in convolutional networks @cite , is by adding pooling and subsampling forcing the net to be insensitive to small translations. It is important to note that transformations such as rotations have a global behaviour, i.e. there is a global consistency between the pixel movement. This global consistency is not totally captured by the pooling and subsampling. As we will see in our experiments, using an invariant metric can be useful even when working with robust features such as HOG.
- Existing work of V2G operation scheduling of EVs can be divided into two classes: charging-only scheduling and bi-directional scheduling. In charging-only scheduling, the algorithms try to optimize the electricity flow from the power grid to the batteries of EVs. For example, Shrestha optimized the EV charging cycles to off-peak periods to flatten the demand curve, in order to reduce the charging cost @cite . Mets presented a smart energy control strategy to charge residential plug-in hybrid EVs (PHEVs) to smooth the system load profile @cite . However, with the development of V2G technology, bi-directional charging, i.e., V2G and G2V, is possible, and bi-directional scheduling algorithm has attracted much research recently. The role of EVs in the power system may change during the day from loads to sources, and vice versa. Binary particle swarm optimization was employed to tackle the V2G scheduling problem to minimize the total running cost and reduce green house gas emission in @cite and @cite . Han proposed an aggregator for V2G frequency regulation in @cite , aiming to maximize the revenue.
- CRO is a recently proposed metaheuristic, which has been developed intensely in the past few years. CRO was originally designed to solve combinatorial optimization problems in @cite , where CRO is adopted to solve the Quadratic Assignment Problem, the Resource-Constraint Project Scheduling Problem, and the Channel Assignment Problem. The Cognitive Radio Spectrum Allocation Problem is addressed in @cite . Yu proposed and solved a Sensor Deployment Problem with CRO in @cite . Lam analyzed the convergence of CRO for combinatorial optimization in @cite . Lam also proposed Real-Coded CRO, a variant of CRO, to solve continuous optimization problems in @cite . Yu solved an Artificial Neural Network training problem in @cite , and proposed several perturbation functions for RCCRO in @cite .
- Researchers have been using metaheuristics to solve UC and its related problems for many years. Mantawy proposed a hybrid algorithm integrating genetic algorithm, tabu search and simulated annealing to solve UC in @cite . Rajan proposed an evolutionary programming-based tabu search method for the same problem in @cite . Yousuf proposed a binary particle swarm optimization to solve the UC with renewable energy sources in @cite . Chen proposed an expert system with elite particle swarm optimization algorithm to solve UC in @cite . As CRO has been applied to solve related power system optimization problems, e.g., @cite @cite and has demonstrated outstanding performance, we adopt CRO to solve this EVUC problem.
- The field of haptics @cite , networked control @cite @cite and teleoperation @cite have also thoroughly studied delays and filtering effects. Specifically, haptics is more related to our work because it is a special case of distributed control in which the master and slave devices require separate feedback controllers. Due to the destabilizing effects of time delays, significant effort has been put forth to ensure systems are stable by enforcing passivity criteria @cite . Other works @cite @cite @cite further relax this constraint and focus on how delay and filtering affect stability. Related work has been performed considering additional real-world effects such as quantization and coulomb friction on system stability @cite . Once more, theses studies do not analyze nor exploit the large sensitivity discrepancy between stiffness and damping feedback loops nor propose solutions to increase performance based on this discrepancy.
- Facebook has its own immune system @cite to safeguard its users from unwanted malicious content. Researchers at Facebook built and deployed a coherent, scalable, and extensible real time system to protect their users and the social graph. This system performs real time checks and classifications on every read and write action. Designers of this complex system used an exhaustive set of components and techniques to differentiate between legitimate actions and spam. These components were standard classifiers like Random Forest, Support Vector Machines, Logistic Regression, a feature extraction language, dynamic model loading, a policy engine, and feature loops. Interestingly, despite this complex immune system deployed by Facebook, unwanted spam, phishing, and other malicious content continues to exist and thrive on Facebook. Although the immune system deployed by Facebook utilizes a variety of techniques to safeguard its users, authors did not present an evaluation of the system to suggest how accurately and efficiently the system is able to capture malicious content.
- , in 2010, presented an initial study to quantify and characterize spam campaigns launched using accounts on Facebook @cite . They studied a large anonymized dataset of 187 million asynchronous wall" messages between Facebook users, and used a set of automated techniques to detect and characterize coordinated spam campaigns. Authors detected roughly 200,000 malicious wall posts with embedded URLs, originating from more than 57,000 user accounts. Following up their work, presented an online spam filtering system that could be deployed as a component of the OSN platform to inspect messages generated by users in real-time @cite . Their approach focused on reconstructing spam messages into campaigns for classification rather than examining each post individually. They were able to achieve a true positive rate of slightly over 80 In an attempt to protect Facebook users from malicious posts, designed an efficient social malware detection method which took advantage of the social context of posts @cite . Authors were able to achieve a maximum true positive accuracy rate of 97
- Multiple machine learning based techniques have been proposed in the past to detect malicious content on other social networks such as Twitter and YouTube @cite @cite @cite @cite . The efficiency of such techniques comes from features like age of the account, number of social connections, past messages of the user, etc. @cite . However, none of these features are available on Facebook publicly. Other techniques make use of OSN specific features like user replies, user mentions, retweets (Twitter) @cite , post views and ratings (YouTube) @cite to identify malicious content, which cannot be ported to Facebook. Blacklists have been shown to be highly ineffective initially, capturing less than 20
- The analysis becomes even more involved when the network is unreliable. Blind and Allg " o wer @cite extended the work of @cite to the case where transmissions from the sensor to the controller take place over an unreliable link. They analytically derived the control cost and the expected inter-event times for different packet loss rates. Rabi and Johansson @cite designed the optimal impulse control and the level triggering mechanism under packet losses with multiple loops sharing a common network. From a different perspective, @cite analyzed the stability of an event-triggered implementation of a controller in the presence of packet losses and limited processing resources. However, in all these works, the unreliable channel appears between sensor and controller, while the controller-actuator communication is lossless.
- The idea of most specific concept for instance checking was first discussed in @cite , and later extensively studied by @cite @cite for the algorithms and the computational complexity. To deal with existential restrictions when computing the most specific concept, @cite @cite @cite discussed the use of cyclic concepts with greatest fixpoint semantics for preservation of information induced by the role assertions, and @cite also proposed an approximation for most specific concept in DLs with existential restrictions.
- On the other hand, for efficient ABox reasoning and instance checking, various optimization techniques have been developed, including lazy unfolding, absorption, heuristic guided search, exploration of Horn clauses of DLs @cite @cite @cite , model merging @cite and extended pseudo model merging technique @cite @cite .
- A common direction of these optimization techniques is to reduce the high degree of nondeterminism that is mainly introduced by GCIs in the TBox: given an GCI @math , it can be converted to a disjunction @math , for which a tableau algorithm will have to nondeterministically choose one of the disjuncts for tableau expansion, resulting in an exponential-time behavior of the tableau algorithm w.r.t. the data size. Absorption optimizations @cite @cite @cite were developed to reduce such nondeterminism by combining GCIs for unfoldable concepts, such that the effectiveness of lazy unfolding can be maximized. For example, axioms @math and @math can be combined into @math , where @math is a named concept; then the inference engine can deduce @math if the ABox contains @math . Notice however, this technique may allow only parts of TBox axioms to be absorbed, thus, may not be able to eliminate all sources of nondeterminism especially when ontologies are complex. Based on the absorption optimization, @cite proposed an approach for efficient ABox reasoning for @math that will convert ABox assertions into TBox axioms, apply a absorption technique on the TBox, and covert instance retrieval into concept satisfaction problems.
- Based on the above lightweight DLs, efficient DL reasoners are developed, such as OWLIM @cite , ELK reasoner @cite , and Oracle's native inference engine for RDF data sets @cite .
- @cite proposed an approximation technique for instance retrieval, which computes both lower bound and upper bound of an answer set of individuals for a given query concept. Their approach invokes an axiom rewriting procedure that converts an ontology in Horn DL into a datalog program, and then uses Oracle's native inference engine to derive the bounds for query answering.
- Recently, techniques for partitioning or modularizing ABoxes into logically-independent fragments have been developed @cite @cite . These techniques partition ABoxes into logically-independent modules, such that each will preserve complete information of a given set of individuals, and thus can be reasoned independently w.r.t. the TBox and be able to take advantage of existing parallel-processing techniques.
- Anonymous games have been studied extensively in the economics literature @cite @cite @cite @cite @cite @cite @cite , where the game being considered is usually nonatomic and consists of a continuum of players but a finite number of strategies. For the discrete setting, two special families of anonymous games are symmetric games @cite @cite and congestion games @cite . @cite gave a polynomial-time for finding an exact Nash equilibrium in a symmetric game. For congestion games, PLS-completeness of pure equilibria was established in @cite @cite @cite 0.03cm These PLS-hardness results have no implication to the setup of this paper since the number of pure strategies in the congestion games considered there are unbounded. , and efficient approximation algorithms for various latency functions were obtained in @cite @cite @cite .
- In @cite , Daskalakis and Papadimitriou presented a PTAS for finding an @math -approximate Nash equilibrium in an anonymous game with two pure strategies, with running time @math , where @math denotes the number of bits required to describe the payoffs. The running time was subsequently improved in @cite to @math . The first PTAS in @cite is based on the existence of an @math -approximate Nash equilibrium consisting of integer multiples of @math , while the second PTAS in @cite is based on the existence of an @math -approximate Nash equilibrium satisfying the following special property: either at most @math players play mixed strategies, or all players who mix play the same mixed strategy. Later @cite extended the result of @cite , giving the only known PTAS for anonymous games with any bounded number of pure strategies with time @math for some function @math of @math , number of pure strategies, and @math .
- We concentrate here on text recognition methods, recognising from a cropped image of a single word, rather than the text detection stages of scene text recognition ( text spotting') that generate the word detections. Traditional text recognition methods are based on sequential character classification, finding characters by sliding window methods ( @cite @cite @cite , after which a word prediction is made by integrating character classifier predictions in a left-to-right manner. The character classifiers include random ferns ( @cite ) in @cite , and CNNs in @cite @cite . Both @cite and @cite use a small fixed lexicon as a language model to constrain word recognition.
- More recent works such as @cite @cite make use of over-segmentation methods, guided by a supervised classifier, to generate candidate character proposals in a single-word image, which are subsequently classified as true or false positives. For example, PhotoOCR ( @cite ) uses binarization and a sliding window classifier to generate candidate character regions, with words recognised through a beam search driven by classifier scores and static N-gram language model, followed by a re-ranking using a dictionary of 100k words. @cite uses the convolutional nature of CNNs to generate response maps for characters and bigrams which are integrated to score lexicon words.
- In contrast to these approaches based on character classification, the work by @cite @cite @cite @cite @cite @cite instead uses the notion of holistic word recognition. @cite @cite still rely on explicit character classifiers, but construct a graph to infer the word, pooling together the full word evidence. @cite use aggregated Fisher Vectors ( @cite ) and a Structured SVM framework to create a joint word-image and text embedding. @cite and more recently @cite also formluate joint embedding spaces, achieving impressive results with minimal training data. @cite use whole word-image features to recognize words by comparing to simple black-and-white font-renderings of lexicon words. In our own previous work ( @cite @cite ) we use large CNNs acting on the full word image region to perform 90k-way classification to a dictionary word.
- @cite had great success using a CNN with multiple position-sensitive character classifier outputs (closely related to the character sequence model in Section ) to perform street number recognition. This model was extended to CAPTCHA sequences (up to 8 characters long) where they demonstrated impressive performance using synthetic training data for a synthetic problem (where the generative model is known), but we show that synthetic training data can be used for a real-world data problem (where the generative model is unknown).
- Several parts of our work are inspired by previous results. Neural network acoustic models and other connectionist approaches were first introduced to speech pipelines in the early 1990s @cite @cite @cite . These systems, similar to DNN acoustic models @cite @cite @cite , replace only one stage of the speech recognition pipeline. Mechanically, our system is similar to other efforts to build end-to-end speech systems from deep learning algorithms. For example, Graves et al. @cite have previously introduced the Connectionist Temporal Classification'' (CTC) loss function for scoring transcriptions produced by RNNs and, with LSTM networks, have previously applied this approach to speech @cite . We similarly adopt the CTC loss for part of our training procedure but use much simpler recurrent networks with rectified-linear activations @cite @cite @cite . Our recurrent network is similar to the bidirectional RNN used by @cite , but with multiple changes to enhance its scalability. By focusing on scalability, we have shown that these simpler networks can be effective even without the more complex LSTM machinery.
- Our work is certainly not the first to exploit scalability to improve performance of DL algorithms. The value of scalability in deep learning is well-studied @cite @cite and the use of parallel processors (including GPUs) has been instrumental to recent large-scale DL results @cite @cite . Early ports of DL algorithms to GPUs revealed significant speed gains @cite . Researchers have also begun choosing designs that map well to GPU hardware to gain even more efficiency, including convolutional @cite @cite @cite and locally connected @cite @cite networks, especially when optimized libraries like cuDNN @cite and BLAS are available. Indeed, using high-performance computing infrastructure, it is possible today to train neural networks with more than 10 billion connections @cite using clusters of GPUs. These results inspired us to focus first on making scalable design choices to efficiently utilize many GPUs before trying to engineer the algorithms and models themselves.
- With the potential to train large models, there is a need for large training sets as well. In other fields, such as computer vision, large labeled training sets have enabled significant leaps in performance as they are used to feed larger and larger DL systems @cite @cite . In speech recognition, however, such large training sets are less common, with typical benchmarks having training sets ranging from tens of hours (e.g. the Wall Street Journal corpus with 80 hours) to several hundreds of hours (e.g. Switchboard and Broadcast News). Larger benchmark datasets, such as the Fisher corpus @cite with 2000 hours of transcribed speech, are rare and only recently being studied. To fully utilize the expressive power of the recurrent networks available to us, we rely not only on large sets of labeled utterances, but also on synthesis techniques to generate novel examples. This approach is well known in computer vision @cite @cite @cite but we have found this especially convenient and effective for speech when done properly.
- A similar line of work to ours in this paper has been proposed by Mishra @cite where they combined monocular cues (color, intensity, and texture) with stereo and motion features to segment a region given an initial user-specified seed point, practically ignoring the first stage in saliency detection (which we address here by automatically generating a seed point). Ultimately, our attempt in this work is to bridge the interactive segmentation algorithms (e.g., @cite @cite ) and saliency detection models and help transcend their applicability.
- Perhaps the most similar work to ours has been published by Li @cite . In their work, they offer two contributions. , they collect an eye movement dataset using annotated images from the PASCAL dataset @cite and call their dataset PASCAL-S. , they propose a model that outperforms other state-of-the-art salient object detection models on this dataset (as well as four other benchmark datasets). Their model decouples the salient object detection problem into two processes: 1) , followed by 2) using fixation prediction. Here, similar to Li , we also take advantage of eye movements to measure object saliency but instead of first fully segmenting the scene, we perform a shallow segmentation using superpixels. We then only focus on segmenting the object that is most likely to attract attention. In other words, the two steps are similar to Li but are performed in the reverse order. This can potentially lead to better efficiency as the first expensive segmentation part is now only an approximation.
- Several salient object detection datasets have been created as more models have been introduced in the literature to extend capabilities of models to more complex scenes. Table lists properties of 19 popular salient object detection datasets. Although these datasets suffer from some biases (e.g., low scene clutter, center-bias, uniform backgrounds, and non-ambiguous objects), they have been very influential for the past progress. Unfortunately, recent efforts to extend existing datasets have only increased the number of images without really addressing core issues specifically background clutter and number of objects. Majority of datasets (in particular large scale ones such as those derived from the MSRA dataset) have scenes with often one object which is usually located at the image center. This has made model evaluation challenging since some high-performing models that emphasize image center fail in detecting and segmenting the most salient off-center object @cite . We believe that now is the time to move on to more versatile datasets and remedy biases in salient object datasets.
- Stochastic finite point sets in the plane, as the one considered in this paper, appear in a natural manner in many database scenarios in which the gathered data has many false positives @cite @cite @cite . This model of random points differs from the model in which @math points are chosen independently at random in some Euclidean region, and questions related to the final positions of the points are considered @cite @cite @cite .
- With respect to the convex hull of stochastic points, in the same model that we consider (called unipoint model @cite ), @cite investigated the most likely convex hull of stochastic points, which is the convex hull that appears with the most probability. They proved that such a convex hull can be computed in @math time in the plane, and its computation is NP-hard in higher dimensions.
- In a more general model of discrete probabilistic points (called multipoint model @cite ), each of the @math points either does not occur or occurs at one of finitely many locations, following its own discrete probability distribution. In this model that generalizes the one considered in this paper, @cite gave exact computations and approximations of the probability that a query point lies in the convex hull, and @cite considered the minimum enclosing ball problem and gave a @math -approximation. In this more general model and other ones, @cite studied approximations of the distribution functions of the solutions of geometric shape-fitting problems, and described the variation of the solutions to these problems with respect to the uncertainty of the points. They noted that in the multipoint model the distribution of area or perimeter of the convex hull may have exponential complexity if all the points lie on or near a circle.
- More recently, in 2014, @cite considered a set of @math points in the plane colored with @math colors, and studied, among other computation problems, the computation of the expected area or perimeter of the convex hull of a random sample of the points. Such random samples are obtained by picking for each color a point of that color uniformly at random. They proved that both expectations can be computed in @math time. We note that their arguments can be used to compute both @math and @math , each one in @math time. In the case of the expected perimeter, similar arguments were discussed by @cite .
- In @cite a polynomial-time optimal solution to the clustering problem is given assuming the unit delay model. In this model, there is no delay that is associated with any gate or with any edge linking two gates within a cluster. A delay of one unit of time is assumed along any edge crossing a cluster boundary. Let us note that in that paper replication of the logic is allowed, that is, a node may be present in many clusters. The more general delay model considered in this paper is presented in @cite . In that paper an algorithm is given that constructs a clustering that aims to minimize the delay under the general delay model. Let us note that the algorithm achieves optimum delay under some specific conditions. In @cite another clustering algorithm is presented, and in the same paper it is proved that the algorithm always finds the optimum clustering.
- Among encryption techniques, only secret sharing @cite handles both data privacy and availability, which is why we focus on this family of approaches. The principle of secret sharing @cite is based on the fact that @math points define a polynomial @math of degree @math . The secret is the polynomial's constant term and the remaining terms are usually randomly selected. Each data piece is transformed into @math shares @math corresponding to points of the polynomial. Reconstruction of the secret is achieved through Lagrange interpolation @cite : there is only one polynomial @math such that @math and @math . Then the secret is @math . Moreover, modern secret sharing schemes, such as multi-secret sharing @cite @cite @cite , verifiable secret sharing @cite , and verifiable multi-secret sharing @cite @cite , also help reduce shared data volume, verify the honesty of CSPs, and both, respectively. We classify secret sharing-based approaches for securing DBs and DWs into two families.
- In the first family of approaches @cite @cite @cite @cite @cite , each table is encrypted into @math shared tables, each of which is stored at one given CSP (Figure ). Recall that only @math of @math shared tables are sufficient to reconstruct the original table. Most of these approaches assume that CSPs are not malicious and that connections between CSPs and users are secure. Only one @cite includes a data verification process that exploits hash-generated signatures: an inner signature (incorporated to the shares) to verify whether CSPs are malicious, and an outer signature that helps detect incorrect or erroneous (lost, damaged, alternative...) data before decryption and prevents useless data transfers. Both signatures are stored at CSPs.
- In the second family of approaches @cite @cite @cite @cite , one or more additional index servers, located at @math , store B++ tree indices and signatures (Figure ). The index servers require higher security and computing power than that of other nodes, and a secure connection to the user's. The index servers support data verification by various means, i.e., homomorphic encryption @cite , a hash function @cite and checksums and a hash function @cite . However, data verification cannot take place if the index server fails.
- Regarding accessing shares, classical secret sharing @cite and most of its above-cited extensions natively support some exact match and aggregation operators, i.e., equality and inequality, sum, average and count. Each approach handles operators needing sorted data, e.g., range operators, maximum and minimum, with various techniques: preaggregation before sharing @cite , a B++ tree index @cite @cite or the rank coefficient used in classical secret sharing @cite @cite @cite . However, extra storage space is needed to store these data structures. Finally, almost all approaches allow updates on shares, since each piece of data is encrypted independently.
- Robotic systems increasingly leverage RGB-D sensors and data for tasks like object recognition @cite , detection @cite @cite , and mapping @cite @cite . RGB-D sensors like the Kinect are cheap, and the extra depth information is invaluable for robots that interact with a 3-D environment.
- Recent work on grasp detection focusses on the problem of finding grasps solely from RGB-D data @cite . These techniques rely on machine learning to find the features of a good grasp from data. Visual models of grasps generalize well to novel objects and only require a single view of the object, not a full physical model @cite @cite .
- Convolutional networks are a powerful model for learning feature extractors and visual models @cite @cite . successfully use convolutional networks for grasp detection as a classifier in a sliding window detection pipeline @cite . We address the same problem as but use a different network architecture and processing pipeline that is capable of higher accuracy at much faster speeds.
- The traditional BnB search algorithm @cite usually ignores the underlying topological information of the residue interaction network constructed based on the backbone template, while AOBB is designed to exploit this property.
- Our work shares the high-level goal of densely annotating the contents of images with many works before us. @cite and @cite studied the multimodal correspondence between words and images to annotate segments of images. Several works @cite @cite @cite @cite studied the problem of holistic scene understanding in which the scene type, objects and their spatial support in the image is inferred. However, the focus of these works is on correctly labeling scenes, objects and regions with a fixed set of categories, while our focus is on richer and higher-level descriptions of regions.
- A number of approaches have been developed for grounding text in the visual domain @cite @cite @cite @cite . Our approach is inspired by @cite who associate words and images through a semantic embedding. More closely related is the work of @cite , who decompose images and sentences into fragments and infer their inter-modal alignment using a ranking objective. In contrast to their model which is based on grounding dependency tree relations, our model aligns contiguous segments of sentences which are more meaningful, interpretable, and not fixed in length.
- Multiple approaches have been developed for representing images and words in higher-level representations. On the image side, Convolutional Neural Networks (CNNs) @cite @cite have recently emerged as a powerful class of models for image classification and object detection @cite . On the sentence side, our work takes advantage of pretrained word vectors @cite @cite @cite to obtain low-dimensional representations of words. Finally, Recurrent Neural Networks have been previously used in language modeling @cite @cite , but we additionally condition these models on images.
- BOINC @cite is an open-source software library used to set up a grid computing network, allowing anyone with a desktop computer connected to the internet to participate in computation; this is called public resource computing . Public resource or volunteer computing was popularized by SETI@Home @cite , a research project that analyzes radio signals from space in the search of signs of extraterrestrial intelligence. More recently, protein folding has emerged as significant success story @cite . Hadoop @cite is an open-source software system for storing very large datasets and executing user application tasks on large networks of computers. MapReduce @cite is a general solution for performing computation on large datasets using computer clusters.
- Some prior studies address the problem of providing to users interactivity in selecting views, while saving on transmitted bandwidth and view-switching delay @cite @cite @cite @cite @cite @cite . The work in @cite is mainly focused on coding views with a minimum level of redundancy in order to simplify the view switching, and the works in @cite @cite optimize the selection of views to be encoded and transmitted based on the user interest. The authors in @cite @cite investigate the transmission of multiview video coded streams on P2P networks and IP multicast, respectively. These works mainly focus on the coding optimization proposed as an a priori defined solution to provide interactive access to the different views @cite . In our work, we rather dynamically optimize the coding modes and the scheduling of video frames for interactive multiview navigation. We extend our preliminary work in @cite to include users' interactivity in the scheduling optimization. This allows the system to dynamically adapt the transmission of the coded frames to various system's dynamics and to outperform the above transmission policies of a priori encoded frames.
- Finally, in our scheduling optimization we aim at minimizing the experienced distortion as well as the temporal variations of the experienced distortion for interactive users. Recently, it has been shown the importance of studying the temporal quality variations in adaptive streaming strategies @cite @cite . These works target single view adaptive streaming over HTTP. We follow similar intuitions and extend the mixed objective function composed on both the perceived distortion and the temporal distortion variation to multiview video navigation applications.
- The papers most relevant to ours are those by @cite and @cite discussed above. @cite also considered the problem of handling queries of the following form (in the uniform-power setting): Given a transmitter @math and query point @math , does @math receive @math by successively applying interference cancellation? (Interference cancellation is a technology that enables a point @math to receive a transmitter @math , even if @math 's signal is not the strongest one received at @math ; see @cite for further details.)
- The hand-crafted features, such as HOG, LBP, and channel features, achieved great success in pedestrian detection. For example, Wang al @cite utilized the LBP+HOG features to deal with partial occlusion of pedestrian. Chen al @cite modeled the context information in a multi-order manner. The deformable part models @cite learned mixture of local templates to account for view and pose variations. Moreover, Doll ' a r al proposed Integral Channel Features (ICF) @cite and Aggregated Channel Features (ACF) @cite , both of which consist of gradient histogram, gradients, and LUV, and can be efficiently extracted. Benenson al @cite combined channel features and depth information. However, the representation of hand-crafted features cannot be optimized for pedestrian detection. They are not able to capture large variations, as shown in Fig. (a) and (b).
- Deep learning methods can learn features from raw pixels to improve the performance of pedestrian detection. For example, ConvNet @cite employed convolutional sparse coding to unsupervised pre-train CNN for pedestrian detection. Ouyang al @cite jointly learned features and the visibility of different body parts to handle occlusion. The JointDeep model @cite designed a deformation hidden layer for CNN to model mixture poses information. Unlike the previous deep models that formulated pedestrian detection as a single binary classification task, TA-CNN jointly optimizes pedestrian detection with related semantic tasks, and the learned features are more robust to large variations, as shown in Fig. (c) and (d).
- . The topic of conditioning a p-DB has been firstly addressed by Koch and Olteanu motivated by data cleaning applications @cite . They have introduced the assert operation to implement, as in AI, a kind of knowledge compilation, viz., world elimination in face of constraints (e.g., FDs). For hypothesis management in @math -DB, nonetheless, there is a need to apply by asserting observed data (not constraints).
- . Finally, our framework is comparable with Bioinformatics' initiatives that address hypothesis encoding into the RDF data model @cite . We point out the Robot Scientist, HyBrow and SWAN (cf. @cite ), all of which consist in some ad-hoc RDF encoding of sequence and genome analysis hypotheses under varying levels of structure (viz., from gene G has function A' statements to free text). Our framework in turn consists in the U-relational encoding of hypotheses from mathematical equations, which is (to our knowledge) the first work on hypothesis relational encoding.
- Given a set @math of @math points in the plane, the optimal placement problem is to compute a translate that contains the maximum number of points in @math ; note that in our problem, the distinct canonical translates contain the points of @math in a locally maximal manner (see Definition ). There are some optimal placement algorithms for basic planar objects: Chazelle and Lee @cite presented @math time algorithm for disks, and Eppstein and Erickson @cite proposed @math time algorithm for rectangles; for convex polygons of @math vertices, @cite devised @math time algorithm, and @cite improved the time complexity to @math , where @math is the maximum number of points in @math that can be contained in a translate.
- Ching-Chi Lin et. al in @cite presented an improved version of Round Robin algorithm used in Eucalyptus. According to Dynamic Round Robin algorithm, if a virtual machine has finished its execution and there are still other virtual machines running on the same physical machine, this physical machine will not accept any new virtual machine requests. Such physical machines are referred to as being in retirement' state, meaning that after the execution of the remaining virtual machines, this physical machine could be shutdown. And if a physical machine is in the retirement' state for a sufficiently long period of time, the currently running virtual machines are forced to migrate on to other physical machines and shutdown after the migration operation is finished. This waiting time threshold is denoted as retirement threshold'. So, a physical machine which is in the retirement state beyond this threshold will be forced to migrate its virtual machines and shutdown.
- In @cite , the authors propose Single Threshold algorithm which sorts all the VMs in decreasing order of their current utilization and allocates each VM to a physical machine that provides the least increase of power consumption due to this allocation. The algorithm does optimization of the current allocation of VMs by choosing the VMs to migrate based on CPU utilization threshold of a particular physical machine called Single Threshold'. The idea is to place VMs while keeping the total utilization of CPU of the physical machine below this threshold. The reason for limiting CPU usage below the threshold is to avoid SLA violation under a circumstance where there is a sudden increase in CPU utilization of a VM, which could be compensated with the reserve. Single Threshold algorithm works better in terms of energy conservation when compared to Dynamic Round Robin Algorithm discussed in . This algorithm is fairly improved one which takes into consideration of power consumption and CPU usage of physical machines.
- Dynamic Voltage Scaling (DVS) is a power management technique where under-volting (decreasing the voltage) is done to conserve power and over-volting (increasing the voltage) is done to increase computing performance. This technique of DVS has been employed in @cite @cite to design power-aware scheduling algorithms that minimize the power consumption. @cite apply a variation of DVS called Dynamic Voltage Frequency Scaling (DVFS) by operating servers at various CPU voltage and frequency levels to reduce overall power consumption.
- Regulatory practice has been part of the wireless market since its inception as the resource being shared was soon recognized to be very valuable. Governmental agencies establish regulatory policies to increase the efficiency of the spectrum market while preserving an acceptable level of fairness in sharing the precious spectrum. The concept of subsidization is one way to involve the government so as to incentivize various long-term goals in the market being regulated. Typically, subsidy regulations are used for markets serving the larger good'' (i.e., the benefit of the whole society) and involving a significant amount of cost producers cannot bare individually. Transportation @cite , agriculture @cite and telecommunications @cite are examples of such markets where new initiatives require significant infrastructure investments, and hence the government's involvement is justified. However, subsidization is known to have negative effects on the efficiency of the market. Thus, subsidization is preferred if at all possible.
- Even though subsidization is heavily employed in several markets, the wireless market has not seen much of its usage beyond a few, limited scenarios. The most known subsidization in a wireless market is the subsidization of the expensive phones to the users by the provider @cite , in that the user pays the phone's cost over a locked, termed contract. Unlocking the contract term either requires return of the phone or payment of a significant fee.
- Only recently, Yu and Kim @cite focused on the concept of using subsidies for spectrum management. Their work analyzed price and quality of service (QoS) subsidy schemes to increase the utility of the consumers from the data plans. The goal is to increase the availability of wireless data plans to more users. In their model, the regulator offers a subsidized (i.e., less expensive) data plan to the end users with lower quality of service via subsidization to the providers. In our work, we consider subsidizing the spectrum to the providers and focus on implementing a subsidy system mainly between the providers and the government. We aim to make the subsidization seamless to the end users and aim to incentivize the providers to be more welcoming to the users subscribed to other providers. As a major difference, we focus on spectrum subsidy rather than data plan subsidy.
- @cite , a roaming market model is introduced, which uses the roaming rate as an incentive for service providers to gain revenue, when they allow other users to access their network. Optimal roaming rate is derived to maximize the social welfare of all the service providers and licensed users. Our approach for subsidization, while also allows inter-operator sharing of spectrum similar to roaming, is fundamentally different than these earlier work, since it involves the government as the subsidy source to facilitate spectrum sharing.
- The question of existence of bi-Lipschitz homeomorphisms between different subsets of @math , satisfying certain conditions, has been studied classically, notably by McMullen @cite . It is shown in that paper that there exists a positive function @math on @math , @math , such that there does not exist a bi-Lipschitz homeomorphism @math with @math . It is also proved that there exists separated nets in @math ( @math ) which cannot be mapped into @math in a bi-Lipschitz way.
- Shortly before uploading this paper we came across a very recent paper @cite on arXiv which investigates related questions and proves a variant of Theorem for sets @math of sufficiently small measure (Proposition 11, @cite ), as well as much more. Their proof uses a covering lemma based on a result of @cite , which is no longer true in higher dimensions. Our proof is different and we believe can be adapted to work in higher dimensions as well. Moreover, the stretching result only for sufficiently small sets is not sufficient for our purposes in proving rough isometry. Our work was independent of @cite .
- The question considered in this paper has connections to several different problems in analysis, an interested reader is referred to @cite and the references therein for more details.
- An important aspect of @cite is the qualitative analysis of the learned intermediate representations. In particular, they show that CNNs trained on ImageNet contain models for basic object shapes at lower layers and object part models at higher layers. This is exactly the property that we make use of in our approach. Although the CNN we use was not particularly trained on the fine-grained task we consider, it can be used for basic shape and part detection.
- Entirely replacing the low-level features is difficult, because a CNN produces global features. Zou al @cite solves this by associating the output of a hidden layer with spatially related parts of the input. This allows them to apply the regionlets approach. However, their approach requires numerous evaluations of the CNN and the features are not arbitrarily dense. In contrast, our approach is working on the full resolution of the input.
- The work of Jain al @cite uses the sliding window approach for part localization with CNNs. They evaluate a CNN at each position of the window in order to detect human body parts in images without using bounding box annotations. This requires hundreds or thousands of CNN evaluations. As sufficiently large CNNs take some time for prediction, this results in long run times. In contrast, only one forward- and one back-propagation pass per part is required for the localization with our approach. In @cite and @cite , the part positions are estimated by a CNN specifically trained for this task. The outputs of their network are the coordinates of each part. In contrast to their work, our approach does not require any separately trained neural network but can exploit a CNN already trained for a different large-scale classification task.
- The proof of combines the second moment arguments from Achlioptas and Naor @cite and its enhancements from @cite @cite with the small subgraph conditioning'' method @cite @cite . More precisely, the key observation on which the proof of is based is that the fluctuations of @math can be attributed to the variations of the number of bounded length cycles in the random graph.
- We expect that the present approach of combining the second moment method with small subgraph conditioning can be applied successfully to a variety of other random constraint problems. Immediate examples that spring to mind include random @math -NAESAT or random @math -XORSAT, random hypergraph @math -colourability or, more generally, the family of problems studied in @cite . (On the other hand, we expect that in problems such as random @math -SAT the logarithm of the number of satisfying assignments exhibits stronger fluctuations, due to a lack of symmetry.)
- As mentioned above, power iteration is a natural approach to tensor factorization and was studied in several earlier papers. Most recently, interest within machine learning was spurred by @cite .
- Our Theorem is analogous to the main result of @cite although incomparable: In @cite the signal' part of the tensor @math is assumed to have an orthogonal decomposition @math with @math bounded away from zero. Here, the signal part has rank one (equivalently, all the @math 's but one vanish). In @cite only the case of third order tensors ( @math ) is considered. We characterize power iteration for general @math . We establish convergence in a number of iterations @math that is @math . In @cite the number of iterations is bounded by a polynomial in @math . We evaluate our bounds in the case of Gaussian noise. This allows a comparison with other methods, such as tensor unfolding.
- In this section, we briefly go through the related work. Mood Weather Relationship in Psychology and Physiology It has long been believed that there should be some sort of correlation between weather and individual mood state. In psychological literature, researches on this topic can be traced back to early 1950s when Nowis @cite tried to explore the external factors that affect individuals' emotions. While correlational analysis between mood and weather has been conducted in multiple psychological researches @cite @cite @cite @cite @cite @cite @cite , conclusions are amazingly diverse. Low levels of humidity @cite , high levels of sunlight @cite @cite @cite , high barometric pressure @cite and high temperatures @cite @cite have been associated with high mood, but high temperature has been associated with low mood in some other work @cite @cite . Meanwhile, some studies traced no direct correlations between mood and any type of weather variables @cite @cite : Watson and Lee @cite traced the daily mood reports of 498 students in Dallas, Texas and found no significant correlations between mood (measured by self-report using the Positive and Negative Affect Scale) and any of the assessed weather variables, including sunshine, barometric pressure, temperature, and precipitation.
- Despite the on-going debates in psychology, most physiologists believe that the weather indeed has significant influence on psychological processes based on strong physiological evidence. For example, @cite spotted rising and dipping production of serotonin, a neurotransmitter in human central nervous system, which is generally thought to be a contributor to feelings of well-being and happiness, as sunlight increased or decreased. Another line of evidence is provided by population-wide behavior studies in social psychology. A good example is seasonal affective disorder (SAD), which refers to the mood disorder in which people who have normal mental health throughout most of the year experience depressive symptoms in the winter http: en.wikipedia.org wiki Seasonal_affective_disorder . Researchers suggest that exposure to sunlight would immediately improve mood and diminishes SAD @cite @cite @cite @cite and that rainy or cloudy days can aggravate the symptoms @cite . Additionally, reliable and significant associations are found between weather and mood-related social behaviors. For example, low barometric pressure @cite and more snow are evidently linked with higher suicide rates @cite and high temperature is associated with violent behavior @cite @cite .
- As Twitter mood indicators have been demonstrated as both robust and sensitive @cite @cite , researchers are exploring ways to predict a variety of real-world outcomes based on indicators, such as economics @cite @cite @cite @cite , stock market @cite @cite or political events @cite @cite @cite @cite or disease @cite @cite . Lindsay @cite built a sentiment classifier with Facebook data set and demonstrates the significant correlation between microblog sentiments and polls during 2008 presidential election. O' @cite tried to predict political events based on user-mood analysis on twitter.
- Our method is motivated by the Kac-Rice test , an exact method for testing and constructing confidence intervals for signals under the global null hypothesis in adaptive regression. Our work corresponds to the Kac-Rice test under the global null scenario, in a penalized regression minimizing the Frobenius norm with a nuclear norm penalty. In this paper, we extend the test and confidence intervals to the general case with improved power. The resulting statistic uses the survival function of the conditional distribution of the eigenvalues of a Wishart matrix. In the context of inference based on the distribution of eigenvalues, @cite and, more recently, @cite have proposed methods for testing essentially the same hypothesis as in this paper. Both @cite and @cite benefit from an asymptotic distribution of the test statistic: @cite forms a likelihood ratio test with the asymptotic Chi-square distribution. @cite use the Tracy-Widom law, which is the asymptotic distribution of the largest eigenvalue of a Wishart matrix, incorporating the result of @cite . We provide a method for constructing confidence intervals in addition to hypothesis testing, with the procedures being exact.
- introduce the Continuous Monitoring Model @cite focusing on systems comprised of a coordinator and @math nodes generating or observing data streams. Consequently, the goal shifts to continuously (that is, in every time step) compute a function depending on the information available across all @math data streams up to the current time at a dedicated coordinator. The major objective becomes the minimization of the overall number of messages exchanged between the nodes and the coordinator. We refer to this model and enhance it by a broadcast channel as proposed by in @cite .
- An important class of problems in this model investigated in literature are threshold computations, i.e., the coordinator is supposed to decide whether the current function value has reached some given threshold @math or not. If the function is monotone, e.g., monitoring the number of distinct values or the sum over all values, exact characterizations including matching lower bounds in the deterministic case are known @cite @cite . However, non-monotone functions, e.g., the entropy @cite , turned out to be much more complex to handle.
- A general approach to reduce the communication when monitoring distrib -uted data streams is proposed in @cite . introduce the notion of , which are also an integral part of our algorithm presented in this paper. They consider a problem called continuous skyline maintenance, in which a coordinator is supposed to continuously maintain the skyline of dynamic objects. As they aim at minimizing the communication overhead between the coordinator and the objects, they use a filter method that helps in avoiding the transmission of updates from the objects to the coordinator in case these updates cannot influence the skyline maintained by the coordinator. More precisely, the dynamic objects are points of a @math -dimensional space and filters are hyper-rectangles assigned by the coordinator to the objects in such a way that as long as these points are within the assigned hyper-rectangle, updates need not be communicated to the coordinator.
- In their work @cite , Yi and Zhang were the first to study streaming algorithms with respect to their competitiveness. In their model there is one node and one coordinator and the goal is to keep the coordinator informed about the current value of a multivalued function @math that is observed by the node and changes its value over time, while minimizing the number of messages. Among others, for @math , Yi and Zhang present an algorithm that is @math -competitive if the last value received by the coordinator might deviate by @math from the current value of @math . For arbitrary @math , a competitiveness of @math is shown.
- Following the idea of studying competitive algorithms for monitoring streams and the notion of filters, @cite present an algorithm for online dominance tracking of distributed streams. In this problem a coordinator always has to be informed about the dominance relationship between @math distributed nodes each observing an online stream of @math -dimensional values. Here, the dominance relation of two nodes @math and @math currently observing @math and @math , respectively, both from @math , is defined as @math dominates @math if @math for some @math and @math for all other @math . Their algorithm is based on the idea of assigning filters to the nodes and they show that a mid-point strategy, which basically sets filters to be the mid-point between neighboring nodes, is @math -competitive with respect to the number of messages sent in comparison to an offline algorithm that sets filters optimally.
- The loosening and decoupling of the DL and UL association has been indicated in @cite , and has been studied in a few selected pioneering contributions @cite @cite @cite @cite .
- Notably, the most recent contributions @cite @cite have studied the impact of having a cell association which is different for downlink and uplink. The authors of @cite have investigated the throughput and outage gains via the real-world simulation tool Atoll; the simulation results have yielded important performance gains which increase with an increasing density of small cells. The authors of @cite then studied the association probabilities in more details, i.e. the probabilities that a device would associate i) both DL and UL to the Mcell; ii) DL to Scell and UL to Mcell; iii) DL to Mcell and UL to Scell; and iv) both DL and UL to the Scell.
- Earlier contributions @cite @cite introduced the early notion of DUDe. In particular, in @cite , DUDe is considered as a part of a broader device-centric'' architectural vision, where the set of network nodes providing connectivity to a given device and the functions of these nodes in a particular communication session are tailored to that specific device and session. A study in @cite tackles the problem from an energy efficiency perspective where the UL DL decoupling allows for more flexibility in switching-off some BSs and also for saving energy at the terminal side.
- Their work spawned a series of new formulations within the framework of Discontinous Galerkin methods, such as the Trefftz formulation of @cite @cite , and the discontinuous enrichment method of @cite ;
- Simultaneously to the ultra weak formulation, the partition of unity method (PUM) by Babuska and Melenk @cite , and the least-squares method by Monk and Wang @cite , were developed.
- proposed the multi-trace formulation @cite , which involves solving an integral equation posed on the interfaces of a decomposed domain, which is naturally well suited for operator preconditionning in the case of piece-wise constant medium;
- Chen and Xiang proposed another instance of efficient domain decomposition where the emphasis is on transferring sources from one subdomain to another @cite @cite ;
- proposed a large-subdomain sweeping method, based on an approximate Green's function by geometric optics and the butterfly algorithm, which can handle transmitted waves in very favorable complexity @cite ; and a variant of this approach that can handle caustics in the geometric optics ansatz @cite ;
- , developped an additive Schwarz iteration coupled with coarse grid preconditioner based on an eigenvalue decomposition of the DtN maps inside each subdomain @cite ;
- Finally, an earlier version of the work presented in this paper can be found in @cite , in which the notion of polarized traces using Green's representation formula was first introduced, obtaining favorable scalings;
- Bhowmik and Stolk recently proposed new rules of optimized coarse grid corrections for a two level multigrid method @cite ; Brandt and Livshits developed the wave-ray method @cite , in which the oscillatory error components are eliminated by a ray-cycle, that exploits a geometric optics approximation of the Green's function; Elman and Ernst use a relaxed multigrid method as a preconditioner for an outer Krylov iteration in @cite ; Haber and McLachlan proposed an alternative formulation for the Hemholtz equation @cite , which reduces the problem to solve an eikonal equation and an advection-diffusion-reaction equation, which can be solved efficiently by a Krylov method using a multigrid method as a preconditioner; and Grote and Schenk @cite proposed an algebraic multi-level preconditioner for the Helmholtz equation.
- implemented a 3D solver using the complex-shifted Laplacian with optimal grids to minimize pollution effects in @cite . Multigrid methods applied to large 3D examples have been implemented by Plessix @cite , @cite and, more recently, @cite . A good review of iterative methods for the Helmholtz equation, including complex-shifted Laplace preconditioners, is in @cite . Another review paper that discussed the difficulties generated by the high-frequency limit is @cite . Finally, beautiful mathematical expositions of the Helmholtz equation and a good reviews are @cite and @cite .
- Consensus and synchronization of node states in a network of coupled dynamics have been extensively studied in the past decades @cite @cite @cite . The basic idea is that @cite nodes in a network with suitable connectivity can reach a common state or trajectory where all nodes' initial values are encoded with distributed node interactions, and the strong involvement of graph-theoretic approaches has reshaped the study of networked control systems @cite . Excellent results have been derived towards the understandings of how bidirectional or directed, fixed or switching, deterministic or random, node interactions influence the consensus value and the speed of consensus, e.g., @cite @cite @cite @cite .
- Scientific interest on consensus and synchronization subject to the laws of quantum mechanics has also been noticed. Recent work @cite introduced the measures of synchronization in quantum systems of coupled Harmonic oscillators. Sepulchre @cite generalized consensus algorithms to non-commutative spaces and presented convergence results for quantum stochastic maps to a fully mixed state. Mazzarella @cite made a systematic study of consensus-seeking in quantum networks, where four classes of consensus quantum states based on invariance and symmetry properties were introduced and a quantum gossip algorithm @cite was proposed for reaching a symmetric consensus state over a quantum network. The class of quantum gossip algorithms was extended to symmetrization problems in a group-theoretic framework in @cite .
- Developments in continuous-time quantum consensus seeking were made in @cite @cite @cite for Markovian quantum dynamics governed by master equations @cite . In @cite , using a group-theoretic analysis, the authors proposed a consensus master equation involving permutation operators and showed that a symmetric state consensus can be achieved under such evolutions. In @cite , a graphical method was systematically introduced for building the connection between the quantum consensus dynamics and its classical analogue by splitting the evolution of the entries in the network density operator. The idea of breaking down large density operators of multiple qubits can in fact be traced back to @cite using Stokes tensors. A type of quantum synchronization was also shown in the sense that the network trajectory tends to an orbit determined by the network Hamiltonian and the symmetrization of the initial state, which implies that all quantum nodes asymptotically reach the same orbit @cite . For research on quantum network control and information processing we refer to @cite @cite @cite ; for a survey for quantum control theory we refer to @cite .
- When it comes to the resource allocation in hierarchical networks, such as the femtocell networks and cognitive radio networks, the Stackelberg game @cite based modeling is widely preferred since it is able to reflect the features of hierarchy and ad-hoc topology in the network @cite @cite . the Stackelberg game is characterized by the sequential decision making manner (namely, the follower-leader strategy updating), and hence suitable for modeling the heterogeneous user behaviors in the network. Due to the computational intensity for obtaining the Stackelberg Equilibrium (SE), most of the existing studies @cite @cite @cite adopt a utility model that favors the derivation of a closed-form SE, and solve for the SEs through transforming the games as hierarchical optimization problems.
- Larivi e re et al @cite examined the concentration of citations at the article level over 1900-2007 and concluded that while distributions of citations remained highly skewed, the fraction of citations to highly cited articles has been decreasing. Looking at broad research areas, they found that for Natural Sciences & Engineering and Medicine , the decrease in concentration started around 1990.
- Lozano et al @cite studied the relationship between impact factor and article citations. They examined three broad categories, Natural Sciences & Medicine , Physics and Social Sciences , over 1902-2009 and found that the correlation between impact factor of the journal and the citations its articles receive has been weakening since around 1990. In another part of their study, they computed the fraction of 10 in the 10 on Natural Sciences & Medicine . They found that a decreasing fraction of most-cited articles is being published in the most-cited journals.
- In a follow-up study @cite , they took a closer look at 13 journals, seven traditionally top-ranked journals and six upcoming journals. They found that since around 1990, the fraction of most-cited papers published in the traditionally top-ranked journals has been dropping and the fraction of such papers published in the upcoming journals has been increasing.
- Our tree-grafting approach is related to a technique used for tree augmentation in @cite , where parse-tree nodes are augmented with semantic categories. @ augment tree nodes with named entities and relations, while we used named entities and modalities. The parser is subsequently retrained for both semantic and syntactic processing. The semantic annotations were done manually by students following a set of guidelines and then merged with the syntactic trees automatically. In our work we tagged our corpus with entities and modalities automatically and then grafted them onto the syntactic trees automatically, for the purpose of training a statistical machine translation system. An added benefit of the extracted translation rules is that they are capable of producing semantically-tagged Urdu parses, despite that the training data were processed by only an English parser and tagger.
- Related work in syntax-based MT includes @cite , where a series of syntax rules are applied to a source language string to produce a target language phrase structure tree. The Penn English Treebank @cite is used as the source for the syntactic labels and syntax trees are relabeled to improve translation quality. In this work, node-internal and node-external information is used to relabel nodes, similar to earlier work where structural context was used to relabel nodes in the parsing domain @cite . Klein and Manning's methods include lexicalizing determiners and percent markers, making more fine-grained VP categories, and marking the properties of sister nodes on nodes. All of these labels are derivable from the trees themselves and not from an auxiliary source.
- & 3 * -1.5pt @math -1.5pt & 2 * & 2 * -1.5pt exact probabilities -1.5pt & -1.5pt 5-5 & & & & 3-5 & & & -1.5pt exact probabilities & 2-5 & 4 * -1.5pt @math -1.5pt & 2 * & -1.5pt exact probabilities -1.5pt & -1.5pt @cite 4-5 & & & -1.5pt zero--one laws & -1.5pt @cite @cite 3-5 & & 2 * & -1.5pt exact probabilities -1.5pt & -1.5pt @cite 4-5 & & & -1.5pt zero--one laws & -1.5pt @cite @cite 8 * & 4 * -1.5pt @math -1.5pt & 2 * & -1.5pt exact probabilities -1.5pt & 4 * -1.5pt -1.5pt 4-4 & & & -1.5pt zero--one laws & 3-4 & & 2 * & -1.5pt exact probabilities -1.5pt & 4-4 & & & -1.5pt zero--one laws & 2-5 & 4 * -1.5pt @math -1.5pt & 2 * & -1.5pt exact probabilities -1.5pt & -1.5pt @cite 4-5 & & & -1.5pt zero--one laws -1.5pt & -1.5pt @cite 3-5 & & 2 * & -1.5pt exact probabilities -1.5pt & -1.5pt @cite 4-5 & & & -1.5pt zero--one laws & -1.5pt @cite @cite 2.5
- For graph @math , Bloznelis and uczak @cite recently consider the following three properties: (i) the graph has a minimum vertex degree at least @math ; (ii) the graph has a perfect matching; (iii) the graph is connected. They present that when @math is even, for each of these three properties, its probability converges to @math as @math , under conditions @math ( @math is the edge probability), @math and @math for some @math .
- Bloznelis @cite investigates the clustering coefficient of graph @math and show that the chance of two neighbors of a given vertex @math to be adjacent decays as @math , where @math is a positive constant and @math is the degree of vertex @math . Bloznelis @cite study the correlation coefficient of degrees of adjacent vertices in @math . Bloznelis @cite demonstrate that a connected component in @math with at at least a constant fraction of @math emerges as @math when the edge probability @math exceeds @math .
- Before the introduction of large stereo datasets @cite @cite , relatively few stereo algorithms used ground-truth information to learn parameters of their models; in this section, we review the ones that did. For a general overview of stereo algorithms see @cite .
- used sum of squared distances to compute an initial matching cost. They trained a model to predict the probability distribution over three classes: the initial disparity is correct, the initial disparity is incorrect due to fattening of a foreground object, and the initial disparity is incorrect due to other reasons. The predicted probabilities were used to adjust the initial matching cost. later extend their work by combining predictions obtained by computing normalized cross-correlation over different window sizes and centers. initialized the matching cost with AD-Census @cite and used multiclass linear discriminant analysis to learn a mapping from the computed matching cost to the final disparity.
- Recent work @cite @cite focused on estimating the confidence of the computed matching cost. used a random forest classifier to combine several confidence measures. Similarly, trained a random forest classifier to predict the confidence of the matching cost and used the predictions as soft constraints in a Markov random field to decrease the error of the stereo method.
- This aforementioned connection was substantially refined recently by Peres and Sousi ( @cite Theorem 1.1) and independently by Oliveira ( @cite Theorem 2). Their approach relied on the theory of random times to stationarity combined with a certain de-randomization" argument which shows that for any lazy reversible irreducible finite chain and any stopping time @math such that @math , @math . As a (somewhat indirect) consequence, they showed that for any @math (this was extended to @math in @cite ), there exist some constants @math such that for any lazy reversible irreducible finite chain
- This work was greatly motivated by the aforementioned results. It is natural to ask whether ) could be further refined so that the cutoff phenomenon could be characterized in terms of concentration of the hitting times of a sequence of sets @math which attain the maximum in the definition of @math (starting from the worst initial states). Corollary 1.5 in @cite asserts that this is indeed the case in the transitive setup. More generally, Theorem 2 in @cite asserts that this is indeed the case for any fixed sequence of initial states @math if one replaces @math and @math by @math and @math (i.e. when the hitting times and the mixing times are defined only w.r.t. these starting states). Alas, Proposition 1.6 in @cite asserts that in general cutoff could not be characterized in this manner.
- Prior work has studied and detected sybil and or fake OSN accounts by relying on tightly-knit community structures @cite @cite @cite @cite @cite . Findings revealed by our work also highlight several characteristics about the social structure and activity of fake profiles attracted by the honeypot pages, e.g., their interconnected nature or the activity bursts. In fact, our analysis does not only confirm a few insights used by sybil detection algorithms but also reveals new patterns that could complement them. A few passive measurement studies have also focused on characterizing fake user accounts and their activity. Nazir al @cite studied phantom profiles in Facebook gaming applications, while Thomas al @cite analyzed over 1.1 million accounts suspended by Twitter. Gao al @cite studied spam campaigns on Facebook originating from approximately 57,000 user accounts. Yang al @cite performed an empirical analysis of social relationships between spam accounts on Twitter, and Dave al @cite proposed a methodology to measure and fingerprint click-spam in ad networks. Our work differs from these studies as they all conduct passive measurements, whereas we rely on the deployment of several honeypot pages and (paid) campaigns to actively engage with fake profiles.
- Stringhini al @cite and Lee al @cite created honeypot profiles in Facebook, MySpace, and Twitter to detect spammers. Their work differs from ours in that (1) their honeypot profiles were designed to look legitimate, while our honeypot pages explicitly indicated they were not real'' (to deflect real profiles), and (2) our honeypot pages actively attracted fake profiles by means of paid campaigns, as opposed to passive honeypot profiles. Also, Thomas al @cite analyzed trafficking of fake accounts in Twitter. They bought fake profiles from 27 merchants and developed a classifier to detect these fake accounts. In a similar study, Stringhini al @cite analyzed the market of Twitter followers , which, akin to Facebook like farms, provide Twitter followers for sale. Note that Twitter follower markets differ from Facebook like farms as Twitter entails a follower-followee relationship among users, while Facebook friendships imply a bidirectional relationships. Also, there is no equivalent of liking a Facebook page in the Twitter ecosystem.
- Beutel al @cite proposed a technique to detect fake likes based on identifying groups of users who liked a set of pages within a given time period. However, their technique does not rely on ground truth data, so it is unclear whether or not the detection mechanism blocks all fake likes, or actually only those exhibiting a certain pattern. By contrast, we focus on actively measuring like fraud activities by means of honeypots, i.e., attracting fake likes to empty pages, through payment. We elicit and study ground truth data, and highlight how some like farms actually try to emulate behavior of regular users and thereby stay below the detection radar. Nonetheless, our work serves as the starting point for improved fake like detection and can complement techniques from @cite .
- HathiTrust @cite is a collaborative repository of digital content from various research institutions and libraries to preserve cultural records and to ensure accessibility and availability of the preserved content in future.
- These three libraries are good sources of scanned images of dictionaries. Mirdeghan conducted a survey to compare the interfaces of these three libraries, which shows that the majority of participants preferred Open Library followed by Google Books @cite . Our interface idea is inspired by the Open Library BookReader interface with extra features specific to dictionary exploration.
- The reason to focus on the Lipschitz condition for @math seems to be fundamental when studying the computational complexity of . Indeed it is well-known (see e.g. [Theorem 7.3] Ko91 ) that if @math is not Lipschitz, then the solution of can have arbitrarily high complexity, even if @math is assumed to be polynomial-time (precision-)computable and has a unique solution. The Lipschitz condition plays an instrumental role in the complexity because it is used to derive the number of steps of the algorithm, for example in Picard-Lindel " o f theorem it is used to bound the number of iterations. It was recently shown @cite , following an open problem from Ko [Section 7.2] Ko91 , that if @math is Lipschitz and polynomial-time (precision-)computable, then the solution of over @math can still be PSPACE-complete (but it was already known not to be of higher complexity @cite ).
- In @cite we have shown that a different kind of restriction on allows for a finer complexity analysis over unbounded time domain. More precisely, if @math is a polynomial (i.e. @math is solution of ) then @math can be computed in time polynomial in @math , in the accuracy of the result, and in @math . More precisely, its running time is polynomial in the product @math (and the size of the coefficients). However this result is not satisfactory for several reasons. First, and we insist on this point, it requires some , i.e. a bound on @math , which is only semicomputable in general. This means the algorithm in @cite is unusable without some knowledge of the result. Second, this result is in some sense a worst-case scenario: if @math spikes'' and then becomes really small, then the resulting complexity will be high. Here we present an algorithm where @math can be computed in time polynomial in @math , which is related to the length'' of the solution curve (see Definition ). The following examples illustrate that the difference in complexity can be huge.
- Finally, this problem has been widely studied in Numerical Analysis but the point of view is usually different and more focused on practically fast algorithms rather than asymptotically efficient algorithms. Some work @cite @cite @cite @cite suggests that any polynomial time algorithm must have variable order, and that adaptive algorithms are theoretically superior to non-adaptive ones This is in contrast with classical results which state the contrary but under usually unrealistic hypothesis, see @cite . While adaptive algorithms have been widely studied, high-order algorithms are not mainstream because they are expensive in practice. Variable order methods have been used in @cite , @cite , @cite , @cite , @cite and some polynomial time algorithms have been obtained over compact domains or over arbitrary domains but with stronger hypothesis.
- Many tensor based methods for completion have been developed @cite @cite @cite @cite @cite , where @cite and @cite are based on tensor decomposition to predict missing values while @cite @cite @cite minimize nuclear norm of a tensor directly. But all of them just focus on a single tensor data. It is worthwhile to notice that TMac proposed in @cite used a similar optimization method to our method. However, we use the Frobenius norm regularization on factor matrices to control the rank of unfolding of tensors but TMac only justifies the number of column of latent matrices to control the nuclear norm.
- In many different field, multi-data analysis or group analysis was also deeply discussed @cite @cite @cite . But the majority of methods focus on common and distinctive component extraction. In this paper, we do not care latent features in each dataset. Thus we can reduce the complexity of algorithms compared to tensor decomposition based method like @cite @cite . A multi-matrix completion method Convex Collective Matrix Factorization (cCMF) was proposed in @cite . Compare to cCMF, The proposed method in this paper can be considered as an extension of cCMF into tensor problem.
- C. Abbas @cite @cite @cite proved the strict chord property for certain Legendrian knots in tight closed contact 3-manifolds.
- In @cite K. Cieliebak proved that Legendrian spheres in the boundaries of certain subcritical Weinstein domains intersect some characteristic for @math at least twice.
- Let @math be a bounded star-shaped domain with smooth boundary and @math a closed Legendrian submanifold of nonpositive curvature. The last condition means that @math that admits a Riemannian metric of nonpositive sectional curvature. In the recent preprint @cite K. Cieliebak and K. Mohnke proved that @math possesses a Reeb chord of length bounded above by the (toroidal) Lagrangian capacity of @math , see [Corollary 1.12] CM . Using [Corollary 1.3] CM , they deduced that @math admits a Reeb chord of length bounded above by @math , if @math , and @math , i.e., @math is the unit sphere.
- As explained in @cite after Corollary 1.13, it follows that there exists no exact Lagrangian embedding into @math of a closed manifold @math of nonpositive curvature. (Corollary is a stabilized version of this without the nonpositive curvature assumption.)
- P. Biran and K. Cieliebak @cite @cite generalized Seidel's result in various ways. In the case @math , for every @math , Corollary follows from their results. Further references about results on the topology of Lagrangian embeddings are provided in @cite @cite .
- More simulator-based results for hand-held and hands-free mobile phone conversations are available from Haigney @cite , using mean and standard deviation of heart rate, speed and variability of accelerator pedal travel before, during and after the call. Both groups of authors comment on compensatory behaviour of drivers while using a mobile phone. Yet another study in this space, by Rakauskas @cite , also investigated the effects of mobile phone conversations on driving performance using a simulator as well as a similar set of measures, namely accelerator variability, speed variability, average speed, steering offset, mean lateral speed, reaction time, collisions and mental workload. Once again, the conclusion is that the effects of mobile phone conversations while driving are higher workloads and a decreased driving performance. Finally, the study by Drews @cite is the closest to ours in terms of the situation and factors considered, i.e., the effects on driving performance of having passenger and mobile phones conversations, again assessed using a simulator. In this case, lane keeping, mean speed and mean distance were used for the assessment.
- The application of signal-space linear IA for cellular networks were considered in @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite , which used non-iterative schemes, and in @cite @cite @cite @cite , which used iterative schemes. Furthermore, the feasibility of non-iterative schemes was studied in @cite @cite @cite @cite .
- The IA scheme given in @cite , which is called subspace IA, is the first IA scheme developed for cellular networks. The scheme exploits channel decomposibility to align interference into a subspace smaller than the desired signal space and can achieve high total DoF when the number of users is large and the number of cells is small. However, only uplink channels in single-antenna cellular networks were considered in @cite , and the scheme can only be applied when the channel has the special property of decomposibility, which, for example, is not satisfied by channels with independent identically distributed coefficients @cite .
- For general multicell multiuser uplink channels, @cite proposed a new scheme called unstructured IA scheme which appeared to provide a high DoF performance, but for a given uplink channel the feasibility of the scheme cannot be determined until the scheme is actually applied and checked by a numerical test. On the other hand, the IA scheme in @cite aimed to align ICI into a one-dimensional subspace, and because this IA constraint was too stringent to be satisfied, only a fraction of the ICI could be aligned. This problem was then resolved in @cite by consolidating ICI into a multi-dimensional subspace rather than a one-dimensional subspace. By extending the scheme in @cite , which was designed only for three-cell multiuser uplink channels, the scheme in @cite can be applied to a general uplink channel; however, its DoF performance decreases when the size of the uplink channel increases as the scheme can only suppress the interference caused by one interfering user regardless of the size of the uplink channel.
- In this paper, we propose to improve the lookup performance, in form of reduction in the average hop count, by adapting the standard neighbour selection scheme. Although the approach differs from earlier improvements, it is orthogonal and hence compatible with them. The model that we present in this paper extends on a prior model of Kademlia-type systems @cite , which allows a very accurate prediction of the routing overhead. Though theoretical analysis of P2P routing performance is widely studied, traditionally only asymptotic bounds have been derived (e.g. @cite @cite @cite @cite ). The few studies deriving exact formulas commonly only consider the average routing length, special cases such as bijective mapping from identifiers to nodes, and are of limited accuracy when compared to measurements or simulations (e.g. @cite @cite @cite ). In particular, @cite @cite model P2P routing using a Markov chain approach similar to the one suggested in @cite , but are restricted to systems without parallelism.
- Other types of active attacks exist. For instance, the maximal vertex coverage (MVC) attack consists in attacking a few nodes so as to delete as many edges of the network as possible. In this attack, the attacker tries to convince some users to leave the social network in order to reduce the number of residual social ties. Metrics to quantify the impact of MVC attacks have been studied in @cite . MVC is not a privacy attack, though.
- Interested readers could refer to @cite @cite @cite for further reading on privacy-preserving publication of social graphs.
- Few research works are dedicated to study MCARS. @cite proposes a method which consists of building a dynamic user's profile based on time and user's experience. The user's preferences in the user's profile are weighted according to the situation (time, location) and the user's behavior. To model the evolution on the user's preferences according to his temporal situation in different periods (like workday or vacations), the weighted association for the concepts in the user's profile is established for every new experience of the user. The user's activity combined with the user's profile are used together to filter and recommend relevant content.
- Another work @cite describes a MCARS operating on three dimensions of context that complement each other to get highly targeted. First, the MCARS analyses information such as clients' address books to estimate the level of social affinity among the users. Second, it combines social affinity with the spatio-temporal dimensions and the user's history in order to improve the quality of the recommendations.
- In @cite the authors propose a mobile recommender system for people in leisure time. The system predicts the current activity (eating,reading or shopping) from the context (time, location) and the behaviour of the user. The predicted activity combined with preference models of the user, are used to filter and recommend relevant content. To provide relevant advertisements to mobile users, authors in @cite build a profile of each region visited by the user. Statistical techniques are then used to extract information from the visited regions, like "the frequency of visits", "duration and time of typical visits", and the user's profile is built on the basis of questionnaires.
- Very frequently used in reinforcement learning to study exr exp, the multi-armed bandit problem was originally described by @cite . The @math -greedy is one of the most used strategies to solve this problem. It chooses a random document with @math -frequency, and chooses otherwise the document with the highest estimated mean, the estimation being based on the observed rewards. The @math is chosen by the user in the open interval ]0, 1[.
- The first variant of the @math -greedy strategy is what @cite @cite refer to as the @math -beginning strategy. This strategy makes exploration all at once at the beginning. For a given number I of iterations, documents are randomly selected during the @math I first iterations; during the rest (1- @math )I iterations, the document of highest estimated mean is selected. Another variant of the @math -greedy strategy is what @cite calls the @math -decreasing. In this strategy, the document with the highest estimated mean is always selected except when a random document is selected instead with @math frequency, where @math = @math i , @math @math ]0,1] and @math is the index of the current round. Besides @math -decreasing, four other strategies was presented in @cite .
- In contrast to the unguided exploration strategy adopted by @math -greedy, another class of algorithms, known as UCB, use a smarter way to balance exr and exp. We can cite UCB methods in @cite for rewards bounded in [0, 1] and the Price Of Knowledge Expected Reward (POKER) strategy @cite for normally distributed rewards.
- In @cite , assuming the expected reward of a document is linear, they perform recommendation based on contextual information about the users' documents. To maximize the total number of user's clicks, this work proposes the LINUCB algorithm which is computationally efficient if the expected rewards of documents are linear which is not always the case.
- In @cite , the authors propose to solve bandit problem in dynamic environment by combining the UCB with the @math -greedy strategy and they dynamically update the @math exploration value. At each iteration, they run a sampling procedure to select a new @math from a finite set of candidates. The probabilities associated to the candidates are uniformly initialized and updated with the Exponentiated Gradient (EG) @cite . This updating rule increases the probability of a candidate @math if it leads to a user's click. Compared to both @math -beginning and @math -decreasing, this technique gives better results. @cite @cite describe a smart way to balance exr exp, but do not consider the user's situation and its associated risk during the recommendation.
- The second type, termed inherent uncertainty, is related to the stochastic nature of the system, like @cite , who consider models where some states are error states representing a catastrophic result. More recently, @cite developed a policy gradient algorithm for criteria that involves both the expected cost and the variance of the cost, and demonstrated the applicability of the algorithm in a portfolio planning problem. However, this work does not consider the risk of the situations in the exr exp problem.
- A recent work, @cite , treated the risk and proposed the VDBE algorithm to extend @math -greedy by introducing a state-dependent exploration probability, instead of hand-tuning a global parameter. The system makes exploration in situations when the knowledge about the environment is uncertain, which is indicated by fluctuating action values during learning. In contrast, the amount of exploration is reduced as far as the system's knowledge becomes certain, which is indicated by very small or no value differences.
- We improve the extension of UCB with @math -greedy (called here @math -UCB) because it gives the best result in an off-line evaluation done by @cite ; however, our amelioration can be applied to any bandit algorithm.
- In the same line, some of our positive results which make use of averaging algorithms can be shown equivalent to results about stochastic matrix products in the vast existing literature on asymptotic consensus @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . Notably Theorem is similar to the central result in @cite , but we give a different proof much simpler and direct as it requires neither star graphs (called strongly rooted graphs in @cite ) nor Sarymsakov graphs @cite . Moreover our proof yields a significantly better upper bound on the time complexity of averaging algorithms in coordinated network models, namely @math instead of @math in @cite . The statement in Theorem for the partial synchronous case with bounded delays already appears in @cite @cite , but our proof strategy, which consists in a reduction to the case of synchronous nonsplit networks, yields a new and simpler proof.
- In solving @math , the algorithm we provide is based on the Douglas-Rachford proximal splitting method @cite . It falls in a growing body of recent work on applying splitting methods, like the alternating directions method of multipliers (ADMM), to obtain distributed algorithms; such work is perhaps most inspired by @cite .
- With the goal to make the system easily assessable by massive consumers, prior research has also been conducted to develop 2D TI systems (e.g. @cite @cite @cite ) based on commodity cameras and computers. These systems share some similarity with our approach where the foreground objects are segmented from undesired background to allow merging two or multiple participants into the same shared background environment. As a result, participants can experience collaboration capability through natural interaction with each other or with the selected background content (e.g. presentation slides, online photo albums), which does not exist in the traditional video conferencing systems such as Skype. However, the key foreground segmentation techniques adopted are often very rudimentary @cite or computationally intensive @cite and provide a quality-compromised segmentation performance.
- Though recent years have witnessed some important advances in the research field of foreground segmentation from a live video @cite @cite @cite @cite , the technology existing today is still distant from what a practical solution really desires, especially when it is put under a context of live video teleconferencing. Some known issues are small video resolutions e.g. @math , inaccurate foreground segmentation under challenging test conditions, and requiring stereo cameras @cite or additional depth sensors @cite . Recently, Lu et al @cite have developed a more advanced segmentation technique to realize a practical 2D TI system. Without supporting a flexible setup of incorporating a depth camera when available, it does not handle challenging situations well by using only a single webcam (e.g. background foreground with very similar colors or difficult hand gestures). Furthermore, their system does not support multimodality and allows only limited interactions with shared contents, while the current design lacks capabilities to support many concurrent meeting sessions and flexibility in functionality extension in different application contexts.
- Previous work @cite uses a probabilistic topic model to solve the cold start problem in three step. Firstly, they map users into a latent space by features of their related items. Every user could be represented by some topics of features. Then a 'folding-in' algorithm is used to fold new items into a user-feature aspect model. Finally, the probability of a user given a new movie (could be seen as some similarity measure) was calculated in order to make recommendation.
- Another matrix factorization based model is the regression-based latent factor model (RLFM). RLFM simultaneously incorporate user features, item features, into a single modeling framework. Different from @cite , where the latent factors' learning phase and mapping phase are separated, a Gaussian prior with a feature-base regression was added to the latent factors for regularization. RLFM provided a modeling framework through hierarchical models which add more flexibility to the factorization methods.
- In @cite , the collaborative filtering and probabilistic topic modeling were combined through latent factors. Making recommendations is a process of balancing the influence of the content of articles and the libraries of the other users. Techniques used for solving cold start problem falling into the directed graphical models include latent Dirichlet allocation @cite , @cite , probabilistic latent semantic indexing @cite etc. Other technique for solving cold start problem include semi-supervised @cite , decision trees @cite etc.
- One approach to unreliable travel times is to take into account the inherent variability in the travel time distributions and compute robust routing strategies. A commonly used method in this setting is to maximize the probability of reaching the destination within a prespecified time budget. There are two different approaches that have been developed to solve this problem. The first approach is to find the optimal path that maximizes the probability of on-time arrival and is known as the shortest path with on-time arrival reliability (SPOTAR) problem. Nie and Wu @cite proposed a general solution to the problem based on first-order stochastic dominance of paths, but the solution has worst-case exponential running times. They also present a pseudo-polynomial algorithm that gives an approximate solution to the problem and performs well in practice. @cite showed how to solve the problem in @math time when the arc travel time distributions are Gaussian.
- A family of periodic solutions to the hollow vortex problem was constructed by Baker, Saffman and Sheffield in @cite . It corresponds to the family of horizontal Scherk surfaces (see Figure , top left) -- the authors were of course not aware of that relationship. The same solution was derived again by Crowdy and Green in @cite , together with another family of solutions called "staggered vortex streets". The corresponding minimal graphs are periodic and take on two different values on the boundary as in Case II. However, they are asymptotic to half-planes of non-zero slope at infinity, so extending these surfaces by reflection yields complete minimal surfaces which are not embedded.
- Under the additional assumption that @math in @math and @math on @math , solutions to the hollow vortex problem are called exceptional domains. Hauswirth, H 'elein and Pacard studied the problem in @cite and discovered an exceptional domain which, as it turns out, corresponds to the horizontal catenoid. Partial classification results were obtained by Khavinson, Lundberg and Teodorescu in @cite . A complete classification is given in @cite .
- Graph querying techniques have been studied extensively in the field of pattern recognition over nearly four decades @cite . Two popular subgraph isomorphism algorithms were developed by Ullman @cite and @cite . The VF2 algorithm @cite employs a filtering and verification strategy and outperforms the original algorithm by Ullman. Over the past decade, the database community has focused strongly on developing indexing and query optimization techniques to speed up the searching process. A common theme of such approaches is to index vertices based on k-hop neighborhood signatures derived from labels and other properties such as degrees and centrality @cite @cite @cite . Other major areas of work involve exploration of subgraph equivalence classes @cite and search techniques for alternative representations such as similarity search in a multi-dimensional vector space @cite . Apart from neighborhood based signatures, is an important area that focuses on generating different synopses of a graph data set @cite . Development of efficient graph sketching algorithms and their applications into query estimation is expected to gain prominence in the near future.
- Design and analysis of resource allocation algorithms in constrained queueing networks have been of great interest for last four decades across various research communities, where the contention resolution protocols in multiple-access broadcast channels have been studied since the mid-1970s @cite . The broadcast channels induce a special constraint in resources usages among queues (i.e., every queue contend with all other queues), and simple distributed contention resolution protocols gained much attention in the ages. It is the year 1992 that the seminal work by Tassiulas and Ephremides proposed a throughput-optimal algorithm, referred to as MW, for general resource constraints. Its main appeal is that it does require only information on the current queue lengths (and instantaneous service rates) and does not rely on the knowledge of underlying system parameters. However, as we mentioned earlier, the MW algorithm suffers from the high implementation complexity, and a huge array of subsequent research has been made to develop algorithms with high performance guarantees and low-complexity.
- Maximal scheduling or Longest-Queue-First algorithm are low-complexity alternatives to MW, but they achieve only some fraction of the maximal throughput region. Parallel Iterative Matching @cite and iSLIP @cite were shown to be 50 sufficient conditions on the network topology for throughput-optimality. Those conditions were further analyzed to obtain fractional throughput results about a class of wireless networks by @cite and @cite . These algorithms are generally not throughput-optimal and require multiple rounds of message exchanges between nodes.
- Some methods concentrate on rigid body path planning @cite @cite @cite , where the relation between the workspace and the configuration space is simpler. The extension of some of these methods to articulated robots is possible @cite , but local minima can appear in the presence of simultaneous contacts.
- To detect narrow passages, the bridge-test @cite focuses on the generation of pairs of samples in collision where the central point between these samples is collision-free. Even if the configurations could be generated easily, this test implicitly assumes that the linear interpolation between two samples is also a valid configuration, which is not the case when the configuration space is an arbitrary manifold. The assumption is also taken by the approaches that use linear interpolation to displace the random samples toward the medial axis of the free configuration space @cite . Moreover, approaches that use linear dimensionality reduction of the nodes already in the RRT to bias the sampling @cite are also affected by this issue.
- The information of the collisions detected while extending an Rapidly-exploring Random Tree (RRT) can be leveraged to bias the sampling @cite . For instance, using this information the dynamic domain of an RRT node can be defined, i.e, the part of the configuration space where it is worth to extend the tree from that node @cite @cite . As we will see, this approach can be adapted to the kinematically-constrained case defining the dynamic domain in the ambient space rather than in the configuration space.
- The idea of using some sort of degree binning, orienting edges, or thresholding for finding and enumerating triangles has been used in many results. Chiba and Nishizeki @cite give bounds for a sequential version of using the degeneracy of a graph. This does not give bounds for , although their algorithm is similar in spirit. Alon, Yuster, and Zwick @cite find triangles in @math using degree thresholding and matrix multiplication ideas from Itai and Rodeh @cite . Chrobak and Eppstien @cite use acyclic orientations for linear time triangle enumeration in planar graphs. Vassilevska Williams and Williams @cite show that fast algorithms for weighted triangle enumeration leads to remarkable consequences, like faster all-pairs shortest paths. In the work most closely to ours, Latapy @cite discusses various triangle finding algorithms, and also focuses on power-law graphs. He shows the trivial bound of @math when the power law exponent is @math . Essentially, the maximum degree is @math and that directly gives a bound on the number of @math s.
- has received attention from various experimental studies. Schank and Wagner @cite perform an experimental study of many algorithms, including a sequential version of which they show to be quite efficient. Cohen @cite specifically describes in the context of Map-Reduce. Suri and Vassilvitskii @cite do many experiments on real graphs in Map-Reduce and show major speedups (a few orders of magnitude) for over the trivial enumeration. Tsourakakis @cite gives a good survey of various methods used in practice for triangle counting and estimation.
- Explicit triangle enumerations have been used for various applications on large graphs. Fudos and Hoffman @cite use triangle enumeration for a graph-based approach for solving systems of geometric constraints. @cite touch every triangle as part of their community detection algorithm for large graphs.
- Configuration models for generating random graphs with given degree sequences have a long history. Bender and Canfield @cite study this model for counting graphs with a given degree sequence. Wormald @cite looks at the connectivity of these graphs. Molloy and Reed @cite @cite study various properties like the largest connected component of this graph distribution. Physicists studying complex networks have also paid attention to this model @cite . Britton, Deijfen, and Martin-L " o f @cite show that the simple graph generated by the ECM asymptotically matches the desired degree sequence. Aiello, Chung, and Lu @cite give a model for power-law graphs, where edge @math are independent inserted with probability @math . This was studied for more general degree sequences in subsequent work by Chung, Lu, and Vu @cite @cite . Mihail and Papadimitriou @cite independently discuss this model. Most of this work focused on eigenvalues and average distances in these graphs. Newman @cite gives an excellent survey of these models, their similarities, and applications.
- The work in @cite studied the large-scale multi-label leanring problems, and proposed an efficient approach M3L. In addition, multi-label learning with incomplete label assignment has also been studied on small moderate-size datasets @cite @cite @cite . The work in @cite proposed an approach to infer missing labels in multi-label learning under transductive settings.
- As of today, there is still no single solution available that is complaint with the definitions of privacy and anonymity introduced in Section 1. Nowadays, every communication done through the Internet that aims to be private and anonymous requires the use of cryptography and hard-trace routing techniques. Proxies and TOR are among the most used anonymity technologies @cite and public-key cryptography is the most popular method aiming for privacy. Nevertheless, it is still possible to perform different attacks over these techniques, leading always to some result that invalidate the required properties for privacy and anonymity mentioned in Section 1. We introduce some of the possible improvements that current lines of research are proposing in order to enforce better privacy and anonymity.
- More in line with our work are greedy approaches. These algorithms reconstruct the signal by identifying elements of the support iteratively. Once an accurate support set is located, a simple least-squares problem recovers the signal accurately. Greedy algorithms like Orthogonal Matching Pursuit (OMP) @cite and Regularized OMP (ROMP) @cite offer a much faster runtime than the convex relaxation approaches but lack comparable strong recovery guarantees. Recent work on greedy methods like Compressive Sampling Matching Pursuit (CoSaMP) and Iterative Hard Thresholding (IHT) offer both the advantage of a fast runtime and essentially the same recovery guarantees as (e.g. @cite @cite @cite @cite ). However, these algorithms are only applied for problems in compressed sensing where the least square loss is used to measure the discrepancy. There certainly exists many loss functions that are commonly used in statistical machine learning and do not exhibit quadratic structure such as log-likelihood loss. Therefore, it is necessary to develop efficient algorithms to solve ).
- There are several methods proposed to solve special instances of ). @cite and @cite propose the forward selection method for sparse vector and low-rank matrix recovery. The method selects each nonzero entry or each rank-one matrix in an iterative fashion. @cite generalizes this algorithm to the more general dictionary @math . @cite proposes the forward-backward method in which an atom can be added or removed from the set, depending on how much it contributes to decrease the loss function. @cite extends this algorithms beyond the quadratic loss studied in @cite . @cite extends the CoSaMP algorithm for a more general loss function. Very recently, @cite further generalizes CoSaMP and proposes the Gradient Matching Pursuit (GradMP) algorithm to solve ). This is perhaps the first greedy algorithm for ) - the very general form of sparse recovery. They show that under a restricted convexity assumption of the objective function, the algorithm linearly converges to the optimal solution. This desirable property is also possessed by CoSaMP. We note that there are other algorithms having also been extended to the setting of sparsity in arbitrary @math but only limited to the quadratic loss setting, see e.g. @cite @cite @cite @cite .
- We outline the GradMP method here, since it will be used as motivation for the work we propose. GradMP @cite is a generalization of the CoSaMP @cite that solves a wider class of sparse reconstruction problems. Like OMP, these methods consist of four main steps: i) form a signal proxy, ii) select a set of large entries of the proxy, iii) use those as the support estimation and estimate the signal via least-squares, and iv) prune the estimation and repeat. Methods like OMP and CoSaMP use the proxy @math ; more general methods like GradMP use the gradient @math (see @cite for details). The analysis of GradMP depends on the restricted strong convexity and restricted strong smoothness properties as in Definitions and below, the first of which is motivated by a similar property introduced in @cite . Under these assumptions, the authors prove linear convergence to the noise floor.
- The IHT, another algorithm that motivates our work, is a simple method that begins with an estimation @math and computes the next estimation using the recursion @math where @math is the thresholding operator that sets all but the largest (in magnitude) @math coefficients of its argument to zero. Blumensath and Davies @cite prove that under the RIP, IHT provides a recovery bound comparable to . @cite extends IHT to the matrix recovery. Very recently, @cite proposes the Gradient Hard Thresholding Pursuit (GraHTP), an extension of IHT to solve a special vector case of ).
- Methods for stochastic convex optimization have been developed in a very related but somewhat independent large body of work. We discuss only a few here which motivated our work, and refer the reader to e.g. @cite @cite for a more complete survey. Stochastic Gradient Descent (SGD) aims to minimize a convex objective function using unbiased stochastic gradient estimates, typically of the form @math where @math is chosen stochastically. For the optimization ) with no constraint, this can be summarized concisely by the update rule @math for some step size @math . For smooth objective functions @math , classical results demonstrate a @math convergence rate with respect to the objective difference @math . In the strongly convex case, Bach and Moulines @cite improve this convergence to a linear rate, depending on the average squared condition number of the system. Recently, Needell et. al. draw on connections to the Kaczmarz method (see @cite @cite and references therein), and improve this to a linear dependence on the uniform condition number @cite . Another line of work is the Stochastic Coordinate Descent (SCD) beginning with the work of @cite . Extension to minimization of composite functions in ) is described in @cite .
- In this paper, we exploit ideas from IHT @cite , CoSaMP @cite and GradMP @cite as well as the recent results in stochastic optimization @cite @cite , and propose two new algorithms to solve ). The IHT and CoSaMP algorithms have been remarkably popular in the signal processing community due to their simplicity and computational efficiency in recovering sparse signals from incomplete linear measurements. However, these algorithms are mostly used to solve problems in which the objective function is quadratic and it would be beneficial to extend the algorithmic ideas to the more general objective function.
- Our algorithm can be viewed as an adaptation of the Best Start two-phase method for global optimization @cite to our context.
- The falsification problem can also be viewed as a boundary value problem which is a classical topic in numerical mathematics @cite . However, classical numerical methods assume a fixed final time, whereas we search for error trajectories of arbitrary length. Moreover, classical methods for boundary values problems are restricted to purely continuous systems and the formulation of boundary conditions as equalities. Note that, unlike the continuous case, the trajectory of a hybrid system can reach a point @math for which @math holds every reaching a point @math for which @math holds.
- Zuthsi and co-authors @cite present a method for falsification of hybrid systems that also uses multiple shooting based local search. However, the method assumes a given upper bound on the length of the error trajectory the method searches for. Moreover, their local search method always follows a given sequence of modes and transitions. They propose to search for such a sequence using tools that compute abstractions of hybrid systems, or by random search. The form of the used trajectory segments is more restricted than in our method since trajectory segments always stay in one mode, and end in the guard leading to another mode.
- Abbas @cite show how to use local search for falsification of hybrid systems with affine dynamics. They propose to start the method from the result of global search algorithms @cite .
- The usage of abstractions for guiding local search for error trajectories has been proposed earlier @cite , in combination with the usage of derivative-free algorithms for local search.
- There is a growing body of work that associates images and sentences. Some approaches focus on describing the contents of images, formulated either as a task of mapping images to a fixed set of sentences written by people @cite @cite , or as a task of automatically generating novel captions @cite @cite @cite @cite @cite @cite . More closely related to our approach are methods that naturally allow bi-drectional mapping between the two modalities. Socher and Fei-Fei @cite and @cite use Kernel Canonical Correlation Analysis to align images and sentences, but their method is not easily scalable since it relies on computing kernels quadratic in number of images and sentences. @cite learn a common meaning space, but their method is limited to representing both images and sentences with a single triplet of (object, action, scene). @cite use a Conditional Random Field to reason about complex relationships of cartoon scenes and their natural language descriptions.
- Our approach falls into a general category of learning from multi-modal data. Several probabilistic models for representing joint multimodal probability distributions over images and sentences have been developed, using Deep Boltzmann Machines @cite , log-bilinear models @cite , and topic models @cite @cite . @cite described an autoencoder that learns audio-video representations through a shared bottleneck layer. More closely related to our task and approach is work of @cite , who introduced a visual semantic embedding model that learns to map images and words to a common semantic embedding with a ranking cost. Adopting a similar approach, @cite described a Dependency Tree Recursive Neural Network that puts entire sentences into correspondence with visual data. However, these methods reason about the image only on global level, using a single, fixed-sized representation from a top layer of a Convolutional Neural Network as a description for the entire image, whereas our model reasons explicitly about objects that make up a complex scene.
- Our model is a neural network that is connected to image pixels on one side and raw 1-of-k word representations on the other. There have been multiple approaches for learning neural representations in these data domains. In Computer Vision, Convolutional Neural Networks (CNNs) @cite have recently been shown to learn powerful image representations that support state of the art image classification @cite @cite @cite and object detection @cite @cite . In language domain, several neural network models have been proposed to learn word n-gram representations @cite @cite @cite @cite @cite @cite , sentence representations @cite and paragraph document representations @cite .
- In recent years, precoding techniques have been widely applied to multi-user MIMO systems. Precoding techniques rely on pre-processing the transmitted data with the help of channel state information (CSI). Different kinds of linear precoding techniques such as Zero-forcing (ZF) precoding, Minimum Mean Square Error (MMSE) precoding and Block Diagonalization (BD) precoding are introduced in @cite - @cite , respectively. Some non-linear precoding techniques like Tomlinson-Harashima precoding (THP) @cite , Vector Perturbation (VP) precoding @cite have also been reported. The precoding techniques can also contribute to the performance of relay systems @cite These precoding techniques have been proven effective in improving the sum-rate and the bit-error rate (BER) performances. However, precoding strategies require CSI at the transmit side, whereas in reality CSI is available to the users of interest but is rarely available to the eavesdroppers. In most situations, a designer only has access to imperfect CSI for the users of interest. Dealing with imperfect CSI and the absence of CSI of the eavesdropper is a major challenge. Moreover, a technique that is particularly effective for improving the secrecy rate of physical-layer security is the introduction of artificial noise at the transmitter @cite .
- Since the power consumption of the routers is as significant as the processor core, the thermal constraint should also be addressed in the routing algorithm design. There have been a lot of works on NoC routing algorithms for various purposes including low power routing @cite , fault-tolerant routing @cite and congestion avoidance routing to improve latency @cite . However, there are only a few works @cite @cite @cite taking temperature issue into account.
- One may want to combine minimax backups or searches without using an evaluation function. The prime example is MCTS-Solver @cite , which backpropagates proven wins and losses as extra information in MCTS. When a node is proven to be a win or a loss, it no longer needs to be searched. This domain-independent modification greatly enhances MCTS with negligible overhead. Score-bounded MCTS extends this idea to games with multiple outcomes, leading to @math -style pruning in the tree @cite . One can use shallow-depth minimax searches in the tree to initialize nodes during expansion, enhance the playout, or to help MCTS-Solver in backpropagation @cite .
- Another work which is close to ours is the CrowdGrader mechanism for peer evaluation from UC Santa Cruz @cite . CrowdGrader lets students submit and collaboratively grade solutions to homework assignments. The tool permitted both ranking and grading of homework. However, de Alfaro and Shavlovsky found that students much preferred to grade than to rank. They expressed uneasiness in ranking their peers, perceiving ranking as a blunt tool compared to grading. At the heart of CrowdGrader is the Vancouver algorithm for combining peer grades. In each iteration, this algorithm computes a consensus estimate of the grade of each submission, weighing grades according to the (estimated) accuracy of each student; the consensus estimates are then used to update the estimated accuracy of the students. The update attempts to minimize variance.
- There are a number of closely related problems to ours in the social choice literature. For example, Holzman and Moulin have studied a related problem in which a set of agents wish to select one amongst them to receive a prize @cite . A fundamental assumption of this work is that nominations are impartial: your message never influences whether you win the prize. In our setting, such an assumption is less appropriate. We want your evaluation of the work of another agent to influence your evaluation. There are several reasons behind this change. First, your ability to evaluate the work of other agents measures in part your command of the subject being examined. Second, you will be incentivized to grade accurately by a better final grade. If your message cannot influence your evaluation, then you have no incentive to provide good evaluations. For this reason, even if we extend the sort of methods proposed by Holzman and Moulin to the task of ranking, the starting assumptions are very different.
- Another related problem is selection from the selectors'' @cite . The goal here is to select a subset of @math agents from a group (e.g. to select a subcommittee). The problem of awarding a prize from a group of peers can be seen as the special case of @math . As approval voting is not impartial, Alon, Fischer, Procaccia and Tennenholtz look for impartial rules that approximate approval voting (that is, guarantee that the total approval scores of the @math winners are within a fixed fraction of the optimal answer). Again, a difference with this work is that we are not trying to achieve or approximate impartiality.
- A closely related problem is the division of cash between a group of partners @cite . Each partner cares selfishly about their share but is supposed to be disinterested about the distribution of the money that he or she does not get. Partners rate the relative contributions of the other partners. With four or more partners, there exist symmetric and impartial division rules. By comparison, whilst our PeerRank rule is symmetric, it is not designed to be impartial. The grades assigned by an agent can definitely influence their final grade.
- Another model of predicting election outcomes is that of @cite . They consider a situation where each voter is undecided regarding several possible votes. That is, for each voter we are given several possible preference orders and a probability distribution over these votes. The question is, what is the probability that a designated candidate wins.
- From a technical standpoint, our research continues the line of work on the complexity of control. This line of work was initiated by Bartholdi, Tovey, and Trick @cite , and then continued by Hemaspaandra, Hemaspaandra, and Rothe (who introduced the destructive cases), by (who considered multiwinner rules and who generalized the idea of the constructive and destructive cases), by Faliszewski, Hemaspaandra, and Hemaspaandra (who introduced multimode model of control), by Faliszewski, Hemaspaandra, and Hemaspaandra @cite (who were first to consider control for weighted elections), by Rothe and Schend @cite (who initiated the empirical study of the complexity of control problems), and by many other researchers, who provided results for specific voting rules and who introduced various other novel means of studying control problems (see, e.g., the following papers @cite @cite @cite @cite @cite @cite @cite ; we also point the readers to the survey @cite ).
- Single-peaked elections were studied for a long time in social choice literature, but they gained popularity in the computational social choice world fairly recently, mostly due to the papers of Walsh @cite and Conitzer @cite . Then, @cite and, later, @cite studied the complexity of control problems for single-peaked elections. Recently, Faliszewski, Hemaspaandra, and Hemaspaandra @cite complemented this line of work by studying nearly single-peaked profiles.
- Link-tracing based sampling methods in the context of networks are somewhat distinct from classical sampling methods @cite . Random walk (RW) @cite is one of the most important and widely used sampling methods in different kind of network contexts such as uniformly sampling Web pages from the Internet @cite , content density in Peer-to-Peer networks @cite , degree distributions of the Facebook social graph @cite , and collecting data from information diffusion networks @cite @cite . A classic RW, samples a graph by moving from a node, @math , to a neighboring node, @math , through an outgoing link, chosen uniformly at random from the neighbors of @math . By this process links and nodes are sampled. In any given connected and non-bipartite undirected graph, the classic RW is biased towards node with higher degree.
- In general, the probability of selecting the next node determines the probability that nodes are visited in sampling procedure. Metropolis-Hastings technique @cite can be used to modify the probabilities of the node selection in random walk in order to have the uniform stationary distribution for visiting each node. This technique is a general Markov Chain Monte Carlo (MCMC) method @cite for sampling from probability distributions based on constructing a Markov Chain that has the desired distribution as its stationary distribution. This approach, known as Metropolis-Hastings Random Walk (MHRW), has been applied to Peer-to-Peer networks @cite , Facebook @cite , and Twitter @cite .
- In @cite , it is shown that when the sample size is relatively small, the RDS method may generate relatively large biases and errors if the studied networks are directed, indicating that estimates from previous RDS studies should be interpreted and generalized with caution. In @cite , a random walk sampling algorithm with jumps, called Directed Unbiased Random Walk (DURW), is proposed that achieves asymptotically unbiased estimates of the outdegree distribution of a directed graph. The authors construct an undirected graph using the nodes that are selected by the random walker on the directed graph. The undirected graph is built to allow the walker to traverse known outgoing links backwards, and guarantees that the probability of sampling a node can be approximated, even though incoming links are not observed. However, this method is based on the assumption that nodes can be sampled uniformly at random from the original graph (i.e., there is some means of obtaining a list of nodes in the network) which is not always feasible. For example, while it is feasible for Wikipedia and Twitter, it is not feasible for the WWW graph.
- Numerous real and successful mobile applications have emerged in recent times such as WAYZ http: www.wayz.com for real-time traffic navigation information and Wazer2 https: www.wazer2.co.il for real-time, location-based citizen journalism, context-aware open-mobile miner (CAROMM) @cite among others. Mobile applications @cite @cite thrive on the data obtained from heterogeneous sets of smart phones owned and operated by humans. Until recently mobile sensing application such as activity recognition (, where people's activity (e.g. walking, talking, sitting) is classified and monitored, required specialised mobile devices @cite @cite . This has significantly changed with advent of smartphones equipped with on-board sensing capabilities. More recently, research efforts have focused on development of activity recognition, context-aware @cite and data mining models for smartphones @cite @cite @cite that leverage on smartphone's processing and on-board sensing capabilities.
- In recent years, studying Twitter has been the main focus for researchers in context of real world events. @cite showed a big overlap between real-world news headlines and Twitter's trending topics. Authors of this work crawled the entire Twitter network, and showed that the majority (over 85
- Twitter has been used widely during emergency situations, such as wildfires @cite , hurricanes @cite , floods @cite and earthquakes @cite @cite @cite . Journalists have hailed the immediacy of the service which allowed to report breaking news quickly - in many cases, more rapidly than most mainstream media outlets'' @cite . @cite explored the potential of the real-time nature of Twitter and proposed an algorithm to detect occurrence of earthquakes by simply monitoring a stream of tweets in real-time. Here, the authors took advantage of the fact that users tweet about events like earthquakes as soon as they take place in the real world, and were able to detect 96
- There are methods @cite @cite that partition the full data into several nodes, obtain a solution on each partition and combine them centrally. This ensemble solution is usually very sub-optimal. Some methods @cite @cite @cite @cite @cite do further processing of those solutions to come closer to full optimality, but this requires heavy communication of data, e.g., passing support vector data between nodes. There are methods that directly parallelize well-known sequential methods such as SMO and SVMlight @cite @cite , but these involve communication costs over a large number of iterations.
- P-packsvm @cite uses primal stochastic gradient descent (SGD) applied in the kernel feature space. Training examples are partitioned into various nodes. The most expensive computation of an SGD iteration is the calculation of the output associated with a given example. This is done by passing the example to all nodes, locally computing the sub-outputs associated with the nodes and then using MPI AllReduce operation to get the summed output to all nodes. A packing strategy is used to do @math iterations together; since @math effort is involved, @math is kept small, say 100. P-packsvm is shown to work well on an MPI environment. On other distributed settings with communication overheads it can become slow due to the large number ( @math ) of iterations.
- There are methods that use a low rank approximation of the kernel. @cite and Woodsend & Gonzio @cite use low rank Cholesky factorization of the kernel matrix. There is a large literature based on using Nystr "om method @cite @cite @cite @cite @cite @cite @cite @cite . While the methods in this class are useful for getting decent sub-optimal performance, they are insufficient if we aim to achieve a performance matching the true kernel machine.
- The main drawback of our attack is that users are not allowed to walk around while entering the PINs because the data is only exploitable for one specific environment. Hence, the data cannot be reused for multiple attacks as in case of Aviv al @cite . Though, their results indicate a rather low success rate of 20 The work of Simon and Anderson @cite additionally requires the permission which potentially gains the user's suspicion. In addition, their attack must deal with the problem of audio-visual feedback, , the shutter sound or the LED, while capturing the required data. Compared to their work we do not restrict our study to specific input methods as long as the user holds the device while operating it. Furthermore, they need to transfer image data to the server, which cannot be represented as compact as sensor values.
- Recent years have seen an explosion of research work analyzing social media (e.g., most prominently the micro-blog Twitter) and the connection between traditional media and this new form of reporting. Twitter studies focus on topics such as detecting political leaning from tweets @cite @cite , sentiment and opinion mining @cite @cite @cite , summarising sporting, economic and other events using Twitter @cite @cite @cite ; analysing news spread through the network @cite @cite ; content curation @cite @cite ; user influence and authority @cite @cite ; detecting breaking news stories from the massive tweet stream, potentially ahead of traditional media reporting @cite @cite . Among the diverse investigations of Twitter data, two categories are most relevant to this paper, namely, research that has focussed on hashtag recommendation or retrieval and studies considering the mapping between news articles and microblogs.
- Laskar and Wallis in @cite have obtained some results about the domination number of the line graph of @math , motivated by combinatorial chessboard problems. However, the domination number of @math itself was not considered by them. The recent thesis of H ' e ger @cite is probably closeset in spirit to the present work: H ' e ger constructs dominating sets of a particular form in projective planes.
- The Common due-date problem has been studied extensively during the last @math years with several variants and special cases @cite @cite . In @math , Kanet presented an O( @math ) algorithm for minimizing the total absolute deviation of the completion of jobs from the due date for the single machine, @math being the number of jobs @cite . Panwalkar considered the problem of common due-date assignment to minimize the total penalty for one machine @cite . The objective of the problem was to determine the optimum value for the due-date and the optimal job sequence to minimize the penalty function, where the penalty function also depends on the due-date along with earliness and tardiness. An algorithm of O( @math ) complexity was presented but the special problem considered by them consisted of symmetric costs for all the jobs @cite @cite .
- Cheng again considered the same problem with slight variations and presented a linear programming formulation @cite . In @math Cheng and Kahlbacher and Hall studied the CDD problem extensively, presenting some useful properties for the general case @cite @cite . A pseudo polynomial algorithm of O( @math ) (where, @math is the common due-date) complexity was presented by Hoogeveen and Van de Velde for the restrictive case with one machine when the earliness and tardiness penalty weights are symmetric for all the jobs @cite . In @math Hall studied the unweighted earliness and tardiness problem and presented a dynamic programming algorithm @cite . Besides these earlier works, there has been some research on heuristic algorithms for the general common due date problem with asymmetric penalty costs. James presented a tabu search algorithm for the general case of the problem in @math @cite .
- More recently in @math , Feldmann and Biskup approached the problem using metaheuristic algorithms namely simulated annealing (SA) and threshold accepting and presented the results for benchmark instances up to @math jobs on a single machine @cite @cite . Another variant of the problem was studied by Toksari and G "uner in @math , where they considered the common due date problem on parallel machines under the effects of time dependence and deterioration @cite . Ronconi and Kawamura proposed a branch and bound algorithm in @math for the general case of the CDD and gave optimal results for small benchmark instances @cite . In @math , Rebai proposed metaheuristic and exact approaches for the common due date problem to schedule preventive maintenance tasks @cite .
- In @math , Banisadr studied the single-machine scheduling problem for the case that each job is considered to have linear earliness and quadratic tardiness penalties with no machine idle time. They proposed a hybrid approach for the problem based upon evolutionary algorithm concepts @cite . Yang investigated the single-machine multiple common due dates assignment and scheduling problems in which the processing time of any job depends on its position in a job sequence and its resource allocation. They proposed a polynomial algorithm to minimize the total penalty function containing earliness, tardiness, due date, and resource consumption costs @cite .
- This chapter is an extension of a research paper presented by the same authors in @cite . We extend our approach for a dynamic case of the problem and for non-identical parallel machines. Useful examples for both the single and parallel machines case are presented.
- Our contributions come in the context of an extensive literature examining social media voting systems. One major research direction is concerned with predicting the helpfulness ratings of product reviews starting from textual and social factors @cite @cite @cite @cite @cite @cite and understanding the underlaying social dynamics @cite @cite @cite @cite . The mechanisms driving user voting behavior and the related community effects have been studied in other contexts, such as Q &A sites @cite , Wikipedia @cite @cite @cite , YouTube @cite , social news aggregation sites @cite @cite @cite and online multiplayer games @cite . Our work adds an important dimension to this general line of research, by providing a framework for analyzing the effects votes have on the author of the evaluated content.
- The setting considered in this paper, that of comments on news sites and blogs, has also been used to study other social phenomena such as controversy @cite , political polarization @cite @cite , and community formation @cite @cite . News commenting systems have also been analyzed from a community design perspective @cite @cite @cite , including a particular focus on understanding what types of articles are likely to attract a large volume of user comments @cite @cite . In contrast, our analysis focuses on the effects of voting on the behavior of the author whose content is being evaluated.
- Our findings here reveal that negative feedback does not lead to a decrease of undesired user behavior, but rather attenuates it. Given the difficulty of moderating undesired user behavior, it is worth pointing out that anti-social behavior in social media systems is a growing concern @cite , as emphasized by work on review spamming @cite @cite @cite , trolling @cite , social deviance @cite and online harassment @cite .
- Another classical application field of integrity constraints, besides SQO, is cooperative query answering @cite , where however the constraints are used only as long as they are valid.
- Besides @cite , there are also several works that automatically derive and exploit soft (also called dynamic) constraints for SQO @cite @cite @cite , but this semantic knowledge is used only as long as it is true, and then either updated or discarded, so that the information inferred from the database by using it is correct.
- For instance, @cite introduces the model of belief databases, where the tuples of a (relational) database are annotated by users' beliefs, which express their agreement or disagreement on the reliability of the data. Beliefs do not measure uncertainty of knowledge, but provide a qualitative evaluation of the presence of conflicts in the information contained in the database: thus, they are taken into account for query answering, but they are not exploited to optimize a query or to provide approximate results.
- Something more about MAS. How that the 2-apx work? Why does not generalize to RMAS? The intuitive argument is that for MAS the optimum is always at least @math . Our interest in RMAS is motivated by the following problem (VP): we are given an undirected (multi-) graph @math , with positive edge budgets @math . Our goal is to assign a non-negative price @math to each node @math so as to maximize the sum @math over the edges @math such that @math . @cite proved that VP is @math -hard to approximate (under UGC) via a reduction from a special case of RMAS. Their reduction exploits instances of RMAS where labels are non-negative, each @math contains @math , and @math for any distinct @math . Note that such instances still generalize MAS. I meant more details about the reduction to VP. Why do you want to stress the special case of RMAS? The reductions are always special in some sense
- In recent times, a large body of work is being concentrated on Energy Harvesting Wireless Sensor Nodes (EHWSN). Several energy harvesting sources like solar, thermo, acoustic, wind, RF and wave for driving low power embedded devices are discussed in @cite . Many energy harvesting WSNs have been implemented in past like Trio @cite , AmbiMax @cite , Prometheus @cite . However, literature on Energy Harvesting systems is mostly limited to single node networks or extensive simulation study. Many of the work focus on the system building, efficiency and viability of energy harvesting mechanisms. In @cite authors shows improvement in lifetime where a supercapacitor and battery are charged together from a solar panel by adjusting their duty cycles. Recently Power management and data rate maximization in EHWSN was discussed in @cite . The authors analytically find the required amount of energy to be harvested to ensure energy neutral operation in each slot.
- Accurate prediction of the energy is an important factor for energy neutral operations. Time series based energy predictions are commonly used for predicting solar energy. Exponential Weighted Moving Average (EWMA) is commonly applied for future energy predictions @cite , @cite and assigns weights to data in the time series, assigning lower weights to older data and giving importance to more recently acquired data. Holt-Winters (HW) time series is widely used in stock markets trend analysis, production planning, healthcare decisions on staffing & purchasing, demand forecasting @cite and for predicting share prices where forecast value takes trend and seasonal variation into account @cite , @cite . In @cite a three state markov chain model is used to predict the solar energy source and their transition probabilities are restricted for day time only. Energy samples from a simulation model are also generated from the model.
- At the data link layer, while there are several MAC protocols been designed for WSN, they are not optimized for energy harvesting sensor nodes. However there are quite a few MAC protocols designed for EHS, the work in @cite proposes dynamically adapting the duty cycle of a node by observing deviations in energy input from an estimated model to ensure energy-neutral operation. In @cite the authors propose to use adaptive control theory to formulate a linear-quadratic optimal tracking problem for maximizing task performance and minimal duty cycle variations. In @cite the authors present analytical models and performance metrics for different MAC protocols. The work however is limited to simulation studies and over single-hop network. In @cite , performance of various sleep and wakeup strategies based on channel state, battery state and environmental factors are analysed. The authors propose a optimal sleep wakeup strategy using game theory approach which provides trade-off between packet dropping and blocking probabilities.
- Several optimization techniques have been discussed in literature for battery driven multihop WSN's in the past where the objective is to maximize the network lifetime by minimizing energy consumption @cite , @cite . With a view on utilizing the harvested energy in an optimized manner, adapting the performance of an application while respecting the limited and time-varying amount of available power in EHWSN is discussed in @cite . A formal model that actively adapts application parameters such as rate of sensing'' is used to optimize the performance. The simulation study does not consider network related parameters or network wide energy levels. A Lazy Scheduling Algorithm'' is proposed and tested for its effectiveness by assigning several power values to the system in @cite . Task admittance and future energy prediction is carried out with energy variability characterization curves generated by modeling the power source.
- If the condition () holds and can be cheaply computed, then problem ) can be solved by iteratively solving a series of problem ) @cite . Such an updating procedure is the same as the ISTA algorithm @cite , which is originally for convex optimization. It can be proved that any accumulation point of @math is a stationary point of problem ). If @math and @math are convex, the Fast ISTA algorithm (FISTA) @cite converges to the globally optimal solution with a convergence rate @math ( @math is the iteration number). But for nonconvex optimization, it is usually very difficult to get the globally optimal solution to problem ). Sometimes, it is also not easy even if @math is convex.
- The multi-stage algorithm in @cite solves problem ) by solving a series of convex problem. However, solving such a convex problem requires other iterative solvers which is not efficient. It also fails when @math is nonconvex.
- Another related work is @cite which aims to solve In each iteration, @math is efficiently updated by solving a series of problem But their solver is only for problem ) which is a special case of ). The convergence proofs also depend on the special property of the @math -norm, thus is not general.
- Furthermore, previous iterative algorithms can only solve the problem with only one variable. They cannot be naively generalized to solve multi-variable problems. However, there are many problems involving two or more variables, e.g. stable robust principle component analysis @cite and robust multi-task feature learning @cite . So it is desirable to extend the iteratively reweighted algorithms for the multi-variable case.
- It is striking that, to date, all of the work in compressive classification has focused on classes of low dimension. This is perhaps an artifact of the mindset of compressed sensing, in which the projection preserves all information on coordinate planes of sufficiently small dimension. However, classification should not require nearly as much information as signal reconstruction does, and so we expect to be able to compressively classify into classes of full dimension; indeed, we allow two points in a common class to be mapped to the same compressive measurement, as this will not affect the classification. A Boolean version of this idea is studied in @cite , which considers both random and optimality constructed projections. In the continuous setting, the closest existing work is that of Dasgupta @cite @cite , which uses random projections to learn a mixture of Gaussians. In particular, Dasgupta shows that sufficiently separated Gaussians stay separated after random projection. In the next section, we prove a similar result about ellipsoids, but with a sharper notion of separation.
- Work on the fingerprinting game described above started in the late 90s, and lower bounds on the code length were established of the order @math @cite , until in 2003 Tardos @cite proved a lower bound of the order @math and described a scheme with @math , showing this bound is tight. Since the leading constants of the upper and lower bounds did not match, later work on fingerprinting focused on finding the optimal leading constant.
- Based on channel capacities, Amiri and Tardos @cite and Huang and Moulin @cite independently derived the optimal leading constant to be @math (i.e. an asymptotic code length of @math ) and many improvements to Tardos's original scheme were made @cite @cite @cite @cite to reduce the leading constant from @math to @math . Recently it was shown that with Tardos' original score function' one cannot achieve capacity @cite , which lead to the study of different score functions. Based on a result of Abbe and Zheng @cite , Meerwald and Furon @cite noted that a score function designed against the worst-case attack achieves capacity against arbitrary attacks. This also lead to a proposal for a capacity-achieving score function in @cite , which achieves the lower bound on the leading constant of @math .
- Within applied probability, the rich history of work on the multi-action restless bandits problem, e.g., @cite @cite @cite , has been summarised by @cite . See the work of Glazebrook @cite @cite for some of the present-best results. The replacement of a single scalar of feedback per arm played has been suggested @cite @cite @cite in the bandits literature, often in connection to revealing the outcome of further arms as well. We are not aware of any bi-level extensions, e.g., seeing the problem from the point of view of the owner of the bandit.
- Within game theory, the social cost is the metric of a number of studies @cite @cite @cite @cite , which show that, even when agents have full information, a natural equilibrium outcome can incur much higher total congestion than a socially optimal outcome. This is known as the price of anarchy. Particularly interesting are studies of the Nash equilibria in connection with ignorance @cite @cite @cite , often concerning the number of players @cite @cite @cite , failures of agents @cite , failures of resources @cite @cite , or stratified and risk-averse populations @cite @cite . Indeed, our work can be seen as showing the benefits of ignorance to a stratified and risk-averse population, albeit over the long run. We can hence describe the attractor, whose existence is often moot in the studies of Nash equilibria. See @cite for further well-developed arguments why considering the fixed-points of a dynamical system is preferable to the study of the Nash equilibrium.
- In the transportation literature, Daganzo and Sheffi @cite have introduced the concept of stochastic user equilibrium, where users have considerable amounts of information, and perhaps surprising analytical powers. Subsequently, a number of variants have been proposed, e.g., @cite consider robust variants and @cite consider the stochastic user equilibrium with distributional uncertainty over the travel times. @cite consider a stratified and risk-averse population, but only as much as a link failure is concerned. There are a number of other notions of stability, perhaps closer to our notion, inasmuch they capture the repeated nature of the problem. For example, @cite introduces the notion of equilibrium as the limit of the congestion distribution if it exists. @cite considers a number of notions of noisy signals and studies greedy policies and equilibria. Our approach is different from those that assign actions to agents, instead of presenting them with information and letting them make the decisions.
- On the interface of transportation and control theory, there has been a recent interest in load balancing @cite @cite . These schemes, however, rely on simple randomisation, without modelling heterogenous agent behaviour and actions and without allowing for the same information to be provided to all agents. On the interface of transportation and behavioral science, Ben- @cite have studied the effects of information on drivers. In a number of subsequent papers @cite @cite and the dissertation of Bottom @cite , fixed points have been used to study deterministic scalar signaling with deterministic response of the population. See @cite for an extensive survey. In contrast, our analysis can be seen as a study of a probabilistic counter-part of fixed points, which allows for the uncertainty in the response of the population.
- Throughout, we are not aware of any theoretical guarantees on the behavior of policies similar to ours, as described in this paper and @cite . Specifically, we are not aware of any other paper, which would study the broadcasting of intervals, instead of scalars, show its superiority, or study the behaviour of systems, where such signals are being provided. Compared to this paper, the set-up of @cite is much simpler, and so are the proofs and simulations. Unlike @cite , the approach presented in this paper does not employ randomisation, whose use may be unacceptable to the general public, allows for the same signal to be broadcast to all agents, such as at road-side displays, and considers a more elaborate model of the populational response, with risk-aware agents. Both this paper and @cite suggest the importance of the control-theoretic aspects of information provision.
- For the theoretical analysis of the achievable performance bounds, available results are relatively fewer. In @cite , the uplink capacity gain was derived when one D2D link was enabled in an FDD CDMA-based cellular cell. In @cite , an interference-limited area (ILA) control scheme was proposed to manage the interference from CUEs to a D2D transaction when multiple antennas were used by the BS. By analyzing the coverage of ILA, a lower bound of the ergodic capacity was also obtained for DUEs using uplink cellular radio resources. After that, a similar approach was extended to the downlink resources sharing scenario in @cite . In @cite , the maximum achievable transmission capacity, which was defined as the spatial density of successful transmissions per unit area, was analyzed for the hybrid D2D and cellular network through stochastic geometry. However, due to the inevitable interference accumulated at the BS, most of the existing analytical results assuming a single D2D pair in a cellular network cannot be directly extended to a scenario with multiple coexisting D2D pairs. Therefore, the performance of D2D communications in the latter is still an under-developed issue, which could further improve the spectrum efficiency and increase the cellular network throughput.
- Flash memory's power consumption has been modeled at a low level @cite . This study constructed a detailed model of flash power consumption derived from the transistor and layout level information. Their model was validated against physical measurements of a flash chip, but requires detailed knowledge of the exact structure of the flash. Additionally, this model is not applicable to embedded flash, which has a different, simpler structure. @cite characterize the energy required to write to multi-level cell flash devices, and develop an energy aware compression method to exploit the value dependent nature of the energy consumption.
- Other software optimizations targeting energy have considered automatically inserting idle instructions @cite , instruction scheduling @cite , use of SIMD @cite and exploiting differences in functional units @cite .
- A number of approaches have arisen for computing dense pyramids of CNN features in various computer vision applications outside of object detection. @cite and @cite construct multiresolution pyramids of 2-layer CNNs. apply their network to scene parsing, and showed multiscale CNN results on human pose estimation. Along the same lines, compute CNN pyramids and perform sliding-window processing for image segmentation @cite . Several years earlier, densely computed CNNs on full images for robust face localization @cite .
- The main result of this article (Theorem ) extends Theorem to parametrised collections of ordered pairs of probability distributions, in contrast with ordered sequences as in @cite @cite . The proof of Theorem is based on measurability properties of related set-valued mappings and an application of a measurable selection theorem of Kuratowski and Ryll-Nardzewski @cite . We are unaware of earlier results which would be directly applicable in this context. However, similar results related to martingale couplings have appeared recently in the context of optimal transport. Beiglboeck and Juillet @cite consider the problem of finding an optimal transport plan under the constraint that the transport plan is a martingale. The work of Fontbona, Gu 'erin and M 'el 'eard @cite has the most similarities with our developments. With the notation above, they consider finding a measurable optimal transport plan between @math and @math . The work of Hobson @cite , brought to our attention by a referee, provides an explicit Skorokhod embedding of two univariate convex ordered distributions. This embedding could be used to prove our result in the scalar case.
- Demand side management and demand response receive increasing attention by research and industry. The research work published so far includes a variety of directions from direct load control or targeted customer interaction to indirect incentive-based control (see @cite for an overview). In order to help balancing demand and supply, demand side management programs require accurate predictions of consumer demand.
- The approaches for demand side management focus on different levels of the power system. On the grid operator level, studies focus for example on the minimization of power flow fluctuations @cite or the integration of renewable energy @cite . The distribution grid operator uses consumption forecasts to balance grids with a high penetration of decentralized generation of renewable energy (e.g., @cite , @cite ). Other studies look at the level of groups of consumers with a focus on game theoretic frameworks @cite or virtual price signals @cite . Most work on demand response, however, focuses on the level of end consumers. Recent research has studied the use of variable price signals for individual customers. These dynamic tariffs penalize consumption during certain periods of time with increased electricity prices, so that customers can respond by adjusting their consumption (e.g., @cite , @cite ). However, @cite points out that demand side management with variable price signals can cause instabilities through load synchronization. To avoid uncontrolled behavior, accurate consumption forecasts can help utilities to select customers that are most suitable for a demand response program.
- First studies have analyzed the potential and first prototypes of consumption forecasts for individual households (e.g. @cite , @cite ). However, most work on household consumption focuses on disaggregation of electricity consumption. Examples include @cite , @cite , @cite , @cite and @cite . The authors of @cite give an overview of the state-of-the-art in this area. In this paper, we benchmark state-of-the-art forecasting models for household consumption and also evaluate how the disaggregation of consumption data influences the prediction of household consumption.
- The power of social media for the automated analysis of real-world events has been universally recognized @cite . The large volumes of timely user-generated content in response to public events such as election campaigns allow the extraction of event insights not easily obtainable via alternative means @cite @cite . Social media users act as sensors that empower the development of real-time event detection methodologies @cite @cite . We follow the trends of employing social media for our analysis, and take advantage of location-based services data to automate the impact assessment of a major sports event.
- One of our main hypothesis is that geographic factors can play a major role in the popularity of places during highly active seasons such as the Olympic Games. An inspiration is drawn from Jensen's work on quantifying the optimal spatial positioning of retail stores @cite @cite . Jensen as well as Porta et. al. @cite @cite demonstrate that pure spatial organization can be indicative of the quality of retail and economic activities in the cities of Lyon (France), Bologna (Italy) and Barcelona (Spain).
- Our work is further related to a stream of research on that aims to extract insights from location and mobility data in order to aid urban planning and the development of smart cities. @cite investigate the interplay between citizen mobility and the well-being of London's census areas through a public transport fare dataset. and @cite @cite , on the other hand, focus on mobility pattern modeling to characterize the structure of cities with respect to neighborhood dynamics and functional regions. Our approach to extracting mobility features in a neighborhood is related to these works in the sense that we qualitatively assess micro-areas in the city to uncover the underlying factors contributing to venue popularity during the Olympics.
- There is a large body of data mining literature regarding how to optimize various algorithms to be more architecturally aware @cite @cite @cite . @cite @cite study the performance of a range of different algorithms, including associated rule mining and decision tree on shared-memory machines, by improving memory locality and data placement in the granularity of cachelines, and decreasing the cost of coherent maintenance between multiple CPU caches. @cite optimize the cache behavior of frequent pattern mining using novel cache-conscious techniques, including spatial and temporal locality, prefetching, and tiling. @cite discuss tradeoffs in replication and locking schemes for K-means, association rule mining, and neural nets. This work considers the hardware efficiency of the algorithm, but not statistical efficiency, which is the focus of . In addition, do not consider lock-free execution, a key aspect of this paper.
- Performance optimization on shared-memory multiprocessors machines is a classical topic. Anderson and Lam @cite and 's @cite seminal work used complier techniques to improve locality on shared-memory multiprocessor machines. 's locality group is inspired by Anderson and Lam's discussion of computation decomposition and data decomposition . These locality groups are the centerpiece of the Legion project @cite . In recent years, there have been a variety of domain specific languages (DSLs) to help the user extract parallelism; two examples of these DSLs include Galois @cite @cite and OptiML @cite for Delite @cite . Our goals are orthogonal: these DSLs require knowledge about the trade-offs of the hardware, such as those provided by our study.
- The database community has recognized that multi-socket, large-memory machines have changed the data processing landscape, and there has been a flurry of recent work about how to build in-memory analytics systems @cite @cite @cite @cite @cite @cite @cite @cite . Classical tradeoffs have been revisited on modern architectures to gain significant improvement: @cite , @cite , @cite , and Li @cite study the tradeoff for joins and shuffling, respectively. This work takes advantage of modern architectures, e.g., NUMA and SIMD, to increase memory bandwidth. We study a new tradeoff space for statistical analytics in which the performance of the system is affected by both hardware efficiency and statistical efficiency.
- * High Performance Computation The techniques that we considered in for efficient implementation () are not new, and they are borrowed from a wide range of literature in high performance computation, database, and systems. Locality is a classical technique: worker and data collocation technique has been advocated since at least 90s @cite @cite and is a common systems design principle @cite .
- The role of dense and sparse computation is well studied in the by the HPC community. For example, efficient computation kernels for matrix-vector and matrix-matrix multiplication @cite @cite @cite @cite . In this work, we only require dense-dense and dense-sparse matrix-vector multiplies. There is recent work on mapping sparse-sparse multiplies to GPUs and SIMD @cite , which is useful for other data mining models beyond what we consider here.
- * Mathematical Optimization Many statistical analytics tasks are mathematical optimization problems. Recently, the mathematical optimization community has been looking at how to parallelize optimization problems @cite @cite @cite . For example, @cite for SGD and Shotgun @cite for SCD. A lock-free asynchronous variant was recently established by @cite .
- With PPPs, using simplifying assumptions, such as Rayleigh fading channel model, and a path-loss exponent of four, we can obtain closed form expressions for aggregate interference and outage probability. Therefore, use of PPP models for performance evaluation of HetNets is appealing due to their simplicity and tractability @cite . Furthermore, the PPP based models provide reasonably close performance results when compared with the real BS deployments. In particular, results in @cite show that, when compared with real BS deployments, PPP and hexagonal grid based models for BS locations provide a lower bound and an upper bound, respectively, on the outage probabilities of UEs. Also, the PPP based models are expected to provide a better fit for analyzing denser HetNet deployments due to higher degree of randomness in small-cell deployments @cite . In this paper, due to their simplicity and reasonable accuracy, we will use PPP based models to characterize and understand the behavior of HetNets in terms of various design parameters.
- There is an intensive literature on object shape matching between a pair of images @cite . Representative works include shape context @cite , graph @cite @cite and hyper-graph @cite @cite @cite matching. In this section, we briefly review several existing methods that use multiple images point sets for object matching, and also the more general MiAP.
- Maciel and Costeira @cite first proposed to use PPM to model both feature matching and outlier rejection in a set of images. They formulated optimization of PPMs as an integer constrained minimization problem. To solve this combinatorial problem, they relaxed both the objective function and integer constraints, resulting in an equivalent concave minimization problem. However, the complexity of concave minimization is still non-polynomial. Moreover, matching criteria used in the cost function of @cite were locally based on pair-wise similarity of features in different images. Instead, our method is based on low-rank and sparse minimization (via convex surrogate functions), whose problem size is polynomial w.r.t. the numbers of features and images, and whose cost function is also globally defined over features in all the images.
- Recently, a low-dimensional embedding method was proposed in @cite for feature matching. Given feature points extracted from each of a set of images, it can learn an embedded feature space, which encodes information of both region descriptors and the geometric structure of points in each image. @cite used k-means clustering in the embedded space for feature matching. As we will show in Section , k-means based on Euclidean distances of embedded features is not a good way to establish correspondences. There is no explicit outlier rejection mechanism in @cite either. Compared to @cite , our method uses the low-rank and sparse constraints to optimize PPMs, which integrates correspondence and outlier rejection in a single step.
- Greedy approaches build a matching that has the lowest cost at each iteration, which has the obvious weakness that decisions once made, are fixed and may later be shown to be suboptimal. GRASP improves over greedy approaches by progressively constructing a list of best candidate matches and randomly selecting one from them. The process is repeated until all matches are exhausted. A final local search over the neighborhood of obtained matches may be used to further optimize the solution. In @cite , Poore and Robertson presented a Lagrangian relaxation method that progressively relaxes the original and intermediate recovery MiAPs to linear assignment problems, by incorporating constraints of each MiAP into its objective function via the Lagrangian. However, this method involves implicitly enumerative procedure, and is difficult to implement and analyze.
- Numerous content-aware image retargeting techniques have recently been proposed. Cropping has been widely used to eliminate the unimportant information from the image periphery or improve the overall composition @cite @cite @cite . Seam carving methods iteratively remove a seam in the input image to preserve visually salient content @cite @cite . A seam is a continuous path with minimum significance. Multi-operator algorithms combine seam carving, homogeneous scaling and cropping to optimally resize images @cite @cite @cite . @cite introduced Shift-Map that removed or added band regions instead of scaling or stretching images. For many cases these approaches can generate pleasing results, however, the seam removal may cause discontinuous artifacts, and cropping is unsuitable for the case when there are visually salient contents near the borders of images.
- retargeting methods have been realized through image warping or mapping by using several deformation and smoothness constraints @cite @cite @cite @cite @cite @cite @cite . A finite element method has also been used to formulate image warping @cite . Recent continuous retargeting methods focus on preserving local structures. @cite minimize warping energy in the space of axis-aligned deformations to avoid harmful distortions. @cite couple mesh deformations with similarity transforms for line features to preserve line structure properties. @cite present a patch-based scheme with an extended significance measurement to preserve shapes of both visual salient objects and structural lines. These approaches perform well on shape preservation of salient objects but often over-squeeze or over-stretch the T-regions to distort all texels since T-regions are usually not salient in the whole image.
- -based retargeting approaches eliminate repetitive patches instead of individual pixels and preserve patch coherence between the source and target image during retargeting @cite @cite @cite . These techniques measure patch similarity and select patch arrangements that fit together well to change the size of an image. However, due to the lack of enough content information, the major drawback of such methods is that the globally visual effect may be discarded and some regions may be over-smoothed when the target size is small.
- The adaptive integration of the color and texture attributes in image segmentation is one of the most investigated topics of research in computer vision (surveyed in @cite ). However, most of the image segmentation algorithm do not clearly illustrate the type of each region (textural or non-textural) in the result. @cite present a fast texture descriptor based on LU transform, but how to determine if a pixel is texture or non-texture according to the feature values is not discussed. @cite present an intuitive texture detection method which is based on contrast and disorganization measurements of image blocks. The method is not effective on noisy images which tend to have decreasing contrast and often generate many disjoint areas. Todorovic and Ahuja @cite formulate the detection of texture subimages as identifying modes of the pdf of region descriptors. However, the method is not efficient (5 minutes for a @math image) for practical image retargeting applications.
- Texture synthesis is a general example-based methodology for synthesizing similar phenomena @cite . However, the basic MRF-based scheme in most existing texture synthesis methods cannot adequately handle the globally visual variation of texels, such as perspective appearance and semantic content distribution. @cite present a perspective-aware texture synthesis algorithm by analyzing the size variation of the texel, but verbatim copying artifacts also often appear in their results. On the other hand, common texture synthesis algorithms are designed for enlargement and can not be directly used for image retargeting applications. Wei @cite presents inverse texture synthesis approach to generate a smaller example from a large input texture. However, for globally varying textures, the output quality of this approach usually depends on the accuracy of the original map. Therefore, if applied to normal T-region retargeting, it will easily lose the globally visual variation or damage the local content continuity of the original image in the result.
- In the discrete setting, works related to the @math -convergence of functionals to continuous functionals involving perimeter include @cite , @cite and @cite . The results by Braides and Yip @cite , can be interpreted as the analogous results in a discrete setting to the ones obtained by Modica and Mortola. They give the description of the limiting functional (in the sense of @math -convergence) after appropriately rescaling the energies. In the discretized version considered, they work on a regular grid and the gradient term gets replaced by a finite-difference approximation that depends on the mesh size @math . Van Gennip and Bertozzi @cite consider a similar problem and obtain analogous results. In @cite , Chambolle, Giacomini and Lussardi consider a very general class of anisotropic perimeters defined on discrete subsets of a finite lattice of the form @math . They prove the @math -convergence of the functionals as @math to an anisotropic perimeter defined on a given domain in @math .
- Joung's original algorithm for the GME problem satisfies the four basic properties. It does not satisfy the FCFS property. Moreover, it has unbounded RMR complexity. Hadzilacos (see @cite ) gave the first solution for the GME problem that has the FCFS property. His algorithm can be thought of as a modular composition of two independent algorithms, one, the FCFS algorithm'' provides FCFS property (but not necessarily guarantee mutual exclusion) and the other, the ME algorithm'' provides mutual exclusion property (but not necessarily FCFS). This algorithm has shared space complexity of @math . It was (mistakenly in hindsight) claimed that the algorithm has RMR complexity of @math in the CC model. The algorithm was using only bounded shared variables and simple read and write operations. It was left as an open problem to develop a solution (satisfying P1 through P5) for the GME problem that runs in linear time and space using only bounded shared variables.
- Subsequently, @cite , presented an algorithm presumed to be of linear time and space, solving the challenge of Hadzilacos. retained the idea of modular composition and also the ME algorithm'' that Hadzilacos used. They came up with a clever modification to the FCFS algorithm'' of Hadzilacos to reduce the space complexity to @math . Unfortunately, though without writing it explicitly, they inherited from Hadzilacos the mistaken impression that the algorithm they use has @math time complexity. For example, the recent paper by Bhatt and Huang @cite explicitly states that the RMR complexity of the algorithm by is @math .
- Takamura and Igarashi also made an attempt in @cite to generalize Lamports Bakery Algorithm to solve the GME problem. They presented three different algorithms in that paper. However, all of their algorithms satisfy neither the concurrent entry property nor the FCFS property.
- In 2004 (which is in the future of @cite and @cite ), Taubenfeld @cite came up with an elegant algorithm called that solves the classical mutual exclusion problem with only bounded shared registers. His approach is a lot simpler than a prior method that bounds the registers of the Lamport's Bakery Algorithm developed by in @cite . Our second algorithm, presented in is a generalization of the Black and White Bakery Algorithm to solve the GME problem. Our algorithm satisfies the properties P1 through P5 and runs in linear time and space using bounded shared registers. Thus, our algorithm is the first one to solve the open problem originally posed by Hadzilacos, a decade and a half ago.
- These two existing, popular approaches are based on subgraph features, which are limited to specific type ones only. In contrast, a series of inspiring studies for has been made . These approaches involve automatic selection of relevant features from during the learning process. Triggered by the seminal paper by @cite , it has been shown that we can perform simultaneous learning of not only the model parameters but also relevant subgraph features from all possible subgraphs in several machine-learning problems such as Adaboost , LARS LASSO , sparse partial-least-squares (PLS) regression , sparse principal component analysis (PCA) , and LPBoost . This paper aims to give a coherent and unifying view to understand how these existing methods work well, and then present a more general framework, which is applicable to a wider class of learning problems.
- Some papers discuss the existence condition of a @math -core subgraph @cite @cite . Some study the relationship between the @math -core and the percolation dynamics @cite , and it is worth noting that the bond percolation is quite similar to the label propagation in community detection @cite . Recently, @math -core is also used to evaluate the robustness of communities, which reflects the collaborative nature in co-authorship graphs @cite .
- Lampson @cite made a basic distinction between legitimate channels, storage channels and covert channels in operating systems, which is considerably extended in our work.
- Lampson @cite introduces covert channels in 1973, which are defined as communication channels not intended for information transfer at all''. Both covert storage channels and covert timing channels have been described by the National Computer Security Center @cite in 1993. Wray @cite presents a study on covert channel terminology where he questions the differentiation between covert storage channels and covert timing channels and concludes that storage nature and timing nature are attributes of the channel, and a given channel may possess either or both.'' A covert channel of this combined type could be identified as both a covert storage channel and a covert timing channel in our taxonomy. In a different approach, @cite present a study on automatic identification of covert channels in Linux. Kemmerer @cite introduced the shared resource matrix that describes a methodology to systematically identify covert storage channels in a computing system. In contrast to the work of Kemmerer, our methodology is not targeted at covert channel identification, but aims to provide a complete information flow analysis in the model of a component-based operating system where different types of attack patterns are identified and described.
- Storage channels are also defined by Lampson @cite , who notes that data is written by the service and read by an unconfined program, either shortly after it is written or at some later time'', describing a storage channel between a confined and an unconfined program. Corrupt policy channels that are present due to a misconfigured access control policy have been addressed by Agreiter @cite , proposing automatic generation of an operating system's MAC policy, and @cite , presenting a new method for automatic analysis of an operating system's MAC policy. Finally, exploiting illegitimate access to components would be a special case of a flawed system policy, where a policy circumvents an operating system guard @math that protects access to a component @math , managing a shared resource, and allows @math and @math to exchange messages via @math . The use of operating system guards to handle shared resources to protect applications from untrusted components has been described by @cite , @cite @cite , @cite , and Hanspach and Keller @cite .
- Graph databases have been studied for at least three decades, for a survey see @cite . Perhaps best examples of modern single-computer database systems are Neo4j @cite and DEX @cite . Compared to this work, they do not provide powerful computational capabilities and have not been designed for extremely large graphs like GraphChi-DB. TurboGraph @cite has very different design than GraphChi-DB, but also supports both database and computational operations. However, the authors do not evaluate the performance of TurboGraph Unfortunately, we were not able to obtain TurboGraph @cite for evaluation, but limited experiments done by other researchers in @cite are available. in the online setting and it is unclear how efficiently it can handle graphs with edge and vertex attributes. Graph storage has been also studied by the Semantic Web community, for storing RDF data. Storing RDF as a graph was first proposed in @cite . Our focus has been on graphs such as social networks and the web, but we suggest GraphChi-DB could also be used as a backend for storing RDF triples.
- Graph kernels can be considered as special cases of convolutional kernels proposed by . In general, graph kernels can be categorized into three classes: graph kernels based on walks and paths @cite , @cite , @cite , graph kernels based on limited-size sub-graphs @cite , @cite , @cite and graph kernels based on subtree patterns @cite . performs a relaxation on the vertices and exploit labeling information embedded in the graphs to derive their so-called Weisfeiler-Lehman kernels. However, their kernel is applicable only to labeled graphs. The sparsity problem of graphlet kernels has been addressed before. Hash kernels proposed by addresses the sparsity problem by applying a sparse projection into a lower dimensional space. The idea here is that many higher order graphlets will collide'' and therefore be mapped to the same lower dimensional representation, thus avoiding the diagonal dominance problem. Unfortunately, we find that in our experiments the hash kernel is very sensitive to the hash value used for embedding and rarely performed well as compared to the MLE estimate.
- The idea of leveraging in-network storage for improving network performance is gaining increasing interest @cite and has been recently proposed also for small cell networks @cite - @cite . Authors in @cite performed the file placement in storage capable base stations based solely on file popularity. The subsequent work in @cite extended their results for the special case that users request video files encoded into multiple quality levels. In our previous work @cite , we studied the impact of SCBSs' wireless capacity constraints on the caching decisions. In contrast to all these studies, our caching policy is carefully designed with concerns on the multicast which is often used by operators to reduce the servicing cost. It is worth emphasizing that this twist increases significantly the complexity of the caching policy design problem. Namely, while for the simple scenario of non-overlapping coverage areas of the SCBS the conventional file placement is a trivial problem @cite , we prove that incorporating multicast transmissions into the system makes it NP-hard.
- Exact algorithms incorporating, among others, dynamic programming, branch-and-bound algorithms and greedy approaches, were proposed by @cite , Irnich and Villeneuve @cite , @cite , @cite and Chabrier @cite . Noteworthy, only 9 instances out of 300 belonging to the Gehring and Homberger's benchmark @cite have been solved to optimality @cite . An extensive review of the exact methods can be found in Kallehauge @cite .
- Heuristic algorithms can be divided into two classes, namely the improvement and the construction techniques. In the construction heuristic algorithms the customers are iteratively inserted into a partial feasible solution without violating the time windows and capacity constraints. Several construction heuristics were proposed by Solomon @cite , Potvin and Rousseau @cite and recently by Pang @cite . On the other hand, the improvement heuristics modify an initial solution and explore the search space by performing local search moves in order to decrease the fleet size and the total traveled distance. The examples of such heuristics can be found in @cite , Russell @cite and Potvin and Rousseau @cite .
- Metaheuristic algorithms incorporate mechanisms to explore the search space and to exploit its most promising regions. They allow infeasible intermediate solutions and the solutions deteriorating during the search process in order to escape the local minima. A number of sequential and parallel algorithms were introduced during the recent years. The simulated annealing was successfully applied by Zhong and Pan @cite , Debudaj-Grabysz and Czech @cite , and recently by Li @cite . The tabu searches were proposed by @cite and @cite . The ant colony approaches can be found in @cite and Qi and Sun @cite . A survey on the metaheuristic algorithms can be found in Br " a ysy and Gendreau @cite .
- Evolutionary algorithms have attracted the scientific attention to solve the VRPTW due to their high search capabilities. Genetic algorithms were applied by Cheng and Wang @cite , @cite and @cite . The evolution strategies were proposed by Gehring and Homberger @cite , @cite and @cite . The sequential and parallel memetic algorithms (MAs), combining the evolutionary algorithms for more distant search with the local optimization and refinements algorithms for local exploitation, were described by Berger and Barkaoui @cite , @cite and Nagata and Br " a ysy @cite . The MA based on the edge-assembly crossover operator (EAX) has been proposed by Nagata and Br " a ysy @cite .
- The development of serious games for training is a complex and time-consuming process @cite . While early works considered a whole story creation without getting any real-time input from the user @cite @cite , the later works focus on interactive narratives where the user is part of the story and can affect the plotline @cite @cite @cite @cite . Some works use also hybrid methods whereby a predefined plot is created and an autonomic agent can later add to or adapt the plot in real-time @cite . However, these studies focus on the general plot of the story but not on the story's details, which were almost exclusively manually created by content experts. A second important direction is the Procedural Content Generation (PCG) for Games discipline. However, while there has been significant in PCG generation for serious games in the past decade, to date, tools for textual content generation are still lacking @cite . To address this need, we created a framework for generating coherent scenarios, which include a sequence of activity events (activities) and also the activities' descriptions (narratives).
- The term scenario have several definitions depending on the context it is used. According to @cite Game scenarios describe, often transparently to the user, the way and order in which game events unfold. Two types of game scenarios can be distinguished, abstract and concrete. The abstract game scenarios describe how other objects inter-relate. The concrete game scenarios are explicitly presented in the game, for example as part of the game narrative.''. In this paper, we refer to concrete scenario types. As such, we use the term scenario to describe a sequence of activity events (activities) and their descriptions (narratives).
- One traditional approach for activity-details generation is planning-based systems @cite @cite @cite @cite . The planning-based approach uses a causality-driven search to link a series of primitive actions in order to achieve a goal or to perform a task. For the domain of descriptive, every day activities, hierarchical scripts can capture common ways to perform the activity. Therefore, we implemented the planning-based generator using a Hierarchical Task Network (HTN), which is one of the best-known approaches for modeling expressive planning knowledge for complex environments. We used the state-of-the-art SHOP2 planner @cite , a well known HTN planner, which has been evaluated and integrated in many real world planning applications and domains including: evacuation planning, evaluation of enemy threats and manufacturing processes. In order to validate the significance of modified scenarios with the activity-details we created, we also implemented a planning-based activity-details generator. As we later report, this approach is more costly to implement and generates less varied scenarios than the crowdsourced approach we now describe.
- In order to be able to reason on the affective dimension of the interaction, conveyed by the non-verbal behaviour of both interlocutors, several models rely on the cognitive structure of emotions and appraisal theories such as CPM @cite or OCC @cite . These theories provide domain-independant descriptions of triggering conditions of emotions, that are required for the development the affective aspect of the ToM reasoner. For instance, @cite @cite are BDI-based implementation of the OCC theory. FAtiMA's double appraisal model @cite , although not implemented using a BDI framework, also encodes the OCC model. However, in these models, the inference mechanism itself encodes the chosen Appraisal Theory. On the contrary, in our model, we propose a theory-independant ToM reasoner. While our experiments were conducted using an OCC-based model, the corresponding rules (described in equation ) could be easily replaced by another theory.
- To support such adaptability, we propose to rely on the BDI model. Several computational models of emotions have already been proposed (e.g. @cite @cite ) that show that BDI is a good basis to represent and to reason about the interlocutor's mental state. Our aim is thus to define a logical model of emotions and ToM in BDI.
- From the philosophical point of view, a debate about how ToM is processed by human adults opposes two theories. The (TT) argues for a folk-psychology reasoning, i.e. a set of rules one acquires regarding human mind functioning. @cite . The (ST) @cite defends a mirroring or projection process allowing for taking someone else's perspective. Various research demonstrated that neither pure TT nor pure ST were realistic @cite and both theorists and simulationists turn toward more hybrid models @cite @cite .
- Existing computational ToM models either imply a choice between the TT and ST theories ( @cite that relies on a ST approach, or @cite and @cite that position in the TT) or implement them separately as in @cite . In our work, we propose a hybrid approach that relies on theory-theory to model the agent's mental states and commonsense rules, but also on simulation-theory to others' perspective by projecting attributed mental states on its own inference engine. Both models are integrated in the same reasoner.
- Detection of deleted posts has been used in previous work to quantify censorship. The most pervasive methodology involves sampling microblog posts over a period of time to capture sensitive political events while querying the service at regular intervals to determine if any of the posts have been deleted @cite @cite @cite .
- @cite uncovered politically sensitive terms more likely to be actively and retroactively deleted in a comparison between censorship on Twitter and China's Sina Weibo microblogging services. A random sample of collected messages found that 16.25 Initial research by @cite shows that active and retroactive censorship to a large extent succeeds in stemming the spread of information on microblogs. In a subsequent work, the authors studied the time distribution of deleted messages and found that nearly 30 Network perturbation and resilience is a closely related field where network metrics are studied under destructive processes that iteratively remove nodes or edges @cite @cite @cite , however, these works do not consider censoring models for these processes nor do they formulate the problem as one of classification.
- Despite these important works, no research to date has explored the effects of censorship on the underlying structure of the network and furthermore no research exists that attempts to automatically detect and classify censorship in these networks. Given that online social networks have certain universal properties @cite , it is likely that common strategies of censorship such as limiting or deleting content or users from the network would have measurable effects on these properties. This research constitutes a first step to fill this gap by studying these effects.
- The performance of BS sleeping has been analyzed in several algorithmic and simulation-centric studies with different scenarios, assumptions, and network parameters (see @cite @cite @cite @cite @cite @cite and references therein). In @cite , optimal energy savings are calculated as a function of the daily traffic pattern while ignoring the wireless link factors. In @cite , network energy consumption is minimized by adjusting the cell sizes when BS sleeping is performed. Nevertheless, the transmit power required by the active BSs to provide coverage to sleeping cell users, i.e., effect of cell-zooming is ignored in the energy efficiency analysis @cite @cite . Other interesting studies optimize network utility functions, e.g., minimize energy expenditure by optimizing the BS activity with traffic load constraints @cite and minimize a weighted combination of power consumption and delay to optimize the service rate @cite .
- Recently, few analytical frameworks have considered to derive the optimal density of sleeping BSs @cite @cite @cite , proposed distributed sleeping mechanisms considering cognitive small cells @cite , and developed different BS switching-off schemes @cite @cite . The development of such frameworks is highly desirable to capture the joint interplay among the network design parameters while extracting in-depth theoretical insights behind various observations and performance trends. A power minimization problem with coverage constraint for a randomly selected sleeping cell user is formulated in @cite @cite such that the BS density is optimized considering random and traffic load-based sleeping. To avoid coverage holes, power of all active BSs is assumed to increase equally regardless of the channel conditions of the sleeping cell users. Due to the complexity of closed-form solution, an optimal value of BS density is computed numerically. In @cite , a BS density minimization problem is formulated with coverage constraints of users in sleeping cells. However, the expression for coverage includes two improper integrals and an infinite summation. Therefore, the authors opt to derive upper and lower bounds on the optimal BS density.
- A distance-aware BS switching algorithm is presented in @cite that recommends to switch-off a BS with maximum average distance from its own and neighboring cell users. In @cite four heuristic-based BS switching-off patterns are proposed and the coordination among the BSs is exploited to provide coverage to the sleeping cell users.
- Two other analytic models of Homeplug exist in the literature ( @cite and @cite ). However, they model mechanisms that differ from the original standardised Homeplug procedure. In @cite , the value of the contention window is fixed for all backoff stages, while in @cite the backoff stage is incremented every time a new packet is overheard (thus having @math ). These new approaches obviously lead to a simplified analysis, as one of the dimensions of the Markov Chain is removed. However, the flexibility the standard provides by allowing to tune both the @math and @math is not captured by these frameworks.
- Consequently, the analytic model in @cite has been widely used both to derive performance metrics and as a basis for extension @cite @cite @cite @cite @cite @cite as it strictly follows the procedure defined in the standard and it considers saturated as well as unsaturated conditions. This widespread use motivates our improvements in terms of reducing complexity while maintaining the accuracy and retaining all features defined in the standard @cite and understanding the transitory nature of the results predicted before saturation in @cite .
- Hypergraphs can easily capture the higher-order relationships while incorporating both group and node level attributes. Moreover, research has shown that several social, biological, ecological and technological systems can be better modeled using hypergraphs than using dyadic proxies @cite . There is an abundant literature of hypergraph theory in past @cite and many work in the spectral theory of hypergraphs recently @cite @cite . The past decade has also seen an increasing interest for hypergraphs in machine learning community @cite @cite . Hypergraphs have been used to model complex networks in different fields including biology @cite , databases @cite and data mining @cite . In the domain of social sciences, Kapoor et a.l @cite have proposed with centrality metrics for weighted hypergraphs. @cite propose hypergraphs based metrics to evaluate various hypothesis, both semantic and structural, regarding team formation.
- As a weaker version of the Existence Conjecture, Erd o s and Hanani @cite posed the question of the existence of asymptotically optimal Steiner systems; equivalently, finding @math edge-disjoint @math 's in @math . This was proved by R "odl @cite , by developing a new semi-random construction method known as the nibble', which has since had a great impact on Combinatorics (see e.g. @cite @cite @cite @cite @cite @cite @cite @cite @cite for related results and improved bounds). It will also play an important role in this paper.
- Even the existence of designs with @math and any non-trivial' @math was open before the breakthrough result of Teirlinck @cite confirming this. An improved bound on @math and a probabilistic method (a local limit theorem for certain random walks in high dimensions) for constructing many other rigid combinatorial structures was recently given by Kuperberg, Lovett and Peled @cite . Their result for designs is somewhat complementary to ours, in that they can allow the parameters @math and @math to grow with @math , whereas we require them to be (essentially) constant. They also obtain much more precise estimates than we do for the number of designs (within their range of parameters).
- Another recent result, due to Ferber, Hod, Krivelevich and Sudakov @cite gives a short probabilistic construction of almost Steiner systems', in which every @math -subset is covered by either one or two @math -subsets.
- Another relaxation of the conjecture, which will play an important role in this paper, is obtained by considering integral designs', in which one assigns integers to the copies of @math in @math such that for every edge @math the sum of the integers assigned to the copies of @math containing @math is a constant independent of @math . Graver and Jurkat @cite and Wilson @cite showed that the divisibility conditions suffice for the existence of integral designs (this is used in @cite to show the existence for large @math of integral designs with non-negative coefficients). Wilson @cite also characterised the existence of integral @math -decompositions for any @math -graph @math .
- In the recent past, zeta functions associated to Heisenberg groups and their various generalizations have often served as a test case for an ensuing general theory. For instance, the seminal paper @cite contains special cases of the computations done in the present paper as examples. Similarly, Ezzat @cite computed the representation zeta functions of the groups @math for quadratic number rings @math , enumerating irreducible finite-dimensional complex representations of such groups up to twists by one-dimensional representations. The paper @cite develops a general framework for the study of representation zeta functions of finitely generated nilpotent groups. Moreover, it generalized Ezzat's explicit formulae to arbitrary number rings and more general group schemes.
- The current paper leaves open a number of challenges. One of them is the computation of the rational functions @math for general @math ; in the special case @math , this has been achieved in @cite . Another one is the computation of the local factors of the @math enumerating subgroups of finite index in @math . This has not even been fully achieved for quadratic number rings @math .
- More generally, it is of interest to compute the (normal) subgroup zeta functions of other finitely generated nilpotent groups, and their behavior under base extension. We refer the reader to @cite for a comprehensive list of examples. In his MSc thesis @cite , Bauer has generalized many of our results to the normal zeta functions of the higher Heisenberg groups @math for all @math , where @math is a centrally amalgamated product of @math Heisenberg groups. In other words, if @math is a ring and we view elements of @math as row vectors, and if @math denotes the @math identity matrix, then ) a , b R^m, , c R . The paper @cite arose from the (uncompleted) project to compute the subgroup zeta functions @math .
- There have been a few attempts in generating invariant time-series features through factorization. A shift-invariant sparse coding of signals has been proposed for reconstructing noisy or missing series segments @cite . In similar domains, sparse coding factorization has been applied for deriving shift and 2D rotation invariant features of hand writing data @cite , and also invariant features of audio data @cite . Moreover, a temporal decomposition of multivariate streams has been used to discover patterns in patients' clinical events @cite . Our method differs from distance metrics principally. Instead of measuring the similarity of series, we project the data into a new representation, where similar instances are positioned close to each other. Furthermore, the proposed method distances away from existing bag-of-patterns methods because we learn a latent decomposition of patterns, instead of counting the occurrence of segments on the original time-series. Finally, our contributions over the existing factorization methods rely on (i) a novel approach in detecting both shift and scale invariant features for time series, and (ii) building a bag-of-patterns representation of the learned invariant features for a classification scenario.
- RANSAC based homography estimation methods have been extended to the problem of detecting multiple planes by removing inliers and re-estimating new homographies iteratively. Further, @cite developed a multi-RANSAC algorithm that is capable of estimating all homographies simultaneously. These methods do not work well in practice and also, need additional knowledge of number of planes.
- @cite observe that J-linkage uses only nearby feature matches to generate initial homographies. This is done to ensure that the computed homographies correspond to real scene planes. Though, the downside of such an approach is that the homographies output by J-linkage are also locally optimal and do not fit a large scene plane. The scene plane is output as multiple small planar patches by J-linkage. @cite solve this by continuing to cluster the matches (after J-linkage) using the distance measure:
- use to train coercion-resistant passwords to users @cite . Through repeated execution of specific tasks the user learns behavioral patterns without being aware of what these patterns are. While the user is unable to tell about these patterns a computer system can validate if the user subconsciously 'knows' these patterns.
- Subliminal Face Recognition. Given that facial recognition is seriously considered as a method of authentication, it becomes even more important to test the possibilities of extracting information from facial stimuli. There is existing neuroscientific work on the subliminal perception of human faces. ERPs in response to unpleasant expressions on faces have been shown to have a higher positive amplitude than pleasant expressions. This effect shows even through very fast unmasked subliminal presentations of stimuli, at 1ms @cite . This shows that visual information regarding faces can be processed and produce variance in the EEG signal even at a very subliminal level. Although in our experiment several subjects had noticed the stimuli, a presentation time as little as 1ms could still probably reveal enough information in their EEG signal to extract desired information about faces.
- An interesting aspect about facial stimuli is that different facial expressions can produce very different ERPs. When expressions of fear are subconsciously exposed to the viewer, there is a large N200 ERP amplitude which seems to be associated with more primal pathways, allowing faster reactions to potentially dangerous stimuli @cite . Conscious exposure to the same fearful stimuli, however, have a stronger p300 amplitude, which is associated with more higher level processing of emotion. This can be important in choosing which stimuli to present to the user, and what information to use for training the classifier. For instance, training on a face with a fearful expression will yield a very different combination of ERP amplitudes, which would lead to inaccuracies if testing on happier expressions.
- Although we have achieved some degree of success in our setup concerning face stimuli, it could be worthwhile to explore other types of stimuli, as the neurological response to faces is in many ways unique from other types of visual processing. Subliminal presentation of faces as opposed to words or randomized dots shows a general greater amplitude of ERP, suggesting that faces are processed differently @cite .
- Our proposed method can be related to work on metric learning, for example @cite @cite . However, instead of enforcing a metric on the feature representation directly, as in @cite , we only implicitly force the representation of transformed images to be mapped close together through the introduced surrogate labels. This enables us to use discriminative training for learning a feature representation which performs well in classification tasks.
- The problem of approximate string dictionaries has received considerable amount of attention. We only review here some recent work. For a more thorough review of the existing literature we refer the reader to @cite @cite . Belazzougui @cite recently presented a dictionary specifically tailored for one edit error. The dictionary has a worst-case query time @math (where @math is the number of reported occurrences), and uses asymptotically optimal @math bits of space. Belazzougui and Venturini @cite presented compressed variants. In one of the variants, the dictionary uses @math bits of space (where @math stands for the @math th order entropy of the strings stored in the dictionary, with @math ) and answers queries in almost @math time.
- Our algorithm is in a sense similar to the FASTSS algorithm introduced in @cite . In this algorithm, a hash table stores all the strings that can be obtained by deleting (at most) @math characters from any string in the dictionary. Then at query time the hash table is queried for all the strings that can be obtained by deleting (at most) @math characters from the string. Then, it can be proved that all the strings at edit distance at most @math will be contained in the resulting set and the remaining task will be to test all the candidates. A clever improvement to FASTSS was achieved in @cite , where instead of storing all the strings obtained by deleting one character, the dictionary stores pointers to the strings. Following this, @cite compresses the strings in a different way: instead of storing the strings, only deleted characters along with their positions are stored. Space-efficient perfect hashing methods @cite are then used to store the lists of positions and characters.
- For more than one error, the work of @cite , was the first to present good worst-case performance for any @math . However the bounds are not competitive with the recent works both in terms of space and time for the case of @math . Also the data structure presented there seems rather complicated and incurs a polylogarithmic additive term in the query time which could in practice dominate the total query time. We are not aware of any implementation of that data structure.
- Several assistance tools exists to overcome visual impairments, most of them exploiting vocal instructions to inform the traveller about his position and the near environment. For example, virtual acoustic displays and verbal commands issued by a synthetic speech display are used in @cite . AudioGPS @cite and Melodious Walkabout @cite use audio cues to provide information on the surrounding environment. Dead-reckoning techniques are employed in Navatar @cite where users interact with the application and help correcting possible navigation errors. In @cite , vibrational feed-back is given by a special glove in the Finger-Braille language. This system requires some dedicated hardware and is specific to the language used. RF-PATH-ID @cite , instead, is based on disseminating passive RFID tags and using a dedicated reader to acquire information on the user location. More examples and detailed information on indoor localization techniques may be found in @cite .
- Model based object recognition has received considerable attention in the computer vision community. A survey by Chin and Dyer @cite shows that model based object recognition algorithms generally fall into three categories based on the type of object representation used - namely 2D representations, 2.5D representations and 3D representations.
- 2D representations store the information of a particular 2D view of an object (a characteristic view) as a model and use this information to identify the object from a 2D image. Global feature methods have been used by Gleason and Algin @cite to identify objects like spanners and nuts on a conveyor belt. Such methods use features such as the area, perimeter, number of holes visible and other global features to model the object. Structural features like boundary segments have been used by Perkins @cite to detect machine parts using 2D models. A relational graph method has been used by Yachida and Tsuji @cite to match objects to a 2D model using graph matching techniques. These 2D representation based algorithms require prior training of the system using a show by example' method.
- 2.5D approaches are also viewer centered, where the object is known to occur in a particular view. They differ from the 2D approach as the model stores additional information such as intrinsic image parameters and surface-orientation maps. The work done by Poje and Delp @cite explain the use of intrinsic scene parameters in the form of range (depth) maps and needle (local surface orientation) maps. Shape from shading @cite and photometric stereo @cite are some other examples of the use of the 2.5D approach used for the recognition of industrial parts. A range of techniques for such 2D 2.5D representations are described by Forsythe and Ponce @cite , by posing the object recognition problem as a correspondence problem. These methods obtain a hypothesis based on the correspondences of a few matching points in the image and the model. The hypothesis is validated against the remaining known points.
- Limitations The 2D and 2.5D representations are insufficient for general purpose applications. For example, in the case of vehicle damage detection, a vehicle may be photographed from an arbitrary view in order to indicate the damaged parts. Similarly, the 3D multi-view feature representation is unsuitable as it restricts the pose of the object to a limited set of views. Therefore, an exact 3D representation is preferred. Little work has been done to date on identifying the pose of an exact 3D model from a single 2D image. Huttenlocher and Ullman @cite use a 3D model that contains the locations of edges. The edges contours identified in the 2D image are matched against the edges in the 3D model to calculate the pose of the object. The method has been implemented for simple 3D objects. However, it is unclear if this method will work well on objects with rounded surfaces without clearly identifiable edges.
- Image gradients Gray scale image gradients have been used to estimate the 3D pose in traffic video footage from a stationary camera by Kollnig and Nagel @cite . The method compares image gradients instead of simple edge segments, for better performance. Image gradients from projected polyhedral models are compared against image gradients in video images. The pose is formulated using 3 degrees of freedom; 2 for position and 1 for angular orientation. Tan and Baker @cite use image gradients and a Hough transform based algorithm for estimating vehicle pose in traffic scenes, once more describing the pose via 3 degrees of freedom. Pose estimation using 3 degrees of freedom is adequate for traffic image sequences, where the camera position remains fixed with respect to the ground plane. However, this approach does not provide a full pose estimate required for a general purpose application.
- Implicit Shape Models Recent work by Arie-Nachimson and Ronen Basri @cite makes use of implicit shape models' to recognize 3D objects from 2D images. The model consists of a set of learned features, their 3D locations and the views in which they are visible. The learning process is further refined using factorization methods. The pose estimation consists of evaluating the transformations of the features that give the best match. A typical model requires around 65 images to be trained. Many different vehicle models exist and new ones are manufactured frequently. Hence, methods that require training vehicle models are too laborious and time consuming for our work.
- Feature-based methods @cite @cite attempt to simultaneously solve the pose and point correspondence problems. The success of these methods are affected by the quality of the features extracted from the object. Objects like vehicles have large homogeneous regions which yield very sparse features. Also, the highly reflective surfaces in vehicles generate a lot of false positives. Our method on the contrary, does not depend on feature extraction.
- Distance metrics can be used to represent a distance between two data sets, and hence give a measure of their similarity. Therefore, distance metrics can be used to measure similarity between different 2D images, as well as 2D images and 2D projections of a 3D model. A basic distance metric would be the or the 2-norm @math . However, this has the disadvantage of being dependent on the scale of measurement. We use the @cite for our work, which is a scale-invariant distance measure. It is used by @cite for clustering. It is also used by Deriche and Faugeras @cite to match line segments in a sequence of time varying images.
- Within the first category, XSEarch @cite presents a query semantics that returns only those XML fragments, the result nodes of which are , i.e., intuitively belong to the same entity. In order to check this, they examine whether a pair of result nodes has two different ancestor nodes that have the same label (e.g., two nodes with label author'', s.th. the first keyword belongs to author1 and the second one to author2).
- @cite proposes an extension of the XML query language XML-QL by keyword search. In order to speed-up the keyword search, it computes the so-called inverted file'' for the XML document -- a set of inverted element lists -- and stores the contents within a relational database.
- Similar to XRANK @cite is the stack-based approach presented in @cite . In contrast to the previous stack-based appraoches, the authors do not used the DeweyID to identify a node and to calculate the ancestor-descendant or even parent-child relationships, but they propose to use a combination of preorder position, postorder position, and depth of the node.
- XKSearch @cite is an indexed-based approach to compute the LCA. They store inverted element lists consisting of DeweyIDs of the nodes. They start searching for the results at node @math of the shortest relevant keyword list, and they check for the other keyword lists whether the node @math being the next node to the left of @math or the node @math being the next node to the right of @math has a smaller distance to @math . Then, they use @math and the nearest node ( @math or @math ) to compute the LCA.
- @cite presents an anchor-based approach to compute the SLCA. From the set of current nodes of each relevant keyword list, they search the so-called anchor, i.e., that node that is closest to all current nodes. As soon as an anchor is identified, they try to exchange each node @math of each other keyword list @math by the next node next( @math ) of @math , in order to check, whether next( @math ) is closer to the anchor than @math and whether next( @math ) defines a new anchor. Finally, the set of anchor nodes form the set of LCA candidates that do not have another LCA candidate child is then reduced to the set of SLCA nodes.
- JDeweyJoin @cite returns the top-k most relevant results. They compute the results bottom-up by computing a kind of join on the list of DeweyIDs of the nodes in the inverted element list. Whenever they find a prefix that is contained in all relevant element lists, the node with this prefix as ID is a result candidate. In addition, they use a weight function to sort the list entries in such a way, that they can stop the computation after k results, returning the top-k most relevant results.
- @cite @cite @cite , the authors develop a way to sample approximately from a posterior distribution when only a small randomized mini-batch of data is used at each step. @cite , the authors used a hypothesis test to decide whether to accept or reject proposals using a small set of data (adaptively) as opposed to the exact Metropolis-Hastings rule. This reduces the amount of time required to compute the acceptance ratio. Since all of these algorithms are still sequential, they can be directly used in our algorithm to generate subposterior samples to further speed up the entire sampling process.
- Several parallel MCMC algorithms have been designed for specific models, such as for topic models @cite @cite and nonparametric mixture models @cite . These approaches still require synchronization to be correct (or approximately correct), while ours aims for more general model settings and does not need synchronization until the final combination stage.
- Consensus Monte Carlo @cite is perhaps the most relevant work to ours. In this algorithm, data is also portioned into different machines and MCMC is performed independently on each machine. Thus, it roughly has the same time complexity as our algorithm. However, the prior is not explicitly reweighted during sampling as we do in Eq , and final samples for the full posterior are generated by averaging subposterior samples. Furthermore, this algorithm has few theoretical guarantees. We find that this algorithm can be viewed as a relaxation of our nonparametric, asymptotically exact sampling procedure, where samples are generated from an evenly weighted mixture (instead of each component having weight @math ) and where each sample is set to @math instead of being drawn from @math . This algorithm is one of our experimental baselines.
- There is a well known duality between the voter model and coalescing random walks, which can be exploited to obtain bounds on the time to reach consensus in the voter model. On the complete graph, it shows that the expected time to reach consensus scales linearly in the number of nodes. Recent work in @cite has used this approach to derive bounds on the consensus time in general graphs in terms of the number of nodes, the variance of node degrees, and the spectral gap of the transition probability matrix for the random walk associated with the voter model. Unfortunately, the duality with random walks does not extend to the generalised voter model considered in this paper. There also doesn't seem to be a natural martingale associated with it; in the voter model, this martingale is intimately related to the dual random walk. Hence, we need a quite different approach for our model, which doesn't extend to general graphs.
- Consensus can also be reached without using variants of voter models. Instead, agents could simply count the number of agents with values 0 and 1, and thereby choose the majority. It is shown in @cite that such counting problems can be approximated using a gossip algorithm which involves propagating real numbers rather than values from a finite set. If the approximation error in counting is smaller than the margin between the 0 and 1 votes, then this leads to the correct decision regarding the initial majority value. The time required by this approximate counting algorithm is shown in @cite to scale logarithmically in the number of nodes in complete graphs, whereas on general graphs, it can be bounded in terms of the expansion properties of the graph.
- On the negative side, Nemhauser and Wolsey @cite showed that it is impossible to improve upon the bound of @math in the value oracle model, even under a single cardinality constraint. In this model, @math is given as a value oracle and an algorithm can evaluate @math on only a polynomial number of sets. Feige @cite showeds that @math is the best possible approximation even when the function is given explicitly, unless @math . In later work, Vondr ' a k @cite introduced the notion of the of a submodular function @math , which unifies many inapproximability results in the value oracle model, and proved new inapproximability results for some specific constrained settings. Later, Dobzinski and Vondr 'ak @cite showed how these inapproximability bounds may be converted to matching complexity-theoretic bounds, which hold when @math is given explicitly, under the assumption that @math .
- Conforti and Cornu ' e jols @cite defined the @math of non-decreasing submodular function @math as They showed that greedy algorithm has an approximation ratio of @math for the problem of maximizing a nondecreasing submodular function with curvature at most @math subject to a single matroid constraint. In the special case of a uniform matroid, they were able to show that the greedy is a @math -approximation algorithm. Later, Vondr ' a k @cite considered the continuous greedy algorithm in the setting of bounded curvature. He introduced the notion of , which is a weaker notion than total curvature, and showed that the continuous greedy algorithm is a @math -approximation for maximizing a nondecreasing submodular function @math subject to an arbitrary matroid constraint whenever @math has curvature at most @math with respect to the optimum. He also showed that it is impossible to obtain a @math -approximation in this setting when evaluating @math on only a polynomial number of sets. Unfortunately, unlike total curvature, it is in general not possible to compute the curvature of a function with respect to the optimum, as it requires knowledge of an optimal solution.
- We shall also consider the problem of minimizing nonincreasing functions @math . By analogy with total curvature, Il'ev @cite defines the @math of a nonincreasing supermodular function. His definition, which is stated in terms of the marginal of the function, is equivalent to when reformulated in terms of marginal gains. He showed that, in contrast to submodular maximization, the simple greedy heuristic does not give a constant factor approximation algorithm in the general case. However, when the supermodular function @math has total curvature at most @math , he shows that the reverse greedy algorithm is an @math -approximation where @math .
- One value related to the ability to approximate TSP is the integrality gap, which is the worst-case ratio between the optimal solution for a TSP instance and the solution to a linear programming relaxation called the subtour relaxation @cite . A long-standing conjecture (see, e.g., @cite ) for Metric TSP is that the integrality gap is @math . One source of motivation for studying Graphic TSP is that the family of graphs with two vertices connected by three paths of length @math has an integrality gap that approaches @math . This family of graphs demonstrates that Graphic TSP captures much of the complexity of the more general Metric TSP problem.
- For several decades, Graphic TSP did not have any approximation algorithms that achieved a better approximation than Christofides' classic @math -approximation algorithm for Metric TSP @cite , further motivating the study of this problem. However, a wave of recent papers @cite @cite @cite @cite @cite @cite @cite have provided significant improvements in approximating Graphic TSP. Currently, the best known approximation algorithm for Graphic TSP is due to Seb o and Vygen @cite , with an approximation factor of @math .
- From the algorithmic side, the studying the complexity of maximization problems parameterized above polynomial-time computable lower bounds is an active area of research. Since the influental survey by @cite , research in this area has led to development of many new algorithmic techniques for fixed-parameter algorithms: algebraic methods @cite @cite , probabilistic methods @cite @cite , combinatorial methods @cite @cite , and methods based on linear programming @cite .
- In particular, our recovery algorithm coincides with @cite @cite @cite when applied to simultaneously sparse and low-rank matrices. When the signal is further assumed to be exactly sparse of sparsity level @math , the pioneering work @cite showed that @math measurements suffice; this result is extended to accommodate sub-Gaussian measurements and approximately sparse signals by our work using a much simpler approach.
- While a great amount of new experimental data appeared, modern high-energy physics requires extremely resource-intensive computations, in particular Monte Carlo lattice simulations. Therefore, only big scientific collaborations, which have enough computation time on supercomputers can run such simulations. As usual such collaborations (UKQCD, USQCD, TWQCD, PTQCD, etc.) have own software packages for simulations. The most well-known among them are: FermiQCD : an open C++ library for development of parallel Lattice Quantum Field Theory computations @cite , MILC : an open code of high performance research software written in C for doing @math lattice gauge theory simulations on several different (MIMD) parallel computers @cite , QDP++ Chroma : package supporting data-parallel programming constructs for lattice field theory and in particular lattice QCD @cite .
- The first paper relating application of GPUs in HEP lattice simulations was published in 2007 @cite . The authors of this work used OpenGL as programming language and for the first time denoted the need to store lattice data in the form of four-component vectors. Shortly after of the publication, NVIDIA unveiled a new architecture CUDA and OpenGL ceased to be used as a GPGPU-computation language in further works.
- Recently some open-source software packages have been developed targeted to use GPUs: QUDA : a library for performing calculations in lattice QCD on CUDA-ready GPUs @cite , PTQCD : a collection of lattice SU(N) gauge production programs on CUDA-ready GPUs @cite , cuLGT : code for gauge fixing in lattice gauge field theories with CUDA-ready GPUs @cite , and several other packages with closed-access source codes ( @cite , @cite , @cite , @cite , @cite , @cite ). Some HEP collaborations link special GPU-libraries for their projects to engage GPGPU computing possibility without code refactoring. In particular, MILC collaboration uses QUDA package @cite , but only in single-device mode now. USQCD collaboration also has powered its QDP++ Chroma software with CUDA @cite .
- @cite @cite @cite @cite @cite discuss parallel distributed verification of (LTL) formulas. They aim at increasing the memory available and reducing the overall time required by LTL formulas verification by employing distributed techniques for searching accepting cycles in B "ichi automata. Distributed and parallel model checking of CTL logic was also proposed. @cite introduced a CTL model checking technique which works by splitting the given state space into several partial state spaces''. Each computer involved in the distributed computation owns a partial state space and performs a model checking algorithm on this incomplete structure. To be able to proceed, the border states are augmented by assumptions about truth values of formulas and the computers exchange assumptions about relevant states to compute more precise information. Other approaches were introduced in @cite @cite .
- The main idea of distributed algorithms for both LTL and CTL model checking is in fact similar: the state graph is partitioned among the network nodes, each network node owns a subset of the state space. The differences are in the way the state space is partitioned (through a ): this is a crucial issue. In order to increase performance of the parallel model checking, it is key to achieve a good load balancing among machines, meaning that each partition should contain nearly the same number of states. The performance of these algorithms depends also on the number of cross-border transitions of the partitioned state space ( transitions having the source state in a component and the target state in another component). This number should be as small as possible, since it has an effect on the number of messages sent over the network during the analysis @cite . In the context of LTL model checking, probabilistic techniques to partition the state space have been used, for example, in @cite @cite , and a technique that exploits some structural properties derived from the verified formula has been proposed in @cite .
- A reformulation of in terms of frequencies instead of supports has been introduced in @cite @cite with the name @math . The two problems are equivalent and have been shown to be in @math and @math -hard. The basic version of the frequency formulation, called , does not fix the number @math of transaction in a feasible database -- the corresponding decision problem has been proved to be @math -complete. A further variant of the problem has been introduced in @cite @cite with the name @math : all itemsets may occur as transactions in @math at most a fixed number of times ( @math ). This problem is in @math and @math -hard.
- A simple solution to exclude unexpected frequent itemset from a feasible solution is the formulation proposed in @cite , which is called @math : only itemsets in @math can be included as transactions in @math . The decision complexity of this problem is @math -complete as stated in @cite and proved in the Appendix. The version of with infrequency support constraint ( @math for short), has been recently proposed in @cite and its decision complexity is @math -complete as proven in @cite .
- There also is a recent body of work studying the phase retrieval under sparsity assumptions about the signal we wish to recover, see @cite @cite @cite @cite @cite as well as the references therein. Finally, a different line of work @cite @cite studies the phase retrieval by polarization, see also @cite for a related approach. This technique comes with an algorithm that can achieve recovery using on the order of @math specially constructed masks codes in the noiseless case. However, to the extent of our knowledge, PhaseLift offers more flexibility in terms of the number and types of masks that can be used since it can be applied regardless of the data acquisition scheme. In addition, when dealing with noisy data PhaseLift behaves very well, see Section below and the experiments in @cite .
- Many MIML approaches were proposed during the past few years. For example, MIMLSVM @cite degenerated the MIML problem into single-instance multi-label tasks to solve. MIMLBoost @cite degenerated MIML to multi-instance single-label learning. A generative model for MIML was proposed by @cite . Nearest neighbor and neural network approaches for MIML were proposed in @cite and @cite , respectively. @cite proposed a hidden conditional random field model for MIML image annotation. @cite proposed to optimize ranking loss for MIML instance annotation. In @cite , the authors tried to discover what patters trigger what labels in MIML learning by constructing a prototype for each label with clustering. Existing MIML approaches achieved success in many applications, most with moderate-sized data owing to the high computational load. To handle large-scale data, MIML approaches with high efficiency are demanded.
- Network migration has been studied using and models. In the system dynamics approach, the migration problem is treated as a dynamic system @cite , where the rate of migration depends on number of migrated agents in the system. On the other hand, in an agent-based approach @cite , the system consists of an ensemble of agents, each trying to increase its own utility. Such studies have mostly been conducted for IPv6 @cite and secure BGP @cite .
- A choice of the prediction interval for the demand and cost forecast has been studied in @cite , where it was shown that short prediction intervals may not be able to sufficiently account for future evolutions, while long prediction intervals may be uncertain. An efficient ant colony meta heuristic for the multi-layer, multi-period migration problem formulated as a path-finding problem is proposed in @cite . Stochastic programming approaches have also been proposed to better handle the uncertainties, as in @cite . Timing issues of migration to a new technology have also been studied, and it was shown that demand growth, migration cost, and cost savings from the new technology must be considered @cite . @cite , the number and location of SDN controllers is studied, which is an important related aspect, but outside the scope of this paper.
- This paper extends our recent studies on network migration. In @cite , we proposed an agent-based model to study benefits of joint migration to multiple technologies, on a case study of Path Computation Element (PCE) and SDN. In @cite , we defined the SDN , and found optimal solution. In this paper, however, our focus is entirely on devising computationally inexpensive but effective heuristics to schedule migration to a single technology, with consideration of techno-economic factors, such as limitations on CapEx investment. To this end, we propose novel greedy algorithms and establish their practical impact in terms of computational complexity, run times, network capacity gains, etc.
- The @math constraint @cite @cite can be represented by a regular counting constraint, but it would require non-unary signature constraints.
- The relation between language and cognition has received considerable attention over the years, mainly on answering whether language impacts thought, and if so, to what extent. Experiments with colour categories have been used both to show that language has an effect on thought @cite @cite and that it does not @cite . However, that line of work does not explicitly deal with word--colour associations. In fact, we did not find any other academic work that gathered large word--colour associations. There is, however, a commercial endeavor---Cymbolism http: www.cymbolism.com about .
- The MRC Psycholinguistic Database @cite has, among other information, the imageability ratings for 9240 words. http: www.psy.uwa.edu.au mrcdatabase uwa .htm The imageability rating is a score given by human judges that reflects how easy it is to visualize the concept. It is a scale from 100 (very hard to visualize) to 700 (very easy to visualize). We use the ratings in our experiments to determine whether there is a correlation between imageability and strength of colour association.
- There have been several works on optimization with energy storages and renewable generation, and we briefly review some of them here. In @cite , the authors investigated the combined optimization of a wind farm and a pumped storage facility from the perspective of a generation company, using a two-step stochastic optimization approach. The optimization produces optimal bids for the day-ahead spot market, and optimal operation strategies of the facilities. The optimal planning of generation and energy storage capacity was not considered. Zhou @cite proposed a composite energy storage system that contains both high energy density storage and high power density storage. The proposed power converter configuration enables actively distributing demands among different energy storages. Brown provided an economical analysis of the benefits of having pumped storage in a small island system with abundant renewable energy, and proposed to find the optimal pumped storage capacity through linear programming. In @cite , the authors considered optimizing the rating of energy storage in a wind-diesel isolated grid, and demonstrated that high wind penetration potentially results in significant cost savings in terms of fuel and operating costs.
- This work is motivated by the trend towards Software-Defined Networking and in particularly the fast failover mechanism which supports the in-band masking of failures (see Section 5.8 of the OpenFlow 1.1 specification). However, as the convergence time of routing algorithms is often relatively high compared to packet forwarding speeds, ranging from 10s of milliseconds to seconds depending on the network @cite , many networks today incorporate some robustness already in the forwarding tables of a router or switch: Thus, robust routing concepts and link protection schemes have been studied intensively for many years, also outside SDN.
- Alternative solutions to make routing more resilient rely on special header bits (e.g., to determine when to switch from primary to backup paths, as in @cite , or to encode failure information to make failure-aware forwarding decisions @cite @cite ), or on fly table modifications @cite . Recently, @cite made an interesting first step towards a better theoretical understanding of resilient SDN tables. The authors prove that routing tables can provide guaranteed resilience (i.e., loop-freeness) against a failure, when the network remains connected.
- Along with understanding the nature of the social media, researchers analyzed user behavior on the social networks in general. By analyzing user activity click logs, aimed to get a better understanding of social interactions social browsing patterns @cite . Zhao and Rosson aimed to explore the reasons of how and why people use Twitter and this use's impact on informal communication at work @cite . Following the how and the why, attempted to answer the next question of what is the user-generated content is about by investigating personal weblogs to detect the effects of personality, topic type, and the general motivation in published blogs @cite . Yang and Counts investigated the information diffusion speed, scale and range in Twitter and how they could be predicted @cite .
- This in-depth analysis and study of the social media, its nature, the information dissemination patterns, and the user behavior and interaction paved the way for the researchers to have a better understanding of how the social media played a major role in narrating publicly significant events. These studies prove that user-generated content in social media is of crucial importance and can be considered the first draft of history. analyzed two natural hazard events (the Oklahoma grass fires and the red River floods in 2009) and how microblogging contributed in raising the situational awareness of the public @cite . Starbird and Palen analyzed how the crowd interact with politically sensitive context regarding the Egyptian revolution of 2011 @cite . in another study utilized collaborative filtering techniques for identifying social media users who are most likely to be on the ground during a mass disruption event @cite . investigated weblogs to examine societ al interactions to a disaster over time and how they reflect the collective public view towards this disaster @cite .
- Much recent work has focused on associating textual mentions with Wikipedia topics @cite @cite @cite @cite @cite @cite @cite . The task is known as , or . Most of the proposed solutions exploit two sources of information compiled from Wikipedia: the link graph, used to infer similarity measures between topics, and anchor text, to estimate how likely a string is to refer to a given topic.
- A few topic model-inspired approaches have been proposed for modeling entities @cite @cite @cite . Early work @cite presents extensions to LDA to model both words and entities; however, they treat entities as , not linked to a knowledge base. @cite @cite model a document as a collection of topic mentions, materializing as words or phrases, with topics being identified with Wikipedia articles. Kataria in particular investigate the use of the Wikipedia category graph as the topology of a hierarchical topic model. The main drawback of this proposal is its scalability both in terms of efficiency and topic coverage; they prune Wikipedia to a subset of approximately 60k entities, reporting training times of 2.6 days. Han & Sun carried out the largest experiment of this kind, training on 3M Wikipedia documents (and no graph), reporting training times of one week with a memory footprint of 20GB on one machine. Our goal is to provide full Wikipedia coverage and high annotation accuracy with reasonable training processing efficiency.
- There are several results on embedding metric spaces with low intrinsic dimension into low dimensional normed space with low distortion: Assouad @cite showed that the snowflakes of doubling metrics For @math , an @math -snowflake of a metric @math is the metric @math , that is, all distances are taken to power @math . embed with constant distortion into constant dimensional Euclidean space. In particular, this is a positive answer to question:d for this special case.
- The first impossibility result on dimension reduction in @math is due to @cite , who showed that there exists an @math -point subset of @math that requires @math for any @math -distortion embedding to @math . Following their work, there have been many different proofs and extensions of this result, using various techniques. The original @cite argument was based on linear programming and duality, then @cite gave a geometric proof. For the @math distortion regime, @cite used combinatorial techniques to show that the dimension must be at least @math (and also gave a different proof of the original result). Recently @cite applied an information theoretic argument to reprove the results of @cite @cite . As for linear dimension reductions, @cite showed a strong lower bound for @math with @math .
- The instances used by the papers mentioned above are based on recursive graph constructions. The papers of @cite @cite used the diamond graph, which has high doubling constant, but @cite showed that their proof can be extended to the Laakso graph, yielding essentially the same result but for a subset of low doubling dimension. For the @math space, there are also strong lower bounds, For instance, the metric induced by an @math -point expander graph (which is in @math as any other finite metric), requires dimension at least @math in any @math distortion embedding. which are based on large girth graphs @cite measure concentration @cite and geometric arguments @cite .
- There are few positive results for @math , such as @cite who showed that @math admits dimension reduction when the aspect ratio of the point set is bounded, and @cite used @math -stable distributions to obtain similar results for all @math , and the Mazur map to obtain a (relatively high-distortion bound) for @math . A weak form of dimensionality reduction in @math was shown by @cite .
- More recently, Hoffman extended a similar study of heuristic topology The terms topography and topology are used interchangeably in the literature when discussing heuristic functions. to general planning @cite @cite . On a set of @math well-established planning benchmarks, he enumerated topological heuristic features and domain properties to sort the benchmarks into a complexity taxonomy. This is similar to our ranking of search spaces based on complexity in Sections and , although Hoffman does not compute empirical correlations to performance. Hoffman concluded that the benchmark taxonomy would shed insight on the levels of success of heuristic search planning in these benchmarks. He also claimed that it could inform subsequent improvement of heuristic functions, allow prediction of planning performance and assist in developing more challenging benchmark sets.
- Another of the complexity measures we use to characterize search spaces is taken from the research of Mizusawa and Kurihara @cite . They successfully demonstrated a link between search performance in gridworld pathfinding domains and two hardness measures'': initial heuristic error and probability of solution existence. They define initial heuristic error for a search problem as @math where @math is the set of all states on some path connecting the start and goal states. The search spaces used by Mizusawa and Kurihara are generated randomly by making random cells in the gridworld untraversable. The percentage of untraversable cells is called the , @math . Since the obstacles are placed randomly, solutions are not guaranteed to exist for search problems. The entropy @math is used as their measure of likeliness of solution existence. Mizusawa and Kurihara demonstrated that @math , @math , and LRTA* and RTA* solution cost are all maximized at a similar obstacle ratio of approximately $41
- Previous work has been conducted to predict the performance of simpler real-time heuristic search algorithms. Citing important applications in planning (e.g., as part of the Heuristic Search Planner @cite and the Fast-Forward Planner @cite ), L o pez sought to model the efficiency of heuristic hill-climbing by modelling the algorithm as a Markov process @cite . Using several sizes of the sliding tile puzzle, the model could reasonably predict the likelihood that a hill-climbing agent reaches a target state in a number of moves equal to the initial heuristic value. L o pez stated that this model is useful not only as a predictor of simple real-time search performance, but as a gauge of heuristic accuracy.
- Recent research by @cite performed a preliminary assessment of the suitability of machine learning as a predictor of search algorithm performance. They discussed several machine learning techniques for predicting which search algorithm from a collection, or portfolio, would have the best performance for a given search problem. Their method uses past algorithm performance and search space features as training data for their models, similar to our use of complexity measures as input to a machine learning predictor presented in . However, to our knowledge, we present the first exploration of machine learning to predict the performance of a collection of real-time heuristic search algorithms.
- [label=( * )] The framework of @cite requires the link of each hop to have a certain outage probability. In practice, as the deployment agent walks away from the previously placed node, he can reach a point where even the maximum node power does not provide a link of the desired quality to the previous relay, and walking any farther is unlikely to provide a workable link. At this point the deployment is considered to have failed. In our present paper, we do not bound the outage probability of each hop but make the sum outage over all the hops a part of the optimization objective.
- In the framework of @cite , the deployment agent can only move forward. In the present paper we introduce backtracking," which permits the deployment agent to compare the link qualities over several potential placement locations before deploying the relay at any one of them.
- The STREAM @cite project focuses on computing approximate results and minimizing the memory requirement of continuous queries over data streams. The Aurora @cite system proposes mechanism to sacrifice result quality, based on user specified quality-of-service profiles, while sufficient resources to ensure scalability are not available. In contrast, we exploit inexpensive shared-nothing clusters to ensure scalability without sacrificing result accuracy. Reference @cite proposes a contract-based load management framework migrating workload among processing nodes based on predefined contracts. The Borealis project proposes a dynamic inter-operator load distribution mechanism by utilizing the operators' load variance coefficients @cite . StreamCloud @cite parallelizes a set of stream queries across a number of virtual machines in a cloud. Stormy @cite uses techniques from key-value stores and replication to provide a fault-tolerant service for processing streams in a cloud system. In comparison, our work consider intra-operator load distribution for window join queries with large states and high arrival rates.
- The Flux operator @cite extends the exchange operator @cite to support adaptive dataflow partitioning and load balancing while processing stateful operators ( e.g. joins, grouping operators) over a shared-nothing cluster. The Flux operator consists of two types of intermediate operators called Flux-Prod and Flux-Cons. The Flux-Prod operator stores stream tuples (from the sources) into a buffer, and distributes the stream tuples among a number of Flux-Cons operators, that are instantiated in each of the nodes processing the stream queries. The Flux operator provides a framework for partitioning dataflows over a number of nodes; however, it does not consider the sliding window joins over a shared nothing environment. Moreover, while implementing a dataflow operator over a number of processing nodes, maintaining and initiating communication among the nodes, over a reliable socket or TCP connection @cite , without any prior synchronization or without any predefined order of data exchange is infeasible, if not impossible.
- There is a lot of research on the maximum independent set problem for geometric shapes in the plane. For the case when all shapes are unit disks (i.e., the resulting intersection graph is a unit disk graph), polynomial time approximation schemes have been presented by Hunt @cite , and by Hochbaum and Maass @cite . Subsequently, Nieberg, Hurink, and Kern @cite presented a PTAS that does not even need the geometric representation of the graph. Observe here that it is @math -hard to decide whether a given graph is a unit disk graph @cite . Using the geometric embedding, Erlebach, Jansen, and Seidel presented a PTAS for disks with arbitrary diameters, which generalizes to arbitrary fat objects @cite .
- When going beyond the fat objects, the problem becomes much less understood. Even for axis-parallel rectangles the best known polynomial time algorithms are @math -approximation for weighted case due to Chan and Har-Peled @cite , and a @math -approximation for unweighted case by Chalermsook and Chuzhoy @cite . Prior to the latter results, many @math -approximation algorithms have been found @cite @cite @cite @cite . Very recently, a QPTAS has been presented @cite .
- For the general case, the best known result is a @math -approximation algorithm for collections of curves due to Fox and Pach @cite , which assumes that any two curves intersect only @math times, i.e., that they are @math -intersecting. Prior to this, Agarwal and Mustafa presented an algorithm for the special case of straight line segments that computes an independent set of size @math @cite , which yields a worst case approximation ratio of @math . Note that it is @math -hard to decide whether a given graph can be represented as an intersection graph of a collection of curves (i.e., is a string graph) @cite .
- The web is ever-changing and what one might share or post today might change or disappear tomorrow. Losing web resources and finding them again has been the scope of several studies. For digital libraries, Nelson and Allen analyzed the persistence and availability of objects in a digital library @cite . From the aspect of web decay Bar- @cite proposed a measure of decay and algorithms to compute it efficiently. Consequently, Klein and Nelson analyzed the loss and rediscovery of websites to pinpoint the reasons behind this behavior @cite .
- The problem of disappearing or changing resources has been well-studied. The changing aboutness of live web pages has been studied in the Walden's Path project @cite and the link vetting system @cite . For link rot, Kahle originally reported the expected lifetime of a web page is 44 days @cite . Loss of references and URIs appearing in the academic literature have been studied numerous times, with exact loss rates varying depending on the corpus @cite . In our Just-in-time'' preservation research we discovered new locations of web pages that are missing in the current web @cite . We investigated a variety of techniques, including using page titles @cite , tags DBLP:conf ercimdl KleinN11 , and lexical signatures @cite , all of which could be used as queries to search engines to find replacement copies of the missing web page.
- Computing the change rates of web resources is a well-studied phenomena. Cho and garcia-Molina studied the change rate of web pages to determine the best policies for web crawlers @cite , as well as studying how to handle late arrivers in a collection @cite . Other studies have been done about understanding the web content dynamics @cite and upon which to develop the crawl policies for enhancing archival coverage @cite .
- In regards to data collection, we are in need of a large data set that captures human temporal intention. To collect this, prior and during the phases of experimental design, we examined several publications depicting crowd sourcing @cite and most specifically Amazon's Mechanical Turk @cite which has been used in generating ground truth data for a similar-scoped study in detecting music moods @cite .
- As for the archiving aspect of our study, the existence of Memento, TimeMaps, and multi-archive aggregators has greatly facilitated research with archives. The motivation for the Memento Framework @cite is achieving a tighter integration between the current web and remnants of the web of the past. Archival versions (or mementos) of web resources do exist, both in special-purpose web archives such as the Internet Archive and the on-demand WebCite archive, or in version-aware servers such as content management systems (CMS, e.g. Wikipedia) and version control systems.
- There are also extensive research about keywords suggestion. In @cite the keywords are suggested based on concept hierarchy mapping therefore the suggestions are not limited to the bag-of-words of the webpage, and may expand to non-obvious ones which are categorized in bigger concepts. The work of @cite @cite recognise that the bid prices for hot keywords are high therefore would cost more, and try to find related non-obvious keywords that are cheaper. Although these keywords may have lower traffic, but when combined the traffic could match that of a hot one, while these keywords cost still less.
- The work of @cite proposed a classifier that uses multiple text features, including how often the term occurs in search query logs, to extract keywords for ads targeting based on logistic regression. The system discussed in @cite first generates candidates by several methods including a translation model capable of generating phrases not appearing in the text of the pages. Then candidates are ranked in a probabilistic framework using both the translation model favouring relevant phrases, as well as a language model favouring well-formed phrases. Another relevant work can be found in @cite . The authors proposed an exploration-exploitation algorithm of sorting keywords in an descending order of profit-to-cost ratio and adaptively identify the set of keywords to bid on based on historical performance, with a daily budget constraint. In this paper we try to find the keywords from the given webpage with additional web knowledge, rather than find profitable ones from a very large set (like 50k). Then we leave the matching between keywords and ads to display networks exchanges, such as Google AdSense.
- In @cite the authors propose a combination algorithm of using upper confidence bound (UCB) and @math -greedy to solve the exploration-exploitation dilemma. Their target is to select high profit ads which would be fed in contextual advertising platforms. The feature vectors of ads are not used in their system as side information, instead they use standard bandits considering the reward following an unknown stochastic process. Our research is partially inspired by their work.
- The online matching problem was originally analyzed by Karp, Vazirani and Vazirani, who introduced the algorithm and showed that it obtains a competitive ratio of @math @cite . Simpler proofs of the algorithm have since been found @cite @cite . A @math competitive algorithm is also known for vertex-weighted online bipartite matching, which was given by Aggarwal, Goel, Karande, and Mehta @cite .
- A routing protocol defines the rules for exchanging the information between nodes. In geographic routing protocols the decision to which neighbor the packet is sent is controlled by the position of the nodes and the distances between them. The position information to each node can be obtained either by devices such as GPS or Galileo (geographic coordinates) or by analyzing the network structure (virtual coordinates). Position awareness can often significantly improve the efficiency of routing. In @cite it is mentioned that protocols using position information for routing like MFR @cite , COP @cite , and GFG @cite are competitive alternatives to the classical routing protocols for wireless ad hoc networks as for example DSR @cite , AODV @cite , and OLSR @cite ).
- Most algorithms based on position awareness first try to deliver the packet using greedy techniques. For example, the simplest greedy routing technique @cite will forward the packet to the neighbor closest to the destination. Most Forward Routing (MFR) @cite and Nearest with Forwarding Progress (NFP) @cite consider the projected distance on the source-destination line. MFR tries to get closer to the destination by sending the packet to the neighbor with maximum projected distance, while NFP suggests to adjust the transmission power by sending the packet to the neighbor with the smallest projected distance.
- Greedy routing algorithms can easily be extended by taking into account power consumption. Power aware greedy routing in most cases tries to minimize the ratio of the energy consumed by a transmission to the progress made. The progress is the distance reduction towards the destination. This cost over progress (COP) power-aware framework is first introduced in @cite . If the cost is equal for all connections, we obtain the simple greedy algorithm as already discussed above @cite . If the cost of a connection is proportional to the distance between the nodes, the resulting routing is similar to compass routing @cite .
- There are several attempts to obtain delivery guarantee for greedy routing algorithms. The authors of @cite propose face routing , which guarantees delivery in two-dimensional unit disk graphs (UDG). Face routing is applied to a planar sub-network obtained by considering the Gabriel Graph @cite @cite , the Relative Neighborhood Graph @cite , or the Morelia Graph @cite . In @cite a greedy-face-greedy (GFG) approach is considered, where greedy routing is based on COP as in @cite and face routing is similar to the one in @cite . Energy-aware routing is also proposed in LEARN @cite , SPFSP @cite , End-to-End (EtE) @cite , and EEGR @cite .
- Landmark-based routing algorithms like VCap @cite , JUMPS @cite , GLIDER @cite , VCost @cite , and BVR @cite use virtual coordinates computed from the distances to specific nodes called landmarks , anchors , or beacons . In the first phase, a global and distributed election mechanism elects a set of nodes acting as landmarks. Then the landmarks flood the entire network or only parts of the network such that every node can compute its virtual coordinate depending on the distances to the landmarks. The virtual coordinates can then be used to route a message greedily through the network. Packet delivery is also not guaranteed if different nodes have the same virtual coordinates.
- There are also several attempts to obtain delivery guarantee for landmark-based greedy algorithms. Most of them are based on a tree coordinate system like LTP @cite and ABVCap @cite . An energy efficient approach is introduced in HECTOR @cite . This protocol mixes the use of the tree-based coordinate system of @cite and the landmark-based coordinate system of @cite and @cite .
- An alternative way for delivery guarantee can be obtained by hierarchical addressing, see for example @cite . Tsuchiya solves this problem by allowing nodes to self configure their addresses. The protocol uses a hierarchical set of landmark nodes that periodically send scoped route discovery messages. A node's address is the concatenation of its closest landmark at each level in the hierarchy. The overhead of route setup can be reduced to @math and nodes only hold state for their immediate neighbors and their next hop to each landmark. However, this requires a protocol that creates and maintains this hierarchy of landmarks and appropriately tunes the landmark scopes. Recent proposals adopting this approach have been fairly complex @cite in contrast to our design goal of configuration simplicity, see also @cite for an overview.
- Our second greedy routing protocol is based on virtual coordinates which we will define by four landmark nodes denoted by @math , @math , @math , and @math . These four landmark nodes are selected similarly as in VCap (virtual coordinate assignment protocol) from @cite . The first landmark node @math is one of the nodes with maximum @math -distance to an arbitrary node @math . The second landmark node @math is one of the nodes with maximum @math -distance to @math . The third landmark node @math is one of the nodes for which [d^ (C,A)+d^ (C,B)-2 |d^ (C,A)-d^ (C,B)| ] is maximum. And finally, the fourth landmark node @math is one of the nodes for which [d^ (D,C)-|d^ (D,A)-d^ (D,B)| ] is maximum. Since will consider energy efficient routing, we use the @math -distances instead of hop distances @cite for the computation of the virtual addresses.
- Various misuses of the SSL protocol are spread both in the desktop environment and in the mobile environment, exposing private data (potentially sensible) to malicious attacks. In particular, @cite analysed the SSL usage across various environments, only to find out that this protocol's implementation is completely broken in many security-critical applications and libraries''. Meanwhile, @cite analysed the SSL usage on 13,500 Android applications, and found out that a large percentage of them suffer from SSL vulnerabilities, which expose them to dangerous man-in-the-middle attacks. To add it up, some of these applications (such as PayPal and Facebook) are very popular, covering up to 185 million users. Both studies just gave some advices to developers, but did not mention any solution to the SSL usage problem.
- XML stream validation is first discussed by Segoufin and Vianu @cite . @cite introduce VPA as executable model for XML that captures the entire class of regular tree languages. @cite extend VPA for approximate XML validation and @cite present an XML Schema framework using VPA.
- For a survey of grammatical inference we direct the reader to the book of de la Higuera @cite . Fernau @cite introduces function distinguishable languages and we apply this concept in Section for state merging. @cite mention that query learning VPA with counterexamples is possible but our setting is different.
- Several results on DTD inference from XML have been published @cite @cite @cite @cite , but we aim for the strictly larger class of XSDs. Ml ' y nkov ' a @cite presents a survey of XSD inference. The general idea is to start with an extended context-free grammar as schema abstraction, inferred from examples, and merge non-terminals @cite . @cite and Chidlovskii @cite also handle datatypes in their presented methods. Our approach is similar to @cite . Their algorithms use tree automata for learning @math -local Single Occurrence XSDs in a probabilistic setting but without datatypes.
- In the field of information retrieval, @cite and @cite give algorithms to infer HTML wrappers as tree automata. Regarding intrusion detection, @cite introduce approximate tree kernels as a similarity measure for trees and use them for anomaly detection in HTML.
- @cite were among the first, in 1996, to analyze consumers' perspectives as to genetic discrimination and reported that people cited fear of losing insurance as a major reason to avoid genetic testing. However, discrimination by insurance companies was not a widespread reality in the 90s, as few of these cases had been filed and even fewer had been won @cite . Many states have passed laws to protect medical (and also genetic) information, such as the Health Insurance Portability and Accountability Act (HIPAA), which provides a general framework for sharing and protecting Protected Health Information. In the U.S., there also exists legislation specific against genetic discrimination -- the Genetic Information Nondiscrimination Act (GINA) -- which prohibits discrimination on the basis of genetic information with respect to health insurance and employment @cite . However, neither GINA or HIPAA placed any limits on health insurance rate setting.
- Also, prior work analyzed the issue of labor discrimination in relationship to disease predisposition, mostly from the legal standpoint. We refer to the work by @cite for details, along with a review of well-known rulings.
- @cite compared the attitude of 279 patients from the United States and Spain who had volunteered to donate a sample for genomic research, and showed that 48 that genetic information poses special risks (69.7 @cite explored the attitude of research participants and possible future participants regarding Genome-Wide Association Studies (GWAS): they found out that participants expressed a variety of opinions about the acceptability of wide sharing of genetic and phenotypic information for research purposes through large, publicly accessible data repositories. Most believed that making de-identified study data available to the research community was a social good to be pursued. Privacy and confidentiality concerns were common, although not necessarily precluding participation. Also, many participants voiced reservations about sharing data with for-profit organizations.
- In @cite the authors propose a novel MAC design with opportunistic MU-MIMO scheduling based on channel sounding feedback, where packets are selected depending on their transmission duration and type of traffic. In @cite is also proposed a novel MAC design for MU-MIMO that focusses on issues such as MAC ACKing of MU-MIMO transmissions. Packets are scheduled for transmission using a weighted queuing mechanism that considers both packets acknowledgements and type of traffic. However, in both @cite @cite fairness and allocation of MU-MIMO transmission patterns amongst flows is not considered. The work in @cite focusses on packet aggregation in an IEEE 802.11ac AP, and considers a fixed MU-MIMO schedule where one flow is allocated per spatial stream. Regarding utility fairness in WLANs, in @cite is presented the first rigorous analysis of proportional fairness in 802.11 WLANs where transmissions are to a single destination.
- Heterogeneous information networks are special kinds of information networks which involve multiple types of nodes or multiple types of links. In a heterogeneous information network, different types of nodes and edges have different semantic meanings. The complex and semantically enriched network possesses great potential for knowledge discovery. In the data mining domain, heterogeneous information networks are ubiquitous in many applications, and have attracted much attention in the last few years @cite @cite @cite . @cite @cite studied the clustering problem and top-k similarity problem in heterogeneous information networks. studied a specialized classification problem on heterogeneous networks, where different types of nodes share a same set of label concepts @cite . However, these approaches are not directly applicable in collective classification problems, since focus on convention classification tasks without exploiting the meta path-based dependencies among objects.
- The concept of network evolution of a technological infrastructure is considered in @cite where a model for the evolution of the Internet Autonomous System (AS) is provided. As most works (e.g., @cite @cite ), this one focuses on creating a model of the existing evolution of the Internet rather than proposing new ways of evolving networks. The interesting aspect on the modeling of the Internet is the better results of the proposed model Parallel Addition and Rewiring Growth (PARG) compared to other models in the literature to capture specific features of the AS topology. In particular, the model is able to capture the dissortativeness of hubs (i.e., the most connected nodes in the network) and linear local assortativeness of nodes. The model proposed proves successful, however one must recognize that the AS connection are mainly logical and are due more to business and contractual agreements than physical interconnections, therefore missing part of the problem that is essential in the case of the topology.
- The framework of partial knowledge at each user is similar in spirit to a @cite @cite . There, the goal is to design competitive strategies that lead to a Nash equilibrium. This is significantly different from the goal of the current paper. The current paper is not concerned with competition or equilibrium. Rather, there is a single utility function that all users desire to maximize. Distributed algorithms are developed to maximize time average utility subject to time average penalty constraints.
- This paper shows that an optimal distributed algorithm can be designed by having users correlate their decisions through an independent source of common randomness (Section ). Related notions of commonly shared randomness are used in game theory to define a , which is typically easier to compute than a standard Nash equilibrium @cite @cite @cite @cite . For the current paper, the shared randomness is crucial for solving the distributed optimization problem. This paper shows that optimality can be achieved by using a shared random variable with @math possible outcomes, where @math is the number of penalty constraints. The solution is computable through a linear program. Unfortunately, the linear program can have a very large number of variables, even for 2-user problems. A reduction to polynomial complexity is shown to be possible in certain cases (Section ). This paper also develops an online algorithm that chooses pure strategies every slot based on a set of weights that are updated at the end of each slot (Section ). The online technique is based on Lyapunov optimization concepts @cite @cite @cite .
- Much prior work on network optimization treats scenarios where it is possible to find distributed solutions with no loss of optimality. For example, network flow problems that are described by linear or separable convex programs can be optimally solved in a distributed manner @cite @cite @cite @cite . Problems where network nodes want to average sensor data @cite or compute convex programs @cite have distributed solutions. Work in @cite solves for an optimal vector of parameters associated with an infinite horizon Markov decision problem using distributed agents. Work in @cite @cite @cite develops distributed multiple access methods that converge to optimality. However, the above problems do not have random events that create a fundamental gap between centralized and distributed performance.
- Recent work in @cite derives structural results for distributed optimization in Markov decision systems with delayed information. Such problems exhibit gaps between centralized and distributed scheduling. The use of in @cite is similar in spirit to the assumption in the current paper that each user observes its own random event @math . The work @cite derives a sufficient statistic for dynamic programming. It does not consider time average constraints and its solutions do not involve correlated scheduling via a pseudorandom sequence. Recent work in @cite considers distributed reporting of events with different qualities, but considers a more restrictive class of policies that do not use correlated scheduling. The current paper treats a different model than @cite and @cite , and shows that correlated scheduling is necessary in systems with constraints. Further, the current paper provides complexity reduction results under a preferred action property (Section ) and provides an online algorithm that does not require a-priori knowledge of event probabilities (Section ).
- A rich line of research in statistics addresses the problem of identifying the non-zero entries in a sparse vector (or matrix) @math from observations @math where @math has typically i.i.d. standard Gaussian entries. In particular @cite @cite @cite @cite study cases in which the sparsity pattern of @math is structured'. For instance, we can take @math a matrix with @math if @math and @math otherwise. This fits the framework studied in this paper, for @math the complete graph and @math , @math . This literature however disregards computational considerations. Greedy search methods were developed in several papers, see e.g. @cite @cite .
- The algorithm we introduce for the case @math is analogous to the linearized BP' algorithm of @cite @cite , and to the approximate message passing (AMP) algorithm of @cite @cite @cite . These ideas have been applied to low-rank approximation in @cite . The present setting poses however several technical challenges with respect to earlier work in this area: @math The entries of the data matrix are not i.i.d.; @math They are non-Gaussian with --in general-- non-zero mean; @math We seek exact recovery instead of estimation; @math The sparsity set @math to be reconstructed scales sublinearly with @math .
- GS constraints are a subclass of CRC constraints. While path consistency is sufficient to solve a CSP over CRC constraints, we show that arc consistency is sufficient to solve a CSP over DS constraints (please refer to Theorem ). However, it is not known that whether US constraints are solvable using arc consistency. For the class of CRC constraints, a path consistency based algorithm was proposed in @cite that runs in @math time. Based on variable-elimination, @cite stated an @math time algorithm for CRC constraints where @math is the degree of elimination of the triangulated graph of the underlying constraint network. Both these algorithms for CRC constraints use arc consistency as a subroutine which takes @math time. US constraints being a subclass of CRC constraints can be solved by either of these techniques mentioned above. Our arc consistency algorithm for GS constraints that runs in @math time, acting as a subroutine, thus improves the time complexity of the algorithm by @cite to @math (which is linear in @math ), when applied to US constraints.
- Referring to the classes of max-closed and min-closed constraints, @cite showed that their solutions can be computed in @math time where @math is the maximum number of supports that a value in a domain can have. In this paper, we show that a constraint is a DS if and only if it is both max-closed and min-closed. Our DS algorithms have significantly lower time complexities for this restricted subclass.
- MAP queries in SRL models can be formulated as integer linear programs (ILPs). In this context, cutting plane inference (CPI) solving multiple smaller ILPs in several iterations has shown remarkable performance @cite . In each CPI iteration, only the ground formulas violated by the current intermediate solution are added to the ILP formulation until no violated ground formulas remain. Since CPI ground formulas satisfied by the evidence, it can be seen as a generalization of pre-processing approaches that count the formulas satisfied by the evidence @cite . In the context of max-margin weight learning for MLNs @cite the MAP query was formulated as a linear relaxation of an ILP and a rounding procedure was applied to extract an approximate MAP state. 's ILP formulation requires less constraints for ground clauses with negative weights and it combines CPI with cutting plane aggregation.
- There are several lifted inference approaches such as lifted message passing @cite @cite , variants of lifted knowledge compilation and theorem proving @cite @cite , and lifted MCMC @cite @cite approaches. While there are some generic parallel machine learning architectures such as @cite which could in principle be used for parallel MAP inference, is the first system that parallelizes MAP inference in SRL models combining CPI and CPA.
- There is an important literature dedicated to the limit system, that is the Oberbeck-Boussinesq equations , and , under various hypotheses over the coefficients @math and @math and the potential @math (although the most common assumption is that @math ). Loosely speaking the classical results concerning the existence issue are (see e.g. @cite @cite @cite and the references therein): Dimension @math : Global existence of strong solutions if @math Dimension @math with @math : Global weak solutions and local strong solutions (which become global if the data are small). Dimension @math with @math : only local-in-time strong solutions are available.
- As discussed in the introduction, a plethora of literature exists on theta graphs and their applications, though most of this work focuses on worst-case analysis. One notable exception is the work of Devroye al @cite who study the maximum degree of theta graphs and show that, if @math is a set of @math points independently and uniformly distributed in a certain unit square, then @math has maximum degree concentrated around @math . Devroye al actually consider the closely-related Yao graphs @cite @cite , but their proofs apply, almost without modification, to theta graphs.
- In contrast, properties of other proximity graphs of random point sets have been studied extensively: Devroye @cite presents a general theorem for obtaining exact leading constants for the expected degree of a number of proximity graphs over point sets drawn from a large class of distributions. This theorem can be applied to Gabriel graphs, relative neighbourhood graphs, and nearest-neighbour graphs. This work [Section 7] devroye:expected also mentions directional nearest-neighbour graphs,'' now commonly known as Yao graphs @cite @cite , and points out that the general theorem does not apply to these (nor does it apply to theta graphs---for the same reasons).
- Bern al @cite study the maximum degree of Delaunay triangulations of random point sets. Devroye al study the maximum degree of Gabriel graphs @cite of random point sets. Arkin al @cite study the length of the longest edge in Delaunay triangulations of random point sets.
- The first studies on model-based reinforcement learning for dialogue management have concentrated on learning from a fixed corpus via Dynamic Programming methods @cite @cite @cite . The literature also contain some recent work on Bayesian techniques. @cite presents an interesting approach that combines Bayesian inference with active learning. @cite is another related work that utilises a sample of solved POMDP models. Both employ offline solution techniques. To our knowledge, the only approaches based on online planning are @cite @cite , although they focussed on the estimation of the observation model. It is worth nothing that most POMDP approaches do integrate statistically estimated transition models in their belief update mechanism, but they typically do not exploit this information to optimise the dialogue policy, preferring to employ model-free methods for this purpose @cite @cite .
- In the Introduction we stated that there are several works studying internet memes. For example, @cite studies the dynamics of competitions of memes spreading in two distinct social networks on different social media (Facebook and Twitter). Also @cite addresses the problem of competition among memes, assuming that each user can follow only a handful of memes at the same time. Meme and information spread is also a problem definition addressed in computer science with different methodological approaches: works like @cite , @cite and @cite provide algorithms and frameworks to analyze how information spreads in a network of people. While those papers examine different cascade information spreads as independent, in @cite different spread events on the same network are analyzed at the same time, as different memes influence each other while trying to span on the same set of minds. Other computer science tools help us tracking memes over the Web @cite . As an application, @cite deals with cooperative behavior.
- We do believe that the social network analysis behind meme spreading is interesting, however it leaves undescribed the fundamental characteristics of the memes themselves. While studying a system, it is important to know how the parts interact, but also how they function themselves, to have a better picture of what actually is happening in the world. Our paper tries to provide some contribution exactly in this last aspect: we do not consider network analysis, only the memes themselves. Closer to our work is @cite , but here the author does not address the issues of collaboration and competition in internet memes.
- Internet memes spread over social media websites. In computer science, many researchers have addressed the problem of describing the dynamics of user behavior in social media websites. The topics touched include the emergence of conventions in online social networks @cite , how to select the critical features in the amount of data generated in these websites @cite , the privacy aspects @cite , the follow'' and friendship dynamics in Twitter @cite and @cite , and many more.
- Finally, there are in literature also some publications about memetics, the proposed branch of science that should study memes. Some of the pioneering works are @cite , @cite and @cite . Our work is different from these examples in literature as we are focused particularly on internet memes. Our approach is more data driven, as internet memes are more easily traceable as they leave a measurable footprint, this makes our paper a further contribution w.r.t these works.
- There has been considerable interest in GPU computing in recent years. Some of the notable works include scan @cite , @cite , sorting @cite , and the like. Other modern architectures that have been studied recently include the IBM Cell and the multi-core machines. @cite have studied list ranking on the Cell architecture and show that by running multiple threads at each SPU, list ranking using the Hellman-JaJa algorithm can be done efficiently. Other notable works on the Cell architecture include @cite @cite . @cite have studied the spmv kernel on various multi-core architectures including those from Intel, Sun, and AMD. Since most of the above cited works do not involve hybrid computing, we do not intend to cite all such works in this paper and refer the reader to other natural sources.
- A recent work that motivated this paper is the work of @cite . In @cite , argue that GPU computing can offer on average only a 3x performance advantage over a multicore CPU on a range of 14 workloads deemed important for throughput oriented applications. Some of our workloads overlap theirs @cite . Their paper also generated a wide amount of debate on the applicability and limitations of GPU computing. Our view however is that it is not a question of whether GPUs can outperform CPUs or vice-versa, but rather what can be achieved when GPUs and CPUs join forces in a viable hybrid computing platform. Further, for the workloads that are included also in @cite , we provided our own GPU and CPU implementations. In workloads such as , we use novel ideas such as precomputing the transcendentals on the GPU for a pure GPU implementation that improve the performance beyond what is reported in @cite .
- Hybrid computing is gaining popularity across application areas such as dense linear algebra kernels @cite @cite @cite , maximum flows @cite , graph BFS @cite and the like. The aim of this paper is to however evaluate the promise and the potential of hybrid computing by considering a rich set of diverse workloads. Further, in some of these works, (cf. @cite @cite @cite ), while both the CPU and the GPU are used in the computation, one of the devices is idle and while the other is performing computation. In contrast, we seek solutions where both the devices are simultaneously involved in the computation.
- @cite proved that in the case of stochastic matrices with equal positive entries in each row (usually referred to as the equal neighbor model ,), and under assumptions A and B with @math , the agreement algorithm achieves asymptotic consensus when all the communication graphs are oriented. Their convergence result thus coincides with Theorem in the particular case of a synchronous multiagent system and with the equal neighbor model.
- After writing the proof of Theorem , we became aware of two recent papers both containing Theorem in the synchronous case. @cite , Hendrickx and Tsitsiklis showed that the agreement algorithm achieves asymptotic consensus under assumptions A, B with @math , D1 and the so-called cut-balance condition. In light of Proposition below, the latter condition turns out to correspond to the decentralized model. Independently, Touri and Nedi 'c @cite established a general convergence result for the infinite product of random stochastic matrices, and to do that, they first proved that this result holds in the deterministic case; see Lemma 5 in @cite . It is easy to see that this lemma actually coincides with our Theorem in the particular case of a synchronous multiagent system. In fact, our technique for proving the lemmas in Sections and infra specialized to @math , is similar to the one used for the proof of the deterministic result in @cite .
- @cite we have analyzed several of these ET implementations in detail and have introduced the notion of SETs and our SET library in particular. @cite we have extended our analysis to more ET-based libraries, focused on the optimization and vectorization capabilities for dense arithmetic, and presented performance results for the CG algorithm, which is fundamental for many applications. In this work we expand our analysis to sparse arithmetic and the sparse matrix-matrix multiplication (spMMM) in particular.
- Much work has been devoted to sparse matrix based algorithms and efficient implementations in the past. However, most publications deal with parallel sparse matrix-vector multiplication @cite @cite @cite , since it is of pivotal importance in solving sparse linear systems and sparse eigenvalue problems. While there has also been substantial work on sparse matrix-matrix multiplication, it mostly deals with execution and communication efficiency in the parallel case @cite @cite . Here, however, we only cover the sequential kernel any try to understand its features in a well-defined setting, and especially in the context of SET frameworks. Consequently, issues of load and communication balancing, which would be crucial in the parallel case, do not arise.
- Homophily, which refers to the tendency of similar individuals to link to each other, is a strong organizing principle of social networks. Numerous studies found that people tend to be friends with other who belong to the same socio-economic class @cite @cite , and they tend to follow others in social media who have similar interests @cite . In the context of information exchange on networks, this means that content users are exposed to in social media depends on their position in the network. Users embedded within a community of strongly tied individuals are likely to share information on topics that other community members are interested in, while users in brokerage positions that bridge different communities receive information on more diverse topics. In this paper, we show that the variance of topics to which a user is exposed to by his friends is highly related to the structure diversity of social network.
- Recently, researchers recognized that cognitive constraints are important to defining social interactions online and in the real world. The number of social relationships that people can maintain is limited to about 150 @cite . This is similar to the limit of the number of conversation partners that Twitter users have @cite . Cognitive constraints, specifically, divided attention, was also shown to limit the spread of information on Twitter @cite . We find a dependence of user activity on network structure that mirrors those imposed by cognitive constraints. We observe that user's activity rate initially increases with the number of friends, until reaching a maximum around 200 friends and then decreases somewhat. The cognitive limits constraints the potential exposures to new information as users have too much information recommended by many friends. Further we argued that the limited activities in high diverse network is either because their lack of interest in processing recommended information, or because they are less willing to devote the greater cognitive resources required to process this diverse information.
- In standard time series analysis, the squared loss is usually considered and the noise terms are assumed to be independent with bounded variance and zero-mean. In this specific setting, one can assume without loss of generality that the noise terms have identical Gaussian distribution (see @cite @cite @cite for more information). This allows the use of statistical methods, such as least squares and maximum likelihood based methods, for the tasks of analysis and prediction. However, when different loss functions are considered these assumptions do not hold in general, and the aforementioned methods are not applicable. We are not aware of a previous approach that tries to relax these assumptions for general convex loss functions. We note that there has been previous work which tries to relax such assumptions for the squared loss, usually under additional modelling assumptions such as -distribution of the noise (e.g., @cite @cite ). We emphasize that the independence assumption is rather strict and previous works that relax this assumption usually offer specific dependency model, e.g., as proposed by @cite for the ARCH model.
- Furthermore, an online approach that relies on regret minimization techniques was never considered for ARMA prediction, and hence regret bounds of the type we are interested simply do not exist. Yet, results on the convergence rate of the coefficient vectors do exist, and regret bounds can be derived from these results. E.g., in @cite such results are presented, and a regret bound of @math can be derived for the squared loss. We are not familiar of these kind of results for general convex loss functions.
- The papers of @cite , Yan @cite , and Kleinberg and Weinberg @cite study the performance of Sequential Posted Price Mechanisms (SPMs) for Bayesian single-parameter auctions, and relate the revenue obtained by SPMs to the optimal (non-posted-price) mechanism given by Myerson @cite . Our algorithm for stochastic probing also yields SPMs for Bayesian auctions where the feasible sets of buyers are specified by, e.g., @math -matroid intersection and unsplittable flow on trees. Our proof relates an LP relaxation of the optimal mechanism to the LP used for stochastic probing. Linear programs have been used to model optimal auctions in a number of settings; e.g., see Vohra @cite . @cite also used LP relaxations to obtain approximately optimal mechanisms in a Bayesian setting with multiple items and budget constrained buyers.
- There is a large body of work on temporal data management including relational databases (see, for example @cite and @cite for excellent surveys on the topic), RDF (e.g., @cite ) and XML documents (e.g., @cite , @cite ). Although maintaining deltas has also been used in such cases, the large scale and the logical model, being in our case a graph, introduces new problems.
- Collecting a sequence of versions of XML documents from the web is considered in @cite . The difference between two consecutive versions is computed and represented by complete deltas based on persistent identifiers assigned to each XML node, while only the current version of the document is maintained. To avoid the overhead of applying deltas to retrieve previous versions, in @cite , they merge all versions of XML data into one hierarchy where an element appearing in multiple versions is stored only once along with a timestamp. To handle temporal RDF data, temporal reasoning is incorporated into RDF in @cite , thus yielding temporal RDF graphs. Semantics were also defined for these graphs which include the notion of temporal entailment as well as a syntax to incorporate this framework into standard RDF graphs by adding temporal labels. Clearly, our approach is different in that it considers time with respect to graph evolution.
- Numerous algorithms and data structures have been proposed for processing graph queries on large graphs. GBASE @cite and Pregel @cite are two general graph management systems that work in parallel and distributed settings and support large-scale graphs for various applications. GBASE is based on a common underlying primitive of several graph mining operations, which is shown to be a generalized form of matrix-vector multiplication @cite , while Pregel is based on a sequence of supersteps that are applied in parallel by each node executing the same user-defined function that expresses the logic of a given algorithm and are separated by global synchronization points. In future work, we plan to explore such techniques for reconstructing snapshots in parallel.
- The most relevant to our work is perhaps the historical graph structure recently proposed in @cite . The authors consider a sequence of graphs produced as the graph evolves over time. Since the graphs in the sequence are very similar to each other, they propose computing graph representatives by clustering similar graphs and then storing appropriate differences from these representatives, instead of storing all graphs. Our approach is different in that we want to support a broad range of historical queries, not just queries that involve a single snapshot graph.
- There is also a large body of work that studies the evolution of real-world networks over time. In @cite , it was shown that for a variety of real-world networks, graph density increases following a power-low and the graph diameter shrinks. Works on specific networks such as Flickr @cite and Facebook @cite also study network growth and their results can be exploited to enrich our model.
- The work presented in this paper is also related to a general category of CF approaches that we refer to as . There, tags are exploited to improve recommendation in various ways, e.g., by integrating tags into traditional user-based CF and item-based CF @cite @cite @cite , by incorporating tags into probabilistic latent semantic model @cite to unify user-item relations and item-tag relations @cite @cite , and by using tag-based user correlations as a regularization for PMF @cite . More recently, another group of state-of-the-art approaches has employed tensor factorization techniques @cite for tag-aware recommendation. Under such approaches, item recommendations or tag recommendations are learned from the user, tag, item triplet ternary data directly @cite @cite . Given the advantages of using tags for improving recommender systems, our work in this paper goes a step further and exploits the potential of tags to introduce mutual benefits between different recommender domains.
- A similar approach is followed by the SymTA S tool @cite , which is based on the model @cite . Another interesting approach is the Modular Performance Analysis (MPA) @cite which is based on Real-Time Calculus @cite . In both cases, the analysis is compositional, therefore less complex than the holistic analysis; nevertheless, these approaches are not fully parametric, in the sense that it is necessary to repeat the analysis for every combination of parameters values in order to obtain the schedulability region.
- Also based on PTA is the approach proposed by @cite . Their work is based on the Inverse Method @cite and it is very general because it permits to perform analysis on any system parameter. However, this generality can be paid in terms of complexity.
- Several quality evaluation toolboxes have been recently proposed, like the implemented by Murthy and Karam @cite . However, in general, these applications are mainly restricted to codec testing and metric comparisons, because they do not consider multimedia transmission methods and the degradation and quality impact derived from this process.
- On the other hand, it is also remarkable the seminal work (EvalVid) proposed by Klaue @cite , a widely used tool-set for video transmission evaluation. This tool-set does actually include transmission, some quality of service (QoS) parameter estimation (packet loss, delay and jitter) and some bitstream parameter evaluations (frame loss rate and frame jitter). EvalVid video quality evaluation techniques include PSNR, Structural Similarity index (SSIM) @cite , Mean Opinion Score (MOS, mapped from PSNR) and Distortion in Inverval (DIV, mapped from MOS as described in @cite ).
- Estimating the available bandwidth to do admission control has been an active topic of research. In @cite @cite , the problem of determining the impact of contention to find the available bandwidth is studied for multi-hop wireless networks, while @cite studies the problem of bandwidth estimation at a node. In @cite , the problem of contention is taken into account under the implicit assumption that the interference and transmission range of a node are the same. Some heuristics to support QoS are presented in @cite @cite , but contention is ignored during admission control.
- The use of packet scheduling to guarantee QoS in multi-hop networks has been considered in @cite . Some solutions for admission control have be implemented centrally @cite @cite , or assume a specific wireless technology like TDMA @cite @cite @cite or CDMA over TDMA @cite @cite . Using implicit synchronization in CSMA CA networks, a performance similar to TDM is achieved in @cite .
- Under the assumption that requests can be split, @cite proposes a solution for multi-hop multichannel networks. If the requests cannot be split, a heuristic is presented using the least-congested, minimum-hop path to route requests.
- Graph spanners were introduced by Peleg and Schaffer @cite in 1989. Spanners have been extensively studied since then, and there are numerous applications involving spanners, such as algorithms for approximate shortest paths @cite @cite @cite , labeling schemes @cite @cite , approximate distance oracles @cite @cite @cite , routing @cite @cite @cite , and network design @cite .
- There are several algorithms for computing multiplicative and additive spanners in weighted and unweighted graphs. In unweighted graphs, for any integer @math , Halperin and Zwick @cite gave a linear time algorithm to compute a multiplicative @math -spanner of size @math , where @math is the number of vertices. Note that for @math one obtains a spanner with multiplicative stretch @math and with size @math : we will use this type of spanner in Theorem . Analogous results are also known for weighted graphs @cite @cite @cite .
- Compared to the preservers in @cite , we achieve sparser pairwise spanners with additive stretch @math for @math , and a sparser subsetwise spanners for @math . Interestingly, our sourcewise spanners are always sparser than the pairwise preservers from @cite .
- The joint optimization of the sensing and transmission strategies has been only partially addressed in the literature, even for simple CR scenarios composed of one PU and one SU. For example, in @cite @cite , the authors proposed alternative centralized schemes that optimize the detection thresholds for a bank of energy detectors, in order to maximize the . The optimization of the sensing time and the sensing time detection thresholds for a given missed detection probability and constant rate of one SU was addressed in @cite @cite and @cite , respectively. A throughput-sensing trade-off for a fixed transmission rate was studied in @cite . In @cite (or @cite ) the authors focused on the joint optimization of the power allocation and the equi-false alarm rate (or the sensing time) of a SU over multi-channel links, for a fixed sensing time (or detection probability). All the aforementioned schemes however are not applicable to scenarios composed of multiple SUs (and PUs). The case of multiple SUs and one PU was considered in @cite (and more recently in @cite ), under the same assumptions of @cite ; however no formal analysis of the proposed formulation was provided.
- There is a significant body of work on robots learning to manipulate autonomously, robots learning to perceive, perception for manipulation that exploits task structure, and active learning methods for using labeling efforts efficiently. In this section, we discuss the relationship between our work and current learning methods as well as work that has demonstrated the effectiveness of using task-relevant cues for perception in human environments. The research we present in this paper builds on our earlier workshop publication @cite .
- The paper @cite provides analysis of power law behavior for the rank distribution of contents; the distribution of most watched videos is found heavily skewed towards the most popular ones.
- @cite @cite used a simple set of program instructions to describe perceptron weight matrix showing that short programs can represent scalable solutions to problems that require a regular weight vector.
- A seminal crash-tolerant read write storage implementation assuming a majority of correct processes was presented in @cite . In the original single-writer variant of @cite , read operations always take 2 rounds between a client and servers with write-backs in the second round. On the other hand all write operations complete in a single round; in the multi-writer variant @cite , the second write round is necessary. Server state modifications by readers introduced by @cite are unavoidable; namely, @cite showed a @math lower bound on the number of servers that a reader has to modify in any wait-free linearizable storage. However, robust storage implementations differ in the strategy employed by readers: in some protocols readers write-back data (e.g., @cite @cite @cite @cite @cite @cite ) whereas in others readers only write metadata to servers (e.g., @cite @cite @cite ).
- Data write-backs are also not needed in case of robust storage implementations that feature single round reads and writes @cite . Namely, @cite presents fast single-writer crash-tolerant and BFT storage implementations in which readers only write metadata reading data in the single round of read and hence, without any write-back. However, fast implementations are fundamentally limited and cannot be optimally resilient, since the number of required servers is inherently linear in number of readers @cite . The limitation on the number of readers of @cite was relaxed in @cite , where a single-writer crash-tolerant robust linearizable storage implementation was presented, in which most of the reads complete in a single round, yet a fraction of reads is permitted to be slow'' and complete in 2 rounds.
- In the unauthenticated model, @cite ruled out the existence of optimally resilient robust Byzantine fault-tolerant storage implementation where all write operations finish after a single communication round. This explained the previous difficulties in reaching optimal resilience in unauthenticated BFT storage implementations where several protocols have used @math servers @cite @cite . Furthermore, @cite showed the impossibility of reading from a robust optimally resilient linearizable storage in two communication rounds; in addition, if operations perform a constant number of rounds, even reading in three rounds is impossible @cite . These results imply that the optimal latency of a robust optimally resilient and linearizable BFT storage in the unauthenticated model is 2 rounds for writes and 4 rounds for reads, even in the single writer case. This can be achieved by the regular-to-linearizable transformation of the regular @cite storage protocol of @cite . Hence, it is not surprising that other robust BFT storage protocols in the unauthenticated model focused on optimizing common-case latency with either an unbounded number of read rounds in the worst case @cite @cite or a number of read rounds dependent on the number of faulty processes @math @cite @cite .
- Clearly, there is a big gap between storage protocols that use self-verifying data and those that assume no authentication. Loft @cite aims at bridging this gap and implements erasure-coded optimally resilient linearizable storage while optimizing the failure-free case. Loft uses homomorphic fingerprints and MACs; it features 3-round wait-free writes, but reads are based on data write-backs and are only obstruction-free @cite , i.e., the number of read rounds is unbounded in case of read write concurrency. Similarly, our (PoW) incorporate lightweight authentication that is, however, sufficient to achieve optimal latency and to facilitate metadata write-backs. We find PoW to be a fundamental improvement in the light of BFT storage implementations that explicitly renounce linearizability in favor of weaker regularity due to the high cost of data write-backs @cite .
- With the advent of programmable graphics hardware that supports flexible shader programs, it became feasible to perform the ray integration on a per-pixel basis at interactive frame rates @cite @cite @cite . In the latter approach the data is converted to a 3D texture and a fragment shader is executed for each pixel that is covered by the projected bounding box of the data volume. The ray is parameterized in texture coordinates and the ray-integral is computed in the fragment shader. GPU-raycasting is particularly attractive for adaptive grids, as it does not suffer from the rendering artifacts inherent to slice-based methods, which can lead to visible artifacts at the interfaces between different resolution levels. GPU-raycasting has been extended to SAMR data, using a kD-tree that is traversed on the CPU and rendered node-by-node in separate rendering passes @cite .
- All previous approaches for single-pass multi-resolution GPU-raycasting were based on regular data structures, such as octrees or other partition strategies using regularly shaped nodes @cite @cite @cite @cite . In principle also AMR data structures can be partitioned in blocks of cells from the same resolution level using octrees. However, the resulting tree is usually inefficient, in particular if higher order interpolation is desired, because of the large number of resulting nodes @cite . In contrast kD-trees allow to minimize the number of nodes by adaptively choosing the position of the spatial subdivision planes and have been successfully applied to CPU-, and GPU-based volume rendering of AMR data @cite @cite @cite @cite . In this paper we present the first single-pass GPU-raycasting approach for AMR data based on a kD-partition of the data domain.
- In our prior work @cite @cite , we revisited the problem of topology inference using end-to-end probes in networks where intermediate nodes are equipped with multicast and network coding capabilities. We built on @cite and extended it, using network coding at intermediate nodes to deterministically distinguish among all possible quartet topologies, which was not possible before. While in @cite @cite , we focused on inferring the quartets fast and accurately, here we assume that any quartet can be queried and learned, and focus on efficiently selecting and merging the quartets to infer the larger topology. To the best of our knowledge, this work is the first to look at this aspect of the problem.
- Topology inference problems have also been studied in the context of phylogenetic trees @cite @cite . The work in @cite built on @cite and proposed robust algorithms for multiple source tree topology inference. The work in @cite inferred the topology of sparse random graphs using end-to-end measurements between a small subset of nodes. However, the quartet structures and the way we measure them are different in our case due to the nature of active probing in network tomography (see problem formulation in ).
- Relation to the conference version. This journal paper builds on our conference paper in @cite . In addition to revisions and elaborating on parts of the writing, new materials contributions in this paper include the following: the formulation of the problem in the GBS framework as well as the performance evaluation of the GBS algorithm via simulation, and its comparison against REA.
- Since many practical multi-hop networks are formed by distributing a finite number of nodes in a finite area, there has been an increasing interest to model and determine the connectivity properties in finite multi-hop networks @cite @cite @cite @cite @cite @cite @cite @cite @cite . This is also due to the fact, established earlier in @cite @cite and recently in @cite , that the asymptotic connectivity results for large-scale networks provide an extremely poor approximation for finite wireless networks. This poor approximation is due to the boundary effects experienced by the nodes near the borders of the finite region over which the nodes are deployed. Since the nodes located close to the physical boundaries of the network have a limited coverage area, they have a greater probability of isolation. Therefore, the boundary effects play an important role in determining the overall network connectivity.
- In this section, we provide a comprehensive comparison between the proposed scheme and existing works. The reader could skip this section and go directly to the system model, if not interested in the detailed comparisons. Although only some works @cite -- @cite use the same problem formulation as ours, we compare against a wide range of related works @cite -- @cite to highlight the technical novelty of our work, and to illustrate that the works @cite -- @cite @cite -- @cite proposed under different problem formulations cannot be adapted to our setting.
- There have been some works that develop nonstationary policies using repeated games @cite @cite , Markov decision processes (MDPs) @cite @cite , and multi-art bandit @cite -- @cite . We summarize the major differences between the existing nonstationary policies and our proposed policy in Table .
- Most related to this work is our previous work @cite . However, the design frameworks proposed in @cite and in this work are significantly different because the design objectives are different. In @cite , we aimed to design TDMA spectrum sharing policies that maximize the users' total throughput without considering energy efficiency. Under this design objective, each user will transmit at the maximum power level in its slot, as long as the interference temperature constraint is not violated. Hence, what we optimized was . In this work, since we aim to minimize the energy consumption subject to the minimum throughput requirements, we need to optimize , which makes the design problem more challenging. Moreover, this work considers the scenario in which users enter and leave the network, which is not considered in @cite .
- A framework for securely outsourcing general computations is given in @cite . However, this framework is based on Gentry's fully homomorphic encryption scheme @cite rendering it impractical due to the high computational costs and ciphertext sizes. A simple test with the Scarab FHE library (hcrypt.com scarab-library) yielded ciphertext sizes more than ten times those generated using Paillier @cite . Very recent work by @cite propose a secure outsourcing solution for problems which only require the encryption scheme to be somewhat'' homomorphic (SHE). They use the SHE scheme of @cite which provides reasonably efficient computational performance but still suffers from large ciphertexts.
- @cite use a more directed approach and present secure outsourcing solutions that are specific to large scale systems of linear equations and matrix multiplication applications. These solutions fall short in that they leak private information, depend on multiple non-colluding servers, and require a large communication overhead, respectively. @cite use an iterative approach for solving linear equations via client-cloud collaboration. However, their approach has several weaknesses. First, their approach requires that the entire unencrypted matrix be present at the client side. Secondly, the client side must perform a problem transformation step with a computation cost of @math . These weaknesses render the approach impractical for big matrices and do not fully utilize the benefit of the cloud.
- Distributed optimization is a classical topic in applied mathematics with several excellent textbooks, e.g., @cite @cite @cite .
- Dynamical system solutions to distributed optimization problem have been considered for more than fifty years. The Arrow-Hurwicz-Uzawa flow was shown to converge to the set of saddle points for a constrained convex optimization problem @cite . In @cite , a simple and elegant continuous-time protocol was presented to solve linear programming problems. More recently, in @cite , a continuous-time solution having second-order node dynamics was proposed for solving distributed optimization problems for fixed bidirectional graphs. In @cite , a smooth vector field was shown to be able to drive the system trajectory to converge to the saddle point of the Lagrangian of a convex and constrained optimization problem. In @cite , a network of first-order dynamical system was proposed to solve convex intersection computation problems with directed time-varying communication graphs. Besides optimization, a continuous-time interpretation to discrete-time algorithms was discussed for recursive stochastic algorithms in @cite .
- Consensus algorithms have been proven to be useful in the design of distributed optimization methods @cite @cite @cite @cite @cite @cite . Consensus methods have also been extensively studied for both discrete-time and continuous-time models in the past decade, some references related to the current paper include @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite .
- The capability mechanism proposed by @cite uses keyed cryptographic hashes in a way similar to Amoeba and supports delegation by chaining hashes. Each entry on the chain can contain regular expressions to express which rights are being delegated. The mechanism is less expensive than our approach, but does not support rights amplification and cannot be used for secure abstraction. The MyProxy service @cite uses X.509 proxy certificates to delegate credentials, but lacks facilities for including and evaluating complex rights functions.
- In the setting where resources and jobs are embodied as intervals, the objective of finding a minimum cost collection of resources that fulfill the jobs is typically called the full cover problem. Full cover problems in the interval context have been dealt with earlier, in various earlier works @cite @cite @cite . Partial cover problems in the interval context have been considered earlier in @cite . The work in existing literature that is closest in spirit to our result is that of Bar- @cite , and @cite . In @cite , the authors consider the full cover version, and present a @math -approximation algorithm. In this case, all the jobs are to be covered, and therefore the demand profile to be covered is fixed. The goal is to find the minimum cost set of resources, for covering this profile. In our setting, we need to cover only @math of the jobs. A solution needs to select @math jobs to be covered in such a manner that the resources required to cover the resulting demand profile has minimum cost.
- In @cite , the authors consider a scenario, wherein the timeslots have demands and a solution must satisfy the demand for at least @math of the timeslots. In contrast, in our setting, a solution needs to satisfy @math jobs , wherein each job can span multiple timeslots. A job may not be completely spanned by any resource, and thus may require multiple resource intervals for covering it.
- We also show a constant factor approximation algorithm for the problem, by reducing it to the zero-one version of the problem. Jain and Vazirani @cite provide a general framework for achieving approximation algorithms for partial covering problems, wherein the prize collecting version is considered. In this framework, under suitable conditions, a constant factor approximation for the prize collecting version implies a constant factor approximation for the partial version as well. However, their result applies only when the prize collecting algorithm has a certain strong property, called the Lagrangian Multiplier Preserving (LMP) property. While we are able to achieve a constant factor approximation for the problem, our algorithm does not have the LMP property. Thus, the Jain-Vazirani framework does not apply to our scenario.
- As mentioned above, for the preemptive single-processor case, @cite proposed an optimal algorithm for finding a feasible schedule with minimum energy consumption. Using an extension of the classical three-field notation, this problem can be denoted as @math . The multiprocessor case, @math , where there are @math available processors has been solved optimally in polynomial time when preemption and migration of jobs are allowed @cite @cite @cite . The migration assumption means that a job may be interrupted and resumed on the same processor or on another processor. However, the parallel execution of parts of the same job is not allowed.
- In Table , we summarize the most related results of the literature. Several other results concerning scheduling problems in the speed-scaling setting have been presented, involving the optimization of some QoS criterion under a budget of energy, or the optimization of a linear combination of the energy consumption and some QoS criterion (see for example @cite @cite @cite ). The interested reader can find more details in the recent survey @cite .
- The interplay between structural properties of networks and the diffusion processes occurring on them contribute to their complexity. This has been realized by several researchers in the past. For example, @cite @cite emphasized that dynamical processes play an important role in characterizing the structure of complex networks. In @cite they measure the quality of a network partition in terms of the statistical property of the dynamic process taking place in the network. In @cite they study the different equilibrium properties of these processes. However, their works focus on what we call conservative processes: unbiased and biased random walks, discrete and continuous time random walks. In contrast, we also study non-conservative dynamical processes. We also relate these processes to centrality. Although the relationship of PageRank to random walk-type processes is well known, we explain how Alpha-Centrality is related to a type of a non-conservative process. We also carry out an empirical study of different centrality measures, unlike previous works.
- @cite studied measures similar to Alpha-Centrality and personalized PageRank (with attenuation factor 1) which they call communicability. They linked the communicability functions to dynamics by showing their relationship to the thermal Green's function of oscillators. They used communicability to identify important actors in small social networks, demonstrating that different communicability functions led to different judgements of centrality, but did not justify the choice of the particular communicability function in terms of the interactions taking place between actors. Although we study a similar function, the goal of our work is to contrast conservative and non-conservative dynamics and explain how these differences should guide the choice of centrality measure for a given social network.
- Map matching has become an increasingly important problem over the past few years due to the proliferation of GPS tracking devices and track-based applications. A number of algorithms have been proposed to address this problem @cite @cite @cite @cite @cite . A class of these works contain statistical methods which are based on Bayesian estimators. The authors in @cite @cite use the fact that the actual positions of the user on a path form a Markov chain. Given the location measurements, a hidden Markov model is defined with the actual positions on the path as the hidden states. The measurement probabilities in the model are determined based on the location measurement noise distribution (normal distribution with mean zero and variance @math ); the transition probabilities are determined based on the spatial geometric and topological restrictions along with the temporal speed constraints of the trajectories. The matched path is the one with maximum posterior probability.
- Another line of research does not use any statistical methods to address the problem. Authors in @cite use curve simplification for approximating the Fr 'echet distance of curves. Given a polygonal curve @math and an embedded graph @math with edges embedded as straight line segments, they attack the problem of finding the closest path in @math to the curve @math with respect to the Fr 'echet distance. @cite , the average Fr 'echet distance is used to reduce the effect of outliers.
- One of the contributions of our work is to introduce a graph Laplacian-based scheme for the multi-track map matching problem. Though graph Laplacians are widely used in machine learning for dimensionality reduction @cite @cite , spectral clustering @cite @cite and semi-supervised learning @cite , using them for map matching is a contribution of the present paper. Given the partial orderings on the locations, we construct an appropriate distance matrix and use the Laplacian of the corresponding weighted graph to find a global ordering of the locations.
- This work fits into the research program of derandomizing PIT, in particular derandomizing black-box PIT. However, for many of the models of algebraic circuits studied, there are corresponding boolean circuit models for which derandomization questions can also be asked. In particular, for a class @math of boolean circuits, we can seek to construct a @math for @math , such that for any circuit @math on @math inputs, we have the @math -closeness of distributions @math , where @math denotes the uniform distribution on @math . Nisan @cite studied pseudorandom generators for space-bounded computation, and for space @math computation gave a generator with seed length @math . Impagliazzo, Nisan and Wigderson @cite later gave a different construction with the same seed length. Randomized space-bounded computation can be modeled with read-once oblivious (boolean) branching programs, and these generators apply to this model of computation as well.
- However, despite the similarities to the boolean regime, our results improve when the branching programs have bounded width. Specifically, our hitting sets ( thm:small width ) achieve a seed length of @math , and it has been a long-standing open problem (see [Open Problem 8.6] Vadhan12 ) to achieve a seed length (for pseudorandom generators, or even hitting sets) of @math for boolean read-once oblivious branching programs of constant width. Despite much recent work (see @cite @cite @cite @cite @cite @cite @cite @cite ), such seed-lengths are only known for branching programs that are restricted even further, such as regular or permutation branching programs. It is an interesting question as to whether any insights of this paper can achieve a similar seed length in the boolean regime, or in general whether there are any formal connections between algebraic and boolean pseudorandomness for read-once oblivious branching programs.
- In this work, we give a quasi-polynomial hitting sets for diagonal circuits ( thm:diagonal ), making some of Mulmuley's weaker results unconditional. More interestingly, in follow-up work ( @cite ), we improved Mulmuley's reduction from Noether Normalization to PIT in the case of the above ring @math of invariants, and showed that derandomizing PIT for read-once oblivious ABPs is sufficient for finding any explicit set @math of invariants generating the desired subring @math . By using the results of this paper ( thm:main ), one can construct an explicit set @math of size @math , despite the conjectured hardness of this problem.
- Similarly, the PIT problem has also been studied in the non-commutative model. While the work of Raz and Shpilka @cite establishes a white-box PIT algorithm for non-commutative ABPs, the black-box PIT question for this model is more intricate since one cannot immediately apply the usual Schwartz-Zippel algorithm over non-commutative domains. However, Bogdanov and Wee @cite showed how, leveraging the ideas in the Amitsur-Levitzki theorem @cite , one can reduce non-commutative black-box PIT questions to commutative black-box PIT questions. By then appealing to Schwartz-Zippel, they give the first randomized algorithm for non-commutative PIT. They also discussed the possibility of derandomizing their result and raise a conjecture that if true would lead to a hitting set of size @math for non-commutative ABPs of size @math . Our Theorem gives a hitting set of size @math and does not require unproven assumptions.
- In @cite , Arvind, Mukhopadhyay and Srinivasan gave a deterministic black-box algorithm for identity testing of sparse non-commutative polynomials. The algorithm runs in time polynomial in the number of variables, degree and sparsity of the unknown polynomial. This is similar to the running time achieved in the commutative setting for sparse polynomials (see e.g., @cite @cite ) and in particular it is better than our quasi-polynomial time algorithm. On the other hand our algorithm is more general and works for any non-commutative polynomial that is computed by a small ABP.
- We note that in the aforementioned @cite , the authors showed how to deterministically learn sparse non-commutative polynomials in time polynomial in the number of variables, degree and sparsity. In contrast, for such polynomials our deterministic algorithm requires quasi-polynomial time. For general non-commutative ABPs @cite also obtained a deterministic polynomial time learning algorithm, but here they need to have the ability to query the ABP also at internal nodes and not just at the output node. Our deterministic algorithm runs in quasi-polynomial time but it does not need to query the ABP at intermediate computations.
- We also mention that in @cite @cite Jansen, Qiao and Sarma studied black-box PIT in various models related to algebraic branching programs. Essentially, all these models can be cast as a problem of obtaining black-box PIT for read-once oblivious branching programs where each variable appears on a small number of edges. Their result gives a hitting set of size (roughly) @math when @math is an upper bound on the number of edges that a variable can label and the ABP is of polynomial size. In comparison, our Theorem gives a hitting set of size @math and works for @math (as long as the size of the ABP is polynomial in @math ). Our techniques are very different from those of @cite , which follows the proof technique of @cite . The later paper @cite use an algebraic analogue of the Impagliazzo-Nisan-Wigderson @cite generator, as we do, but the details are different.
- In our previous paper @cite , we introduced the modal logic of forcing and proved that the -provably valid principles of forcing were exactly those in the modal theory known as S4.2 . The modal logic of forcing corresponds to the monomodal fragment of the modal logic discussed in , that only uses the relation @math ; or to the part of the generic multiverse that is generated only by the operation of taking forcing extensions and not ground models. In @cite , we not only consider the @math -provable modal logic of forcing, but also the modal logic of forcing of particular universes @math . We show that this modal logic always lies between @math and @math and that the two extreme values are realized (for more details, cf. ,). Various other aspects of the modal logic of forcing are considered in @cite @cite @cite @cite @cite @cite @cite @cite @cite . The paper @cite presented at ICLA 2009 gives an overview of the status of research and creates a connection between the modal logic of forcing and set-theoretic geology'', i.e., going down from a universe to its ground models. This connection is further developed in this paper.
- One of the authors worked on another one permutation'' scheme named Conditional Random Sampling (CRS) @cite @cite since 2005. Basically, CRS works by continuously taking the first @math nonzeros after applying one permutation on the data, then it uses a simple trick'' to construct a random sample for each pair with the effective sample size determined at the estimation stage. By taking the nonzeros continuously, however, the samples are no longer aligned'' and hence we can not write the estimator as an inner product in a unified fashion. In comparison, our new one permutation scheme works by first breaking the columns evenly into @math bins and then taking the first nonzero in each bin, so that the hashed data can be nicely aligned.
- Interestingly, in the original minwise hashing'' paper @cite (we use quotes because the scheme was not called minwise hashing'' at that time), only one permutation was used and a sample was the first @math nonzeros after the permutation. After the authors of @cite realized that the estimators could not be written as an inner product and hence the scheme was not suitable for many applications such as sublinear time near neighbor search using hash tables, they quickly moved to the @math -permutation minwise hashing scheme @cite . In the context of large-scale linear learning, the importance of having estimators which are inner products should become more obvious after @cite introduced the idea of using ( @math -bit) minwise hashing for linear learning.
- We are also inspired by the work on very sparse random projections'' @cite . The regular random projection method also has the expensive preprocessing cost as it needs @math projections. The work of @cite showed that one can substantially reduce the preprocessing cost by using an extremely sparse projection matrix. The preprocessing cost of very sparse random projections'' can be as small as merely doing one projection. See http: www.stanford.edu group mmds slides2012 s-pli.pdf for the experimental results on clustering classification regression using very sparse random projections @cite .
- Figure presents the fixed-length'' scheme, while in Sec. we will also develop a variable-length'' scheme. Two schemes are more or less equivalent, although we believe the fixed-length scheme is more convenient to implement (and it is slightly more accurate). The variable-length hashing scheme is to some extent related to the Count-Min (CM) sketch @cite and the Vowpal Wabbit (VW) @cite @cite hashing algorithms.
- In @cite , study the effect of bi-directional traffic on TCP congestion control algorithm. In particular, they observe that in the BSD Tahoe TCP implementation packets from a single connection are clustered together, similarly to what observed by the same authors in @cite over one-way traffic. This causes ACK compression, which significantly reduces the available bandwidth for TCP connections. They also show that in case of two-way traffic, the issue of ACK compression is made worse by the interaction of ACKs and data packets in the queue.
- A different scheme for control of sampling can use the principles of bounded rationality @cite and rational metareasoning @cite @cite . In search, one maintains a current best action @math , and finds the expected gain from finding another action @math to be better than the current best.
- In the Multi-armed Bandit problem @cite we have a set of @math arms (see Figure .a). Each arm can be pulled multiple times. Sometimes a cost is associated with each pulling action. When the @math th arm is pulled, a random reward @math from an unknown stationary distribution is encountered. The reward is usually bounded between 0 and 1. In the cumulative setting (the focus of much of the research literature on Multi-armed bandits), all encountered rewards are collected by the agent. The UCB scheme was shown to be near-optimal in this respect @cite :
- The UCT algorithm, an extension of UCB to Monte-Carlo Tree Search is described in @cite , and shown to outperform many state of the art search algorithms in both MDP and adversarial games @cite @cite .
- Statistics on tree-shaped objects receive growing interest in the statistical community. Wang and Marron @cite study metric spaces of trees and define a notion of average tree called the median-mean as well as a version of PCA, which finds modes of variation in terms of , encoding the maximum amount of structural and attributal variation. @cite extend this work by finding efficient algorithms for PCA. This is applied to analysis of brain blood vessels. The metric defined by Wang and Marron does not give a natural geodesic structure on the space of trees, as it places a large emphasis on the tree-topological structure of the trees. The metric has discontinuities in the sense that a sequence of trees with a shrinking branch will not converge to a tree that does not have that branch. Such a metric is not suitable for studying trees with continuous topological variations and noise, such as anatomical tree-structures extracted from medical images, since the emphasis on topology makes the metric sensitive to structural noise.
- Trees also appear in genetics. @cite visualize large sets of phylogenetic trees using multidimensional scaling. @cite have invented a phylogenetic tree-space suitable for geodesic analysis of phylogenetic trees, and Owen and Provan @cite have developed fast algorithms for computing geodesics in phylogenetic tree-space. Nye @cite has developed a notion of PCA in phylogenetic tree-space, but is forced to make strict assumptions on possible principle components being ''simple lines'' for the sake of computability. Phylogenetic trees are not geometric, and have fixed, labeled leaf sets, making the space of phylogenetic trees much simpler than the space of tree-like shapes.
- We have previously @cite @cite studied geodesics between small tree-shapes in the same type of singular shape space as studied here, but most proofs have been left out. @cite , we study different algorithms for computing average trees based on the QED metric. This paper extends and continues @cite , giving proofs, in-depth explanations and more extensive examples illustrating the potential of the QED metric.
- As a consequence of the process of embodiment, there is a rising need of adequateness to the context the embodied agent is currently in and the ones it has experienced and related to which it has associated memories. @cite evaluated the importance of situatedness in dialogue when the system is using embodied agents. In order to support situated dialogue, work on generation and resolution of referring expressions has been accomplished based on vision, in which the dialogue system depends on input from a vision subsystem to allow a reference resolver, along with spatial reasoning, to match linguistic references to world entities @cite . Further experiments by @cite used a bidirectional layer model for resolution and generation of referring expressions for entities that might not be in the current context and therefore must be accounted for as such when producing and interpreting dialogue in a human-robot interaction. Lison and Kruijff @cite proposed a solution for dialogue systems to cope with open domains through priming speech recognition based on the concept of salience, from both linguistic and visual points of view. This concept was also a main focus target of Kelleher and Costello @cite .
- On the other hand, Bohus and Horvitz @cite proposed an open-world platform which attempts to allow a dialogue system to support multi-dynamic user interaction along with heavily situated context information acquired, mostly from vision features, to adapt the dialogue domain. We show that open domain dialogue adaptation can also rely on linguistic information rather than there approach, which focused mainly on visual information.
- Given their role in NLP, multiple efforts @cite @cite @cite @cite @cite have looked into @math -gram statistics computation. While these approaches typically consider document collections of modest size, recently @cite and @cite targeted web-scale data. Among the aforementioned work, @cite is closest to ours, also focusing on less frequent @math -grams and using a cluster of machines. However, they only consider @math -grams consisting of up to eleven words and do not provide details on how their methods can be adapted to MapReduce. Yamamoto and Church @cite augment suffix arrays, so that the collection frequency of substrings in a document collection can be determined efficiently. Bernstein and Zobel @cite identify long @math -grams as a means to spot co-derivative documents. @cite and @cite describe the @math -gram statistics made available by Google and Microsoft, respectively. Zhai @cite gives details on the use of @math -gram statistics in language models. @cite demonstrated recently that @math -gram time series are powerful tools to understand the evolution of culture and language.
- A variety of strategies have been used to shrink'' a graph for purposes of analysis. Shrinking a graph by extracting an actual subgraph allows one to discover patterns in the subgraph that can be validated later in the original graph because of the one-to-one correspondence between the nodes in the two graphs. The generation of synthetic topologies which have a specific set of properties in them @cite @cite , though useful in many contexts, is not considered in this report.
- A sample subgraph induced by a randomly selected set of nodes has been discussed in several works on graph sampling @cite @cite @cite @cite . Selecting nodes randomly ensures that nodes of a given degree are chosen with probability proportional to the number of such nodes in the network. The selected set of nodes have a degree distribution very similar to that of the original graph, but these degrees are the degrees of the nodes in the original graph @math and not the degrees in the induced subgraph @math . There are at least two additional problems with such a sampling strategy: (i) when the desired sample size is as small as 5 is highly likely to be a disconnected graph even if @math is connected and thus, unrepresentative; and (ii) in real networks that have to be crawled, it is usually very hard or infeasible to generate a statistically valid set of uncorrelated random nodes from the full graph @math given that the full graph is not known (even though a few random nodes can always be selected from within the known portion of the graph).
- A related set of sampling strategies is based on selecting random edges instead of random nodes or a combination of node and edge sampling @cite . In general, however, edge sampling does not overcome the problems of node sampling mentioned above. Sampling strategies based on random deletion @cite @cite instead of selection also suffer the same problems and are not suitable as solutions to the problem statement expressed in Section . Node or edge sampling is useful in contexts where the goal is to infer properties of nodes but not necessarily the topological properties of the graph. The choice of nodes guided by simulated annealing can target a specific set of topological properties @cite , but this method also relies on randomly choosing nodes from the entire network.
- A further improvement in graph sampling is achieved with Snowball sampling , which chooses a random node from the known portion of the graph @math and then proceeds with a breadth-first search until the desired size of the sample graph is achieved @cite . Snowball sampling and its derivatives have been used in social network analysis @cite @cite . As reported in @cite , for small sample sizes, it is inconclusive if the clustering co-efficient of the sample network converges to that of the complete network (as will be verified in our work as well). In addition, Snowball sampling has been shown to over-sample hubs'' or large-degree nodes in a network because of its breadth-first strategy which hits a hub with a greater likelihood.
- A related strategy is one called Forest Fire , first introduced in @cite . In this method, as in Snowball sampling, we choose a random node from the known part of @math and use a breadth-first approach. With a forward burning probability'' @math , the node burns links attached to it. The nodes at the other end of a burned link are added to the sample subgraph and they now continue spreading the fire'' by burning links attached to them. This continues until the desired size of the sample subgraph is achieved. It has been found in @cite that the Forest Fire sampling strategy works best with @math and this is what we use in all our simulations in this report. In general, it has been found that methods based on BFS search are likely to overestimate node-degrees and underestimate symmetry @cite . As we will show later, the Forest Fire sampling strategy, being based on a scaled-down'' BFS, is not entirely able to reduce the likelihood of adding high-degree nodes to the sample subgraph.
- A more rigorous but different approach to random subgraph sampling has only recently been attempted in @cite which evaluates a number of different strategies including Random Vertex Expansion @cite . However, while the subgraphs sampled in the methods proposed in @cite achieve a sampling of subgraphs uniformly at random, they do not actually extract a single subgraph that is most representative of the full graph with respect to any given property. As a result, random sampling of subgraphs do not readily help us discern properties of the full graph, especially since the sampling of subgraphs uniformly at random leads to an over-representation of properties from dense portions of the graph.
- In the literature, there are many models for the self-organization of neuronal networks. @cite developed a toolkit for computational neuroscientists to explore developmental changes in biological neural networks. However, details of the methodology used (e.g., how the initial random network is constructed) and of simulation parameters (e.g., how the threshold parameter for pruning is obtained) are not clear. @cite present an approach to self-organization in a dynamic neural network by assembling cooperative neuro-agents. However, their intent is not to explore synaptic connectivity. @cite addresses the development of brain-inspired models that will be embedded in robotic systems to support their cognitive abilities. However, this work focuses on brain slices rather than reflex pathways and aims to improve cognitive capabilities of robotic systems rather than exploring synaptic functional connectivity.
- The two-Gaussian model for approximately sparse signal eq. ) was used in compressed sensing e.g. in @cite @cite .
- Belief propagation based reconstruction algorithms were introduced in compressed sensing by @cite . Authors of @cite used sparse measurement matrices and treated the BP messages as probabilities over real numbers, that were represented by a histogram. The messages, however, can be represented only by their mean and variance as done by @cite @cite . Moreover, one does not need to send messages between every signal-components and every measurements @cite , this leads to the approximate message passing (AMP). In the context of physics of spin glasses this transformation of the belief propagation equations corresponds to the Thouless-Anderson-Palmer equations @cite . The AMP was generalized for general signal models in @cite @cite and called G-AMP. The algorithm used in @cite @cite is equivalent to G-AMP. We also want to note that we find the name approximate'' message passing a little misleading since, as argued e.g. in @cite , for dense random measurement matrices the G-AMP is asymptotically equivalent to BP, i.e. all the leading terms in @math are included in G-AMP.
- For random matrices the evolution of iterations of G-AMP on large system sizes is described by state evolution @cite . The exactness of this description was proven in large generality in @cite . See also @cite @cite @cite for discussions and results on the state evolution.
- The optimal reconstruction was studied extensively in @cite . The replica method was used to analyse the optimal reconstruction in compressed sensing in e.g. @cite @cite . In the statistical physics point of view the replica method is closely related to the state evolution @cite .
- As we shall see the G-AMP algorithm for homogeneous measurement matrices matches asymptotically the performance of the optimal reconstruction in a large part of the parameter space. In some region of parameters, however, it is suboptimal. For the sparse signals, it was demonstrated heuristically in @cite that optimality can be restored using seeding matrices (the concept is called spatial coupling), rigorous proof of this was worked out in @cite . The robustness to measurement noise was also discussed in @cite @cite . Note that the concept of spatial coupling'' thanks to which theoretical thresholds can be saturated was developed in error-correcting codes @cite @cite @cite . In compressed sensing the spatial coupling'' was first tested in @cite who did not observe any improvement for the two-Gaussian model for reasons that we will clarify later in this paper. Basically, the spatial coupling provides improvements only if a first order phase transition is present, but for the variance of small components that was tested in @cite there is no such transition: it appears only for slightly smaller values of the variance.
- The theme of Web Data Extraction is covered by a number of reviews. @cite presented a survey that offers a rigorous to classify Web Data Extraction systems. The authors introduced a set of criteria and a qualitative analysis of various Web Data Extraction tools.
- The need for 802.11 resource allocation schemes has been extensively studied in the literature @cite @cite @cite . Many of the proposed schemes rely on either non-standard compliant features @cite , or completely develop an entire new MAC protocol @cite . Both strategies may be undesirable, and so we avoid them. Given that, the resource allocation scheme that more closely relates to ours is @cite , that studied the problem of absence of application-specific 802.11 resource allocation schemes. As a solution, they designed and implemented an overlay MAC layer (OML) to divide the time into slots of equal size. Then, they used a distributed algorithm to allocate the slots across the competing nodes, where each competing node receives a number of slots proportional to its weight function. However, the authors let as an open issue the understanding of the increased delay for TCP flows in presence of the slotted mechanism @cite .
- Although overlay solutions are easy to be implemented, they are often sub-optimal and difficult to scale because of the overlapping and duplication of similar functionalities at different layers (e.g. in the driver and in the card firmware). The VirtualWiFi project @cite proposed an architecture that abstracts a single 802.11 WLAN card to appear as multiple virtual clients to the user. Each client instance adopts standard PHY MAC protocols, but it can be separately configured at the driver level. An interesting application was the idea of connecting to multiple APs through a single radio interface. The authors rely on the 802.11 Power Save (PS) mode feature to switch among different 802.11 WLAN nodes in a time-division fashion. A station can inform the current 802.11 WLAN node that it is going into PS mode --- so that it can buffer packets directed to it --- and switch the radio-frequency to other 802.11 WLAN nodes, only to come back to the original node before the PS period expires.
- Automated testing is the most common way of evaluating students' programs @cite . Test cases are usually supplied by a teacher and or randomly generated @cite . A lot of systems use this approach, for example, PSGE @cite , Kassandra @cite , BOSS @cite , WebToTeach @cite , Schemerobe @cite , TRY @cite , HoGG @cite , BAGS @cite , on-line Judge @cite , JEWL @cite , Quiver @cite , and JUnit @cite .
- Software verification techniques are not commonly used in automated evaluation of programs. There are limited experiments on using Java PathFinder model checker for automated test case generation @cite . Tools with integrated support for automated testing and verification, e.g. Ceasar @cite , are usually too complex and not aimed for educational purposes. To the authors' knowledge, there is no other software verification tool deployed in process of automated bug finding as a complement to automated testing of students' programs. The tool LAV was already used, to a limited extent, for finding bugs in students' programs @cite . In that work, a different sort of corpus was used, as discussed in Section . Also, that application did not aim at automated grading, and instead was made in the wider context of design and development of LAV as a general-purpose SMT-based error finding platform.
- Apart of assignment grading, regression techniques were also used for final grade forecasting with good results. For this purpose, used data from learning management system and identified variables most useful for the prediction, e.g., number of assessments completed and number of discussion and mail messages sent @cite . Kotsiantis performed successful forecasting based on demographic characteristics of students, results of several written assignments, and class attendance @cite .
- The problems associated to multi-hop packet radio networks (also known as multi-hop radio networks or wireless mesh networks in recent literature) have attracted the attention of the research community for a long time. As early as 1987, Tobagi presented a survey on the modeling and analysis of multi-hop packet radio networks @cite , where he identified the difficulties intrinsic to this kind of networks. First, because of the broadcast and shared nature of the wireless channel, medium arbitration protocols are needed. Moreover, the action of each wireless station inevitably affects several surrounding stations, thus incurring interdependencies that complicate the analysis.
- To further complicate the issue, wireless propagation introduces an additional degree of randomness and unpredictability to the network. In order to advance in the analysis of multi-hop packet radio networks, some simplifications are in order. One of the most common modeling assumptions is the use of a graph representation of the network, where an edge exists between two stations if they are in the transmission range of each other. As an example, @cite uses a graph representation to show that determining whether a traffic matrix belongs to the capacity region of a packet radio network is NP hard. We will use the graph topology representation and several other assumptions in the present paper.
- Our goal is that the different stations of the network settle down in a satisfactory transmission schedule that efficiently uses the radio resources. In @cite the problem is treated from a theoretical perspective in which the network topology is known and the schedule can be constructed in a centralized way. An interesting alternative is considered in @cite were contention MAC protocols are proposed in which the stations learn from their neighbourhood. These two works rely on a slotted channel access, which means that neighbouring stations need to have closely synchronized clocks. The problem of gradient clock synchronization is treated in @cite .
- A possible solution to avert the problems of network synchronization and schedule construction is to use random media access control. If all the stations use the Aloha protocol, the contention parameter can be optimized to maximize proportional fairness as demonstrated in @cite . The contention parameter regulates how aggressively an Aloha station contends for the medium. If the contention parameter is too aggressive, a large fraction of channel time is wasted in the form of collisions. If it is not aggressive enough, a large fraction of the channel time remains idle.
- Later works have explored the validity of the idea for different kinds of traffic @cite , modelled the network performance metrics @cite , and studied the possibility of traffic differentiation @cite . A comprehensive simulation study, together with a model of the learning process is presented in @cite . This last paper also includes performance measures in non-ideal conditions, such as the presence of legacy stations or channel errors. More results regarding the learning process and fairness with legacy stations can be found in @cite . An evaluation of a collision-free protocol and its interaction with the autorate fallback mechanism (ARF) in realistic multiple-input-multiple-output (MIMO) channels is presented in @cite .
- A general discussion of this class of protocols, together with performance comparisons and protocol refinements, is offered in @cite . This last paper also introduces the concepts of stickiness and variable schedule length to accommodate a larger number of contenders in slotted networks. In the present work, we will reuse both concepts in the context of multi-hop packet radio networks.
- Over the years, many PADS tools, languages and middlewares have been proposed (a comprehensive but somewhat outdated list can be found in @cite ); in this section we highlight some of the most significant results with specific attention to the implementations of the Time Warp synchronization mechanism.
- DSIM @cite is a Time Warp simulator which targets clusters comprised of thousands of processors and that implements some advanced techniques for the memory management (e.g. Time Quantum GVT and Local Fossil Collection).
- A recent work @cite investigated the use of the Go programming language http: golang.org to implement an optimistic parallel simulator for multicore processors. The simulator, called Go-Warp, is based on the Time Warp mechanism. Go provides mechanisms for concurrent execution and inter-process communication, which facilitate the development of parallel applications. Like Erlang, all these mechanisms are part of the language core and are not provided as external libraries. However, Go-Warp can not be executed on a distributed memory cluster without a major redesign; with this respect, ErlangTW represents a significant improvement, since the simulator runs without any modification on both shared memory and distributed memory architectures. To the best of our knowledge, Erlang has not been used to implement a Time Warp simulation engine.
- Link prediction refers to the problem of inferring new interactions among members in a network. The first systematic treatment of the problem appeared in @cite , where a variety of proximity measures, such as Common Neighbors @cite and the Katz measure @cite are used as effective methods for link prediction. In addition to unsupervised approaches, there is also rising interest in supervised approaches for link prediction @cite @cite @cite . In supervised link prediction, node and or edge features are extracted from the network and treated as a classification problem. However, engineering good features and how to encode the class imbalance problem are still challenging tasks. Recently, link prediction has been shown to benefit from exploring additional information external to the network, such as node or edge attributes @cite @cite . However, these approaches require additional information, which may be difficult to obtain due to privacy and security issues.
- Many popular proximity measures that are used for link prediction have high computational complexity and do not scale well to large-scale networks. A great deal of recent work has been devoted to speedup the computation. For example, @cite truncates the series expansion of Katz and only considers paths of length up to some threshold. In @cite @cite , dimensionality reduction methods, such as the eigen-decomposition, are used to construct low rank approximations of a graph, which are then used to compute approximated proximity measures. The more recent work in @cite applies the Lanczos Stieltjes procedure to iteratively compute upper and lower bounds of a single Katz value and shows that these eventually converge to the real Katz value.
- Efficient proximity estimation is essential for scalable link prediction. However, one should be able to make accurate and robust predictions with the estimated measures. For example, @cite @cite explore the low rank approximation of social networks to speed up large-scale link prediction. Another way to improve the link prediction performance is to explore the community structure of a network. For example, LinkBoost @cite explores the community structure by a novel degree dependent cost function and shows that minimization of the associated risk can lead to more links predicted within communities than between communities. However, considering a single community structure may not lead to robust predictions, because even detecting the best' community structure itself is still an open question.
- Very little work has been done using hierarchical structures for link prediction. One exception is the method proposed by @cite , which works by sampling a number of competitive hierarchical random graphs from a large pool of such graphs. Each sampled graph is associated with a probability indicating the strength of community structure over the original network. The probability of a link appearing between any two nodes is averaged over the corresponding connecting probability on the sampled graphs. However, to predict potential links, this algorithm needs to enumerate and average over almost all possible hierarchical partitions of a given network and thus is very costly to compute even with small networks. Compared with @cite , our algorithm is much more efficient in terms of speed and thus can be scaled up to large-scale link prediction problems with millions of users.
- Worst-case coverage has been first studied in @cite for a traditional sensor network. Polynomial-time algorithms are devised to find maximum breach and maximum support paths between two locations. An efficient algorithm is proposed in @cite to solve the best-coverage problem raised in @cite . In @cite , efficient algorithms are developed to find the minimum exposure path in sensor networks. Localized algorithms are designed in @cite to solve the minimum exposure path problem. A new coverage measure that captures both the best and worst-case coverage is studied in @cite . The deployment problem to improve the maximal breach path is considered by @cite @cite . Barrier coverage is an intimately related problem to worst-case coverage. The concept of weak and strong barrier coverage has been introduced in @cite , where critical condition of weak barrier coverage is obtained for random deployment. The critical condition of strong barrier coverage is derived in @cite using percolation theory. An effective metric of barrier coverage quality is proposed in @cite . @cite studies constructing barrier by sensors with limited mobility after initial deployment. A novel full-view coverage model is proposed in @cite for constructing barrier in camera sensor networks.
- While little research attention was dedicated to the precision-biased task @cite @cite , several studies address the parse-selection task. Yates et.al. perform parse-selection by filtering out parses containing semantically implausible'' relations, where semantic-plausibility is estimated by high co-occurrence of the words in relation in a large corpora ( i.e. , the web).
- In contrast, we are primarily interested in selecting high-quality edges rather than complete parses. We view parse-selection as an extension of the precision-biased parsing task, and perform parse-selection based on the number of risky attachment decisions. Our assessment of the riskiness or reliability of a particular decision is not based on aggregate corpus counts nor on global features of the input sentence (though such kinds of information may be integrated in the future). In our first method, we adopt a committee-based approach, but apply it primarily for edge-selection. In the second method we present below, we investigate features which may help the parser assess edge riskiness. We note that the marginal edge probabilities obtained from a log-linear parsing model as in @cite are not reliable predictors of edge riskiness: indeed, the pruning procedure used in @cite consider edges with marginal scores of up to @math of the highest scoring edge as possible candidates in order to ensure sufficient coverage, indicating that such models may greatly overestimate the marginals of incorrect edges, while underestimating the marginal values of correct edges.
- @cite were the first to consider buffer management and scheduling in the context of network processors with heterogeneous processing requirements for the arriving traffic. They study both SRPT (shortest remaining processing time) and FIFO (first-in-first-out) schedulers with recycles, in both push-out and non-push-out buffer management cases, where a packet is recycled after processing according to the priority policy (FIFO or SRPT). They showed competitive algorithms and worst-case lower bounds for such settings. Although they considered a different architecture (FIFO with recycles) than the one we consider in this paper, they provided only a lower bound for the push-out FIFO case, and it remains unknown if it can be attained.
- Various models have been proposed and studied, including, among others, QoS-oriented models where packets have weights @cite @cite @cite @cite and models where packets have dependencies @cite @cite . A related field that has received much attention in recent years focuses on various switch architectures and aims at designing competitive algorithms for such multi-queue scenarios; see, e.g., @cite @cite @cite @cite @cite . Some other works also provide experimental studies of these algorithms and further validate their performance @cite .
- Leech proposes in @cite @cite an approach that largely resembles the method of : it is the same, with the exception that a different updating rule is used. The method that Leech uses for computing the Banzhaf index is not mentioned. The focus in this paper is on the results that are obtained after applying the method to the 15-member EU council (also see @cite ), and to the board of governors of the International Monetary Fund.
- There are two more recent interesting works on the voting game design problem. One is by Kurz @cite . Kurz proposes an exact method using integer linear programming, for solving the weighted voting game design problem for both the Shapley-Shubik index and the Banzhaf index. The set of linear games is taken as the search space, and branch-and-bound techniques (along with various insights about the set of weighted voting games) are used in order to find in this set a weighted voting game with a power index closest to the target. Kurz does not provide a runtime analysis. The experiments performed show that the algorithm works well for small numbers of players. As mentioned in Section , our work is independent and differs from @cite because we are interested in devising an algorithm with a provably good runtime. Moreover, the approach we take is different from that of @cite , and the theory necessary to develop our algorithm can be considered interesting in itself.
- The other recent work is @cite , by This paper gives provides as a main result an algorithm for the inverse power index problem for the case of the Shapley-Shubik index, and has a certain approximation guarantee: in addition to a target power index, the algorithm takes a precision parameter @math and guarantees to output a weighted voting game of which the power index is @math -close to it, on the precondition that there an @math -close weighted voting game with the property that the quota is not too skewed, in a particular sense. This is, to our knowledge, the only polynomial time algorithm for a power index voting game design problem that provides an approximation guarantee in any sense.
- Closely related to our work are two papers that deal with the Chow parameters problem @cite @cite . The results in their paper are stated in terms of boolean function theory and learning theory, but when translated to our setting, these papers can be seen to deal with approximation algorithms for a type of value that can be considered a power index: The Chow parameters of a given player in a given game is defined to be the total number of winning coalitions that the players is in. The authors present in these papers, as a main result, a polynomial time approximation scheme for computing the Chow parameters of a weighted voting game.
- The problem of the set of weighted voting games on a fixed number of players is, as we will see, closely related to the approach that we take for solving the weighted voting game design problem. This enumeration problem has been studied before in a paper by Kurz @cite , where the author uses integer programming techniques in order to enumerate all canonical weighted voting games on up to nine players. The author generates integer weighted representations for all of these games and classifies the games that do not have a unique minimum-sum integer weighted representation.
- Threshold functions @cite @cite are of fundamental research interest in voting games, circuit complexity and neural networks. The problem of realizing Boolean threshold functions by neural networks has been extensively studied @cite @cite @cite , where upper and lower bounds are derived on the synaptic weights for such realization. The enumeration of threshold functions is closely related to the enumeration of weighted voting games (as threshold functions are essentially weighted voting games where negative weights are allowed.). The enumeration of threshold functions up to six variables has been done in @cite . Subsequently, in @cite @cite , all threshold functions of respectively seven and eight variables were enumerated. Krohn and Sudh "olter @cite enumerated the canonical weighted voting games up to eight players, as well as the class of canonical linear games. Kurz @cite was the first to enumerate all nine player canonical weighted voting games, and Freixas and Molinero @cite were the first to enumerate all nine player canonical linear games.
- There exists some litature on enumeration of special subclasses of voting games as well: see @cite for linear games with two desirability classes; @cite for weighted voting games with one roof; and @cite for linear games with certain special types of voters and few desirability classes.
- Alon and Edelman observe that we need to know estimates of what power indices are achievable in simple games, in order to analyze the accuracy of these kinds of iterative algorithms, i.e., there is a need for information about the distribution of power indices in @math . As a first step into solving this problem, they prove in @cite a specific result for the case of the Banzhaf index for monotonic simple games.
- Also, some applied work has been done on the design of voting games. In two papers, one by Laruelle and Widgr 'en @cite and one by Sutter @cite , the distribution of voting power in the European Union is analyzed and designed using iterative methods that resemble the algorithm of Aziz @cite . Similar work was done by Leech for the EU @cite , and for the IMF @cite .
- Finally, a research direction that is related to our problem is that of studying minimal integer representation for a weighted voting game: Bounds on the maximum weight in such a representation provide us with a finite set of weighted representations to search through, as a means of solving our design problem. We explain this in greater detail in the next section. Some classical relevant bounds can be found in @cite , Section 9.3. See @cite @cite for some recent work in this direction.
- Another line of work has studied spreading scaling laws using more sophisticated non-gossip schemes over static wireless networks, e.g. @cite @cite . Recently, Resta @cite began investigating broadcast schemes for mobile networks with a source constantly propagating new data, while we focus on a different problem with multiple mobile sources each sharing distinct message. Besides, @cite analyzed how to combat the adverse effect of mobility to ensure the same pipelined broadcasting as in static networks, whereas we are interested in how to take advantage of mobility to overcome the geometric constraints. In fact, with the help of mobility, simply performing random gossiping -- which is simpler than most non-gossip schemes and does not require additional overhead -- is sufficient to achieve optimality.
- Finally, we note that gossip algorithms have also been employed and analyzed for other scenarios like distributed averaging, where each node is willing to compute the average of all initial values given at all nodes in a decentralized manner, e.g. @cite @cite . The objective of such distributed consensus is to minimize the total number of computations. It turns out that the convergence rates of both message sharing and distributed averaging are largely dependent on the eigenvalues or, more specifically, the mixing times of the graph matrices associated with the network geometry @cite @cite .
- The decreasing diagram technique @cite is a powerful technique to obtain many confluence criteria. In @cite , the technique is extended to obtain a criterion for CRM.
- The use of expected kernels in dealing with the uncertainty in the input data has a connection to robust SVMs. For instance, a generalized form of the SVM in @cite incorporates the probabilistic uncertainty into the maximization of the margin. This results in a second-order cone programming (SOCP) that generalizes the standard SVM. In SOCP, one needs to specify the parameter @math that reflects the probability of correctly classifying the @math th training example. The parameter @math is therefore closely related to the parameter @math , which specifies the variance of the distribution centered at the @math th example. @cite showed the equivalence between SVMs using expected kernels and SOCP when @math . When @math , the mean and covariance of missing kernel entries have to be estimated explicitly, making the SOCP more involved for nonlinear kernels. Although achieving comparable performance to the standard SVM with expected kernels, the SOCP requires a more computationally extensive SOCP solver, as opposed to simple quadratic programming (QP).
- The definition of interference used in this paper was introduced by von Rickenbach al @cite who proved upper and lower bounds on the interference of one dimensional point sets: The point set, @math , in this lower-bound consists of any sequence of points @math , all on a line, such that @math , for all @math . That is, the gaps between consecutive points decrease exponentially.
- This lower bound is matched by an upper-bound: The upper bound in twod-upper is obtained by selecting @math vertices to act as , connecting the hubs into any connected network and then having each of the remaining nodes connect to its nearest hub. This idea was extended to two and higher dimensions by Halld 'orsson and Tokuyama @cite , by using a special type of @math -net as the set of hubs:
- Several authors have shown that the interference of a point set is related to the (logarithm of) the ratio between the longest and shortest distance defined by the point set. In particular, different versions of the following theorem have been proven by Halld 'orsson and Tokuyama @cite ; Khabbazian, Durocher, and Haghnegahdar @cite ; and Maheshwari, Smid, and Zeh @cite : At least two of the proofs of log proceed by showing that @math . A strengthening of this theorem is that the numerator in the definition of @math can be replaced with the length of the longest edge in @math @cite @cite .
- log suggests that point sets with very high interference are unlikely to occur in practice. This intuition is born out by the results of Kranakis al @cite , who show that high interference is unlikely to occur in random point sets in one dimension: Note that, in this one-dimensional case, the minimum spanning tree, @math , is simply a path that connects the points of @math in order, from left to right. Taken together, Part 1 of Theorems and generalize sqrtlogn to arbitrary constant dimensions @math .
- In higher dimensions, Khabbazian, Durocher, and Haghnegahdar @cite use their version of log to show that minimum spanning trees of random point sets have at most logarithmic interference. Part 1 of main improves the upper bound in durocher to @math and Part 1 of lower-bound gives a matching lower bound.
- Existing works on the performance of 802.11 focus primarily on the throughput and capacity. Bianchi proposed a Markov chain model of 802.11 @cite . A. proposed a probability model of 802.11 @cite which simplifies Bianchi's model and it is shown to be quite accurate even in the multi-hop case @cite . In our paper, we adopt A. Kumar's model to derive the stochastic service curves of an 802.11 node. There are some works on 802.11 queueing analysis based on traditional queueing theory. assumed Poisson traffic arrival and proposed an M G 1 queueing model of 802.11 @cite . Tickoo proposed a G G 1 queueing model of 802.11 @cite @cite . Bredel and Fidler modeled the 802.11 DCF as a fluid GPS scheduler yielding a fair average service rate @cite . And Ciucu analyzed the non-asymptotic throughput and delay distribution in multi-hop wireless networks by network calculus approach considering Aloha systems @cite .
- Link prediction is one of the most fundamental problems in networked data mining that attempts to estimate the likelihood of the existence of a link between two nodes, according to observed links and the attributes of nodes @cite @cite . It has found applications in predicting missing links of biological networks and recommending social mates in online social networks. To evaluate the algorithmic performance, we usually divide the data into two parts and use the training set to predict the testing set. The algorithmic accuracy is quantified by counting the overlap of prediction and the testing set and or the ranks of links in the testing set among all non-observed links (i.e., node pairs not in the training set). Link prediction algorithm can be used in judging the driven mechanisms of network formation. The very nice performance of common-neighbor-based similarity indices strongly supports the validity of clustering mechanism @cite @cite . On several real friendship networks, @cite showed that when combined with topological features, topical similarity achieves a link prediction accuracy of about 92
- The literature of timed automata theory is a rich literature since it was introduced by Alur and Dill in 1990. Alur and Madhusudan @cite present a full survey of known results for decidability problems in timed automata theory. Tripakis @cite gives algorithms and techniques to verify timed systems using TCTL logic and Timed Buchi Automata, which have implemented in KRONOS model checking tool. One of the disadvantages of KRONOS is that its input language supports a very restricted data types that allow only the declaration of clock variables. For this reason we have not included KRONOS in our comparative study.
- Community detection is an extremely active research area, with a variety of methods proposed, including spectral clustering, graph partitioning and modularity maximization @cite . We show in this paper that these methods can be expressed in terms of the generalized linear interaction model that assumes a specific type of interaction. We also demonstrate that to get the full picture of network's emergent structure, community detection method must account for the dynamic process occurring on the network.
- It might be argued that taking interactions into account eventually leads to a weighted graph and off-the shelf community detection algorithms for weighted graphs @cite might be applied. However, like in the unweighted case, application of a community detection method on a weighted graph without taking the nature of interaction process into account, might lead to unsatisfactory results. For example, if conductance minimization algorithm is applied on a weighted graph, whose weights are a consequence of a non-conservative process, the structure detected might differ significantly from ground truth. Our method on the other hand learns the weights from the interaction process and then detects structure dynamically. Learning the underlying interaction process from the activity logs of nodes of the network and using this process to determine the community structure is the course of future work.
- Community detection methods are used to reveal the structure of complex networks. @cite found core and whiskers' structure of real-world networks using conductance-based methods and argued that these methods cannot reveal any further structure in the giant core. Song @cite claimed that there exist self-repeating patterns in complex networks at all length scales. Our results corroborate this claim, as we show a repeating core and whiskers' pattern in the Digg social network at many different length scales.
- It can be shown that some of the interaction models described above not only solve certain regularized Semi-Definite Programs but also give fast solutions to these problems @cite .
- As well as the final bound we obtain being of a similar order to that of Aubrun, the reader familiar with @cite will notice that we need to prove some analogous combinatorial lemmas. However, it does not seem clear that the results given here could be obtained as a formal consequence of @cite , or indeed vice versa; it would be interesting to determine whether this is the case. We also note two minor technical differences between this work and @cite : Aubrun's result is only stated for @math (some recent work by Banica and Nechita removes this restriction @cite ), and only for @math constant (so @math grows as a constant fraction of @math ). By contrast, here @math , @math and @math can be essentially arbitrary, although the bound we obtain becomes trivial if @math is too large as a fraction of @math .
- Some very recent work by Collins, Fukuda and Nechita @cite also uses related techniques to those which we use here to prove Theorem (e.g. calculations with Weingarten functions @cite , see Section below). The goal of @cite was to find the state which, when input to a tensor product of two random quantum channels (either the same channel, or a channel and its conjugate), achieves minimal output entropy. A sequence of recent papers by Collins and Nechita @cite @cite @cite @cite carries out a number of interesting quantum information-theoretic calculations using Weingarten functions. This method seems to be a powerful tool which may be expected to find many other applications in quantum information.
- @cite give a polynomial algorithm which augments any geometric planar graph to 2-vertex connected or 2-edge connected geometric planar graph, respectively, but no bounds are given on the length of the augmented edges. T 'oth @cite improves the bound on the number of necessary edges in such augmentations, and Rutter and Wolff @cite prove that it is NP-hard to determine the minimum number of edges that have to be added in such augmentations.
- T 'oth and Valter @cite characterize geometric planar graphs that can be augmented to 3-edge connected planar graphs. Later Al- @cite gave a tight upper bound on the number of added edges in such augmentations. Finally, Garc ' @cite show how to construct a 3-connected geometric planar graph on a set of points in the planar with the minimum number of straight line edges of unbounded length.
- A related problem is studied in @cite . The authors prove that it is NP-hard to decide whether @math contains a spanning planar graph of minimum degree 2 even if @math itself has minimum degree 2. They also posed and studied the problem of finding the minimum radius @math so that @math has a geometric planar spanning subgraph of minimum degree 3 provided that @math has a spanning subgraph of minimum degree 3.
- Closely related is the research by @cite which shows that if @math is connected then @math has a 2-edge connected geometric planar spanning subgraph. The construction starts from a minimum spanning tree of @math which in turn is augmented to a 2-edge connected geometric planar spanning subgraph of @math . In the same paper several other constructions are given (starting from more general connected planar subgraphs) and also bounds are given on the minimum number of augmented edges required. However, the question of providing an algorithm for constructing the smallest @math such that @math has a 2-edge connected geometric planar spanning subgraph remained open. This question turns out to be the main focus of our current study.
- Our problem is also related to the well-known bottleneck traveling salesman problem, i.e. finding a Hamiltonian cycle that minimizes the length of the longest edge, since such a cycle is 2 edge conected (but not necessarily planar). @cite gave a 2-approximation algorithm for this problem and also showed that there is no better algorithm unless @math . There is also literature on constructing @math edge connected subgraphs with minimum number of edges. In @cite it is proved that given a @math -edge connected graph there is an algorithm running in time @math which finds a @math -edge connected spanning subgraph whose number of edges is @math times the optimal, where @math is the number of edges and @math the number of vertices of the graph. An improvement is provided in @cite in which a 4 3 approximation algorithm is given. Later, @cite provided a 5 4-approximation algorithm. However in these results the resulting spanning subgraphs are not guaranteed to be planar.
- in @cite use dynamic programming to assign charging schedules that provide frequency regulation. Similarly, in the work by @cite an algorithm is constructed that makes dynamic decisions for lowest-cost charging schedules of PEVs. Neither of these works considers an obligation to the market that needs to be met, and the demand of each vehicle is considered individually (i.e., not as part of a fleet); both suggested algorithms would result in an increased peak demand. In @cite , establish a decentralized algorithm that determines an equilibrium price so that the total amount of charging done in the fleet fills the overnight demand valley.' This algorithm takes into account all vehicles in the fleet, but is not dynamic (i.e., it is solves an offline problem) and assumes only a few types of driving behaviors exist. A dynamic (or online) algorithm does not require that all vehicles are connected to the grid at the same time to exactly report their future driving schedules, and makes decisions for each vehicle without solving a problem that depends on knowing the demands of the entire fleet.
- Embeddings of subsets @math into normed spaces were studied in geometric functional analysis @cite @cite . In particular, Klartag and Mendelson @cite were concerned with embeddings into @math . They showed that for @math there exists a linear map @math such that @math One can choose @math to be an @math random matrix with Gaussian entries as in Theorem , or with sub-gaussian entries. Schechtman @cite gave a simpler argument for a Gaussian matrix, which also works for embeddings into general normed spaces @math . In the specific case of @math , Schechtman's result states that for @math one has @math This result also follows from Lemma below.
- The problem of one-bit compressed sensing was introduced by Boufounos and Baraniuk @cite . Jacques, Laska, Boufounos and Baraniuk @cite realized a connection of this problem to uniform tessellations of the set of sparse signals @math , and to almost isometric embedding of @math into the Hamming cube @math . For this set @math , they proved Corollary with @math and a version of Theorem for @math . The authors of the present paper analyzed in @cite a bigger set of compressible'' signals @math and proved for @math a version of Corollary with @math . Since the mean widths of both sets @math and @math are of the order @math , Theorem holds for these sets with @math . In other words, apart from the dependence of @math (which is an interesting problem), the prior results follow as partial cases from Theorem .
- It is important to note that Theorem addresses only the theoretical aspect of one-bit compressed sensing problem, which guarantees that the quantized measurement map @math well preserves the geometry of signals. But one also faces an algorithmic challenge -- how to efficiently recover @math from @math , and specifically in polynomial time. We will not touch on this algorithmic aspect here but rather refer the reader to @cite and to our forthcoming work which is based on the results of this paper.
- The lattice of global snapshots is a key notion in modeling behavior of asynchronous systems @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite , and is widely used in areas such as distributed program debugging @cite @cite and fault-tolerance @cite . One critical challenge is that the lattice of snapshots evolves to exponential size in the worst-case. Various schemes are used to cope with the lattice explosion problem @cite @cite @cite @cite @cite . For example, in @cite @cite , the authors proposed the computation slice to efficiently compute all global states which satisfy a regular predicate @cite . In @cite , the authors discussed that certain (useless) part of the lattice can be removed at runtime to reduce the size of lattice. In this work, we make use of the observation that, in many tracking monitoring applications, it is often prohibitive and, more importantly, unnecessary to process the entire streams. Thus, we use sliding windows over distributed event streams to reduce the size of the lattice.
- Finite-state have been an active area of research, with numerous articles studying their algebraic and algorithmic properties. See @cite for a comprehensive exposition. An important problem for is that of determinization @cite @cite . A deterministic is defined in the usual sense: no two outgoing transitions from a state share the same input label. It has been shown that there are that do not admit equivalent deterministic . In contrast, the cost register automata that we introduce in this paper are deterministic machines with equivalent expressive power and equally efficient decision problems as weighted automata. We believe that this makes them a more suitable model for expressing weighted computations.
- In @cite , the authors introduce weighted logic for infinite words. In order to address convergence of the weighted sum, the authors assume discounting along later edges in a path ( , future discounting). Extending the results of this paper to discounted weighted computations over infinite words remains open.
- A wide variety of different models have been proposed to model string and tree transductions. The models that are most relevant to this paper are MSO-definable transductions @cite @cite and macro tree transducers @cite @cite . An MSO-definable graph transduction specifies a function between sets of graphs; the nodes, edges and labels of the output graph are described in terms of MSO formulas over the nodes, edges and labels of a finite number of copies of the input graph. A macro tree transducer (MTT) is a top-down tree to tree transducer equipped with parameters. Parameters can store temporary trees and append them to the final tree during the computation. In general, MTT are more expressive than MSO-definable tree transductions. A subclass of MTTs obtained by restricting the number of times a subtree and a parameter can be used has been shown to be equi-expressive as MSO-definable tree transductions @cite . In addition to these models, formalisms such as attribute grammars @cite , attribute tree transducers @cite have also been studied.
- Streaming tree transducers @cite (STTs), introduced by two of the co-authors in this paper are a new formalism for expressing MSO-definable tree-to-tree and string-to-tree transductions. In comparison to some of the transducer models discussed above, STTs have distinguishing features that make them desirable as a canonical model for specifying regular or MSO-definable transductions: (1) STTs produce the output in linear time by performing a single pass over the input, (2) they preserve desirable properties such as closure under sequential composition and regular look-ahead, and (3) they have good algorithmic properties such as decidability of functional equivalence.
- Data languages allow finite strings over data values that can be drawn from a possibly infinite data domain @cite , @cite @cite . Register automata are often used as acceptors for data languages. A key feature of such automata is that they allow registers to store and test data values. Beyond the similarity in nomenclature, register automata that are studied in this line of work are quite distinct from cost register automata introduced in this paper. The former are essentially defined over an infinite input alphabet, and the critical difference lies in the fact that almost every variant of data automata allows testing equality of data values, which mostly causes interesting decision problems to become undecidable. Cost register automata use the cost registers in a strictly write-only fashion, which makes them incomparable to variants of data automata that use read write registers.
- It is clear that the notions proposed in this line of work are orthogonal to our characterization of regularity of cost functions. The authors state that the motivation for the work in @cite @cite is preserving nice algorithmic and closure properties of regular languages for problems such as equivalence and projection. However, the integer values in these functions are considered modulo an equivalence which preserves existence of bounds on the function values, but not the values themselves. We believe that the notion of regularity of cost functions that we propose in this paper is closer to the classical notions of regularity such as MSO-definability.
- In @cite , and more recently in @cite @cite , the authors present the problem of deriving affine relations among variables of a program. An affine relation is a property of the form @math , where @math are program variables that range over a field such as the rationals or reals and @math are constants over the same domain. An affine program is a program with nondeterministic branching where each edge of the program is labeled with an assignment statement of the form @math , , where the RHS is an affine expression. We could define a over the cost model @math , with the cost grammar @math . An affine program is then simply obtained by ignoring the input labels of the transitions in such a . While the cost functions defined by such do not have interesting regularity properties, we remark that the equivalence of such can be checked in polynomial time by using ideas similar to the ones in @cite .
- A quantitative language @cite @cite @cite over infinite words is a function @math . Such languages are generated by weighted automata, where the value of a word @math is set as the maximal value of all runs over @math . By defining various value functions such as @math , @math , @math , @math , different values can be computed for the run of a weighted automaton on the string @math . Quantitative languages use the fixed syntax of weighted automata, and thereby restricted to having a single weight along each transition in their underlying automata. Moreover, they face similar difficulties in determinization: for interesting models of value functions, the corresponding automata cannot be determinized. An extension of to @math -regular cost functions could prove to be a more expressive and robust model to specify quantitative languages and to analyze their decision problems.
- Erd o s @cite gave an @math algorithm that reconstructs a phylogeny with high probability, assuming the Cavender-Farris model of evolution, for sufficiently long sequences. For most trees, their algorithm runs in @math time and requires @math sequence length. Cs u ros @cite provided a @math algorithm with similar performance guarantees. Recent papers @cite @cite give similar algorithms to identify parts of the tree that can be reconstructed. These approaches use quartet queries chosen so that, with high probability, only correct quartets are queried. The only sub-quadratic time algorithm with guarantees on reconstruction accuracy is by King @cite ; for most trees, its running time is @math provided that the sequences are @math in length.
- Mossel @cite gave a phase transition for phylogenetic reconstruction. Suppose we have a balanced phylogeny with all edges having mutation probability @math . If @math is less than @math , then the phylogeny can be reconstructed correctly from sequences of length @math sequence data. For larger @math , sequences must be of polynomial length to allow constant probability of reconstructing the phylogeny. Daskalakis, Mossel and Roch @cite extended the phase transition result to unbalanced trees, and provided an @math algorithm that reconstructs phylogenies with lengths below the phase transition. Mihaescu, Hill and Rao @cite provided an @math algorithm for the same problem, which bears some resemblance to our approach.
- In parallel to these theoretical results, many researchers have developed phylogeny reconstruction algorithms that can analyze alignments of tens, or even hundreds, of thousands of taxa. Fast Neighbour Joining @cite runs in @math time and gives results similar to neighbour joining, which requires @math time. FastTree @cite reconstructs phylogenies without computing the full distance matrix, which results in @math runtime. Our own recent algorithm, QTree @cite @cite , runs in @math time, using an incremental approach to building trees. No theoretical guarantees exist for the quality of solutions obtained from these fast algorithms, however, under realistic assumptions about the evolutionary model, for short sequences.
- The localization problem and its variants has attracted significant research interest in recent years. A general survey of the area and an overview of recent techniques can be found in @cite and @cite , respectively. The problem is also closely related to dimensionality reduction @cite and manifold learning @cite in which the objects data come from a high dimensional space, and the goal is to compute a low-dimensional, neighbourhood preserving embeddings.
- Perhaps a more practical and interesting case is when there is no central infrastructure. @cite identifies a common three-phase structure of three, popular, distributed sensor-localization algorithms, namely robust positioning @cite , ad-hoc positioning @cite and N-hop multilateration @cite . Table illustrates the structure of these algorithms. In the first phase, nodes share information to collectively determine the distances from each of the nodes to a number of anchors. Anchors are special nodes with a priori knowledge of their own position in some global coordinate system. In the second phase, nodes determine their position based on the estimated distances to the anchors provided by the first phase and the known positions of the anchors. In the last phase, the initial estimated positions are iteratively refined. It is empirically demonstrated that these simple three-phase distributed sensor-localization algorithms are robust and energy-efficient @cite . However, depending on which method is used in each phase, there are different tradeoffs between localization accuracy, computation complexity and power requirements. In @cite , a distributed algorithm-called the Gradient algorithm- was proposed; it is similar to ad-hoc positioning @cite but uses a different method for estimating the average distance per hop.
- Our first result, specifically the analysis of MDS-MAP , has a similar flavour as in @cite . We provide a theoretical guarantee that backs up experimental results. We use shortest paths as our primary guess for the missing entries in the distance matrix and apply MDS to find the topology of the network. In contrast to @cite , we require weaker assumptions for our results. More specifically, we assume that only neighbouring sensors have information about each other and that only connectivity information is known. Furthermore, the knowledge of detection probabilities plays no role in our analysis or the algorithm. And last, in our analysis we assume that the average degree grows logarithmically-not linearly- with the number of sensors, which results in needing many less revealed entries in the distance matrix. In particular, the last condition is quite realistic: If the average degree grows any slower then the network is not even connected (more on this issue in ). As the shortest paths algorithm works for both rage-free and range-aware cases, our analysis includes both and provides the first error bounds on the performance of MDS-MAP .
- Of particular interest are the two new results on the performance of sensor localization algorithms. In @cite , proposes a new reconstruction algorithm based on semidefinite programming where they could establish lower and upper bounds on the reconstruction errors of their algorithm. Similarly, in @cite , due to new advances in matrix completion methods @cite , the authors analyse the performance of OptSpace @cite , a novel matrix completion algorithm, in localizing the sensors. Interestingly, they did not need to adhere to the assumptions made by @cite . However, they have a restrictive assumption about the topology of the network, specifically, sensors are scattered inside an annulus.
- All the above analytical results crucially rely on the fact that there is a central processor with access to the inter-sensor distance measurements. However, as we have mentioned earlier, centralized algorithms suffer from the scalability problem and require higher computational complexity. Hence, a distributed algorithm with similar a performance bound is desirable. In our second result, we analyse the reconstruction error of a distributed sensor localization algorithm. To the best of our knowledge we show for the first time that Hop-TERRAIN , introduced in @cite , achieves a bounded error when only local connectivity information is given.
- in @cite use a decision analysis that integrates and extends Receiver Operating Characteristics (ROCs) to provide an expected cost metric. They demonstrate that the optimal operation point of an IDS depends not only on the system's own ROC curve and quantities such as the expected rate of false positives, false negatives, and the cost of operation, but also on the degree of hostility of an environment in which the IDS is situated, such as the probability and the type of an intrusion. Hence, the performance evaluation of an IDS has to take into account both the defender's side and attacker's side. In @cite , a network security configuration problem is studied. A nonzero-sum stochastic game is formulated to capture the interactions among distributed intrusion detection systems in the network as well as their interactions against exogenous intruders. The authors have proposed the notion of security capacity as the largest achievable payoff to an agent at an equilibrium to yield performance limits on the network security, and a mathematical programming approach is used to characterize the equilibrium as well as the feasibility of a given security target.
- To identify important factors for the performance of an IDS is another crucial investigation. In @cite , observe several architectural and system parameters that contribute to the effectiveness of an IDS, such as operating system structure, main memory bandwidth and latency as well as the processor micro-architecture. Memory bandwidth and latency are identified as the most significant contributors to sustainable throughput. CPU power is important as well; however, it has been overlooked in the experiments due to the existence of other closely related architectural parameters, such as deep pipelining, level of parallelism, and caching.
- In @cite , the authors investigate the prediction of resource consumption based on traffic profile. An interesting result, which we assume to be available in this paper, is that both CPU and memory usage can be predicted with a model linear in the number of connections. Equally important is the confirmation that the factoring of IDS resource usage with per-analyzer and per-connection scaling is a reasonable assumption. The authors use this finding to build an analyzer selection and configuration tool that estimates resource consumption per analyzer to determine whether a given configuration is feasible or not. The constraint used is a target CPU load below which the load should remain for a predefined percentage of time. The actual selection of a feasible analyzer set is however left as a manual task for the IDS operator. In our work, we propose a more informed and automated way of IDS configuration decision that takes into account the resource utilization per IDS library as well as the expected intrusion context based on experienced attack sequences or graphs.
- Seminal work by Mancinelli et. al @cite has shown how to encode the installability problem for software packages as a SAT problem, established the (NP-Hard) complexity of the problem, and shown applications of the encoding to improve the quality of package repositories by avoiding non installable packages. Based on the same formalization, various quality metrics have been established, such as strong dependency and sensitivity @cite (to evaluate the importance'' of a package in a given repository) and strong conflicts @cite (to pinpoint packages which might hinder the installation of several other packages). In the same vein, package meta-data have also been used to predict future (non) installability of software packages @cite . The abundance of studies that rely on package meta-data testifies the importance of the correctness of those meta-data.
- An advantage of self-contained software bundles is the ease of testing and deployment, as system-specific configurations and libraries have only limited impact on the software bundle. However, statically linking all libraries used by a bundle requires much disk space. If many applications include the same statically-linked libraries, these libraries are duplicated within the same system. Deduplication addresses this problem @cite @cite . Memory and storage deduplication merge same-contents chunks on block level, and reduce the consumptions of physical memory. By sharing identical chunks of storage, logical-level redundancies caused by static linking are resolved on the physical level.
- p04 and pdb06 studied a more general class of aggregates and developed a systematic theory of aggregates in logic programming based on the approximation theory @cite . The resulting theory covers not only the stable models semantics but also the supported-model semantics and extensions of 3-valued Kripke-Kleene and well-founded semantics. The formalism introduced and studied by p04 and pdb06 allows for arbitrary aggregates (not only monotone ones) to appear in the bodies of rules. However, it does not allow for aggregates to appear in the heads of program clauses. Due to differences in the syntax and the scope of semantics studied there is no simple way to relate p04 and pdb06 formalism to programs with monotone (convex) constraints. We note though that programs with abstract monotone constraints with the heads of rules of the form @math can be viewed almost literally as programs in the formalism by p04 and pdb06 and that they have the same stable models according to the definitions we used in this paper and those by p04 and pdb06 .
- flp04 developed the theory of disjunctive logic programs with aggregates. Similarly as p04 and pdb06 , flp04 do not allow for aggregates to appear in the heads of program clauses. This is one of the differences between that approach and programs with monotone (convex) constraints we studied here. The other major difference is related to the postulate of the minimality of stable models (called answer sets in the context of the formalism considered by flp04 ). In keeping with the spirit of the original answer-set semantics @cite , answer sets of disjunctive programs with aggregates, as defined by flp04 , are minimal models. Stable models of programs with abstract constraints do not have this property. However, for the class of programs with abstract monotone constraints with the heads of rules of the form @math the semantics of answer sets defined by flp04 coincides with the semantics of stable models by mt04 and mnt03,mnt06 .
- For many graph problems, the state-of-the-art algorithms are extremely fast even if the network is very large---provided that @math is small. For example, the following problems can be solved in @math synchronous communication rounds (assuming @math -bit node identifiers): [noitemsep] maximal matching @cite , vertex colouring with @math colours @cite @cite , edge colouring with @math colours @cite . There are also problems that can be solved in @math rounds, independently of @math (even in anonymous networks without unique identifiers): [noitemsep] maximal matching in @math -coloured graphs @cite , maximal edge packing @cite , @math -approximation of minimum vertex cover @cite . For each of these problems, the dependence on @math in the running time is well-understood. In particular, Linial's @cite lower bound shows that maximal matching, vertex colouring, and edge colouring require @math rounds, even if @math .
- Some @math upper bounds are known for graph problems. For example, good approximations of fractional matchings can be found in @math rounds @cite ; however, this does not seem to yield a deterministic @math -time algorithm for any of the above problems. Ha 'n ''s @cite algorithm finds a maximal matching in @math rounds, avoiding the linear dependence on @math ; however, it comes at the cost of a non-optimal dependence on @math .
- It is easy to come up with an artificial problem with the complexity of @math , but so far no such tight results are known for classical graph problems such as maximal matchings. The lower-bound result by Kuhn and Wattenhofer @cite comes close, but it only applies to a restricted family of algorithms.
- As far as we know, the first work to investigate the relation between differential privacy and information-theoretic leakage was @cite . In this work, a channel is relative to a given database @math , and the channel inputs are all possible databases adjacent to @math . Two bounds on leakage were presented, one for teh R 'enyi min entropy, and one for Shannon entropy. Our bound in Proposition is an improvement with respect to the (R 'enyi min entropy) bound in @cite .
- Clarkson and Schneider also considered differential privacy as a case study of their proposal for quantification of integrity @cite . There, the authors analyze database privacy conditions from the literature (such as differential privacy, @math -anonymity, and @math -diversity) using their framework for utility quantification. In particular, they study the relationship between differential privacy and a notion of leakage (which is different from ours - in particular their definition is based on Shannon entropy) and they provide a tight bound on leakage.
- In @cite the authors aim at obtaining optimal-utility randomization mechanisms while preserving differential privacy. The authors propose adding noise to the output of the query according to the geometric mechanism. Their framework is very interesting in the sense it provides a general definition of utility for a mechanism @math that captures any possible side information and preference (defined as a loss function) the users of @math may have. They prove that the geometric mechanism is optimal in the particular case of counting queries. Our results in Section do not restrict to counting queries, but on the other hand we only consider the case of binary loss function.
- As mentioned earlier, diffusion and growth processes have been studied intensely in the past few years in relation to many areas such as sociology, economics and engineering. Among the models studied are (see, for example @cite , @cite , @cite ), which was first introduced by Clifford and Sudbury in @cite and has been much studied since in, for example, @cite , @cite , @cite , (see, for example, @cite ), (see @cite , @cite ), and (see @cite ).
- The main result of this paper is a quantitative relation between entropic uncertainty and nonlocality. The fact that the incompatibility of local measurements and nonlocality are related in some way is folklore knowledge and follows, for example, from the work of Tsirelson @cite . For the case when the systems are restricted to qubits, a bound on the maximal CHSH value in terms of the angle between local measurements has been derived by Seevink and Uffink @cite . The analytical form of Relation has been conjectured by Horodecki @cite and derived independently by Lim @cite for the case of single qubit systems. Mayers and Yao have shown that in order to reach the maximal CHSH value allowed by quantum physics, the state and measurements essentially need to be (equivalent to) a fully entangled state and optimal CHSH measurements even when they are embedded in higher dimensions @cite @cite . They also employed this result in quantum cryptography, where they used it to construct self-testing sources.
- On a related topic, Oppenheim and Wehner @cite ,| ,for a class of generalized physical theories that includes quantum mechanics and classical theory ,| ,showed that the presence of uncertainty, via steering, directly limits the maximally achievable nonlocality. Our result can be seen as complementary to theirs, as we show that in order to achieve a certain nonlocality, at least some specific amount of uncertainty is necessary.
- Device-independent quantum key distribution @cite @cite @cite @cite @cite and randomness generation @cite usually bases security on a relation between nonlocality and the randomness of the outcomes relative to some (quantum) adversary. Our result allows to split the security analysis of these protocols into two parts: the nonlocality of the measured correlations first gives a bound on the uncertainty of local measurement outcomes, which in turn can be used to ensure security. The two parts can be analyzed independently and thus our methods can be used to simplify such an analysis and, potentially, reduce the required assumptions.
- Traditionally a regular expression query is processed by constructing a non-deterministic finite automata (NFA) which recognizes the language defined by the regular expression pattern @cite @cite @cite . The entire database is processed one character at a time resulting in @math time and @math space, where @math and @math are the size of the regular expression and database respectively.
- The two most commonly indexes which have been currently using in both database systems are B+trees @cite @cite and bitmaps @cite . Both of them are designed to support exact pattern matching rather than regular expression querying. Therefore, some database vendor tailor these indexes to support regular expression query, such as: full text indexing in PostgreSQL, and function-based index in Oracle. However, their application is criticized as quite restricted.
- In order to reduce the number of node while sustaining trie's performance, Weiner @cite transformed Trie to the first suffix tree structure, that the index is built by processing the database string from right to left which takes @math time. Later, Ukkonen @cite developed a left-to-right built algorithm that maintains a suffix tree which takes @math time. In later research, this suffix tree was later augmented with suffix link @cite to form a directed acyclic word graph which leads to an algorithm for the construction of the automata.
- In another stream of research, inverted files @cite and q-grams @cite @cite @cite are two designs which support information retrieval and informatics. However, both are not suitable for regular expression queries because they rely on a predefined list of words.
- More recently, a relatively different research strand has been developed to support regular expression querying. The initial work in this area was carried out by Cho and Rajgopalan - the authors who denote their index as FREE multigram index @cite .
- To overcome the weakness of FREE, Hore et. al. @cite (in a 2004 CIKM paper) proposed a multigram selection algorithm called BEST which takes both the database a query workload into consideration. In the BEST algorithm, each multigram is associated with a cost factor @math (equivalent to the support) and at the same time associated with the query set by a benefit factor, @math . The benefit of the multigram is equal to the number of records that can be pruned when utilized by a query. The ratio of @math forms the which is defined as the objective function to optimize both the index efficiency and query hit rate at the same time.
- Hore formalizes the multigram selection problem as an instance of the Budgeted Maximum Coverage (BMC) @cite problem. Specifically the cover set forms the Budgeted' part while the utility of the index is captured in the Coverage' component. The main principle of the BEST algorithm is to select multigrams which increase the index hit rate so more queries can utilize the algorithm. However, a major weakness of the BEST algorithm is that it neither scales to large datasets nor large query workloads because it uses a cartesian product of query workload and database as the search space.
- The idea of identifying choices to avoid combining in some expression the left and right alternatives of the same choice appears in @cite . The idea is developed in the framework of a natural semantics for the translation of (flat) Curry programs into Haskell. A proof of the correctness of this idea will appear in @cite which also addresses the similarities between the natural semantics and graph rewriting. This discussion, although informal, is enlightening.
- The subject of user participation in mass collaboration systems has been already touched by several authors, for example on social networking sites @cite , and knowledge sharing platforms @cite . A momentum'' law has been proposed for the distribution of user life edits of inactive users @cite . The distribution of user account lifespans has been shown to decay with a heavy tail, and a power-law model has been proposed after this observation @cite . Empirical data from Wikipedia, however, seem to support a super-position of different regimes @cite ; a feature of the model we present here is indeed a bimodal distribution of user lifespans. In the context of wikis and other free open source initiatives some authors have used survival analysis to outline the diffences between different communities, @cite but this modeling technique is not suited to understand the connection between social influence, group coordination, and user retention. We advocate the need to explicitly model such processes explicitly.
- Essentially, the objective of each of the above is to realize a transformation of the normative framework to accommodate some form of shortcoming. These shortcomings can be identified in several ways: All of these can be viewed as characterising emergent @cite approaches to the evolution of normative frameworks, where some mechanism, either in the framework, or in the environment, is used to revise the norms. In the approach taken here, the designer presents use cases that effectively capture the behavioural requirements for the system, in order to fix' bad states. This has an interesting parallel with the scheme put forward by Serrano and Saugar @cite , where they propose the specification of incomplete theories and their management through incomplete normative states identified as pending''.
- Besides the already cited papers @cite @cite on Bethe-approximation-based methods to the permanent of a non-negative matrix, some aspects of the Bethe free energy function were analyzed by Watanabe and Chertkov in @cite and by Chertkov, Kroc, Krzakala, Vergassola, and Zdeborov 'a in @cite . (In particular, the paper @cite applied the loop calculus technique by Chertkov and Chernyak @cite .) Very recent work in that line of research is presented in a paper by A. B. Yedidia and Chertkov @cite that studies so-called fractional free energy functionals, and resulting lower and upper bounds on the permanent of a non-negative matrix.
- Because computing the permanent is related to counting perfect matchings, the paper by Bayati and Nair @cite on counting matchings in graphs with the help of the SPA is very relevant. Note that their setup is such that the perfect matching case can be seen as a limiting case (namely the zero-temperature limit) of the matching setup. However, for the perfect matching case (a case for which the authors of @cite make no claims) the convergence proof of the SPA in @cite is incomplete. Moreover, their matchings are weighted only inasmuch as the weight of a matching depends on the size of the matching. Consequently, because all perfect matchings have the same size, they all are assigned the same weight. (See also the related paper by Bayati, Gamarnik, Katz, Nair, and Tet ali @cite , and an extension to counting perfect matchings in certain types of graph by Gamarnik and Katz @cite .) For an SPA convergence analysis of a slightly generalized weighted matching setup, the interested reader is referred to a recent paper by Williams and Lau @cite .
- The present paper has some similarities with recent papers by Barvinok on counting zero one matrices with prescribed row and column sums @cite and by Barvinok and Samorodnitsky on computing the partition function for perfect matchings in hypergraphs @cite . However, these papers pursue what would be called a mean-field theory approach in the physics literature @cite . An exception to the previous statement is Section 3.2 in @cite , which contains Bethe-approximation-type computations. (See the references in that section for further papers that investigate similar approaches.)
- As mentioned in the abstract, the present paper discusses a combinatorial characterization of the Bethe permanent in terms of permanents of so-called lifted versions of the matrix under consideration. For this we use results from @cite that give a combinatorial characterization of the Bethe partition function of a factor graph in terms of the partition function of graph covers of this factor graph. Interestingly, very similar objects were considered by Greenhill, Janson, and Ruci 'n ski @cite ; we will comment on this connection in .
- Basically, the research directions are represented either by modifications of the underlying graph during preprocessing @cite @cite @cite , or by adjusting a query algorithm @cite @cite in order to resolve simple types of restrictions during queries.
- The first, and seemingly the simplest, solution is commonly used as it makes a road network graph maneuver-free and so there is no need to adjust the queries in any way. Unfortunately, it can significantly increase the size of the graph @cite ; for instance, replacing a single turn-prohibition can add up to eight new vertices in place of one original @cite . A solution like this one thus conflicts with the aforementioned (graph-size) objectives. Another approach @cite uses so-called dual graph representation instead of the original one, where allowed turns are modeled by dual edges.
- These and other initiatives, aiming at integration of ground-based monitoring efforts, are leading to an evolution from single site environmental monitoring into networks for environment observation @cite . This evolution culminates with the current vision for a Sensor Web @cite @cite @cite , encompassing several types of deployments of sensor systems, interconnecting them globally through a Web-based integration strategy using standards developed by the Sensor Web Enablement ( http: www.opengeospatial.org projects groups sensorweb ) Working Group of the Open Geospatial Consortium, Inc. (OGC) ( http: www.opengeospatial.org ).
- A small clarification on the definition for (wireless) sensor networks may be in order. Mainly within Computing Science (CS) research @cite @cite and in earlier Sensor Web related efforts @cite , this definition is narrower than what is used in this paper. In this more restrictive definition, a (wireless) sensor network is based on nodes (also known as motes'') that have sensing, data storage processing, and communication components plus a power source. These nodes are usually autonomous and operate cooperatively---by communicating amongst themselves---to collect and process data, also being programmable, , able to behave differently according to, for instance, the type of application, power supply conditions, environmental conditions, Although we have used this type of wireless nodes in our deployments, we do not require the capability of offering communication amongst network's components. Instead, we adopt the centralized type of processing architecture as classified by @cite , being more in line with the current Sensor Web approach to networks @cite . It is sufficient for us, for instance, that the connection of sensing elements be done at the level of integrated data products.
- To the best of our knowledge, none of the deployment efforts reviewed here address the same scenario as ours: having (multi-year) long term deployments, based on cooperative efforts of several (heterogeneous) teams, using commercially available equipment from multiple manufacturers, with an integrated effort of data retrieval, quality control and data availability through an easy to use Web-based platform. We believe this is a more realistic scenario for ground-based environmental monitoring efforts. The current efforts within the Life Under Your Feet project ( http: lifeunderyourfeet.org ) @cite are the closest to our own, also having long term, spatially distributed deployments with a Web-based data visualization interface integrated with geolocation information. However, they do not seem to deal with heterogeneous equipment and data formats, nor offer filtering aggregation options, derived datasets or quality information in their data management solution.
- Although it is common to think about sensor data management as stream data management, with the associated challenges (on-line aggregation, classification, ) @cite , at least within environmental research, particularly in ground-based monitoring, this is not a frequent scenario. Most of the current applications based on sensor data use the perspective of historical (or an archive of) time series data. Applications using the stream data perspective are only beginning to appear, and the current applications that do require that perspective---e.g., volcano monitoring @cite ---are still the exception. Data manipulation for most of the current applications is done after having the data collected and stored, applying a variety of analytical operations in an offline fashion @cite @cite .
- Middleware software for automating control of deployments is also the focus of current research efforts, in form of architectures for integrating different network deployments @cite , or Web-based interfaces for interaction with and control of wireless deployments @cite . Our focus, on the other hand, is on managing the data products rather than controlling the equipment from within our system.
- Well known ideas and algorithms from frequent itemset mining can be used for MRDs unaltered if applied on the join of all tables. The syntax of this type of patterns is essentially that of itemsets, with items in this case being attribute values and transactions being the tuples of the join table @cite @cite @cite . The characteristic of this pattern syntax is that a tuple always contains one attribute value per attribute and as a result it is impossible to have two values of the same attribute in the same pattern. An itemset of this type for instance would not be able to capture the fact that a director can be related to many films. This is something that an MCCS pattern naturally captures. However, itemsets on the join table can still capture co-occurrences of attribute values that belong to different attributes.
- On the other hand, the support, measured as the ratio of the tuples of the join table that contain an itemset, does not have a clear meaning as attribute values are replicated due to the join operation. A different approach is taken by Smurfig @cite where the support is measured with respect to every table, as the relative number of keys that the items correspond to.
- Finally one could see our work being connected to frequent sub-graph mining @cite @cite however besides being based on frequency, these methods are aimed at databases of many graphs rather than one connected graph.
- Bollob ' a s, Riordan, Spencer and Tusn ' a dy @cite proved rigorously the power law distribution of the Barab ' a si-Albert model @cite . Random Apollonian Networks were introduced in @cite . Their degree sequence was analyzed inaccurately in @cite (see comment in @cite ) and subsequently in @cite using physicist's methodology. Cooper & Uehara @cite and Gao @cite analyzed the degree distribution of random @math trees, a closely related model to RANs. In RANs --in contrast to random @math trees-- the random @math clique chosen at each step has never previously been selected. For example in the two dimensional case any chosen triangular face is being subdivided into three new triangular faces by connecting the incoming vertex to the vertices of the boundary. Darrasse and Soria analyzed the degree distribution of random Apollonian network structures in @cite .
- A channel model similar to the finite field channel considered in this paper is the deterministic (noiseless) channel. In the deterministic model, the channel output is the arithmetic summation of the channel inputs, and there is no noise. The deterministic model has been used to construct coding strategies and to gain insights for more general channels. This approach has been applied to the multiple-access channel @cite , the broadcast channel @cite , the interference channel @cite @cite , the deterministic TWRC @cite , and the deterministic multi-pair TWRC @cite . For the deterministic TWRC and the deterministic multi-pair TWRC, it has been shown that linear coding achieves the capacities, an observation similar to that in this paper for the finite field MWRC.
- The MWRC we consider herein, where each user is to decode the messages from all other users, can be seen as a generalization of the TWRC. Different extensions of the TWRC include: The multi-pair TWRC where multiple source-destination pairs exchange messages via one relay @cite @cite . Here, each destination only decodes the message from one source. The multi-pair TWRC where multiple users exchange messages with a base station via a relay @cite . Here, each user sends its message to the base station, and the base station sends different messages to each user. The TWRC with additional private messages from the users to the relay @cite @cite . The MWRC where the users are separated into different groups and all users in each group exchange messages among themselves @cite .
- The MWRC has also been studied from the point of view of source coding, where multiple users exchange possibly correlated data via a relay. In the source coding setting, the channel from the users to the relay and that from the relay to the users are assumed to be noiseless. The problem formulation is how many bits the users need to encode their respective messages to be sent to the relay; and after the relay receives these encoded messages, how many bits the relay needs to transmit to the users in order for each user to recover the messages of all other users. The three-user lossless case (where each user perfectly reconstructs the other two users' messages) was studied by Wyner @cite , the two-user lossless case and lossy case (where each user reconstructs the other user's message with a prescribed distortion) was studied by Su and El Gamal @cite , and the two-user lossy case with common reconstructions (where each user must also be able to determine the lossy reconstructed message of the other user) was studied by Timo @cite .
- Based on the valley-free property, Gao @cite proposed a relationship inference heuristic that classifies the AS links according to their connectivity degree. Gao's algorithm was refined by a follow-up work @cite (PTE) which introduced the use of Partial relationship Information (PI) as a starting point for the inference process.
- In @cite the Acyclic Type of Relationship (AToR) problem is defined. According to AToR when p2c relationships are assigned a directed edge, the resulting AS graph should be acyclic. In @cite the authors validate the acyclicity of the AS graph and propose a heuristic to solve the maximal AToR problem.
- These works are common in that they mainly relied on a single data source, i.e. the AS connectivity data. Although sophisticated heuristics have been used, the connectivity data itself is inherently limited in providing useful information for inferring AS relationships. Some heuristics can even introduce errors. Inference results produced by different existing algorithms are often inconsistent and sometimes conflict to each other. Two recent works @cite @cite showed that BGP simulations based on these data lead to poor results. In addition, existing algorithms are not capable of discovering any unconventional AS relationships.
- The classic work on social networks is rooted in the field of ; in late sixties Milgram and Travers @cite introduced the well-known theories of the and the . Zachary @cite , in his PhD thesis, formalized the first model of a real-life social network. Then, social networks started attracting the interest of different sciences, including Computer Science; for example Kleinberg @cite studied the algorithmic aspects of social networks while @cite analyzed their range of applicability.
- First, Leskovec and Faloutsos @cite cover sampling techniques applied to graphs, exploiting several algorithms for efficiently visiting large graphs and avoiding bias of data. Several work covers aspects regarding crawling techniques for mining social networks: @cite provide an exhaustive walk-through about this field.
- With respect to data collection aspects, the work by @cite on OSNs (in particular on Facebook) is closely related to our work.
- In this paper we describe in details several results, some of them already presented in a preliminary work @cite based on a significantly smaller sample acquired with a naive technique.
- @cite focus on the parallel crawling of OSNs; @cite refer they used a parallel crawler written in Python and running on a cluster of 56 machines, but avoiding technical details.
- In the second category we find works whose main goal is to discover properties of online social networks. Sometimes companies provide the complete OSN dataset, e.g. @cite studied the graph of a South Korean OSN, named CyWorld, and its scalability properties, in order to correctly estimate degree distribution and clustering coefficient in smaller samples. Leskovec @cite , in his PhD thesis, studied dynamics of large social networks analyzing data provided by Microsoft about the usage of their instant messaging platform, formerly called MSN. More frequently, social network services companies like Facebook are reluctant to share their data for research purposes. The only viable solution is to acquire this information crawling the front-end of the social networks, wrapping and managing public accessible data. @cite provided an exhaustive survey of these approaches, commonly referred to as Web Information Extraction techniques. @cite defined a breadth-first-search sampling methodology applied to the Facebook social graph and formalized some properties as and , easily verifiable in small regions, but not generalizable to the whole graph.
- Work belonging to the last category usually present SNA techniques applied to OSNs. @cite formalized a rigorous methodology to model OSNs, in particular to discover, if existing, aggregations of nodes covering particular positions in the graph (e.g. centrality positions) and clusters of particular relevance. @cite @cite focused on analyzing the structure of the of the graph, trying to define a generative model to describe the evolution of the network; they also introduced techniques of comparison of simulations against actual data to verify the reliability of the model. @cite illustrated several aspects of the analysis of OSNs; they crawled large OSNs, then analyzed several networks using methods formalized in the SNA, e.g. inspecting link symmetry, power-law node degrees, groups formation, etc.
- Finally, literature on analysis of the behavior of OSNs users is abundant: Golbeck and Hendler @cite described the evolution of trust relationships among them. Liben-nowell and Kleinberg @cite studied the link-prediction problem, formalizing the concept of proximity of networks users. Several studies characterized OSNs users behaviors @cite @cite @cite , thus @cite provided a starting point illustrating how to analyze OSNs (in particular Facebook) from a network perspective.
- In the field of , topologies are explored using pairwise end-to-end measurements, without the cooperation of nodes along these paths. This approach is quite flexible and applicable in various contexts, e.g., in social networks @cite . For a good discussion of this approach as well as results for a routing model along shortest and second shortest paths see @cite . For example, @cite shows that for sparse random graphs, a relatively small number of cooperating participants is sufficient to discover a network fairly well.
- The classic tool to discover Internet topologies is traceroute @cite . Unfortunately, there are several problems with this approach that render topology inference difficult, such as or , which has motivated researchers to develop new tools such as @cite @cite . Another complication stems from the fact that routers may appear as stars in the trace since they are overloaded or since they are configured not to send out any ICMP responses. The lack of complete information in the trace set renders the accurate characterization of Internet topologies difficult.
- This paper attends to the problem of anonymous nodes and assumes a conservative, worst-case'' perspective that does not rely on any assumptions on the underlying network. There are already several works on the subject. @cite initiated the study of possible candidate topologies for a given trace set and suggested computing the , that is, the topology with the minimal number of anonymous nodes, which turns out to be NP-hard. Consequently, different heuristics have been proposed @cite @cite .
- Our work is motivated by a series of papers by Acharya and Gouda. @cite , a network tracing theory model is introduced where nodes are irregular'' in the sense that each node appears in at least one trace with its real identifier. @cite , hardness results are derived for this model. However, as pointed out by the authors themselves, the irregular node model---where nodes are anonymous due to high loads---is less relevant in practice and hence they consider strictly anonymous nodes in their follow-up studies @cite . As proved in @cite , the problem is still hard (in the sense that there are many minimal networks corresponding to a trace set), even with only two anonymous nodes, symmetric routing and without aliasing.
- In contrast to this line of research on cardinalities, we are interested in the . If the inferred topologies share the most important characteristics, the negative results in @cite @cite may be of little concern. Moreover, we believe that a study limited to minimal topologies only may miss important redundancy aspects of the Internet. Unlike @cite @cite , our work is constructive in the sense that algorithms can be derived to compute inferred topologies.
- The four papers most related to the present one are the following. insightfully raised the idea that general complexity results may change in single-peaked societies. His manipulative-action example (STV) actually provides a case where single-peakedness fails to lower manipulation complexity, but in a different context he did find a lowering of complexity for single-peakedness. The papers , the first of which was in the TARK conference, then broadly explored the effect of single-peakedness on manipulative actions. These three papers are all in the model of (perfect) single-peakedness. , in the context of preference elicitation, raised and experimentally studied the issue of single-peaked societies. also discussed nearness to single-peakedness, and the papers @cite @cite both raise as open issues whether shield-evaporation (complexity) results for single-peakedness will withstand near-single-peakedness. The present paper seeks to bring the nearly single-peaked'' lens to the study of manipulative actions.
- Routing is a critical component of the Internet, and as such has long been studied by the scientific community. The problem of finding a interconnecting any two nodes of a graph is solved by well-known algorithms like Dijkstra and Bellman-Ford, which have been implemented in widely deployed protocols such as OSPF and RIP, respectively. However, interconnecting nodes through a single path (typically, the shortest) does not make the network resilient against failures and traffic surges. Hence, different techniques relying on have been proposed. For instance, ECMP @cite aims at balancing load over multiple paths of equal cost. In standard IP MPLS networks, the control and data planes are generally considered jointly; multipath routing is then achieved through a centralized algorithm, solving some standard multicommodity flow problem @cite @cite .
- Gabor functions @cite , in the form of Gabor filters (GFs) @cite and Gabor wavelets @cite , are applied for a multitude of purposes in many areas of image processing and pattern recognition. Basically, the intentions for using GF and log-GF @cite can be grouped into two categories: first, GF aim at enhancing images @cite and the second common goal is to extract Gabor features obtained from responses of filterbanks. Typical fields of application include:
- Texture segmentation @cite and classification @cite , with applications such as e.g. recognizing species of tropical wood @cite or classifying developmental stages of fruit flies @cite .
- In medical imaging, GFs are applied for the enhancement of structures like e.g. finger veins @cite and muscle fibers in ultrasound images @cite , for the detection of blood vessels in retinal images @cite , as well as for many other tasks like e.g. analyzing event-related brain activity @cite , assessing osteoporosis in radiographs @cite and for modeling the behavior of simple cells in the mammalian visual cortex @cite .
- GFs are utilized for text segmentation @cite , character recognition @cite , font recognition @cite , and license plate recognition @cite .
- Objects can be detected by GFs @cite , e.g. cars @cite . Moreover, GFs can be used for performing content-based image retrieval @cite .
- Gabor functions play an important role in biometric recognition. They are employed for many physical or behavioral traits including iris @cite , face @cite , facial expression @cite , speaker @cite , speech @cite , emotion recognition in speech @cite , gait @cite , handwriting @cite , palmprint @cite , and fingerprint recognition.
- Gabor filterbanks are used for the segmentation @cite and quality estimation @cite of fingerprint images, for core point estimation @cite , classification @cite and fingerprint matching based on Gabor features @cite @cite . GFs are also employed for generating synthetic fingerprints @cite . The use of GF for fingerprint image enhancement was introduced in @cite .
- Image quality @cite has a big impact on the performance of a fingerprint recognition system (see e.g. @cite and @cite ). The goal of image enhancement is to improve the overall performance by optimally preparing input images for later processing stages. Most systems extract minutiae from fingerprints @cite , and the presence of noise can interfere with the extraction. As a result, true minutiae may be missed and false minutiae may be detected, both having a negative effect on the recognition rate. In order to avoid these two types of errors, image enhancement aims at improving the clarity of the ridge and valley structure. With special consideration to the typical types of noise occurring in fingerprints, an image enhancement method should have three important properties:
- Enhancement of low quality images (occurring e.g. in all databases of FVC2004 @cite ) and very low quality prints like latents (e.g. NIST SD27 @cite ) is still a challenge. Techniques based on contextual filtering are widely used for fingerprint image enhancement @cite and a major difficulty lies in an automatic and reliable estimation of the local context, i.e. the local orientation and ridge frequency as input of the GF. Failure to correctly estimate the local context can lead to the creation of artifacts in the enhanced image @cite which consequently tends to increase the number of identification or verification errors.
- For low quality images, there is a substantial risk that an image enhancement step may impair the recognition performance as shown in @cite (results are cited in Table of Section ). The situation is even worse for very low quality images, and current approaches focus on minimizing the efforts required by a human expert for manually marking information in images of latent prints (see @cite and @cite ).
- The methods for constructing graphs with a given degree distribution are primarily either reductions to perfect matchings or sequential sampling methods. There are two popular perfect matching methods. The first is the @cite @cite : @math mini-vertices are created for each degree @math vertex, and all the mini-vertices are connected. Any perfect matching in the configuration graph corresponds to a graph with the correct degree distribution by merging all of the identified mini-vertices. This allows multiple edges and self-loops, which are often undesirable. See Figure . The second approach, the , prevents multi-edges and self-loops by creating a gadget for each vertex. If @math has degree @math , then it is replaced with a complete bipartite graph @math with @math and @math . Exactly one node in each @math is connected to each other @math , representing edge @math @cite . Any perfect matching in this model corresponds exactly to a simple graph by using the edges in the matching that correspond with edges connecting any @math to any @math . We use a natural extension of the first configuration model to the joint degree distribution problem.
- There are also sequential sampling methods that will construct a graph with a given degree distribution. Some of these are based on the necessary and sufficient Erd o s-Gallai conditions for a degree sequence to be graphical @cite , while others follow the method of Steger and Wormald @cite @cite @cite @cite @cite . These combine the construction and sampling parts of the problem and can be quite fast. The current best work can sample graphs where @math in @math time @cite .
- Another vein of related work is that of who introduce the concept of @math -series @cite @cite . In this model, @math refers to the dimension of the distribution and @math is the joint degree distribution. They propose a heuristic for generating random @math -graphs for a fixed @math distribution via edge rewirings. However, their method can get stuck if there exists a degree in the graph for which there is only 1 node with that degree. This is because the state space is not connected. We provide a theoretically sound method of doing this. Finally, Newman also studies the problem of fixing an assortativity value, finding a with that value, and then sampling a random graph with that distribution using Markov Chains @cite @cite . His Markov Chain starts at any graph with the correct degree distribution and converges to a pseudograph with the correct joint remaining degree distribution. By contrast, our work provides a theoretically sound way of constructing a simple graph with a given joint degree distribution first, and our Markov Chain only has simple graphs with the same joint degree distribution as its state space.
- In the area of environmental effects simulation, several methods for reproducing weather phenomena, like particle-based rain techniques, are presented in @cite . The authors describe in details those methods to render, in a real-time system, very complex atmospheric physical phenomena such as strong rainfall, falling raindrops dripping off objects' surfaces, various reflections in surface materials and puddles, water ripples and puddles on the streets and so on.
- Realistic animations of water, smoke, explosions, and related phenomena are reproduced via fluids simulation. @cite face the problem of high-resolution fluid motion in real-time videogame applications, describing some techniques on a scale previously unattainable in computer graphics. The central idea is to cover the simulation domain with a small set of simulation primitives, called tiles. Each tile consists of a velocity basis representing the possible flow within its sub-domain. The boundaries between sub-domains correspond to tile faces.
- An overview of Halo 3 's unique lighting and material system and its main components is treated in @cite . Halo includes key innovations in the following areas: spherical harmonics lightmap generation, compression and rendering; rendering complex materials under area light sources; HDR rendering and post-processing. Some of the effects presented in that work have been adopted also here (e.g. the HDR rendering).
- The idea of turning a collision of two simultaneous wireless transmissions into a useful transmission was first introduced in PNC @cite . In particular, the authors proposed a frame-based decode-and-forward strategy in packet forwarding. In their scenario of a relay network, two nodes transmit simultaneously to a common receiver. Assuming perfect transmission synchronization at the physical layer, based on the additive nature of simultaneously arriving (EM), the receiver detects a single collided signal which is the sum of the two transmitted signals. Using a suitable mapping scheme, they show that for certain modulation schemes, there exists a mapping scheme such that the relationship between the two transmitted binary bits and the decoded binary bit follows the (XOR) principle. ANC @cite was further proposed to relax the restrictions of symbol-level synchronization, carrier-frequency synchronization and carrier-phase synchronization required in PNC, which makes ANC more practical. Specifically, ANC is able to decode an unknown packet @math from a collided packet @math We use the notation @math to denote a collision of two packet transmissions. based on the known packet @math by leveraging the co-channel FM signal separation technique @cite and network layer information to cancel the interference.
- Without incentive-compatibility constraints, the welfare maximization problem with submodular bidder valuations is completely solved. Vondr 'a k @cite gave a @math -approximation algorithm for the problem, improving over the @math -approximation given in @cite . The algorithm in @cite works in the value oracle model, where each valuation @math is modeled as a black box'' that returns the value @math of a queried set @math in a single operation. The approximation factor of @math is unconditionally optimal in the value-oracle model (for polynomial communication) @cite , and is also optimal (for polynomial time) for certain succinctly represented submodular valuations, assuming @math @cite . The result of @cite implies that @math is the optimal approximation factor in our model as well, assuming @math . We show in Appendix that our oracle model is no more powerful than polynomial-time computation in the special case of explicitly represented coverage functions, for which @math is optimal assuming @math @cite . In contrast, the work of @cite improves on the approximation factor of @math by using , which can not be simulated in polynomial time for explicit coverage functions.
- Despite intense study, prior to this work, there were no truthful-in-expectation and polynomial-time constant-factor approximation mechanisms for welfare maximization with any non-trivial subclass of submodular bidder valuations. The best previous results, which apply to all submodular valuations, are a truthful-in-expectation @math approximation mechanism in the communication complexity model due to Dobzinski, Fu and Kleinberg @cite , and a universally-truthful A mechanism is universally-truthful if, for realization of a the mechanism's coins, each player maximizes his payoff by bidding truthfully. Universally truthful mechanisms are defined formally in Section @math approximation mechanism in the model due to Dobzinski @cite .
- The aforementioned works @cite @cite are precursors to our general design framework that optimizes directly over the output of a randomized rounding algorithm. In the framework of Lavi and Swamy @cite , the input to and output of the rounding algorithm are assumed to coincide up to a scaling factor, so optimizing over its input (as they do) is equivalent to optimizing over its output (as we do). In the result of @cite , optimizing with respect to their proxy bidders'' is equivalent to optimizing over the output of a particular randomized rounding algorithm.
- Distributed algorithms can be expressed using a variety of communication models ( message passing, mailboxes, shared memory). Although a vast majority of algorithms is designed in one of these models -- predominantly the message passing model --, the very fact that one of them is chosen implies that the obtained results ( positive or negative characterizations and associated proofs) are limited to the scope of this model. This problem of diversity among formalisms and results, already pointed out twenty years ago in @cite , led researchers to consider higher abstractions when studying fundamental properties of distributed systems.
- Let us repeat that an algorithm does not specify how the nodes synchronize, how they select each other to perform a common computation step. From the abstraction level of local computations, this underlying synchronization is seen as an implementation choice (dedicated procedures were designed to fit the various models, e.g. local elections @cite and local rendezvous @cite for starwise and pairwise interactions, respectively). A direct consequence is that the execution of an algorithm at this level may not be deterministic. Another consequence is that the characterization of conditions on the dynamics will additionally require assumptions on the synchronization -- we suggest later a generic progression hypothesis that serves this purpose. Note that the three algorithms provided in this paper rely on pairwise interactions, but the concepts and methodology involved apply to local computations in general.
- In a different context, @cite were proposed as a combinatorial model for dynamic networks. The initial purpose of this model was to provide a suitable representation of (FSDNs), in order to compute optimal communication routes such as shortest, fastest and foremost journeys @cite . In such a context, the evolution of the network was known beforehand. In the present work, we use evolving graphs in a very different purpose, which is to express properties on the network dynamics. It is important to keep in mind that the analyzed algorithms are never supposed to know the evolution of the network beforehand.
- Among the uniprocessor protocols, the authors of @cite @cite @cite proposed the following protocols. @math A protocol @cite where tasks are assigned priorities according to the Deadline Monotonic Scheduling algorithm and are scheduled with time offsets during the mode change only.
- A protocol has been introduced by in @cite , assuming Fixed-Task-Priority scheduling. Then, the authors of @cite extended this protocol to the Earliest Deadline First @cite scheduling algorithm.
- The authors of @cite introduced a particular protocol which allows tasks to modify their parameters (period, execution time, etc.) during the mode changes. As in @cite , this study assumes that the tasks are scheduled according to the Deadline Monotonic scheduling algorithm.
- There has been some prior work on analyzing connections on Twitter. @cite studied social interactions on Twitter to reveal that the driving process for usage is a sparse hidden network underlying the friends and followers, while most of the links represent meaningless interactions. @cite have examined Twitter as a mechanism for word-of-mouth advertising. They considered particular brands and products and examined the structure of the postings and the change in sentiments. @cite proposed a propagation model that predicts which users will tweet about which URL based on the history of past user activity.
- Yang and Leskovec @cite examined patterns of temporal behavior for hashtags in Twitter. They presented a stable time series clustering algorithm and demonstrate the common temporal patterns that tweets containing hashtags follow. There have also been earlier studies focused on social influence and propagation. @cite studied the problem of identifying influential bloggers in the blogosphere. They discovered that the most influential bloggers were not necessarily the most active. , @cite have distinguished the effects of homophily from influence as motivators for propagation. As to the study of influence within Twitter, @cite performed a comparison of three different measures of influence - indegree, retweets, and user mentions. They discovered that while retweets and mentions correlated well with each other, the indegree of users did not correlate well with the other two measures. Based on this, they hypothesized that the number of followers may not a good measure of influence. Recently, Romero and others @cite introduced a novel influence measure that takes into account the passivity of the audience in the social network. They developed an iterative algorithm to compute influence in the style of the HITS algorithm and empirically demonstrated that the number of followers is a poor measure of influence.
- The rendezvous problem was first mentioned in @cite . Authors investigating rendezvous (cf. @cite for an extensive survey) considered either the geometric scenario (rendezvous in an interval of the real line, see, e.g., @cite @cite @cite , or in the plane, see, e.g., @cite @cite ), or rendezvous in networks, see e.g., @cite @cite @cite . Many papers, e.g., @cite @cite @cite @cite @cite study the probabilistic setting: inputs and or rendezvous strategies are random.
- A natural extension of the rendezvous problem is that of gathering @cite @cite @cite @cite , when more than two agents have to meet in one location. @cite the authors considered rendezvous of many agents with unique labels.
- Apart from the synchronous model used in this paper, several authors have investigated asynchronous rendezvous in the plane @cite @cite and in network environments @cite @cite @cite . In the latter scenario the agent chooses the edge which it decides to traverse but the adversary controls the speed of the agent. Under this assumption rendezvous in a node cannot be guaranteed even in very simple graphs, hence the rendezvous requirement is relaxed to permit the agents to meet inside an edge.
- Range Mode Query Naturally, a mode of the query interval @math can be computed directly without preprocessing using any of the methods described in . @cite describe data structures that provide constant-time queries using @math space and @math -time queries using @math space, for any fixed @math . Petersen and Grabowski @cite improve the first bound to constant time and @math space and Petersen @cite improves the second bound to @math -time queries using @math space, for any fixed @math . When @math , the data structure of @cite requires only linear space and provides @math query time. Although its space requirement is almost linear in @math as @math approaches @math , the data structure of Petersen @cite requires @math space. Furthermore, the construction becomes impractical as @math approaches @math (the number of levels in a hierarchical set of tables and hash functions approaches @math as @math ) and no obvious modification reduces its space requirement to @math . @cite prove a lower bound of @math query time for any data structure that uses @math memory cells of @math bits.
- @cite consider approximate range mode queries, in which the objective is to return an element whose frequency is at least @math . They give a data structure that requires @math space and answers approximate range mode queries in @math time for any fixed @math , as well as data structures that provide constant-time queries for @math , using space @math , @math , and @math , respectively. @cite give a linear-space data structure that supports approximate range mode queries in constant time for @math , and an @math -space data structure that supports approximate range mode queries in @math time for any fixed @math .
- In the same line of task management and recovery, Parnin and Gorg @cite propose an approach for capturing the context relevant for a task from a programmer's interactions with an IDE, which is then used to aid the programmer recovering the mental state associated with a task and to facilitate the exploration of source code using recommendation systems. Their approach is focused on analyzing the interactions of the programmer with the source code, in order to create techniques for supporting recovery and exploration. Again, this approach is largely restricted to the IDE and the developer interaction with it.
- The foregoing work is in the context of wired links, and to our knowledge the question of buffer sizing for 802.11 wireless links has received almost no attention in the literature. Exceptions include @cite @cite @cite . Sizing of buffers for voice traffic in WLANs is investigated in @cite . The impact of fixed buffer sizes on TCP flows is studied in @cite . In @cite , TCP performance with a variety of AP buffer sizes and 802.11e parameter settings is investigated. In @cite @cite , initial investigations are reported related to the eBDP algorithm and the ALT algorithm of the A* algorithm. We substantially extend the previous work in this paper with theoretical analysis, experiment implementations in both testbed and a production WLAN, and additional NS simulations.
- -0.05in Locality issues have been thoroughly studied in the literature, via the analysis of various construction problems, including @math -coloring and Maximal Independent Set (MIS) @cite @cite @cite @cite @cite @cite @cite , Minimum Spanning Tree (MST) @cite @cite @cite , Maximal Matching @cite , Maximum Weighted Matching @cite @cite @cite , Minimum Dominating Set @cite @cite , Spanners @cite @cite @cite , etc. For some problems (e.g., coloring @cite @cite @cite ), there are still large gaps between the best known results on specific families of graphs (e.g., bounded degree graphs) and on arbitrary graphs. .0in
- The question of what can be computed in a constant number of communication rounds was investigated in the seminal work of Naor and Stockmeyer @cite . In particular, that paper considers a subclass of @math , called LCL, which is essentially @math restricted to languages involving graphs of constant maximum degree, and involving processor inputs taken from a set of constant size, and studies the question of how to compute in @math rounds the constructive versions of decision problems in LCL. The paper provides some beautiful general results. In particular, the authors show that if there exists a randomized algorithm that constructs a solution for a problem in LCL in @math rounds, then there is also a deterministic algorithm constructing a solution for this problem in @math rounds. Unfortunately, the proof of this result relies heavily on the definition of LCL. Indeed, the constant bound constraints on the degrees and input sizes allow the authors to cleverly use Ramsey theory. It is thus not clear whether it is possible to extend this result to all languages in @math .
- The question of whether randomization helps in decreasing the locality parameter of construction problems has been the focus of numerous studies. To date, there exists evidence that, for some problems at least, randomization does not help. For instance, @cite proves this for 3-coloring the ring. In fact, for low degree graphs, the gaps between the efficiencies of the best known randomized and deterministic algorithms for problems like MIS, @math -coloring, and Maximal Matching are very small. On the other hand, for graphs of arbitrarily large degrees, there seem to be indications that randomization does help, at least in some cases. For instance, @math -coloring can be randomly computed in expected @math communication rounds on @math -node graphs @cite @cite , whereas the best known deterministic algorithm for this problem performs in @math rounds @cite . @math -coloring results whose performances are measured also with respect to the maximum degree @math illustrate this phenomena as well. Specifically, @cite shows that @math -coloring can be randomly computed in expected @math communication rounds whereas the best known deterministic algorithm performs in @math rounds @cite @cite .
- The theory of @cite @cite was designed to tackle the issue of locally verifying (with the aid of a proof , i.e., a certificate, at each node) solutions to problems that cannot be decided locally (e.g., is the given subgraph a spanning tree of the network?'', or, is it an MST?''). In fact, the model of proof-labeling schemes has some resemblance to our definition of the class @math . Investigations in the framework of proof-labeling schemes mostly focus on the minimum size of the certificate necessary so that verification can be performed in a single round. The notion of proof-labeling schemes also has interesting similarities with the notions of local detection @cite , local checking @cite , or silent stabilization @cite , which were introduced in the context of self-stabilization @cite .
- The use of oracles that provide information to nodes was studied intensively in the context of distributed construction tasks. For instance, this framework, called local computation with advice , was studied in @cite for MST construction and in @cite for 3-coloring a cycle.
- Finally, we note that our notion of NLD seems to be related to the theory of lifts , e.g., @cite .
- Partially observable Markov decision processes (POMDPs) extend MDPs to situations where the state is not directly observable @cite @cite @cite . In this circumstance, an agent can plan using a continuous with dimensionality equal to the number of hidden states in the POMDP @. When the number of hidden states is large, dimensionality reduction in POMDPs can be achieved by projecting a high dimensional belief space to a lower dimensional one; of course, the difficulty is to find a projection which preserves decision quality. Strategies for finding good projections include value-directed compression @cite and non-negative matrix factorization @cite @cite . The resulting model after compression is a Predictive State Representation (PSR) @cite @cite , an Observable Operator Model @cite , or a multiplicity automaton @cite . Moving to one of these representations can often compress a POMDP by a large factor with little or no loss in accuracy: examples exist with arbitrarily large lossless compression factors, and in practice, we can often achieve large compression ratios with little loss.
- The drawback of all of the approaches enumerated above is that they first assume that the dynamical system model is known, and only then give us a way of finding a compact representation and a value function. In practice, we would like to be able to find a good set of features, . Kolter and Ng @cite contend with this problem from a sparse feature selection standpoint. Given a large set of possibly-relevant features of observations, they proposed augmenting LSTD by applying an @math penalty to the coefficients, forcing LSTD to select a sparse set of features for value function estimation. The resulting algorithm, LARS-TD, works well in certain situations (for example, see ), but only if our original large set of features contains a small subset of highly-relevant features.
- Recently, looked at the problem of value function estimation from the perspective of both model-free and model-based reinforcement learning @cite . The model-free approach estimates a value function directly from sample trajectories, i.e., from sequences of feature vectors of visited states. The model-based approach, by contrast, first learns a model and then computes the value function from the learned model. compared LSTD (a model-free method) to a model-based method in which we first learn a linear model by viewing features as a proxy for state (leading to a linear transition matrix that predicts future features from past features), and then compute a value function from this approximate model. demonstrated that these two approaches compute exactly the same value function @cite , formalizing a fact that has been recognized to some degree before @cite .
- Second, we look at the problem of value function estimation from a model-based perspective (). Instead of learning a linear transition model from features, as in @cite , we use subspace identification @cite @cite to learn a PSR from our samples. Then we compute a value function via the Bellman equations for our learned PSR @. This new approach has a substantial benefit: while the linear feature-to-feature transition model of @cite does not seem to have any common uses outside that paper, PSRs have been proposed numerous times on their own merits (including being invented independently at least three times), and are a strict generalization of POMDPs.
- Just as showed for the two simpler methods, we show that our two improved methods (model-free and model-based) are equivalent. This result yields some appealing theoretical benefits: for example, PSTD features can be explicitly interpreted as a statistically consistent estimate of the true underlying system state. And, the feasibility of finding the true value function can be shown to depend on the of the dynamical system, or equivalently, the dimensionality of the predictive state representation--- on the cardinality of the POMDP state space. Therefore our representation is naturally compressed'' in the sense of @cite , speeding up convergence.
- Gupta and Kumar studied throughput of static wireless networks, @cite . They have considered protocol model and physical model for the studying of impact of interfering transmission on SNR. They observed that in a network comprising of @math identical nodes, each of which communicating with another nodes, the throughput per node under protocol model is of order @math if placement of nodes is random. The throughput per node becomes @math if node placement and communication patterns is optimal. The later result is valid for physical model as explained intuitively by @cite . While the overall on-hop throughput of the network grows as @math , the average path length grows as @math , which makes the throughput per node to vary as @math .
- used linear programming approach to characterize networks with interference, @cite . They used a conflict graph to model constraints on simultaneous transmissions. In the paper @cite , both approximation algorithms that solve both the end-to-end flow routing problem and link scheduling problem near optimal are proposed. In the paper @cite , it is shown that the problem of solving the optimal scheduling given the concurrency constraints to maximize network throughput, is NP-hard.
- Many authors have discussed route discovery process @cite , @cite , @cite but neither suggested when to initiate route discovery as their case is related to static case nor they have suggested how frequently to initiate the discovery process, in case of mobile network. Most of them suggested initiation of discovery only when a link, in the existing route, is disrupted i.e., in case of route break. @cite , the authors proposed a modified AODV which uses the concept of reliable distance that change dynamically. Peng @cite suggested distributed route discovery method that uses reinforcement learning. @cite , the authors have used fuzzy controller in every node. In their paper, the destination evaluates performance of all those routes and arranges it in order of preference, when route-request packet reaches its destination.
- In paper @cite , the authors discussed the route discovery initiation to reduce the frequency of flooding request by elongating the link duration of the selected paths. In @cite , the authors suggested extension in storing multiple paths as route rather than unipath as route .
- Aggregated approaches show the temporal data in a single drawing. LifeLines @cite visualizes a person's disease pattern. For each condition there is a timeline, i.e., a horizontal bar along a time axis. Coloration and thickness of these bars change to show the status of the condition at different times. TimeRadarTrees @cite uses a radial drawing to show how several entities are related with each other. Unlike our approach, there is no ego and all relations between the visible nodes are displayed. Instead of node-link, it uses colored segments which fill a circle with multiple layers. The approach is limited to small graphs but supports hierarchical nesting to compensate this. Segments at the perimeter represent recent events while those near the center represent old influences. The intensity view is similar to this drawing but has a much lower information density. ConfSearch @cite searches DBLP for relations between conferences, authors and keywords. The related entities of an ego are presented as a rated list with additional information. ConfSearch does not show the evolution of relations, but we adopted some of the rating functions used for the examples in Section .
- Many systems combine different types of visualizations. Paper Lense @cite and Facet Lense @cite provide bar charts, textual result lists and nested node drawings to show entities in faceted data sets. Among others, data can be plotted against time, like the number of an author's publications by year. Both tools provide extensive filtering and sorting functions. The DB-Browser @cite features similar views including simple graph drawings. PaperLense and DB-Browser visualize DBLP data. Both provide aggregated information like the number of joint papers for two given authors.
- Not all approaches use the entity and relation abstraction. The ThemeRiver @cite application shows how the frequency of a term in a set of documents changes over time. The results for multiple terms are presented as a plot where one axis shows the time and the other axis the frequency.
- Another popular approach is to write applications for environment @math and completely environment @math on top of the target environment @math . One of the most extreme examples here is http: www.winehq.org , that completely emulates the Windows-API on top of POSIX systems. The opposite is Cygwin @cite , that emulates the POSIX API on Windows platforms.
- The Lazy Flipper is related in at least four ways to existing work. First of all, it generalizes Iterated Conditional Modes (ICM) for binary variables @cite . While ICM leaves all variables except one fixed in each step, the Lazy Flipper can optimize over larger (for small models: all) connected subgraphs of a graphical model. Furthermore, it extends Block-ICM @cite that optimizes over specific subsets of variables in grid graphs to irregular and higher-order graphical models.
- Naive attempts to generalize ICM and Block-ICM to optimize over subgraphs of size @math would consider all sequences of @math connected variables and ignore the fact that many of these sequences represent the same set. This causes substantial problems because the redundancy is large, as we show in . The Lazy Flipper avoids this redundancy, at the cost of storing one unique representative for each subset. Compared to randomized algorithms that sample from the set of subgraphs @cite @cite @cite , this is a memory intensive approach. Up to 8 GB of RAM are required for the optimizations shown in . Now that servers with much larger RAM are available, it has become a practical option.
- Second, the Lazy Flipper is a deterministic alternative to the randomized search for tighter bounds proposed and analyzed in 2009 by @cite . Exactly as in @cite , sets of variables that are connected via potentials in the graphical model are considered and variables flipped if these flips lead to a smaller upper bound on the sum of potentials. In contrast to @cite , unique representatives of these sets are visited in a deterministic order. Both algorithms maintain a current best assignment of values to the variables and are thus related with the Swendsen-Wang algorithm @cite @cite and Wolff algorithm @cite .
- Third, lazy flipping with a limited search depth as a means of approximate optimization competes with message passing algorithms @cite @cite @cite @cite and with algorithms based on convex programming relaxations of the optimization problem @cite @cite @cite @cite , in particular with Tree-reweighted Belief Propagation (TRBP) @cite @cite @cite and sub-gradient descent @cite @cite .
- Fourth, the Lazy Flipper guarantees that the best approximation found with a search depth @math is optimal within a Hamming distance @math . A similar guarantee known as the Single Loop Tree (SLT) neighborhood @cite is given by BP in case of convergence. The SLT condition states that in any alteration of an assignment of values to the variables that leads to a lower energy, the altered variables form a subgraph in the graphical model that has at least two loops. The fact that Hamming optimality and SLT optimality differ can be exploited in practice. We show in one experiment in that BP approximations can be further improved by means of lazy flipping.
- The study of computational complexity in constraint programming has tended to focus on the structure of the constraint graph (e.g. especially measures like tree width @cite @cite ) or on the semantics of the constraints (e.g. @cite ). However, these lines of research are mostly concerned with constraint satisfaction problems as a whole, and do not say much about individual (global) constraints. For global constraints of bounded arity, asymptotic analysis has been used to characterize the complexity of propagation both in general and for constraints with a particular semantics. For example, the generic domain consistency algorithm of @cite has an @math time complexity on constraints of arity @math and domains of size @math , whilst the domain consistency algorithm of @cite for the @math -ary constraint has @math time complexity. Bessiere showed that many global constraints like are also intractable to propagate @cite . More recently, Samer and Szeider have studied the parameterized complexity of the constraint @cite . Szeider has also studied the complexity of symmetry in a propositional resolution calculus @cite . See Chapter 10 in @cite for more about symmetry of propositional systems.
- Data maintenance is cheap in our scenario, where it is performed by a data owner with a local copy. When maintenance is delegated to nodes that do not have a local copy of the backup objects, various coding schemes can be used @cite @cite to limit the amount of required data transit. For these settings, cryptographic protocols @cite @cite have been designed to verify the authenticity of stored data.
- A recurrent problem for P2P applications is creating incentives to encourage nodes in contributing more resources. This can be done via reputation systems @cite or virtual currency @cite . Specifically for storage systems, an easy and efficient solution is segregating nodes in sub-networks with roughly homogeneous characteristics such as uptime and storage space @cite @cite .
- Backup objects, whose confidentiality can be ensured by standard encryption techniques, should encode incremental differences between archive versions. Recently, various techniques have been proposed to optimize computational time and size of these differences @cite .
- Packet retransmission based on network coding for a one-to-many, single-hop multicast network is a recent field of study, first proposed by D. @cite , which was later further elaborated into @cite by D. @cite the authors demonstrate bandwidth effectiveness achieved by employing greedy network coding for retransmission over traditional ARQ schemes through simulation work. @cite the authors follow up the work in @cite by comparing various packet coding algorithms for packet retransmissions. While in @cite , the authors presents an analytical work on the reliability performance of network coding compared with ARQ and FEC in a lossy network. Network Coded Piggy Back (NCPB) @cite demonstrates an efficient and practical testbed implemented random linear network coding based many-to-many reliable network model for real-time multi-player game network. Since our work primarily focuses on proposing an efficient network coding based retransmission algorithm for a one-to-many single-hop network, we will be comparing our results with the algorithm given in @cite which is the most closely related work.
- The novelty of our work is the development of a computationally feasible network coding based retransmission algorithm whose gains are two-folded: 1) Our algorithm BENEFIT delivers better throughput with respect to the current best single-hop, NC based retransmission algorithm, and 2) we also demonstrate that our algorithm achieves minimum time to decode packets. None of the previous works @cite - @cite on NC based retransmission incorporates consideration of packet latency in their work. Here we will also show that it is no longer necessary to follow the packet coding rule @cite , @cite strictly. This relaxation in the coding rule has the potential for modification and development of other network coding based applications.
- Integrating ASP with description logics has attracted a great deal of attention recently. The existing approaches can be roughly classified into three categories. The first is to adopt a nonmonotonic formalism that covers both ASP and first-order logic (if not for the latter, then extend it to the first-order case) @cite @cite , where ontologies and rules are written in the same language, resulting in a tight coupling. The second is a loose approach: An ontology knowledge base and the rules share the same constants but not the same predicates, and the communication is via a well-defined interface, such as dl-atoms @cite . The third is to combine ontologies with hybrid rules @cite @cite @cite , where predicates in the language of ontologies are interpreted classically, whereas those in the language of rules are interpreted nonmonotonically.
- Although each approach above has its own merits, the loose approach possesses some unique advantages. In many situations, we would like to combine existing knowledge bases, possibly under different logics. In this case, a notion of interface is natural and necessary. The loose approach seems particularly intuitive, as it does not rely on the use of modal operators nor on a multi-valued logic. One notices that dl-programs share similar characteristics with another recent interest, multi-context systems , in which knowledge bases of arbitrary logics communicate through bridge rules @cite .
- However, the relationships among these different approaches are currently not well understood. For example, although we know how to translate a dl-program without the nonmonotonic operator @math to an MKNF theory while preserving the strong answer set semantics @cite , when @math is involved, no such a translation is known. Similarly, although a variant of Quantified Equilibrium Logic (QEL) captures the existing hybrid approaches, as shown by @cite , it is not clear how one would apply the loop formulas for logic programs with arbitrary sentences @cite to dl-programs, since, to the best of our knowledge, there is no syntactic, semantics-preserving translation from dl-programs to logic programs with arbitrary sentences or to QEL.
- In related work, @cite attempts to enhance the utilization of MPR channels by allowing stations to count down and transmit as long as there are less than @math ongoing transmissions in the air. To do this, one key assumption is that a station is able to detect the number of ongoing transmissions using an energy detector. This assumption, however, is not valid in wireless networks, where the received energy from each transmitting station is random and unknown a priori.
- The theory of optimal stopping has been widely studied in the fields of statistics, economics, and mathematical finance since 1960's @cite . It was not until very recently that optimal stopping theory started to find application in wireless networks. In @cite , the tradeoff between the spectrum access opportunity and spectrum sensing overhead in cognitive radio systems is formulated as a finite-horizon optimal stopping problem, which is solved using backward induction. Likewise, a finite-horizon optimal stopping problem is formulated in @cite to derive an optimal next-hop selection strategy in multi-hop ad hoc networks. The problem of maximizing the rate of return (MR) was applied to opportunistic scheduling in ad-hoc networks in @cite and opportunistic spectrum access of cognitive radio networks in @cite . Notably, the application of optimal stopping theory in wireless systems is still at its infancy stage. Our work in this paper is an attempt to introduce it to wireless random-access networks.
- In contrast, high interaction honeypots'' and virtualization technologies (e.g., VMware, Xen, Qemu) execute native system and application code, but the price of this fidelity is quite high. For example, the RINSE approach @cite is implemented over the iSSFNet network simulator, which runs on parallel machines to support real-time simulation of large-scale networks. All these solutions share the same principle of simulating almost every aspect of a real machine or real network, but share the similar problems too: expensive configuration cost and expensive hardware and software licenses. Moreover, most of these solutions are not fully compatible with standard network protections (e.g., firewalls, IDSs), suffering a lack of integration between all security actors in complex cyber-attack scenarios.
- Other interesting approaches to solve these problems include the framework developed by @cite . While they focus on distributed denial of service attacks (DDoS) and defensive IDS analysis, we focus on offensive strategies to understand the scenarios and develop countermeasures. Also @cite have integrated @cite and @cite to create a flexible and very detailed network laboratory and simulation tool. The latter project has privileged accuracy and virtualization over scalability and performance.
- The @cite is another interesting prototype. It improves high-fidelity honeypot scalability by up to six times while still closely emulating the execution behavior of individual Internet hosts. Potemkin uses quite sophisticated on-demand techniques for instantiating hosts Including file system optimizations implemented also in , as we are going to see it in . , but this approach focuses on attracting real attacks and it shows the same honeypot limitations to reach this goal. As an example, to capture e-mail viruses, a honeypot must posses an e-mail address, must be scripted to read mail (executing attachments like a naive user) and, most critically, must be influenced to add the honeypot to their address books. Passive malware (e.g., many spyware applications) may require a honeypot to generate explicit requests, and focused malware (e.g., targeting only financial institutions) may carefully select its victims and never touch a large-scale honeyfarm. In each of these cases there are partial solutions, and they require careful engineering to truly mimic the target environment.
- @cite provides the theoretical analysis on the equivalences between orthogonal NMF to @math -means clustering for both rectangular data matrices and symmetric matrices. However as their proofs utilize the zero gradient conditions, the hidden assumptions (setting the Lagrange multipliers to zeros) are not revealed there. Actually it can be easily shown that their approach is the KKT conditions applied to the unconstrained version of eq. . Thus there is no guarantee that minimizing eq. by using the zero gradient conditions leads to the stationary point located on the nonnegative orthant as required by the objective.
- Because early theoretical work on incomplete data focuses largely on algebras and representation systems (e.g., @cite @cite ), it was only natural to extend this line of thinking to probabilities @cite @cite @cite @cite @cite benjelloun06uldbs . However, this extension is quite difficult since the probabilities in query results must include expressions derived from the confidence values originally embedded in the database. Systems meeting these theoretical conditions must overcome a set of challenges that are often satisfied at the expense of modeling-power or understandability.
- Although there is a vast body of work on probabilistic databases, graphical models have largely been ignored until recently. The work of @cite @cite casts query evaluation as inference in a graphical model and BayesStore @cite makes explicit use of Bayesian networks to represent uncertainty in the database. While expressive, generative Bayesian networks have difficulty representing the types of dependencies handled automatically in discriminative models @cite , motivating a database approach to linear chain conditional random fields @cite . We, however, present a more general representation based on factor graphs, an umbrella framework for both Bayesian networks and conditional random fields. Perhaps more importantly we directly address the problem of scalable query evaluation in these representations|with an MCMC sampler|whereas previous systems based on graphical models are severely restricted by this bottleneck. Furthermore our approach can easily evaluate any relational algebra query without the need to close the graphical model under the semantics of each operator.
- Our goal is to make use of the elegance of programming for signal processing. Our work is driven by the experience that today compiled code cannot compete with traditional signal processing packages written in C. There has been a lot of progress in recent years, most notably the improved support for arrays without overhead, the elimination of temporary arrays ( fusion ) and the Data-Parallel Haskell project @cite that aims at utilising multiple cores of modern processors for array oriented data processing. However there is still a considerable gap in performance between idiomatic code and idiomatic C code. A recent development is an LLVM -backend for the GHC Glasgow Haskell Compiler . that adds all of the low-level optimisations of LLVM to GHC . However we still need some tuning of the high-level optimisation and a support for processor vector types in order to catch up with our EDSL method.
- Another special purpose language is ChucK @cite . Distinguishing features of ChucK are the generalisation to many different rates and the possibility of programming while the program is running, that is while the sound is playing. As explained in internal-parameter we can already cope with control signals at different rates, however the management of sample rates at all could be better if it was integrated in our framework for physical dimensions. Since the systems Hugs and GHC both have a fine interactive mode, can in principle also be used for live coding. However it still requires better support by LLVM (shared libraries) and by our implementation.
- A fundamental bound of Matthews @cite shows that @math where we recall that @math is the expected hitting time from @math to @math . Using the straightforward lower bound @math , this fact provides a deterministic @math -approximation to @math in @math -node graphs.
- Furthermore, for a few families of specific examples, the asymptotics of the cover time have been calculated more precisely. These include the work of Aldous @cite for regular trees, Dembo, Peres, Rosen, and Zeitouni @cite for the 2-dimensional discrete torus, and Cooper and Frieze @cite for the giant component of various random graphs.
- In this section we explore the relationship between our approach and previously developed solutions @cite @cite @cite @cite .
- The work in @cite was the first to address efficient sampling of pulse streams, e.g., diracs. Their approach for solving the periodic case was ideal lowpass filtering, followed by uniform sampling, which allowed to obtain the Fourier series coefficients of the signal. These coefficients are then processed by the annihilating filter to obtain the unknown time-delays and amplitudes. In , we derived a general condition on the sampling kernel , under which recovery is guaranteed. The lowpass filter of @cite is a special case of this result. The noise robustness of both the lowpass approach and our more general method is high as long as the pulses are well separated, since reconstruction from Fourier series coefficients is stable in this case. Both approaches achieve the minimal number of samples.
- The authors of @cite proposed a Gaussian sampling kernel for sampling finite streams of Diracs. The Gaussian method is numerically unstable, as mentioned in @cite , since the samples are multiplied by a rapidly diverging or decaying exponent. Therefore, this approach is unsuitable for @math . Modifications proposed in @cite exhibit better performance and stability. However, these methods require substantial oversampling, and still exhibit instability for @math .
- In @cite the family of polynomial reproducing kernels was introduced as sampling filters for the model . B-splines were proposed as a specific example. The B-spline sampling filter enables obtaining moments of the signal, rather than Fourier coefficients. The moments are then processed with the same annihilating filter used in previous methods. However, as mentioned by the authors, this approach is unstable for high values of @math . This is due to the fact that in contrast to the estimation of Fourier coefficients, estimating high order moments is unstable, since unstable weighting of the samples is carried out during the process.
- Another general family introduced in @cite for the finite model is the class of exponential reproducing kernels. As a specific case, the authors propose E-spline sampling kernels. The CTFT of an E-spline of order @math is described by where @math are free parameters. In order to use E-splines as sampling kernels for pulse streams, the authors propose a specific structure on the @math 's, @math . Choosing exponents having a non-vanishing real part results in unstable weighting, as in the B-spline case. However, choosing the special case of pure imaginary exponents in the E-splines, already suggested by the authors, results in a reconstruction method based on Fourier coefficients, which demonstrates an interesting relation to our method. The Fourier coefficients are obtained by applying a matrix consisting of the exponent spanning coefficients @math , (see @cite ), instead of our Vandermonde matrix relation . With this specific choice of parameters the E-spline function satisfies .
- When the E-spline coefficients @math are pure imaginary, it can be easily shown that becomes a multiplication of shifted sincs. This is in contrast to the SoS filter which consists of a sum of sincs in the frequency domain. Since multiplication in the frequency domain translates to convolution in the time domain, it is clear that the support of the E-spline grows with its order, and in turn with the order of the problem @math . In contrast, the support of the SoS filter remains unchanged. This observation becomes important when examining the infinite case. The constraint on the signal in @cite is that no more than @math pulses be in any interval of length @math , @math being the support of the filter, and @math the sampling period. Since @math grows linearly with @math , the constraint cast on the infinite stream becomes more stringent, quadratically with @math . On the other hand, the constraint on the infinite stream using the SoS filter is independent of @math .
- The work in @cite addressed the infinite stream case, with @math . They proposed filtering the signal with a polynomial reproducing sampling kernel prior to sampling. If the signal has at most @math diracs within any interval of duration @math , where @math denotes the support of the sampling filter and @math the sampling period, then the samples are a sufficient characterization of the signal. This condition allows to divide the infinite stream into a sequence of finite case problems. In our approach the quiet phases of @math between the bursts of length @math enable the reduction to the finite case.
- Regarding the sampling rate, the number of degrees of freedom of the signal per unit time, also known as the rate of innovation, is @math , which is the critical sampling rate. Our sampling rate is @math and therefore we oversample by a factor of @math . In the same scenario, the method in @cite would require a sampling rate of @math , i.e., oversampling by a factor of @math . Properties of polynomial reproducing kernels imply that @math , therefore for any @math , our method exhibits more efficient sampling. A table comparing the various features is shown in Table .
- Recent work @cite presented a low complexity method for reconstructing streams of pulses (both infinite and finite cases) consisting of diracs. However the basic assumption of this method is that there is at most one dirac per sampling period. This means we must have prior knowledge about a lower limit on the spacing between two consecutive deltas, in order to guarantee correct reconstruction. In some cases such a limit may not exist; even if it does it will usually force us to sample at a much higher rate than the critical one. p 2.4cm
- A non-statistical approach for analyzing the homology and the genetic semihomology'' in protein sequences was presented in @cite @cite . Instead of using a statistically computed scoring matrix, amino acid similarities are scored according to the complexity of the substitution process at the DNA level, depending on the number and type (transition transversion) of nucleotide changes that are necessary for replacing one amino acid by the other. This ensures a differentiated treatment of amino acid substitutions at different positions of the protein sequence, thus avoiding possible rough approximations resulting from scoring them equally, based on a classic scoring matrix. The main drawback of this approach is that it was not designed to cope with frameshift mutations..
- Regarding frameshift mutation discovery , many studies @cite @cite @cite @cite preferred the plain BLAST @cite @cite alignment approach: BLASTN on DNA and mRNA, or BLASTX on mRNA and proteins, applicable only when the DNA sequences are sufficiently similar. BLASTX programs, although capable of insightful results thanks to the six frame translations, have the limitation of not being able to transparently manage frameshifts that occur inside the sequence, for example by reconstructing an alignment from pieces obtained on different reading frames.
- An interesting approach for handling frameshifts at the protein level was developed in @cite . Several substitution matrices were designed for aligning amino acids encoded on different reading frames, based on nucleotide pair matches between respective codons. This idea has the advantage of being easy to use with any classic protein alignment tool. However, it lacks flexibility in gap positioning.
- A few years prior to the introduction of Ebert's Hat Game, in 1994, a similar game was described by Aspnes, Beigel, Furst and Rudich @cite . In their version of the game, players are not allowed to pass, and the objective is for a majority of the players to guess correctly. For the three-player game, it is easy to describe a strategy that will succeed with probability @math , just as in Ebert's game: Alice votes the opposite of Bob's hat colour; Bob votes the opposite of Charlie's hat colour; and Charlie votes the opposite of Alice's hat colour.
- The importance of regularization in matrix completion is well known to practitioners. For instance, one important component of many algorithms competing for the Netflix challenge @cite , consisted in minimizing the cost function @math (this is also known as @cite @cite ). Here the minimization variables are @math , @math . Unlike in , these matrices are not constrained to be orthogonal, and as a consequence the problem becomes significantly more degenerate. Notice that, in our approach, the orthogonality constraint fixes the norms @math , @math . This motivates the use of @math as a regularization term.
- We note that the question of putting lower bounds on nonadaptive quantum query algorithms has been studied previously. First, Zalka has obtained a tight lower bound on the nonadaptive quantum query complexity of the unordered search problem, which is a particular learning problem @cite . Second, in @cite , Nishimura and Yamakami give lower bounds on the nonadaptive quantum query complexity of a multiple-block variant of the ordered search problem. Finally, @cite develop the weighted adversary argument of Ambainis @cite to obtain lower bounds that are specific to the nonadaptive setting. Unlike the situation considered here, their bounds also apply to quantum algorithms for computing partial functions.
- One of the first works that gained insight into the general performance of OSA networks, considering impact of PU activity on blocking and throughput of the SU network was @cite , where the capacity of a multichannel OSA system was assessed by comparing centrally coordinated versus random SU channel assignment. A spectrum sensing process was not considered. A similar problem was investigated in @cite where the spectrum sharing gains for PU and SU networks were obtained for a distributed and multichannel ad hoc OSA network. Unfortunately, a zero delay spectrum sensing process was assumed with genie-aided channel selection, i.e. in every time slot the receiver knew of the exact channel the transmitter will use to send data [Sec. III-C1] srinivasa_twc_2008 .
- In later works, assumptions on the OSA network model became more realistic. Specifically, Markovian analysis of SU traffic buffering on the event of PU arrival was presented for a SU exponential service time @cite and for a SU phase-type service time @cite . Unfortunately the impact of spectrum sensing detection time overhead on the OSA network performance was not investigated and the connection arrangement process for new SU arrivals, i.e. method to select and access a channel for a new sender-receiver pair, was assumed to be performed by a centralized entity. A different option of the above model has been analyzed in @cite , with only PU channels dedicated to the OSA network and with a mixture of PU and SU exclusive channels. SU connection buffering was not allowed, however, SU connections were able to switch to an empty SU exclusive channel on the event of channel preemption by the PU.
- A similar analysis, but with a different channelization structure, where the PU occupied more than one SU channel (contrary to @cite @cite ) was performed in @cite . The authors addressed the cases of (i) connection blocking, and (ii) channel reservation and switching of SU connections to empty channels on PU arrival. This analysis was later extended to the case of finite SU population and packet queuing @cite , and buffering and switching of SU connections preempted by PU arrivals @cite . Again, in all papers listed above the spectrum sensing process was assumed to have no overhead and perfect reliability. Moreover the connection arrangement process for SUs was not considered.
- Considering the final group of papers, when coupling spectrum sensing procedures with link layer protocols, there is a fundamental tradeoff between sensing time, sensing quality and OSA network throughput. This has been independently found for general OSA network models with a single sensing band @cite , multiple sensing bands @cite with and without cooperative detection and centralized resource allocation, and in a context of MAC protocol abstraction @cite for a non-cooperative sensing case. See also recent discussion in [Sec. 2.3.1, 7.3, and 10.2.4] hossain_book_2009 . This tradeoff will be especially clear, while evaluating microscopic models, since the detection time creates a significant overhead for the data exchange phase. Recently the model of @cite was extended to the case of @math out of @math '' rule in cooperative sensing @cite , optimizing parameters of the model to maximize the throughput given detection rate requirements. Unfortunately, the delay caused by exchanging sensing information was not included.
- Because the elements, @math , are time-varying, a na 'ive counting mechanism requires a system of @math counters to compute @math exactly (unless @math ). This is not always realistic. Estimating @math in data streams is heavily studied @cite @cite @cite @cite @cite . We have mentioned that computing @math in strict-Turnstile model is trivial using a simple counter. One might naturally speculate that when @math , computing (approximating) @math should be also easy. However, before Compressed Counting (CC) , none of the prior algorithms could capture this intuition.
- CC improves symmetric stable random projections @cite @cite uniformly for all @math as shown in Figure in Section . However, one can still considerably improve CC around @math , by developing better estimators, as in this study. In addition, no empirical studies on CC were reported.
- @cite applied symmetric stable random projections to approximate the moments and Shannon entropy. The nice theoretical work @cite @cite provided the criterion to choose the @math so that Shannon entropy can be approximated with a guaranteed accuracy, using the @math th frequency moment.
- An early approach to apply term-positional data in IR is the work of Attar and Fraenkel @cite . The authors propose different models to generate clusters of terms related to a query (searchonyms) and use these clusters in a local feedback process. In their experiments they confirm that metrical methods based on functions of the distance between terms are superior to methods based merely on weighted co-occurrences of terms. There are several other approaches that use metrical information @cite @cite .
- One of the first approaches using abstract representations of term distributions in documents is Fourier Domain Scoring (FDS), proposed by @cite . FDS performs a separate magnitude and phase analysis of term position signals to produce an optimized ranking. It creates an index based on page segmentation, storing term frequency and approximated positions in the document. FDS processes the indexed data using the to perform the corresponding spectral analysis.
- A recent approach based on an abstract representation of term position is Fourier Vector Scoring (FVS) @cite . It represents the term information (Fourier coefficients) directly as an @math -dimensional vector using the analytic Fourier transform, permitting an immediate and simple term comparison process.
- Hayrapetyan et. al @cite analyze among similar lines as us but for their cost function they assume a constant term for the per unit flow price charged by ISPs but in the real world scenario generally the per unit charge decreases if the flow required by the user increases i.e. ISPs provide concession for more flow. Our model captures this. They have not done any analysis of the effect of Braess' Paradox in their network, too. We show that in our model the severity of Braess' Paradox is reduced.
- These two problems have been well studied. It has been proven that they are both @math -hard ( @cite , @cite ).
- In @cite the SRAP problem is considered. They propose three greedy algorithms with different heuristics, the , the and the . The first two algorithms start by assigning each node to a different ring. At each iteration they reduce the number of rings by merging two rings @math and @math if @math is a feasible ring for the capacity constraint. In the edge-based heuristic, the two rings with the maximum weight edge are merged. While in the cut-based heuristic, the two rings with the maximum total weight of the edges with one endpoint in each of them, are merged. Algorithm shows the pseudo code for the edge-based heuristic.
- A special case of the IDP problem where all the edges have the same weight, is studied in @cite . This special case is called the problem. Given a simple undirected graph @math and a value @math , we want to find a partitioning of @math , @math such that @math . The authors present two linear-time-approximation algorithms with fixed performance guarantee.
- Y. Lee, H. Sherali, J. Han and S. Kim in 2000 ( @cite ), have studied the IDP problem with an additional constraint such that for each ring @math , @math . The authors present a mixed-integer programming model for the problem, and develop a branch-and-cut algorithm. They also introduce a heuristic to generate an initial feasible solution, and another one to improve the initial solution. To initialize a ring, the heuristic first, adds the node @math with the maximum graph degree, with respect to unassigned edges, and then adds to the partition the edge @math such that the graph degree of @math is maximum. It iteratively increases the partition by choosing a node such that the total traffic does not exceed the limit @math . A set of 40 instances is generated to test these heuristics and the branch-and-cut.
- More recently, in @cite , these two problems have been studied. The authors have developed different metaheuristic algorithms, all based on the Tabu Search. The metaheuristics are the (BTS), two versions of the (PR1, PR2), the (XTS), the (SS), and the (DMN). These local search algorithms are detailed further.
- Previously, we saw that with local search it is necessary to define a neighborhood to choose the next solution. The authors of @cite use the same for all of their metaheuristics. It tries to assign an item @math from a partition, @math , to another partition, @math . The authors also consider the neighborhood obtained by swapping two items, @math and @math , from two different partitions, @math and @math . But instead of trying all the pairs of items, it will only try to swap the two items if the resulting solution of the assignment of @math to the partition @math is unfeasible.
- In order to compute a starting solution for the IDP problem, the authors describe four different heuristics. The first heuristic introduced in @cite ordered the edges by decreasing weight, at each iteration it tries to assign the edge with the biggest weight which is not already assigned, to the ring with the smallest residual capacity regarding to capacity constraint. If no assignment is possible, the current edge is assigned to a new ring. The second one, sorts the edges by increasing weight, and tries to assign the current edge to the current ring if the capacity constraint is respected, otherwise the ring is no longer considered and a new ring is initialized with the current edge.
- The two other methods described in @cite are based on the idea that to save ADMs a good solution should have very dense rings. They are both greedy and rely on a clique algorithm. In graph theory, a clique in an undirected graph @math is a subset of the vertex set @math , such that for every two vertices in @math , there exists an edge connecting the two. Finding a clique is not that easy, a way to do it is to use an "Union-Find" strategie, two clique @math and @math such that each node in @math is adjacent to each node in @math then merge the two cliques (). The associated heuristic starts by considering each node to be a clique of size one, and to merge two cliques into a larger clique until there are no more possible merges.
- In @cite fairness of a matching procedure is defined in terms of four axioms, two of which are gender neutrality and peer indifference. Then, the existence of a matching procedures satisfying all or a subset of the axioms is considered in terms of restrictions on preference orderings. Here, instead, we propose a preprocessing step that allows to obtain a gender neutral matching procedure from any matching procedure without imposing any restrictions on the preferences in the input. A detailed description of results about manipulation of stable marriage procedures can be found in @cite . In particular, several early results @cite @cite @cite @cite indicated the futility of men lying, focusing later work mostly on strategies in which the women lie. Gale and Sotomayor @cite presented the manipulation strategy in which women truncate their preference lists. Roth and Vate @cite discussed strategic issues when the stable matching is chosen at random, proposed a truncation strategy and showed that every stable matching can be achieved as an equilibrium in truncation strategies. We instead do not allow the elimination of men from a woman's preference ordering, but permit reordering of the preference lists.
- @cite suggested lying strategies for an individual woman, and proposed an algorithm to find the best partner with the male optimal procedure. We instead focus on the complexity of determining if the procedure can be manipulated to obtain a better result. Moreover, we also provide a universal manipulation scheme that, under certain conditions on the profile, assures that the female optimal partner is returned.
- Coalition manipulation is considered in @cite . Huang shows how a coalition of men can get a better result in the men-proposing Gale-Shapley algorithm. By contrast, we do not consider a coalition but just a single manipulator, and do not consider just the Gale-Shapley algorithm.
- Prior work on rumor spreading has primarily focused on viral epidemics in populations. The natural (and somewhat standard) model for viral epidemics is known as the or SIR model @cite . In this model, there are three types of nodes: (i) susceptible nodes, capable of being infected; (ii) infected nodes that can spread the virus further; and (iii) recovered nodes that are cured and can no longer become infected. Research in the SIR model has focused on understanding how the structure of the network and rates of infection cure lead to large epidemics @cite , @cite , @cite , @cite . This motivated various researchers to propose network inference techniques for learning the relevant network parameters @cite , @cite , @cite , @cite , @cite . However, there has been little (or no) work done on inferring the source of an epidemic.
- The most related work is @cite Like our paper, this considers the computational complexity of determining winners for sequential majority voting. However, they start from an incomplete majority graph which throws away information about individual votes, whilst we start from an incomplete profile.
- Pini prove that computing the possible and necessary winners for the STV rule is NP-hard @cite . They show it is NP-hard even to approximate these sets within some constant factor in size. They also give a preference elicitation procedure which focuses just on the set of possible winners.
- Finally, Brandt consider different notions of winners starting from incomplete majority graphs @cite . We plan to investigate these kinds of winners in our framework.
- Much of the work on envy free revenue maximization is on item pricing rather than on subset pricing. @cite give an @math -approximation for the general single minded problem, where @math is the number of items and @math is the number of agents. This result was extended by @cite to an @math -approximation for arbitrary valuations and unlimited supply using single fixed pricing which is basically pricing all bundles with the same price. @cite show that the general item pricing problem with unlimited availability of items is hard to approximate within a (semi-)logarithmic factor.
- For the unlimited supply the problem is NP-hard (Briest and Kriesta @cite ). Also the problem is given @math -approximation by @cite . When the length of each interval is bounded by a constant or the valuation of each agent is bounded by a constant, @cite give a fully polynomial time approximation scheme (FPTAS). If the intervals requested by different agents have a nested structure then an FPTAS is possible @cite @cite .
- @cite proposed a way to avoid computing eq:z_sampling for each @math by getting an upper bound on @math using Holder's inequality and computing eq:z_sampling for the most probable topics first, leading to a speed up of up to 8x of the sampling process.
- @cite broke eq:z_sampling in three components and took leverage on the resulting sparsity in @math of some of them -- that, combined with an efficient storage scheme led to a speed up of the order of 20x.
- There has been a lot of research on providing QoS over wireless channels. Most of the research has focused on admission control and scheduling policies. Hou, Borkar, and Kumar @cite and Hou and Kumar @cite have proposed analytical models to characterize QoS requirements, and have also proposed both admission control and scheduling policies. Ni, Romdhani, and Turletti @cite provides an overview of the IEEE 802.11 mechanisms and discusses the limitations and challenges in providing QoS in 802.11. Gao, Cai, and Ngan @cite , Niyato and Hossain @cite , and Ahmed @cite have surveyed existing admission control algorithms in different types of wireless networks. On the other hand, Fattah and Leung @cite and Cao and Li @cite have provided extensive surveys on scheduling policies for providing QoS.
- There is also research on utility maximization for both wireline and wireless networks. Kelly @cite and Kelly, Maulloo, and Tan @cite have considered the rate control algorithm to achieve maximum utility in a wireline network. Lin and Shroff @cite has studied the same problem with multi-path routing. As for wireless networks, Xiao, Shroff, and Chong @cite has proposed a power-control framework to maximize utility, which is defined as a function of the signal-to-interference ratio and cannot reflect channel unreliability. Cao and Li @cite has proposed a bandwidth allocation policy that also considers channel degradation. Bianchi, Campbell, and Liao @cite has studied utility-fair services in wireless networks. However, all the aforementioned works assume that the utility is only determined by the allocated bandwidth. Thus, they do not consider applications that require delay bounds.
- A number of probabilistic approaches have been proposed in the past for the problem of gene-regulatory network reconstruction @cite @cite @cite @cite . Some take into account the information on the prior network topology @cite , which is not always available. Most assume the number of factors is known. To get around this, one can perform model selection via Reversible Jump MCMC @cite or evolutionary stochastic model search @cite . Unfortunately, these methods are often difficult to design and may take quite long to converge. Moreover, they are difficult to integrate with other forms of prior knowledge (eg., factor hierarchies). A somewhat similar approach to ours is the infinite independent component analysis (iICA) model of @cite which treats factor analysis as a special case of ICA. However, their model is limited to factor analysis and does not take into account feature selection, factor hierarchy and factor regression. As a generalization to the standard ICA model, @cite proposed a model in which the components can be related via a tree-structured graphical model. It, however, assumes a fixed number of components.
- Surprisingly, researchers have thought about navigating and browsing for information as a single user activity, centered on eliciting users' information needs and improving the relevance of search results. For example, Choo, Detlor & Turnbull @cite discussed categories of search behaviors and motivations in information seeking, but they overlooked the role of other individuals in search. On the other hand, library scientists @cite have observed for some time that friends and colleagues may be valuable information resources during search. Similarly, recent authors have begun to recognize the prevalence and benefits of @cite @cite .
- However, in addition to explicit collaboration in joint search tasks @cite , we believe that even implicit social experiences could improve the search process. Therefore, the general term social search'' may more suitably describe information seeking and sensemaking habits that make use of a range of possible social interactions: including searches that utilize social and expertise networks or that may be done in shared social workspaces. This notion certainly encompasses collaborative co-located search, as well as remote and asynchronous collaborative and collective search. Our focus in this paper is to explore a model of social search that may offer suggestions for supporting social interactions in the information seeking process.
- @cite implements a network repository of interface adapters for adapting Java interfaces using single chains of adapters. @cite implements a similar adaptation framework for network services. Although it allows an interface adapter to adapt a target interface from multiple source interfaces, only a single interface adapter is used for each target interface, so it has the same limitations as single adapter chains in that not all methods that could be adapted may actually be adapted. Both mention the possibility of lossy interface adaptation, but neither considers how to minimize such loss.
- @cite proposes an interface adaptation framework which attempts to minimize the loss incurred by an interface adapter chain, and @cite rigorously defines the mathematical background required to implement such a framework. These only consider the use of single chains of interface adapters.
- Stable matching problems have drawn the intensive attention of researchers in various disciplines in the past decades since the seminal paper of Gale and Shapley @cite . For a summary, see @cite @cite @cite . Vande Vate @cite initiated the study of stable matching using mathematical programming approach; further developments using this approach can be found in @cite @cite @cite @cite @cite @cite @cite .
- Fleiner @cite studied the many-many stable matching in a much more general context. Using a fixed-point approach, he proved that stable matchings always exist provided that the preference of each entity is a . Roughly speaking, such a function can be realized by imposing a matroid over a linear order of elements. In , supposing that there is no lower bound on the classes, then each laminar family is equivalent to a partition matroid. We prove that stable matchings always exist in this situation. Hence, our algorithm in Section 2 can be seen as a constructive proof of a special case of Fleiner's existence theorem.
- Abraham, Irving and Manlove introduced the student-project allocation problem @cite . It can be shown that in , if all classifications are just partitions over the applicants and there is no lower bound, our problem is equivalent to a special case of their problem. They posed the open question whether there is a polynomial time algorithm for their problem if there is lower bound on the projects (classes). Our result in Section 2 gives a partial positive answer.
- More general and holistic models have been proposed to build an interactive retrieval system (e.g. Fuhr @cite and @cite ) that rely on a decision-theoretic framework to determine what is the best next action the system should perform, i.e. what documents should be presented to the user. In such approaches, decisions are made based on the relevance of documents when considering past interactions. In this paper, we focus on the latter problem and do not discuss how to select the best next action the system has to perform.
- The most related work in that field is that of Melucci's @cite , which computes the probability of having a given context @math , where @math is the probability distribution generated by the document vector @math , and the subspace @math is equalled to the context and is built through user interaction. More specifically, given a set of documents deemed relevant, either using user feedback or pseudo-relevance feedback, one can compute a subspace @math corresponding to the principal components of a subspace spanned by those documents. A document vector fully included in this space will be fully relevant (probability of 1), an orthogonal one will be fully irrelevant (zero probability). Melucci's approach is dual to ours, in the sense that instead of representing users in an IN space, he considers documents in a contextual space. Our approach, which relies on an IN space, facilitates the possibility of using the different quantum evolution mechanisms to model the interaction between the user and the retrieval system.
- @cite @cite the authors introduced an AIS based misbehavior detection system for ad hoc wireless networks. They used Glomosim for simulating data traffic, their setup was an area of 800 @math 600m with 40 mobile nodes (speed 1 m s) of which 5-20 are misbehaving; the routing protocol was DSR. Four genes were used to capture local behavior at the network layer. The misbehavior implemented is a subset of misbehavior introduced in this paper; their observed detection rate is about 55 @cite the authors describe an AIS able to detect anomalies at the transport layer of the OSI protocol stack; only a wired TCP IP network is considered. Self is defined as normal pairwise connections. Each detector is represented as a 49-bit string. The pattern matching is based on r-contiguous bits with a fixed @math .
- Ref. @cite discusses a network intrusion system that aims at detecting misbehavior by capturing TCP packet headers. They report that their AIS is unsuitable for detecting anomalies in communication networks. This result is questioned in @cite where it is stated that this is due to the choice of problem representation and due to the choice of matching threshold @math for @math -contiguous bits matching.
- To overcome the deficiencies of the generate-and-test approach a different approach is outlined in @cite . Several signals each having a different function are employed in order to detect a specific misbehavior in sensor wireless networks. Unfortunately, no performance analysis was presented and the properties of these signals were not evaluated with respect to their misuse.
- The main discerning factor between our work and works shortly discussed above is that we carefully considered hardware parameters of current sensor devices, the set of input parameters was designed in order to target specifically sensor networks and our simulation setup reflects structural qualities of such networks with regards to existence of multiple independent routing paths. In comparison to @cite @cite we showed that in case of static sensor networks it is reasonable to expect the detection rate to be above 80
- Usage of genetic algorithm for weak learner acceleration was already proposed in several works. For example, in @cite genetic weak learner with special crossover and mutation operators was used to learn classifier based on extended haar feature set. In @cite genetic algorithm was used to select a few thousand weak classifiers with smallest error on unweighed training set before boosting process starts. Then exhaustive search over selected classifiers was performed on each boosting iteration to select the one with minimal weighed loss. In @cite boosting procedure was completly integrated with genetic algorithm. Few classifiers were selected on each boosting iteration from solution population and added to the strong classifier. That selected classifiers were then used to produce new population members by applying genetic operators. Then, in @cite authors used for weak learner some special evolutionary algorithm they've called Evolutionary Hill-Climbing . Crossover operator was not used in it. Instead, @math different mutations were applied to every population member on each algorithm iteration. Result of each mutation was rejected when it did not improve fitness function value.
- There were two main reasons for using genetic search instead of any other approaches in these works. Most of the classifiers used in mentioned works were some extensions of the haar classifier family originally proposed in @cite . So, huge size of a weak classifier family do not allow to apply exhaustive search based optimization. And complicated discrete structure of a weak classifier blocks all other optimization options.
- There have been other attempts to address the problem of finding good coordinate representations of simple non-Euclidean data spaces. One approach @cite is to use modified versions of multidimensional scaling specifically devised to find the best embedding of a data set into the cylinder, the sphere and so on. The target space has to be chosen in advance. Another class of approaches @cite @cite involves cutting the data manifold along arcs and curves until it has trivial topology. The resulting configuration can then be embedded in Euclidean space in the usual way. In our approach, the number of circular coordinates is not fixed in advance, but is determined experimentally after a persistent homology calculation. Moreover, there is no cutting involved; the coordinate functions respect the original topology of the data.
- There have been a number of other recent theoretical results about the computational complexity of manipulating elections. For instance, Procaccia and Rosenschein give a simple greedy procedure that will find a manipulation of a scoring rule for any junta'' distribution of weighted votes in polynomial time with a probability of failure that is an inverse polynomial in @math @cite . A junta'' distribution is concentrated on the hard instances.
- Coleman and Teague provide polynomial algorithms to compute a manipulation for the STV rule when either the number of voters or the number of candidates is fixed @cite . They also conducted an empirical study which demonstrates that only relatively small coalitions are needed to change the elimination order of the STV rule. They observe that most uniform and random elections are not trivially manipulable using a simple greedy heuristic.
- Finally, similar phenomena have been observed in the phase transition for the Hamiltonian cycle problem @cite @cite . If the number of edges is small, there is likely to be a node of degree smaller than 2. There cannot therefore be any Hamiltonian cycle. By the time that there are enough edges for all nodes to be of degree 2, there are likely to be many possible Hamiltonian cycles and even a simple heuristic can find one. Thus, the phase transition in the existence of a Hamiltonian cycle is not associated with hard instances of the problem. The behavior seen here is similar. By the time the coalition is large enough to manipulate the result, the variance in scores between the candidates is likely to be so large that computing a successful manipulation or proving none is possible is easy.
- The paper by Leung and Ng @cite investigates the idea of either enlarging the query sub-image to match the size of an image block obtained by the four-level multiscale representation of the database images, or conversely contracting the image blocks of the database images so that they become as small as the query sub-image. The paper presents an analytical cost model and focuses on avoiding I O overhead during query processing time. To find a good strategy to search multiple resolutions, four techniques are investigated: the branch-and-bound algorithm, Pure Vertical (PV), Pure Horizontal (PH) and Horizontal-and-Vertical (HV). The HV strategy is argued to be the best considering efficiency. However, the authors do not report clear conclusions regarding the effectiveness (e.g., Precision and or Recall) of their approach.
- In @cite a new method called HTM (Hierarchical Tree Matching) for the CBsIR problem was proposed. It has three main components: (1) a tree structure that models a hierarchical partition of images into tiles using color features, (2) an index sequence to represent the tree structure (allowing fast access during the search phase), and (3) a search strategy based on the tree structures of both database images and the query image. Since the tree structure presented in @cite is re-used in our work, we detail it in the following.
- An straigthforward way to model images is to use its average color. This is obviously not effective in any non-trivial situation. Another simple, and in many situations cost-effective means is to use a global color histogram (GCH) (c.f., @cite ). A common critique to GCHs is that it is unable to capture any notion of spatial distribution. To address this several other approaches have been proposed A comprehensive survey thereof is beyond the scope of this paper. , but they add complexity as a trade-off in order to gain effectiveness. Nevertheless, the use of color only, without any notion of spatial distribution, may be effective, if one is able to capture other features of the images, e.g., texture. That is exactly the advantage of the BIC technique proposed in @cite and which we re-use within our proposal.
- The BIC approach was shown in @cite to outperform several other CBIR approaches and, as such, we adopt it in our CBsIR proposal to extract and compare the visual feature of each tile with the goal of improving the retrieval accuracy.
- Concerning set packings the situation is analogous, albeit the research has been somewhat less extensive. Deciding whether a given family of @math subsets of an @math -element universe contains a @math -packing is known to be W[1]-hard @cite , and thus it is unlikely that the problem is fixed parameter tractable, that is, solvable in time @math for some function @math and constant @math . If @math is fairly large, say exponential in @math , the fastest known algorithms actually count the packings by employing the inclusion--exclusion machinery @cite @cite and run in time @math . This bound holds also for the presented algorithm (cf. Theorem ).
- The presented meet-in-the-middle approach resembles the randomized divide-and-conquer technique by Chen, Lu, Sze, and Zhang @cite and the similar divide-and-color method by Kneis, M " o lle, Richter, and Rossmanith @cite , designed for parameterized decision problems. These can, in turn, be viewed as extensions of the recursive partitioning technique of Gurevich and Shelah @cite for the Hamiltonian Path problem. That said, our contribution is rather in the observation that, in the counting context, the join operation can be done efficiently using the inclusion--exclusion machinery. While our formalization of the problem as the Disjoint Sum problem is new, the solution itself can, in essence, already be found in Kennes @cite , even though in terms of possibility calculus and without the idea of trimming,'' that is, restricting the computations to small subsets. Kennes's results were rediscovered in a dual form and extended to accommodate trimming in the authors' recent works @cite @cite @cite .
- There have been several approaches to solve fault-tolerant consensus in anonymous networks deterministically. In @cite , fault-tolerant consensus is solved under the assumption that failure detector @math @cite exists, i.e. exactly one correct process eventually knows forever that it is the leader. In @cite , fault-tolerant and obstruction-free For obstruction-free consensus, termination is only guaranteed if a process can take enough steps without beeing interrupted by other processes. consensus is solved if registers are available.
- As noted in @cite , there is an interesting connection between quantitative Arrow statements and the concept of testing introduced in @cite @cite which was studied and used extensively since. Roughly speaking a property of functions is testable if it is possible to perform a randomized test for the property such that if the probability that the function passes the test is close to @math , then the function has to be close to a function with the property (say in the hamming distance). In terms of testing, our result states that among all functions satisfying the IIA property, the Transitivity property is testable. Moreover, the natural test "works": i.e., in order to test for transitivity, one can pick a random input and check if the outcome is transitive.
- Geographic information can largely reduce complexity of routing in spontaneous mesh networks. The most simple and widely used protocol is greedy geographic routing @cite @cite @cite @cite : when a node receives a packet, it uses the following forwarding rule: is usually defined with respect to the distance towards the destination. Since improvement is not negative, there is no routing loops. Moreover, routing is scalable, because all routing decisions are local.
- Geographical routing requires addresses based on geographical coordinates: a node must obtain its location either with a dedicated physical device (e.g. GPS) or through a more complex algorithm, e.g. by estimating the position with respect to its neighbors. propose to construct a local coordinate system for each node and determine the coordinates of its neighbors @cite . Then, they aggregate the local coordinate systems into global coordinates. The authors assume the distance to each neighbor known, but usually it is difficult to obtain. follow a similar approach, but based on the of packets coming from neighbors @cite . A pragmatic approach to this problem is to assume that a subset of mesh routers know their exact positions via GPS devices and other nodes can compute their positions with respect to its neighbors @cite .
- One special case of ) is when a noisy version of @math is known only to the transmitter; our result in this case is a generalization of Costa's celebrated result @cite . @cite , it is shown that the achievable rate when the noise @math is perfectly known at the transmitter is equivalent to the rate when @math is known at the receiver, and this rate does not depend on the variance of @math . A new coding strategy to achieve this capacity was also introduced in @cite and is popularly referred to as dirty paper coding (DPC). We generalize Costa's result to the case of noisy interference knowledge. We show that the capacity with knowledge of a noisy version of @math at the transmitter is equal to the capacity with knowledge of a statistically equivalent noisy version of @math at the receiver. However, unlike @cite where the capacity does not depend on the variance of @math , in the general noisy side information case, the capacity decreases as the variance of @math increases.
- @cite , Costa adopted the random coding argument given by @cite @cite . Based on the channel capacity @math given in @cite @cite , Costa constructed the auxiliary variable @math as a linear combination of @math and @math and showed that this simple construction of @math achieves capacity.
- Following Costa's work, several extensions of DPC have been studied, , colored Gaussian noise @cite , arbitrary distributions of @math @cite and deterministic sequences @cite . The case when @math is perfectly known to the encoder and a noisy version is known to the decoder is considered in @cite , mainly focusing on discrete memoryless channels. The only result in @cite for Gaussian channel reveals no additional gain due to the presence of the noisy estimate at the decoder, since perfect knowledge is available at the encoder and DPC can be used. In contrast, in this paper we study the case when only noisy knowledge of @math is available at both transmitter and receiver.
- In @cite , the authors attempted to derive a bound on the number of samples needed to recover block sparse signals, where the coefficients in each block are either all zero or all nonzero. In our terminology, this corresponds to the case of group sparsity with equal size groups. The algorithm considered there is a special case of ) with @math . However, their result is very loose, and does not demonstrate the advantage of group Lasso over standard Lasso.
- Finally, we shall mention that independent of the authors, results similar to those presented in this paper have also been obtained in @cite with a similar technical analysis. However, while our paper studies the general group Lasso formulation, only the special case of multi-task learning is considered in @cite .
- Newton iteration has also been applied to finding perfect polynomial roots of lacunary (or other) polynomials given by straight-line programs. @cite shows how to compute a straight-line program for @math , given a straight-line program for @math and the value of @math . This method has complexity polynomial in the size of the straight-line program for @math , and in the degree of @math , and in particular is effective for large @math . We do not address the powerful generality of straight-line programs, but do avoid the dependence on the degree of @math .
- Closest to this current work, @cite shows how to recognize whether @math for a lacunary polynomial @math . Shparlinski uses random evaluations and tests for quadratic residues. How to determine whether a lacunary polynomial is perfect power is posed as an open question.
- Security of biometric systems is widely studied -- cf. @cite @cite @cite -- and although a lot of vulnerabilities are now well understood and controlled, it is still difficult to achieve an end-to-end system which satisfies all constraints. In particular, biometric template privacy is an important issue due to the non-revocability and non-renewability of biometric features.
- Abraham al @cite study almost stable matchings in the stable roommates problem. The recent work by Bir al @cite is particularly close to ours: they, too, consider the stable marriage problem with incomplete preference lists, and aim at finding a matching with few unstable edges. However, in terms of computational complexity, their work goes in the opposite direction. Their task is to find a matching that minimises the number of unstable edges. It turns out that this makes the problem computationally much more : the problem is NP-hard, unlike the classical stable marriage problem. In contrast, we do not require that the matching is a maximum matching, which makes the problem computationally : the problem admits a constant-time distributed algorithm, unlike the classical stable marriage problem. The algorithm works even when ties in the preferences lists are allowed; this should be contrasted with the fact that if ties are allowed, it is NP-hard to find a stable perfect matching @cite @cite @cite .
- Note that switching partners along unstable edges can be done in a distributed manner. The Gale--Shapley algorithm is also parallel by its nature: the proposals rejects can be undertaken by all men women simultaneously during synchronised rounds (albeit it can happen that only one man is free at a round [Section A.3] gusfield89stable , @cite ). Lower bounds on the running time of the algorithm [Section 1.5] gusfield89stable show that a linear number of rounds is required to attain stability. But can a nearly stable matching be obtained with fewer rounds?
- Several works have addressed the last question with experiments. Quinn @cite observes experimentally that a matching with only a fraction of unstable edges emerges long before the Gale--Shapley algorithm converges. Lu and Zheng @cite propose a parallel algorithm that outperforms the Gale--Shapley algorithm in practice. Theorem gives theoretical support to the findings in Quinn @cite . Theorem also addresses the concern expressed in the conclusions of Lu and Zheng @cite where it is claimed that Most of existing parallel stable matching algorithms cannot guarantee a matching with a small number of unstable pairs within a given time interval.'' Theorem suggests that if the number of acceptable partners for each participant is bounded, the Gale--Shapley algorithm guarantees a small relative number of unstable edges.
- From a theory perspective, apparently only a few papers address decentralised implementations of the Gale--Shapley algorithm and or stability after early termination of a stable matching algorithm. In a recent paper @cite it is claimed that little theory exists concerning instability.'' @cite give bounds on the performance of a simple online algorithm. Feder al @cite show that a stable matching can be found on a polynomial number of processors in sublinear time; their algorithm is not local. Other work on parallel stable matching include Tseng and Lee @cite , Tseng @cite , and Hull @cite . Eriksson and H " a ggstr " o m @cite prove that a simple heuristic works well for random inputs. Our Theorem shows that the Gale--Shapley algorithm works well for an arbitrary input.
- As we mentioned in , there is a range of negative results related to local algorithms (constant-time distributed algorithms) for maximal matching @cite and approximate maximum matching @cite @cite @cite @cite . Even if each node is assigned a unique identifier and the network topology is an @math -cycle, it is not possible to break the symmetry in the network and find a constant-factor approximation for maximum matching. Without any auxiliary information beyond unique node identifiers, positive results are known only in rare special cases, most notably for graphs where each node has an odd degree @cite @cite .
- Other work on constant-time distributed algorithms for matching usually assumes either randomness @cite @cite @cite @cite @cite or geometric information @cite @cite . We refer to the survey @cite for further information on local algorithms.
- Our centralised constant-time approximation algorithm in Theorem is based on the ideas of Parnas and Ron @cite and Nguyen and Onak @cite . Their work presents constant-time approximation algorithms for estimating the size of a maximal matching, maximum-cardinality matching, and maximum-weight matching. Our work complements this line of research by presenting an algorithm for estimating the size of a stable matching.
- Liberatore @cite proposes a framework for reasoning about actions in which it is possible to express a given semantics of belief update, like Winslett's @cite and Katsuno and Mendelzon's @cite . This means it is the formalism, essentially an action description language, that is used to describe updates (the change of propositions from one state of the world to another) by expressing them as laws in the action theory. The main difference between Liberatore's work and Li and Pereira's is that, despite not being concerned, at least a priori, with changing action laws, Liberatore's framework allows for abductively introducing in the action theory new effect propositions (effect laws, in our terms) that consistently explain the occurrence of an event.
- The work by Eiter al @cite @cite is similar to ours in that they also propose a framework that is oriented to updating action laws. They mainly investigate the case where a new effect law is added to the description (and then has to be true in all models of the modified theory). This problem is the dual of contraction and is then closer to our definition of revision ( ).
- Herzig al @cite define a method for action theory contraction that, despite the similarity with the current work and the common underlying motivations, is more limited than the present constructions. First, with the referred approach we do not get minimal change. For example, in the referred work the operator for contracting executability laws is such that in the resulting theory the modified set of executabilities is given by [ - = ( i ) : i ] which, according to its semantics, gives theories among whose models are those resulting from removing arrows from all @math -worlds. A similar comment can be made contraction of effect laws.
- The orthogonal question of designing testers or @math s with as few queries as possible was also considered. In a highly influential paper @cite , H stad constructed a @math system making only three queries. Many variants also followed. In particular @math systems with perfect completeness making three queries were also achieved in @cite @cite . Similar to our approach, O'Donnell and Wu @cite designed an optimal three bit dictatorship test with perfect completeness, and later the same authors constructed a conditional @math system @cite .
- One popular approach of tackling the issues related to transient failures of network elements is that of using proactive recovery schemes . These schemes typically work by precomputing alternate paths at the network setup time for the failure scenarios, and then using these alternate paths to re-route the traffic when the failure actually occurs. Also, the information of the failure is suppressed in the hope that it is a transient failure. The local rerouting based solutions proposed in @cite @cite @cite @cite @cite fall into this category.
- Refs. @cite @cite present protocols based on local re-routing for dealing with transient single link and single node failures respectively. They demonstrate via simulations that the recovery paths computed by their algorithm are usually within 15 the theoretically optimal alternate paths.
- Wang and Gao's Backup Route Aware Protocol @cite also uses some precomputed backup routes in order to handle transient single link failures. One problem central to their solution asks for the availability of reverse paths at each node. However, they do not discuss the computation of these reverse paths. Interestingly, the alternate paths that our algorithm computes qualify as the reverse paths required by the BRAP protocol of @cite .
- Slosiar and Latin @cite studied the single link failure recovery problem and presented an @math time for computing the link-avoiding alternate paths. A faster algorithm, with a running time of @math for this problem was presented in @cite . Our central protocol presented in this paper can be generalized to handle single link failures as well. Unlike the protocol of @cite , this single link failure recovery protocol would use optimal recovery paths.
- There is a long history of calendar studies in human-com -pu -ter interaction literature. Early research on calendar use predates electronic calendars. In 1982, Kelley and Chapanis @cite interviewed 23 professionals to discover how people in the business world kept track of their schedules. They found that for the individuals interviewed, calendars were indispensable and showed a lot of diversity in their use. The use of multiple calendars was prevalent, and a wide variation was seen in the time spans viewed, as well as in other aspects such as archiving, editing and portable access. Many of the problems identified in paper calendars could be solved in electronic calendars, and they concluded with a list of features for emerging electronic calendars to implement. Soon afterwards, Kincaid and Pierre @cite examined the use of paper and electronic calendars in two groups, and concluded that electronic calendars failed to provide several key features such as flexibility, power, and convenience, that paper calendars did. They recommended many useful features to be incorporated into electronic calendar systems as well.
- Nearly 10 years after Kelley and Chapanis' original study, Payne @cite conducted interviews with 30 knowledge workers about both calendars and to-do lists, followed by a task analysis of his observations. He concluded that the central task supported by calendars was . Prospective remembering is the use of memory for remembering to do things in the future, as different from retrospective memory functions such as recalling past events.
- Several interesting design concepts have been suggested to make electronic calendar systems less error-prone and smarter. These cover a wide range, from systems that retrieve tasks from email messages @cite , to systems that learn from users' behavior to recommend intelligent defaults @cite , to calendar systems that predict attendance at events @cite . @cite assessed the effectiveness of a priority-based calendar prototype and concluded that integration with other personal information systems (such as email) would make the system more useful for users. Visualizing calendar information on desktop computers and mobile devices has been explored in several studies @cite @cite .
- In the field of Personal Information Management, the management of various information collections such as files @cite , folders @cite , email @cite bookmarks @cite and cross-collection issues @cite @cite have been studied widely. Calendars are an important part of users' personal information, and this domain can benefit from a re-examination in the wake of electronic and ubiquitous calendar systems.
- Jones @cite framed the problems in PIM in terms of the canonical tasks of , , and . Keeping any kind of information involves a tradeoff between the likelihood it will be recalled in the future, and the costs of capturing and retaining it. Organizing involves filing it away such that it can be retrieved easily in the future, and while keeping the cost of organizing less than the cost of finding. Re-finding is a different problem from finding, since there are aspects to encountered information that make it personal.
- With the increased use of mobile devices, more and more calendaring tasks are performed off the desktop computer. @cite report on issues faced by mobile workers, their need for access to people and information located remotely, and the planful opportunism they engage in when utilizing their for tasks.
- A recent unpublished work by Swamy @cite considers two stage risk-averse models for stochastic set cover and related combinatorial optimization problems. In the two stage recourse model, some sets can be chosen in the first stage at a low cost, and then if a scenario is not covered, more sets can be bought in the second stage as a recourse action. The risk averse problem is to minimize the sum of first stage cost and value-at-risk for the second stage. It was observed that if the value-at-risk for second stage is fixed to be @math , the problem reduces to chance-constrained set cover without recourse - same as our non-adaptive set cover problem. Although the algorithms in @cite can be used under more general assumptions of black box distributions", we present faster algorithms that achieve better approximation factors for the special case of product distributions. Specifically, in contrast to the results in @cite , we do not incur any approximation in the probabilistic constraint, and the running time of our algorithms is independent of the input threshold @math .
- Connections with discrete conformal mappings. One can view the minimizer of (or, more appropriately, the maximizer of the vertex version ) as a sort of global uniformizing'' metric for general graphs. In the setting of discrete conformal mappings, a number of variationally defined objects appear, and duality is often an important component in their analysis. We mention, for instance, the extremal length @cite as a prominent example. It also often happens that one chooses a weight function @math as the minimizer of some convex functional, and this weight function plays the role of a discrete Riemannian metric (much as is the case in Section ); see, e.g. the work of Schramm @cite and He and Schramm @cite .
- Most previous work on dynamic subgraph connectivity concerns special cases only. Frigioni and Italiano @cite considered vertex updates in planar graphs, and described a polylogarithmic solution.
- If vertices have constant degree, vertex updates are equivalent to edge updates. For edge updates, Henzinger and King @cite were first to obtain polylogarithmic update times (randomized). This was improved by @cite to a deterministic solution with @math time per update, and by Thorup @cite to a randomized solution with @math update time. The randomized bound almost matches the @math lower bound from @cite . All these data structures maintain a spanning forest as a certificate for connectivity. This idea fails for vertex updates in the general case, since the certificate can change substantially after just one update.
- For more difficult dynamic graph problems, the goal is typically changed from getting polylogarithmic bounds to finding better exponents in polynomial bounds; for example, see all the papers on directed reachability @cite @cite @cite @cite . Evidence suggests that dynamic subgraph connectivity fits this category. It was observed @cite that finding triangles (3-cycles) or quadrilaterals (4-cycles) in directed graphs can be reduced to @math vertex updates. Thus, an update bound better than @math appears unlikely without FMM, since the best running time for finding triangles without FMM is @math , dating back to STOC'77 @cite . Even with FMM, known results are only slightly better: finding triangles and quadrilaterals takes time @math @cite and @math @cite respectively. Thus, current knowledge prevents an update bound better than @math .
- It was shown @cite that subgraph connectivity can be reduced to dynamic connectivity of axis-parallel line segments in 3 dimensions. Thus, as soon as one gets enough combinatorial richness in the host geometric space, subgraph connectivity becomes the possible way to solve geometric connectivity.
- Almost all polynomial-time heuristics suggested so far for random instances (either SAT or graph optimization problems) were analyzed when the input is sampled according to a planted-solution distribution, or various semi-random variants thereof. Alon and Kahale @cite suggest a polynomial time algorithm based on spectral techniques that @math properly @math -colors a random graph from the planted @math -coloring distribution (the distribution of graphs generated by partitioning the @math vertices into @math equally-sized color classes, and including every edge connecting two different color classes with probability @math ), for graphs with average degree greater than some constant. In the SAT context, Flaxman's algorithm, drawing on ideas from @cite , solves @math planted 3SAT instances where the clause-variable ratio is greater than some constant. Also @cite @cite @cite address the planted 3SAT distribution.
- We now describe previous efforts in collecting avatar traces from networked virtual environments and games. @cite collected a 5-hour trace of 400 players from an online game called FreeWar. @cite collected a trace of 28 players from a game they developed called Orbius. The focus of their work is not on the trace, but rather, the trace is a way to evaluate their proposed algorithms. use their trace to evaluate a load balancing scheme, while use their trace to evaluate different interest management algorithms. Beside traces collected from games, both works use randomly generated movements in their evaluation, and both observe significant differences in their results evaluated using the traces and using generated movements. Their results highlight the importance of having real mobility traces for researchers to evaluate their work.
- @cite and @cite collected traces from Quake III, a popular, multi-player, first person shooting (FPS) game and developed mobility models to describe the movement of the players. @cite collected a large trace, comparable in scale to ours, of players movement from World of Warcraft (WoW), a massively multi-player online role playing game (MMORPG), and analyzed the dynamics of the populations, players arrival departure rate, session length, player distribution, and player movements. FPS games and MMORPGs have different characteristics than NVEs. Players in fast-action, FPS games tend to move around constantly. In MMORPGs, players usually engage in quests to gain level and new abilities. Players tend to gather in a location for an event (e.g. new monsters to fight) and disperse afterwards. Players also tend to move in groups. We observed a different pattern for NVEs.
- Most recently, La and Pietro have independently conducted a similar study on mobility in Second Life @cite . Their study, however, focuses on metrics relevant to mobile communications, such as graph theoretic properties of line-of-sight networks formed by the avatars, travel length and time of avatars, and contact opportunities among avatars. Their goal is to use the mobility traces of avatars to model human mobility for applications related to wireless and delay-tolerant networks. On the other hand, we focus on metrics that are of interest to systems design of NVEs.
- In the marriage model, a set @math of men and a set @math of women is given, where each man and woman is endowed with a ranked list of members of the opposite sex. Men and women are to be matched in a one to one fasion. A matching is considered stable if there is no man and a woman who would simultaneously prefer each other to their respective assigned partners. A stable matching is guaranteed to exist, and the algorithm can be used to find it. The stable matching found by this algorithm is , in that every man prefers it to any other stable matching. Moreover when using the deferred acceptance algorithm, no man has an incentive to misreport his true preference order @cite .
- The assignment model @cite , (see also @cite @cite ) differs in that each player derives a certain value from being matched to each person of the opposite sex, and side payments between partners are allowed. The goal of each player is to maximize his or her payoff which is the sum of partner's value and monetary payment (positive or negative negative) from the partner. The set of stable outcomes is non-empty by a linear programming argument. In fact, each stable outcome corresponds to a maximum-weight matching, and player payoffs correpond to dual variables of the maximum matching LP. A man-optimal outcome is guaranteed to exist, and its allocation and prices are identical to the VCG mechanism for maximum weight matchings @cite @cite .
- Social network analysis @cite is a study of social structures, which are made of nodes (individuals, organizations etc.), which are linked by one or more specific types of relationship (transmission of influence, presence of trust etc.). Terrorist or criminal organizations have been studied empirically @cite . Factor analysis is applied to study email exchange in Enron, which ended in bankruptcy due to the institutionalized accounting fraud @cite . The criminal organizations tend to be strings of inter-linked small groups that lack a central leader, but to coordinate their activities along logistic trails and through bonds of friends. Hypothesis can be built by paying attention to remarkable white spots and hard-to-fill positions in a network @cite . The conspirators in the 9 11 terrorist organization are relevant in reducing the distance between hijackers, and enhancing communication efficiently @cite . The 9 11 terrorists' social network is investigated from the viewpoint of efficiency and security trade-off @cite . More security-oriented structure arises from longer time-to-task of the terrorists' objectives. The conspirators improve communication efficiency, preserving hijackers' small visibility and exposure.
- On the other hand, the node discovery predicts the existence of an unknown node around the known nodes from the information on the collective behavior of the network. Related works in the node discovery is, however, limited. Heuristic method for node discovery is proposed in @cite , @cite . The method is applied to analyze the covert social network foundation behind the terrorism disasters @cite . Learning techniques of latent variables can be employed, once the presence of a node is known. @cite studied learning of a structure of a linear latent variable graph. @cite studied learning of a structure of a dynamic probabilistic network. But, while the accuracy of the heuristic method is limited, these principled analytic approaches in learning are not practical to handle real human relationship and communication observed in a social network, where much complexity appears. The complexity includes bi-directional and cyclic influence among many observed and latent nodes. We need an efficient and accurate method to solve the node discovery problem.
- Relevant to our work is the literature on spectral analysis, where, however, several studies deal with regularly sampled signals (e.g., @cite and references therein). An excellent guide to irregular sampling is @cite , which presents a large number of techniques, algorithms, and applications. Reconstruction techniques for irregularly or randomly sampled signals can be found in @cite @cite @cite , just to name few. In particular, Feichtinger and Gr "ochenig in @cite provide an error analysis of an iterative reconstruction algorithm taking into account round-off errors, jitters, truncation errors and aliasing. From the theoretical point of view, irregular sampling has been studied in @cite @cite @cite @cite @cite @cite @cite and references therein.
- In the context of sensor networks, efficient techniques for spatial sampling are proposed in @cite @cite . In particular, in @cite , an adaptive sampling is described, which allows the central data-collector to vary the number of active sensors, i.e., samples, according to the desired resolution level. Data acquisition is also studied in @cite , where the authors consider a unidimensional field, uniformly sampled at the Nyquist frequency by low precision sensors. The authors show that the number of samples can be traded-off with the precision of sensors. The problem of the reconstruction of a bandlimited signal from an irregular set of samples at unknown locations is addressed in @cite . There, different solution methods are proposed, and the conditions for which there exist multiple solutions or a unique solution are discussed. Differently from @cite , we assume that the sink can either acquire or estimate the sensor locations and that sensors are randomly deployed.
- The field reconstruction at the sink node with spatial and temporal correlation among sensor measures is studied, for instance, in @cite @cite @cite @cite @cite . Other interesting studies can be found in @cite @cite , which address the perturbations of regular sampling in shift-invariant spaces @cite and the reconstruction of irregularly sampled images in presence of measurement noise @cite .
- We point out that our main contribution with respect to previous work on signal sampling and reconstruction is the probabilistic approach we adopt to analyze the quality level of a signal reconstructed from a set of irregular, noisy samples. Our analysis, however, applies to sampling systems where the field reconstruction is performed in a centralized manner. Finally, we highlight that our previous work @cite assumes that sensors are uniformly distributed over the spatial observation interval and may be displaced around a known average location. The effects of noisy measures and jittered positions are analyzed when linear reconstruction techniques are employed. However, only the unidimensional case is studied and semi-analytical derivations of the MSE of the reconstructed field are obtained. In @cite , instead, sensors are assumed to be fixed, and the objective is to evaluate the performance of a linear reconstruction technique in the presence of quasi-equally spaced sensor layouts.
- The goal of this work is to provide an analytical study on the reconstruction quality of a multidimensional physical field, with uncorrelated spectrum. The field samples are (i) irregularly spaced, since they are gathered by a randomly deployed sensor network and (ii) affected by i.i.d. noise. The sink node receives the field samples and runs the reconstruction algorithm in a centralized manner. Our major contributions with respect to previous work are as follows. [1.] Given a @math -dimensional problem formulation, we obtain analytical expressions for the moments of the eigenvalue distribution of the reconstruction matrix. Using the expressions of the moments, we show that the eigenvalue distribution tends to the Mar c enko-Pastur distribution @cite as the field dimension @math . [2.] We apply our results to the study of the quality of a reconstructed field and derive a tight approximation to the MSE of the estimated field.
- Symmetries are a well-known research topic, that serve to tackle complexity in many combinatorial problems. The first ideas on symmetry breaking were developed in the 90s @cite @cite , by relating symmetries with the graph automorphism problem, and by proposing the first approach for generating symmetry breaking predicates. This work was later extended and optimized for propositional satisfiability @cite .
- The class @math , or Quantum Merlin-Arthur, consists of all languages that admit a proof protocol in which Merlin sends Arthur a polynomial-size quantum state @math , and then Arthur decides whether to accept or reject in quantum polynomial time. This class was introduced by Knill @cite , Kitaev @cite , and Watrous @cite as a quantum analogue of @math . By now we know a reasonable amount about @math : for example, it allows amplification of success probabilities, is contained in @math , and has natural complete promise problems. (See Aharonov and Naveh @cite for a survey.)
- In 2003, Kobayashi, Matsumoto, and Yamakami @cite defined a generalization of @math called @math . Here there are @math Merlins, who send Arthur @math quantum proofs @math respectively that are guaranteed to be unentangled with each other. (Thus @math .) Notice that in the classical case, this generalization is completely uninteresting: we have @math for all @math , since we can always simulate @math Merlins by a single Merlin who sends Arthur a concatenation of the @math proofs. In the quantum case, however, a single Merlin could cheat by the @math proofs, and we know of no general way to detect such entanglement.
- When we try to understand @math , we encounter at least three basic questions. First, do multiple quantum proofs ever actually help? That is, can we find some sort of evidence that @math for some @math ? Second, can @math protocols be amplified to exponentially small error? Third, are two Merlins the most we ever need? That is, does @math for all @math ? The second and third questions are motivated, in part, by an analogy to classical ---where the Parallel Repetition Theorem of Raz @cite and the @math theorem of Ben- @cite turned out to be crucial for understanding the class @math .
- First, in their original paper on @math , @cite proved that a positive answer to the second question implies a positive answer to the third. That is, if @math protocols can be amplified, then @math for all @math .
- Second, Liu, Christandl, and Verstraete @cite gave a natural problem from quantum chemistry, called @math , which is in @math but is not known to be in @math .
- Third, Blier and Tapp @cite recently (and independently of us) gave an interesting @math protocol for an @math -complete problem, namely @math . In this protocol, Arthur verifies that an @math -vertex graph @math is @math -colorable, using two unentangled witnesses with only @math qubits each. There is a crucial caveat, though: if @math is @math -colorable, then Arthur can only detect this with probability @math rather than constant probability. Indeed, if the soundness gap were constant rather than @math , then Blier and Tapp's protocol could presumably be scaled up by an exponential to show @math !
- However, their work does not fix, from our point of view, clear semantics, and their concept of role becomes ambiguous as we pointed out in @cite . Another work based on policy refinement is the RBNS model @cite . However, and although the authors claim that their work is based on the RBAC model @cite , it seems that they only keep from this model only the concept of role. Indeed, the specification of network entities and role and permission assignments are not rigorous and does not fit any reality @cite .
- The application of the autonomic computing paradigm to the problem of overload control in web systems poses some key problems concerning the design of the monitoring module. The authors of @cite propose a technique for learning dynamic patterns of web user behavior. A finite state machine representing the typical user behavior is constructed on the basis of past history and used for prediction and prefetching techniques. In paper @cite the problem of delay prediction is analyzed on the basis of a learning activity exploiting passive measurements of query executions. Such predictive capability is exploited to enhance traditional query optimizers.
- The cited proposals @cite @cite can partially contribute to improve the QoS of web systems, but differently from our work, none of them directly formulate a complete autonomic solution that at the same time gives directions on how to take measures, and make corresponding admission control decisions for web cluster architectures.
- The authors of @cite also address a very important decision problem in the design of the monitoring module: the timing of performance control. They propose to adapt the time interval between successive decisions to the size of workload dependent system parameters, such as the processor queue length. The dynamic adjustment of this interval is of primary importance for threshold based policies for which a constant time interval between decisions may lead to an oscillatory behavior in high load scenarios as we show in Section . Simulations reveal that our algorithm is not subject to oscillations and shows a very little dependence on the time interval between decisions.
- The problem of designing adaptive component-level thresholds is analyzed in @cite for a general context of autonomic computing. The mechanism proposed in the paper consists in monitoring the threshold values in use by keeping track of false alarms with respect to possible violations of service level agreements. A regression model is used to to fit the observed history. When a sufficiently confident fit is attained the thresholds are calculated accordingly. On the contrary if the required confidence is not attained, the thresholds are set to random values as if there was no history. A critical problem of this proposal is the fact that the most common threshold policies cause on off behaviors that often result in unacceptable performance. Our proposal is instead based on a probabilistic approach and on a learning technique, that dynamically creates a knowledge basis for the online evaluation of the best decision to make even for traffic situations that never occurred in the past history.
- @cite study the computational complexity of finding acceptable trades among a set of bids in a Boolean combinatorial market. In their setting, the center is an who takes no risk, only matching together willing traders. They study a call market setting in which bids are collected together and processed once en masse. They show that the auctioneer matching problem is co-NP-complete when orders are divisible and @math -complete when orders are indivisible, but identify a tractable special case in which participants are restricted to bet on disjunctions of positive events or single negative events.
- Hanson highlights the use of LMSR for Boolean combinatorial markets, noting that the subsidy required to run a combinatorial market on @math outcomes is no greater than that required to run @math independent one-dimensional markets @cite @cite . Hanson discusses the computational difficulty of maintaining LMSR prices on a combinatorial space, and proposes some solutions, including running market makers on overlapping subsets of events, allowing traders to synchronize the markets via arbitrage.
- The work closest to our own is that of Chen, Goel, and Pennock @cite , who study a special case of Boolean combinatorics in which participants bet on how far a team will advance in a single elimination tournament, for example a sports playoff like the NCAA college basketball tournament. They provide a polynomial-time algorithm for LMSR pricing in this setting based on a Bayesian network representation of prices. They also show that LMSR pricing is NP-hard for a very general bidding language. They suggest an approximation scheme based on Monte Carlo simulation or importance sampling.
- The parameterized MAX-SAT problem (a complementary problem to the one considered in the present paper) where the goal is to satisfy at least @math clauses of arbitrary sizes received a considerable attention from the researchers resulted in a series of improvements of the worst-case upper bound on the runtime of this problem. Currently the best algorithm is given in @cite and solves this problem in @math , where @math is the size of the given formula.
- The notion of doubling dimension was introduced by Assouad @cite and first used in algorithm design by Clarkson @cite . The properties of doubling metrics and their algorithmic applications have since been studied extensively, a few examples of which appear in @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite .
- Somewhat similar in spirit to our work is the @math -extension problem @cite @cite @cite . Given a graph @math , the 0-extension ( cf. Lipschitz Extendability @cite @cite @cite ) problem deals with extending a (Euclidean) embedding of the vertices of the graph to an embedding of the convex closure of the graph, while approximately preserving the Lipschitz constant of the embedding. Our results can be interpreted as analogues to the above where the goal is to approximately preserve the doubling dimension.
- Graph animation is widely used for exploring and navigating large graphs (e.g., @cite .) There is a vast amount of work in this area, so we focus on systems that aim to animate changing graphs.
- Offline graph animation tools develop an animation from a sequence of key frames (layouts of static graphs). Such systems find layouts for key frame graphs, and then interpolate between the key frames in an appropriate way (e.g. @cite ).
- The key frame approach can be adapted to address the online problem by computing a new key frame each time a request arrives, and then interpolating to the new key frame. For example, @cite developed a system for browsing large, partially known graphs, where navigation actions add and remove subgraphs. They use force-directed layout for key frame graphs, and interpolate between them.
- Another approach is to take an existing graph layout algorithm and incrementalize (or dynamize) it. For example, the Dynagraph system @cite @cite uses an incrementalized version of the batch Sugiyama-Tagawa-Toda algorithm @cite .
- Research interests have been moving from describing organizational structure to discovering dynamical phenomena on a social network. A link discovery predicts the existence of an unknown link between two nodes from the information on the known attributes of the nodes and the known links @cite . It is one of the tasks of link mining @cite . The link discovery techniques are combined with domain-specific heuristics. The collaboration between scientists can be predicted from the published co-authorship @cite . The friendship between people is inferred from the information available on their web pages @cite .
- Markov random network is a model of the joint probability distribution of random variables. It is an undirected graphical model similar to a Bayesian network. The Markov random network is used to learn the dependency between the links which shares a node. The Markov random network is one of the dependence graphs @cite , which models the dependency between links. Extension to hierarchical models @cite , multiple networks (treating different types of relationships) @cite , valued networks (with nodal attributes) @cite , higher order dependency between the links which share no nodes @cite , and 2-block chain graphs (associating one set of explanatory variables with the other set of outcome variables) @cite are studied. A family of such extensions and model elaborations is named the exponential random graph @cite .
- In addition to the link discovery, the related research topics are the exploration of an unknown network structure @cite , the discovery of a community structure @cite , the inference of a network topology @cite , the detection of an anomaly in a network @cite , and the discovery of unknown nodes @cite , @cite . Stochastic modeling to predict terrorism attacks @cite is relevant practically. The idea of machine learning of latent variables @cite is potentially applicable to discovering an unknown network structure.
- As already mentioned, the Neumann problem was treated in @cite , @cite , @cite , KucZen:ASNLTD @cite , @cite . For Dirichlet boundary conditions, Post @cite derived the first two terms of in the case of 'small' vertex neighborhoods, see Theorem . In the recent preprint @cite Molchanov and Vainberg study the Dirichlet problem and show that, in the context of Theorem , the @math converge to eigenvalues of the quantum graph described in Theorem ; this was conjectured in @cite , where also some results on the scattering theory on non-compact graphs are obtained. However, their statements are unclear as to whether the multiplicities coincide; also, they do not consider the effect of @math eigenvalues on @math or uniform asymptotics for large @math . In @cite a related model is considered. The method in the previously cited papers is to compare quadratic forms or to show resolvent convergence of some sort, and in all cases only the leading asymptotic behavior is obtained.
- A large number of dimensionality reduction methods have been proposed that focused on local correlation (e.g. @cite @cite @cite @cite ). Many of these methods do not assume any specific characteristics. Now, we are concentrating on the dimensionality reduction of time-series signals, and therefore we take advantage of their continuity and local correlation. The computational cost for obtaining such feature subsets is expected to be very small compared with that of existing methods that do not utilize the continuity and local correlation of time-series signals.
- Dimensionality reduction methods for time-series signals are categorized into two types: temporal dimensionality reduction , namely dimensionality reduction along the temporal axis (e.g. feature sampling), and spatial dimensionality reduction , namely the dimensionality reduction of each multi-dimensional feature sample. Keogh @cite @cite and Wang @cite have introduced temporal dimensionality reduction into waveform signal retrieval. Their framework considers the waveform itself as a feature for detecting similar signal segments. That is why they mainly focused on temporal dimensionality reduction. When considering audio fingerprinting, however, we handle sequences of high-dimensional features that are necessary to identify various kinds of audio segments. Thus, both spatial and temporal dimensionality reduction are required. To this end, our method mainly focuses on spatial dimensionality reduction. We also incorporate a temporal dimensionality reduction technique inspired by the method of @cite , which is described in Section .
- Dynamic segmentation is a generic term that refers to techniques for dividing sequences into segments of various lengths. Dynamic segmentation methods for time-series signals have already been applied to various kinds of applications such as speech coding (e.g. @cite ), the temporal compression of waveform signals @cite , the automatic segmentation of speech signals into phonic units @cite , sinusoidal modeling of audio signals @cite @cite @cite and motion segmentation in video signals @cite . We employ dynamic segmentation to minimize the average dimensionality of high-dimensional feature trajectories.
- Dynamic segmentation can improve dimension reduction performance. However, finding the optimal boundaries still requires a substantial calculation. With this in mind, several studies have adopted suboptimal approaches, such as longest line fitting @cite , wavelet decomposition @cite @cite and the bottom-up merging of segments @cite . The first two approaches still incur a substantial calculation cost for long time-series signals. The last approach is promising as regards obtaining a rough global approximation at a practical calculation cost. This method is compatible with ours, however, we mainly focus on a more precise local optimization.
- In general, it is crucial to note that the reconstruction method depends on the slope-measuring sensor and the properties of the acquired data. For example, slope data acquired by Shape from Shading is rather noisy, exhibits curl, and is usually located on a full grid of millions of points. Here, a fast subspace approximation method like the one proposed by Frankot and Chellappa @cite is appropriate. On the other hand, wavefront reconstruction deals with much smaller data sets, and the surface is known to be rather smooth and flat. In this case, a direct finite-difference solver can be applied @cite . Deflectometric sensors deliver a third type of data: It consists of very large data sets with rather small noise and curl, but the data may not be complete, depending on the local reflectance of the measured surface. Furthermore, the measuring field may have an unknown, irregularly shaped boundary. These properties render most of the aforementioned methods unusable for deflectometric data. In the following sections, we will describe a surface reconstruction method which is especially able to deal with slope data acquired by sensors such as Phase-measuring Deflectometry.
- Taimanov @cite has proven that if a compact manifold @math admits a real-analytically completely integrable geodesic flow, then @math is almost abelian of rank at most @math ; @math ; and there is an injection @math where @math . These constraints are ineffective for exotic tori.
- @cite , Rudnev and Ten assume that a geodesic flow is completely integrable with a non-degenerate first-integral map on an @math -dimensional compact manifold with first Betti number equal to @math . Non-degeneracy means, amongst other things, that the singular set is stratified by the rank of the first integral map and each stratum is a symplectic submanifold on which the system is completely integrable. From these hypotheses, they deduce that there is a lagrangian torus @math such that the natural map @math (figure ) is a homeomorphism . Theorem 2 of @cite states that @math is a diffeomorphism, but this is mistaken. It is shown only that @math is a @math smooth map, hence by invariance of domain, a homeomorphism. To prove that @math is a diffeomorphism one must prove that the Maslov cocycle of @math vanishes, or something equivalent. This is the first difficulty in proving theorem .
- Despite significant research progress in using MIMO for single-user communications, research on multi-user multi-hop MIMO networks is still in its inception stage. There are many open problems, and many areas are still poorly understood @cite . Currently, the relatively well-studied research area of multi-user MIMO systems are cellular systems, which are single-hop and infrastructure-based. For multi-hop MIMO-based mesh networks, research results remain limited. In @cite , Hu and Zhang studied the problem of joint medium access control and routing, with a consideration of optimal hop distance to minimize end-to-end delay. In @cite , Sundaresan and Sivakumar used simulations to study various characteristics and tradeoffs (multiplexing gain vs. diversity gain) of MIMO links that can be leveraged by routing layer protocols in rich multipath environments to improve performance. In @cite , proposed a distributed algorithm for MIMO-based multi-hop ad hoc networks, in which diversity and multiplexing gains of each link are controlled to achieve the optimal rate-reliability tradeoff. The optimization problem assumes fixed SINRs and fixed routes between source and destination nodes. However, in these works, there is no explicit consideration of per-antenna power allocation and their impact on upper layers. Moreover, DPC in cross-layer design has never been studied either.
- We will briefly survey a few results on the complexity of the densest @math -subgraph problem. The best approximation algorithm known for the general problem (when @math is specified as part of the input) is the algorithm of Feige, Peleg, and Kortsarz @cite , which has ratio @math for some @math . For any particular value of @math , the greedy algorithm of @cite gives the ratio @math . Algorithms based on linear programming and semidefinite programming have produced approximation ratios better than @math for certain values of @math , but have not improved the approximation ratio of @math for the general case @cite @cite .
- Feige and Seltser @cite showed the densest @math -subgraph problem is @math -complete when restricted to bipartite graphs of maximum degree 3, by a reduction from max-clique. This reduction does not produce a hardness of approximation result for DkS. In fact, they showed that if a graph contains a @math -clique, a subgraph with @math vertices and @math edges can be found in subexponential time. Khot @cite proved there can be no PTAS for the densest @math -subgraph problem, under a standard complexity assumption.
- Arora, Karger, and Karpinski @cite gave a PTAS for the special case @math and @math . Asahiro, Hassin, and Iwama @cite showed that the problem is still @math -complete in very sparse graphs.
- Constraint based verification using counting abstractions @cite @cite @cite , provides complete procedures for checking safety properties of broadcast protocols. However, such approaches have theoretically non-primitive recursive upper bounds for decision procedures (although they work well for small, interesting, examples) --- in our case the upper bounds are definitely primitive-recursive;
- More recently, cooperative approaches have been proposed in the network community. , @cite show how cooperation amongst MNs can be beneficial for all the MNs in the network in terms of bit-rate, coverage and throughput. Each MN builds a table in which possible helpers for that MN are listed. If an MN has a poor link with the AP and its bit-rate is low, it sends packets to the helper who relays them to the AP. The advantage in doing this is that the link from the MN to the helper and from the helper to the AP is a high bit-rate link. In this way the MN can use two high bit-rate links via the helper instead of the low bit-rate one directly to the AP.
- , @cite introduce a location sensing mechanism based on cooperative behavior among stations. Stations share location information about other stations and about landmarks so to improve position prediction and reduce training.
- In @cite suggest an algorithm called syncscan which does not require changes to either the protocol or the infrastructure. It does require, however, that all the APs in the network are synchronized and only accelerates unauthenticated L2 handoffs.
- @cite proposed a query-based debugger to understand object relationships. Their query language is expressed in the same language as the target object-oriented language (Self), and thus a programmer does not need to learn a new language. Queries consist of a search domain and a constraint. Lencevicius' query-based debugger provides incremental delivery of results, a feature that is useful in dealing with queries that takes considerable time to find all answers.
- Recently, PQL (Program Query Language) was developed by @cite to query over source code and program trace for finding errors and security flaws in programs. Queries may formulate application-specific code patterns that may result in vulnerabilities at run-time. Queries are translated to Datalog (which is essentially declarative Prolog without function symbols), and provide the ability to take an action once a match found. A combination of static and dynamic analyses is performed to answer queries. The PQL compiler generates code that is weaved into the target application and matches against a history of relevant events at execution time. A number of interesting security violations are found by this technique.
- @cite proposed the PTQL (Program Trace Query Language) as a relational query language designed to query program trace. Similar in goals with PQL, PTQL employs an SQL-like query language. Partiqle compiles the PTQL queries into instrumentation in a given Java program. PTQL queries can be used to specify what is to be recorded during program execution, and hence this technique can be effective with programs that generate many irrelevant events.
- Hy @math @cite is a visual debugger for distributed programs. The system works as follows. The program is instrumented to obtain trace which is used to build database implemented in CORAL. Programmers can specify debugging queries and visualizations using a visual declarative query language called GraphLog. These visual queries once formulated can be saved and applied to other programs since these queries are application independent. This technique allows the programmere to visualize a specific program behavior pattern and filter out irrelevant events. Hy @math performs static trace analysis and has a simple postmortem dynamic trace analysis by animating the program trace.
- JIVE's @cite @cite (Java Interactive Visualization Engine) design is based on the following seven criteria: (1) depict objects as environment of method execution; (2) display object states at different levels of granularity; (3) provide a sequence diagram to capture the history of execution; (4) support forwards and backwards execution of programs; (5) support queries on the runtime state; (6) produce clear and legible drawings; (7) uses exiting Java technologies. JIVE interacts with the JPDA to extract program trace. An on-line dynamic trace analysis is applied while the program runs for the first time in the forwards direction and postmortem trace analysis is applied in the backwards direction or in the forwards direction once program terminates.
- The omniscient debugger (ODB) developed by Bil Lewis @cite aims at easing the navigation of program trace in both forwards and backwards directions. ODB obtains program trace by a load-time instrumentation of the byte code of the debugged program. Execution events are recorded while the program runs, once finished a program state display is provided. ODB uses a static trace analysis and the program trace is kept in memory. Lewis proposed three techniques to reduce the size of the recorded program trace (1) delete old events; (2) allow the programmer to exclude a set of classes and methods form instrumentation and recording 3) a recording interval can be specified. The recording technique applied in the ODB is fast and efficient.
- WhyLine @cite is an interrogative debugger for the Alice programming environment. It allows the user to ask why a given event did or did not occur. The WhyLine gives the answer in the form of an execution path that leads or was supposed to lead to the execution of the given event. The path is annotated with control flow information. The comparison among these systems are based on four features: (1) automatic program trace extraction; (2) program trace navigation features such as forwards and backwards stepping, breakpoint, and conditional breakpoint; (3) query language support; (4) built-in trace analyses including a set of the most recurring debugging queries or abstract views of program behavior. Table shows the comparison among the 10 system. JavaTA currently does not support trace navigation; however, it is straight forward to implement. Opium supports similar features as JavaTA especially the built-in trace analyses; however, these analyses are hard to be compared since they target two different programming paradigms namely declarative and imperative respectively.
- A motion signal is comprised of two components: orientation and translation. The orientation vector indicates where the object is facing, whereas the translation component determines the object's location. Recent work has focused on smoothing the orientation vectors @cite @cite , whereas the results of the present paper apply equally well to orientation vectors (points on the surface of a unit sphere) as to arbitrary translation signals.
- @cite @cite @cite , the authors chose to define monotonicity for curves or chains with an arbitrary direction vector: a curve is monotone if its projection on a line is does not backtrack. While this is a sensible choice given the lack of definition elsewhere, we argue that not all applications support an arbitrary direction that can be used to define monotonicity.
- One approach to chain smoothing is to use B-splines and Bezier curves with the @math norm @cite . Correspondingly, we could measure the smoothness'' of a chain by measuring how closely one can fit it to a smooth curve. Our approach differs in that we do not use polygonal approximations or curve fitting: we consider chains to be first-class citizens.
- In the media streaming community, the idea of path diversity is traditionally combined with multiple-description coding: complementary streams are simultaneously sent over independent paths, to achieve resilience to loss in a bandwidth-efficient manner. @cite proposed to transmit multiple- description video over independent paths; in follow-up work @cite , the same authors used this idea to design a content-delivery network. @cite applied the same idea to Voice-over-IP and also designed an playout scheduling algorithm to handle multi-path transmission. The same authors did a simulation study on the effect of replication and path diversity on TCP transfers @cite .
- Our work fits in this scope as follows. It is related to multi-homing and overlay approaches in that it tries to improve end-to-end performance by connecting edge-networks via several different ISPs and by exploiting their path diversity. We compare to related work as follows. The novel aspect we are focusing on is proactive replication of every packet over the available paths in a single RAIL. This aspect is orthogonal to the online decision of switching traffic between RAILs (i.e. sets of paths). However, in this paper we still explore how to choose and manage the physical paths that constitute a single RAIL. Similarly to @cite @cite , we are looking at application-level metrics, particularly for VoIP and TCP. In contrast to the media-streaming work, we transmit redundant as opposed to complementary descriptions, operating on the assumption that bandwidth is not the issue. Our delay padding algorithm resembles playout buffering @cite in that it tries to smooth out the network delay jitter; however, it is implemented at an edge device instead of the end-point, and acts only as a playout-proxy without dropping packets.
- The geometry of has been extensively studied in the literature, e.g. @cite @cite and more recently in @cite @cite and the references therein. The latter two references studied algorithms based on formulations of the completion problem.
- The relaxations solve a closest matrix problem and generally use the @math norm. The @math norm is used in @cite , where the noise in the radio signal is assumed to come from a multivariate normal distribution with mean @math and variance-covariance matrix @math , i.e. from a spherical normal distribution so that the least squares estimates are the maximum likelihood estimates. (We use the @math norm as well in this paper. Our approach follows that in @cite for completion without anchors.)
- It has been shown that the global minimum of @math with @math is reached when the output @math is proportional to the source with the lowest entropy @cite . It is proven in @cite that when a fixed-variance output is proportional to one of the sources, then, under some technical conditions, the cumulant-based approximation of entropy @math used in FastICA @cite reaches a non-mixing local minimum. Finally, based on the entropy power inequality @cite , it is also proven in @cite that, in the two-dimensional case, Shannon's entropy has a local minimum when the output is proportional to a non-Gaussian source.
- As for the mutual information, simulations results in @cite suggest that mixing local entropy minima exist in specific cases (i.e. when the source pdfs are strongly multimodal, which sometimes occur in practice, for sinusoid waveforms among other). These results, based on density estimation using the Parzen kernel method, are confirmed by other simulations using directly entropy estimation, such as Vasicek's one in @cite or based on the approximator analyzed in this paper in @cite . Rigorously speaking, the above results do not constitute an absolute proof since error bounds are not available for the approximation procedure. By contrast, a theoretical proof is given in @cite , but for a specific example only (two bimodal sources sharing the same symmetric pdf). The existence of mixing local entropy minima has also been shown in @cite (without detailed proof) in the case of two non symmetric sources with strongly multimodal pdfs.
- Similar approaches derive discrete equations of rigid body motion for optimal control problems. Bloch, Crouch, Marsden and Ratiu @cite , for example, derive the symmetrised rigid body equations by introducing optimality constraints in the action principle. We distinguish our approach from theirs in two ways. Firstly, although they consider the rigid body motion as an optimal control problem with an associated constrained action principle, they do not identify the constraints as Clebsch variables and derive the momentum maps. Secondly, they present left and right trivialisations of @math where as we present body and spatial representations of a left SO(3) action invariant Lagrangian only. The authors make this point when distinguishing their approach from that of Holm and Kupershmidt @cite . We use the expression for the (left) momentum map to prove that the flow on the cotangent bundle preserves spatial angular momentum and derive the equations of motion.
- INS @cite identifies network services using intentional names, which specify the kind of network service desired instead of the network address. It supports the lazy binding of names to resources by combining naming and transport. Network services must have access to all relevant contextual information when registering an intentional name. INS uses a network of intentional name resolvers as its infrastructure.
- There have been naming systems not targeted for ubiquitous computing environments that also use relative naming. Tilde @cite and Prospero @cite are file systems based on relative naming. Prospero is also able to support a limited form of location-aware computing by creating symbolic links according to the login terminal of a user @cite .
- Like INS, Active Names @cite combines naming and transport. Its purpose is to provide an extensible network infrastructure based on names. The routing mechanism is similar to the name resolution process in NUN in that a name can be divided into multiple components, and each component in the name determines the next program used in routing a data packet. The work done by each program is arbitrary, so a great deal of flexibility is possible when routing packets.
- french02 cites Structure Mapping Theory (SMT) @cite and its implementation in the Structure Mapping Engine (SME) @cite as the most influential work on modeling of analogy-making. The goal of computational modeling of analogy-making is to understand how people form complex, structured analogies. SME takes representations of a source domain and a target domain, and produces an analogical mapping between the source and target. The domains are given structured propositional representations, using predicate logic. These descriptions include attributes, relations, and higher-order relations (expressing relations between relations). The analogical mapping connects source domain relations to target domain relations.
- For example, there is an analogy between the solar system and Rutherford's model of the atom @cite . The solar system is the source domain and Rutherford's model of the atom is the target domain. The basic objects in the source model are the planets and the sun. The basic objects in the target model are the electrons and the nucleus. The planets and the sun have various attributes, such as mass(sun) and mass(planet), and various relations, such as revolve(planet, sun) and attracts(sun, planet). Likewise, the nucleus and the electrons have attributes, such as charge(electron) and charge(nucleus), and relations, such as revolve(electron, nucleus) and attracts(nucleus, electron). SME maps revolve(planet, sun) to revolve(electron, nucleus) and attracts(sun, planet) to attracts(nucleus, electron).
- Metaphorical language is very common in our daily life; so common that we are usually unaware of it @cite . gentner01 argue that novel metaphors are understood using analogy, but conventional metaphors are simply recalled from memory. A conventional metaphor is a metaphor that has become entrenched in our language @cite . dolan95 describes an algorithm that can recognize conventional metaphors, but is not suited to novel metaphors. This suggests that it may be fruitful to combine Dolan's algorithm for handling conventional metaphorical language with LRA and SME for handling novel metaphors.
- The problem of relation extraction is, given an input document and a specific relation @math , extract all pairs of entities (if any) that have the relation @math in the document. The problem was introduced as part of the Message Understanding Conferences (MUC) in 1998. zelenko03 present a kernel method for extracting the relations person-affiliation and organization-location . For example, in the sentence John Smith is the chief scientist of the Hardcom Corporation,'' there is a person-affiliation relation between John Smith'' and Hardcom Corporation'' @cite . This is similar to the problem of classifying semantic relations (), except that information extraction focuses on the relation between a specific pair of entities in a specific document, rather than a general pair of words in general text. Therefore an algorithm for classifying semantic relations should be useful for information extraction.
- hearst92a presents an algorithm for learning hyponym ( type of ) relations from a corpus and berland99 describe how to learn meronym ( part of ) relations from a corpus. These algorithms could be used to automatically generate a thesaurus or dictionary, but we would like to handle more relations than hyponymy and meronymy. WordNet distinguishes more than a dozen semantic relations between words @cite and nastase03 list 30 semantic relations for noun-modifier pairs. hearst92a and berland99 use manually generated rules to mine text for semantic relations. turneylittman05 also use a manually generated set of 64 patterns.
- A semantic frame for an event such as judgement contains semantic roles such as judge , evaluee , and reason , whereas an event such as statement contains roles such as speaker , addressee , and message @cite . The task of identifying semantic roles is to label the parts of a sentence according to their semantic roles. We believe that it may be helpful to view semantic frames and their semantic roles as sets of semantic relations; thus a measure of relational similarity should help us to identify semantic roles. moldovan04 argue that semantic roles are merely a special case of semantic relations (), since semantic roles always involve verbs or predicates, but semantic relations can involve words of any part of speech.
- First of all, there are those projects that also deal with predicate-argument structures in some way, in particular FrameNet @cite (which is mainly a lexicographical project but can, of course, be adopted for extensive corpus annotation, as is currently done in the project @cite ), PropBank @cite , and NomBank @cite . In these projects, the predicate-argument annotation is the main objective, so they all try some kind of generalisation by organising their predicates in semantic frames (FrameNet) or by following the Levin classes (PropBank, and for nominalisations also NomBank). In FuSe, however, this type of annotation is not an end in itself -- predicates and their arguments simply constitute the transemes. Consequently, their annotation is kept deliberately simple and is entirely predicate-group specific without any attempt at generalisation.
- In the project @cite , texts from six languages (Arabic, French, Hindi, Japanese, Korean, and Spanish) and their translations into English are annotated for interlingual content. For each original text, at least two English translations are being annotated (so as to be able to study paraphrases), and the annotation proceeds incrementally over three increasingly abstract levels of representation.
- In recent years, overlay networks have proven a popular way of disseminating potentially large files (such as a new software product or a video) from a single server @math to a potentially large group of @math end users via the Internet. A number of algorithms and protocols have been suggested, implemented and studied. In particular, much attention has been given to peer-to-peer (P2P) systems such as BitTorrent @cite , Slurpie @cite , SplitStream @cite , Bullet' @cite and Avalanche @cite , to name but a few. The key idea is that the file is divided into @math parts of equal size and that a given user may download any one of these (or, for network coding based systems such as Avalanche, linear combinations of these) either from the server or from a peer who has previously downloaded it. That is, the end users collaborate by forming a P2P network of peers, so they can download from one another as well as from the server. Our motivation for revisiting the broadcasting problem is the performance analysis of such systems.
- With the BitTorrent protocol http: bitconjurer.org BitTorrent protocol.html , for example, when the load on the server is heavy, the protocol delegates most of the uploading burden to the users who have already downloaded parts of the file, and who can start uploading those parts to their peers. File parts are typically @math megabyte (MB) in size. An application helps downloading peers to find each other by supplying lists of contact information about randomly selected peers also downloading the file. Peers use this information to connect to a number of neighbours. A full description can be found in @cite . The BitTorrent protocol has been implemented successfully and is deployed widely. A detailed measurement study of the BitTorrent system is reported in @cite . According to @cite , BitTorrent's share of the total P2P traffic has reached 53 . For recent measurements of the total P2P traffic on Internet backbones see @cite .
- Slurpie @cite is a very similar protocol, although, unlike BitTorrent, it does not fix the number of neighbours and it adapts to varying bandwidth conditions. Other P2P overlay networks have also been proposed. For example see SplitStream @cite and Bullet' @cite .
- In this paper, we provide the scheduling background, proofs and discussion of the results in our extended abstracts @cite and @cite . It is essentially Chapter 2 of @cite , but we have added Theorem and the part on theoretical bounds in Section . @cite the authors also consider problems concerned with the service capacity of P2P networks, however, they only give a heuristic argument for the makespan with equal upload capacities when @math is of the simple form @math . @cite a fluid model for BitTorrent-like networks is introduced and studied, also looking at the effect of incentive mechanisms to address free-riding. Link utilization and fairness are issues in @cite . @cite , also motivated by the BitTorrent protocol and file swarming systems in general, the authors consider a probabilistic model of coupon replication systems. Multi-torrent systems are discussed in @cite . There is other related work in @cite .
- In @cite , the authors considered the bidirectional telephone model in which nodes can both send one message and receive one message simultaneously, but they must be matched pairwise. That is, in each given round, a node can only receive a message from the same node to which it sends a message. They provide an optimal algorithm for odd @math , which takes @math rounds. For even @math their algorithm is optimal up to an additive term of @math , taking @math rounds.
- The simultaneous send receive model @cite supposes that during each round every user may receive one message and send one message. Unlike the telephone model, it is not required that a user can send a message only to the same user from which it receives a message. The optimal number of rounds turns out to be @math and we will return to this result in Section .
- As noted above, SVM for regression (as opposed to SVM classification) is rarely applied in physics. There are, however, several successful examples of the support vector regression application. In @cite introduced the regression type of the SVM technique to the civil engineering community and showed that SVM can be successfully applied to the problem of stream flow data estimation based on records of rainfall and other climatic data. By using three types of kernels, Polynomial, RBF, and Neural Network, and choosing the best values for SVM free parameters via trial and error, the authors point out that the SVM with the RBF kernel performs the best. Finally, this research is the first attempt to apply support vector regression in data analysis of VISAR measurements obtained from experiments on shock melted and damaged met al.
- Linear averaging @cite , @cite is arguably the most popular aggregation principle, given its simplicity, various axiomatic justifications, and documented empirical success. To illustrate this natural approach, consider the panel exhibited in Table 2. Here, three judges provide forecasts for three events, a conjunction and its conjuncts. The Aggregate" forecast is the simple un-weighted average of the three judges' forecasts. Though appealing, linear averaging is not without pitfalls, as can be illustrated with a few examples.
- @cite consider a Bayesian approach to reconciling probability forecasts, whereby noisy" observations @math are assumed to arise from a coherent set @math . CAP can be viewed as a special-case of their model, since as discussed above, the solution to ) admits a Bayesian interpretation as the maximum-likelihood coherent forecasts given additive white noise corrupted observations @math . However, note that @cite sought to eliminate incoherence from a single judge, whereas CAP was introduced to address the panel aggregation problem. Moreover, Osherson and Vardi were motivated by non-statistical interpretations of CAP and as here, addressed the computational issue of implementing CAP.
- A panel-aggregation problem is addressed in the online" learning model, which is frequently studied in learning theory @cite @cite . In that setting, a panel of experts predicts the true outcome of a set of events. A central agent constructs its own forecast by fusing the experts' predictions, and upon learning the truth, suffers a loss sometimes specified by a quadratic penalty function. In repeated trials, the agent updates its fusion rule (e.g., the weights" in a weighted average), taking into account the performance of each expert. Under minimal assumptions on the evolution of these trials, bounds are derived that compare the trial-averaged performance of the central agent with that of the best (weighted combination of) expert(s). In contrast to the current framework, the online model typically assumes that each expert provides a forecast for the same event or partition of events. Thus, fusion strategies such as weighted averaging are appropriate in the online model, for the same reasons discussed above. Also, observe that the present model concerns a single trial", not many.
- Finally, proponents of Dempster-Shafer theory @cite (and associated fusion rules) object to probability as an idiom for belief, in part because of its inability to distinguish uncertainty from ignorance. The merits of Dempster-Shafer aside, one could argue for abstention as an expression of ignorance. As the preceding examples illustrate, even abstaining experts may disagree (i.e., experts' forecasts may be mutually incoherent), and therefore the panel aggregation problem remains. Thus, CAP is a natural aggregation principle in the setting where judges express uncertainty with probability and ignorance through abstention, and thereby extends the utility of probabilistic forecasts by affording experts more expressive beliefs with abstention.
- Many algorithms to find community structures in graphs exist. Most of them result from very recent works, but this topic is related to the classical problem of that consists in splitting a graph into a given number of groups while minimizing the cost of the edge cut @cite @cite @cite . However, these algorithms are not well suited to our case because they need the number of communities and their size as parameters. The recent interest in the domain has started with a new approach proposed by Girvan and Newman @cite @cite : the edges with the largest (number of shortest paths passing through an edge) are removed one by one in order to split hierarchically the graph into communities. This algorithm runs in time @math . Similar algorithms were proposed by Radicchi @cite and by Fortunato @cite . The first one uses a local quantity (the number of loops of a given length containing an edge) to choose the edges to remove and runs in time @math . The second one uses a more complex notion of information centrality that gives better results but poor performances in @math .
- The CSIRO group participating in INEX 2002 proposed a similar XML retrieval approach where PADRE, the core of CSIRO's Panoptic Enterprise Search Engine http: www.panopticsearch.com is used to rank full articles and elements within articles @cite . Unlike many full-text information retrieval systems, PADRE combines full-text and metadata indexing and retrieval and is also capable of indexing and retrieving more specific elements within articles. A post processing module is then used to extract and re-rank the full articles and elements within articles returned by PADRE. However, unlike our CRE retrieval module, the above approach ignores the structural elements within articles that contain the indexed element. Less specific and more general elements are therefore not likely to appear in the final answer list.
- For the purpose of ranking the resulting answers of XML retrieval topics, @cite extend the probabilistic ranking model by incorporating the notion of structural roles'', which can be determined manually from the document schema. However, the term frequencies are measured only for the structural elements belonging to a particular role, without taking into account the entire context where all these elements belong in the document hierarchy. XRank @cite and XSearch @cite furthermore aim at producing effective ranked results for XML queries. XRank generally focuses on hyperlinked XML documents, while XSearch retrieves answers comprising semantically related nodes. However, since the structure of IEEE XML documents in the INEX document collection does not typically meet the above requirements, neither of them (without some modifications) could be used in a straightforward fashion with the CAS retrieval task.
- In this paper, we consider a specific class of cooperative information agents without considering effects of their actions on the environment e.g. in @cite , @cite , @cite . We are currently working to extend the framework towards this generalized issue.
- In this paper communications for agents are based on push-technologies. It is interesting to see how the results could be extended to multiagent systems whose communication is based on pull-technologies ( @cite , @cite ).
- In @cite @cite @cite cyclic term graph rewriting is considered using the algorithmic way. Pointer redirection is limited to global redirection of all edges pointing to the root of a redex by redirecting them to point to the root of the instance of the right-hand side. In @cite , Banach, inspired by features found in implementations of declarative languages, proposed rewrite systems close to ours. We share the same graphs and global redirection of pointers. However, Banach did not discuss local redirections of pointers. We differ also in the way to express rewriting. Rewriting steps in @cite are defined by using the notion of opfibration of a category while our approach is based on double-pushouts.
- In @cite , Habel and Plump proposed a kernel language for graph transformation. This language has been improved recently in @cite . Basic rules in this framework are of the form @math satisfying some conditions such as the inclusion @math . Unfortunately, our rewrite rules do not fulfill such condition ; particularly when performing local edge redirections. Furthermore, inverse pushouts (or pushout complements) are not unique in our setting which is not the case in @cite @cite .
- Recently, in @cite the authors are also interested in classical data-structures built by using pointers. Their work is complementary to ours in the sense that they are rather concerned by data-structure shapes by means of so called Graph reduction specifications.
- Sequential algorithms turned out to be quite central in the study of sequentiality. First, let us mention that Kleene has developed (for lower types) similar notions @cite , under the nice name of oracles, in his late works on the semantics of higher order recursion theory (see @cite for a detailed comparison).
- Two important models of functions that have been constructed since turned out to be the extensional collapse (i.e. the hereditary quotient equating sequential algorithms computing the same function, i.e. (in the affine case) two algorithms @math and @math such that @math ): Bucciarelli and Ehrhard's model of strongly stable functions @cite @cite , and Longley's model of sequentially realizable functionals @cite . The first model arose from an algebraic characterization of sequential (first-order) functions, that carries over to all types. The second one is a realizability model over a combinatory algebra in which the interaction at work in sequential algorithms is encoded.
- Protocols selection in agents interactions design is something generally done at design time. Indeed, most of the agent-oriented design methodologies ( @cite and @cite to quote a few) all make designers decide which role agents should play for each single interaction. However dynamic behaviours and openness in MAS demand greater flexibility.
- To date, there have been some efforts to overcome this limitation. @cite introduces more flexibility in agents' coordination but it only applies to planning mechanisms of the individual agents. @cite also proposes a framework based on multi-agent Markov decision processes. Rather than identifying a coordination mechanism which suits best for a situation, this work deals with optimal reasoning within the context of a given coordination mechanism. @cite proposed a framework that enables autonomous agents to dynamically select the mechanism they employ in order to coordinate their inter-related activities. Using this framework, agents select their coordination mechanisms reasoning about the rewards they can obtain from collaborative tasks execution as well as the probability for these tasks to succeed.
- Our work has been widely inspired by the JML-JUnit approach @cite . The JML-JUnit tool generates test cases for a method which consist of a combination of calls of this method with various parameter values. The tester must supply the object invoking the method and the parameter values. With this approach, interesting values could easily be forgotten by the tester. Moreover, as a test case only consists of one method call, it is not possible to detect errors which result of several calls of different methods. At last, the JML-JUnit approach compels the user to construct the test data, which may require the call of several constructors. Our approach thus has the advantage of being more automatic, and of being able to detect more potential errors.
- Plaxton @cite proposed a distributed routing protocol based on hypercubes for a static network with given collection of nodes. Plaxton's algorithm uses the technique to locate the shared resources on an overlay network in which each node only maintains a small-sized routing table. Pastry @cite and Tapestry @cite use Plaxton's scheme in the dynamic distributed environment. The difference between them is that Pastry uses routing scheme, whereas Tapestry uses scheme. The number of bits per digit for both Tapestry and Pastry can be reconfigured but it remains fixed during run-time. Both Pastry and Tapestry can build the overlay topology using proximity neighbor selection. However, it is still unclear whether there is any better approach to achieve globally effective routing.
- It is difficult to say which one of above proposed DHTs is best". Each routing algorithm offers some insight on routing in overlay network. One appropriate strategy is to combine these insights and formulate an even better scheme @cite .
- There are other pseudo-telepathy games that are related to the magic square game. Ad 'an Cabello's game @cite @cite does not resembles the magic square game on first approach. However, closer analysis reveals that the two games are totally equivalent!
- Also, Aravind has generalized his own magic square idea @cite to a two-player pseudo-telepathy game in which the players share @math Bell states, @math being an arbitrary odd number larger than 1.
- Long-range percolation, of which our model is an example, originated in the mathema -ti -cal-physics literature as a model that exhibits a phase transition even in spatial dimension one (e.g., Newman and Schulman @cite , Schulman @cite , Aizenman and Newman @cite , Imbrie and Newman @cite ). It soon became clear that @math and @math are two distinguished values; for @math the model is essentially mean-field (or complete-graph) alike, for @math the behavior is more or less as for the nearest-neighbor percolation. The regime @math turned out to be quite interesting; indeed, it is the only general class of percolation models with Euclidean (or amenable) geometry where one can prove absence of percolation at the percolation threshold (Berger @cite ). In all dimensions, the model with @math has a natural continuum scaling limit.
- Recently, long-range percolation has been invoked as a fruitful source of graphs with non-trivial growth properties. Our interest was stirred by the work of Benjamini and Berger @cite who proposed (and studied) long-range percolation as a model of social networks. It is this context where the graph distance scaling, and volume growth, are particularly of much interest. Thanks to numerous contributions that followed @cite , this scaling is now known for most values of @math and @math . Explicitly, for @math , a corollary to the main result of Benjamini, Kesten, Peres and Schramm @cite asserts that almost surely. As @math , the right-hand side tends to infinity and so, at @math , we expect @math . And, indeed, the precise growth rate in this case has been established by Coppersmith, Gamarnik and Sviridenko @cite , where @math '' means that the ratio of left and right-hand side is a random variable that is bounded away from zero and infinity with probability tending to one.
- For @math , the present paper states @math . Here we note that @math as @math which, formally, is in agreement with . For @math we in turn have @math and so, at @math , a polylogarithmic growth is no longer sustainable. Instead, for the case of the decay @math one expects that where @math varies through @math as @math sweeps through @math . This claim is supported by upper and lower bounds in somewhat restricted one-dimensional cases (Benjamini and Berger @cite , Coppersmith, Gamarnik and Sviridenko @cite ). However, even the existence of a sharp exponent @math has been elusive so far.
- For @math one expects @cite the same behavior as for the original graph. And indeed, the linear asymptotic, has been established by Berger @cite . For the nearest-neighbor percolation case, this statement goes back to the work of Antal and Pisztora @cite .
- Further motivation comes from the recent interest in diffusive properties of graphs arising via long-range percolation. An early work in this respect was that of Berger @cite who characterized regimes of recurrence and transience for the simple random walk on such graphs. Benjamini, Berger and Yadin @cite later showed that the mixing time @math of the random walk on @math in @math scales like with an apparent jump in the exponent when @math passes through 2. Misumi @cite found estimates on the effective resistance in @math that exhibit a similar transition.
- Very recently, precise bounds for the heat kernel and spectral gap of such random walks have been derived by Crawford and Sly @cite . These are claimed to lead to the proof that the law of such random walks scales to @math -stable processes for @math in @math and @math in @math . For @math on the increasing side of these regimes, the random walk is expected to scale to Brownian motion.
- Let us now mention some other results concerning estimates on decay of correlations for non-H "older observables. Most of these are stated in the context of one-sided finite alphabet shift maps, or subshifts of finite type. (For a comprehensive discussion of shift maps and equilibrium measures, we suggest the book of Baladi, @cite .) Shift maps are relatively simple dynamical systems, but are often used to more complicated systems via a semi-conjugacy, in much the same way that each of the examples we consider can be represented by a suitable (see ). Where a system @math being coded has an invariant measure @math which is absolutely continuous with respect to Lebesgue measure, @math is an equilibrium measure for the potential @math , where @math is the Jacobian with respect to Lebesgue measure. Most results for shift maps work with an equilibruim measure given by a potential @math which is H "older continuous (in terms of the usual metric on shift spaces - two sequences are said to be distance @math apart if they agree for exactly the first @math symbols). This assumption corresponds to assuming good distortion for @math .
- The have been various results concerned primarily with weakening the assumption on the regularity of @math , and obtaining (slower) upper bounds for the rate of mixing with respect to the corresponding equilibrium measures. Kondah, Maume and Schmitt ( @cite ) used a method of Birkhoff cones and projective metrics, Bressaud, Fernandez and Galves ( @cite ) used a coupling method (different from the one described here), with estimates given in terms of , and Pollicott ( @cite ) introduced a method involving composing transfer operators with conditional expectations. Each of these results has slightly different assumptions and gives slightly different estimates, but in each case a number of different classes of potentials are considered, and estimates are given for for observables of some similar regularity to (usually than) the potential. In particular, in all three examples polynomial mixing is given for a potential and observables with variations decaying at suitable polynomial rates.
- Finally, we mention a result which applies directly to certain non-uniformly expanding systems, rather than to a symbolic space or tower. Pollicott and Yuri ( @cite ) consider a class of maps of arbitrary dimension with a single indifferent periodic orbit and a given Markov structure, including in particular the Manneville-Pomeau interval maps. The class of observables considered is dynamically defined; each observable is required to be Lipschitz with respect to a Markov partition corresponding to some induced map, chosen to have good distortion properties. This class includes all functions which are Lipschitz with respect to the manifold, and while some estimates are weaker than comparable results for H "older observables, bounds are obtained for some observables which cannot be dealt with at all by our methods, such as certain unbounded functions.
- Some systems @cite @cite use inference engines to discover semantic relationships between data from ontology representations. Inference engines usually have specialized languages for expressing queries different from the language used to retrieve data, therefore user queries have to be either expressed in or translated into the language of the inference engine. The ontology is either global (i.e., domain independent) or domain-specific (i.e., only a single domain) ontology. Domain-specific ontologies are smaller and more commonly found than global ontologies because they are easier to specify. Additionally, there are systems that use mapping functions exclusively and do not have inference engines @cite @cite . In these systems, mapping functions serve the role of an inference engine.
- To improve scalability, peer-to-peer database systems are looking in the direction of semantic routing. HyperCuP @cite uses common ontology to dynamically cluster peers based on the data they contain. A cluster is identified using a more general concept then those associated with its members in the ontology. Concepts in the ontology map to cluster addresses so a node can determine appropriate route for a query by looking up more general concepts of the query terms in the concept hierarchy. Edutella @cite uses query hubs (functionally similar to brokers) to collect user metadata and present the peer-to-peer network as a virtual database which users query. All queries are routed though a query hub which forwards queries only to those nodes that can answer it.
- Zambonelli and Netzer @cite proposed a method that, by taking on-line decisions on whether to log or not to log a monitored event, deviates from the strict FIFO-solution. However, sometimes logging will debit the system with a jitter in the execution time, and so will also the algorithm it self. As larger jitter will force more extensive efforts for validation @cite , an increase in jitter is counterproductive to the validation effort.
- In @cite Mahajan propose mechanisms for detecting and controlling high bandwidth traffic aggregates. One part of their work discusses how a node determines whether it is congested and how it identifies the aggregate(s) responsible for the congestion. In contrast, we start from the point where the node has identified the undesired flow(s). In that sense, their work and our work are complementary. Another part of their work discusses how much to rate-limit an annoying aggregate due to a DoS attack or a flash crowd. In contrast, our mechanism focuses on DoS attack traffic and attempts to limit it to rate @math . We believe that DoS attacks should be addressed separately from flash crowds: Flash crowd aggregates are created by legitimate traffic. Therefore, it makes sense to rate-limit them instead of completely blocking them. On the contrary, DoS attack traffic aims at disrupting the victim's operation. Therefore, it makes sense to block it. Blocking a traffic flow is simpler and cheaper than rate-limiting it. Moreover, DoS attack traffic is generated by malicious compromised nodes. Therefore, it demands a more intelligent defense mechanism.
- In @cite Park and Lee propose DPF (Distributed Packet Filtering), a distributed ingress-filtering mechanism for pro-actively blocking spoofed flows. In contrast, AITF aims at blocking undesired -- including spoofed -- flows as close as possible to their sources. Thus, it cannot be replaced by DPF. On the other hand, DPF blocks most spoofed flows they reach their destination i.e., DPF is proactive, whereas AITF is reactive. In that sense, DPF and AITF are complementary.
- In @cite Keromytis propose SOS (Secure Overlay Services), an architecture for pro-actively protecting against DoS attacks the communication between a pre-determined location and a specific set of users who have authorized access to communicate with that location. In contrast, AITF addresses the more general problem of protecting against DoS attacks any location accessible to all Internet users.
- To see what problems MIPs model, note, from constraints (i) and (iii) of MIPs, that for all @math , any feasible solution will make the set @math have precisely one 1, with all other elements being 0; MIPs thus model many choice'' scenarios. Consider, e.g., global routing in VLSI gate arrays @cite . Given are an undirected graph @math , a function @math , and @math , a set @math of paths in @math , each connecting @math to @math ; we must connect each @math with @math using exactly one path from @math , so that the maximum number of times that any edge in @math is used for, is minimized--an MIP formulation is obvious, with @math being the indicator variable for picking the @math th path in @math . This problem, the vector-selection problem of @cite , and the discrepancy-type problems of , are all modeled by MIPs; many MIP instances, e.g., global routing, are NP-hard.
- Next, there is growing interest in optimization, since different participating individuals and or organizations may have different objective functions in a given problem instance; see, e.g., @cite . Motivated by this, we study multi-criteria optimization in the setting of covering problems:
- Given an ILP, we can find an optimal solution @math to its LP relaxation efficiently, but need to round fractional entries in @math to integers. The idea of randomized rounding is: given a real @math , round @math to @math with probability @math , and round @math to @math with probability @math . This has the nice property that the mean outcome is @math . Starting with this idea, the analysis of @cite produces an integral solution of value at most @math for MIPs (though phrased a bit differently); this is derandomized in @cite . But this does not exploit the sparsity of @math ; the previously-mentioned result of @cite produces an integral solution of value at most @math .
- For CIPs, the idea is to solve the LP relaxation, scale the components of @math suitably, and then perform randomized rounding; see for the details. Starting with this idea, the work of @cite leads to certain approximation bounds; similar bounds are achieved through different means by Plotkin, Shmoys & Tardos @cite . Work of this author @cite improved upon these results by observing a correlation'' property of CIPs, getting an approximation ratio of @math . Thus, while the work of @cite gives a general approximation bound for MIPs, the result of @cite gives good results for sparse MIPs. For CIPs, the current-best results are those of @cite ; however, no better results were known for sparse CIPs.
- A key corollary of our results is that for families of instances of CIPs, we get a good ( @math or @math ) integrality gap if @math grows at least as fast as @math . Bounds on the result of a greedy algorithm for CIPs relative to the optimal solution, are known @cite @cite . Our bound improves that of @cite and is incomparable with @cite ; for any given @math , @math , and the unit vector @math , our bound improves on @cite if @math is more than a certain threshold. As it stands, randomized rounding produces such improved solutions for several CIPs only with a very low, sometimes exponentially small, probability. Thus, it does not imply a randomized algorithm, often. To this end, we generalize Raghavan's method of pessimistic estimators to derive an algorithmic (polynomial-time) version of our results for CIPs, in .
- There is ongoing work in the field of complexity that attempts to study they dynamics of complex adaptive systems @cite . Our approach is based on ideas borrowed from the use of NK landscapes for the analysis of co-evolving systems. As such, we are using some of the results from that field. However, complexity theory is more concerned with explaining the dynamic behavior of existing systems, while we are more concerned with the engineering of multiagent systems for distributed service allocation.
- The Collective Intelligence (COIN) framework @cite shares many of the same goals of our research. They start with a global utility function from which they derive the rewards functions for each agent. The agents are assumed to use some form of reinforcement learning. They show that the global utility is maximized when using their prescribed reward functions. They do not, however, consider how agent communication might affect the individual agent's utility landscape.
- The task allocation problem has been studied in @cite , but the service allocation problem we present in this paper has received very little attention. There is also work being done on the analysis of the dynamics of multiagent systems for other domains such as e-commerce @cite and automated manufacturing @cite . It is possible that extensions to our approach will shed some light into the dynamics of these domains.
- This result immediately implies that @math can be solved by solving a linear system with @math variables and equations over the rationals (with total bit-size @math ), thus easily yielding @math . That @math then follows immediately from the fact that linear algebra can be efficiently parallelized @cite .
- In the pilot task, we focus on citations and the text spans they cite in the original article. The importance of citations for summarization is discussed in @cite , which compared summaries that were based on three different things: only the reference article; only the abstract; and, only citations. The best results were based on citations. @cite also showed that the information from citations is different from that which can be gleaned from just the abstract or reference article. However, it is cautioned that citations often focus on very specific aspects of a paper @cite .
- Because of this recognized importance of citation information, research has also been done on properly tagging or marking the actual citation. Powley and Dale @cite give insight into recognizing text that is a citation. Siddharthan and Teufel demonstrate how this is useful in reducing the noise when comparing citation text to reference text @cite . Siddharthan and Teufel also introduce scientific attribution'' which can help in discourse classification. The importance of discourse classification is further developed in @cite : they were able to show how identifying the discourse facets helps produce coherent summaries.
- The choice of proper features is very important in handling citation text. Previous research @cite @cite gives insight into these features. We find in @cite an in-depth analysis of the usefulness of certain features. As a result, we have used it to guide our selection of which features to include.
- In addition to these features, we have to consider that multiple citation markers may be present in a sentence. Thus, only certain parts of a sentence may be relevant to identifying the target of a particular citation marker. Qazvinian and Radev @cite share an approach to find the fragment of a sentence that applies to a citation, especially in the case of sentences with multiple citation markers. The research of Abu-Jbara and Radev @cite further argues that a fragment need not always be continguous.
- In order to leverage the great success of deep neural networks for image classification @cite @cite , considerable object detection methods based on deep learning have been proposed @cite @cite @cite @cite @cite . Although there are end-to-end detection frameworks, like SSD @cite , YOLO @cite and DenseBox @cite , region-based systems ( Fast Faster R-CNN @cite @cite and R-FCN @cite ) still dominate the detection accuracy on generic benchmarks @cite @cite .
- The type of contextual recommendations that can be made is shaped by sensors and signal processing used. Nowadays it is possible to accurately detect activities such as biking, driving, running, or walking based on smartphone sensors @cite , or based on environmental sound cues @cite . It is also possible to detect personality traits based on phone call patterns and social network data of the user @cite . Similarly, interest in an object can be inferred based on ambient noise levels, and positions of people and objects in relation to each other @cite . In the SenSay system, phone settings and preferences are set based on detected environmental and physiological states @cite .
- With improvements in smartphone technology, there is a lot of potential for using rich contextual information to improve recommendations, in particular considering that people prefer to listen to different music in different contexts @cite @cite @cite . Among the first to propose a context-aware music recommendation system are @cite . They used weather data (from sensors and external data sources), and user information, to predict the appropriate music genre, tempo, and mood. Music can also be recommended based on user's heart beat to bring its rate to a normal level @cite ; activities detected automatically (e.g. running, walking, sleeping, working, studying, and shopping) @cite ; driving style, road type, landscape, sleepiness, traffic conditions, mood, weather, natural phenomena @cite ; and emotional state to help to transition to a desired state @cite . Soundtracks have also been recommended for smartphone videos based on location (using GPS and compass data for orientation), and extra information from 3rd party services such as Foursquare @cite .
- These examples use sensors and external data sources for music recommendation. Some of these context-aware music discovery systems recommend not just relevant, but new music to users @cite . Our contribution is to combine rich context in a way that is a) fault tolerant, and b) aims to facilitate music discovery, by constructing a momentary ephemeral context.
- Unsupervised learning of visual representations is a research area of particular interest. Approaches to unsupervised learning can be roughly categorized into two main streams: (i) generative models, and (ii) self-supervised learning. Earlier methods for generative models include Anto-Encoders @cite @cite @cite @cite and Restricted Boltzmann Machines (RBMs) @cite @cite @cite @cite . For example, Le al @cite trained a multi-layer auto-encoder on a large-scale dataset of YouTube videos: although no label is provided, some neurons in high-level layers can recognize cats and human faces. Recent generative models such as Generative Adversarial Networks @cite and Variational Auto-Encoders @cite are capable of generating more realistic images. The generated examples or the neural networks that learn to generate examples can be exploited to learn representations of data @cite @cite .
- Self-supervised learning is another popular stream for learning invariant features. Visual invariance can be captured by the same instance scene taken in a sequence of video frames @cite @cite @cite @cite @cite @cite @cite @cite @cite @cite . For example, Wang and Gupta @cite leverage tracking of objects in videos to learn visual invariance within individual objects; Jayaraman and Grauman @cite train a Siamese network to model the ego-motion between two frames in a scene; Mathieu al @cite propose to learn representations by predicting future frames; Pathak al @cite train a network to segment the foreground objects where are acquired via motion cues. On the other hand, common characteristics of different object instances can also be mined from data @cite @cite @cite @cite @cite . For example, relative positions of image patches @cite may reflect feasible spatial layouts of objects; possible colors can be inferred @cite @cite if the networks can relate colors to object appearances. Rather than rely on temporal changes in video, these methods are able to exploit still images.
- Our work is also closely related to mid-level patch clustering @cite @cite @cite and unsupervised discovery of semantic classes @cite @cite as we attempt to find reliable clusters in our affinity graph. In addition, the ranking function used in this paper is related to deep metric learning with Siamese architectures @cite @cite @cite @cite @cite .
- Our generic framework can be instantiated by any two self-supervised methods that can respectively learn inter- intra-instance invariance. In this paper we adopt Doersch al's @cite context prediction method to build inter-instance invariance, and Wang and Gupta's @cite tracking method to build intra-instance invariance. We analyze their behaviors as follows.
- The context prediction task in @cite randomly samples a patch (blue in Figure ) and one of its eight neighbors (red), and trains the network to predict their relative position, defined as an 8-way classification problem. In the first two examples in Figure , the context prediction model is able to predict that the leg" patch is below the face'' patch of the cat, indicating that the model has learned some commonality of spatial layout from the training data. However, the model would fail if the pose, viewpoint, or deformation of the object is changed drastically, , in the third example of Figure --- unless the dataset is diversified and large enough to include gradually changing poses, it is hard for the models to learn that the changed pose can be of the same object type.
- On the other hand, these changes can be more successfully captured by the visual tracking method presented in @cite , , see A A' and B B' in Figure . But by tracking an identical instance we cannot associate different instances of the same semantics. Thus we expect the representations learned in @cite are weak in handling the variations between different objects in the same category.
- In recent years, generative adversarial network (GAN) @cite has gained a wide range of interests in generative modeling. In a GAN, a generator is trained to produce fake but plausible images, while a discriminator is trained to distinguish difference between real and fake images. Conditional generative adversarial network (CGAN) @cite is the conditional version of GAN in which the generator is feeded with noise vector together with additional data (e.g., class labels) that conditions on both the generator and discriminator. Deep convolutional generative adversarial network (DCGAN) @cite is an extensive exploration of convolution neural network architectures in GAN and contributes to improve the quality of image synthesis. GANs have been successfully leveraged to many image generation applications @cite @cite @cite @cite . Our method adopts the adversarial loss to render images from the generators to be real in the target domain and make meta-training performance improve meta-learners' generalization.
- Early works in lane detection and departure warning system date back to the 1990s. Previously proposed methods in this area can be classified as low-level image feature based, machine deep learning (DL) based approaches, or a hybrid between the two. The most widely used LDW systems are either vision-based (e.g., histogram analysis, Hough transformation) or more recently on DL. In general, vision-based and DL lane detection systems start by capturing images using a selected type of sensor, pre-processing the image, followed by lane line detection and tracking. While many types of sensors have been proposed for capturing lanes images such as radars, laser range, lidar, active infrared etc., the most widely used device is a mobile camera. An alternative to vision- and DL-based systems is the use of global-positioning systems (GPS) combined with Geographic Information Systems @cite . However, current LDW based on GPS can be unreliable, mainly because of the often poor reliability and resolution of GPS location and speed detection, signal loss (e.g., in covered areas), and inaccurate map databases. Due to these limitations, most modern research conducted in LDW involves a utilization of Neural Networks-based solutions in some form.
- Neural Networks have been a subject of investigation in the autonomous vehicles field for a while. Among the very first attempts to use a neural network for vehicle navigation, ALNINN @cite is considered a pioneer and one of the most influential paper. This model is comprised of a shallow neural network that predicts actions out of captured images from a forward facing camera mounted on-board a vehicle, with few obstacles, leading to the potential use of neural networks for autonomous navigation. More recently, advances in object detection such as the contribution made by DL and Region Convolutional Neural Network (R-CNN) @cite in combination with Region Proposal Network (RPN) @cite have created models such as Mask R-CNN @cite that provide state of the art predictions. New trends in Neural Network object detection include segmentation, which we applied in our model as an estimator for LDW.
- Image feature-based lane detection is a well researched area of computer vision @cite . The majority of existing image-based methods use detected lane line features such as colors, gray-scale intensities, and textural information to perform edge detection. This approach is very sensitive to illumination and environmental conditions. On the Generic Obstacle and Lane Detection system proposed by Bertozzi and Broggi @cite , lane detection was done using inverse perspective mapping to remove the perspective effect and horizontal black-white-black transaction. Their methodology was able to locate lane markings even in the presence of shadows or other artifacts in about 95 In 2005, Lee and Yi @cite introduced the use of Sobel operator plus non-local maximum suppression (NLMS). It was built upon methods previously proposed by Lee @cite proposing linear lane model and edge distribution function (EDF) as well as lane boundary pixel extractor (LBPE) plus Hough transform. The model was able to overcome weak points of the EDF based lane-departure identification (LDI) system by increasing lane parameters. The LBPE improved the robustness of lane detection by minimizing missed detections and false positives (FPs) by taking advantage of linear regression analysis. Despite improvements, the model performed poorly at detecting curved lanes.
- Some of the low-level image feature based models include an initial layer to normalize illumination across consecutive images, other methods rely on filters or statistic models such as random sample consensus (RANSAC) @cite . Lately, approaches have been incorporating machine learning, more specifically, deep learning in regards to increase image quality before detection is conducted. However, image feature-based approaches require continuous lane detections and often fail to detect lanes when edges and colors are not clearly delineated (noisy), which results in inability to capture local image feature based information. End-to-end learning from deep neural networks substantially improves model robustness in the face of noisy images or roadway features by learning useful features from deeper layers of convolution.
- To create lane detection models that are robust to environmental (e.g., illumination, weather) and road variation (e.g., clarity of lane markings), CNN is becoming an increasingly popular method. Lane detection on the images shown in Fig. (a-d) are near to impossible without using CNN. Kim and Lee @cite combined a CNN with the RANSAC algorithm to detect lanes edges on complex scenes with includes roadside trees, fences, or intersections. In their method, CNN was primarily used to enhance images. @cite , they showed how existing CNNs can be used to perform lane detection while running at frame rates required for a real-time system. Also, @cite discussed how they overcame the difficulties of detecting traffic signs from low-quality noisy videos using chain-code aggregated channel features (ACF)-based model and a CNN model, more specifically Fast-RCNN.
- More recently, in @cite , they used a Dual-View Convolutional Neural Network (DVCNN) with hat-like filter and optimized simultaneously the frontal-view and the top-view cameras. The hat-like filter extracts all potential lane line candidates, thus removing most of FPs. With the front-view camera, FPs such as moving vehicles, barriers, and curbs were excluded. Within the top-view image, structures other than lane lines such as ground arrows and words were also removed.
- The objective of Lane Departure Prediction (LDP) is to predict if the driver is likely to leave the lane with the goal of warning drivers in advance of the lane departure so that they may correct the error before it occurs (avoiding a potential collision). This improves on LDW systems, which simply alert the driver to the error after it has occurred. LDP algorithms can be classified into one of the following three categories: vehicle-variable-based, vehicle-position estimation, and detection of the lane boundary using real-time captured road images. They all use real-time captured images @cite .
- The TLC model has been extensively used on production vehicles @cite . TLC systems evaluate the lane and vehicle state relying on vision-based equipment and perform TLC calculations online using a variety of algorithms. A TLC threshold is used to trigger an alert to the driver. Different computational methods are used with regard to the road geometries and vehicle types. Among these methods, the most common method used is to predict the road boundary, the vehicle trajectory, and then calculate intersection time of the two at the current driving speed. On small curvature roads, the TLC can be computed as the ratio of lateral distance to lateral velocity or the ratio of the distance to the line crossing. @cite Studies suggest that TLC tend to have a higher false alarm rate (FAR) when the vehicle is driven close to lane boundary @cite @cite .
- @cite proposed a online learning-based approach to predict unintended lane-departure behaviors (LDB) depending on personalized driver model (PDM) and Hidden Markov Model (HMM). The PDM describes the drivers lane-keeping and lane-departure behaviors by using a joint-probability density distribution of Gaussian mixture model (GMM) between vehicle speed, relative yaw angle, relative yaw rate, lateral displacement, and road curvature. PDM can discern the characteristics of individuals driving style. In combination with HMM to estimate the vehicles lateral displacement, they were able to reduce the FAR by 3.07.
- Single-RAT Multi-BS Communication. Prior works have studied the problem of traffic aggregation when a client can simultaneously communicate with multiple same technology BSs. For example, @cite uses game theory to model selfish traffic splitting by each client in WLANs. On the other hand, the resource allocation problem in HetNets is primarily addressed at the BS side. Similarly, @cite proposes an approximation algorithm to address the problem of client association and traffic splitting in LTE DC. Our algorithm (AFRA) goes beyond this and other related work by guaranteeing optimal resource allocation for any number of RATs and BSs . Other works have developed centralized client association algorithms to achieve max-min @cite and proportional fairness @cite in multi-rate WLANs. In contrast, the problem of resource allocation in HetNets needs to be solved in a fully distributed manner.
- An early effort on handling defeasible causal rules in reasoning about action was due to the author's previous work @cite , in which the author identified the restriction of McCain and Turner's causal theory of actions @cite and claimed that in general a causal rule should be treated as a defeasible rule in order to solve the ramification problem properly. In @cite , constraints ) and ) simply correspond to defaults @math and @math respectively. By combining Reiter's default theory @cite and Winslett's PMA @cite the author developed a causality-based minimal change principle for reasoning about action and change which subsumes McCain and Turner's causal theory.
- Although the work presented in @cite provided a natural way to represent causality in reasoning about action, there were several restrictions in this action theory. First, due to technical restrictions, only normal defaults or defaults without justifications are the suitable forms to represent causal rules in problem domains. Second, this action theory did not handle the other two major defeasibilities - defeasible observations and actions with defeasible and abnormal effects.
- Learning to Navigate. Navigation has traditionally been approached by either employing supervised learning (SL) methods @cite @cite @cite @cite @cite @cite @cite or reinforcement learning (RL) methods @cite @cite @cite @cite @cite @cite . Furthermore, combinations of the two have been proposed in an effort to leverage advantages of both techniques, e.g. for increasing sample efficiency for RL methods @cite @cite @cite @cite @cite . For the case of controlling physics-driven vehicles, SL can be advantageous when acquiring labeled data is not too costly or inefficient, and has been proven to have relative success in the field of autonomous driving, among other applications, in recent years @cite @cite @cite . However, the use of neural networks for SL in autonomous driving goes back to much earlier work @cite @cite .
- In the work of @cite , a deep neural network (DNN) is trained to map recorded camera views to 3-DoF steering commands (steering wheel angle, throttle, and brake). Seventy-two hours of human driven training data was tediously collected from a forward facing camera and augmented with two additional views to provide data for simulated drifting and corrective maneuvering. The simulated and on-road results of this pioneering work demonstrate the ability of a DNN to learn (end-to-end) the control process of a self-driving car from raw video data.
- Similar to our work but for cars, @cite use TORCS (The Open Racing Car Simulator) @cite to train a DNN to drive at casual speeds through a course and properly pass or follow other vehicles in its lane. This work builds off earlier work using TORCS, which focused on keeping the car on a track @cite . In contrast to our work, the vehicle controls to be predicted in the work of @cite are limited, since only a small discrete set of expected control outputs are available: turn-left, turn-right, throttle, and brake. Recently, TORCS has also been successfully used in several RL approaches for autonomous car driving @cite @cite @cite ; however, in these cases, RL was used to teach the agent to drive specific tracks or all available tracks rather than learning to drive never before seen tracks.
- @cite trained a network on autonomous car datasets and then deployed it to control a drone. For this, they used full supervision by providing image and measured steering angle pairs from pre-collected datasets, and collecting their own dataset containing image and binary obstacle indication pairs. While they demonstrate an ability to transfer successfully to other environments, their approach does not model and exploit the full six degrees of freedom available. It also focuses on slow and safe navigation, rather than optimizing for speed as is the case for racing. Finally, with their network being fairly complex, they report an inference speed of 20fps (CPU) for remote processing, which is more than three times lower than the estimated frame rate for our proposed method when running on-board processing, and more than 27 times lower compared to our method running remotely on GPU.
- Simulation. As mentioned earlier, generating diverse natural' training data for sequential decision making through SL is tedious. Generating additional data for exploration purposes (i.e. in scenarios where both input and output pairs have to be generated) is much more so. Therefore, a lot of attention from the community is being given to simulators (or games) for this source of data. In fact, a broad range of work has exploited them recently for these types of learning, namely in animation and motion planning @cite @cite @cite @cite @cite @cite @cite , scene understanding @cite @cite , pedestrian detection @cite , and identification of 2D 3D objects @cite @cite @cite . For instance, the authors of @cite used Unity, a video game engine similar to Unreal Engine, to teach a bird how to fly in simulation.
- Moreover, there is another line of work that uses hardware-in-the-loop (HILT) simulation. Examples include JMAVSim @cite @cite which was used to develop and evaluate controllers and RotorS @cite which was used to study visual servoing. The visual quality of most HIL simulators is very basic and far from photo-realistic with the exception of AirSim @cite . While there are multiple established simulators such as Realflight, Flightgear, or XPlane for simulating aerial platforms, they have several limitations. In contrast to Unreal Engine, advanced shading and post-processing settings are not available and the selection of assets and textures is limited. Recent work @cite @cite @cite @cite @cite highlights how modern game engines can be used to generate photo-realistic training datasets and pixel-accurate segmentation masks. The goal of this work is to build an automated UAV flying system (based on imitation learning) that can relatively easily be transitioned from a simulated world to the real one. Therefore, we choose Sim4CV @cite @cite as our simulator, which uses the open source game engine UE4 and provides a full software in-the-loop UAV simulation. The simulator also provides a lot of flexibility in terms of assets, textures, and communication interfaces.
- Early works, from @math s, have made efforts on automating neural network design which often searched good architecture by the genetic algorithm or other evolutionary algorithms @cite @cite @cite @cite @cite @cite @cite . Nevertheless, these works, to our best knowledge, cannot perform competitively compared with hand-crafted networks. Recent works, Neural Architecture Search (NAS) @cite and MetaQNN @cite , adopted reinforcement learning to automatically search a good network architecture. Although they can yield good performance on small datasets such as CIFAR- @math , CIFAR- @math , the direct use of MetaQNN or NAS for architecture design on big datasets like ImageNet @cite is computationally expensive via searching in a huge space. Besides, the network generated by this kind of methods is task-specific or dataset-specific, that is, it cannot been well transferred to other tasks nor datasets with different input data sizes. For example, the network designed for CIFAR- @math cannot been generalized to ImageNet.
- Instead, our approach is aimed to design network block architecture by an efficient search method with a distributed asynchronous Q-learning framework as well as an early-stop strategy. The block design conception follows the modern convolutional neural networks such as Inception @cite @cite @cite and Resnet @cite @cite . The inception-based networks construct the inception blocks via a hand-crafted multi-level feature extractor strategy by computing @math , @math , and @math convolutions, while the Resnet uses residue blocks with shortcut connection to make it easier to represent the identity mapping which allows a very deep network. The blocks automatically generated by our approach have similar structures such as some blocks contain short cut connections and inception-like multi-branch combination. We will discuss the details in .
- Another bunch of related works include hyper-parameter optimization @cite , meta-learning @cite and learning to learn methods @cite @cite . However, the goal of these works is to use meta-data to improve the performance of the existing algorithms, such as finding the optimal learning rate of optimization methods or the optimal number of hidden layers to construct the network. In this paper, we focus on learning the entire topological architecture of network blocks to improve the performance.
- @cite present an empirical study of generalization in Deep-RL, testing interpolation and extrapolation performance of state-of-the-art algorithms when varying simulation parameters in control tasks. The authors provide an experimental assessment of generalization under varying training and testing distributions. Our work extends these results by providing results for the case when the training distribution parameters are learned and change during policy training.
- @cite propose training policies on a distribution of simulators, whose parameters are fit to real-world data. Their proposed algorithm switches back and forth between optimizing the policy under the DR distribution and updating the DR distribution by minimizing the discrepancy between simulated and real world trajectories. In contrast, we aim to learn policies that maximize performance over a diverse distribution of environments where the task is feasible, as a way of minimizing the interactions with the real robot system.
- @cite propose a related approach for learning robust policies over a distribution of simulator models. The proposed approach, based on the the @math -percentile conditional value at risk (CVaR) @cite objective, improves the policy performance on a small proportion of environments where the policy performs the worst. The authors propose an algorithm that updates the distribution of simulation models to maximize the likelihood of real-world trajectories, via Bayesian inference. The combination of worst-case performance optimization and Bayesian updates ensures that the resulting policy is robust to errors in the estimation of the simulation model parameters. Our method can be combined with the CVaR objective to encourage diversity of the learned DR distribution.
- Related to learning the DR distribution, @cite propose using Bayesian Optimization (BO) to update from the simulation model distribution. This is done by evaluating the improvement over the current policy by using a policy gradient algorithm with data sampled from the current simulator distribution. The parameters of the simulator distribution for the next iteration are selected to maximize said improvement.
- @cite also use context-conditioned policies, where the context is implicitly encoded into a vector @math . During the training phase, their proposed algorithm improves on the performance of the policy while learning a probabilistic mapping from trajectory data to context vectors. At test time, the learned mapping is used for online inference of the context vector. This is similar in spirit to the Universal Policies with Online System Identification method @cite , which instead uses deterministic context inference with an explicit context encoding. Again, these methods use a fixed DR distribution and could benefit from adapting it during training, as we propose in this work.
- Traditional NMF @cite assumes that noise obeys a Gaussian distribution and derives the following squared @math -norm based objective function: @math , where @math signifies the matrix Frobenius norm. It is commonly known that NMF can be solved by using the multiplicative update rule (MUR, @cite ). Because of the nice mathematical property of squared @math -norm and the efficiency of MUR, NMF has been extended for various applications @cite @cite @cite . However, NMF and its extensions are non-robust because the @math -norm is sensitive to outliers.
- Zhang . @cite assumed that the dataset contains both Laplace distributed noise and Gaussian distributed noise and proposed an @math -norm regularized Robust NMF (RNMF- @math ) as follows: @math , where @math is a positive constant that trades off the sparsity of @math . Similar to @math -NMF, RNMF- @math is also less sensitive to outliers than NMF, but they are both non-robust to large numbers of outliers because the @math -minimization model has a low breakdown point. Moreover, it is non-trivial to determine the tradeoff parameter @math .
- Recently, Clingempeel-Le Floch-Romo @cite compared the hemisphere partition functions (which in our language correspond to certain solutions of the quantum D-modules) of the cyclic quotient singularities @math and their Hirzebruch-Jung resolutions. They discussed the relation to a semiorthogonal decomposition of the the derived categories, extending the work of Herbst-Hori-Page @cite to the anomalous (discrepant) case. Their examples are complementary to ours: the Hirzebruch-Jung resolutions are type (II-ii) discrepant transformations whereas transformations in Theorem are of type (II-i) or (III) (see Remark for these types).
- For three-coloring, we know of several relevant references. Lawler @cite is primarily concerned with the general chromatic number, but he also gives the following very simple algorithm for 3-coloring: for each maximal independent set, test whether the complement is bipartite. The maximal independent sets can be listed with polynomial delay @cite , and there are at most @math such sets @cite , so this algorithm takes time @math . Schiermeyer @cite gives a complicated algorithm for solving 3-colorability in time @math , based on the following idea: if there is one vertex @math of degree @math then the graph is 3-colorable iff @math is bipartite, and the problem is easily solved. Otherwise, Schiermeyer performs certain reductions involving maximal independent sets that attempt to increase the degree of @math while partitioning the problem into subproblems, at least one of which will remain solvable. Our @math bound significantly improves both of these results.
- There has also been some related work on approximate or heuristic 3-coloring algorithms. Blum and Karger @cite show that any 3-chromatic graph can be colored with @math colors in polynomial time. Alon and Kahale @cite describe a technique for coloring random 3-chromatic graphs in expected polynomial time, and Petford and Welsh @cite present a randomized algorithm for 3-coloring graphs which also works well empirically on random graphs although they prove no bounds on its running time. Finally, Vlasie @cite has described a class of instances which are (unlike random 3-chromatic graphs) difficult to color.
- Several attempts to apply formal verification to network protocols have been made. Assertional proof techniques were used to prove distance vector routing @cite , path vector routing @cite and route diffusion algorithms @cite @cite and @cite using communicating finite state machines. An example point-to-point mobile application was proved using assertional reasoning in @cite using UNITY @cite . Axiomatic reasoning was used in proving a simple transmission protocol in @cite . Algebraic systems based on the calculus of communicating systems (CCS) @cite have been used to prove CSMA CD @cite . Formal verification has been applied to TCP and T TCP in @cite .
- The combination of timed automata, invariants, simulation mappings, automaton composition, and temporal logic @cite seem to be very useful tools for proving (or disproving) and reasoning about safety or liveness properties of distributed algorithms. It may also be used to establish asymptotic bounds on the complexity of the distributed algorithms. It is not clear, however, how theorem proving techniques can be used in test synthesis to construct event sequences and topologies that stress network protocols. Parts of our work draw from distributed algorithms verification principles. Yet we feel that our work complements such work, as we focus on test synthesis problems.
- Lassez87 proposed the seminal algorithm for computing relative complements and introduced the now familiar restriction to linear terms. We quote the definition of the @math '' algorithm for the (singleton) complement problem given in @cite which we generalize in Definition . Given a finite signature @math and a linear term @math they define: [ ] The relative complement problem is then solved by composing the above complement operation with term intersection implemented via first-order unification.
- The class of higher-order patterns inherits many properties from first-order terms. However, as we will see, it is closed under complement, but a special subclass is. We call a canonical pattern @math if each occurrence of an existential variable @math under binders @math is applied to some permutation of the variables in @math and @math . Fully applied patterns play an important role in functional logic programming and rewriting @cite , because any fully applied existential variable @math denotes all canonical terms of type @math with parameters from @math . It is this property which makes complementation particularly simple.
- Log-based reconciliation is a new topic for which few algorithms have been developed. The only implementation we know of is the IceCube system reported in @cite . It is worth noting that the objective function of maximizing the number of accepted actions, is different from maximizing the number of satisfied constraints. For that reason, the modeling of log-based reconciliation as a max-CSP problem is inadequate. This is also the main reason why in our second program based on local search, the min-conflict heuristics @cite or the adaptive search method of @cite do not perform well in our modeling, and we use instead a randomized Tabu heuristics.
- The urgent need for a network-wide, scalable approach to the problem of healthy network traffic profile creation is expressed in works of @cite @cite @cite @cite @cite @cite . There are several studies with the promising results, which demonstrate that the traffic anomalous events cause the temporal changes in statistical properties of traffic features. Lakhina, Crovella and Diot presented the characterization of the network-wide anomalies of the traffic flows. The authors studied three different types of traffic flows and fused the information from flow measurements taken throughout the entire network. They obtained and classified a different set of anomalies for different traffic types using the subspace method @cite .
- The same group of researchers extended their work in @cite . Under the new assumption that any network anomaly induces the changes in distributional aspects of packet header fields, they detected and identified large set of anomalies using the entropy measurement tool.
- Hidden Markov model has been proposed to model the distribution of network-wide traffic in @cite . The observation window is used to distinguish denial of service (DoS) flooding attack mixed with the normal background traffic.
- In the past few decades, many works focus on designing different types of features to capture the characteristics of images such as color, SIFT and HoG @cite . Based on these feature descriptors, Bag-of-Feature (BoF) model seems to be the most classical image representation method in computer vision and related multimedia applications. Several promising studies @cite @cite @cite were published to improve this traditional approach in different aspects. Among these extension, a class of sparse coding based methods @cite @cite , which employ spatial pyramid matching kernel (SPM) proposed by Lazebnik , has achieved great success in image classification problem. Despite we are developing more and more effective representation methods, the lack of high-level image expression still plagues us to build up the ideal vision system.
- On the other hand, learning hierarchical models to simultaneously construct multiple levels of visual representation has received much attention recently @cite . Our deep boosting method is partially motivated by recent developed deep learning techniques @cite @cite @cite . Different from previous hand-craft feature design method, deep model learns the feature representation from raw data and validly generates the high-level semantic representation. However, as shown in recent study @cite , these network-based hierarchical models always contain thousands of nodes in a single layer, and is too complex to control in real multimedia application. In contrast, an obvious characteristic of our study is that we build up the deep architecture to generate expressive image representation simply and obtains the near optimal classification rate in each layer.
- Finding the transformation given by a set of point correspondences is a common problem in computer vision, e.g., for ego-motion estimation. A method to get a closed-form solution by means of a Least-Squares Estimation (LSE) is given in @cite . However, when dealing with sets of point correspondences containing wrong associations, the result given by a LSE is distorted by the outliers. This is problem is commonly addressed by using a sample consensus method such as RANSAC @cite .
- Zhang @cite provide an uncertainty estimation of a 3D stereo-based localization approach using a correspondence-based method to estimate the robot pose. As in our work, visual features are extracted from the image and RANSAC is used to remove the outliers from the initial matches between features in two consecutive images. In contrast, our approach establishes correspondences by image-to-map matching. Thus, additional sources of false correspondences arise, such as repeated objects in the world, the presence of several features in the map extracted from the same point in the world, or the much larger number of features in the map, which increases the chance for random matches.
- MCL overcomes the limitations of EKFs as mentioned earlier. It was successfully used in @cite for vision-based localization of a tour-guide robot in a museum using a map of the ceiling and a camera pointing to it. In contrast to this approach, we do not rely on odometry measurements to predict the pose, and are not restricted to planar motion. Additionally, MCL would report incorrect locations after unexpected robot motions or sensor outages. Sensor Resetting Localization @cite partially substitutes particles by new ones directly generated from the sensor measurements when the position estimate is uncertain. Mixture-MCL @cite combines standard MCL with dual-MCL to drastically reduce computational cost and localization error. Dual-MCL also generates particles from the current sensor measurements and was shown to deal well with the kidnapped robot problem when properly combined with standard MCL. We do not need a reset process, since our estimation is independent from the prior state. Our approach could be used in combination with Monte Carlo Localization (MCL) for efficient particle initialization and weighting. However, the results of our experiments show that our estimate is accurate enough to be used as final result, without any filtering.
- To the best of our knowledge, at the time being, there is no other dedicated global localization approach for the recently introduced RGB-D sensors. However, a number of novel approaches for visual odometry have been proposed, which exploit the available combination of color, density of depth and the high frame rate to improve alignment performance as compared, e.g., to the iterative closest point algorithm @cite . In @cite and @cite adaptations of ICP are proposed to process the high amounts of data more efficiently. Steinbruecker @cite present a transformation estimation based on the minimization of an energy function. For frames close to each other, they achieve enhanced runtime performance and accuracy compared to Generalized ICP @cite . Using the distribution of normals, Osteen @cite improve the initialization of ICP by efficiently computing the difference in orientation between two frames, which allows a substantial drift reduction. These approaches work well for computing the transformation for small incremental changes between consecutive frames, but they are of limited applicability for global localization in a map.
